{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"82085ca8-fc6a-8f05-ffb5-05abcce1177c"},"source":"In this script I will explore the basic and famous classification algorithm of SVM, in comparison to the logistic regression. including:\n\n 1. SVM and logistic regression decision line and accuracy\n 2. SVM effect of regularization\n 3. SVM and non-linear Kernel\n 4. SVM and class imbalance \n\nThis script is the first in a series of scripts going over Andrew Ng's canonical [ML course][1] on Coursera. In the future you can find other scripts exploring the algorithms in my Kaggle profile.\n\n I have used some of the code which is available in [scikit-learn documentation][2].\n\n \n\n\n  [1]: https://www.coursera.org/learn/machine-learning/home/welcome\n  [2]: http://scikit-learn.org/stable/modules/svm.html#svm"},{"cell_type":"markdown","metadata":{"_cell_guid":"1f9286f4-3201-0bf3-7f5a-a134b8f14977"},"source":"First, let's load all the relevant libraries and read the data: "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3020c0a-f7fc-c769-97fa-94ebf9a26461"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom subprocess import check_output\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nplt.style.use('fivethirtyeight')\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\ndf = pd.read_csv('../input/data.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"d8239c24-09a2-5f21-f028-36102df21e7c"},"source":"Now, let's explore some correlations, similarly to what other Kagglers have done with this data set:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1afbc993-7050-e0d1-6536-612512263914"},"outputs":[],"source":"g = sns.PairGrid(df[[df.columns[1],df.columns[2],df.columns[3],df.columns[4], df.columns[5],df.columns[6]]],hue='diagnosis')\ng = g.map_diag(plt.hist)\ng = g.map_offdiag(plt.scatter, s = 3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7a12ef7f-2ee5-dc64-18f4-03ed911c8941"},"source":"For a more visual exploration, it would be best to work in a 2-dimensional world. However, a lot of feature pairs divide nicely the data to a similar extent. therefore, it makes sense to use one of the dimensionality reduction methods to try to use as many features as possible and maintian as much information as possible when working with only 2 dimensions. I will use PCA. based on [Anistropic][1]'s great script it also seems that standardizing the features is crucial.\n\n\n  [1]: https://www.kaggle.com/arthurtok/d/uciml/breast-cancer-wisconsin-data/tsne-pca-quick-and-dirty-visuals"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7dedb4cf-0972-0875-d207-66287a730290"},"outputs":[],"source":"\ndf_std = StandardScaler().fit_transform(df.drop(['id','diagnosis','Unnamed: 32'], axis = 1))\npca = PCA(n_components=2)\npca.fit(df_std)\nTwoD_Data = pca.transform(df_std)\nPCA_df = pd.DataFrame()\nPCA_df['PCA_1'] = TwoD_Data[:,0]\nPCA_df['PCA_2'] = TwoD_Data[:,1]\n\n\nplt.plot(PCA_df['PCA_1'][df.diagnosis == 'M'],PCA_df['PCA_2'][df.diagnosis == 'M'],'o', alpha = 0.7, color = 'r')\nplt.plot(PCA_df['PCA_1'][df.diagnosis == 'B'],PCA_df['PCA_2'][df.diagnosis == 'B'],'o', alpha = 0.7, color = 'b')\nplt.xlabel('PCA_1')\nplt.ylabel('PCA_2')\nplt.legend(['Malignant','Benign'])\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"52817a4d-06b1-46c7-76b2-b0f030217481"},"source":"## Linear Logistic Regrssion\n\nLet's now use a logistic regression classifier. first, we will assess the sensitivity of the results to the regularization coefficient:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85525511-8b2f-5300-b734-1c3aa43364ce"},"outputs":[],"source":"def model(x):\n    return 1 / (1 + np.exp(-x))\n\nPCA_df['target'] = 0\nPCA_df['target'][df.diagnosis == 'M'] = 1\n\ntraindf, testdf = train_test_split(PCA_df, test_size = 0.3)\n\nX = traindf[['PCA_1','PCA_2']]\ny = traindf['target']\nReg = np.linspace(0.1,10,100)\naccuracy = []\nfor C in Reg:\n    clf = LogisticRegression(penalty='l2',C=C)\n    clf.fit(X,y)\n    prediction = clf.predict(testdf[['PCA_1','PCA_2']])\n    loss = prediction - testdf['target']\n    accuracy.append(1 - np.true_divide(sum(np.abs(loss)),len(loss)))\n#loss = model(clf.coef_*X + clf.intercept_)\n\n\nplt.plot(Reg,accuracy,'o')\nplt.xlabel('Regularization')\nplt.ylabel('Validation Score')\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"b7336a11-7f66-75b2-b840-45e60c8b9420"},"source":"The model is not very sensitive to the regularization coefficient.\n\nNow let's train the model with the default regularization and predict on the test set. let us also plot the boundary line determined by the classifier:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"251f97a7-e283-962a-6955-2204024ff09f"},"outputs":[],"source":"clf = LogisticRegression(penalty='l2',C=0.5)\nclf.fit(X,y)\nprint('Training Accuracy.....',clf.score(X,y))\nprediction = clf.predict(testdf[['PCA_1','PCA_2']])\nprint('Validation Accuracy....',clf.score(testdf[['PCA_1','PCA_2']],testdf['target']))\nloss = prediction - testdf['target']\naccuracy = 1 - np.true_divide(sum(np.abs(loss)),len(loss))\n\nradius = np.linspace(min(X.PCA_1), max(X.PCA_2), 100)\nline = (-clf.coef_[0][0]/clf.coef_[0][1])*radius + np.ones(len(radius))*(-clf.intercept_/clf.coef_[0][1])\nplt.plot(radius,line)\nplt.plot(PCA_df['PCA_1'][df.diagnosis == 'M'],PCA_df['PCA_2'][df.diagnosis == 'M'],'o', alpha = 0.7)\nplt.plot(PCA_df['PCA_1'][df.diagnosis == 'B'],PCA_df['PCA_2'][df.diagnosis == 'B'],'o', color = 'b', alpha = 0.7)\nplt.legend(['Decision Line','Malignant','Benign'])\nplt.title('Logistic Regression. Accuracy:' + str(accuracy)[0:4])\nplt.xlabel('PCA_1')\nplt.ylabel('PCA_2')"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c957931-73ce-be8e-075b-9914f59fa6e1"},"source":"## SVM with a linear kernel\n\nNow let's train a linear Support Vector Classifier. a SVM classifier finds the hyper-plane that maximizes the margin between the 2 groups if they are linearly separable. If the data is not linearly separable, as in our case, there is a trade-off between the margin size and the number of points that are on the wrong side of the decision boundary. this is determined by the regularization coefficient. We would see that in our case the effect would be very small."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfdb5f09-1943-940b-80fa-cb3e80a6c0a5"},"outputs":[],"source":"C = 1\nclf2 = SVC(kernel = 'linear',C =C)\nclf2.fit(X, y)\nprint('training accuracy...',clf2.score(X, y, sample_weight=None))\nprint('validation accuracy...',clf2.score(testdf[['PCA_1','PCA_2']],testdf['target']))\n\nw = clf2.coef_[0]\na = -w[0] / w[1]\nxx =  np.linspace(min(X.PCA_1), max(X.PCA_2), 100)\nyy = a * xx - (clf2.intercept_[0]) / w[1]\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\nplt.scatter(clf2.support_vectors_[:, 0], clf2.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10, color = 'g')\nplt.plot(xx, yy)\nplt.title('SVM.' + ' Reg =' + str(C) + 'Accuracy:' + str(clf2.score(testdf[['PCA_1','PCA_2']],testdf['target']))[0:4], fontsize = 10)\n\n\nmu_vec1 = np.array([0,0])\ncov_mat1 = np.array([[2,0],[0,2]])\nx1_samples = np.random.multivariate_normal(mu_vec1, cov_mat1, 100)\nmu_vec1 = mu_vec1.reshape(1,2).T # to 1-col vector\n\nmu_vec2 = np.array([1,2])\ncov_mat2 = np.array([[1,0],[0,1]])\nx2_samples = np.random.multivariate_normal(mu_vec2, cov_mat2, 100)\nmu_vec2 = mu_vec2.reshape(1,2).T\nprint('number of supporting points...',clf2.n_support_ )\n\nplt.legend(['Decision Line','Malignant','Benign'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"d495a5f8-26d5-c740-9e22-5a0de4fb1691"},"source":"The samples denoted with a green circle are the supporting points. In a linearly separable case, these would determine the margin. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a812a7d8-2df1-ee87-ad85-9ff6051cf7f6"},"outputs":[],"source":"plt.plot(xx, yy,'g')\nplt.plot(radius,line,'m')\nplt.title('Comparison of Decision Boundaries')\nplt.legend(['SVM','Logistic Regression'])\nplt.ylim([-10,15])\nplt.xlim([-5,15])\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')"},{"cell_type":"markdown","metadata":{"_cell_guid":"b85891bc-90bb-ef6f-1b3a-6a7c08c351b3"},"source":"We can see in the pot above that the decision boundaries of the 2 classifiers are almost parallel. The accuracy level is therefore similar too."},{"cell_type":"markdown","metadata":{"_cell_guid":"084c299a-88ab-9070-dc03-48e9d52eeb91"},"source":"## SVM with different regularization\n\nthe parameter C denotes the regularization strength (though inversely). If it is very large, the classifier will act like the hard margin classifier for the linearly separable case. Higher C values would yield a smaller number of support vectors.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9308e71a-292f-7700-4664-ae4c92edf05d"},"outputs":[],"source":"plt.subplot(1,2,1)\nC = 1\nclf2 = SVC(kernel = 'linear',C =C)\nclf2.fit(X, y)\nprint('training accuracy...',clf2.score(X, y, sample_weight=None))\nprint('validation accuracy...',clf2.score(testdf[['PCA_1','PCA_2']],testdf['target']))\n\nw = clf2.coef_[0]\na = -w[0] / w[1]\nxx =  np.linspace(min(X.PCA_1), max(X.PCA_2), 100)\nyy = a * xx - (clf2.intercept_[0]) / w[1]\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\nplt.scatter(clf2.support_vectors_[:, 0], clf2.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10, color = 'g')\nplt.plot(xx, yy)\nplt.title('SVM.' + ' Reg =' + str(C) + 'Accuracy:' + str(clf2.score(testdf[['PCA_1','PCA_2']],testdf['target']))[0:4], fontsize = 10)\n\n\nmu_vec1 = np.array([0,0])\ncov_mat1 = np.array([[2,0],[0,2]])\nx1_samples = np.random.multivariate_normal(mu_vec1, cov_mat1, 100)\nmu_vec1 = mu_vec1.reshape(1,2).T # to 1-col vector\n\nmu_vec2 = np.array([1,2])\ncov_mat2 = np.array([[1,0],[0,1]])\nx2_samples = np.random.multivariate_normal(mu_vec2, cov_mat2, 100)\nmu_vec2 = mu_vec2.reshape(1,2).T\nprint(clf2.n_support_ )\n\nplt.subplot(1,2,2)\nC = 300\nclf2 = SVC(kernel = 'linear',C =C)\nclf2.fit(X, y)\nprint('training accuracy...',clf2.score(X, y, sample_weight=None))\nprint('validation accuracy...',clf2.score(testdf[['PCA_1','PCA_2']],testdf['target']))\n\nw = clf2.coef_[0]\na = -w[0] / w[1]\nxx =  np.linspace(min(X.PCA_1), max(X.PCA_2), 100)\nyy = a * xx - (clf2.intercept_[0]) / w[1]\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\nplt.scatter(clf2.support_vectors_[:, 0], clf2.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10, color = 'g')\nplt.plot(xx, yy)\nplt.title('SVM.' + ' Reg =' + str(C) + 'Accuracy:' + str(clf2.score(testdf[['PCA_1','PCA_2']],testdf['target']))[0:4], fontsize = 10)\n\nmu_vec1 = np.array([0,0])\ncov_mat1 = np.array([[2,0],[0,2]])\nx1_samples = np.random.multivariate_normal(mu_vec1, cov_mat1, 100)\nmu_vec1 = mu_vec1.reshape(1,2).T # to 1-col vector\n\nmu_vec2 = np.array([1,2])\ncov_mat2 = np.array([[1,0],[0,1]])\nx2_samples = np.random.multivariate_normal(mu_vec2, cov_mat2, 100)\nmu_vec2 = mu_vec2.reshape(1,2).T\nprint(clf2.n_support_ )\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"82e26654-3f0e-df0d-a606-0dd623304544"},"source":"Such as in the case of Logistic Regression, the model sensitivity to regularization is very low. a large change in C results in just a small change in the number of support vectors and almost no change in accuracy"},{"cell_type":"markdown","metadata":{"_cell_guid":"f242891f-ebc0-b5b4-af87-7a11263d6501"},"source":"## Non-linear kernels\n\nSVM can use the \"kernel trick\" in order to produce a non-linear decision boundary. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a4adaa8-e196-053f-012b-cfea085766ed"},"outputs":[],"source":"clf3 = SVC(kernel = 'poly',degree = 3)\nclf3.fit(X, y)\nprint('Polynomial kernel - training accuracy...',clf3.score(X, y, sample_weight=None))\nprint('Polynomial kernel - validation accuracy...',clf3.score(testdf[['PCA_1','PCA_2']],testdf['target']))\nprint('Polynomial kernel - number of supporting points...',clf3.n_support_ )\n\nclf4 = SVC(kernel = 'rbf',gamma=0.1)\nclf4.fit(X, y)\nprint('Gaussian kernel - training accuracy...',clf4.score(X, y, sample_weight=None))\nprint('Gaussian kernel - validation accuracy...',clf4.score(testdf[['PCA_1','PCA_2']],testdf['target']))\nprint('Gaussian kernel - number of supporting points...',clf4.n_support_ )\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"bde65831-7a67-235e-60c6-127e90628ef6"},"source":"## Decision boundaries of different classifiers\n\nLet's see the decision boundaries produced by the linear, Gaussian and polynomial classifiers. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3d67d59-67fa-6e28-a935-c0ea763e87a1"},"outputs":[],"source":"plt.figure(figsize = (10,10))\nplt.subplot(3,1,1)\nx_min = X.PCA_1.min()\nx_max = X.PCA_1.max()\ny_min = X.PCA_2.min()\ny_max = X.PCA_2.max()\n\nXX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\nZ = clf2.decision_function(np.c_[XX.ravel(), YY.ravel()])\nZ = clf2.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\nZ = Z.reshape(XX.shape)\nplt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\nplt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\n\nplt.title('Linear Kernel')\nplt.legend(['Malignant', ' Benign'])\n\n\nplt.subplot(3,1,2)\nx_min = X.PCA_1.min()\nx_max = X.PCA_1.max()\ny_min = X.PCA_2.min()\ny_max = X.PCA_2.max()\n\nXX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\nZ = clf3.decision_function(np.c_[XX.ravel(), YY.ravel()])\nZ = clf3.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\nZ = Z.reshape(XX.shape)\nplt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\nplt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\n\nplt.ylabel('PCA_2')\nplt.title('Polynomial Kernel')\n\nplt.subplot(3,1,3)\n\nx_min = X.PCA_1.min()\nx_max = X.PCA_1.max()\ny_min = X.PCA_2.min()\ny_max = X.PCA_2.max()\n\nXX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\nZ = clf4.decision_function(np.c_[XX.ravel(), YY.ravel()])\nZ = clf4.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\nZ = Z.reshape(XX.shape)\nplt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\nplt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\n\nplt.xlabel('PCA_1')\nplt.title('Gaussian Kernel')\n\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"f1d95271-3d35-3df2-9cac-33c40fe037ee"},"source":"## SVM and class imbalance\n\nLet's quantify our class imbalance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"614c3f7e-8657-c9b7-a4ef-a6db98d17d7a"},"outputs":[],"source":"print('Malignant samples...',len(df[df.diagnosis == 'M']))\nprint('Benign samples...',len(df[df.diagnosis == 'B']))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0547dbd9-2b9d-d73f-8565-93eba0325d63"},"source":"So we have a mild class imbalance problem. However, since we deal with cancer, we have a class importance imbalance - false negative is worse than false positive. \n\nIn SVM, we can introduce a weight to take this into account.\n\nLet's first see how the non-weighted classifier performs in terms of precision and recall, in addition to accuracy. Let's only deal with the linear classier as it's simplest, and performs as well as the more complex kernels.\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c04456d-8dcf-ff8a-0849-9f70d9d5f930"},"outputs":[],"source":"from sklearn.metrics import precision_recall_fscore_support\n\ny_true = testdf['target']\ny_pred = clf2.predict(testdf[['PCA_1','PCA_2']])\n[precision,recall,fscore,support] = precision_recall_fscore_support(y_true, y_pred,pos_label=1)\nprint('Precision:', precision,'Recall:', recall, 'fscore:',fscore)\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b8f43c2-c9fb-075b-ec5b-e6b4b135afa4"},"source":"Now let's introduced a weighted classifier and put a larger weight on the malignant class:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68b4007d-4c03-84b2-7083-ebe679502758"},"outputs":[],"source":"wclf = SVC(kernel='linear', class_weight={1:10})\nwclf.fit(X,y)\n\ny_true = testdf['target']\ny_pred = wclf.predict(testdf[['PCA_1','PCA_2']])\n[precision,recall,fscore,support] = precision_recall_fscore_support(y_true, y_pred,pos_label=1)\nprint(' Weighted Precision:', precision,' Weighted Recall:', recall, ' Weighted fscore:',fscore)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6733433-b3eb-a1a9-5b9a-fa6dcbc65e53"},"source":"We see that we have higher recall values for the positive class. this means lower chance to miss a true positive. let's verify that the false negative is indeed 0:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7959cf14-14d2-2c24-08d5-c4d47d5db783"},"outputs":[],"source":"diff = y_pred - y_true\nprint('Number of false negative:',len(diff[diff == -1]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8ed4e6c-cf95-f40e-c328-f263f8f79c37"},"source":"Let's now plot the weighted and non-weighted decision boundaries:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"592354a9-5723-ce5b-be2b-f3f4fb08910d"},"outputs":[],"source":"plt.figure(figsize = (10,10))\nplt.subplot(2,1,1)\nx_min = X.PCA_1.min()\nx_max = X.PCA_1.max()\ny_min = X.PCA_2.min()\ny_max = X.PCA_2.max()\n\nXX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\nZ = clf2.decision_function(np.c_[XX.ravel(), YY.ravel()])\nZ = clf2.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\nZ = Z.reshape(XX.shape)\nplt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\nplt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\n\nplt.title('Linear Kernel')\nplt.legend(['Malignant', ' Benign'])\n\n\nplt.subplot(2,1,2)\nx_min = X.PCA_1.min()\nx_max = X.PCA_1.max()\ny_min = X.PCA_2.min()\ny_max = X.PCA_2.max()\n\nXX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\nZ = wclf.decision_function(np.c_[XX.ravel(), YY.ravel()])\nZ = wclf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\nZ = Z.reshape(XX.shape)\nplt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\nplt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 1],PCA_df.PCA_2[PCA_df.target == 1], alpha = 0.8, color = 'r')\nplt.scatter(PCA_df.PCA_1[PCA_df.target == 0],PCA_df.PCA_2[PCA_df.target == 0], alpha = 0.8, color = 'b')\n\nplt.title('Weighted Linear Kernel')\nplt.legend(['Malignant', ' Benign'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"d63954f2-8b50-8839-8eab-c256ce3a4cb5"},"source":"We can see how the decision boundary moved to the right to make sure to include all malignant cases, with the price of more false positives."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}