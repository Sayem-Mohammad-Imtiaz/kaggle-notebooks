{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spam Detection"},{"metadata":{},"cell_type":"markdown","source":"## Use case\nYou were recently hired in start up company and you were asked to build a system to identify spam emails.\n"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import libs\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom collections import Counter\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/lingspam-dataset/messages.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting all messages to lower case\n\ndf['message'] = df['message'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data once \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleansing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checing null values \ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can observe that data is missing here ."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(df['subject'].mode().values[0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's once again \ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's looking perfect and move on to next step's ."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering "},{"metadata":{},"cell_type":"markdown","source":"To get clarity about mail i'm going to merge both subject and message ."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sub_mssg']=df['subject']+df['message']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sub_mssg'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['length']=df['sub_mssg'].apply(len)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now i'm going to drop un-necessary features \ndf.drop('subject',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check it once \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"lb=df['label'].value_counts().index.tolist()\nval=df['label'].value_counts().values.tolist()\nexp=(0.025,0)\nclr=('orange','blue')\nplt.figure(figsize=(10,5),dpi=140)\nplt.pie(x=val,explode=exp,labels=lb,colors=clr,autopct='%2.0f%%',pctdistance=0.5, shadow=True,radius=0.9)\nplt.legend([\"0 = NO SPAM\",'1 = SPAM'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing Email Messages :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['message'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontact(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mssg=decontact(df['message'][70])\nmssg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#REPLACING NUMBERS\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'\\d+(\\.\\d+)?', 'numbers')\ndf['sub_mssg'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CONVRTING EVERYTHING TO LOWERCASE\ndf['sub_mssg']=df['sub_mssg'].str.lower()\n#REPLACING NEXT LINES BY 'WHITE SPACE'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'\\n',\" \") \n# REPLACING EMAIL IDs BY 'MAILID'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','MailID')\n# REPLACING URLs  BY 'Links'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','Links')\n# REPLACING CURRENCY SIGNS BY 'MONEY'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'£|\\$', 'Money')\n# REPLACING LARGE WHITE SPACE BY SINGLE WHITE SPACE\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'\\s+', ' ')\n\n# REPLACING LEADING AND TRAILING WHITE SPACE BY SINGLE WHITE SPACE\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^\\s+|\\s+?$', '')\n#REPLACING CONTACT NUMBERS\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','contact number')\n#REPLACING SPECIAL CHARACTERS  BY WHITE SPACE \ndf['sub_mssg']=df['sub_mssg'].str.replace(r\"[^a-zA-Z0-9]+\", \" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CONVRTING EVERYTHING TO LOWERCASE\ndf['message']=df['message'].str.lower()\n#REPLACING NEXT LINES BY 'WHITE SPACE'\ndf['message']=df['message'].str.replace(r'\\n',\" \") \n# REPLACING EMAIL IDs BY 'MAILID'\ndf['message']=df['message'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','MailID')\n# REPLACING URLs  BY 'Links'\ndf['message']=df['message'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','Links')\n# REPLACING CURRENCY SIGNS BY 'MONEY'\ndf['message']=df['message'].str.replace(r'£|\\$', 'Money')\n# REPLACING LARGE WHITE SPACE BY SINGLE WHITE SPACE\ndf['message']=df['message'].str.replace(r'\\s+', ' ')\n\n# REPLACING LEADING AND TRAILING WHITE SPACE BY SINGLE WHITE SPACE\ndf['message']=df['message'].str.replace(r'^\\s+|\\s+?$', '')\n#REPLACING CONTACT NUMBERS\ndf['message']=df['message'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','contact number')\n#REPLACING SPECIAL CHARACTERS  BY WHITE SPACE \ndf['message']=df['message'].str.replace(r\"[^a-zA-Z0-9]+\", \" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sub_mssg'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now message looking perfect ."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n# removing stopwords \nstop = stopwords.words('english')\ndf['Cleaned_Text'] = df['sub_mssg'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('message',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('sub_mssg',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['lgth_clean']=df['Cleaned_Text'].apply(len)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_length=sum(df['length'])\nafter_cleaning=sum(df['lgth_clean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"original_length\",original_length)\nprint('after_cleaning',after_cleaning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Convert text into vectors using TF-IDF\n# 2. Instantiate MultinomialNB classifier\n# 3. Split feature and label\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"tvec = TfidfVectorizer()\nlr = LogisticRegression(solver = \"lbfgs\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.Cleaned_Text\nY = df.label\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 225,stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([('vectorizer',tvec),('classifier',lr)])\n\nmodel.fit(X_train,Y_train)\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = model.predict(X_test)\n\nconfusion_matrix(y_pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNeighbors Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"knc = KNeighborsClassifier()\nmodel_1 = Pipeline([('vectorizer',tvec),('classifier',knc)])\nmodel_1.fit(X_train,Y_train)\n\n\ny_pred = model_1.predict(X_test)\n\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ada Boost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = AdaBoostClassifier()\nmodel_3 = Pipeline([('vectorizer',tvec),('classifier',abc)])\nmodel_3.fit(X_train,Y_train)\n\n\ny_pred = model_3.predict(X_test)\n\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB()\nmodel_5 = Pipeline([('vectorizer',tvec),('classifier',mnb)])\nmodel_5.fit(X_train,Y_train)\n\n\ny_pred = model_5.predict(X_test)\n\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc = GradientBoostingClassifier()\nmodel_6= Pipeline([('vectorizer',tvec),('classifier',gbc)])\nmodel_6.fit(X_train,Y_train)\n\n\ny_pred = model_6.predict(X_test)\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFC\nrfc = RFC(random_state=42)\nmodel_7 = Pipeline([('vectorizer',tvec),('classifier',rfc)])\n\nmodel_7.fit(X_train,Y_train)\n\ny_pred = model_7.predict(X_test)\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can observe that Random forest classifier working well compared to all other algorithms"},{"metadata":{},"cell_type":"markdown","source":"## Testing Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"result=model_7.predict(['your microsoft account has been compromised ,you must update before or else your account going to close click to update'])\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=model_7.predict(['Today we want to inform you that the application period for 15.000 free Udacity Scholarships in Data Science is now open! Please apply by November 16th, 2020 via https://www.udacity.com/bertelsmann-tech-scholarships.'])\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here 0 is spam and 1 is normal message."},{"metadata":{},"cell_type":"markdown","source":"#  EOF DONE CLASSIFICATION OF SPAM CLASSIFICATION "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}