{"cells":[{"metadata":{},"cell_type":"markdown","source":"**PLEASE UPVOTE IF FOUND INTERESTING**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nfrom collections import Counter\nimport numpy as np \nimport string\nimport re\nfrom datetime import datetime\nfrom matplotlib import pyplot\nimport spacy\nimport seaborn as sns\n\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import classification_report\nstop_words = stopwords.words('english')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Reading the data\nAnswers = pd.read_csv(\"../input/Answers.csv\" , encoding='latin-1')\nQuestions = pd.read_csv(\"../input/Questions.csv\", encoding='latin-1')\nTags = pd.read_csv(\"../input/Tags.csv\", encoding='latin-1')\n\n#Cleaning the data\ndef clean_text(text):\n    global EMPTY\n    EMPTY = ''\n    \n    if not isinstance(text, str): \n        return text\n    text = re.sub('<pre><code>.*?</code></pre>', EMPTY, text)\n\n    def replace_link(match):\n        return EMPTY if re.match('[a-z]+://', match.group(1)) else match.group(1)\n    \n    text = re.sub('<a[^>]+>(.*)</a>', replace_link, text)\n    return re.sub('<[^>]+>', EMPTY, text)\nQuestions['Text'] = Questions['Body'].apply(clean_text).str.lower()\nQuestions.Text = Questions.Text.apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\r\",\"\"))\ntotal = pd.merge(Questions,Tags,on='Id')\ntotal['date'] = pd.to_datetime(total.CreationDate)\ntotal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2c3a0bd3fc94c54a51633b88859746516087609"},"cell_type":"markdown","source":"#Most Popular Tags"},{"metadata":{"trusted":true,"_uuid":"b15f5d07ea5840b04d2f7d0b2f063cdbefca43a8","scrolled":true},"cell_type":"code","source":"tagCount =  Counter(list(total['Tag'])).most_common(10)\nprint(tagCount)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d62adf9dd884e829f0176d13778fe7395c3bf9f3"},"cell_type":"markdown","source":"**Trend of Questions per Year**"},{"metadata":{"trusted":true,"_uuid":"b14b531f627b8fb86c3669c2c31fa01a19d0dbf2","scrolled":true},"cell_type":"code","source":"total['year'] = total['date'].apply(lambda x: x.year)\nseries = total.year.value_counts().sort_index()\n# print(series)\nseries.plot(figsize=(5,5), grid=True)\npyplot.xlabel(\"YEARS\")\npyplot.ylabel(\"NUMBER OF QUESTIONS ASKED\")\npyplot.title(\"GRAPH SHOWING NUMBER OF QUESTIONS ASKED PER YEAR\")\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce5ef8891f896bd1c3cdd131e53cb96a6a3ae048"},"cell_type":"markdown","source":"* From the above line graph we can deduce that number of questions being asked is increasing every year apart from 2016 year(may be due to incorrect or partially collected data) "},{"metadata":{"_uuid":"fd77222354707ce361b3daf348664c6e18a09a8c"},"cell_type":"markdown","source":"**Which Month had highest Questions?**"},{"metadata":{"trusted":true,"_uuid":"25ed4e6be9ea7e207571530317eb6fc7cfb1a9a1"},"cell_type":"code","source":"look_up = [\"January\",\n          \"Febuary\",\n          \"March\",\n          \"April\",\n          \"May\",\n          \"June\",\n          \"July\",\n          \"August\",\n          \"September\",\n          \"October\",\n          \"November\",\n          \"December\"]\nmonthname = pd.Series( (v for v in look_up) )\nmonthname.index += 1\ntotal['Month'] = total['date'].apply(lambda x: x.month)\nseries = total.Month.value_counts().sort_index()\nmonth_wise_data = pd.concat([monthname, series], axis=1)\nmonth_wise_data.rename(columns={0:'Months'}, inplace=True)\nmonth_wise_data.rename(columns={'Month':'No_of_Questions'}, inplace=True)\n# print(month_wise_data)\nax = month_wise_data.plot(figsize=(20,7),grid=True)\nax.set_xticks(month_wise_data.index)\nax.set_xticklabels(month_wise_data.Months)\npyplot.title(\"GRAPH SHOWING NUMBER OF QUESTIONS ASKED PER MONTH\")\npyplot.xlabel(\"Months\")\npyplot.ylabel(\"NUMBER OF QUESTIONS ASKED\")\npyplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bc299e3da66ecb22faa06d7f3e433c1f1ebe7d4"},"cell_type":"markdown","source":"* Above graph shows that most of the questions were posted in mid year (June to July) and questions posted were least during December."},{"metadata":{"trusted":true,"_uuid":"08bf042d4234dd900955866babe96b67139fe6cd"},"cell_type":"markdown","source":"** Most Common Words in Question Titles**"},{"metadata":{"trusted":true,"_uuid":"b8c120cc7fbcd67086659a97d3b30c32cb23ac5d"},"cell_type":"code","source":"spacy_nlp = spacy.load('en_core_web_sm')\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nspacy_stopwords.add(\"/\")\nspacy_stopwords.add(\"=\")\nspacy_stopwords.add(\">\")\nspacy_stopwords.add(\"<\")\nspacy_stopwords.add(\"#\")\n\ncustomize_stop_words = [\n    '/', '=','>','<','#','`','|','}','{','Â¦',';'\n    \n]\nfor w in customize_stop_words:\n    spacy_nlp.vocab[w].is_stop = True\n\nprint('Number of stop words: %d' % len(spacy_stopwords))\n# print('First ten stop words: %s' % list(spacy_stopwords))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d46191d3d63eafe450dab38b5d15215c0e5fdda"},"cell_type":"code","source":"titleWords = []\ni = 0\nfor word in Questions.Title:\n    word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\":\",\"\").replace(\"\\\"\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\"_\",\"\").replace(\"*\",\"\").replace(\"-\",\"\").replace(\"'\",\"\").replace(\" \",\"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\")\",\"\").replace(\"(\",\"\")\n    doc = spacy_nlp(word.lower())\n    tokens = [token.text for token in doc if not token.is_stop]\n    titleWords= titleWords + tokens\n    i += 1\n    if (i == 10000):\n        break\n# print(titleWords)\nword_counter = Counter(titleWords)\n# for word, count in word_counter.most_common(10):\n#     print(word, \": \", count)\n\nlst = word_counter.most_common(20)\n# print(lst)\ndf = pd.DataFrame(lst, columns = ['Word', 'Count'])\nax = sns.barplot(x=\"Count\", y=\"Word\", data=df)\n# df.plot.bar(x='Word',y='Count')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd0290e62373b84542c3387cc6ed55f77ff2ae43"},"cell_type":"code","source":"total.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48054fb61ff215254eb9e3d18e255909a399e7ab"},"cell_type":"code","source":"total.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"922788247ef60d1804f3f2f00150179f6154cf22"},"cell_type":"code","source":"total['wordcount'] = total.Text.apply(lambda x: len(x.split()))\ntotal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62442cf4aad2f0524d375e976df335341c6d2fb2"},"cell_type":"code","source":"total = total[total.wordcount < 75]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6afcde96622b8f050630b8a781235fbdd321d01"},"cell_type":"code","source":"total.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"29a63b9b5c14e542cd26962bd2a0381bed396b7d"},"cell_type":"code","source":"col = ['Id', 'OwnerUserId', 'CreationDate', 'date', 'Score', 'Title', 'Body', 'Month', 'year','wordcount']\ntotal = total[(total.Tag == 'python') | (total.Tag == 'django') | (total.Tag == 'python-2.7') | (total.Tag =='pandas') | (total.Tag =='python-3.x') | (total.Tag == 'numpy') | (total.Tag == 'list') | (total.Tag == 'matplotlib') | (total.Tag == 'regex') | (total.Tag == 'dictionary')]\ntotal.drop(col, axis=1, inplace=True)\ntotal['Tag'].value_counts()\ncategories = total['Tag'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c48739b7270926e8d36ba06cb47233a99aa0e9a0"},"cell_type":"code","source":"fig = pyplot.figure(figsize=(10,6))\ntotal.groupby('Tag').Text.count().plot.bar(ylim=0)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b86188f94ad12dcef969cfc230aba1626d3a8d9b"},"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(total.Tag.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb0e50aaa53c726b2c39463ee9d03ff94a71b44a"},"cell_type":"code","source":"xtrain, xtest , ytrain,ytest = train_test_split(total.Text, total.Tag, \n                                                  stratify=total.Tag, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47249dc35b3d6278720a6f9300e1f7652868e173"},"cell_type":"code","source":"print(xtrain.shape)\nprint(xtest.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab0092a9f58f22ea8aa6381bd9d54a669ce83191"},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xtest))\nxtrain_tfv =  tfv.transform(xtrain) \nxtest_tfv = tfv.transform(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bb6afd708f77be8a9801db815a10c456a0ed7be"},"cell_type":"code","source":"clf_Multinomial = MultinomialNB(alpha=0.01)\nclf_Multinomial.fit(xtrain_tfv, ytrain)\npredictions_Multinomial = clf_Multinomial.predict(xtest_tfv)\nscore_Multinomial = metrics.accuracy_score(ytest, predictions_Multinomial)\nprint(\"accuracy:   %0.3f\" % score_Multinomial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"466bd2eb30e0404622348c195ddd373a6c84b4d2"},"cell_type":"code","source":"clf_LogisticRegression = LogisticRegression(C=1.0)\nclf_LogisticRegression.fit(xtrain_tfv, ytrain)\npredictions_LogisticRegression = clf_LogisticRegression.predict(xtest_tfv)\nscore_LogisticRegression = metrics.accuracy_score(ytest, predictions_LogisticRegression)\nprint(\"accuracy:   %0.3f\" % score_LogisticRegression)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b89dc0788239365afc0538d3b3f6167618aff778"},"cell_type":"code","source":"clf_RandomForestClassifier = RandomForestClassifier()\nclf_RandomForestClassifier.fit(xtrain_tfv, ytrain)\npredictions_RandomForestClassifier = clf_RandomForestClassifier.predict(xtest_tfv)\nscore_RandomForestClassifier = metrics.accuracy_score(ytest, predictions_RandomForestClassifier)\nprint(\"accuracy:   %0.3f\" % score_RandomForestClassifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9749a1e104958cbe73631552638f3745b954b2b6"},"cell_type":"code","source":"print(classification_report(ytest, predictions, target_names=categories))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}