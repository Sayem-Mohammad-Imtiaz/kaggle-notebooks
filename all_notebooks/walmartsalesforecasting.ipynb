{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Issues faced:-\n* \"Store\" and \"Dept\" are both categorical variables but the are too many categories \n* Dates of week are unique. There are 143 total unique dates which have to be replaced with week number.\n* Don't know which feature is important inorder to split the data using Stratified Split.\n* Averaging the MarkDowns from Nov 2011 to Oct 2012 and filling them for repective weeks from Feb 2010 to Sep 2010.\n* Binary Encoding \"Dept\" and \"Store\" due to presence of large number of classes in these variables.\n* One Hot Encoding the \"Type\" variable as it had only 3 classes.\n* Changed the Boolean entries to 1 and 0 in \"IsHoliday\" variable.\n* Scaled the numerical values using Standard Scaler.\n* Implemented a pipeline to transform the data for above numberical and categorical transformations.\n* Worst performance with linear regression.\n* Hyperparameter tunning Random Forest Regressor, Extra Trees Regressor and XGBoost Regressor.\n* Individually, the increasing order of better performace is Random Forest Regressor, XGBoost Regressor, Extra Trees Regressor.\n* Tried to create a Voting Regressor of Random Forest Regressor, Extra Trees Regressor and XGBoost but the kernal is running of memory( Exceeding 16GB. The data set size if just 50 MB).\n* Hence, tried out combinations of (Random Forest,Extra Trees) and (Extra Trees, XGBoost). Here, the later one performed better.\n* Got the best results with Stacking well tunned XGBoost Regressor, Random Forest Regressor and Extra Trees Regressor as base estimators and Linear Regressor as final estimator.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Some Info\n* CPI  = Price of basket of goods in Given Year/Price of basket of goods in base year\n* CPI is an indicator of inflation. Hence if CPI is more then people can purchase less goods in same amount of money. (https://www.investopedia.com/terms/c/consumerpriceindex.asp)\n* Unemployment Rate = Percentage of unemployed people divided by the total number of people in the labour force(employed + unemployed) (https://www.investopedia.com/terms/u/unemploymentrate.asp)","metadata":{}},{"cell_type":"markdown","source":"# Importing all the libraries","metadata":{}},{"cell_type":"code","source":"# Importing data processing libraries\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport category_encoders as ce\nfrom category_encoders.binary import BinaryEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import FeatureUnion\nimport csv\n\n# Importing data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model libraries \nfrom sklearn.ensemble import VotingRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\nimport xgboost\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model evaluation \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n# Hyperparameter tunning libraries\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nimport random","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:08.841081Z","iopub.execute_input":"2021-06-23T06:15:08.841533Z","iopub.status.idle":"2021-06-23T06:15:10.33647Z","shell.execute_reply.started":"2021-06-23T06:15:08.841443Z","shell.execute_reply":"2021-06-23T06:15:10.335631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data","metadata":{}},{"cell_type":"code","source":"# Function to import dataset\n\ndef importData(feature = None):\n    data = pd.read_csv(\"../input/course-material-walmart-challenge/train.csv\")\n    train = test = 0\n    if feature is None:\n        train, test = train_test_split(data, test_size = 0.2, random_state = 21)\n    else:\n        split = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, random_state = 21)\n        for train_index, test_index in split.split(data, data[feature]):\n            train = data.loc[train_index]\n            test = data.loc[test_index]\n    return [train,test]","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:10.337618Z","iopub.execute_input":"2021-06-23T06:15:10.338106Z","iopub.status.idle":"2021-06-23T06:15:10.344257Z","shell.execute_reply.started":"2021-06-23T06:15:10.338074Z","shell.execute_reply":"2021-06-23T06:15:10.342865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simply splitting the data\ntrain, test = importData(\"Date\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:10.34648Z","iopub.execute_input":"2021-06-23T06:15:10.347067Z","iopub.status.idle":"2021-06-23T06:15:11.93231Z","shell.execute_reply.started":"2021-06-23T06:15:10.347021Z","shell.execute_reply":"2021-06-23T06:15:11.931375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Taking a quick look into the data","metadata":{}},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:11.933595Z","iopub.execute_input":"2021-06-23T06:15:11.9339Z","iopub.status.idle":"2021-06-23T06:15:11.972814Z","shell.execute_reply.started":"2021-06-23T06:15:11.933871Z","shell.execute_reply":"2021-06-23T06:15:11.971733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:11.974129Z","iopub.execute_input":"2021-06-23T06:15:11.974453Z","iopub.status.idle":"2021-06-23T06:15:12.040415Z","shell.execute_reply.started":"2021-06-23T06:15:11.974421Z","shell.execute_reply":"2021-06-23T06:15:12.039338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['Store'].unique())\nprint('Total number of stores = {}'.format(len(train['Store'].unique())))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:12.041995Z","iopub.execute_input":"2021-06-23T06:15:12.042437Z","iopub.status.idle":"2021-06-23T06:15:12.054727Z","shell.execute_reply.started":"2021-06-23T06:15:12.042388Z","shell.execute_reply":"2021-06-23T06:15:12.053668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['Dept'].unique())\nprint('Total number of departments = {}'.format(len(train['Dept'].unique())))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:12.056119Z","iopub.execute_input":"2021-06-23T06:15:12.056423Z","iopub.status.idle":"2021-06-23T06:15:12.064916Z","shell.execute_reply.started":"2021-06-23T06:15:12.056387Z","shell.execute_reply":"2021-06-23T06:15:12.064022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Dates of the weeks on which data was recorded :- ')\nprint(train['Date'].unique())\nprint('Total number of weeks = {}'.format(len(train['Date'].unique())))\nyear12 = []\nyear11 = []\nyear10 = []\ndate_list = train['Date'].unique()\nfor i in range(len(train['Date'].unique())):\n    l = list(map(int, date_list[i].split('-')))\n    if l[0] == 2012:\n        year12.append(l[0])\n    elif l[0] == 2011:\n        year11.append(l[0])\n    else:\n        year10.append(l[0])\nprint('Number of Entries in Year 2010 = {}'.format(len(year10)))\nprint('Number of Entries in Year 2011 = {}'.format(len(year11)))\nprint('Number of Entries in Year 2012 = {}'.format(len(year12)))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:12.067826Z","iopub.execute_input":"2021-06-23T06:15:12.068122Z","iopub.status.idle":"2021-06-23T06:15:12.147667Z","shell.execute_reply.started":"2021-06-23T06:15:12.068094Z","shell.execute_reply":"2021-06-23T06:15:12.146728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The entries are from February 2010 to October 2012, consisting of 143 entries. We can replace the dates by week number which will help us in visualization.**","metadata":{}},{"cell_type":"markdown","source":"# Replacing the \"Date\" variable with \"Week\" variable. ","metadata":{}},{"cell_type":"code","source":"class replaceDateWithWeek(BaseEstimator, TransformerMixin):\n    def __init__(self,arg=None):\n        self.arg = arg\n    def fit(self,X,y=None):\n        return self\n    def transform(self,data,y=None):\n        #Finding the dates and converting them to int\n        dates_list = []\n        dates = data['Date'].unique()\n        print(\"Number of weeks in the dataset = {}\".format(len(dates)))\n        for i in range(len(dates)):\n            l = list(map(int, dates[i].split('-')))\n            dates_list.append(l)\n\n        #Sorting the dates\n        weeks = []\n        for i in range(2010,2013):\n            for j in range(1,13):\n                for k in range(1,32):\n                    for date in dates_list:\n                        if date[0] == i and date[1] == j and date[2] == k:\n                            weeks.append(date)\n\n        #Reconverting the dates back to string   \n        for i in range(len(weeks)):\n            if weeks[i][1] >= 10 and weeks[i][2] >= 10:\n                weeks[i] = str(weeks[i][0])+\"-\"+str(weeks[i][1])+\"-\"+str(weeks[i][2])\n            elif weeks[i][1] >= 10 and weeks[i][2] < 10:\n                weeks[i] = str(weeks[i][0])+\"-\"+str(weeks[i][1])+\"-0\"+str(weeks[i][2])\n            elif weeks[i][1] < 10 and weeks[i][2] >= 10:\n                weeks[i] = str(weeks[i][0])+\"-0\"+str(weeks[i][1])+\"-\"+str(weeks[i][2])\n            elif weeks[i][1] < 10 and weeks[i][2] < 10:\n                weeks[i] = str(weeks[i][0])+\"-0\"+str(weeks[i][1])+\"-0\"+str(weeks[i][2])\n\n        #Replacing dates with week number\n        week_num = []\n        l = data['Date'].tolist()\n        for i in range(len(l)):\n            week_num.append(weeks.index(l[i]) + 1)\n\n        data['Week'] = week_num\n        data.drop(['Date'], axis = 1, inplace = True)\n        return data\n    \ntemp = replaceDateWithWeek()\ntemp.transform(train)\ntrain.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:12.149544Z","iopub.execute_input":"2021-06-23T06:15:12.149861Z","iopub.status.idle":"2021-06-23T06:15:12.812145Z","shell.execute_reply.started":"2021-06-23T06:15:12.149822Z","shell.execute_reply":"2021-06-23T06:15:12.811115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The above transformation can also be done using OrdinalEncoder**","metadata":{}},{"cell_type":"markdown","source":"# Performing EDA","metadata":{}},{"cell_type":"markdown","source":"# Checking the store wise sale in 3 years","metadata":{}},{"cell_type":"code","source":"sales_sum = []\nstore_index = [i for i in range(1,46)]\n\nfor i in range(1,46):\n    sales_sum.append(sum(train[train['Store'] == i]['Weekly_Sales'])/1000000)\n\nplt.figure(figsize = (15,10))\nplt.title('Store wise sales in 3 years')\nsns.barplot(x = store_index, y = sales_sum)\nsns.set(style = 'darkgrid')\nplt.xlabel(\"Stores\")\nplt.ylabel(\"Sales (In Million $)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:12.813655Z","iopub.execute_input":"2021-06-23T06:15:12.814005Z","iopub.status.idle":"2021-06-23T06:15:13.744891Z","shell.execute_reply.started":"2021-06-23T06:15:12.813971Z","shell.execute_reply":"2021-06-23T06:15:13.743929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Few stores have very high sales while few have comparatively very low sales***","metadata":{}},{"cell_type":"markdown","source":"# Testing difference in sales on Holiday Weeks","metadata":{}},{"cell_type":"code","source":"y = []\nx = ['Holiday', 'Normal']\n\ny.append(sum(train[train['IsHoliday'] == True]['Weekly_Sales'])/len(train[train['IsHoliday'] == True]['Weekly_Sales']))\ny.append(sum(train[train['IsHoliday'] == False]['Weekly_Sales'])/len(train[train['IsHoliday'] == False]['Weekly_Sales']))\n\nplt.figure(figsize = (10,8))\nplt.title('Difference in average sales on Holiday Weeks')\nsns.barplot(x = x, y = y)\nsns.set(style = 'darkgrid')\nplt.xlabel(\"Type of week\")\nplt.ylabel(\"Average Sales (In $)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:13.746055Z","iopub.execute_input":"2021-06-23T06:15:13.746336Z","iopub.status.idle":"2021-06-23T06:15:13.986509Z","shell.execute_reply.started":"2021-06-23T06:15:13.746309Z","shell.execute_reply":"2021-06-23T06:15:13.985682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Not much difference is seen in Holiday and Normal week Average Weekly Sales***","metadata":{}},{"cell_type":"markdown","source":"# Time Series subplot of Weekly Sales, CPI and unemployment rate","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 1, figsize = (20,15), sharex = True, sharey = False)\nfig.suptitle(\"Time Series subplot of Weekly Sales, CPI and unemployment rate\")\nsns.lineplot(ax=axes[0],x = 'Week', y = 'Weekly_Sales', data=train)\naxes[0].set_title(\"Average week wise sales(in $) in 3 years\")\naxes[0].set_ylabel(\"Average Weekly Sales\")\nsns.lineplot(ax=axes[1],x = 'Week', y = 'CPI', data=train)\naxes[1].set_title(\"Average week wise CPI in 3 years\")\naxes[1].set_ylabel(\"Average CPI\")\nsns.lineplot(ax=axes[2],x = 'Week', y = 'Unemployment', data=train)\naxes[2].set_title(\"Average week wise unemployment rate in 3 years\")\naxes[2].set_xlabel(\"Week\")\naxes[2].set_ylabel(\"Average Unemployment\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:13.987571Z","iopub.execute_input":"2021-06-23T06:15:13.987885Z","iopub.status.idle":"2021-06-23T06:15:35.115143Z","shell.execute_reply.started":"2021-06-23T06:15:13.987856Z","shell.execute_reply":"2021-06-23T06:15:35.114132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The following points are observed:**\n* The weekly sales have taken a jump around 48th and 100th week.\n* The weekly sales have declined just after the jump.\n* The CPI is increasin overall.\n* The unemployment rate is decreasing overall.\n* No clear correlation is seen among the three variables.( Weekly Sales, CPI, Unemployment Rate)","metadata":{}},{"cell_type":"markdown","source":"# Checking the temperature wise weekly sales","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 1, figsize = (20,20), sharex = False, sharey = False)\nfig.suptitle(\"Time Series subplot of Weekly Sales and Temperature\")\nsns.lineplot(ax=axes[0],x = 'Temperature', y = 'Weekly_Sales', data=train)\naxes[0].set_title(\"Average week wise sales(in $) based on temperature\")\naxes[0].set_xlabel(\"Temperature (In Fahrenheit)\")\naxes[0].set_ylabel(\"Average weekly sales\")\nsns.lineplot(ax=axes[1],x = 'Week', y = 'Temperature', data=train)\naxes[1].set_title(\"Variation of average Temperature with Week\")\naxes[1].set_xlabel(\"Week\")\naxes[1].set_ylabel(\"Average Temperature(In Fahrenheit)\")\nsns.lineplot(ax=axes[2],x = 'Week', y = 'Weekly_Sales', data=train)\naxes[2].set_title(\"Average week wise sales(in $) in 3 years\")\naxes[2].set_xlabel(\"Week\")\naxes[2].set_ylabel(\"Average Weekly Sales\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:15:35.116355Z","iopub.execute_input":"2021-06-23T06:15:35.116631Z","iopub.status.idle":"2021-06-23T06:17:44.168934Z","shell.execute_reply.started":"2021-06-23T06:15:35.116605Z","shell.execute_reply":"2021-06-23T06:17:44.167962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***The steep rise in the graph of Average Weekly Sales vs Week may be during Christmas (i.e in December) as the temperature is seen to be very low at that time***","metadata":{}},{"cell_type":"markdown","source":"# Filling up Markdowns for 2010 and 2011 based on data after Nov 2011\n* The entries for MarkDowns have started from November 11, 2011 and upto October 26, 2012.\n* Need to fill the missing entries from February 5, 2010 upto October 28, 2011 ","metadata":{}},{"cell_type":"code","source":"class fillMarkDowns(BaseEstimator, TransformerMixin):\n    def __init__(self,arg = None):\n        self.arg = arg\n    def fit(self,X,y = None):\n        return self\n    def transform(self,data, y = None):\n        #Getting week numbers from Nov 2011 to Oct 2012\n        weeksFebToOct12 = [x for x in range(105, 144)]\n        weeksNovToJan12 = [x for x in range(92,105)]\n\n        #Getting Markdowns from Nov 2011 to Oct 2012\n        MarkDowns = []\n        for i in range(1,6):\n            markdowns = []\n            for _ in range(2):\n                weeks = []\n                if _ == 0:\n                    weeks = weeksFebToOct12\n                else:\n                    weeks = weeksNovToJan12\n                for week in weeks:\n                    k = data[data['Week'] == week]['MarkDown'+str(i)]\n                    k = [0 if math.isnan(x) else x for x in k]\n                    markdowns.append(sum(k)/len(k))\n            MarkDowns.append(markdowns)\n\n        #Filling missing values in MarkDowns from week 1 to week 91\n        for k in range(5):\n            i = 1\n            j = 0\n            while i <= 91:\n                data.loc[data['Week'] == i,'MarkDown'+str(k+1)] = MarkDowns[k][j]\n                i += 1\n                j += 1\n                if j == 52:\n                    j = 0\n\n        #Filling missing values in MarkDowns from week 92 to week 143\n        id = [x for x in range(len(data))]\n        data['id'] = id\n        for i in range(len(data)):\n            for j in range(5):\n                if math.isnan(data.iloc[i]['MarkDown'+str(j+1)]):\n                    data.loc[data['id'] == i,'MarkDown'+str(j+1)] = MarkDowns[j][data.iloc[i]['Week']-92]\n\n        data.drop('id', axis = 1, inplace = True)\n        return data\n\ntemp = fillMarkDowns()\ntemp.transform(train)\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:17:44.170345Z","iopub.execute_input":"2021-06-23T06:17:44.170652Z","iopub.status.idle":"2021-06-23T06:22:04.462736Z","shell.execute_reply.started":"2021-06-23T06:17:44.170623Z","shell.execute_reply":"2021-06-23T06:22:04.461682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking correlation","metadata":{}},{"cell_type":"code","source":"corr_matrix = train.corr()\nplt.figure(figsize = (15,10))\nsns.heatmap(corr_matrix, vmin = -1, vmax = 1, cmap = 'seismic')\nplt.gca().patch.set(hatch = \"X\", edgecolor = \"#0080ff\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:04.464372Z","iopub.execute_input":"2021-06-23T06:22:04.464818Z","iopub.status.idle":"2021-06-23T06:22:05.132438Z","shell.execute_reply.started":"2021-06-23T06:22:04.464752Z","shell.execute_reply":"2021-06-23T06:22:05.131312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As seen from graph CPI and unemployment rate are inversely correlated\n* Markdown 2 and Temperature are inversely correlated\n* There is a slight correlation between Store and CPI, Unemployment rate and Size.\n* Week and fuel price are strongly correlated. It tells us that the fuel price are consistently rising.\n* MarkDown4 and MarkDown 1 are also stronly correlated.","metadata":{}},{"cell_type":"markdown","source":"# Replacing the boolean values in IsHoliday variable ","metadata":{}},{"cell_type":"code","source":"class replaceBoolean(BaseEstimator, TransformerMixin):\n    def __init__(self,arg=None):\n        self.arg = arg\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        X.loc[X['IsHoliday'] == True, 'IsHoliday'] = 1\n        X.loc[X['IsHoliday'] == False, 'IsHoliday'] = 0\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.135613Z","iopub.execute_input":"2021-06-23T06:22:05.135949Z","iopub.status.idle":"2021-06-23T06:22:05.143852Z","shell.execute_reply.started":"2021-06-23T06:22:05.135916Z","shell.execute_reply":"2021-06-23T06:22:05.143004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Catergorical Features\n\n* \"Date\" being an ordinal categorical variable. We can keep it as \"Week\" variable.\n* As \"Store\" and \"Dept\" have many categories we will use Binary Encoding for them.\n* \"Type\" variable has only 3 classes, hence we will use One Hot Encoding for it.\n* \"IsHoliday\" is a boolen variable hence we will encode it using 1 and 0.","metadata":{}},{"cell_type":"code","source":"class FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Select only specified columns.\"\"\"\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X[self.columns]","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.145061Z","iopub.execute_input":"2021-06-23T06:22:05.145617Z","iopub.status.idle":"2021-06-23T06:22:05.162079Z","shell.execute_reply.started":"2021-06-23T06:22:05.145574Z","shell.execute_reply":"2021-06-23T06:22:05.161218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating Pipeline\ndef runPipeline(data):\n    #First transformation\n    temp = replaceDateWithWeek()\n    data = temp.transform(data)\n    \n    #Second transformation\n    temp = fillMarkDowns()\n    data = temp.transform(data)\n    \n    completePipeline = ColumnTransformer([\n        ('binary_encoder', BinaryEncoder(cols = ['Store','Dept'], return_df=True),['Store','Dept']),\n        ('one_hot_encoder',ce.OneHotEncoder(),['Type']),\n        ('scalar', StandardScaler(),['Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment','Size']),\n        ('boolean_converter',replaceBoolean(),['IsHoliday'])\n    ],n_jobs = -1,verbose = True)\n    \n    data = completePipeline.fit_transform(data)\n    data = pd.DataFrame(data, columns = [x for x in range(1,len(data[0])+1)])\n    data = data.astype(dtype = np.float64)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.163384Z","iopub.execute_input":"2021-06-23T06:22:05.16396Z","iopub.status.idle":"2021-06-23T06:22:05.175827Z","shell.execute_reply.started":"2021-06-23T06:22:05.163915Z","shell.execute_reply":"2021-06-23T06:22:05.175056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Full Pipeline with Final Model","metadata":{}},{"cell_type":"code","source":"def fullPipeline():\n    # Preprocessing and splitting the data into train and test set\n    data = pd.read_csv(\"../input/course-material-walmart-challenge/train.csv\")\n    weekly_sales = data[\"Weekly_Sales\"].copy()\n    data.drop(\"Weekly_Sales\", axis = 1, inplace = True)\n    data = runPipeline(data)\n    data['Weekly_Sales'] = weekly_sales.tolist()\n    X, X_test = train_test_split(data, test_size = 0.2, random_state = 21)\n    y = X[\"Weekly_Sales\"].copy()\n    y_test = X_test['Weekly_Sales'].copy()\n    X.drop(['Weekly_Sales'],axis = 1, inplace = True)\n    X_test.drop(['Weekly_Sales'], axis = 1, inplace = True)\n    \n    # Initializing all the base models.\n    rf_reg = RandomForestRegressor(max_depth=232,max_features=23,n_estimators=162)\n    ext_reg = ExtraTreesRegressor(max_depth=239,max_features=27,n_estimators=283)\n    ext_reg = ExtraTreesRegressor(max_depth=150,max_features=27,n_estimators=283)\n    xgb_reg = xgboost.XGBRegressor(eta=0.15,gamma=0,max_depth=13,min_child_weight=5)\n    lin_reg = LinearRegression()\n    \n    # Declaring the parameters of Stacking Regressor\n    estimators = [('rf', rf_reg),('ext', ext_reg),('xgb',xgb_reg)]\n    final_estimator = lin_reg\n    \n    # Initializing and fitting the Stacking Regressor\n    stacking_reg = StackingRegressor(estimators=estimators,final_estimator=final_estimator,cv = 2)\n    stacking_reg.fit(X,y)\n    \n    # Predicting on the training set\n    y_pred = stacking_reg.predict(X)\n    stacking_train_mse = mean_squared_error(y, y_pred)\n    stacking_train_rmse = np.sqrt(stacking_train_mse)\n    print(\"Training Error = {}\".format(stacking_train_rmse))\n    acc_stacking_train = round( stacking_reg.score(X, y) * 100, 2)\n    print (\"Coefficient of determination R^2 of the prediction on the training set: \", str(acc_stacking_train) + ' percent')\n\n    # Predicting on the test set\n    y_pred = stacking_reg.predict(X_test)\n    stacking_test_mse = mean_squared_error(y_test, y_pred)\n    stacking_test_rmse = np.sqrt(stacking_test_mse)\n    print(\"Test Error = {}\".format(stacking_test_rmse))\n    acc_stacking_test = round( stacking_reg.score(X_test, y_test) * 100, 2)\n    print (\"Coefficient of determination R^2 of the prediction on the test set: \", str(acc_stacking_test) + ' percent')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:54:09.643203Z","iopub.execute_input":"2021-06-23T06:54:09.643609Z","iopub.status.idle":"2021-06-23T06:54:09.657422Z","shell.execute_reply.started":"2021-06-23T06:54:09.643579Z","shell.execute_reply":"2021-06-23T06:54:09.656596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fullPipeline()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:54:14.221371Z","iopub.execute_input":"2021-06-23T06:54:14.221904Z","iopub.status.idle":"2021-06-23T07:25:55.197185Z","shell.execute_reply.started":"2021-06-23T06:54:14.221869Z","shell.execute_reply":"2021-06-23T07:25:55.195989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Importing the whole data and applying the complete pipeline again on the data\n# train, test = importData('Date')\n# y = train['Weekly_Sales'].copy()\n# X_raw = train.drop(['Weekly_Sales'],axis = 1)\n# y_test = test['Weekly_Sales'].copy()\n# X_test_raw = test.drop(['Weekly_Sales'], axis = 1)\n# X = runPipeline(X_raw)\n# X_test = runPipeline(X_test_raw)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.207354Z","iopub.execute_input":"2021-06-23T06:22:05.207823Z","iopub.status.idle":"2021-06-23T06:22:05.217384Z","shell.execute_reply.started":"2021-06-23T06:22:05.207769Z","shell.execute_reply":"2021-06-23T06:22:05.216624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.21855Z","iopub.execute_input":"2021-06-23T06:22:05.219036Z","iopub.status.idle":"2021-06-23T06:22:05.232277Z","shell.execute_reply.started":"2021-06-23T06:22:05.219006Z","shell.execute_reply":"2021-06-23T06:22:05.231503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.235503Z","iopub.execute_input":"2021-06-23T06:22:05.235995Z","iopub.status.idle":"2021-06-23T06:22:05.243603Z","shell.execute_reply.started":"2021-06-23T06:22:05.235965Z","shell.execute_reply":"2021-06-23T06:22:05.24276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying out various regression models","metadata":{}},{"cell_type":"code","source":"# lin_reg = LinearRegression()\n# rnf_reg = RandomForestRegressor()\n# ext_reg = ExtraTreesRegressor()\n# voting_reg = VotingRegressor(\n#     estimators=[('rf', rnf_reg), ('et', ext_reg)]\n# )","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.244963Z","iopub.execute_input":"2021-06-23T06:22:05.245324Z","iopub.status.idle":"2021-06-23T06:22:05.254531Z","shell.execute_reply.started":"2021-06-23T06:22:05.245298Z","shell.execute_reply":"2021-06-23T06:22:05.253743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = X.copy()\n# train['Weekly_Sales'] = y.tolist()\n# X_train, X_valid = train_test_split(train, test_size = 0.2, random_state = 21)\n\n# y_train = X_train['Weekly_Sales'].copy()\n# X_train.drop([\"Weekly_Sales\"], axis = 1, inplace = True)\n# y_valid = X_valid['Weekly_Sales'].copy()\n# X_valid.drop([\"Weekly_Sales\"], axis = 1, inplace = True)\n\n\n# # for reg in (lin_reg, rnf_reg, ext_reg, voting_reg):\n# #     reg.fit(X_train,y_train)\n# #     y_pred = reg.predict(X_train)\n# #     y_valid_pred = reg.predict(X_valid)\n# #     reg_mse = mean_squared_error(y_train, y_pred)\n# #     reg_rmse = np.sqrt(reg_mse)\n# #     print(reg.__class__.__name__,\": \")\n# #     print(\"Training Error = {}\".format(reg_rmse))\n# #     reg_mse = mean_squared_error(y_valid, y_valid_pred)\n# #     reg_rmse = np.sqrt(reg_mse)\n# #     print(\"Validation Error = {}\".format(reg_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.255678Z","iopub.execute_input":"2021-06-23T06:22:05.255961Z","iopub.status.idle":"2021-06-23T06:22:05.265799Z","shell.execute_reply.started":"2021-06-23T06:22:05.255935Z","shell.execute_reply":"2021-06-23T06:22:05.264878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the training and validation data set","metadata":{}},{"cell_type":"code","source":"# rows = [[x for x in range(1,30)]]\n# rows[0].append(\"Weekly_Sales\")\n# for i in range(225960):\n#     rows.append([train[x][i] for x in range(1,30)])\n#     rows[i+1].append(train['Weekly_Sales'][i])\n# with open(\"train.csv\", 'w', newline='') as file:\n#     writer = csv.writer(file)\n#     writer.writerows(rows)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.267095Z","iopub.execute_input":"2021-06-23T06:22:05.267386Z","iopub.status.idle":"2021-06-23T06:22:05.276394Z","shell.execute_reply.started":"2021-06-23T06:22:05.267361Z","shell.execute_reply":"2021-06-23T06:22:05.275593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the training data set","metadata":{}},{"cell_type":"code","source":"# train = pd.read_csv(\"./train.csv\")\n# train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.277644Z","iopub.execute_input":"2021-06-23T06:22:05.277952Z","iopub.status.idle":"2021-06-23T06:22:05.285801Z","shell.execute_reply.started":"2021-06-23T06:22:05.277925Z","shell.execute_reply":"2021-06-23T06:22:05.285122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying out Boosting Algorithms","metadata":{}},{"cell_type":"code","source":"# #Initializing and fitting the model\n# gbrt = GradientBoostingRegressor()\n# gbrt.fit(X_train, y_train)\n\n# #Predicting on traing set\n# y_pred = gbrt.predict(X_train)\n# gbrt_mse = mean_squared_error(y_train, y_pred)\n# gbrt_rmse = np.sqrt(gbrt_mse)\n# print(\"Gradient Boosting Regressor Training Error = {}\".format(gbrt_rmse))\n\n# #Predicting on validation set\n# y_pred = gbrt.predict(X_valid)\n# gbrt_mse = mean_squared_error(y_valid, y_pred)\n# gbrt_rmse = np.sqrt(gbrt_mse)\n# print(\"Gradient Boosting Regressor Validation Error = {}\".format(gbrt_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.286758Z","iopub.execute_input":"2021-06-23T06:22:05.287215Z","iopub.status.idle":"2021-06-23T06:22:05.296752Z","shell.execute_reply.started":"2021-06-23T06:22:05.287186Z","shell.execute_reply":"2021-06-23T06:22:05.295895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Initializing and fitting the model\n# xgb_reg = xgboost.XGBRegressor()\n# xgb_reg.fit(X_train, y_train)\n\n# #Predicting on traing set\n# y_pred = xgb_reg.predict(X_train)\n# xgb_reg_mse = mean_squared_error(y_train, y_pred)\n# xgb_reg_rmse = np.sqrt(xgb_reg_mse)\n# print(\"Extreme Gradient Boosting Regressor Training Error = {}\".format(xgb_reg_rmse))\n\n# #Predicting on validation set\n# y_pred = xgb_reg.predict(X_valid)\n# xgb_reg_mse = mean_squared_error(y_valid, y_pred)\n# xgb_reg_rmse = np.sqrt(xgb_reg_mse)\n# print(\"Extreme Gradient Boosting Regressor Validation Error = {}\".format(xgb_reg_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.29793Z","iopub.execute_input":"2021-06-23T06:22:05.298216Z","iopub.status.idle":"2021-06-23T06:22:05.310475Z","shell.execute_reply.started":"2021-06-23T06:22:05.29819Z","shell.execute_reply":"2021-06-23T06:22:05.309598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Initializing and fitting the model\n# ext_reg = ExtraTreesRegressor(n_estimators = 20,max_depth = 15)\n# adb_reg = AdaBoostRegressor(base_estimator = ext_reg)\n# adb_reg.fit(X_train, y_train)\n\n# #Predicting on traing set\n# y_pred = adb_reg.predict(X_train)\n# adb_reg_mse = mean_squared_error(y_train, y_pred)\n# adb_reg_rmse = np.sqrt(adb_reg_mse)\n# print(\"Adaptive Boosting Regressor Training Error = {}\".format(adb_reg_rmse))\n\n# #Predicting on validation set\n# y_pred = adb_reg.predict(X_valid)\n# adb_reg_mse = mean_squared_error(y_valid, y_pred)\n# adb_reg_rmse = np.sqrt(adb_reg_mse)\n# print(\"Adaptive Boosting Regressor Validation Error = {}\".format(adb_reg_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.311745Z","iopub.execute_input":"2021-06-23T06:22:05.312091Z","iopub.status.idle":"2021-06-23T06:22:05.320811Z","shell.execute_reply.started":"2021-06-23T06:22:05.31205Z","shell.execute_reply":"2021-06-23T06:22:05.319873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tunning Random Forest Regressor ","metadata":{}},{"cell_type":"code","source":"# param_distribs = {\n#     'n_estimators': randint(low=150, high=300),\n#     'max_features': randint(low=15, high=30),\n#     'max_depth':randint(low=120, high = 250)\n# }\n\n# forest_reg = RandomForestRegressor(random_state=21)\n# forest_rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, n_jobs = -1, return_train_score = True,\n#                                 n_iter=15, cv=2, scoring='neg_mean_squared_error', random_state=21)\n# forest_rnd_search.fit(X_train,y_train)\n\n# cvres = forest_rnd_search.cv_results_\n# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n#     print(np.sqrt(-mean_score), params)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.322004Z","iopub.execute_input":"2021-06-23T06:22:05.322278Z","iopub.status.idle":"2021-06-23T06:22:05.330529Z","shell.execute_reply.started":"2021-06-23T06:22:05.322251Z","shell.execute_reply":"2021-06-23T06:22:05.329725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best Params:  {'max_depth': 232, 'max_features': 23, 'n_estimators': 162}\n# Random Forest Regressor Validation Error = 5202.5457875042375\n\n# print(\"Best Params: \", forest_rnd_search.best_params_)\n# y_pred = forest_rnd_search.best_estimator_.predict(X_valid)\n# forest_mse = mean_squared_error(y_valid, y_pred)\n# forest_rmse = np.sqrt(forest_mse)\n# print(\"Random Forest Regressor Validation Error = {}\".format(forest_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.331687Z","iopub.execute_input":"2021-06-23T06:22:05.331974Z","iopub.status.idle":"2021-06-23T06:22:05.343277Z","shell.execute_reply.started":"2021-06-23T06:22:05.331948Z","shell.execute_reply":"2021-06-23T06:22:05.342342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tunning Extra Trees Regressor ","metadata":{}},{"cell_type":"code","source":"# param_distribs = {\n#     'n_estimators': randint(low=150, high=300),\n#     'max_features': randint(low=15, high=30),\n#     'max_depth':randint(low=120, high = 250)\n# }\n\n# ext_reg = ExtraTreesRegressor(random_state=21)\n# ext_rnd_search = RandomizedSearchCV(ext_reg, param_distributions=param_distribs, n_jobs = -1, return_train_score = True,\n#                                 n_iter=15, cv=2, scoring='neg_mean_squared_error', random_state=21)\n# ext_rnd_search.fit(X_train,y_train)\n\n# cvres = ext_rnd_search.cv_results_\n# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n#     print(np.sqrt(-mean_score), params)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.344599Z","iopub.execute_input":"2021-06-23T06:22:05.34494Z","iopub.status.idle":"2021-06-23T06:22:05.352739Z","shell.execute_reply.started":"2021-06-23T06:22:05.344911Z","shell.execute_reply":"2021-06-23T06:22:05.351914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best Params:  {'max_depth': 239, 'max_features': 27, 'n_estimators': 283}\n\n# # Extra Trees Regressor Validation Error = 4782.645726024579\n# print(\"Best Params: \", ext_rnd_search.best_params_)\n# y_pred = ext_rnd_search.best_estimator_.predict(X_valid)\n# ext_mse = mean_squared_error(y_valid, y_pred)\n# ext_rmse = np.sqrt(ext_mse)\n# print(\"Extra Trees Regressor Validation Error = {}\".format(ext_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.353933Z","iopub.execute_input":"2021-06-23T06:22:05.354206Z","iopub.status.idle":"2021-06-23T06:22:05.362722Z","shell.execute_reply.started":"2021-06-23T06:22:05.35418Z","shell.execute_reply":"2021-06-23T06:22:05.36157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tuning XGBoost","metadata":{}},{"cell_type":"code","source":"# param_distribs = {\n#     'eta':[0.15,0.17,0.19,0.2,0.22,0.24,0.25,0.27,0.29,0.3],\n#     'min_child_weight':randint(low = 1, high = 7),\n#     'max_depth':randint(low=10, high = 25),\n#     'gamma':randint(low = 0,high = 5),\n# }\n\n# xgb_reg = xgboost.XGBRegressor()\n# xgb_rnd_search = RandomizedSearchCV(xgb_reg, param_distributions=param_distribs, n_jobs = -1, return_train_score = True,\n#                                 n_iter=15, cv=2, scoring='neg_mean_squared_error', random_state=21)\n# xgb_rnd_search.fit(X_train,y_train)\n\n# cvres = xgb_rnd_search.cv_results_\n# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n#     print(np.sqrt(-mean_score), params)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.36409Z","iopub.execute_input":"2021-06-23T06:22:05.364365Z","iopub.status.idle":"2021-06-23T06:22:05.376235Z","shell.execute_reply.started":"2021-06-23T06:22:05.364338Z","shell.execute_reply":"2021-06-23T06:22:05.37536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best Params:  {'eta': 0.15, 'gamma': 0, 'max_depth': 13, 'min_child_weight': 5}\n\n# print(\"Best Params: \", xgb_rnd_search.best_params_)\n# y_pred = xgb_rnd_search.best_estimator_.predict(X_valid)\n# xgb_mse = mean_squared_error(y_valid, y_pred)\n# xgb_rmse = np.sqrt(xgb_mse)\n# print(\"Extra Trees Regressor Validation Error = {}\".format(xgb_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.37784Z","iopub.execute_input":"2021-06-23T06:22:05.378278Z","iopub.status.idle":"2021-06-23T06:22:05.386627Z","shell.execute_reply.started":"2021-06-23T06:22:05.378236Z","shell.execute_reply":"2021-06-23T06:22:05.385592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing Votting Regressor","metadata":{}},{"cell_type":"code","source":"# Training Error = 939.4560063275186 (ext+xgb)\n# Validation Error = 4582.014285496773\n\n# ext_reg = ExtraTreesRegressor(max_depth=239,max_features=27,n_estimators=283)\n# xgb_reg = xgboost.XGBRegressor(eta=0.15,gamma=0,max_depth=13,min_child_weight=5)\n# voting_reg = VotingRegressor(estimators=[('xgb', xgb_reg), ('et', ext_reg), ('rnf', rnf_reg)])\n\n# voting_reg.fit(X_train, y_train)\n# y_pred = voting_reg.predict(X_train)\n# voting_train_mse = mean_squared_error(y_train, y_pred)\n# voting_train_rmse = np.sqrt(voting_train_mse)\n# print(\"Training Error = {}\".format(voting_train_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.388104Z","iopub.execute_input":"2021-06-23T06:22:05.388579Z","iopub.status.idle":"2021-06-23T06:22:05.397591Z","shell.execute_reply.started":"2021-06-23T06:22:05.388505Z","shell.execute_reply":"2021-06-23T06:22:05.396843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = voting_reg.predict(X_valid)\n# voting_valid_mse = mean_squared_error(y_valid, y_pred)\n# voting_valid_rmse = np.sqrt(voting_valid_mse)\n# print(\"Validation Error = {}\".format(voting_valid_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.399531Z","iopub.execute_input":"2021-06-23T06:22:05.400171Z","iopub.status.idle":"2021-06-23T06:22:05.406181Z","shell.execute_reply.started":"2021-06-23T06:22:05.400137Z","shell.execute_reply":"2021-06-23T06:22:05.405508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing Stacking Regressor","metadata":{}},{"cell_type":"code","source":"# # Training Error = 736.3944328793483\n# # Validation Error = 4517.871439867831\n\n# rf_reg = RandomForestRegressor(max_depth=232,max_features=23,n_estimators=162)\n# ext_reg = ExtraTreesRegressor(max_depth=239,max_features=27,n_estimators=283)\n# xgb_reg = xgboost.XGBRegressor(eta=0.15,gamma=0,max_depth=13,min_child_weight=5)\n# lin_reg = LinearRegression()\n\n# estimators = [('rf', rf_reg),('ext', ext_reg),('xgb',xgb_reg)]\n# final_estimator = lin_reg\n\n# stacking_reg = StackingRegressor(estimators=estimators,final_estimator=final_estimator,cv = 2)\n# stacking_reg.fit(X_train,y_train)\n\n# y_pred = stacking_reg.predict(X_train)\n# stacking_train_mse = mean_squared_error(y_train, y_pred)\n# stacking_train_rmse = np.sqrt(stacking_train_mse)\n# print(\"Training Error = {}\".format(stacking_train_rmse))\n# acc_stacking_train = round( stacking_reg.score(X_train, y_train) * 100, 2)\n# print (\"Coefficient of determination R^2 of the prediction on the test set: \", str(acc_stacking_train) + ' percent')\n    \n# y_pred = stacking_reg.predict(X_valid)\n# stacking_valid_mse = mean_squared_error(y_valid, y_pred)\n# stacking_valid_rmse = np.sqrt(stacking_valid_mse)\n# print(\"Validation Error = {}\".format(stacking_valid_rmse))\n# acc_stacking_test = round( stacking_reg.score(X_valid, y_valid) * 100, 2)\n# print (\"Coefficient of determination R^2 of the prediction on the test set: \", str(acc_stacking_test) + ' percent')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T06:22:05.407381Z","iopub.execute_input":"2021-06-23T06:22:05.407933Z","iopub.status.idle":"2021-06-23T06:22:05.419711Z","shell.execute_reply.started":"2021-06-23T06:22:05.407837Z","shell.execute_reply":"2021-06-23T06:22:05.418899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}