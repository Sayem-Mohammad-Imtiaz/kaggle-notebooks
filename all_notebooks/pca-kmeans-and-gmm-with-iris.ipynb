{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport itertools\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\niris = pd.read_csv('/kaggle/input/iris-dataset/iris.data.csv', names=iris_columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the continuous attributes to a matrix X\nX = iris.iloc[:,0:4].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the class attribute as y\ny = iris.iloc[:,4].astype('category').values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import decomposition\npca = decomposition.PCA(n_components=2)\npca.fit(X)\nX_fit = pca.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Cumulative Variance explained by 1 principal components: %.2f%%' % ( sum(pca.explained_variance_ratio_) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put principal components into a data frame so we can plot it.\ndfpc = pd.DataFrame(X_fit, columns=['pc1', 'pc2'])\ndfpc['class'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize=(10,10), dpi=100)\nplt.clf()\nsns.lmplot(data=dfpc, x=\"pc1\", y=\"pc2\", fit_reg=False, hue='class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nn_clusters = 5\nkmeans = KMeans(n_clusters=n_clusters, random_state=123)\nkmeans.fit(X_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_labels = kmeans.labels_\ncluster_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.subplots(figsize=(10,5))\nax=sns.countplot(cluster_labels)\ntitle=\"Histogram of Cluster Counts of K-means\"\nax.set_title(title, fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris['X'] = X_fit[:,[0]]\niris['Y'] = X_fit[:,[1]]\niris['cluster'] = cluster_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.subplots(figsize=(10,10))\nax = sns.scatterplot(x='X', y='Y', hue='cluster', legend=\"full\", palette=\"Set1\", data=iris)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GMM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best Gaussian Mixture Model\nfrom sklearn.mixture import GaussianMixture\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\n\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a Gaussian mixture\n        gmm = GaussianMixture(n_components=n_components, covariance_type=cv_type)\n        gmm.fit(X_fit)\n        bic.append(gmm.bic(X_fit))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n\nbic = np.array(bic) # convert to Numpy array\nprint('Best model: {}'.format(best_gmm.covariance_type))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_gmm.covariances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot unclustered and clustered points side by side\nfrom scipy import linalg\ncolor_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue', 'darkorange'])\nplt.figure(figsize=(12, 5))\n\nax1 = plt.subplot(1, 2, 1)\nax1.scatter(X_fit[:, 0], X_fit[:, 1], marker='.', c='k', s=20, edgecolor='')\nax1.set_title('Original Points')\nax1.set_xlim(-4, 4)\nax1.set_ylim(-2, 2)\n\n\n# Plot the winner\nsplot = plt.subplot(1, 2, 2)\nsplot.set_xlim(-4, 4)\nsplot.set_ylim(-2, 2)\nY_fit = best_gmm.predict(X_fit)\n\nfor i, (mean, cov, color) in enumerate(zip(best_gmm.means_, best_gmm.covariances_, color_iter)):\n    # Convert covariance to square matrix\n    if best_gmm.covariance_type == 'full':\n        cov = best_gmm.covariances_[i][:2, :2]\n    elif best_gmm.covariance_type == 'tied':\n        cov = best_gmm.covariances_[:2, :2]\n    elif best_gmm.covariance_type == 'diag':\n        cov = np.diag(best_gmm.covariances_[i][:2])\n    elif best_gmm.covariance_type == 'spherical':\n        cov = np.eye(best_gmm.means_.shape[1]) * best_gmm.covariances_[i]\n\n    v, w = linalg.eigh(cov)\n    if not np.any(Y_fit == i):\n        continue\n        \n    plt.scatter(X_fit[Y_fit ==i, 0], X_fit[Y_fit ==i, 1], marker='.', s=20, edgecolor='', color=color)\n\n    # Plot an ellipse to show the Gaussian component\n    angle = np.arctan2(w[0][1], w[0][0])\n    angle = 180. * angle / np.pi  # convert to degrees\n    v = 2. * np.sqrt(2.) * np.sqrt(v)\n    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(.5)\n    splot.add_artist(ell)\n\nplt.title('Selected GMM: {} model, {} components'.format(best_gmm.covariance_type, best_gmm.n_components))\nplt.subplots_adjust(hspace=.35, bottom=.02)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"1. In order to analyze the clustering effect, we use the PCA algorithm to reduce multi-dimensions to two dimensions.\n2. K-means algorithm uses the distance between data points to divide the data into different clusters, and the points with shorter  distances are collected in the same cluster. \n3. I would perfer using GMM to do classification. In the process of classification with GMM, we can obtain a probability distribution of Y value through unknown data X, that is, the output obtained by the training model is not a specific value, but the probability of a series of values. In other words, the data in the sample is projected on several Gaussian models (clusters), and GMM can return the probability which a record of data belonged to each clusters. Then we can choose the cluster with the highest probability as the classification of the records. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}