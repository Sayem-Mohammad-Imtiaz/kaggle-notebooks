{"cells":[{"metadata":{},"cell_type":"markdown","source":"Credits:\n\nhttps://www.kaggle.com/phiitm/aspect-based-sentiment-analysis\n\nhttps://www.kaggle.com/paultimothymooney/most-common-words-in-the-cord-19-dataset"},{"metadata":{},"cell_type":"markdown","source":"# Most Common Words in the CORD-19 Dataset"},{"metadata":{},"cell_type":"markdown","source":"[CORD-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) is a resource of over 24,000 scholarly articles, including over 12,000 with full text, about COVID-19 and the coronavirus group. \n\nThese are the most common words in the titles of the papers from the CORD-19 dataset. "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndef count_ngrams(dataframe,column,begin_ngram,end_ngram):\n    # adapted from https://stackoverflow.com/questions/36572221/how-to-find-ngram-frequency-of-a-column-in-a-pandas-dataframe\n    word_vectorizer = CountVectorizer(ngram_range=(begin_ngram,end_ngram), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(df['title'].dropna())\n    frequencies = sum(sparse_matrix).toarray()[0]\n    most_common = pd.DataFrame(frequencies, \n                               index=word_vectorizer.get_feature_names(), \n                               columns=['frequency']).sort_values('frequency',ascending=False)\n    most_common['ngram'] = most_common.index\n    most_common.reset_index()\n    return most_common\n\n\ndef word_bar_graph_function(df,column,title):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()\n    \ndf = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')  \nthree_gram = count_ngrams(df,'title',3,3)\nwords_to_exclude = [\"my\",\"to\",\"at\",\"for\",\"it\",\"the\",\"with\",\"from\",\"would\",\"there\",\"or\",\"if\",\"it\",\"but\",\"of\",\"in\",\"as\",\"and\",'NaN','dtype']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most Common Words"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nword_bar_graph_function(df,'title','Most common words in the titles of the papers in the CORD-19 dataset')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.bar(three_gram.sort_values('frequency',ascending=False)[0:10], \n             x=\"frequency\", \n             y=\"ngram\",\n             title='Most Common 3-Words in Titles of Papers in CORD-19 Dataset',\n             orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most Common Journals"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"value_counts = df['journal'].value_counts()\nvalue_counts_df = pd.DataFrame(value_counts)\nvalue_counts_df['journal_name'] = value_counts_df.index\nvalue_counts_df['count'] = value_counts_df['journal']\nfig = px.bar(value_counts_df[0:20].sort_values('count'), \n             x=\"count\", \n             y=\"journal_name\",\n             title='Most Common Journals in the CORD-19 Dataset',\n             orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"value_counts = df['publish_time'].value_counts()\nvalue_counts_df = pd.DataFrame(value_counts)\nvalue_counts_df['which_year'] = value_counts_df.index\nvalue_counts_df['count'] = value_counts_df['publish_time']\nfig = px.bar(value_counts_df[0:5].sort_values('count'), \n             x=\"count\", \n             y=\"which_year\",\n             title='Most Common Dates of Publication',\n             orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SpaCy: Aspect based key words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport random\nrandom.seed(2019)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim import corpora, models\nimport gensim\n\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.tokenize import WordPunctTokenizer\n\nimport datetime\n\nfrom collections import Counter\ndef freqx(l, a=10):    \n    counter=Counter(l)\n    #print(counter)\n    #print(counter.values())\n    #print(counter.keys())\n    return counter.most_common(a)\n\nimport spacy\nfrom spacy import displacy\nimport en_core_web_sm\nfrom tqdm import tqdm\nnlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['title'].astype(str) + \". \"+ df['abstract'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def asb(data):\n    data = data.reset_index(drop = True)\n    aspect_terms = []\n    comp_terms = []\n    for x in tqdm(range(len(data['text']))):\n        amod_pairs = []\n        advmod_pairs = []\n        compound_pairs = []\n        xcomp_pairs = []\n        neg_pairs = []\n        if len(str(data['text'][x])) != 0:\n            lines = str(data['text'][x]).replace('*',' ').replace('-',' ').replace('so ',' ').replace('be ',' ').replace('are ',' ').replace('just ',' ').replace('get ','').replace('were ',' ').replace('When ','').replace('when ','').replace('again ',' ').replace('where ','').replace('how ',' ').replace('has ',' ').replace('Here ',' ').replace('here ',' ').replace('now ',' ').replace('see ',' ').replace('why ',' ').split('.')       \n            for line in lines:\n                doc = nlp(line)\n                str1=''\n                str2=''\n                for token in doc:\n                    if token.pos_ is 'NOUN':\n                        for j in token.lefts:\n                            if j.dep_ == 'compound':\n                                compound_pairs.append((j.text+' '+token.text,token.text))\n                            if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n                                str1 = j.text+' '+token.text\n                                amod_pairs.append(j.text+' '+token.text)\n                                for k in j.lefts:\n                                    if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n                                        str2 = k.text+' '+j.text+' '+token.text\n                                        amod_pairs.append(k.text+' '+j.text+' '+token.text)\n                                mtch = re.search(re.escape(str1),re.escape(str2))\n                                if mtch is not None:\n                                    amod_pairs.remove(str1)\n                    if token.pos_ is 'VERB':\n                        for j in token.lefts:\n                            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n                                advmod_pairs.append(j.text+' '+token.text)\n                            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n                                neg_pairs.append(j.text+' '+token.text)\n                        for j in token.rights:\n                            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n                                advmod_pairs.append(token.text+' '+j.text)\n                    if token.pos_ is 'ADJ':\n                        for j,h in zip(token.rights,token.lefts):\n                            if j.dep_ is 'xcomp' and h.dep_ is not 'neg':\n                                for k in j.lefts:\n                                    if k.dep_ is 'aux':\n                                        xcomp_pairs.append(token.text+' '+k.text+' '+j.text)\n                            elif j.dep_ is 'xcomp' and h.dep_ is 'neg':\n                                if k.dep_ is 'aux':\n                                        neg_pairs.append(h.text +' '+token.text+' '+k.text+' '+j.text)\n\n            pairs = list(set(amod_pairs+advmod_pairs+neg_pairs+xcomp_pairs))\n            for i in range(len(pairs)):\n                if len(compound_pairs)!=0:\n                    for comp in compound_pairs:\n                        mtch = re.search(re.escape(comp[1]),re.escape(pairs[i]))\n                        if mtch is not None:\n                            pairs[i] = pairs[i].replace(mtch.group(),comp[0])\n\n        aspect_terms.append(pairs)\n        comp_terms.append(compound_pairs)\n\n\n\n    data['compound_nouns'] = comp_terms\n    data['aspect_keywords'] = aspect_terms\n    term1 = []\n    for j in range(len(aspect_terms)):\n        for i in aspect_terms[j]:\n            if len(i)>1:\n                term1.append(i)\n    term2 = []\n    for j in range(len(comp_terms)):\n        for i in comp_terms[j]:\n            if len(i[0])>1:\n                term2.append(i[0])\n    \n    z1 = freqx(term1, 1000)\n    z2 = freqx(term2, 1000)\n    \n    return(pd.DataFrame(z1), pd.DataFrame(z2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ab, df_cn = asb(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ab.columns = ['Ab_words', 'frequency']   # aspect words\ndf_cn.columns = ['Cn_words', 'frequency']   #compound nouns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df_ab[0:20].sort_values('frequency'), \n             x=\"frequency\", \n             y=\"Ab_words\",\n             title='Most common Aspect based keywords in Titles & Abstract of Papers in CORD-19 Dataset',\n             orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"fig = px.bar(df_cn[0:20].sort_values('frequency'), \n             x=\"frequency\", \n             y=\"Cn_words\",\n             title='Most common Compound nouns in Titles & Abstract of Papers in CORD-19 Dataset',\n             orientation='h')\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}