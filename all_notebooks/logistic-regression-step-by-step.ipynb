{"cells":[{"execution_count":null,"cell_type":"markdown","outputs":[],"source":"### Hello everyone, if you are beginner in machine learning then after reading this short tutorial, you ain't gonna be a newbie in Logistic Regression.This is a short tutorial as to how to write step-by-step logistic regression code.Since the formulas are too complex,I have simplified my code a lot so that it is easier to understand.","metadata":{"_uuid":"bd96ed125e1fa6c54d2de82e747cf60d4bf1343a","_cell_guid":"dc24c916-c36a-48d6-bb80-ae5eb0d57add"}},{"execution_count":null,"cell_type":"markdown","outputs":[],"source":"**This notebook contains step by step implementation of logistic regression and if there is an piece of suggestion or doubt regarding the notebook, please feel free to put forward.**","metadata":{"_uuid":"a0c6db011fa0f1cddfdd990e91a0a853bb75198a","_cell_guid":"12229e03-157b-4756-bc50-2ac57a5e9ea6"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import os\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas import DataFrame\nimport scipy.optimize as opt  \n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","metadata":{"_cell_guid":"81ce5c8c-7fca-4ec1-9a3f-e0e6639e1a1e","trusted":false,"_execution_state":"idle","_uuid":"20d486efb3de03b4bbc7b046981a19858701ff62","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"f=pd.read_csv(\"../input/mushrooms.csv\")\ndf=DataFrame(f)\ndf.head()[:2]","metadata":{"_cell_guid":"07f927e3-a9b5-49e1-99fb-cd50389a29d8","trusted":false,"_execution_state":"idle","_uuid":"a1d94d2bc6d0c6d2e2d7f1ea662ca9a4b8687d53","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df.dtypes","metadata":{"_cell_guid":"5d01d7a1-9cfe-415f-8925-18c5e9e3e52b","trusted":false,"_execution_state":"idle","_uuid":"81d58674cd997e064ea89754f7738c0752dff492","collapsed":false}},{"execution_count":null,"cell_type":"markdown","outputs":[],"source":"#### **Here we can see that all the columns of the dataframe are of the object type so in order to properly analyze them, we need to encode the object values in each column with the appropriate numerical value.**","metadata":{"_uuid":"e1a4f5064901d5bef961a20c3d93221b4c7b9e4a","_cell_guid":"7ceff958-6010-442c-9d43-75a209d3a471"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in df.columns:\n    df[col]=labelencoder.fit_transform(df[col])\ndf.head()[:2]","metadata":{"_cell_guid":"3d5ba2a5-7ff8-4031-9ace-aef4842ff302","trusted":false,"_execution_state":"idle","_uuid":"015c08cfe4a794ba65d5a1818f47e3dfeadb32df","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"count_var=[]\nfor col in df.columns:\n    count_var.append(df[col].unique().sum())\nsize=np.arange(len(count_var))\nfig=plt.figure(figsize = (15,10))\nax=fig.add_subplot(1,1,1, axisbg='red')\nax.bar(size,count_var, color = 'k')\nax.set(title=\"Unique elements per column\",\n      ylabel='No of unique elements',\n      xlabel='Features')","metadata":{"_cell_guid":"be9e91e4-8fbd-4552-ae94-1cbbe98e7f9c","trusted":false,"_execution_state":"idle","_uuid":"67c77e47d95a2cdf3430cc665493267812fe503d","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df.corr()","metadata":{"_cell_guid":"fa4cdeef-f1c9-4637-a01f-bd93900a62ed","trusted":false,"_execution_state":"idle","_uuid":"8943adcee6789ade78e3876ba7848d3746dda459","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import seaborn as sns\nplt.figure(figsize = (10,10))\nsns.heatmap(df.corr(), cmap = 'inferno',square=True)","metadata":{"_cell_guid":"1c62b263-4e22-48c1-8d56-e06fc72c771e","trusted":false,"_execution_state":"idle","_uuid":"5966f4494ac83901c3076501a319388aabecec7a","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Separating the train and target values.Lets select only two features so that things can be intuitively easy.So lers select the most corellated features to 'class' . \ntarget=df['class']\ntrain=df[['gill-size','gill-color']]\nprint(train.shape)\nprint(target.shape)","metadata":{"_cell_guid":"6845ac67-773d-49b4-ab96-205618a5d79a","trusted":false,"_execution_state":"idle","_uuid":"d66d034586b7e843ffe5ddfdbac88a185ba67699","collapsed":false}},{"execution_count":null,"cell_type":"markdown","outputs":[],"source":"*Lets have a look at the distribution of the numerical values among all the columns*","metadata":{"_uuid":"2dde1ebe40dad74aed5ef7905555faaa57f9ac9b","_cell_guid":"af443f94-798a-4969-b1be-5c9625831285"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Count of the classes\nfig=plt.figure(figsize = (15,10))\nax=fig.add_subplot(1,1,1, axisbg='blue')\npd.value_counts(target).plot(kind='bar', cmap = 'cool')\nplt.title(\"Class distribution\")","metadata":{"_cell_guid":"a7fa41ff-4759-47f5-86b4-4c9663833f7b","trusted":false,"_execution_state":"idle","_uuid":"a363ccc48043010cfe2df16506080fe4a51f13c3","collapsed":false}},{"execution_count":null,"cell_type":"markdown","outputs":[],"source":"  **In logistic regression, the link function is the sigmoid. We can implement this really easily.The sigmoid function has special properties that can result values in the range [0,1].  So you have large positive values of X, the sigmoid should be close to 1, while for large negative values,  the sigmoid should be close to 0.\n**","metadata":{"_uuid":"efe98b14ddd41c23223b809b586f1261c83b7c30","_cell_guid":"6efc7404-9754-4667-8638-3fa232d0baee"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"def sigmoid(theta,X):  \n    X = np.array(X)\n    theta = np.asarray(theta)\n    return((1/(1+math.e**(-X.dot(theta)))))","metadata":{"_cell_guid":"b6186cee-a984-4f36-9bd9-db0ba04fd499","trusted":false,"_execution_state":"idle","_uuid":"df41eee12e31facebffd6d8a9aad7646dc548ddf","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Function for the cost function of the logistic regression.\ndef cost(theta, X, Y):\n    first = np.multiply(-Y, np.log(sigmoid(theta,X)))\n    second = np.multiply((1 - Y), np.log(1 - sigmoid(theta,X)))\n    return np.sum(first - second) / (len(X))","metadata":{"_cell_guid":"7df45a3c-e53c-42b5-a9e1-7cd97eb36758","trusted":false,"_execution_state":"idle","_uuid":"b8a0520ff41e8a09f0dbd6e58212392a77178ed3","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# It calculates the gradient of the log-likelihood function.\ndef log_gradient(theta,X,Y):\n    first_calc = sigmoid(theta, X) - np.squeeze(Y).T\n    final_calc = first_calc.T.dot(X)\n    return(final_calc.T)","metadata":{"_cell_guid":"8b952dea-b2e5-411d-bd1f-6e570924a8c0","trusted":false,"_execution_state":"idle","_uuid":"cf018e033435d08c9c49497a174c73d69e591461","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# This is the function performing gradient descent.\ndef gradient_Descent(theta,X,Y,itr_val,learning_rate=0.00001):\n    cost_iter=[]\n    cost_val=cost(theta,X,Y)\n    cost_iter.append([0,cost_val])\n    change_cost = 1\n    itr = 0\n    while(itr < itr_val):\n        old_cost = cost_val\n        theta = theta - (0.01 * log_gradient(theta,X,Y))\n        cost_val = cost(theta,X,Y)\n        cost_iter.append([i,cost])\n        itr += 1\n    return theta","metadata":{"_cell_guid":"3e6e3704-67d1-49a7-b4b9-f26e9e371561","trusted":false,"_execution_state":"idle","_uuid":"fb08ec2ddfe0a5158d042f82ba04001d213857af","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"def pred_values(theta,X,hard=True):\n    X = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n    pred_prob = sigmoid(theta,X)\n    pred_value = np.where(pred_prob >= .5 ,1, 0)\n    return pred_value","metadata":{"_cell_guid":"ed058db1-703c-4440-8e20-8d07c1bec1fc","trusted":false,"_execution_state":"idle","_uuid":"429b5f336a1baa950deb36e1162f0494ea8997db","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"theta = np.zeros((train.shape)[1])\ntheta = np.asmatrix(theta)\ntheta = theta.T\ntarget = np.asmatrix(target).T\ny_test = list(target)","metadata":{"_cell_guid":"0b83b391-d18f-4193-a931-a5161fb9a79a","trusted":false,"_execution_state":"idle","_uuid":"3d5890a07ed8994581d73ff83b190a12e1b05506","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"params = [10,20,30,50,100]\nfor i in range(len(params)):\n    th = gradient_Descent(theta,train,target,params[i])\n    y_pred = list(pred_values(th, train))\n    score = float(sum(1 for x,y in zip(y_pred,y_test) if x == y)) / len(y_pred)\n    print(\"The accuracy after \" + '{}'.format(params[i]) + \" iterations is \" + '{}'.format(score))","metadata":{"_cell_guid":"654c7f44-c7d1-43a0-8c07-6e2eadb06bc7","trusted":false,"_execution_state":"idle","_uuid":"6b5a7bd14b59671bc37320b5b23ba707021f2346","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(train, target)\nclf.score(train, target)","metadata":{"_cell_guid":"d91d5376-48e1-4fea-b58f-daf3afa8fcb7","trusted":false,"_execution_state":"idle","_uuid":"fd5e4dc4e16170fe948a5fa26c4c1d38fffb2230","collapsed":false}}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":2}