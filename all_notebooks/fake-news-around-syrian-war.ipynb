{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Background**\n\nMost currently available fake news datasets revolve around US politics, entrainment news or satire. They are typically scraped from fact-checking websites, where the articles are labeled by human experts.\nThis dataset around the Syrian war. Given the specific nature of news reporting on incidents of wars and the lack of available sources from which manually-labeled news articles can be scraped."},{"metadata":{},"cell_type":"markdown","source":"**About the dataset**\n\nThe dataset consists of news articles from several media outlets representing mobilisation press, loyalist press, and diverse print media.Also,consists of a set of articles/news labeled by 0 (fake) or 1 (credible).\nThe dataset consists of 804 articles labeled as true or fake and that is ideal for training machine learning models to predict the credibility of news articles.\n\nCredibility of articles are computed with respect to a ground truth information obtained from the Syrian Violations Documentation Center (VDC). This dataset is collected by researchers at American University of Beirut(AUB)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nimport itertools\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading the data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/a-fake-news-dataset-around-the-syrian-war/FA-KES-Dataset.csv',encoding='latin1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the dataset contains the articl title, article content, media source, date of incident,where and the incident happend and the labels(real or fake)."},{"metadata":{},"cell_type":"markdown","source":"**Preliminary text exploration**\n\n\nBefore we proceed with any text pre-processing, it is advisable to quickly explore the dataset in terms of word counts, most common and most uncommon words."},{"metadata":{},"cell_type":"markdown","source":"Count NaN or missing values in DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.article_content.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have duplicated rows in our dataset"},{"metadata":{},"cell_type":"markdown","source":"Find Duplicate Rows based on all columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"ddf = df[df.duplicated()]\nprint(ddf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Duplicated rows might affect on our results, So, we should remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(keep=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddf = df[df.duplicated()]\nprint(ddf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can move forward in our task!"},{"metadata":{},"cell_type":"markdown","source":"It's better to strat with understaning how our dataset distributed according to the label(labels 0/1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show Labels distribution\n\ndf['labels'].value_counts(normalize=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset is a bit unbalanced towords real news(1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='labels', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Data Analysis of News**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['source'].value_counts().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here sources of news in an ascending order"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.groupby(['source','labels']).size().unstack().plot(kind='bar',stacked=False)\nplt.figure(figsize=(20,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It showes here how each source is contributing in real or fake news"},{"metadata":{},"cell_type":"markdown","source":"We will do very basic analysis,that is character level,word level and sentence level analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrue_len=df[df['labels']==1]['article_content'].str.len()\nax1.hist(true_len,color='green')\nax1.set_title('Real News')\nfake_len=df[df['labels']==0]['article_content'].str.len()\nax2.hist(fake_len,color='red')\nax2.set_title('Fake News')\nfig.suptitle('Characters in an article')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of words in a article"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrue_len=df[df['labels']==1]['article_content'].str.split().map(lambda x: len(x))\nax1.hist(true_len,color='green')\nax1.set_title('Real News')\nfake_len=df[df['labels']==0]['article_content'].str.split().map(lambda x: len(x))\nax2.hist(fake_len,color='red')\nax2.set_title('Fake News')\nfig.suptitle('Words in an article')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average word length in a article"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=df[df['labels']==1]['article_content'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='green')\nax1.set_title('Real')\nword=df[df['labels']==0]['article_content'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Fake')\nfig.suptitle('Average word length in each article')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Most common words in Real news**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mfreq = pd.Series(' '.join(df[df['labels']==1]['article_content']).split()).value_counts()[:25]\nmfreq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Exploration****"},{"metadata":{},"cell_type":"markdown","source":"We will now visualize the text  to get insights on the most frequently used words."},{"metadata":{},"cell_type":"markdown","source":"We will use TfidfVectorizer for some text pre-processing like removing stop words and to get the vocabularies in our articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(use_idf=True,max_df=0.40,min_df=0.1,stop_words='english').fit(df[df['labels']==1]['article_content'])\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(vect.vocabulary_.keys())[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wordcloud for words in real news after some cleaning and deleting stop words using TfidfVectorizer "},{"metadata":{"trusted":true},"cell_type":"code","source":"true_tfidf=list(vect.vocabulary_.keys())\nwordcloud = WordCloud(width=1600, height=800).generate(str(true_tfidf))\n#  plot word cloud image.\n\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's see the most common words in fake news"},{"metadata":{"trusted":true},"cell_type":"code","source":"mfreq = pd.Series(' '.join(df[df['labels']==0]['article_content']).split()).value_counts()[:25]\nmfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(use_idf=True,max_df=0.40,min_df=0.1,stop_words='english').fit(df[df['labels']==0]['article_content'])\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wordcloud for words in fake news after some cleaning and deleting stop words using TfidfVectorizer "},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_tfidf=list(vect.vocabulary_.keys())\nwordcloud = WordCloud(width=1600, height=800).generate(str(fake_tfidf))\n#  plot word cloud image.\n\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see down wordclod for the whole articles(real and fake) from article_content"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Intialize TfidfVectorizer\ntfidf_vect=TfidfVectorizer(stop_words='english',max_df=0.4,min_df=0.1).fit(df['article_content'])\nlen(tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_tfidf=list(tfidf_vect.vocabulary_.keys())\nwordcloud = WordCloud(width=1600, height=800).generate(str(txt_tfidf))\n#  plot word cloud image.\n\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classifier: Features and Design**"},{"metadata":{},"cell_type":"markdown","source":"* To train supervised classifiers, we first transformed the “article_content” into a vector of numbers. We explored vector representations such as TF-IDF weighted vectors.\n\n* After having this vector representations of the text we can train supervised classifiers to train unseen “article_content” and predict the “labels”(0/1) on which they fall.\n\nAfter all the above data transformation, now that we have all the features and labels, it is time to train the classifiers. There are a number of algorithms we can use for this type of problem.\n\nNaive Bayes Classifier: the one most suitable for word counts is the multinomial variant:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n\nfeatures = tfidf.fit_transform(df.article_content).toarray()\nlabels = df.labels\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes Classifier: the one most suitable for word counts is the multinomial variant:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['article_content'], df['labels'], random_state = 0)\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nclf = MultinomialNB().fit(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try predicting on recent news ???"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.predict(count_vect.transform([\"The Syrian army has taken control of a strategic northwestern crossroads town, its latest gain in a weeks-long offensive against the country's last major rebel bastion.\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome!!!!!!!!!!!!!!!!!!!! That's correct"},{"metadata":{},"cell_type":"markdown","source":"**Model Selection**\n\n\nWe are now ready to experiment with different machine learning models, evaluate their accuracy and find the source of any potential issues.\n\nWe will benchmark the following four models:\n\n* Logistic Regression \n* (Multinomial) Naive Bayes \n* Linear Support Vector Machine \n* Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0)]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\n\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_df.groupby('model_name').accuracy.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**"},{"metadata":{},"cell_type":"markdown","source":"\nThe accuracy of these models on predicting is low.in this case I think it's better to go and collect more data rathar than trying another model to get better accuracy"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}