{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%cd /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n'''\nimport sys\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\ndef get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print('==> Computing mean and std..')\n    for inputs, _ in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode='fan_out')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\nTERM_WIDTH = 100\nTOTAL_BAR_LENGTH = 20.\nlast_time = time.time()\nbegin_time = last_time\n\n\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(TERM_WIDTH-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(TERM_WIDTH-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Dataset 1 ==> airplane automobile dog ship truck\n# Dataset 2 ==> the rest\n!ls cinic10/train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nimport argparse\nimport os\nfrom datetime import datetime\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom skimage import io, transform\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader \nimport random\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass FilteredDataset(datasets.ImageFolder):\n    def __init__(self, root, wanted_labels=[], transform=None, target_transform=None, is_valid_file=None):\n        self.wanted_labels=wanted_labels\n        self.root= root\n        datasets.ImageFolder.__init__(self,root= root, transform=transform, target_transform=target_transform,  is_valid_file=is_valid_file)\n    def _find_classes(self, root ):\n        \n        classes_temp = [d.name for d in os.scandir(root) if d.is_dir()]\n        classes_temp.sort()\n        \n        classes=[]\n        class_to_idx={}\n        class_to_idx_temp = {cls_name: i for i, cls_name in enumerate(classes_temp)}\n        \n        for i in self.wanted_labels:\n            for cls in classes_temp:\n                if class_to_idx_temp[str(cls)]==i:\n                    classes.append(cls)\n                    class_to_idx.update({cls:i})\n                    \n              \n            \n        return classes, class_to_idx\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindir = os.path.join('cinic10/', 'train')\nvalidatedir = os.path.join('cinic10', 'valid')\ntestdir = os.path.join('cinic10', 'test')\ncinic_mean = [0.47889522, 0.47227842, 0.43047404]\ncinic_std = [0.24205776, 0.23828046, 0.25874835]\nnormalize = transforms.Normalize(mean=cinic_mean, std=cinic_std)\n\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=cinic_mean, std=cinic_std)\n])\ndef target_trans(x):\n    return x-5\n#trainset_primary = datasets.ImageFolder(root=traindir, transform=train_transform)\ntrainset_1 = FilteredDataset(root=traindir, wanted_labels=[0, 1, 2, 3, 4],  transform=train_transform)\ntrainset_2 = FilteredDataset(root=traindir, wanted_labels=[5,6,7,8,9],  transform=train_transform)\ntrain_loader_1=DataLoader(trainset_1, batch_size=64, shuffle= True)\ntrain_loader_2=DataLoader(trainset_2, batch_size=64, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(next(iter(train_loader_1))[0][0].size())\nX,Y=next(iter(train_loader_1))\nprint(Y[0])\n\n\nplt.imshow(X[0].numpy().transpose(1,2,0))\nplt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nshould_buffer = True\n\ncfg = {\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n}\n\n# Both workers and master run a VGG16 but the master's VGG is modified to have only one output value\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n        self.buffer = []\n\n    def forward(self, x):\n        \n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        if should_buffer:\n            self.buffer.append(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n    \n    def return_buffer(self):\n        return self.buffer\n\nclass VGG_MASTER(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG_MASTER, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n        self.classifier1= nn.Linear(10,1)\n        \n        self.buffer = []\n\n    def forward(self, x):\n        \n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        out= self.classifier1(out)\n        if should_buffer:\n            self.buffer.append(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n    \n    def return_buffer(self):\n        return self.buffer\n\ndef vgg16():\n    return VGG('VGG16')\n\ndef vgg16_master():\n    return VGG_MASTER('VGG16')\n\ndef test():\n    net = vgg16()\n    x = torch.randn(2, 3, 32, 32)\n    y = net(x)\n    print(y.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nclass Discriminator(nn.Module):\n  def __init__(self):\n    super(Discriminator, self).__init__()\n    self.conv1 = nn.Conv2d(3,64,4,2,1)    #16*16\n    self.conv2=  nn.Conv2d(64,128,4,2,1)  #8*8\n    self.conv3 = nn.Conv2d(128,256,4,2,1) #4*4\n    self.conv4 = nn.Conv2d(256,1,4,1,0)   #1*1\n    self.drop1 = nn.Dropout(0.3)\n    self.norm1_2d=nn.BatchNorm2d(64)\n    self.norm2_2d=nn.BatchNorm2d(128)\n    self.norm3_2d=nn.BatchNorm2d(256)\n    \n\n  def forward(self,x):\n\n   \n    #Three fully connected Layers\n    \n    #FC1\n    x= self.conv1(x)\n    x= self.norm1_2d(x)\n    x= F.leaky_relu(x,0.2)    \n    \n    #FC2\n    x= self.conv2(x)    \n    x= self.norm2_2d(x)\n    x= F.leaky_relu(x,0.2)\n    x= self.drop1(x)\n\n    #Fc3\n    x= self.conv3(x)\n    x= self.norm3_2d(x)\n    x= F.leaky_relu(x,0.2)\n       \n\n    #FC4\n    x= self.conv4(x)\n    x=x.view(-1)\n    x= torch.sigmoid(x)\n    \n\n\n    return x\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parser = argparse.ArgumentParser(description='PyTorch CINIC10 Training')\nparser.add_argument('--data', metavar='DIR', default='cinic10',\n                    help='path to dataset (default: cinic10)')\nparser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n                    help='number of data loading workers (default: 2)')\nparser.add_argument('--epochs', default=10, type=int, metavar='N',\n                    help='number of total epochs to run')\nparser.add_argument('-b', '--batch-size', default=64, type=int,\n                    metavar='N',\n                    help='mini-batch size (default: 64), this is the total '\n                         'batch size of all GPUs on the current node when '\n                         'using Data Parallel or Distributed Data Parallel')\nparser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n                    metavar='LR', help='initial learning rate', dest='lr')\nparser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n                    help='momentum')\nparser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n                    metavar='W', help='weight decay (default: 1e-4)',\n                    dest='weight_decay')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\n\nargs = parser.parse_args(['--data', 'cinic10'])\nargs.cuda = torch.cuda.is_available()\n\nmodel1 = vgg16().float()\nmodel2 = vgg16().float()\n\nif args.cuda:\n    model1.features = torch.nn.DataParallel(model1.features)\n    model2.features = torch.nn.DataParallel(model2.features)\n    model1.cuda()\n    model2.cuda()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define loss function (criterion), optimizer and learning rate scheduler\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer1 = torch.optim.SGD(model1.parameters(),\n                            lr=args.lr,\n                            momentum=args.momentum,\n                            weight_decay=args.weight_decay)\noptimizer2 = torch.optim.SGD(model2.parameters(),\n                            lr=args.lr,\n                            momentum=args.momentum,\n                            weight_decay=args.weight_decay)\nscheduler1 = CosineAnnealingLR(optimizer=optimizer1, T_max=args.epochs, eta_min=0)\nscheduler2 = CosineAnnealingLR(optimizer=optimizer2, T_max=args.epochs, eta_min=0)\n\n\ndef train1(epoch):\n    should_buffer = False\n    print('\\nEpoch: %d' % epoch)\n    cudnn.benchmark = True\n    model1.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(train_loader_1):\n        \n        \n        if args.cuda:\n            \n            inputs, targets = inputs.cuda(), targets.cuda()\n        optimizer1.zero_grad()\n        outputs = model1(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer1.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(train_loader_1), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\ndef train2(epoch):\n    should_buffer = False\n    print('\\nEpoch: %d' % epoch)\n    cudnn.benchmark = True\n    model2.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(train_loader_2):\n        if args.cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        optimizer2.zero_grad()\n        outputs = model2(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer2.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(train_loader_2), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(0, args.epochs):\n    scheduler1.step()\n    train1(epoch)\n    scheduler2.step()\n    train2(epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=cinic_mean, std=cinic_std)\n])\nepochs_master=100\nmasterset = FilteredDataset(root=validatedir, wanted_labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  transform=master_transform)\n# Our validate set is our trainset for master\nmaster_loader = DataLoader(masterset, batch_size=64, shuffle= False)\n\nmaster_model = Discriminator().cuda()\n\ndef alpha_from_vectors(pred1, pred2, real):\n    p1 = F.softmax(pred1).cpu().detach().numpy()\n    \n    p2 = F.softmax(pred2).cpu().detach().numpy()\n    r = np.eye(10)[real.cpu().detach().numpy()]\n    \n    ratio = np.sum((p2 - r)*(p2-p1),axis=1)*1.0 / np.sum((p1-r)*(p2-p1), axis=1).ravel()\n    \n    # alpha / (1-alpha) = ratio\n    # alpha = ratio / (1+ratio)\n    alpha = ratio / (1+ratio)\n    alpha= (alpha>0.5).astype(int)\n    return alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ninputs1 = []\noutputs1 = []\ntargets1 = []\ninputs2 = []\noutputs2 = []\ntargets2 = []\n\ndef test1():\n    model1.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(master_loader):\n            if args.cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            outputs = model1(inputs)\n            \n            inputs1.append(inputs)\n            outputs1.append(outputs)\n            targets1.append(targets)\n            \n            loss1 = criterion(outputs, targets)\n\n            test_loss += loss1.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(master_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\ndef test2():\n    model2.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(master_loader):\n            if args.cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            outputs = model2(inputs)\n            \n            inputs2.append(inputs)\n            outputs2.append(outputs)\n            targets2.append(targets)\n            \n            loss2 = criterion(outputs, targets)\n\n            test_loss += loss2.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(master_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n            \n\n\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = []\ndef fill_alphas():\n    # targets1 == targets2 and inputs1 == inputs2\n    for i in range(len(targets1)):\n        alphas.append(alpha_from_vectors(outputs1[i], outputs2[i], targets1[i]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1()\ntest2()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_alphas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(alphas[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master_criterion = torch.nn.BCELoss()\n\nmaster_optimizer = torch.optim.Adam(master_model.parameters(),\n                            lr=1e-03)        \n\nmaster_scheduler = CosineAnnealingLR(optimizer=master_optimizer, T_max=epochs_master, eta_min=0)\n\ndef train_master(epoch):\n    should_buffer = False\n    print('\\nEpoch: %d' % epoch)\n    cudnn.benchmark = True\n    master_model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    perm=np.random.permutation(len(inputs1))\n    for i in range(len(inputs1)):\n        ind=perm[i]\n        inputs = inputs1[ind]\n        targets = torch.from_numpy(alphas[ind]).double()\n        if args.cuda:\n            inputs, targets = inputs.cuda(), targets.double().cuda()\n        master_optimizer.zero_grad()\n        outputs = master_model(inputs).double()\n        #outputs= torch.sigmoid(outputs).double()\n        #print(outputs)\n        #print(outputs.type())\n        loss = master_criterion(outputs, targets)\n        #print(loss.type())\n        loss.backward()\n        master_optimizer.step()\n\n        train_loss += loss.item()\n        \n\n        progress_bar(i, len(inputs1), 'Loss: %.3f '\n                     % (train_loss/(i+1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(0, epochs_master):\n    master_scheduler.step()\n    train_master(epoch)\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testset = FilteredDataset(root=testdir, wanted_labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  transform=master_transform)\n# Our validate set is our trainset for master\ntest_loader = DataLoader(testset, batch_size=64, shuffle= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef test_model():\n    model1.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    master_model.eval()\n    model2.eval()\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(test_loader):\n            if args.cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            beta=master_model(inputs)\n            beta = beta.view(-1,1)\n            outputs=beta*model1(inputs)+(1-beta)*model2(inputs)\n            \n           \n            \n            \n            _, predicted = outputs.max(1)\n            #print(beta, outputs, predicted)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(master_loader), ' Acc: %.3f%% (%d/%d)'\n                         % (100.*correct/total, correct, total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}