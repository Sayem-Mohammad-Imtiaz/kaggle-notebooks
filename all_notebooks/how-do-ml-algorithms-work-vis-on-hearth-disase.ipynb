{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Disease\nContent:\n1. [Introduction](#1)\n1.  [Load and check data](#2)\n1. [Check Missing Data with missingno](#3)\n1. [Visualization](#4)\n1. [Machine learning- Manuel And SckitLearns](#5)\n    * [All Accuracy with Sklearn](#6)\n    * [Descriptions of our models](#7)\n        * [Light GBM(lgbm)](#8)\n        * [Logistic Regression](#9)\n        * [K-Nearest Neighbour (KNN) Classification](#10)\n        * [Random Forest Classifier](#11)\n        * [XGBoost Classifier](#12)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"1\"></a>\n# Introduction\nAccording to the columns, we have a data set that shows whether individuals have heart disease. First we will process and visualize the data by making it meaningful, then we will predict the individuals who have heart disease by using the Machine Learning algorithms,\n\n\n**Good reading!**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport missingno as msno # check missing value\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2\"></a>\n# Load And Check Data\nLoad the data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Check a head","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 11 Columns;\n* Age: Person's Age\n* Sex: The person's Sex (1= Male 0= Female)\n* cp: Chest pain degrees(1: typical angina, 2: atypical angina, 3: non-anginal pain,  4: asymptomatic)\n* testbps: Resting Blood pressure(mmHg)\n* Chol:  person's cholesterol in mg/dl\n* fbs:  person's  blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n* restecg: Resting electrocardiographic (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* thalach: person's maximum heart rate \n* exang: Exercise induced angina (1 = yes; 0 = no)\n* oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n* slope: the slope of the peak exercise ST segment ( 1: upsloping,  2: flat,  3: downsloping)\n* ca: Large number of vessels (0-3)\n* thal:Blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* target: Heart disease (0 = no, 1 = yes)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a>\n# Check Missing Data with missingno","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we haven't missing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a>\n# Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import The libs.\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.offline as py\nfrom bokeh.io import output_file,show,output_notebook,push_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource,HoverTool,CategoricalColorMapper\nfrom bokeh.layouts import row,column,gridplot\nfrom bokeh.models.widgets import Tabs,Panel\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sex'][data['sex'] == 0] = 'female'\ndata['sex'][data['sex'] == 1] = 'male'\ndata.columns = ['age', 'sex', 'chest_pain_degree', 'blood_pressure', 'cholesterol', 'blood_sugar', 'rest_ecg', 'max_heart_rate',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'long_vessels', 'thalassemia', 'target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,12))\nsns.barplot(x = data['target'].value_counts().index,\n           y=data['target'].value_counts().values)\nplt.xlabel('Target')\nplt.ylabel('Frequency')\nplt.title('Target Barplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x=data[\"chest_pain_degree\"], y=data[\"target\"])])\nfig.update_layout(title_text=\"Chest_Pain_degree-Target\")\npy.iplot(fig, filename=\"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x = data[\"long_vessels\"],y = data[\"target\"])])\nfig.update_layout(title_text =\"vessels-target\")\npy.iplot(fig, filename = \"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x = data['st_depression'],y = data['target'])])\nfig.update_layout(title_text = 'Depression-target')\npy.iplot(fig,filename = 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x = data['max_heart_rate'],y = data['target'])])\nfig.update_layout(title_text = 'max_heart_rate-Target')\npy.iplot(fig,filename = 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x=data[\"sex\"], y=data[\"target\"])])\nfig.update_layout(title_text=\"sex-Target\")\npy.iplot(fig, filename=\"test\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, men have higher rates of heart disease than women.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('target').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n49/5000\n# Age by target visualization in two different ways","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Bar(x=data[\"age\"], y=data[\"target\"])])\nfig.update_layout(title_text=\"Target by age\")\npy.iplot(fig, filename=\"test\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.age,data.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot (Maximum_heart_rate-Age) Colors(target)\nplt.scatter(x=data.age[data.target==1], y=data.max_heart_rate[(data.target==1)], color=\"black\")\nplt.scatter(x=data.age[data.target==0], y=data.max_heart_rate[(data.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = data.hist(figsize =(10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Check the Correlation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data.corr(),annot=True,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can make a lot of sense here, but the most important ones are;\n* heart rate slows as age increases\n* Chest pain decreases as the maximum heart rate achieved increases\n* chest pain increases as the heart rate increases\n* Heart disease decreases as exercise-induced ST depression increases with respect to rest\n* heart disease decreases as vessel size increases\n(etc.)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a>\n# Machine learning-  Manuel And SckitLearns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding \ndata = pd.get_dummies(data,columns = ['sex'],prefix = ['sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the test and train\nx = data.drop('target',axis=1)\ny = data['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['sex_male'] = x['sex_male'].astype(int)\nx['sex_female'] = x['sex_female'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the model\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV , StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n# All methods we use\nknn_model=KNeighborsClassifier().fit(X_train,y_train)\nlr_model=LogisticRegression().fit(X_train,y_train)\nrf_model=RandomForestClassifier().fit(X_train,y_train)\nlgb_model=LGBMClassifier().fit(X_train,y_train)\nxgb_model=XGBClassifier().fit(X_train,y_train)\ngbm_model=GradientBoostingClassifier().fit(X_train,y_train)\n\n\nmodels=[lr_model,rf_model,lgb_model,gbm_model,xgb_model,knn_model]\n\nsc_fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n\nfor model in models:\n    names=model.__class__.__name__\n    accuracy=cross_val_score(model,X_train,y_train,cv=sc_fold)\n    print(\"{}s score:{}\".format(names,accuracy.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id = \"7\"></a>\n # Descriptions of our models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"8\"></a>\n# Light GBM(lgbm):\n# 1.What is Light GBM?\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.\n\nSince it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’.\n\nBefore is a diagrammatic representation by the makers of the Light GBM to explain the difference clearly.\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/11194110/leaf.png)\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/11194227/depth.png)\n\nLeaf wise tree growth in Light GBM.\n\nLeaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max-depth which specifies the depth to which splitting will occur.\n\nBelow, we will see the steps to install Light GBM and run a model using it. We will be comparing the results with XGBOOST results to prove that you should take Light GBM in a ‘LIGHT MANNER’.\n\nLet us look at some of the advantages of Light GBM.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Advantages of Light GBM\n1. **Faster training speed and higher efficiency:** Light GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure.\n2. **Lower memory usage:** Replaces continuous values to discrete bins which result in lower memory usage.\n3. **Better accuracy than any other boosting algorithm:** It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. However, it can sometimes lead to overfitting which can be avoided by setting the max_depth parameter.\n4. **Compatibility with Large Datasets:** It is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST.\nParallel learning supported.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.Important Parameters of light GBM\n* task : default value = train ; options = train , prediction ; Specifies the task we wish to perform which is either train or prediction.\n* application: default=regression, type=enum, options= options :\n* regression : perform regression task\n* binary : Binary classification\n* multiclass: Multiclass Classification\n* lambdarank : lambdarank application\n* data: type=string; training data , LightGBM will train from this data\n* num_iterations: number of boosting iterations to be performed ; default=100; type=int\n* num_leaves : number of leaves in one tree ; default = 31 ; type =int\n* device : default= cpu ; options = gpu,cpu. Device on which we want to train our model. Choose GPU for faster training.\n* max_depth: Specify the max depth to which tree will grow. This parameter is used to deal with overfitting.\n* min_data_in_leaf: Min number of data in one leaf.\n* feature_fraction: default=1 ; specifies the fraction of features to be taken for each iteration\n* bagging_fraction: default=1 ; specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting.\n* min_gain_to_split: default=.1 ; min gain to perform splitting\n* max_bin : max number of bins to bucket the feature values.\n* min_data_in_bin : min number of data in one bin\n* num_threads: default=OpenMP_default, type=int ;Number of threads for Light GBM.\n* label : type=string ; specify the label column\n* categorical_feature : type=string ; specify the categorical features we want to use for training our model\n* num_class: default=1 ; type=int ; used only for multi-class classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using model with sckitlearn\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libary's\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV , StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model=LGBMClassifier().fit(X_train,y_train)\nsc_fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\nnames=lgb_model.__class__.__name__\naccuracy=cross_val_score(lgb_model,X_train,y_train,cv=sc_fold)\nprint(\"{}s score:{}\".format(names,accuracy.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"9\"></a>\n# Logistic Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.\n\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio.\n\nIn a binary logistic regression model, the dependent variable has two levels (categorical). Outputs with more than two values are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model[2]). The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. The coefficients are generally not computed by a closed-form expression, unlike linear least squares; see § Model fitting.[](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression).)\n\nIn Data Science we can also create a model 2 methods; first is manual,second is Sckit learn\nSckit learn is the easiest way to get accuracy (\"less code, more efficient work\")\nlet's use both methods","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.target.values\nx_data = data.drop(['target'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1-First we need the normalize the data\n![](https://beyondbacktesting.files.wordpress.com/2017/07/normalization.png?w=863)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the dataset\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the model (train test split)\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"%70 of our data will be train and %30 will be test data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize\ndef initialize(dimension):\n    \n    weight = np.full((dimension,1),0.01)\n    bias = 0.0\n    return weight,bias","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weight = 0.01\nbias = 0.0","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2-Sigmoid function \n![](https://qph.fs.quoracdn.net/main-qimg-05edc1873d0103e36064862a45566dba)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3-Forward-Backward Propagation \n![](https://miro.medium.com/max/3652/1*FczAiD6e8zWjWupOQkP_-Q.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4-Cost Function \n![](https://miro.medium.com/max/1400/1*tQTcGTLZqnI5rp3JYO_4NA.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5-Gradient Descent\n![](https://miro.medium.com/max/3336/1*UwZgyyfmfO2I5UUfzCPfdw.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def forwardBackward(weight,bias,x_train,y_train):\n    # Forward\n    \n    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) / x_train.shape[1]\n    \n    # Backward\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Update parameters\ndef update(weight,bias,x_train,y_train,learningRate,iteration) :\n    costList = []\n    index = []\n    \n    #for each iteration, update weight and bias values\n    for i in range(iteration):\n        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n        \n        costList.append(cost)\n        index.append(i)\n\n    parameters = {\"weight\": weight,\"bias\": bias}\n    \n    print(\"iteration:\",iteration)\n    print(\"cost:\",cost)\n\n    plt.plot(index,costList)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(weight,bias,x_test):\n    z = np.dot(weight.T,x_test) + bias\n    y_head = sigmoid(z)\n\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):\n    dimension = x_train.shape[0]\n    weight,bias = initialize(dimension)\n    \n    parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)\n\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"Manuel Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train,y_train,x_test,y_test,1,100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"10\"></a>\n# K-Nearest Neighbour (KNN) Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.[](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn?utm_source=adwords_ppc&utm_campaignid=10267161064&utm_adgroupid=102842301792&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=278443377095&utm_targetid=aud-299261629574:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1012783&gclid=Cj0KCQjwgo_5BRDuARIsADDEntQjahjkcs3b6XNbQFjct_fvxTcUVydMr1xVumoUV6aEYt-LRYh5ibQaAsECEALw_wcB)\n\n# 1-How does the KNN algorithm work?\nIn KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the one closest point to P1 and then the label of the nearest point assigned to P1.\n![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/Knn_k1_z96jba.png)\n\nSuppose P1 is the point, for which label needs to predict. First, you find the k closest point to P1 and then classify points by majority vote of its k neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, you find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance. KNN has the following basic steps:\n\n1-Calculate distance\n2-Find closest neighbors\n3-Vote for labels\n![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"11\"></a>\n# Random Forest Classifier :\nIt is an ensemble tree-based learning algorithm. The Random Forest Classifier is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object.\n# Ensemble Algorithm :\nEnsemble algorithms are those which combines more than one algorithms of same or different kind for classifying objects. For example, running prediction over Naive Bayes, SVM and Decision Tree and then taking vote for final consideration of class for test object.\n![](https://miro.medium.com/max/718/0*a8KgF1IINziv7KIQ.png)\n# Types of Random Forest models:\n1. Random Forest Prediction for a classification problem:\nf(x) = majority vote of all predicted classes over B trees\n2. Random Forest Prediction for a regression problem:\nf(x) = sum of all sub-tree predictions divided over B trees\n# An Example of Random Forest Classification :\n![](https://miro.medium.com/max/809/0*edh34CKyDT7sDHgL.png)\n![](https://miro.medium.com/max/743/0*pCV1ZFzLBTJN5NhE.png)\nThe 9 decision tree classifiers shown above can be aggregated into a random forest ensemble which combines their input (on the right). The horizontal and vertical axes of the above decision tree outputs can be thought of as features x1 and x2. At certain values of each feature, the decision tree outputs a classification of “blue”, “green”, “red”, etc.\nThese above results are aggregated, through model votes or averaging, into a single\nensemble model that ends up outperforming any individual decision tree’s output.\n# Features and Advantages of Random Forest :\n1. It is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.\n2. It runs efficiently on large databases.\n3. It can handle thousands of input variables without variable deletion.\n4. It gives estimates of what variables that are important in the classification.\n5. It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n6. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n# Disadvantages of Random Forest :\n1. Random forests have been observed to overfit for some datasets with noisy classification/regression tasks.\n2. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Therefore, the variable importance scores from random forest are not reliable for this type of data.\n[](https://towardsdatascience.com/random-forest-classification-and-its-implementation-d5d840dbead0)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"12\"></a>\n# XGBoost Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"XGBoost is an open source library providing a high-performance implementation of gradient boosted decision trees. An underlying C++ codebase combined with a Python interface sitting on top makes for an extremely powerful yet easy to implement package.\nThe performance of XGBoost is no joke — it’s become the go-to library for winning many Kaggle competitions. Its gradient boosting implementation is second to none and there’s only more to come as the library continues to garner praise.\nIn this post we’re going to go through the basics of the XGBoost library. We’ll start with a practical explanation of how gradient boosting actually works and then go through a Python example of how XGBoost makes it oh-so quick and easy to do it.\n# 1-Boosting Trees\nWith a regular machine learning model, like a decision tree, we’d simply train a single model on our dataset and use that for prediction. We might play around with the parameters for a bit or augment the data, but in the end we are still using a single model. Even if we build an ensemble, all of the models are trained and applied to our data separately.\n**Boosting, on the other hand**, takes a more iterative approach. It’s still technically an ensemble technique in that many models are combined together to perform the final one, but takes a more clever approach.\nRather than training all of the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones. Models are added sequentially until no further improvements can be made.\nThe advantage of this iterative approach is that the new models being added are focused on correcting the mistakes which were caused by other models. In a standard ensemble method where models are trained in isolation, all of the models might simply end up making the same mistakes!\n**Gradient Boosting** specifically is an approach where new models are trained to predict the residuals (i.e errors) of prior models. I’ve outlined the approach in the diagram below.\n![](https://miro.medium.com/max/519/1*A9myadIB_CqJv-EJA-G_bA.png)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Your comments are very valuable to me, I am also new, please comment so that I can learn from you!!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}