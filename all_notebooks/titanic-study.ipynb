{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [speed&direction] (캐글 따라하기) 타이타닉 데이터 분석","metadata":{}},{"cell_type":"markdown","source":"이번 블로그는 다음의 참고자료를 통해 재구성한 것이다.","metadata":{}},{"cell_type":"markdown","source":"- 캐글 코리아 블로그(타이타닉 튜토리얼) : https://kaggle-kr.tistory.com/17#2_6\n- [수비니움의 캐글 따라하기] 타이타닉 : Beginner Ver. : https://www.kaggle.com/subinium/subinium-tutorial-titanic-beginner/data?select=train.csv\n- 인문계공돌이님 : https://bizzengine.tistory.com/178","metadata":{}},{"cell_type":"markdown","source":"- 데이터 분석을 공부하는 초보자로써 캐글에서 유명한 타이타닉 데이터 분석을 하려고 한다.\n- 타이타닉은 역사상 최악의 해난 사고로 영화로도 그 스토리가 제작이 되었다.\n- 이번 블로그에서 타이타닉에 탑승한 사람들의 여러가지 피처를 활용하여, 승선한 사람들의 생존여부를 예측하는 모델을 생성할 것이다.\n- 1912년 당시에는 계급이 있었고, 영화에서도 나왔지만 타이타닉의 선실은 1.2.3 등급의 선실로 나뉘어져 있었으며 구명 보트에 사람을 태울때도 1등급의 선실에 있는 사람을 먼저 태운 후 여자와 아이들을 태웠다. 캐글의 데이터도 이와 같은지 분석을 해보려고 한다.","metadata":{}},{"cell_type":"markdown","source":"#### 시작에 앞서\n\n- 데이터 정보를 최소한으로 살피면서 분석한다.\n- 어려운 메서드나 복잡한 함수는 최대한 피하면서 분석한다.\n- 분석을 위한 필수적인 요소와 순서를 서술한다.\n- 초보자가 쉽게 따라할수 있게 한다.","metadata":{}},{"cell_type":"markdown","source":"#### 1. 라이브러리 불러오기\n\n\n타이타닉 데이터 분석에 앞서 필요한 라이브러리를 불러오자.","metadata":{}},{"cell_type":"code","source":"# 데이터 분석에 관련한 라이브러리 불러오기.\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터 시각화 관련한 라이브러리 불러오기.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid') \n%matplotlib inline ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scikit-Learn의 다양한 머신러닝 모듈 불러오기.\n# 분류 알고리즘 중에서 LogisticRegression, LinearSVC, RandomForestClassifier, KNeighborsClassifier 알고리즘을 사용해 보자.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. 데이터 가져오기\n캐글 뿐만 아니라 데이터 분석에서 가장 많이 사용되는 파일 형식은 csv 파일이다. 코드로 데이터를 읽는 방법은 여러가지 있지만, 그 중에서도 가장 유용한 것은 pd.read_csv로 읽는 방법이다.","metadata":{}},{"cell_type":"code","source":"# 데이터 가져오기\ntrain_df = pd.read_csv(\"../input/titanic-machine-learning-from-disaster/train.csv\")\ntest_df = pd.read_csv(\"../input/titanic-machine-learning-from-disaster/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# head() 메서드를 이용해 앞의 5번째까지의 데이터 확인하기\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위의 결과를 보면 앞의 번호는 큰 의미를 가지지 않는다. 이름과 티켓의 경우에는 복잡하기 때문에 처리하기 어려워 보인다. 데이터의 정보는 info 메서드로 확인할 수 있다. train 데이터와 test 데이터를 확인해보자.","metadata":{}},{"cell_type":"code","source":"train_df.info()\nprint('-'*20)\ntest_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위 결과를 보면 각각의 데이터 개수는 891개, 418개 이고, column은 train 데이터는 12개, test 데이터는 11개이다. train 데이터가 12개인 이유는 survived 즉, 생존 여부도 train 데이터는 알고 있기 때문이다.","metadata":{}},{"cell_type":"markdown","source":"#### 주의해야할 점\n\n- 결측값을 drop할지 아니면 default 값으로 채워넣을 것인가\n- cabin, age, embarked 피처\n\n","metadata":{}},{"cell_type":"markdown","source":"데이터 분석을 위해 필요없다고 생각되는 부분을 지우자. 우리는 복잡한 형태의 PassengerID와 Name, Ticket을 지울 것이다. 사실 이름과 티켓에서 가져올 수 있는 데이터는 없기 때문이다. 하지만 이번 분석에서 결과물은 'PassengerID, 'Survived' 피처가 필요하기 때문에 train 데이터에서만 지우자.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\ntest_df = test_df.drop(['Name','Ticket'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. 각각의 데이터 처리하기","metadata":{}},{"cell_type":"markdown","source":"남은 데이터의 종류는 아래와 같다.\n\n- Pclass\n- Sex\n- Age\n- SibSp\n- Parch\n- Fare\n- Cabin\n- Embarked","metadata":{}},{"cell_type":"markdown","source":"##### 3.1 Pclass\n\nPclass는 서수형 데이터이다. 선실의 1,2,3 등급을 나타낸다. 위에서 확인했을 때 결측값이 없다고 나왔다.\n데이터 확인과 데이터를 변환해보자. 우선 각 unique한 value에 대한 카운팅은 value_counts() 메서드로 확인할 수 있다.","metadata":{}},{"cell_type":"code","source":"train_df['Pclass'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1,2,3은 정수이기 때문에 실수로 바꾸면 되지 않을까 생각할 수 있다. 하지만 1,2,3 등급은 경우에 따라 다를 수 있지만 연속적인 데이터가 아니다. 그리고 각각의 차이 또한 균등하지 않다. 그렇기 때문에 범주형 데이터로 인식하고 인코딩 해야 한다.\n이 데이터는 범주형 데이터이기 때문에 one-hot-encoding을 pd.get_dummies() 메서드로 인코딩하자.","metadata":{}},{"cell_type":"code","source":"pclass_train_dummies = pd.get_dummies(train_df['Pclass'])\npclass_test_dummies = pd.get_dummies(test_df['Pclass'])\n\ntrain_df.drop(['Pclass'], axis=1, inplace=True)\ntest_df.drop(['Pclass'], axis=1, inplace=True)\n\ntrain_df = train_df.join(pclass_train_dummies)\ntest_df = test_df.join(pclass_test_dummies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2 Sex\n\nSex는 성별이다. 남과 여로 나뉘기 때문에 이 또한 one-hot-encoding을 하자.\n","metadata":{}},{"cell_type":"code","source":"sex_train_dummies = pd.get_dummies(train_df['Sex'])\nsex_test_dummies = pd.get_dummies(test_df['Sex'])\n\nsex_train_dummies.columns = ['Female', 'Male']\nsex_test_dummies.columns = ['Female', 'Male']\n\ntrain_df.drop(['Sex'], axis=1, inplace=True)\ntest_df.drop(['Sex'], axis=1, inplace=True)\n\ntrain_df = train_df.join(sex_train_dummies)\ntest_df = test_df.join(sex_test_dummies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.3 Age\n\nAge는 나이이다. 나이는 연속형 데이터이기 때문에 큰 처리가 필요없다. 하지만 일부 결측값이 존재한다. 이를 채울 수 있는 방법은 아래 4가지가 있다.","metadata":{}},{"cell_type":"markdown","source":"- 랜덤값\n- 평균값\n- 중간값\n- drop","metadata":{}},{"cell_type":"markdown","source":"이번에는 우선 평균값으로 대체하자. 데이터의 통일성을 가지기 위해 train 데이터셋의 평균값으로 train,test 데이터셋을 채워보자.","metadata":{}},{"cell_type":"code","source":"train_df[\"Age\"].fillna(train_df[\"Age\"].mean() , inplace=True)\ntest_df[\"Age\"].fillna(train_df[\"Age\"].mean() , inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.4 Sibsp & Panch\n\n형제 자매와 부모님은 가족으로 함께 처리할 수 있다. 하지만 바꿀 필요는 없다.","metadata":{}},{"cell_type":"markdown","source":"##### 3.5 Fare \n\nFare는 탑승료이다. test 데이터셋에 1개의 데이터가 이상하게 비어있다. 이 부분은 fillna 메서드로 채울 것이다. 무임 승선이라 생각하고 0으로 입력할 것이다.","metadata":{}},{"cell_type":"code","source":"test_df[\"Fare\"].fillna(0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.6 cabin\n\nCabin은 객실을 뜻한다. 결측값이 너무 많기 때문에 drop하자.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.7 Embarked\n\nEmabarked는 탑승하는 항구를 뜻한다. 우선 데이터를 확인해 보자.","metadata":{}},{"cell_type":"code","source":"train_df['Embarked'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Embarked'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위 결과 S가 대다수이고 일부 결측값이 존재한다. 빈 부분은 S로 채우고 시작하자.","metadata":{}},{"cell_type":"code","source":"train_df['Embarked'].fillna('S', inplace=True)\ntest_df['Embarked'].fillna('S', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embarked_train_dummies = pd.get_dummies(train_df['Embarked'])\nembarked_test_dummies = pd.get_dummies(test_df['Embarked'])\n\nembarked_train_dummies.columns = ['S', 'C', 'Q']\nembarked_test_dummies.columns = ['S', 'C', 'Q']\n\ntrain_df.drop(['Embarked'], axis=1, inplace=True)\ntest_df.drop(['Embarked'], axis=1, inplace=True)\n\ntrain_df = train_df.join(embarked_train_dummies)\ntest_df = test_df.join(embarked_test_dummies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. 데이터 나누기\n\n이제 학습용 데이터를 위해 데이터를 나누어야 한다.\nPassenger Id와 Survived 형태로 아래와 같이 데이터를 나눠준다.","metadata":{}},{"cell_type":"code","source":"X_train = train_df.drop(\"Survived\",axis=1)\nY_train = train_df[\"Survived\"]\nX_test = test_df.drop(\"PassengerId\",axis=1).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. 머신러닝 알고리즘 적용하기\n\n\n이제 LogisticRegression, LinearSVC, RandomForestClassifier, KNeighborsClassifier 알고리즘을 각각 적용해 보자.","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nlogreg.score(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVC\n\nsvc = SVC()\n\nsvc.fit(X_train, Y_train)\n\nY_pred = svc.predict(X_test)\n\nsvc.score(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forests\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train, Y_train)\n\nY_pred = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nknn.score(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"랜덤 포레스트가 가장 좋은 결과를 내는 것을 알 수 있다.","metadata":{}},{"cell_type":"markdown","source":"#### 7. 제출용 파일 만들기","metadata":{}},{"cell_type":"code","source":"# Random Forests\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}