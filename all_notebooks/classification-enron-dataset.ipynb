{"cells":[{"metadata":{},"cell_type":"markdown","source":"### The dataset contains two categories of emails already classified for us - spam and ham. In this notebook, we will explore Classification algorithms and in the end, do a cross validation to choose the best model."},{"metadata":{},"cell_type":"markdown","source":"### Importing Relevant Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading and Cleaning Data\n\nWe are not interested in two columns - Unnamed:0, and label_num. So we'll be dropping them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/spam_ham_dataset.csv\", header=0)\ndata = data.drop('Unnamed: 0', axis=1)\ndata = data.drop('label_num', axis=1)\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline\n\nWe'll create three pipelines to look at three models as below: \n* pipeline to cater CountVectorization and MultinomialNaiveBayes\n* pipeline to cater TF-IDF and LogisticRegression\n* pipeline to cater CountVectoriztion and ComplementNaiveBayes\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('counts', CountVectorizer(ngram_range=(1,2))),\n    ('nb', MultinomialNB())\n])\n\npipeline1 = Pipeline([\n    ('tfid', TfidfVectorizer()),\n    ('lr', LogisticRegression())\n])\n\npipeline2 = Pipeline([\n    ('counts', CountVectorizer(ngram_range=(1,2))),\n    ('cnb', ComplementNB())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data['text'], data['label'],test_size=.20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Fit \n\n#### Pipeline 1 - CountVectorization and MultinomialNaiveBayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, pipeline.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, pipeline.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pipeline2 - TF-IDF and LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline1.fit(x_train, y_train)\nprint(classification_report(y_test, pipeline1.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, pipeline1.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pipeline 3 - CountVectoriztion and ComplementNaiveBayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline2.fit(x_train, y_train)\nprint(classification_report(y_test, pipeline2.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, pipeline2.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Cross Validation: K-Fold Validation to pick the best model."},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold = KFold(n_splits=6)\nscores = []\nconfusion = np.array([[0, 0], [0, 0]])\nfor train_indices, test_indices in k_fold.split(data):\n    train_text = data.iloc[train_indices]['text'].values\n    train_y = data.iloc[train_indices]['label'].values\n\n    test_text = data.iloc[test_indices]['text'].values\n    test_y = data.iloc[test_indices]['label'].values\n\n    pipeline.fit(train_text, train_y)\n    predictions = pipeline.predict(test_text)\n\n    confusion += confusion_matrix(test_y, predictions)\n    score = f1_score(test_y, predictions, pos_label='spam')\n    scores.append(score)\n\nprint('Total emails classified:', len(data))\nprint('Score:', sum(scores)/len(scores))\nprint('Confusion matrix:')\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold = KFold(n_splits=6)\nscores = []\nconfusion = np.array([[0, 0], [0, 0]])\nfor train_indices, test_indices in k_fold.split(data):\n    train_text = data.iloc[train_indices]['text'].values\n    train_y = data.iloc[train_indices]['label'].values\n\n    test_text = data.iloc[test_indices]['text'].values\n    test_y = data.iloc[test_indices]['label'].values\n\n    pipeline1.fit(train_text, train_y)\n    predictions = pipeline1.predict(test_text)\n\n    confusion += confusion_matrix(test_y, predictions)\n    score = f1_score(test_y, predictions, pos_label='spam')\n    scores.append(score)\n\nprint('Total emails classified:', len(data))\nprint('Score:', sum(scores)/len(scores))\nprint('Confusion matrix:')\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold = KFold(n_splits=6)\nscores = []\nconfusion = np.array([[0, 0], [0, 0]])\nfor train_indices, test_indices in k_fold.split(data):\n    train_text = data.iloc[train_indices]['text'].values\n    train_y = data.iloc[train_indices]['label'].values\n\n    test_text = data.iloc[test_indices]['text'].values\n    test_y = data.iloc[test_indices]['label'].values\n\n    pipeline2.fit(train_text, train_y)\n    predictions = pipeline2.predict(test_text)\n\n    confusion += confusion_matrix(test_y, predictions)\n    score = f1_score(test_y, predictions, pos_label='spam')\n    scores.append(score)\n\nprint('Total emails classified:', len(data))\nprint('Score:', sum(scores)/len(scores))\nprint('Confusion matrix:')\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This completes this notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}