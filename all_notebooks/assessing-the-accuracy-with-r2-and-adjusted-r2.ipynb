{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Assess the accuracy of a model with R2 and Adjusted R2 and also understand the difference between them\nWe will predict balance in the credit dataset which is a continuous variable. We will use linear regression and then understand how R2 and adjusted R2 differs. This will help to know why adjusted R2 is best suited to select a model with set of features.\n\n### R2:\n* The R2 always increases as more variables are added.\n* The model containing all of the predictors will always the largest R2, since these quantities are related to the training error. Instead, we wish to choose a model with a low test error. R2 is not suitable for selecting the best model among a collection of models with different numbers of predictors.\n\nR2 = 1-RSS/TSS\n\nwhere TSS = sum(yi − yhat)^2 is the total sum of squares for the response,\nand TSS = sum(yi − ybar)^2 is the total sum of squares,\n\n### Adjusted R2:\n* The intuition behind the adjusted R2 is that once all of the correct variables have been included in the model, adding additional noise variables\n* A large value of adjusted R2 indicates a model with a small test error. The model with the largest adjusted R2 will have only correct variables and no noise variables. Unlike the R2 statistic, the adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model.\n\nAdj R2 = 1-(1-R2)*(n-1)/(n-p-1)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/ISLR-Auto/Credit.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns='Unnamed: 0', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 5))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpretation\n* There are several catgorical features like Sex, Student, Married, Gender\n* Balance is correlated with limit, rating, income\n* There is also multicollinearity. for example, limit and rating are correlated.\n* Valnce is not normally distributed , it is skewed to the left."},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.loc[:, 'Income':'Ethnicity']\ny = df.loc[:, 'Balance']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FEATURES = ['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education']\nCAT_FEATURES = ['Gender', 'Student', 'Married', 'Ethnicity']\n\nnum_pipe = Pipeline(steps=[\n    ('scale', StandardScaler()),   \n])\ncat_pipe = Pipeline(steps=[    \n    ('encode', OneHotEncoder(drop='first')),   \n    ('scale', StandardScaler(with_mean=False)),\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipe, NUM_FEATURES),\n    ('cat', cat_pipe, CAT_FEATURES),\n], remainder='drop')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature or subset selection\nWe will also use cross validation to find R2 and Adjusted R2 scores\nWe will use sklearn's SelectKBest to find the optimum number of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"r2scores=[]\nadjustedr2 = []\nfeature_names=[]\nfor i in range(1, 10):   \n    reduce_dim_pipe = Pipeline(steps=[\n        ('preprocess', preprocessor),\n        ('reduce_dim', SelectKBest(k=i, score_func=f_regression)),       \n    ])\n    \n    pipeline = Pipeline(steps=[\n        ('reduce_dim_pipe', reduce_dim_pipe),       \n        ('regress', LinearRegression())\n    ])\n    \n    #calculate cross validated R2\n    R2 = cross_val_score(pipeline, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \n    r2scores.append(R2)\n        \n    #calculate Adj R2\n    n= len(X_train)\n    p = i #len(X.columns)\n    adj_R2 = 1- ((1-R2) * (n-1)/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)/(n-p-1)\n#     print(r2, adjustedr2)\n    adjustedr2.append(adj_R2)\n    \n    reduce_dim_pipe.fit(X=X_train, y=y_train)\n    # Get columns to keep    \n    cols = reduce_dim_pipe.named_steps['reduce_dim'].get_support(indices=True)\n    # Create new dataframe with only desired columns\n#     print(cols)\n    features_df_new = X_train.iloc[:, cols]\n    best_features = list(features_df_new.columns)\n#     print(best_features)\n    feature_names.append(best_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\nscoring_df['feature_names'] = feature_names\nscoring_df['features'] = range(1, 10)\nscoring_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpretation\n* There is almost always a increase in the value of R2 score with addition of new features. On the other hand, there is no substantial increase in value of Adj R2 after number of features become 4.\n* We can also see that with addition of limit when number of selected features are 2, the increase in adjusted R2 and R2 are less because limit is not adding much valeu to the model. The reason is that limit and rating are correlated.\n* The increase in value of Adj R2 is less compared to R2 when number of features increase from 4 to 5. The Adj R2 remains constant or reduces with addition of every new feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 6))\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\n# https://stackoverflow.com/questions/52308749/how-do-i-create-a-multiline-plot-using-seaborn\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\nax.set_xlabel('No of features')\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\nax.set_title('Plot between number of features and R2/Adj R2 scores')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpretation\n* The most important feature is 'rating'\n* We can see that after 4-5 features, there is no improvement in R2 scores. \n* We should not add more features to our model and that can add additional noise and can cause overfitting."},{"metadata":{},"cell_type":"markdown","source":"### References\n* https://datascience.stackexchange.com/questions/14693/what-is-the-difference-of-r-squared-and-adjusted-r-squared\n* https://www.listendata.com/2014/08/adjusted-r-squared.html\n* https://thestatsgeek.com/2013/10/28/r-squared-and-adjusted-r-squared/\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}