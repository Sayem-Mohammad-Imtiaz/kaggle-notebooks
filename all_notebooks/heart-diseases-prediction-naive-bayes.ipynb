{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Heart Diseases (Naive Bayes Algorithm )\n\nusing a several  machine learning models to see which Algorithm is better to be sure if the patient is infected(have heart Diseases) or not infected (not heart Diseases)","metadata":{}},{"cell_type":"markdown","source":"There some feature should be understand well to be sure you will get good accurcy to predict if the patient have or not have heart diseases\nthe used coulmns in this dataset is 14 of them are used in published experiments there is \n1. age\n2. sex\n3. chest pain type (4 values)\n4. resting blood pressure\n5. serum cholestoral in mg/dl\n6. fasting blood sugar > 120 mg/dl\n7. resting electrocardiographic results (values 0,1,2)\n8. maximum heart rate achieved\n9. exercise induced angina\n10. oldpeak = ST depression induced by exercise relative to rest\n11. the slope of the peak exercise ST segment\n12. number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target","metadata":{}},{"cell_type":"markdown","source":"# Data Preparation and Cleaning","metadata":{}},{"cell_type":"markdown","source":"# Importing required libraries/packages","metadata":{}},{"cell_type":"code","source":"#Importing required libraries/packages\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport pandas as pd                # this using in data processing and using to import  file of Data and read it  \nimport numpy as np                 # this is for linear elgebra\nimport seaborn as sns              # using for plotting \nimport matplotlib.pyplot as plt    #using in visualisation  data \nimport plotly.express as px        # using in plots\nimport os\n# this library using to help split and show an operation in data \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n# Algoritm models of machine learning classification  that will using in this application \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import Data","metadata":{}},{"cell_type":"code","source":" heart= pd.read_csv(\"../input/heart-disease-uci/heart.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to show some of dataset \nheart.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart. info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to show the feature name of the columns \nheart.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this step used to  get some of mathimatic operation like  the mean,STD ,min ,max and other information about the data\nheart.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this step to see if there missing data or not \nheart.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this step to fine if there duplicated data or not \nheart.drop_duplicates(subset=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization Data and showing some figures","metadata":{}},{"cell_type":"code","source":"# Checking labels distributions  this to show the final result in dataset \n\nsns.set_theme(context = 'paper')\n\nplt.figure(figsize = (10,5))\nsns.countplot(heart['target'])\nplt.title('Class Distributions \\n (0: dignose || 1: not_dignose )', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this step to show the relation between target and age \nplt.scatter(x=heart.age[heart.target==1], y=heart.thalach[(heart.target==1)], c=\"red\", s=60)\nplt.scatter(x=heart.age[heart.target==0], y=heart.thalach[(heart.target==0)], s=60)\nplt.legend([\"Disease\", \"No Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this step using to show the relation between each columns  related with the target in Dataset in figure   \nfor feature in heart.columns[1:]:\n    fig = px.histogram(heart, x = feature, color=\"target\", nbins=60)\n    fig.update_layout(\n        autosize=False,\n        width=1000,\n        height=400,)\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# the most important step to show an  overview on Data","metadata":{}},{"cell_type":"code","source":"\n# Let's plot the numerical faetures (Histograms & Scatterplots)\n\nplt.figure(figsize = (20,15))\nsns.pairplot(heart)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#VISUALIZATIONS\n# Correlation Heatmap  confirms what found with the pairplot\ncor_mat=heart.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nsns.heatmap(cor_mat,annot=True,linewidths=0.5,fmt=\".3f\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train_test_split the data\n# \n","metadata":{}},{"cell_type":"code","source":"X = heart.loc[:,heart.columns!='target']\ny = heart.iloc[:,-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.25,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.count()   # to count no of training data ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.count ()   # to count no of testing Data ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling the data\nFirst model of machine learning  Naive_Bayes_Algorithm \n","metadata":{}},{"cell_type":"code","source":"#Create a Gaussian Classifier\ngnb = GaussianNB()\n# Train the Classifier/fitting the model\ngnb.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  to show the accuracy of model \nnb_model = GaussianNB()\nnb_model.fit(X_train, y_train)\nprint(\"Naive Bayes Model Accuracy: \", nb_model.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to be sure the model is working \nnb_model.predict([[63, 1, 3, 145, 233, 1, 0, 150, 0, 2.3, 0, 0, 1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"that mean the patient  not have heart diseases    but if the result is 1 that mean he have ","metadata":{}},{"cell_type":"code","source":"# Confusion matrix to be sure the data is correct \ny_pred = gnb.predict(X_test)\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True, cmap=\"GnBu\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Second model of machine learning  Random_forest_Algorithm \n# ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest_model = RandomForestClassifier(n_estimators=100)\nforest_model.fit(X_train, y_train)\nprint(\"Random Forest Model Accuracy: \", forest_model.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)\nrf.feature_importances_       #to which feature will more effective in Dat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance = pd.Series(rf.feature_importances_, index=X_train.columns)\n\nplt.figure(figsize = (15,6))\nsns.barplot(feat_importance.nlargest(20),feat_importance.nlargest(20).index)\nplt.title(\"Random Forest Features Importance\", fontsize = 13)\nplt.xlabel('Feature Importance')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thired model Decision Tree Algorithm","metadata":{}},{"cell_type":"code","source":"tree_model = DecisionTreeClassifier()\ntree_model.fit(X_train, y_train)\nprint(\"Decision Tree Model Accuracy: \", tree_model.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fourth model K Nearest Neighbor Algorithm","metadata":{}},{"cell_type":"code","source":"knn_model = KNeighborsClassifier()\nknn_model.fit(X_train, y_train)\nprint(\"K Nearest Neighbor Model Accuracy: \", knn_model.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fifth model Support Vector Machine SVM Algorithm\n","metadata":{}},{"cell_type":"code","source":"svm_model = SVC(gamma='auto')\nsvm_model.fit(X_train, y_train)\nprint(\"Support Vector Machine Model Accuracy: \", svm_model.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">  *  Algorithm              Accuracy \n>  * 1. Naive Bayes          0.881578947368421\n>  * 2. Random forest        0.8421052631578947\n>  * 3. Decision Tree        0.7631578947368421\n>  * 4. K Nearest Neighbor   0.6973684210526315\n>  * 5. SVM                  0.5526315789473685\n \n ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# this model show the better accuracy is 88.1578  for Naive Bayes Algorithm ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}