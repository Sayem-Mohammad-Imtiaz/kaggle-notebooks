{"cells":[{"metadata":{"_uuid":"48829147-ab96-4529-9ff8-0be371d6423f","_cell_guid":"e7d7ab37-0e1e-483a-850a-293b07fa98c9","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nimport glob\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"346f784b-7ef2-43bb-945d-5165a24eceaa","_cell_guid":"35a32a5a-dae8-40fa-ba02-a2a92ccb5bd6","trusted":true},"cell_type":"code","source":"#insert the appropriate path of the dataset of your choice for training\n\nData = pd.read_csv(\"../input/spam-or-not-spam-dataset/spam_or_not_spam.csv\")\nData.head()\nData.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dd0814b-c4d5-4991-a046-3a0990e0f494","_cell_guid":"2c252328-24e7-46a5-ad15-262c4e1ca896","trusted":true},"cell_type":"code","source":"#Functions\n\n#function to clean the string (pre-processing)\ndef  clean_string(str_arg):\n    cleaned_str=re.sub('[^a-z\\s]+',\" \",str_arg,flags=re.IGNORECASE) \n    cleaned_str=re.sub('(\\s+)',\" \",cleaned_str)\n    cleaned_str=cleaned_str.lower() \n    return cleaned_str\n\n\n#add words of email to bow_dicts\ndef add_To_Bag_of_Words(example,dict_index,bow_dicts):\n        if isinstance(example,np.ndarray): example=example[0]\n        for token_word in example.split(): \n            bow_dicts[dict_index][token_word]+=1\n            \n\n#calculate posterior probability for each class\ndef getTestMailProbability(test_example, cats_info):\n    likelihood_prob=np.zeros(classes.shape[0]) #to store probability w.r.t each class\n    for cat in classes: \n        for test_token in test_example.split(): #split the test example and get p of each test word\n            test_token_counts=cats_info[cat][0].get(test_token,0)+1\n            #now get likelihood of this test_token word                              \n            test_token_prob=test_token_counts/float(cats_info[cat][2])                              \n            #remember why taking log? To prevent underflow!\n            likelihood_prob[cat]+=np.log(test_token_prob)\n            \n    post_prob=np.empty(classes.shape[0])\n    for cat in classes:\n        post_prob[cat]=likelihood_prob[cat]+np.log(cats_info[cat][1])\n    return post_prob\n\n#classifying for an example if spam or ham\ndef get_prediction(example, predictions, cats_info):\n        #preprocess the test example the same way we did for training set examples \n        cleaned_example=clean_string(str(example)) \n        #get the posterior probability of every example                                  \n        post_prob=getTestMailProbability(cleaned_example, cats_info) #get prob of this example for both classes\n        predictions.append(classes[np.argmax(post_prob)])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f6a9de3-993a-4ce9-90f5-28178252392d","_cell_guid":"e08a9d25-e5c8-4405-855f-5e8a2098798b","trusted":true},"cell_type":"code","source":"#split data in train data and test data for training and testing\n# taking 20% of the examples from the data set as testing examples\n\ntrain, test= train_test_split(Data, test_size=0.2)\ntrain.columns =[\"Training Examples\",\"Training Labels\"] \ntest.columns =[\"Testing Examples\",\"Testing Labels\"]\n\n#separating the data and labels of both the training and testing examples\ntrain_data=train[\"Training Examples\"]\ntrain_labels=train[\"Training Labels\"]\ntest_data=test[\"Testing Examples\"]\ntest_labels=test[\"Testing Labels\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eabfe574-74a5-4991-965f-e839706b3624","_cell_guid":"720ea409-e844-4dd3-a8ab-1c19b7609a8e","trusted":true},"cell_type":"code","source":"#finding the unique labels\n\nclasses=np.unique(train_labels)\nprint(\"unique labels:\", classes)\n\n#creating bag of words\nbow_dicts=np.array([defaultdict(lambda:0) for index in range(classes.shape[0])])\nif not isinstance(train_data,np.ndarray): train_data=np.array(train_data)\nif not isinstance(train_labels,np.ndarray):train_labels=np.array(train_labels)\n\nfor cat in classes:\n    all_cat_examples=train_data[train_labels==cat]\n    cleaned_examples=[clean_string(str(cat_example)) for cat_example in all_cat_examples]\n    cleaned_examples=pd.DataFrame(data=cleaned_examples)\n    np.apply_along_axis(add_To_Bag_of_Words,1,cleaned_examples,cat,bow_dicts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b23b44a5-f1ff-4ba9-abff-e6494a59c5e4","_cell_guid":"0fd91158-30b4-4ad2-984e-645fb6ef7ed7","trusted":true},"cell_type":"code","source":"prob_classes=np.empty(classes.shape[0])\nall_words=[]\ncat_word_counts=np.empty(classes.shape[0])\n\nfor cat in classes:\n    prob_classes[cat]=np.sum(train_labels==cat)/float(train_labels.shape[0])\n    #Calculating total counts of all the words of each class \n    count=list(bow_dicts[cat].values())\n    cat_word_counts[cat]=np.sum( np.array( list(bow_dicts[cat].values()) ))+1 # |v| is remaining to be added\n    #get all words of this category \n    print(cat_word_counts)\n    all_words+=bow_dicts[cat].keys()\n    \nvocab=np.unique(np.array(all_words))\nvocab_length=vocab.shape[0]\ndenoms=np.array([cat_word_counts[cat]+vocab_length+1 for cat in classes])\ncats_info=[(bow_dicts[cat],prob_classes[cat],denoms[cat]) for cat in classes]                               \ncats_info=np.array(cats_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=[] #to store prediction of each test email\n\nfor example in test_data: \n    get_prediction(example, predictions, cats_info)\n    \npclasses=np.array(predictions)\ntest_acc=np.sum(pclasses==test_labels)/float(test_labels.shape[0]) \nprint (\"Test Set Examples: \",test_labels.shape[0])\nprint (\"Test Set Accuracy: \",test_acc*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def desiredFunction(path_of_test_email_folder, cats_info):\n    total_test_files = len(glob.glob1(path_of_test_email_folder,\"*.txt\"))\n\n    predictions=[] #to store prediction of each test example\n    for i in range(1,total_test_files+1):\n        fpath = path_of_test_email_folder + \"/email\" +str(i) + \".txt\"\n        f=open(fpath, \"r\")\n        if f.mode == 'r':\n            example =f.read()\n        get_prediction(example, predictions, cats_info)\n    pclass=np.array(predictions)   \n\n\n    path_of_test_email_folder = \"/kaggle/working/\"\n    \n    path_of_output_file = path_of_test_email_folder + \"/output.txt\"\n    output_file = open(path_of_output_file, 'w') \n\n    col_name=\"test_email\"+ \"       \" + \"spam(1)/ham(0)\\n\\n\\n\"\n    output_file.write(col_name)\n\n    print(col_name)\n    for i in range(0,len(pclass)):\n        email=\"  email\"+str(i+1)+\"                \"+str(pclass[i])+\"\\n\"\n        output_file.write(email)\n        print(email)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Insert appropriate path of test folder\n# path = \"test\"\npath = \"../input/dataset\"\ndesiredFunction(path, cats_info)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fea2875-1249-4956-96bc-e63aba62b93b","_cell_guid":"c4691c79-54c1-48b6-b776-a1324025e36f","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}