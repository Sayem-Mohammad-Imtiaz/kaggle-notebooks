{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install tensorflow-gpu==1.15.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!git clone https://github.com/tensorflow/models.git\n!apt-get -qq install libprotobuf-java protobuf-compiler\n!protoc ./models/research/object_detection/protos/string_int_label_map.proto --python_out=.\n!cp -R models/research/object_detection/ object_detection/\n!rm -rf models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport glob\n\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = \"../input/object-detection-for-ppe-covid19-dataset/model/frozen_inference_graph.pb\"\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = \"../input/object-detection-for-ppe-covid19-dataset/model/label_map.pbtxt\"\n\n# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"PATH_TO_TEST_IMAGES_DIR =  os.path.join(\"../input\", \"object-detection-for-ppe-covid19-dataset/dataset/test\")\n\nassert os.path.isfile(PATH_TO_CKPT)\nassert os.path.isfile(PATH_TO_LABELS)\nTEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg\"))\nassert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n\n#only on first 10 images\nTEST_IMAGE_PATHS= TEST_IMAGE_PATHS[:10]\nprint(TEST_IMAGE_PATHS)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install tf-slim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd object_detection\n\nimport numpy as np\nimport os\nimport six.moves.urllib as urllib\nimport sys\nimport tarfile\nimport tensorflow as tf\nimport zipfile\n\nfrom collections import defaultdict\nfrom io import StringIO\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nnum_classes=5\n\n# This is needed since the notebook is stored in the object_detection folder.\nsys.path.append(\"..\")\n\nfrom object_detection.utils import ops as utils_ops\n\n\n# This is needed to display the images.\n%matplotlib inline\n\n\nfrom object_detection.utils import label_map_util\n\nfrom object_detection.utils import visualization_utils as vis_util\n\n!cd ..\n\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name='')\n\n\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(\n    label_map, max_num_classes=num_classes, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\n\ndef load_image_into_numpy_array(image):\n    (im_width, im_height) = image.size\n    return np.array(image.getdata()).reshape(\n        (im_height, im_width, 3)).astype(np.uint8)\n\n# Size, in inches, of the output images.\nIMAGE_SIZE = (12, 8)\n\n\ndef run_inference_for_single_image(image, graph):\n    with graph.as_default():\n        with tf.Session() as sess:\n            # Get handles to input and output tensors\n            ops = tf.get_default_graph().get_operations()\n            all_tensor_names = {\n                output.name for op in ops for output in op.outputs}\n            tensor_dict = {}\n            for key in [\n                'num_detections', 'detection_boxes', 'detection_scores',\n                'detection_classes', 'detection_masks'\n            ]:\n                tensor_name = key + ':0'\n                if tensor_name in all_tensor_names:\n                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n                        tensor_name)\n            if 'detection_masks' in tensor_dict:\n                # The following processing is only for single image\n                detection_boxes = tf.squeeze(\n                    tensor_dict['detection_boxes'], [0])\n                detection_masks = tf.squeeze(\n                    tensor_dict['detection_masks'], [0])\n                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n                real_num_detection = tf.cast(\n                    tensor_dict['num_detections'][0], tf.int32)\n                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n                                           real_num_detection, -1])\n                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n                                           real_num_detection, -1, -1])\n                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n                detection_masks_reframed = tf.cast(\n                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n                # Follow the convention by adding back the batch dimension\n                tensor_dict['detection_masks'] = tf.expand_dims(\n                    detection_masks_reframed, 0)\n            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n\n            # Run inference\n            output_dict = sess.run(tensor_dict,\n                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n            # all outputs are float32 numpy arrays, so convert types as appropriate\n            output_dict['num_detections'] = int(\n                output_dict['num_detections'][0])\n            output_dict['detection_classes'] = output_dict[\n                'detection_classes'][0].astype(np.uint8)\n            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n            if 'detection_masks' in output_dict:\n                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n    return output_dict\n\n\nfor image_path in TEST_IMAGE_PATHS:\n    image = Image.open(image_path)\n    # the array based representation of the image will be used later in order to prepare the\n    # result image with boxes and labels on it.\n    image_np = load_image_into_numpy_array(image)\n    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n    image_np_expanded = np.expand_dims(image_np, axis=0)\n    # Actual detection.\n    output_dict = run_inference_for_single_image(image_np, detection_graph)\n    # Visualization of the results of a detection.\n    vis_util.visualize_boxes_and_labels_on_image_array(\n        image_np,\n        output_dict['detection_boxes'],\n        output_dict['detection_classes'],\n        output_dict['detection_scores'],\n        category_index,\n        instance_masks=output_dict.get('detection_masks'),\n        use_normalized_coordinates=True,\n        line_thickness=8)\n    plt.figure(figsize=IMAGE_SIZE)\n    plt.imshow(image_np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}