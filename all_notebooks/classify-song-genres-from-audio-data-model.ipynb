{"cells":[{"metadata":{"_uuid":"b178c0cac221f9e959e0c7cb14464fb0be6f04e9"},"cell_type":"markdown","source":"## 1. Preparing our dataset\n<p><em>These recommendations are so on point! How does this playlist know me so well?</em></p>\n<p><img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_449/img/iphone_music.jpg\" alt=\"Project Image Record\" width=\"600px\"></p>\n<p>Over the past few years, streaming services with huge catalogs have become the primary means through which most people listen to their favorite music. But at the same time, the sheer amount of music on offer can mean users might be a bit overwhelmed when trying to look for newer music that suits their tastes.</p>\n<p>For this reason, streaming services have looked into means of categorizing music to allow for personalized recommendations. One method involves direct analysis of the raw audio information in a given song, scoring the raw data on a variety of metrics. Today, we'll be examining data compiled by a research group known as The Echo Nest. Our goal is to look through this dataset and classify songs as being either 'Hip-Hop' or 'Rock' - all without listening to a single one ourselves. In doing so, we will learn how to clean our data, do some exploratory data visualization, and use feature reduction towards the goal of feeding our data through some simple machine learning algorithms, such as decision trees and logistic regression.</p>\n<p>To begin with, let's load the metadata about our tracks alongside the track metrics compiled by The Echo Nest. A song is about more than its title, artist, and number of listens. We have another dataset that has musical features of each track such as <code>danceability</code> and <code>acousticness</code> on a scale from -1 to 1. These exist in two different files, which are in different formats - CSV and JSON. While CSV is a popular file format for denoting tabular data, JSON is another common file format in which databases often return the results of a given query.</p>\n<p>Let's start by creating two pandas <code>DataFrames</code> out of these files that we can merge so we have features and labels (often also referred to as <code>X</code> and <code>y</code>) for the classification later on.</p>"},{"metadata":{"_uuid":"76a29e0b70276798e27c58c7ef1695f11252262a"},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true,"_uuid":"2e02f76267dc744b7e5dd4808ae031b77b963d0b"},"cell_type":"code","source":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport time\nimport random\n\nrandom.seed(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4028418c38ee6171def765f885879228f5b48614"},"cell_type":"code","source":"# Read in track metadata with genre labels\ntracks = pd.read_csv(\"../input/fma-rock-vs-hiphop.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed44df5f1c24b7b1f12208f3fac1c928655421a1"},"cell_type":"code","source":"tracks.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0db034eac859b7cd5b2b980b9ecde0ee72017bbb"},"cell_type":"code","source":"# Read in track metrics with the features\nechonest_metrics = pd.read_json(\"../input/echonest-metrics.json\", precise_float=True)\nechonest_metrics.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a83044cf9f114034c662e1e07e53810cd5ada7f"},"cell_type":"code","source":"# Merge the relevant columns of tracks and echonest_metrics\necho_tracks = pd.merge(echonest_metrics, tracks[[\"track_id\", \"genre_top\"]], on=\"track_id\")\necho_tracks.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69e5d6c55eacb850bd9621fa61d7f54435b570f0"},"cell_type":"code","source":"# Inspect the resultant dataframe\necho_tracks.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd5640e1226df5301a7c68b1efd882131962cd7a"},"cell_type":"code","source":"display(echo_tracks[\"genre_top\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5b7c98139f2e741201e03d7d583226dd4ad4d50"},"cell_type":"markdown","source":"##  Pairwise relationships between continuous variables\n<p>We typically want to avoid using variables that have strong correlations with each other -- hence avoiding feature redundancy -- for a few reasons:</p>\n<ul>\n<li>To keep the model simple and improve interpretability (with many features, we run the risk of overfitting).</li>\n<li>When our datasets are very large, using fewer features can drastically speed up our computation time.</li>\n</ul>\n<p>To get a sense of whether there are any strongly correlated features in our data, we will use built-in functions in the <code>pandas</code> package.</p>"},{"metadata":{"trusted":true,"_uuid":"0e14c85298e3392fe30a56e19852e709c88fd10a"},"cell_type":"code","source":"# Create a correlation matrix\ncorr_metrics = echo_tracks.corr()\ncorr_metrics.style.background_gradient()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65195537164deab5770b31d806df21c51af8bd7d"},"cell_type":"code","source":"echo_tracks.drop([\"track_id\"], axis=1).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"574e1576898b2618b5c55cd00b733b0545e46fd3"},"cell_type":"code","source":"# Define our features \nfeatures = echo_tracks.drop([\"genre_top\",\"track_id\"], axis=1)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b64971dbe8035754118560d4f70633b4f273469"},"cell_type":"code","source":"# Define our labels\nlabels = echo_tracks[\"genre_top\"]\nlabels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd1bd34cfe04038fce940b6c468e838f1366a625"},"cell_type":"code","source":"# Scale the features and set the values to a new variable\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nfeatures = sc_X.fit_transform(features)\npd.DataFrame(features).head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1705a9b7b20283b6c1c57ccc837a5a31ec66955b"},"cell_type":"code","source":"Cat=echo_tracks.drop([\"track_id\"], axis=1)\nCat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0355b924b0e7083cf1648688d93a67573a93dbec"},"cell_type":"code","source":"Cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35938c35af7e3ad8ed364b0496a6b3b99429b72a"},"cell_type":"code","source":"# Encoding categorical data\n# Encoding the Independent Variable\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nCat['genre_top'] = labelencoder_X.fit_transform(Cat['genre_top'])\nCat.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e11de36ac6f0c18f0def5458434f21c77852975"},"cell_type":"code","source":"pd.DataFrame(Cat['genre_top']).info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22a95555efe53b5823a4a46f9d2012f4d6ad9d9b"},"cell_type":"code","source":"## Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = Cat.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"054ace2711a09c5ab414242dd7bb13bebe882fd3"},"cell_type":"code","source":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f66fae554e98c365640119970f042f5b6a22dda"},"cell_type":"markdown","source":"# Feature Selection "},{"metadata":{"trusted":true,"_uuid":"d6d4072b30519b47f7a26e38312aff67faa9c4df"},"cell_type":"code","source":"#Correlation with Quality with respect to attributes\nCat.corrwith(Cat.genre_top).plot.bar(\n        figsize = (20, 10), title = \"Correlation with quality\", fontsize = 15,\n        rot = 45, grid = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6960e168fad51c5045b9d34593d2c702ff9bc377"},"cell_type":"code","source":"#Assigning and dividing the dataset\nX = Cat.drop('genre_top',axis=1)\ny=Cat['genre_top']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfad616a85cab677486069d6e57416429d285b62"},"cell_type":"code","source":"Cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20aa49f692b54e9b85e7c070d87923acaa9966cc"},"cell_type":"code","source":"features_label = Cat.columns[:9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1d19592de9b2a265cfcdcc59b38266a3478c644"},"cell_type":"code","source":"#Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 0)\nclassifier.fit(X, y)\nimportances = classifier.feature_importances_\nindices = np. argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i],importances[indices[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e838037c042e230877b6fb120fe3294019f4c106"},"cell_type":"code","source":"plt.title('Feature Importances')\nplt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\nplt.xticks(range(X.shape[1]),features_label, rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d57b9b230a3d59d50cf4858605af93a39db48d7e"},"cell_type":"markdown","source":"##  Normalizing the feature data and Model Training\n<p>As mentioned earlier, it can be particularly useful to simplify our models and use as few features as necessary to achieve the best result. Since we didn't find any particular strong correlations between our features, we can instead use a common approach to reduce the number of features called <strong>principal component analysis (PCA)</strong>. </p>\n<p>It is possible that the variance between genres can be explained by just a few features in the dataset. PCA rotates the data along the axis of highest variance, thus allowing us to determine the relative contribution of each feature of our data towards the variance between classes. </p>\n<p>However, since PCA uses the absolute variance of a feature to rotate the data, a feature with a broader range of values will overpower and bias the algorithm relative to the other features. To avoid this, we must first normalize our data. There are a few methods to do this, but a common way is through <em>standardization</em>, such that all features have a mean = 0 and standard deviation = 1 (the resultant is a z-score).</p>"},{"metadata":{"trusted":true,"_uuid":"f4e0056e6c2704a5d2896cd5cd9cbb8059c7061f"},"cell_type":"code","source":"\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97a9820baaeb39b4408b74b16cc24d85e5756369"},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train2 = pd.DataFrame(sc.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec81618d0b5a0b2a67e9fc60a75533069a1dbb1a"},"cell_type":"code","source":"#Using Principal Dimensional Reduction\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None )\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(pd.DataFrame(explained_variance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f505c2cfbc5a8f649d002a3df90058094c45a4e4"},"cell_type":"code","source":"# plot the explained variance using a barplot\nfig, ax = plt.subplots()\nax.bar(range(8), explained_variance)\nax.set_xlabel('Principal Component #')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a18161439dcb6eab1a6c7e9b37e2e2135275e78"},"cell_type":"code","source":"# Import numpy\nimport numpy as np\n\n# Calculate the cumulative explained variance\ncum_exp_variance = np.cumsum(explained_variance)\n\n# Plot the cumulative explained variance and draw a dashed line at 0.90.\nfig, ax = plt.subplots()\nax.plot(range(8), cum_exp_variance)\nax.axhline(y=0.9, linestyle='--')\nn_components = 6\n\n# Perform PCA with the chosen number of components and project data onto components\npca = PCA(n_components, random_state=10)\npca.fit(features)\npca_projection = pca.transform(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73f05d4623fa5e344b6e12e4c95f47cd0644817e"},"cell_type":"code","source":"#### Model Building ####\n\n### Comparing Models\n\n## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l1')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"897df6fe8066069f4b0cbd3d2cfd3647f378628e"},"cell_type":"code","source":"## Randomforest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1fae34554f5b5fae21950943951fb51a37ee2b8"},"cell_type":"code","source":"## Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Decision Tree ', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a3ab2302d403b0ab81c72ff4203cc92a6f97527"},"cell_type":"code","source":"## SVM (Linear)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'linear')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c0bbece6868e647138fb82e3dc9b848a69cf7be"},"cell_type":"code","source":"# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['KNN', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69b918d6343d905a8901250abe1b44d6da17a09f"},"cell_type":"code","source":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59d3122754fa54d434c94fbf2b21d6ee31114064"},"cell_type":"markdown","source":"So the SVM kernal is the highest accuracy model"},{"metadata":{"trusted":true,"_uuid":"ba85a34d097d5d9d14e3c4303f4f372b6180db44"},"cell_type":"code","source":"## K-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X= X_train, y = y_train,\n                             cv = 10)\nprint(\"SVM Classifier Accuracy: %0.2f (+/- %0.2f)\"  % (accuracies.mean(), accuracies.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c90c2193a4ae460b5c241c3b3f841943b73c6726"},"cell_type":"code","source":"\nparameters = {\"C\": [0.1,0.3,1,3,10,30],\n              \n              'gamma': [.001,.01,.1,.3,1],\n             }\n\n     \n\n\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = GridSearchCV(  estimator = classifier, # Make sure classifier points to the SVM RBF model\n                           param_grid = parameters,\n                           scoring = \"accuracy\",\n                           cv = 5,\n                           n_jobs = -1\n                          ,verbose=1)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e201effeed147b3356d0a3031b60943d65c9530"},"cell_type":"code","source":"rf_best_accuracy, rf_best_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffa13b54455001b82359e6635c45b6ab2cc26e76"},"cell_type":"code","source":"# Predicting Test Set\ny_pred = grid_search.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM RBF Grid results', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ca00612cbebdbc53fee2abd1a138be678d2d67a"},"cell_type":"code","source":"# Create the classification report for both models\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d89c6802ebc79600a90ce7e509d5aef0c4be9825"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcd7a9247f9e1a7a87dd708037ebe837f1c0108d"},"cell_type":"code","source":"# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08834d7c9e662622831e036a219c2760207f438f"},"cell_type":"code","source":"#so the model accuracy is 90%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"551839c0fb4176c5d0c3bc1f4e35408c0695e6f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}