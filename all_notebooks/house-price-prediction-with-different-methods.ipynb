{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"### importing libraries\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(font_scale=1.5)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#uploading the data and check its head\ndata= pd.read_csv('../input/kc-housesales-data/kc_house_data.csv')\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the data types of the columns and the number of missing values\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if the zipcode has rapition which lead to it will have effect on the result\ndata['zipcode'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#after looking at the data info, the lat and long and id should be droped as the location is not defined with refrence to city or any other refrence\n# the date should be converted as a month and year columns\n#zipcode should be converted as catagory as so as the condition column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#deleting columns\ndel data['id']; del data['lat'];del data['long']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the year and month of the selling dates\ndata['date']= pd.to_datetime(data['date']) \ndata['year']= (pd.DatetimeIndex(data['date']).year)\ndata['month']= (pd.DatetimeIndex(data['date']).month)\ndel data['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fixing the data types\ndata['year']=data['year'].astype(int)\ndata['month']= data['month'].astype(str)\ndata['zipcode']=data['zipcode'].astype(str)\ndata['condition']=data['condition'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the date of renoved as int type 1 for it has and 0 for nune\n# data['yr_renovated']=(data['yr_renovated']>0).astype(int) \n#or \ndata['yr_renovated'] = data['yr_renovated'].apply(lambda x : 1 if x>0 else 0)\n #this command will convert the numbers to boolen based on the condition and the results are True or False and back to integer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make sure the data is ready \ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature Engineering: it is about creating new columns that may help to find direct relation between the goal and the data\ndata['age']=data['year']-data['yr_built']\ndata['sqft_with_basembent']=data['sqft_above']+data['sqft_basement']\ndata['sqft']=data['sqft_living']+data['sqft_lot']\ndata['sqft15']=data['sqft_living15']+data['sqft_lot15']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the data to X for data given and Y for the price column which is our Target\ny = data.pop('price')\n#convert objects columns to binary columns by getting dummy values for them\nX = pd.get_dummies(data, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply spiliting for the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,  random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply standard scaling. and import cross validation function\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import linear regretion models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying Linear Regression\nmodel = LinearRegression()\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nL_R_S=model.score( X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying Ridge\nmodel = Ridge()\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nRidge_S=model.score( X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying RidgeCV as the model\nmodel = RidgeCV( normalize=True, cv=20)\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nRidge_CV_S=model.score( X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Another lasso try \nmodel = LassoCV(alphas=np.logspace(-50,100, 5), cv=5) \nmodel.fit(X_train, y_train)\nprint('Best alpha:', model.alpha_)\nprint('Training score:', model.score(X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying lasso as the model\nmodel = Lasso()\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nLasso_S=model.score( X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying lassoCV as the model\nmodel = LassoCV(normalize=True,cv=20)\n\nscores = cross_val_score(model, X_train, y_train, cv=20)\n\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel.fit( X_train, y_train)\nprint(\"Training Score:\", model.score( X_train, y_train))\nprint(\"Test Score:\", model.score( X_test, y_test))\nLasso_CV_S=model.score( X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Models=['Linear Regression','Ridge','RidgeCV','Lasso','LassoCV']\nscores=[L_R_S,Ridge_S,Ridge_CV_S,Lasso_S,Lasso_CV_S]\nfig = plt.figure()\nax = fig.add_axes([0,0,3,1])\nax.bar(Models,scores)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the accurecy of all models almost the same, however Ridge and Lasso are slightly better","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply Decision Tree Regressor,\nfrom sklearn.tree import DecisionTreeRegressor\ndtr1 = DecisionTreeRegressor(max_depth=1) # change depth to 5 and see the difference\ndtr2 =DecisionTreeRegressor(max_depth=2) \ndtr3 = DecisionTreeRegressor(max_depth=3)\ndtr4 = DecisionTreeRegressor(max_depth=None) \n\n# fit the 4 models\ndtr1.fit(X_train, y_train)\ndtr2.fit(X_train, y_train)\ndtr3.fit(X_train, y_train)\ndtr4.fit(X_train, y_train)\n\ndtr1_scores = cross_val_score(dtr1,X_train, y_train, cv=20)\ndtr2_scores = cross_val_score(dtr2, X_train, y_train, cv=20)\ndtr3_scores =cross_val_score(dtr3,X_train, y_train, cv=20)\ndtrN_scores =cross_val_score(dtr4, X_train, y_train, cv=20)\nprint('the score of deciton tree for the depth of one to three and None')\nprint (dtr1_scores.mean() ,dtr2_scores.mean(), dtr3_scores.mean() ,dtrN_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"all the results are not as good as the models above","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply KNN modelto the train\nfrom sklearn.neighbors import KNeighborsRegressor\nknr = KNeighborsRegressor(n_neighbors=7)\nknr.fit(X_train, y_train)\nknr_scores = cross_val_score(knr,X_train, y_train, cv=20)\nprint(knr_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# KNeighborsRegressor + evaluation\nfrom sklearn import metrics\nKs =20\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    knn_model = KNeighborsRegressor(n_neighbors = n).fit(X_train,y_train)\n    mean_acc[n-1] = knn_model.score(X_test, y_test)\n    yhat=knn_model.predict(X_test)\n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc \n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()\nprint( \"The best test accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsRegressor(n_neighbors = 6).fit(X_train,y_train)\nprint(\"Training Score:\", knn_model.score( X_train, y_train))\nprint(\"Test Score:\", knn_model.score( X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsRegressor(n_neighbors = n).fit(X_train,y_train)\nyhat=knn_model.predict(X_test)\nprint(len(yhat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(yhat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(yhat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(np.array(y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForest without grid search\nfrom sklearn.ensemble import RandomForestRegressor\nforest_cv = RandomForestRegressor(n_jobs=-1)\nforest_cv.fit(X_train,y_train)\nprint(\"Training Score:\", forest_cv.score( X_train, y_train))\nprint(\"Test Score:\", forest_cv.score( X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Random forest with grid search\nfrom sklearn.model_selection import GridSearchCV\ndef warn(*args, **kwargs): #disable warnings\n    pass\nimport warnings\nwarnings.warn = warn\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 1000]\n}\nrf = RandomForestRegressor()\nclf = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\nclf.fit(X_train, y_train)\nrf = clf.best_estimator_\nrf = rf.fit(X_train, y_train) \nrf.score(X_train,y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoostRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nregr = AdaBoostRegressor(random_state=0, n_estimators=100)\nregr.fit(X_train, y_train)\npredicted_regr = regr.predict(X_train)\nprint(\"Test Score:\", model.score( X_test, y_test))\nprint(regr.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=100)\npca.fit(X_train)\npca_transformed = pca.fit_transform(X_train)\noriginal_pca = pd.DataFrame(data = pca.components_)\noriginal_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_.sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pca(pca):\n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 4))\n    size = len(pca.explained_variance_ratio_)\n    ax1.bar(range(size), pca.explained_variance_ratio_)\n    ax2.plot(range(size), np.cumsum(pca.explained_variance_ratio_), '--')\n    plt.show()\nplot_pca(pca)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}