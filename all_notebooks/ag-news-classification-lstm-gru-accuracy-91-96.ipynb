{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T17:03:03.992075Z","iopub.execute_input":"2021-08-08T17:03:03.992685Z","iopub.status.idle":"2021-08-08T17:03:04.013423Z","shell.execute_reply.started":"2021-08-08T17:03:03.992581Z","shell.execute_reply":"2021-08-08T17:03:04.012282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FILE PATH","metadata":{}},{"cell_type":"code","source":"TRAIN_FILE_PATH = '/kaggle/input/ag-news-classification-dataset/train.csv'\nTEST_FILE_PATH = '/kaggle/input/ag-news-classification-dataset/test.csv'","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:08.203115Z","iopub.execute_input":"2021-08-08T17:03:08.203527Z","iopub.status.idle":"2021-08-08T17:03:08.209231Z","shell.execute_reply.started":"2021-08-08T17:03:08.203494Z","shell.execute_reply":"2021-08-08T17:03:08.206732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a classification problem where we need to classify a news article consisting of title and description into following category : 1-World, 2-Sports, 3-Business, 4-Sci/Tech\nHere we have sequentional data \n\nThis is a sequential problem - \nSince we have the existing data available we can use bidirectional LSTM for this classification problem.","metadata":{}},{"cell_type":"markdown","source":"## Reading the dataframe from the data set.","metadata":{}},{"cell_type":"code","source":"pd1 = pd.read_csv(TRAIN_FILE_PATH)\npd2 = pd.read_csv(TEST_FILE_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:11.353112Z","iopub.execute_input":"2021-08-08T17:03:11.353528Z","iopub.status.idle":"2021-08-08T17:03:12.206501Z","shell.execute_reply.started":"2021-08-08T17:03:11.353495Z","shell.execute_reply":"2021-08-08T17:03:12.205529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd1.head())\nprint(pd2.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:15.397244Z","iopub.execute_input":"2021-08-08T17:03:15.397716Z","iopub.status.idle":"2021-08-08T17:03:15.427445Z","shell.execute_reply.started":"2021-08-08T17:03:15.397672Z","shell.execute_reply":"2021-08-08T17:03:15.426482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd1.columns)\nprint(pd2.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:18.349014Z","iopub.execute_input":"2021-08-08T17:03:18.34939Z","iopub.status.idle":"2021-08-08T17:03:18.355853Z","shell.execute_reply.started":"2021-08-08T17:03:18.349355Z","shell.execute_reply":"2021-08-08T17:03:18.354623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd1.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:20.624505Z","iopub.execute_input":"2021-08-08T17:03:20.62488Z","iopub.status.idle":"2021-08-08T17:03:20.635701Z","shell.execute_reply.started":"2021-08-08T17:03:20.624847Z","shell.execute_reply":"2021-08-08T17:03:20.63434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets analyze data","metadata":{}},{"cell_type":"markdown","source":"#### Installing Sweetviz library","metadata":{}},{"cell_type":"code","source":"! pip install sweetviz","metadata":{"execution":{"iopub.status.busy":"2021-08-08T16:42:59.97158Z","iopub.execute_input":"2021-08-08T16:42:59.971949Z","iopub.status.idle":"2021-08-08T16:43:11.060872Z","shell.execute_reply.started":"2021-08-08T16:42:59.971917Z","shell.execute_reply":"2021-08-08T16:43:11.059981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sweetviz as sv","metadata":{"execution":{"iopub.status.busy":"2021-08-08T16:44:21.777944Z","iopub.execute_input":"2021-08-08T16:44:21.778287Z","iopub.status.idle":"2021-08-08T16:44:22.478731Z","shell.execute_reply.started":"2021-08-08T16:44:21.778252Z","shell.execute_reply":"2021-08-08T16:44:22.477823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = sv.analyze(pd1)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T16:44:42.496648Z","iopub.execute_input":"2021-08-08T16:44:42.497007Z","iopub.status.idle":"2021-08-08T16:44:46.050569Z","shell.execute_reply.started":"2021-08-08T16:44:42.496971Z","shell.execute_reply":"2021-08-08T16:44:46.04985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report.show_notebook()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T16:45:31.315058Z","iopub.execute_input":"2021-08-08T16:45:31.315517Z","iopub.status.idle":"2021-08-08T16:45:31.462701Z","shell.execute_reply.started":"2021-08-08T16:45:31.315488Z","shell.execute_reply":"2021-08-08T16:45:31.462066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd1.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:00:21.818804Z","iopub.execute_input":"2021-08-08T17:00:21.819172Z","iopub.status.idle":"2021-08-08T17:00:21.824736Z","shell.execute_reply.started":"2021-08-08T17:00:21.819138Z","shell.execute_reply":"2021-08-08T17:00:21.823915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Data Pre-Processing ","metadata":{}},{"cell_type":"code","source":"## we can combine title and description together before feeding it to bi directional lstm \n\nX_train  =  pd1['Title']+' '+pd1['Description'] # also removing the class from the training dataset\n\nX_test   =  pd2['Title']+'  '+pd2['Description'] # also removing the class from the training dataset\n\n\ny_train  =   pd1['Class Index'].apply(lambda x: x-1)  # assigning label of train\n\ny_test =    pd2['Class Index'].apply(lambda x: x-1) # assigning lale of test\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:25.259176Z","iopub.execute_input":"2021-08-08T17:03:25.259552Z","iopub.status.idle":"2021-08-08T17:03:25.419223Z","shell.execute_reply.started":"2021-08-08T17:03:25.259519Z","shell.execute_reply":"2021-08-08T17:03:25.41827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Finding the max no of words in a sentence in complete data set \n\nmax_len = X_train.map(lambda x : len(x.split())).max()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:27.475307Z","iopub.execute_input":"2021-08-08T17:03:27.475643Z","iopub.status.idle":"2021-08-08T17:03:27.830563Z","shell.execute_reply.started":"2021-08-08T17:03:27.475614Z","shell.execute_reply":"2021-08-08T17:03:27.829567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.Data Generator ( Shuffling the data  ) \n2.Pipeline ","metadata":{}},{"cell_type":"markdown","source":"## Lets tokenize the text data set.\n\nTokenization is one of the most important step of pre processing while modelling the text dataset ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocabulary_size = 10000 # random value\nembed_size      = 32    # random value \n\ntok = Tokenizer(num_words=vocabulary_size)\ntok.fit_on_texts(X_train.values)\n\n\n# Token \nX_train = tok.texts_to_sequences(X_train)\nX_test  = tok.texts_to_sequences(X_test)\n\n# Now we need to pad all the sequences based on the max value \n\nX_train = pad_sequences(X_train,maxlen=max_len)\nX_test = pad_sequences(X_test,maxlen=max_len)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:30.921946Z","iopub.execute_input":"2021-08-08T17:03:30.922324Z","iopub.status.idle":"2021-08-08T17:03:46.721294Z","shell.execute_reply.started":"2021-08-08T17:03:30.92229Z","shell.execute_reply":"2021-08-08T17:03:46.720282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MODEL","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:03:55.425894Z","iopub.execute_input":"2021-08-08T17:03:55.426246Z","iopub.status.idle":"2021-08-08T17:03:55.431685Z","shell.execute_reply.started":"2021-08-08T17:03:55.426213Z","shell.execute_reply":"2021-08-08T17:03:55.429667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport matplotlib.pyplot as plt\n\n#Text Color\nfrom termcolor import colored\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n#Model Evaluation\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\n\n#Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:04:00.333898Z","iopub.execute_input":"2021-08-08T17:04:00.334253Z","iopub.status.idle":"2021-08-08T17:04:01.107719Z","shell.execute_reply.started":"2021-08-08T17:04:00.334222Z","shell.execute_reply":"2021-08-08T17:04:01.106739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:04:07.289996Z","iopub.execute_input":"2021-08-08T17:04:07.290402Z","iopub.status.idle":"2021-08-08T17:04:07.29416Z","shell.execute_reply.started":"2021-08-08T17:04:07.290366Z","shell.execute_reply":"2021-08-08T17:04:07.293178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary_size = 10000 # random value\nembed_size      = 32  # random value","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:04:09.233735Z","iopub.execute_input":"2021-08-08T17:04:09.234177Z","iopub.status.idle":"2021-08-08T17:04:09.239518Z","shell.execute_reply.started":"2021-08-08T17:04:09.234122Z","shell.execute_reply":"2021-08-08T17:04:09.238301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing a sequential model\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size,embed_size,input_length = max_len)) #input layer is embedding layer\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))              # Bidirectinal LSTM\nmodel.add(Bidirectional(LSTM(64, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())                                         # Flattening layer to reduce everything in a vector form\nmodel.add(Dense(256, activation='relu'))                                                  # Dense layer\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu')) \nmodel.add(Dropout(0.25))                                                # doing regularization in Neural Network\nmodel.add(Dense(64, activation='relu')) \nmodel.add(Dropout(0.25))\nmodel.add(Dense(4, activation='softmax'))                               #  we have 4 labels as output\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T10:19:33.229665Z","iopub.execute_input":"2021-08-08T10:19:33.230082Z","iopub.status.idle":"2021-08-08T10:19:34.044763Z","shell.execute_reply.started":"2021-08-08T10:19:33.230047Z","shell.execute_reply":"2021-08-08T10:19:34.04392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T10:19:38.742545Z","iopub.execute_input":"2021-08-08T10:19:38.742918Z","iopub.status.idle":"2021-08-08T10:19:38.760035Z","shell.execute_reply.started":"2021-08-08T10:19:38.742887Z","shell.execute_reply":"2021-08-08T10:19:38.759042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks = [\n#     EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n#         monitor='val_accuracy',\n#         min_delta=1e-4,\n#         patience=4,\n#         verbose=1\n#     ),\n#     ModelCheckpoint(\n#         filepath='weights.h5',\n#         monitor='val_accuracy', \n#         mode='max', \n#         save_best_only=True,\n#         save_weights_only=True,\n#         verbose=1\n#     )\n# ]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T09:52:20.263617Z","iopub.execute_input":"2021-08-08T09:52:20.264132Z","iopub.status.idle":"2021-08-08T09:52:20.268812Z","shell.execute_reply.started":"2021-08-08T09:52:20.264095Z","shell.execute_reply":"2021-08-08T09:52:20.267902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy']             \n             )","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:04:19.880844Z","iopub.execute_input":"2021-08-08T17:04:19.881295Z","iopub.status.idle":"2021-08-08T17:04:20.36902Z","shell.execute_reply.started":"2021-08-08T17:04:19.881254Z","shell.execute_reply":"2021-08-08T17:04:20.367403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=20, \n#           callbacks=callbacks)\n\nmodel.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T10:20:03.865452Z","iopub.execute_input":"2021-08-08T10:20:03.865808Z","iopub.status.idle":"2021-08-08T10:32:17.291033Z","shell.execute_reply.started":"2021-08-08T10:20:03.865776Z","shell.execute_reply":"2021-08-08T10:32:17.290225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We can observe that we can achieve maximum validation accuracy of 90.49 and training accuracy of 98.16","metadata":{}},{"cell_type":"markdown","source":"## Lets analyze the accuracy using GRU model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import GRU","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:06:10.926325Z","iopub.execute_input":"2021-08-08T17:06:10.926695Z","iopub.status.idle":"2021-08-08T17:06:10.932852Z","shell.execute_reply.started":"2021-08-08T17:06:10.926662Z","shell.execute_reply":"2021-08-08T17:06:10.931725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocabulary_size,embed_size,input_length = max_len)) #input layer is embedding layer\nmodel.add(Bidirectional(GRU(128, return_sequences=True)))              # Bidirectinal LSTM\nmodel.add(Bidirectional(GRU(64, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())                                         # Flattening layer to reduce everything in a vector form\nmodel.add(Dense(256, activation='relu'))                                                  # Dense layer\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu')) \nmodel.add(Dropout(0.25))                                                # doing regularization in Neural Network\nmodel.add(Dense(64, activation='relu')) \nmodel.add(Dropout(0.25))\nmodel.add(Dense(4, activation='softmax'))                               #  we have 4 labels as output","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:06:13.228035Z","iopub.execute_input":"2021-08-08T17:06:13.228431Z","iopub.status.idle":"2021-08-08T17:06:14.135498Z","shell.execute_reply.started":"2021-08-08T17:06:13.228398Z","shell.execute_reply":"2021-08-08T17:06:14.134493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:06:17.907215Z","iopub.execute_input":"2021-08-08T17:06:17.907569Z","iopub.status.idle":"2021-08-08T17:06:17.918354Z","shell.execute_reply.started":"2021-08-08T17:06:17.907538Z","shell.execute_reply":"2021-08-08T17:06:17.915666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = 'rmsprop',\n              metrics = ['accuracy']             \n             )","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:07:15.889966Z","iopub.execute_input":"2021-08-08T17:07:15.890375Z","iopub.status.idle":"2021-08-08T17:07:15.910292Z","shell.execute_reply.started":"2021-08-08T17:07:15.890336Z","shell.execute_reply":"2021-08-08T17:07:15.909306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:07:26.810836Z","iopub.execute_input":"2021-08-08T17:07:26.811208Z","iopub.status.idle":"2021-08-08T17:13:22.749262Z","shell.execute_reply.started":"2021-08-08T17:07:26.811156Z","shell.execute_reply":"2021-08-08T17:13:22.748342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We can observe that we can achieve maximum validation accuracy of 91.96 and training accuracy of 92.88","metadata":{}},{"cell_type":"code","source":"# Trying Optimizer as adam\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy']             \n             )","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:18:37.586753Z","iopub.execute_input":"2021-08-08T17:18:37.587107Z","iopub.status.idle":"2021-08-08T17:18:37.605519Z","shell.execute_reply.started":"2021-08-08T17:18:37.587073Z","shell.execute_reply":"2021-08-08T17:18:37.60448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T17:18:51.089029Z","iopub.execute_input":"2021-08-08T17:18:51.089402Z","iopub.status.idle":"2021-08-08T17:24:45.560762Z","shell.execute_reply.started":"2021-08-08T17:18:51.089369Z","shell.execute_reply":"2021-08-08T17:24:45.559777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}