{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport torchvision.utils as vutils\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport copy\nimport time\nimport cv2 as cv\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.image as mpimg\n\n\n\n\nimport torchvision.transforms.functional as TF\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntransform = transforms.Compose([transforms.ToTensor(),])\ncrop = transforms.CenterCrop((240,240))\nhrscale = transforms.Scale((160,256))\nlrscale = transforms.Scale((40,64))\n\nimg = mpimg.imread(\"/kaggle/input/tiger.jpg\")\nprint(img.shape)\nimg = (TF.to_pil_image(img))\n\n\nhrimg = (transform(hrscale(img)) -0.5) /0.5\nlrimg = (transform(lrscale(img)) -0.5) /0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hrimgv = hrimg *0.5 + 0.5\n\nlrimgv = lrimg*0.5 + 0.5\n\nf, axarr = plt.subplots(1,2)\naxarr[0].title.set_text('Original \\n Image')\naxarr[1].title.set_text('DownSampled Image')\n\naxarr[0].imshow(hrimgv.permute(1,2,0))\naxarr[1].imshow(lrimgv.permute(1,2,0))\nf.set_figheight(11)\nf.set_figwidth(11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#https://dmitryulyanov.github.io/deep_image_prior","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generator / Decoder Model\nnum_channels_in_encoder = 8\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        # DECODER\n#         self.latent_fc1 = nn.Sequential(\n#             nn.Linear(latent_size,1000),\n#             nn.Sigmoid(),\n#         )\n#         self.latent_fc2 = nn.Sequential(\n#             nn.Linear(1000,54*44),\n#             nn.Sigmoid(),\n#         )\n        # 128x64x64\n        self.d_up_conv_1 = nn.Sequential(\n        nn.Conv2d(in_channels=num_channels_in_encoder, out_channels=64, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(2, 2))\n        )\n\n        # 128x64x64\n        self.d_block_1 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_2 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 128x64x64\n        self.d_block_3 = nn.Sequential(\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1)),\n        )\n\n        # 256x128x128\n        self.d_up_conv_2 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=(3, 3), stride=(1, 1)),\n            nn.LeakyReLU(),\n\n            nn.ZeroPad2d((1, 1, 1, 1)),\n            nn.ConvTranspose2d(in_channels=32, out_channels=256, kernel_size=(2, 2), stride=(2, 2))\n        )\n\n        # 3x128x128\n        self.d_up_conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(3, 3), stride=(1, 1)),\n            nn.Sigmoid(),\n\n            nn.ReflectionPad2d((3, 3, 3, 3)),\n            nn.Conv2d(in_channels=16, out_channels=3, kernel_size=(3, 3), stride=(1, 1)),\n            nn.Sigmoid()\n        )\n\n        self.image = None\n        \n    def forward(self, x):\n        uc1 = self.d_up_conv_1(x)\n        dblock1 = self.d_block_1(uc1) + uc1\n        dblock2 = self.d_block_2(dblock1) + dblock1\n        dblock3 = self.d_block_3(dblock2) + dblock2\n        uc2 = self.d_up_conv_2(dblock3)\n        dec = self.d_up_conv_3(uc2)\n        self.image = dec\n#         dec = F.interpolate(dec,size=(40,64), mode='bilinear')\n        return dec\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z_size = [num_channels_in_encoder,39,63]\n\nnetG = Generator().to(device)\nnetG.apply(weights_init)\n# inp = torch.randn(1*z_size[0]*z_size[1]*z_size[2]).view((-1,z_size[0],z_size[1],z_size[2])).to(device)\n# output = netG(inp)\n# print(output.shape)\n# #218 * 178\n# del inp\n# del output\n# torch.cuda.empty_cache()\nfixed_inp = torch.randn(1*z_size[0]*z_size[1]*z_size[2]).view((-1,z_size[0],z_size[1],z_size[2])).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 0.00002\n# Initialize BCELoss function\ncriterion = nn.BCELoss()\nmsecriterion = nn.MSELoss()\nl1criterion = nn.L1Loss()\n\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# optimizerG = optim.SGD(netG.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hrimgs = hrimg.view(-1,hrimg.shape[0],hrimg.shape[1],hrimg.shape[2]).to(device)\nlrimgs = lrimg.view(-1,lrimg.shape[0],lrimg.shape[1],lrimg.shape[2]).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Starting prior training loop...\")\nnum_epochs = 160000\nindex = 0\n# For each epoch\n\n\n\n\nfor epoch in range(num_epochs):\n    netG.train()\n    netG.zero_grad()\n    output = netG(fixed_inp)\n#     print(output.shape)\n\n    # now compute loss and backpropogate\n    optimizerG.zero_grad()\n    output = F.interpolate(output,size=(40,64), mode='bicubic')\n#     loss = msecriterion(loutput, lrimgs)\n    \n    \n    loss = 2 * l1criterion(output, lrimgs) + 2*msecriterion(output, lrimgs)\n    \n#     print(loss.item())\n    loss.backward()\n    optimizerG.step()\n\n    \n    if index % 100 ==0:\n        netG.eval()\n        print(loss.item())\n        showimg = (netG.image[0].cpu().detach().permute(1, 2, 0)) * 0.5 + 0.5\n        f, axarr = plt.subplots(1)\n        axarr.imshow(showimg)\n        plt.show()\n    \n    index = index + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"netG.eval()\nshowimg = (netG.image[0].cpu().detach().permute(1, 2, 0)) * 0.5 + 0.5\nf, axarr = plt.subplots(1,3)\n\n\naxarr[0].title.set_text('Original \\n Image')\naxarr[1].title.set_text('DownSampled Image')\naxarr[2].title.set_text('Generated Image')\n\n\n\naxarr[0].imshow(hrimgv.permute(1,2,0))\naxarr[1].imshow(lrimgv.permute(1,2,0))\naxarr[2].imshow(showimg)\n\n\nf.set_figheight(25)\nf.set_figwidth(25)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"showimg = (output[0].cpu().detach().permute(1, 2, 0)) * 0.5 + 0.5\nplt.imshow(showimg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_inp = torch.randn(1*z_size[0]*z_size[1]*z_size[2]).view((-1,z_size[0],z_size[1],z_size[2])).to(device)\nnetG.eval()\nshowimg = (netG(random_inp)[0].cpu().detach().permute(1, 2, 0)) * 0.5 + 0.5\nplt.imshow(showimg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}