{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Wine Quality Prediction: ( Challenge at the end )\n\n### Date : 14-02-2020\n### @justsuyash (linkedin)"},{"metadata":{},"cell_type":"raw","source":"For more information, read [Cortez et al., 2009].\nInput variables (based on physicochemical tests):\n1 - fixed acidity\n2 - volatile acidity\n3 - citric acid\n4 - residual sugar\n5 - chlorides\n6 - free sulfur dioxide\n7 - total sulfur dioxide\n8 - density\n9 - pH\n10 - sulphates\n11 - alcohol\nOutput variable (based on sensory data):\n12 - quality (score between 0 and 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n#import the necessary modelling algos.\nfrom sklearn.decomposition import PCA\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['quality'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp = data['quality'].value_counts()\nsp = pd.DataFrame(sp)\nsp.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = sp.index, y=sp['quality'])\nplt.xlabel(\"Quality Score\")\nplt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,7))\nsns.set(font_scale=1.2)\nsns.heatmap(data.corr(), annot=True, linewidths=0.5, cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We see that there are a lot of corelated variables like :"},{"metadata":{},"cell_type":"raw","source":"#1) 'fixed acidity' is corealted to 'citric acid' \n#2) 'free sulphur' and 'total sulphur dioxide' are corealted\n#3) 'ph' and 'acidity' are negatively corelated(obviously)"},{"metadata":{},"cell_type":"markdown","source":"## But we are not dropping them as we will be doing PCA  and If N variables are highly correlated than they will all load out on the SAME Principal Component (Eigenvector) and hence we do not need to remove them"},{"metadata":{},"cell_type":"raw","source":"#1) Use the PCA, and interpret it according to what variables load out on it\n#2) Choose one of the highly correlated variables as identified as those that all load onto the same variable and analyse only it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##   Lets See an interesting result :"},{"metadata":{},"cell_type":"markdown","source":"## As the kernel suggests and like many people have done as well, we divide the quality in two bins."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_try  = data_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndata_try['category'] = pd.cut(data_try['quality'], bins = bins, labels = group_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We are looking at a highly imbalanced dataset and hence lets make a guess that all wines are bad and lets see the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_try['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = data_try[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']]\n\ny1 = data_try['category']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_dummy,x_test_dummy,y_train_dummy,y_test_dummy = train_test_split(x1,y1,test_size = 0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Creating a dummy prediction of all bad quality:\n\n\nThis is being done in light to get a becnchmark of how good should our model be, I am creating  a dummy prediction in which I have predicted all qualities as bad, as the kernel decscription has told us to. ( Like bin(2,6.5,8))."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_dummy.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_dummy_predict = []\ny_dummy_predict = ['bad']*y_test_dummy.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_dummy_predict,y_test_dummy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_dummy, y_dummy_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  So we See the problem with most kernels here they achieve an accuracy of 80-88% and are just slightly better or worse than our guess. This is due to the imbalance in data, even if the prediction goes upto 95% its not doing a whole lot of good"},{"metadata":{},"cell_type":"markdown","source":"##  What we will try do instead is consider wines above 5 as 'good' quality wines, and below it as 'bad' quality wines this would lead to somewhat equal distribution and even if we want a extremely good quality wine we would have reduced our options to taste test by a little less than 50%."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quality =  data_cleaned['quality'].values\n\ncategory_balanced = []\n\nfor num in quality:\n    if num<=5:\n        category_balanced.append('bad')\n    elif num>=6:\n        category_balanced.append('good')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_balanced  = pd.DataFrame(data=category_balanced,columns=['category_balanced'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_balanced.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned = pd.concat([data_cleaned,category_balanced],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned = data_cleaned.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned['category_balanced'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_cleaned[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']]\ny = data_cleaned['category_balanced']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We are scaling the data as we do not know the units for each field :"},{"metadata":{"trusted":true},"cell_type":"code","source":"scl = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = scl.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets perform PCA and see what we get"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pca = pca.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So we see that 8 features explain about 99% of the variablity so we will use 8 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_new = PCA(n_components=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pca_8 = pca_new.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state=420)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[LogisticRegression(),SVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}\nd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Let's now have a look at what outcome would a guess work have come up with :"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_dummy_predict ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  So we that, it would have only got an accuracy of 20% and now we have something close to 80% with hyperparameter tuning we can get even better result bit I will leave this kernel at this point, Please fell fee to do hyperparameter tuning and other adjustments like dropping corelated columns and share the result with me."},{"metadata":{},"cell_type":"markdown","source":"Think like a Data Scientist and So you become!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Challenge :\n\nI have tried removing outliers and it slightly degrades my results, if anyone can remove the outliers and get a better result let me know. \n\nFind me on linkedin - @justsuyash"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}