{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U imbalanced-learn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom scipy.special import softmax\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Analysis"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/website-phishing-data-set/Website Phishing.csv\")\nfeats = data.columns.tolist()[:-1]\nnum_feats = len(feats)\ny = data[\"Result\"].tolist()\nX = data.drop(columns=[\"Result\"]).to_numpy()\nclasses = set(y)\nprint(\"Features: \", feats)\nprint(\"Classes: \", classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = data.columns.tolist()\nnum_exem = len(data[feats[0]])\nnum_missing_values = 0\nfor feat in feats:\n    num_missing_values += abs(len(data[feat]) - num_exem)\n    \nprint(\"Number of missing values: \",num_missing_values)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in classes:\n    print(\"Number of exemples of class {}: {}\".format(label, len(data[data[\"Result\"]==label])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apesar de não existirem dados faltando, o dataset está claramente desbalanceado. Isso pode afetar consideramente os resultados, porquê ainda que a avalições de acurácia sejam boas, alguns classes podem apresentar um nível de erro maior do que as demais principalmente aquelas que não possuem muitos exemplos , bem como, podemos enfretar casos de overfitting nas que tem muito. Considerando isso, uma boa tentativa de melhorar o desempenho é balancear o data-set. Usando *RandomOverSampler*, podemos balancear todas as classes, assim, as classes com menos exemplos teram a quantidade aumentada."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_balanced, y_balanced = ros.fit_resample(X, y)\nprint(\"Nova quantidade de exemplos: \", Counter(y_balanced))\ny_balanced = np.array(y_balanced)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Information\n\n\nConsiderando que temos features que apresentam valores negativos, podemos utilizar a função softmax para garantir que todos os valores sejam positivos. Isso é importante para cálculo da média que poderia apresentar um valor negativo, o que é indesejado. Sendo assim, Isso garante a proporção dos dados e, portanto, o significado atribuido a eles."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscl_X_balanced = scaler.fit_transform(X_balanced)\nscl_X = scaler.fit_transform(X_balanced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_percentil(X):\n    percent_array=np.zeros((num_feats, 100))\n\n    # Calculates the percentiles for each feature\n    for f in range(num_feats):\n        for i in range(1,101):\n            percent_array[f][i-1] = np.percentile(X[:,f], i)\n \n    return percent_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef plot_info(feats_mean, feats_std, percent_array):\n    fig, axis = plt.subplots(1,2,figsize=(16,5))\n\n    plt.xlabel(\"Features\")\n\n    axis[0].hist(feats_mean) \n    axis[1].hist(feats_std)\n    \n    axis[0].legend([\"Mean\"])\n    axis[1].legend([\"Stantard Desviation\"])\n   \n    plt.xlabel(\"Percentiles\")\n    fold1 = percent_array[:3]\n    fold2 = percent_array[3:6]\n    fold3 = percent_array[6:9]\n    for fold in [fold1,fold2,fold2]:\n        fig1, axis1 = plt.subplots(1,3,figsize=(25,10))\n        for i, percent in enumerate(fold):\n            axis1[i].plot(range(100), percent)\n    fig1.subplots_adjust(wspace=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats_mean, feats_std = feats_mean = np.mean(X, axis=0), np.std(X, axis=0)\npercent_array = get_percentil(X)\nplot_info(feats_mean, feats_std, percent_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como pode ser visto, há uma distribuição heterogênea das features o que pode fazer com que uma se sopreponha as demais por apresentar um desvio padrão acima da média. Além disso, o percentile mostra que os dados dos últimos valores (próximos a 100) tende a ter regiões um pouco distintas de concentração dos dados. Isso significa que o classificador terá, de alguma forma, possiveis regiões de classificação mais comuns, sendo mais díficil classificar um dado fora dessa tendência. "},{"metadata":{"trusted":true},"cell_type":"code","source":"feats_mean, feats_std = feats_mean = np.mean(scl_X_balanced, axis=0), np.std(scl_X_balanced, axis=0)\npercent_array = get_percentil(scl_X_balanced)\nplot_info(feats_mean, feats_std, percent_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n Os resultados para o data-set balanceado apresentam um equilíbrio maior em relação a média de cada features o que consequentemente ajudará no processo de classificação. Além disso, a diminuisão do desvisão padrão criou um distribuição homogenea das features no data-set, isso faça com que as diferentes características dos dados sejam igualmente relevantes evitando que um sopreponha a outra. Isso permite que classificadores lineares como SVM, por exemplo, obtenha resultados consideravelmente melhores."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfig = plt.figure(1, figsize=(4, 3))\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(scl_X_balanced[:100])\ny_pred = kmeans.predict(scl_X_balanced[100:200])\n\nplt.scatter(range(len(y_pred)), y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como pode ser visto os dados podem ser agrupados e portanto separados por um classificador"},{"metadata":{},"cell_type":"markdown","source":"# Classification\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['class '+str(c) for c in classes] # tornando as classes strings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM -> Support Vector Machine "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, shuffle=True)\nsvm_clf = svm.SVC()\n\nsvm_clf.fit(X_train,y_train)\npreds = svm_clf.predict(X_test)\n\nprint(\"Training Count: \", Counter(y_train))\nprint(\"Testing Count: \", Counter(y_test))\nprint(classification_report(y_test, preds, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Análise Preliminar\n\nComo pode ser visto, os resultados que utilizam os dados originais, apesar de terem um resultado considerável, apresentam um score totalmente desbalanceado em relação as classes. A classe 1, por exemplo, apresenta um recall de 0.16, enquanto a classe 0 tem um score de 0.92 representando uma diferença de 75%! Isso mostra que os dados do nosso classificador precisam ser balanceados e escalonados. Como vemos a seguir, quando isso acontece, obtemos resultados significativamente melhores. Isso ocorre tanto em relação as métricas obtidas em cada classe, como também, em relação a acurácia e as médias de recall, f1-score e precision."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scl_X_balanced, y_balanced,test_size=0.2, shuffle=True)\nprint(\"Training Count: \", Counter(y_train))\nprint(\"Testing Count: \", Counter(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification using SVM -> Support Vector Machine \n# spliting training and validation\n\nsvm_clf = svm.SVC()\n\nsvm_clf.fit(X_train,y_train)\npreds = svm_clf.predict(X_test)\n\nsvm_result = classification_report(y_test, preds, target_names=classes, output_dict=True)\nprint(classification_report(y_test, preds, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Análise \n\nEsse se apresenta como um dos melhores classificadores testados. Apesar de não termos um quantidade grande de dimenssões nos nossos dados (9 apenas), o classificador consegui gerar bem um hiperplano capaz de separar os dados. Além disso, o desempenho do classificador melhora consideravelmente quando regularizamos os dados, o que de fato é o esperado para esse tipo de classificador.\nComo pode ser visto, ele apresenta uma acurácia de 0.92, o que um bom resultado. Além disso, a classificação de todas as classes obtem resultados consideravelmente bons igualmente. Considerando a precisão e recall, podemos afirmar que o classificador tem bons níveis de verdadeiro positivo como de verdadeiro negativo."},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\npreds = clf_rf.predict(X_test)\nrf_result = classification_report(y_test, preds, target_names=classes, output_dict=True)\nprint(classification_report(y_test, preds, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Análise\n\nO random forest assim como o Adabooster (próximo classificador), apresentam um abordagem distinta. Ao invés de utilizarem apenas um algoritmo para gerar as previsões, eles usam a combinação de vários classificadores para chegar a um predição mais precisa. De fato, é o que se apresenta nesse caso ontem a acurácia supera a do algoritmo SVM. Assim como o primeiro classificador (SVM), há um ótima distribuição de scores entre as classes, evitando que uma classe seja mais bem classificado do que as demais. Contudo, existe uma clara tendência de over-fitting, principalmente se analisarmos a class 1 onde os exemplos positivos tem um score de 1.0. Isso faz com que a árvore de decisão seja, também, um ótimo candidato."},{"metadata":{},"cell_type":"markdown","source":"# AdaBoostClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nclf_ada = AdaBoostClassifier(n_estimators=100)\ny_pred = clf_ada.fit(X_train, y_train).predict(X_test)\nada_result = classification_report(y_test, y_pred, target_names=classes, output_dict=True)\nprint(classification_report(y_test, y_pred, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Análise\n\nApesar de ser um classificador baseado em diversos estimadores (nesse caso 100) o Adabooster não apresentam um resultado tão promissor quantos os demais. Ainda que mantenha uma boa distribuição dos acertos ao longo de todas as classes, o algoritimo, na média, apresenta um resultado inferior. Um dos possíveis motivos é o não melhor ajuste do meta-estimator aos dados, deixando muitos exemplos dífices para os demais estimators. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_result.keys())\nmetrics = ['accuracy']\n\nresults = []\n\n\nfor metric in metrics:\n    score = []\n    for i, r in enumerate([svm_result, rf_result, ada_result]):\n        print(r[metric])\n        score.append(r[metric]) \n    \n    results.append(score)\n    \nfor j,result in enumerate(results): \n    fig, axis = plt.subplots(1,1,figsize=(16,5))\n    fig.suptitle('Resultado de Classificação Metric: {}'.format(metrics[j]), fontsize=16)\n    for i, res in enumerate(results):\n        print(res)\n        axis.bar([1,2,3],res,\n                    tick_label=['svm_result', 'rf_result', 'ada_result'], color=['green', 'blue', 'yellow'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# É preciso rolar o output para visualizar todas as métricas\nmetrics = ['f1-score', 'recall', 'precision']\nmetric_avg = ['macro avg', 'weighted avg']\nfor metric in metrics:\n    for label in metric_avg:\n        score = []\n        for i, result in enumerate([svm_result, rf_result, ada_result]):\n            result_f1 = result[label][metric]\n            score.append(result_f1) \n        results.append(score)\n    for j,label in enumerate(metric_avg): \n        fig, axis = plt.subplots(1,1,figsize=(5,3))\n        fig.suptitle('Resultado de Classificação {} Metric: {}'.format(metric_avg[j], metric), fontsize=16)\n        for i, result in enumerate(results):\n            axis.bar([1,2,3],result,\n                        tick_label=['svm_result', 'rf_result', 'ada_result'], color=['green', 'blue', 'yellow'])        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusão\n\nEm resumo, os classificadores apresentam um bom desempenho no geral com destalhe para a SVM e o RandomForest. \nNo requisito acuráicia o random forest se apresenta melhor, porém apresenta sinais de over-fitting em um das classes \nenquanto o SVM distribui um pouco melhor o scores entre as classes existentes. O adabooster não obteve um resultado satisfatório, e apresenta\num distribuição dos scores finais mais sparsas em relação as classes. Portanto, entre os classificadores o que se \napresenta com melhor resultado é a SVM, devido a não existência de overfitting em relação as classes e ao mesmo tempo\napresentar uma acuráicia e scores médias melhores."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}