{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport requests\nfrom scipy import stats\nfrom functools import partial\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, LeaveOneGroupOut, LeavePGroupsOut\nimport lightgbm as lgb\nimport os\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# def is_interactive():\n#    return 'runtime' in get_ipython().config.IPKernelApp.connection_file\n\n# if is_interactive(): \n#     plt.style.use('dark_background')\n\n#     COLOR = 'white' # used 'white' when editing in interactive mode with dark theme ON\n#     import matplotlib\n#     matplotlib.rcParams['text.color'] = COLOR\n#     matplotlib.rcParams['axes.labelcolor'] = COLOR\n#     matplotlib.rcParams['xtick.color'] = COLOR\n#     matplotlib.rcParams['ytick.color'] = COLOR\n#     else: plt.style.use('ggplot')\n        \nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_axes_grid(numplots_y, numplots_x, plotsize_x=6, plotsize_y=3):\n    fig, axes = plt.subplots(numplots_y, numplots_x)\n    fig.set_size_inches(plotsize_x * numplots_x, plotsize_y * numplots_y)\n    fig.subplots_adjust(wspace=0.1, hspace=0.25)\n    return fig, axes\n\ndef set_axes(axes, use_grid=True):\n    axes.grid(use_grid)\n    axes.tick_params(which='both', direction='inout', top=True, right=True, labelbottom=True, labelleft=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\n\nclass generalCallback():\n    def __init__(self, model_trainer, *args, **kwargs):\n        self.model_trainer = model_trainer\n        \n    def onTrainStart(self, *args, **kwargs):\n        pass\n    def onFoldStart(self, *args, **kwargs):\n        pass\n    def onFoldEnd(self, *args, **kwargs):\n        pass\n    def onTrainEnd(self, *args, **kwargs):\n        pass\n        \n        \nclass Base_Model(object):\n    \"\"\"\n    Parent model class, contains functions universal for all models used. Initial implementation credits to Bruno Aquino.\n    \"\"\"\n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True, target='accuracy_group', val_metric=metrics.r2_score, params=None, cb=None):\n        if not verbose: warnings.filterwarnings(\"ignore\")\n        self.val_metric = val_metric\n        if cb is None: \n            self.cb = generalCallback(self)\n        else:\n            self.cb = cb(self)\n        self.train_df = train_df\n        self.test_df = test_df\n        self.params = params\n        self.features = features\n        if verbose:print(f'Using {len(features)} features')\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = target\n        self.verbose = verbose\n        self.all_data = []\n        \n    def __call__(self):\n        self.params = self.get_params()\n        self.cv = self.get_cv()\n        self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        cv = GroupKFold(5)\n        return cv.split(self.train_df, self.train_df[self.target], groups=self.train_df.area_code)\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n        self.val_preds = []\n        self.val_ys = []\n        self.model = []\n#         self.area_codes = []\n        self.cb.onTrainStart()\n        \n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n#             self.area_codes.append(self.train_df['area_code'].iloc[val_idx])\n            self.cb.onFoldStart(val_idx=val_idx, train_idx=train_idx)\n            \n#             if sum(self.train_df.iloc[train_idx]['area_code'].isin(self.train_df.iloc[val_idx]['area_code'])) > 0: raise Exception('Same area codes in trn and val sets, may be overfitting')\n            \n            \n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            \n            all_data_fold = {\n                'x_train': x_train,\n                'x_val': x_val,\n                'y_train': y_train,\n                'y_val': y_val\n            }\n            self.all_data.append(all_data_fold)\n            \n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            \n            model = self.train_model(train_set, val_set)\n            self.model.append(model)\n            conv_x_val = self.convert_x(x_val.reset_index(drop=True))\n            \n            preds_all = model.predict(conv_x_val)\n            self.val_preds.append(preds_all)\n            \n            oof_score = self.val_metric(y_val, np.array(preds_all)) if type(self.val_metric)!=list else [i(y_val, np.array(preds_all)) for i in self.val_metric]\n            if self.verbose: print(f'Partial score (all) of fold {fold} is: {oof_score}')\n\n            self.val_ys.append(y_val.reset_index(drop=True).values)\n            \n            self.cb.onFoldEnd()\n\n        self.val_ys = np.concatenate(self.val_ys)\n        self.val_preds = np.concatenate(self.val_preds)\n#         self.area_codes = np.concatenate(self.area_codes)\n        self.cb.onTrainEnd()\n        \n        self.score = self.val_metric(self.val_ys, self.val_preds) if type(self.val_metric)!=list else [i(self.val_ys, self.val_preds) for i in self.val_metric]\n\n        if self.verbose: print(f'Our oof rmse score (all) is: {self.score}')\n\nclass uk_hp_model_cb(generalCallback):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n    def onTrainStart(self, *args, **kwargs):\n        self.model_trainer.area_codes = []\n    \n    def onFoldStart(self, *args, **kwargs):\n        val_idx = kwargs['val_idx']\n        train_idx = kwargs['train_idx']\n        self.model_trainer.area_codes.append(self.model_trainer.train_df['area_code'].iloc[val_idx])\n        if sum(self.model_trainer.train_df.iloc[train_idx]['area_code'].isin(self.model_trainer.train_df.iloc[val_idx]['area_code'])) > 0: \n            raise Exception('Same area codes in trn and val sets, may be overfitting')\n        \n    def onFoldEnd(self, *args, **kwargs):\n        pass\n    \n    def onTrainEnd(self, *args, **kwargs):\n        self.model_trainer.area_codes = np.concatenate(self.model_trainer.area_codes)   \n    \n    \nclass Lgb_Model(Base_Model):\n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.75,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.8,\n                    'max_depth': 150, # was 15\n                  'num_leaves': 50,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 300,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 400\n                    } if not self.params else self.params\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class check():\n    def __init__(self, model):\n        self.m = model\n    def test(self):\n        print(self.m.b)\n\nclass asd():\n    def __init__(self):\n        self.a = 123\n        self.cb = check(self)\n    \n    def addNewVar(self):\n        self.b = '1111111'\n        \n    def test(self):\n        self.cb.test()\n\ntobj = asd()\ntobj.addNewVar()\ntobj.test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prep_features_list(train_data, exc_col, cor_trh):\n    train_features = [i for i in train_data.columns if i not in exc_col]\n    # train_features = important_features\n    counter = 0\n    to_remove = []\n    for feat_a in train_features:\n        for feat_b in train_features:\n            if (feat_a != feat_b) & (feat_a not in to_remove) & (feat_b not in to_remove):\n                c = np.corrcoef(train_data[feat_a], train_data[feat_b])[0][1]\n                if c > cor_trh:\n                    counter += 1\n                    to_remove.append(feat_a)\n                    print(f'{counter}: FEAT_A: {feat_a} ||| FEAT_B: {feat_b} ||| Correlation: {np.round(c,3)}')\n\n\n    train_features = [i for i in train_data.columns if i not in exc_col + to_remove]\n\n    for i in to_remove:print(i)\n    \n    return train_features\n\ndef adj_r2_score(truth, preds, n_predictors):\n    n = truth.shape[0]\n    r2 = metrics.r2_score(truth, preds)\n    res = 1-(1-r2)*(n-1)/(n-n_predictors-1)\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get health data from PHE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I have downloaded UK data on various health profiles across UK regions\n\nLink - https://fingertips.phe.org.uk/profile/health-profiles/data#page/6/gid/8000073/pat/6/par/E12000004/ati/201/are/E07000032/iid/20201/age/1/sex/2/cid/4/page-options/map-ao-4_cin-ci-4_ovw-tdo-0","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The dataset contains a lot of different indicators. Some of them contain values for different age groups, different sexes and for different time periods. The values of indicators themselves also contain confidence intervals and comparisons to other UK statistics.\n\nNotes\n* Despite all that information, at the moment I only use value by indicator by group, everything else being summed up. \n* Summing indicator values up is appropriate for indicators measured in absolute numbers, but is not very appropriate for percentages.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hp_original = pd.read_csv('../input/uk-covid-19-related-data/indicators-DistrictUA.data.csv')\nhp = hp_original.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prep_health_profiles(hp):\n    hp.rename({i:i.replace(' ','_').lower() for i in hp.columns}, axis=1,inplace=True)\n    \n    hp_cols_to_use = ['indicator_name','area_code','area_name','sex','age','category_type','category','time_period','value']#,'count','denominator']\n    hp = hp.loc[~hp.value.isna(),hp_cols_to_use]\n    \n    last_periods = hp.groupby('indicator_name', as_index=False).last()[['indicator_name','time_period']]\n    hp = last_periods.merge(hp, on=['indicator_name','time_period'], validate='one_to_many')\n    \n    hp = hp.groupby(['indicator_name', 'area_code'], as_index=False).mean()[['indicator_name', 'area_code','value']]#,'count','denominator']]\n    \n    hp = hp.pivot(columns='indicator_name',index='area_code',values='value')\n    \n    return hp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"health_profiles = prep_health_profiles(hp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get covid mortality data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The data comes from UK's Office for National Statistics\n\nLink: https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/causesofdeath/datasets/deathregistrationsandoccurrencesbylocalauthorityandhealthboard","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The dataset contains registered deaths by UK regions, weeks, causes (covid vs all), place\n\nAgain, at the moment only used part of this data. Here using regions and weeks.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prep_mortality_data(d):\n    cols = d.loc[2,:]\n    d = d.loc[3:,:]\n    d.columns = cols\n    \n    d.reset_index(drop=True,inplace=True)\n    d.index.name=None\n    d.columns.name=None\n    \n    d.rename({i:i.replace(' ','_').lower() for i in d.columns}, axis=1,inplace=True)\n    d.rename({'area_name_':'area_name'}, axis=1,inplace=True)\n    \n    d_weekly = d.groupby(['area_code', 'cause_of_death', 'week_number'], as_index=False).sum().drop(['place_of_death'], axis=1)\n    d_all = d.groupby(['area_code', 'cause_of_death'], as_index=False).sum().drop(['week_number', 'place_of_death'], axis=1)\n    \n    return d_weekly, d_all\n    \ndef get_mortality_ratios(d, USE_WEEK):\n    d.drop(['geography_type', 'area_name'], axis=1, inplace=True)\n    \n    d_all = d.loc[d.cause_of_death=='All causes',:].drop('cause_of_death', axis=1).rename({'number_of_deaths':'dnum_all'}, axis=1)\n    d_cv = d.loc[d.cause_of_death=='COVID 19',:].drop('cause_of_death', axis=1).rename({'number_of_deaths':'dnum_cv'}, axis=1)\n\n    d_prop = d_cv.merge(d_all, how='left', on=['area_code', 'week_number'], validate='one_to_one') if USE_WEEK else d_cv.merge(d_all, how='left', on=['area_code'], validate='one_to_one')\n    d_prop.loc[:,'ratio'] = d_prop.dnum_cv/d_prop.dnum_all\n\n    d_prop.drop(['dnum_cv','dnum_all'], axis=1, inplace=True)\n    \n    d_prop = d_prop.loc[d_prop.ratio>0,:].reset_index(drop=True)\n    \n    return d_prop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"d_original = pd.read_excel('../input/uk-covid-19-related-data/lahbtablesweek22.xlsx', sheet_name='Registrations - All data')\nd = d_original.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mortality_data_weekly, mortality_data_weekly_sum = prep_mortality_data(d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"weekly_ratios = get_mortality_ratios(mortality_data_weekly, USE_WEEK=True)\nsum_ratios = get_mortality_ratios(mortality_data_weekly_sum, USE_WEEK=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Population data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesanalysistool","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_population_data(cd):\n    cd.rename({i:i.replace(' ','_').lower() for i in cd.columns}, axis=1,inplace=True)\n    cd.loc[:,'internal_migration_ratio'] = cd.loc[:,'internal_migration_net']/cd.loc[:,'estimated_population_2016.']\n    cd.loc[:,'international_migration_ratio'] = cd.loc[:,'international_migration_net']/cd.loc[:,'estimated_population_2016.']\n    \n    cd.rename({'la_code':'area_code'}, axis=1, inplace=True)\n    cols_to_use = [i for i in cd.columns if i not in ['la_name','country_code','country_name','region_code','region_name','county_code','county_name','estimated_population_2015']]\n    \n    cd = cd[cols_to_use]\n    \n    return cd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# cd_original = pd.read_excel('../input/uk-covid-19-related-data/Analysis Tool mid-2016 UK.xlsx', sheet_name='Components of Change')\ncd_original = pd.read_csv('../input/uk-covid-19-related-data/Analysis Tool mid-2016 UK.csv')\ncd = cd_original.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"population_data = get_population_data(cd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Density","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From https://www.nomisweb.co.uk/census/2011/wd102ew","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"density = pd.read_csv('../input/uk-covid-19-related-data/density.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"density = density[['geography code', 'Area/Population Density: Density (number of persons per hectare); measures: Value']]\ndensity = density.rename({\n    'Area/Population Density: Density (number of persons per hectare); measures: Value':'density',\n    'geography code':'area_code'\n}, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"density.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merging","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def merge_all(mortality_ratios, health_profiles, population_data):\n    hp_and_d = mortality_ratios.merge(health_profiles, left_on='area_code', right_index=True, how='left', validate='many_to_one')\n\n    hp_and_d.rename({i:i.replace(' ','_').lower() for i in hp_and_d.columns}, axis=1,inplace=True)\n    hp_and_d.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in hp_and_d.columns]\n    \n    hp_and_d.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    hp_and_d.reset_index(drop=True,inplace=True)\n    \n    hp_d_cd = hp_and_d.merge(population_data, on='area_code', how='left')\n    \n    hp_d_cd = hp_d_cd.merge(density, on='area_code', how='left')\n    \n    return hp_d_cd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_data_weekly = merge_all(weekly_ratios, health_profiles, population_data)\ntrain_data_sum = merge_all(sum_ratios, health_profiles, population_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Map","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import geopandas as gpd\n\nukmap = gpd.read_file('../input/uk-covid-19-related-data/Local_Authority_Districts__April_2019__Boundaries_UK_BFE-shp/Local_Authority_Districts__April_2019__Boundaries_UK_BFE.shp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for_map = ukmap.merge(mortality_data_weekly_sum, left_on='LAD19CD', right_on='area_code', how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity. Data as of w/e 12-Jun.')\nfor_map.plot(column='number_of_deaths', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for_map = ukmap.merge(population_data, left_on='LAD19CD', right_on='area_code', how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity. Data as of w/e 12-Jun.')\nfor_map.plot(column='births', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for_map = ukmap.merge(health_profiles, left_on='LAD19CD', right_index=True, how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity. Data as of w/e 12-Jun.')\nfor_map.plot(column='Suicide rate', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity')\nfor_map.plot(column='ratio', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(np.log(train_data_sum.density))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from textwrap import wrap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# for_map.loc[:,'supporting_information_____population_from_ethnic_minorities'] -= for_map.loc[:,'supporting_information_____population_from_ethnic_minorities']\n\nfig, ax = plt.subplots(1,2, figsize=(20, 12))\n\nvar = 'density'\nax[0].set_title('Population density (persons per hectare) (log scale)', pad=20)\nfor_map.loc[:, var] = np.log(for_map.loc[:, var])\n# for_map.loc[:,var] /= for_map['estimated_population_2016.']\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[0])\n\n# divider = make_axes_locatable(ax[0])\n# cax = divider.append_axes('right', size='5%', pad=0.05)\n# fig.colorbar(im1, cax=cax, orientation='vertical')\n\nvar = 'supporting_information_____population_from_ethnic_minorities'\nax[1].set_title('Ethnic minorities (%)', pad=20)\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[1], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# for_map.loc[:,'supporting_information_____population_from_ethnic_minorities'] -= for_map.loc[:,'supporting_information_____population_from_ethnic_minorities']\n\nfig, ax = plt.subplots(1,2, figsize=(20, 12))\n\nvar = 'tb_incidence__three_year_average_'\nax[0].set_title('Tuberculosis incidence per 100,000 (3-year average)', pad=20)\n# for_map.loc[:,var] /= 100\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[0])\n\n# divider = make_axes_locatable(ax[0])\n# cax = divider.append_axes('right', size='5%', pad=0.05)\n# fig.colorbar(im1, cax=cax, orientation='vertical')\n\nvar = 'smoking_status_at_time_of_delivery'\nax[1].set_title('Smoking status at time of delivery (%)', pad=20)\n# for_map.loc[:,var] /= 100\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[1], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_sum.sort_values('births', ascending=False)[['area_code','births']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# for_map.loc[:,'supporting_information_____population_from_ethnic_minorities'] -= for_map.loc[:,'supporting_information_____population_from_ethnic_minorities']\n\nfig, ax = plt.subplots(1,2, figsize=(20, 12))\n\nvar = 'supporting_information_____population_aged_under_18'\nax[0].set_title('Population aged under 18 (%)', pad=20)\n# for_map.loc[:,var] /= 100\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[0])\n\n# divider = make_axes_locatable(ax[0])\n# cax = divider.append_axes('right', size='5%', pad=0.05)\n# fig.colorbar(im1, cax=cax, orientation='vertical')\n\nvar = 'births'\nax[1].set_title('Births (log scale)', pad=20)\nfor_map.loc[:,var] = np.log(for_map.loc[:,var])\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[1], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_sum.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# World data","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"os.listdir(\"../input/uk-covid-19-related-data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wdata = pd.read_csv('../input/uk-covid-19-related-data/owid-covid-data.csv', parse_dates=['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wdata.location.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"w = wdata.loc[(wdata.total_cases_per_million!=0) & (~wdata.total_cases_per_million.isna())]\ncount_by_location = w.groupby('location').count().sort_values('date')\nmode = count_by_location.date.mode().iloc[0]\nprint(mode)\ncountries_include = count_by_location.loc[count_by_location.date>=mode].index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"w = w.loc[w.location.isin(countries_include)]\n\nall_ts = np.zeros((countries_include.shape[0], mode, 1))\nall_ts_scaled = np.zeros((countries_include.shape[0], mode, 1))\nall_labels = np.zeros(countries_include.shape[0], dtype=object)\n\nfor idx, c in enumerate(countries_include):\n    curr_country = w.loc[w.location==c]\n    values = curr_country.total_cases_per_million.iloc[0:mode].values\n    all_ts[idx, :, 0] = np.gradient(values)\n    \n    values -= np.min(values)\n    values /= np.max(values)\n    all_ts_scaled[idx, :, 0] = values\n\n    all_labels[idx] = c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tslearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tslearn.clustering import TimeSeriesKMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n_clusters = 6\nkm = TimeSeriesKMeans(n_clusters, n_jobs=-1).fit_predict(all_ts)\n\nfig, axes = create_axes_grid(n_clusters//2,2,10,5)\ncol = 0\n\nfor row, cluster_id in zip(np.repeat(np.arange(n_clusters/2, dtype='int'), 2), range(n_clusters)):\n    names = all_labels[km==cluster_id]\n    name = ', '.join([i for i in names[0:5]])\n    \n    cases = all_ts[km==cluster_id, :, 0]\n    \n    axes[row, col].set_title(f'C {cluster_id} | {len(names)} countries | {name}')\n    axes[row, col].set_ylabel('Total cases per million')\n    axes[row, col].set_xlabel('Days since case 1')\n    \n    days = np.arange(0, mode)\n    \n    for i in range(sum(km==cluster_id)):\n        axes[row, col].plot(days, cases[i,:], color='blue')\n        \n    col = 1 if col == 0 else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n_clusters = 6\nkm = TimeSeriesKMeans(n_clusters, n_jobs=-1).fit_predict(all_ts_scaled)\n\nfig, axes = create_axes_grid(n_clusters//2,2,10,5)\ncol = 0\n\nfor row, cluster_id in zip(np.repeat(np.arange(n_clusters/2, dtype='int'), 2), range(n_clusters)):\n    names = all_labels[km==cluster_id]\n    name = ', '.join([i for i in names[0:5]])\n    \n    cases = all_ts_scaled[km==cluster_id, :, 0]\n    \n    axes[row, col].set_title(f'C {cluster_id} | {len(names)} countries | {name}')\n    axes[row, col].set_ylabel('Total cases per million')\n    axes[row, col].set_xlabel('Days since case 1')\n    \n    days = np.arange(0, mode)\n    \n    for i in range(sum(km==cluster_id)):\n        axes[row, col].plot(days, cases[i,:], color='blue')\n        \n    col = 1 if col == 0 else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for row, cluster_id in zip(np.repeat(np.arange(n_clusters/2, dtype='int'), 2), range(n_clusters)):\n    cluster_countries = all_labels[km==cluster_id]\n    \n    wdata.loc[wdata.location.isin(cluster_countries), 'cluster_id'] = cluster_id\n    \n    names = sorted(cluster_countries)\n    name = [', '.join(names[i:i+10]) for i in range(0, len(names), 10)]\n\n    name = '\\n '.join([i for i in name])\n    \n    print(f'C {cluster_id} | {len(names)} countries | {name}  \\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cols_include = ['cluster_id','total_cases','total_cases_per_million','total_tests','total_tests_per_thousand','tests_units','population_density','median_age','aged_65_older','aged_70_older', 'gdp_per_capita', 'cvd_death_rate', 'diabetes_prevalence']\ncluster_details = wdata[cols_include+['location']].groupby(['cluster_id','location'], as_index=False).last()\ncluster_details","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cluster_details_mean = cluster_details[cols_include].groupby('cluster_id').median()\ncluster_details_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_data = cluster_details\n\ntrain_features = [i for i in cluster_details if i not in ['cluster_id','location','tests_units',]]#'total_cases', 'total_tests']]\n\ndef wrapper(y, yhat, func):\n    y_m = np.zeros((yhat.shape[0],yhat.shape[1]))\n    if type(y) == pd.Series:\n        y_m[np.arange(0,y_m.shape[0],dtype=int), y.values.astype('int')] = 1\n    else:\n        y_m[np.arange(0,y_m.shape[0],dtype=int), y.astype('int')] = 1\n    r = func(y_m, yhat)\n    return r\n\n# validation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\nvalidation_metrics = [partial(wrapper, func=metrics.roc_auc_score)]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'multiclass',\n                      'num_classes': n_clusters,\n                    'metric': 'multiclass',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\nimport types\n\nlgb_model = Lgb_Model(train_df=train_data, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='cluster_id', val_metric=validation_metrics, params=params, cb=None)\n# lgb_model.get_cv = types.MethodType( lambda x: KFold(2, shuffle=True, random_state=1997).split(x.train_df, x.train_df[x.target]), lgb_model )\nlgb_model.get_cv = types.MethodType( lambda x: StratifiedKFold(2).split(x.train_df, x.train_df[x.target]), lgb_model )\nlgb_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model = lgb_model.model[0]\nfig, ax = plt.subplots(1,1,figsize=(15, 10))\nlgb.plot_importance(model, max_num_features = 20, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,6,figsize=(33, 10))\n\nvar = 'total_cases'\nax[0, 0].set_title(var)\nbplot0 = ax[0, 0].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<1e6), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'total_tests'\nax[0, 1].set_title(var)\nbplot1 = ax[0, 1].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<0.6*1e7), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'cvd_death_rate'\nax[0, 2].set_title(var)\nbplot2 = ax[0, 2].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'total_cases_per_million'\nax[0, 3].set_title(var)\nbplot3 = ax[0, 3].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'median_age'\nax[0, 4].set_title(var)\nbplot4 = ax[0, 4].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'aged_65_older'\nax[0, 5].set_title(var)\nbplot5 = ax[0, 5].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'population_density'\nax[1, 0].set_title(var)\nbplot6 = ax[1, 0].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<750), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'diabetes_prevalence'\nax[1, 1].set_title(var)\nbplot7 = ax[1, 1].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<0.6*1e7), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'total_tests_per_thousand'\nax[1, 2].set_title(var)\nbplot8 = ax[1, 2].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'gdp_per_capita'\nax[1, 3].set_title(var)\nbplot9 = ax[1, 3].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'aged_70_older'\nax[1, 4].set_title(var)\nbplot10 = ax[1, 4].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\n# var = 'aged_65_older'\n# ax[1, 5].set_title(var)\n# ax[1, 5].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6))\n\nfor bp in [bplot0, bplot1, bplot2,bplot3,bplot4,bplot5,bplot6,bplot7,bplot8,bplot9,bplot10]:\n    for b, c in zip(bp['boxes'], ['yellow','green','red','green','red','red']):\n        b.set_facecolor(c)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb_model.model[0]\n\nfi = [(i,f) for i, f in zip(model.feature_name(), model.feature_importance())]\nfi = sorted(fi, key = lambda x: x[1], reverse=True)\ncutoff_trh = np.percentile([i[1] for i in fi], 10)\nprint(cutoff_trh)\nimportant_features = [i[0] for i in fi if i[1] > cutoff_trh]\nfor i,z in fi: print((i,z))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%time\n\nimport shap\nx_val = lgb_model.all_data[0]['x_val']\n\nshap_values = shap.TreeExplainer(lgb_model.model[0]).shap_values(x_val)\n\nshap.summary_plot(shap_values, x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# for f in important_features[0:8]:\n#     shap.dependence_plot(f, shap_values[-1], x_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sweden","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = create_axes_grid(1,2,10,5)\n\ny = 'total_cases_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[0].set_title(f'{y} in {x} &  {x2}')\naxes[0].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y], label=x);\naxes[0].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y], label=x2);\naxes[0].legend()\n\ny = 'total_deaths_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[1].set_title(f'{y} in {x} &  {x2}')\naxes[1].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y], label=x);\naxes[1].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y], label=x2);\naxes[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = create_axes_grid(1,2,10,5)\n\ny = 'new_cases_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[0].set_title(f'{y} in {x} &  {x2}')\naxes[0].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y].rolling(7).mean(), label=x);\naxes[0].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y].rolling(7).mean(), label=x2);\naxes[0].legend()\n\ny = 'new_deaths_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[1].set_title(f'{y} in {x} &  {x2}')\naxes[1].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y].rolling(7).mean(), label=x);\naxes[1].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y].rolling(7).mean(), label=x2);\naxes[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm(arr):\n    return (arr - np.mean(arr))/ np.std(arr)\n\nfig, axes = create_axes_grid(1,2,10,5)\n\ny = 'total_cases_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[0].set_title(f'{y} in {x} &  {x2}')\naxes[0].hist(norm(wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values.reshape(-1,1)), label=x);\naxes[0].hist(norm(wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values.reshape(-1,1)), label=x2);\naxes[0].legend()\n\ny = 'total_deaths_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[1].set_title(f'{y} in {x} &  {x2}')\naxes[1].hist(norm(wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values.reshape(-1,1)), label=x);\naxes[1].hist(norm(wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values.reshape(-1,1)), label=x2);\naxes[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\ny = 'total_cases_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = 'total_deaths_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = 'new_cases_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = 'new_deaths_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = wdata.loc[wdata.location.isin([x, x2]), ['location', 'date', 'total_cases_per_million', 'total_deaths_per_million']].reset_index(drop=True)\ntrain_data.loc[train_data.location==x, 'date'] = np.arange(0, train_data.loc[train_data.location==x, 'date'].shape[0])\ntrain_data.loc[train_data.location==x2, 'date'] = np.arange(0, train_data.loc[train_data.location==x2, 'date'].shape[0])\n\nnp.random.seed(123321)\nridx = np.random.permutation(train_data.shape[0])\ntrain_data = train_data.loc[ridx,:].reset_index(drop=True)\nval_cutoff = round(0.8*train_data.shape[0])\n\ncat_dict = {\n    'United Kingdom': 0,\n    'Sweden': 1\n}\ntrain_data.loc[:, 'location'] = train_data.loc[:, 'location'].map(cat_dict)\n\ntrain_features = ['date', 'total_cases_per_million', 'total_deaths_per_million']\nvalidation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\n\nlgb_model = Lgb_Model(train_df=train_data, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='location', val_metric=validation_metrics, params=params, cb=None)\nlgb_model.get_cv = lambda: [(np.arange(0, val_cutoff, dtype=int), np.arange(val_cutoff, train_data.shape[0], dtype=int))]\nlgb_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriptive plots","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"area_code_to_name = cd_original.groupby(['LA Code','LA Name'], as_index=False).first()[['LA Code','LA Name']]\narea_code_to_name.rename({'LA Code':'area_code','LA Name':'la_name'},  axis=1, inplace=True)\n\nnum_areas = 20\nhp_d_cd = train_data_weekly.sort_values('ratio')\nareas = hp_d_cd.area_code.unique()[0:num_areas]\nhp_d_cd = hp_d_cd.sort_values(['area_code','week_number'])\n\nfig, axes = create_axes_grid(num_areas//2,2,10,5)\ncol = 0\n\nfor row, area in zip(np.repeat(np.arange(num_areas/2, dtype='int'), 2), areas):\n    name = area_code_to_name.loc[area_code_to_name.area_code==area,'la_name'].values[0]\n    population = hp_d_cd.loc[hp_d_cd.area_code==area, 'estimated_population_2016.'].values[0]\n    axes[row, col].set_title(f'{name} - {area} | Population: {population}')\n    axes[row, col].set_ylabel('Cov deaths / all deaths ratio')\n    \n    weeks = hp_d_cd.loc[hp_d_cd.area_code==area, 'week_number']\n    ratios = hp_d_cd.loc[hp_d_cd.area_code==area, 'ratio']\n    \n    axes[row, col].plot(weeks, ratios, color='blue')\n    col = 1 if col == 0 else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using light gbm, one of the most mainstream and most powerful and easy-to-use models\n\nI did not adjust hyperparameters specifically, just used what I used in one of previous competitions I took part in.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Weekly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train_data_weekly = train_data_weekly.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_weekly =original_train_data_weekly\n\nsmoking_related = [i for i in train_data_weekly.columns if 'smoking' in i]\n# train_data_weekly[smoking_related] -= train_data_weekly[smoking_related].min()\n# train_data_weekly[smoking_related] /= train_data_weekly[smoking_related].max()\n# train_data_weekly.loc[:, 'smoking_related'] = train_data_weekly[smoking_related].mean(axis=1)\n# train_data_weekly.drop(smoking_related, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[i for i in train_data_weekly.columns if 'smoking' in i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = prep_features_list(train_data_weekly, exc_col = ['area_code', 'geography_type', 'area_name', 'ratio'], cor_trh=0.95)\nvalidation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\n\nlgb_model = Lgb_Model(train_df=train_data_weekly, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='ratio', val_metric=validation_metrics, params=params, cb=uk_hp_model_cb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'FINAL SCORE:', lgb_model.score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Truth vs preds')\naxes.set_ylabel('True ratio')\naxes.set_xlabel('Predicted ratio')\naxes.scatter(lgb_model.val_preds, lgb_model.val_ys, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb_model.model[0]\n\nfi = [(i,f) for i, f in zip(model.feature_name(), model.feature_importance())]\nfi = sorted(fi, key = lambda x: x[1], reverse=True)\ncutoff_trh = np.percentile([i[1] for i in fi], 10)\nprint(cutoff_trh)\nimportant_features = [i[0] for i in fi if i[1] > cutoff_trh]\nfor i,z in fi: print((i,z))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"individual_weeks_fi = fi.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(15, 10))\nlgb.plot_importance(model, max_num_features = 20, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summed weeks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train_data_sum = train_data_sum.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_sum.area_code.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_sum = original_train_data_sum\n\nsmoking_related = [i for i in train_data_sum.columns if 'smoking' in i]\n# train_data_sum[smoking_related] -= train_data_sum[smoking_related].min()\n# train_data_sum[smoking_related] /= train_data_sum[smoking_related].max()\n# train_data_sum.loc[:, 'smoking_related'] = train_data_sum[smoking_related].sum(axis=1)\n# train_data_sum.drop(smoking_related, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data_sum.drop('smoking_status_at_time_of_delivery',axis=1,inplace=True)\n[i for i in train_data_sum.columns if 'smoking' in i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from itertools import combinations\n\n# for f1,f2 in combinations(smoking_related, 2):\n#     print(f1,f2,stats.pearsonr(train_data_sum[f1],train_data_sum[f2]))\n#     plt.scatter(train_data_sum[f1],train_data_sum[f2])\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[i for i in train_data_sum.columns if 'smoking' in i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = prep_features_list(train_data_sum, exc_col = ['area_code', 'geography_type', 'area_name', 'ratio'], cor_trh=0.95)\nvalidation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\n\nlgb_model = Lgb_Model(train_df=train_data_sum, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='ratio', val_metric=validation_metrics, params=params, cb=uk_hp_model_cb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'FINAL SCORE:', lgb_model.score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Truth vs preds')\naxes.set_ylabel('True ratio')\naxes.set_xlabel('Predicted ratio')\naxes.scatter(lgb_model.val_preds, lgb_model.val_ys, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb_model.model[0]\n\nfi = [(i,f) for i, f in zip(model.feature_name(), model.feature_importance())]\nfi = sorted(fi, key = lambda x: x[1], reverse=True)\ncutoff_trh = np.percentile([i[1] for i in fi], 10)\nprint(cutoff_trh)\nimportant_features = [i[0] for i in fi if i[1] > cutoff_trh]\nfor i,z in fi: print((i,z))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summed_weeks_fi = fi.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(15, 10))\nlgb.plot_importance(model, max_num_features = 20, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature importance across 2 models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_both = pd.DataFrame({'Feature': [i for i,z in summed_weeks_fi]})\nfi_both.loc[:,'Rank_summed'] = [i for i in range(1, fi_both.shape[0]+1)]\nfi_both.loc[:,'Rank_weekly'] = fi_both.loc[:,'Feature'].map({i:r for r, (i,z) in enumerate(individual_weeks_fi, 1)})\n\nfi_both.loc[:, 'Lowest_rank'] = fi_both.apply(lambda x: max([x['Rank_weekly'], x['Rank_summed']]), axis=1)\nfi_both.loc[:, 'Diff_in_rank'] = np.abs(fi_both['Rank_weekly'] - fi_both['Rank_summed'])\nfi_both = fi_both.sort_values('Lowest_rank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_both","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef title_prettify(x):\n    x = re.sub('_+', ' ', x)\n    x = x[0].upper() + x[1:]\n    return  x\n\ntitle_prettify('under_18s_conception_rate___1_000')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"specific_week = None\ntrain_data = train_data_sum\n\nnum_features = 6\nfig, axes = create_axes_grid(num_features//2,2,10,5)\ncol = 0\n\ndo_log = ['Density','Births']\n\nfor row, f_idx in zip(np.repeat(np.arange(num_features/2, dtype='int'), 2), range(num_features)):\n#     f = fi_both.iloc[f_idx]['Feature']\n    f = summed_weeks_fi[f_idx][0]\n#     axes[row, col].set_title(f'{f} (rank: {f_idx+1})')\n    title = title_prettify(f)\n    axes[row, col].set_title(title)\n    axes[row, col].set_xlabel(title)\n    axes[row, col].set_ylabel('Covid / all deaths ratio')\n    \n    if title not in do_log:\n        if specific_week:\n            axes[row, col].scatter(train_data.loc[train_data.week_number==specific_week, f], train_data.loc[train_data.week_number==specific_week, 'ratio'], color='blue')\n        else:\n            axes[row, col].scatter(train_data[f], train_data.ratio, color='blue')\n    else:\n        title = f\"{title} (log scale)\"\n        axes[row, col].set_title(title)\n        axes[row, col].set_xlabel(title)\n        if specific_week:\n            axes[row, col].scatter(np.log(train_data.loc[train_data.week_number==specific_week, f]), train_data.loc[train_data.week_number==specific_week, 'ratio'], color='blue')\n        else:\n            axes[row, col].scatter(np.log(train_data[f]), train_data.ratio, color='blue')\n    col = 1 if col == 0 else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time \nx_val = lgb_model.all_data[0]['x_val']\n\nshap_values = shap.TreeExplainer(lgb_model.model[0]).shap_values(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in fi_both.iloc[0:6]['Feature']:\n    shap.dependence_plot(f, shap_values, x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top10_f = hp_and_d.sort_values('tb_incidence__three_year_average_',ascending=False)[['area_code','ratio','tb_incidence__three_year_average_']].iloc[0:10]\n# top10_f = top10_f.merge(hp_original[['Area Code', 'Area Name', 'Area Type']].drop_duplicates(),  left_on='area_code', right_on='Area Code', how='left')\n# top10_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction errors per region","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"errors = pd.DataFrame({'area_code':lgb_model.area_codes,\n                      'prediction':lgb_model.val_preds,\n                      'truth':lgb_model.val_ys})\n\nerrors.loc[:,'error'] = abs(errors.prediction - errors.truth)\n\nerrors = errors.merge(hp_original[['Area Code', 'Area Name', 'Area Type']].drop_duplicates(),  left_on='area_code', right_on='Area Code', how='left')\n\nerrors = errors.groupby('Area Name').mean()\n\nerrors = errors.sort_values('error',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errors.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errors.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}