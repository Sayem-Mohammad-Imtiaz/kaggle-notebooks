{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)  \npd.set_option('display.max_rows', None)  \n# pd.set_option('display.', None)  \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport collections\nimport operator\nimport itertools\nfrom functools import reduce\nimport re\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airports_df = pd.read_csv('/kaggle/input/flight-delays/airports.csv')\nairports_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flights_df = pd.read_csv('/kaggle/input/flight-delays/flights.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flights_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flights_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Step 1) Relevant columns are filtered"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df = flights_df[['YEAR','MONTH','DAY','DAY_OF_WEEK','AIRLINE','FLIGHT_NUMBER','ORIGIN_AIRPORT','DESTINATION_AIRPORT',\n                          'DEPARTURE_DELAY','ARRIVAL_DELAY','CANCELLED','DIVERTED']]\nmy_flights_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2) Cancelled flights are removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df = my_flights_df[my_flights_df['CANCELLED']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3) Diverted flights are removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df = my_flights_df[my_flights_df['DIVERTED']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4) Less-than-zero delays are converted to zero"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we don't consider negative delay as delay, so we replace 0 for delays < 0.\nmy_flights_df['ARRIVAL_DELAY'][my_flights_df['ARRIVAL_DELAY']<0] = 0\nmy_flights_df['DEPARTURE_DELAY'][my_flights_df['DEPARTURE_DELAY']<0] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 5) Arrival delays and departure delays are combined"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df['DELAY'] = my_flights_df['ARRIVAL_DELAY'] + my_flights_df['DEPARTURE_DELAY']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 6) Flights with no delay are removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df = my_flights_df[my_flights_df['DELAY']!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 7) 'Cancelled' and 'Diverted' columns are removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see that these two columns are always zero in this resulting dataframe, so we drop them\nmy_flights_df = my_flights_df.drop(['CANCELLED','DIVERTED'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 8) Destination and origin city and state of the flight is added"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df = my_flights_df.merge(airports_df, how='left',left_on='ORIGIN_AIRPORT', \n                                    right_on='IATA_CODE').drop(['IATA_CODE','AIRPORT','COUNTRY','LATITUDE','LONGITUDE'], axis=1)\nmy_flights_df = my_flights_df.rename({'CITY':'ORIGIN_CITY','STATE':'ORIGIN_STATE'}, axis=1)\nmy_flights_df = my_flights_df.merge(airports_df, how='left',left_on='DESTINATION_AIRPORT', \n                                    right_on='IATA_CODE').drop(['IATA_CODE','AIRPORT','COUNTRY','LATITUDE','LONGITUDE'], axis=1)\nmy_flights_df = my_flights_df.rename({'CITY':'DESTINATION_CITY','STATE':'DESTINATION_STATE'}, axis=1)\nmy_flights_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 9) Null values are checked"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 10) Flights with no origin/departure city are removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are some airports that their city is not provided, we remove these rows too\nmy_flights_df = my_flights_df[~(my_flights_df['ORIGIN_CITY'].isnull() | my_flights_df['DESTINATION_CITY'].isnull())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 11) Interstate flights are removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to reduce the size of the dataset, only intrastate flights are condidered\nmy_flights_df = my_flights_df[my_flights_df['ORIGIN_STATE']!=my_flights_df['DESTINATION_STATE']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 12) \"O_D\" column is added"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df['O_D']= my_flights_df['ORIGIN_STATE']+'_'+my_flights_df['DESTINATION_STATE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 13) The size of the dataset is reduced"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(10)\nbefore_size = my_flights_df.shape[0]\n# del_rate = 0.99\n# remove_n = int(del_rate * my_flights_df.shape[0])\n# remove_n = 10000\nremainder = 4000\nremove_n = before_size - remainder\ndrop_indices = np.random.choice(my_flights_df.index, remove_n, replace=False)\ntruncated_flights_df = my_flights_df.drop(drop_indices)\nafter_size = truncated_flights_df.shape[0]\nprint('size changed from {} to {}'.format(before_size, after_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"truncated_flights_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"(my_flights_df['DESTINATION_STATE'].value_counts()+my_flights_df['ORIGIN_STATE'].value_counts()).sort_values(ascending=False).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(my_flights_df['DESTINATION_CITY'].value_counts()+my_flights_df['ORIGIN_CITY'].value_counts()).sort_values(ascending=False)[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(my_flights_df['ORIGIN_STATE'].value_counts()+my_flights_df['DESTINATION_STATE'].value_counts()).plot(kind='bar', figsize=(15, 4), title='Origin State of the Flights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df['DESTINATION_STATE'].value_counts().plot(kind='bar', figsize=(15, 4), title='Destination State of the Flights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df['DESTINATION_CITY'].value_counts()[:50].plot(kind='bar', figsize=(15, 4), title='Destination City of the Flights (top 50)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df['ORIGIN_CITY'].value_counts()[:50].plot(kind='bar', figsize=(15, 4), title='Origin City of the Flights (top 50)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(my_flights_df['DESTINATION_CITY'].value_counts()+my_flights_df['DESTINATION_CITY'].value_counts())[:50].plot(kind='bar', figsize=(15, 4), title='Involved Cities of the Delayed Flights (top 50)')\nplt.ylabel('# of delayed flights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.groupby('MONTH').count()['YEAR'].plot(kind='bar')\nplt.ylabel('# delayed flights')\nplt.title('# of delayed flights each month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.groupby('DAY_OF_WEEK').count()['YEAR'].plot(kind='bar')\nplt.ylabel('# delayed flights')\nplt.title('# of delayed flights in different days of week')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.groupby('DAY').count()['YEAR'].plot(kind='bar')\nplt.ylabel('# delayed flights')\nplt.title('# of delayed flights in different days of month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Here is top ten destination-origin pair flights')\nprint(my_flights_df['O_D'].value_counts()[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\nmy_flights_df.to_csv(r'flights.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'flights.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_flights_df.to_csv(r'flights_4000.csv')\nFileLink(r'flights_4000.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequent itemset mining for the truncated dataset"},{"metadata":{},"cell_type":"markdown","source":"### Making the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# making the dataset\ngp = truncated_flights_df.groupby('MONTH')\ndataset = []\nfor i in list(truncated_flights_df['MONTH'].value_counts().sort_index().index):\n    lst = (gp.get_group(i)['O_D']).tolist()\n    dataset.append(lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install mlxtend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from mlxtend.preprocessing import TransactionEncoder\n# from mlxtend.frequent_patterns import fpgrowth\n# from mlxtend.frequent_patterns import association_rules\n\n# te = TransactionEncoder()\n# te_ary = te.fit(dataset).transform(dataset)\n# df = pd.DataFrame(te_ary, columns=te.columns_)\n# res = fpgrowth(df, min_support=1, use_colnames=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# association_rules(res, metric=\"confidence\", min_threshold=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting frequent itemsets and association rules"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficient-apriori","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from efficient_apriori import apriori\nitemsets, rules = apriori(dataset, min_support=1,  min_confidence=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"note that itemsets is in the format of:  \n{1: {('a',): 3, ('b',): 2, ('c',): 1},  \n    ...             2: {('a', 'b'): 2, ('a', 'c'): 1}}  "},{"metadata":{},"cell_type":"markdown","source":"### Putting all itemsets in a list"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_itemsets_dicts = []\nfor i in range(1, len(itemsets)):\n  my_itemsets_dicts.append(itemsets[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_itemsets = []\nfor d in my_itemsets_dicts:\n  my_itemsets.append(list(d.keys()))\n# flattening the list\nmy_itemsets = [item for sublist in my_itemsets for item in sublist]\nprint('There are {} frequent itemsets'.format(len(my_itemsets)))\nprint('There are {} rules'.format(len(rules)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mining Maximal Itemsets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_power_of_two(n):\n    \"\"\"Returns True iff n is a power of two.  Assumes n > 0.\"\"\"\n    return (n & (n - 1)) == 0\n\ndef get_maximal_subsets(sequence_of_sets):\n    \"\"\"Return a list of the elements of `sequence_of_sets`, removing all\n    elements that are subsets of other elements.  Assumes that each\n    element is a set or frozenset and that no element is repeated.\"\"\"\n    # The code below does not handle the case of a sequence containing\n    # only the empty set, so let's just handle all easy cases now.\n    if len(sequence_of_sets) <= 1:\n        return list(sequence_of_sets)\n    # We need an indexable sequence so that we can use a bitmap to\n    # represent each set.\n    if not isinstance(sequence_of_sets, collections.Sequence):\n        sequence_of_sets = list(sequence_of_sets)\n    # For each element, construct the list of all sets containing that\n    # element.\n    sets_containing_element = {}\n    for i, s in enumerate(sequence_of_sets):\n        for element in s:\n            try:\n                sets_containing_element[element] |= 1 << i\n            except KeyError:\n                sets_containing_element[element] = 1 << i\n    # For each set, if the intersection of all of the lists in which it is\n    # contained has length != 1, this set can be eliminated.\n    out = [s for s in sequence_of_sets\n           if s and is_power_of_two(reduce(\n               operator.and_, (sets_containing_element[x] for x in s)))]\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maximal_itemsets = get_maximal_subsets(my_itemsets)\nprint('There are {} maximal frequent itemsets'.format(len(maximal_itemsets)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maximal_itemsets[-10:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maximal_itemsets[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bootstrap Sampling for Confidence Interval"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sample(data_df, size):\n    \"\"\"\n    provides a bootstrap sample of the dataset\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        dataset that the sample is drawn from\n    size : Integer\n        size of the sample\n    \"\"\"\n    chosen_indices = np.random.choice(data_df.index, size, replace=True)\n    sample_flights_df = my_flights_df.loc[chosen_indices]\n    return sample_flights_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_df_to_dataset(data_df):\n    \"\"\"\n    Converts a DataFrame to a list of baskets, the baskets are months and the items are the fligths\n\n    Parameters\n    ----------\n    data_df : DataFrame\n        dataframe to be converted to the dataset\n    \"\"\"\n    gp = data_df.groupby('MONTH')\n    dataset = []\n    for i in list(data_df['MONTH'].value_counts().sort_index().index):\n        lst = (gp.get_group(i)['O_D']).tolist()\n        dataset.append(lst)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_rsupport(sample_dataset, pattern):\n    \"\"\"\n    computes the support of a pattern in the given dataset\n\n    Parameters\n    ----------\n    sample_df : List\n        sample dataset that is a list of baskets\n    pattern : Tuple\n        pattern to be found\n    \"\"\"\n    support = 0\n    for item in sample_dataset:\n        if set(pattern).issubset(item):\n            support = support + 1\n    return support/len(sample_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lower_bound(cumlative_dist, lower_bound_index):\n    lower_bound = 0\n    for item in cumlative_dist:\n        if item < lower_bound_index:\n            continue\n        lower_bound = cumlative_dist[cumlative_dist == item].index[0]\n        break\n    return lower_bound","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_upper_bound(cumlative_dist, upper_bound_index):\n    upper_bound = 0\n    for item in cumlative_dist.sort_values(ascending=False):\n        if item > upper_bound_index:\n            continue\n        upper_bound = cumlative_dist[cumlative_dist == item].index[0]\n        break\n    return upper_bound","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_rsupport_bound(dataset_df, pattern, k=50, n =2000, alpha=0.99):\n    lower_bound_index = (1-alpha)/2\n    upper_bound_index = (1+alpha)/2\n    rsupport = []\n    for i in range(k):\n        sample_df = get_sample(dataset_df, n)\n        sample_dataset = convert_df_to_dataset(sample_df)  \n        rsupport.append(compute_rsupport(sample_dataset, pattern))\n\n    cumlative_dist = (1/k) * np.cumsum(pd.Series(rsupport).value_counts().sort_index())\n    rsup_bound = get_lower_bound(cumlative_dist, lower_bound_index), get_upper_bound(cumlative_dist, upper_bound_index)\n    return rsup_bound","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_rsupport_bound(my_flights_df, maximal_itemsets[0], k=500, n=2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bounds = []\nfor itemset in maximal_itemsets[:20]:\n    bounds.append((itemset,get_rsupport_bound(my_flights_df, itemset, n=5000)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taking itemsets with the highest mean bounds\nitem_mean_bound = {}\nfor item in bounds:\n    item_mean_bound.update({item[0]: 0.5*(item[1][0]+ item[1][1])})\n# sorting the dictionary\nitem_mean_bound = {k: v for k, v in sorted(item_mean_bound.items(), key=lambda item: item[1], reverse=True)}\ndict(itertools.islice(item_mean_bound.items(), 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Influential Flights\nflights that appear alone at the right side of frequent association rules and the left side has the maximum (7) number of flights"},{"metadata":{"trusted":true},"cell_type":"code","source":"rhs_lens = []\nfor rule in rules:\n    rhs_lens.append(len(rule.rhs))\nmax_consequent = np.max(rhs_lens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"influential_rules = []\nfor rule in rules:\n    if len(rule.lhs) == 1 and len(rule.rhs) == max_consequent:\n        influential_rules.append(rule)\nprint('There are {} influential rules'.format(len(influential_rules)))\ninfluential_rules[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"influential_flights = []\nfor rule in influential_rules:\n    influential_flights.append(rule.rhs)\ninfluential_flights = np.unique(influential_flights)\nprint('There are {} influential flights'.format(len(influential_flights)))\ninfluential_flights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Influential States\nthe most common states in the influential flights are influential states"},{"metadata":{"trusted":true},"cell_type":"code","source":"influential_states = []\nstates = []\nflatten = lambda l: [item for sublist in l for item in sublist]\nfor flight in influential_flights:\n    states.append(re.split(\"_\", flight))\nstates = flatten(states)\n# finding top 5 \npd.Series(states).value_counts()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(states).value_counts()[:5].index","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}