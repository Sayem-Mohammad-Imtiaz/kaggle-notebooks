{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## January Flight Delays - Decision Tree Models\nThis notebook is an early analysis of the January Flight Delay Prediction.  I used a Decision Tree Analysis to build a predictive model to predict the amount of delays on the arrival destination.  Some parts may run slow, due to one hot encoding of a very large dataset.  Feel free to add any comments or issues. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%  Read both csv files\n## Reading csv files and adding them\nfilepath1 = os.path.join(dirname, filenames[0])\nfilepath2 = os.path.join(dirname, filenames[1])\ndf1 = pd.read_csv(filepath1)\ndf2 = pd.read_csv(filepath1)\nprint(df1.shape)\nprint(df2.shape)\ndf = pd.concat([df1, df2], ignore_index=True)\nprint(df.info())\ndel(df1)\ndel(df2)\nseed = 1001","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Picking relevant columns\nI want do drop some columns from the data frame as many columns are redundant.  Here are some of the columns/notes\n- 'OP_UNIQUE_CARRIER' and 'OP_CARRIER' are redundant were eliminated.\n- 'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID' are redundant and mean the same thing, used Origin instead.\n- 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID' are redundant used 'DEST' insead\n- Did not use 'DEP_TIME_BLK', instead rounded the departure times and arrival times down in a further section.\n- 'Unnamed: 21' not used and removed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"column1 = ['DAY_OF_MONTH', 'DAY_OF_WEEK','OP_CARRIER_AIRLINE_ID', 'TAIL_NUM','OP_CARRIER_FL_NUM',\n 'DEST','DEP_TIME','DEP_DEL15', 'ARR_TIME', 'ARR_DEL15', \n 'CANCELLED','DIVERTED', 'DISTANCE']\n\n\ndf = df[column1]\n\n# Drops rows with na values from dataframe.\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data set has a slight class imbalance, as there are less delays than not delays, however I think a model can still run and predict well. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Data set has a class imbalance.  There are 921482 flights that weren't delayed, however \n## there were only 210444 flights that were delayed.  I have tried to add more balance by downsampling the \n## number of not delayed classes.\n\nnot_delay = df[df['ARR_DEL15']==0]\ndelay = df[df['ARR_DEL15']==1]\n\nprint(len(not_delay))\nprint(len(delay))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding for Column Tail Number\nI assume TAIL_Num is the number to label a plane.  I wanted to break down planes based on the amount of delays they had.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df[['TAIL_NUM', 'ARR_DEL15']]\ndf2 = df2.groupby(['TAIL_NUM']).sum()\ndf2 = df2.sort_values(by=['ARR_DEL15'], ascending=False)\ndf2.columns = ['tail_delay']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots histogram of the amount of delays each plane has experienced.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(df2['tail_delay'], 8, facecolor='green', alpha=0.75)\nplt.xlabel('Number of Flight Delays')\nplt.ylabel('Counts')\nplt.title(r'Histogram of flight delays a plane has')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the histogram, I decided to create a column tail_delay which groups the planes on how many delays they had. I split into 7 columns, where 6 is a high number of delays and 0 is low number of delays.  ALso I rounded the columns DEP_TIME and ARR_TIME.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Converts TAIL_NUM into a category from 1-6 based on the amount of delays the plane has\ndf2['tail_delay'] = np.floor(df2['tail_delay']/20)\ndf = pd.merge(df, df2, on='TAIL_NUM')\ndel(df2)\ndf.drop(columns = ['TAIL_NUM'], inplace=True)\n\n## rounds departure time down to the nearest hour. Example 6:15 or 6:45 would be both be rounded to 6.\ndf['DEP_TIME'] = round((df['DEP_TIME']/100), 0)\ndf['DEP_TIME']  = df['DEP_TIME'].astype(int)\ndf['ARR_TIME'] = round((df['ARR_TIME']/100), 0)\ndf['ARR_TIME']  = df['ARR_TIME'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding for Column DEST\nWanted to do a similar strategy for the column DEST which is the departure city.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ncolumn = 'DEST'\ndf2 = df[[column, 'ARR_DEL15']]\ndf2 = df2.groupby(column).sum()\ndf2 = df2.sort_values(by=['ARR_DEL15'], ascending=False)\ndf2.columns = ['city_delay']\n\n##plt.boxplot(df2['delay_counts'], notch=True)\nplt.hist(df2['city_delay'], 8, facecolor='green', alpha=0.75)\nplt.xlabel('Number of Flight Delays')\nplt.ylabel('Counts')\nplt.title(r'Histogram of Flight Delays based on Destination Airport')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['city_delay'] = np.floor(df2['city_delay']/1000)\ndf = pd.merge(df, df2, on=column)\ndel(df2)\ndf.drop(columns = column, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Hot Encoding Code\nThe column 'OP_CARRIER_AIRLINE_ID' is a categorical columns.  I chose to use one hot encoding to convert the columns\nin a usable format. Label encoding assigns a number value to each value in the column. For example one airline ID '20363' would have a value of 1\nand airline id '19805' would have a value of 2.  However the issue is that the model may infer a numerical relationship between \nthese values so it is easier to use One Hot Encoding to convert each variable into a feature/column. column2 is a list containing the names of all the columns to convert using one hot encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\n\ncolumn2 = ['OP_CARRIER_AIRLINE_ID', 'tail_delay', 'city_delay']\n\nfor col in column2:\n    enc = OneHotEncoder(sparse = False, handle_unknown='ignore')\n    encoded_frame = enc.fit_transform(np.array(df[col]).reshape(-1,1))\n    column_name = enc.get_feature_names()\n    column_name = col + column_name\n    one_hot_encoded_frame =  pd.DataFrame(encoded_frame, columns= column_name)\n    \n    df = pd.concat([df, one_hot_encoded_frame], axis=1)\n    del(encoded_frame)\n    del(one_hot_encoded_frame)\n    df.drop(columns = col, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## splitting data into training and test sets.\nUsed 80/20 split for training and evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%    splitting data into training and test sets.\n## 80/20 split for training and evaluation\nsplit   = 0.8 \nx_train = df.sample(frac=split, random_state=seed) \nx_test  = df.drop(x_train.index)\nprint(x_train.shape, x_test.shape)\n\n# Extract and remove the label (to be predicted) set\ny_train = x_train.pop('ARR_DEL15')\ny_test  = x_test.pop('ARR_DEL15')\nprint(y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Decision Tree Model\nDecision trees are a good model to use because they are easy to implement and understand the results.  They work by splitting/partitioning the data based on features that have the lowest misclassification rate of the predicted value.\nOnce the data is split enough, it will make a final decision based on the rules and probabilities given by the tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state = 0)\nclf.fit(x_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Graphing Decision Tree\nI used GraphViz package to plot the Decision tree as it seemed cleaner and neater than just plotting through matplotlib.  If running on your local machine you would need to pip install the GraphViz library and add to your PATH variable on Windows. I also set a max-depth to 3 for simplicity, so it doesn't show the entire tree, just the first three splits on the tree.  \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom sklearn.externals.six import StringIO  \nimport pydotplus\nfrom IPython.display import Image \ndot_data = StringIO()\nfrom sklearn import tree\n\ntree.export_graphviz(clf, max_depth = 3, feature_names=x_test.columns, out_file= dot_data)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \ngraph.write_png('tree.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the tree below, the most important feature is if the plane departed late 'DEP_DEL15'.  After that split is made, the airline '19393' was probably an airline which had a lot of delays.  Then the features that split with lowest misclassification, was the day of the month and distance of the flight. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation\nFrom sklearn I am using a confusion matrix, which helps to evaluate the performance of classification models.  Notes are below\n* Positive (P) : Observation is positive, flight was delayed on arrival\n* Negative (N) : Observation is not positive, flight was not delayed on arrival\n* recall - ratio of correctly classified positive examples over total number of positive examples. recall = TP/(TP + FN)\n* precision – ratio of correctly classified positive examples over total number of predicted positive examples. \n* Precision = TP(TP + FP).\n* accuracy - ratio of correctly classified predictions over all of the predictions.  accuracy = (TP + TN)/(TP+TN+FP+FN)\n* F1 score – helps measure both precision and recall at the same time.  \n* F-measure = (2*recall * precision)/(recall + precision)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![image.png](attachment:image.png)","attachments":{"image.png":{"image/png":"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"}},"execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \n  \ny_predict = clf.predict(x_test)\ncmatrix = confusion_matrix(y_test, y_predict) \naccuracy = accuracy_score(y_test, y_predict) \nclassification_report = classification_report(y_test, y_predict) \n\nprint(cmatrix)\nprint(classification_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##Cross Validation to find true accuracy of model\n\nI Used cross validation package from sk-learn to run k-fold cross validation for 5 folds and append the scores to list results.  It will test the model several times across a subset of data, however I specified 5 subsets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\n\nx_test2 = df\ny_test2 = x_test2.pop('ARR_DEL15')\nscoring='f1_macro'\nscores = cross_validate(clf, x_test2, y_test2, cv=5, scoring = ['accuracy', 'f1'])\nresults = list()\nresults.append([scores['test_accuracy'].mean(), scores['test_f1'].mean()])\nprint('The average accuracy is %f and the average F1 score is %f' %((results[0][0]),(results[0][1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### ROC Curve\nReciever Operator Curves are a good way to illustrate a classification model, and plots the true positive rate against the false positive rate. The closer the graph is to the top and left-hand borders, the more accurate the test. The output was not the probabilities beween 0 and 1, so the ROC curvve isn't as smooth as compared to other models. The greater the area under the curve the more accurate the model is, with a uppder limit of 1, and .5 meaning it doesn'treally predict anything. The area under the curve is .96 which is good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nprobs = clf.predict_proba(x_test)\nprobs = probs[:, 1]\nauc = roc_auc_score(y_test, probs)\nprint('AUC: %.2f' % auc)\nfpr, tpr, thresholds = roc_curve(y_test, probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(fpr, tpr, color='orange', label='ROC')\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Conclusion this is my notebook on using a Decision Tree approach to the Flight Prediction Dataset. You could get higher accuracy using Random Forest models and boosted tree algorithms> However Decision Tree algorithms are a good way to start to visualize and interpret which variables that are important. I got a somewhat higher accuracy running my code on the kaggle than I did on my local machine. I cannot explain why.\n\nThis is my first Kaggle notebook, so I welcome any feedback or suggestions. Thanks for reading!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}