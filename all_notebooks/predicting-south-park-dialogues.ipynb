{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9efd5f2b-b016-46cd-6da5-d952e8c92ff9"},"source":"## Objective: using random word inputs, predict which South Park character is speaking from a list of top characters\nData source: https://www.kaggle.com/tovarischsukhov/southparklines"},{"cell_type":"markdown","metadata":{"_cell_guid":"22597d8e-78b9-4b79-86f9-24eb5d5e2438"},"source":"## Import libraries"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c6cb82d-79de-fd8b-a872-23dcf514584c"},"outputs":[],"source":"import numpy as np\nimport matplotlib as plt\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.pipeline import make_pipeline"},{"cell_type":"markdown","metadata":{"_cell_guid":"285ef778-7a99-85ad-3cf6-50de416fd637"},"source":"---"},{"cell_type":"markdown","metadata":{"_cell_guid":"19b0094a-a10b-6769-6f2e-04484139ae5e"},"source":"## Import dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21f6cdd3-2ef1-9792-ff5c-aae4457a694e"},"outputs":[],"source":"South_Park_raw = pd.read_csv('../input/All-seasons.csv')\nSouth_Park_raw.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"976b8942-9103-ae3d-8db2-984c886f81dd"},"outputs":[],"source":"# Head and shape of dataset\nprint(South_Park_raw.head())\nprint(South_Park_raw.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"425a7625-30f7-2c9f-6f1d-91fc5b3e004c"},"outputs":[],"source":"print (South_Park_raw.describe())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ba5c2d8-ebf7-3215-d3b2-f54a32c8717a"},"outputs":[],"source":"#Select just speakers with more than 500 lines\n\ntop_speakers = South_Park_raw.groupby(['Character']).size().loc[South_Park_raw.groupby(['Character']).size() > 500]\nprint (top_speakers.sort_values(ascending=False))\n\n#Select rows top speakers   \n\"\"\" This is the dataset we will be working with\"\"\"\n\nmain_char_lines = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\ndel main_char_lines['Season']\ndel main_char_lines['Episode']\n\nmain_char_lines = main_char_lines.reset_index(drop=True)\n\nprint (main_char_lines.describe())"},{"cell_type":"markdown","metadata":{"_cell_guid":"8b18f420-eb10-0997-8b40-1a9958c4974e"},"source":"---"},{"cell_type":"markdown","metadata":{"_cell_guid":"fe4f6db6-d168-f299-1fa1-8ba0be567e55"},"source":"## Define train and test datasets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bca4ce43-7849-6da5-8166-1229b370e289"},"outputs":[],"source":"# define X and y\nX = main_char_lines.Line\ny = main_char_lines.Character\n\n#print (y.value_counts(normalize=True))\n\n# split the new DataFrame into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"21057699-e135-406b-d51d-da4486412034"},"source":"---"},{"cell_type":"markdown","metadata":{"_cell_guid":"a7f4fe92-aee7-12a9-f9fb-d84f06f03967"},"source":"## Search for best parameters to use in model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86b2abcc-d279-6be5-5981-1dbb0ad5f218"},"outputs":[],"source":"#pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n#pipe.steps\n\n#param_grid = {}\n#param_grid[\"tfidfvectorizer__max_features\"] = [500, 1000, 15000]\n#param_grid[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n#param_grid[\"tfidfvectorizer__lowercase\"] = [True, False]\n#param_grid[\"tfidfvectorizer__stop_words\"] = [\"english\", None]\n#param_grid[\"tfidfvectorizer__strip_accents\"] = [\"ascii\", \"unicode\", None]\n#param_grid[\"tfidfvectorizer__analyzer\"] = [\"word\", \"char\"]\n#param_grid[\"tfidfvectorizer__binary\"] = [True, False]\n#param_grid[\"tfidfvectorizer__norm\"] = [\"l1\", \"l2\", None]\n#param_grid[\"tfidfvectorizer__use_idf\"] = [True, False]\n#param_grid[\"tfidfvectorizer__smooth_idf\"] = [True, False]\n#param_grid[\"tfidfvectorizer__sublinear_tf\"] = [True, False]\n\n#grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n\n#Helpful for understanding how to create your param grid.\n#grid.get_params().keys()"},{"cell_type":"markdown","metadata":{"_cell_guid":"73445647-0c07-4e20-689b-0af6bdeafbd9"},"source":"#### (This can take a while to run)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ce4422f-bd30-b564-da7f-3af0f47ca6d5"},"outputs":[],"source":"#grid.fit(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd554197-ef51-c26c-6b5c-f6cc41cb0885"},"outputs":[],"source":"#print(grid.best_params_)\n#print(grid.best_score_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"64027bc8-1f5a-efab-baa9-52b00dda3787"},"source":"---"},{"cell_type":"markdown","metadata":{"_cell_guid":"a1cb84c4-c32f-abdf-0c41-b51cd7355253"},"source":"## Define Model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c750631-a3b1-9ca1-985f-aefb4b5413f5"},"outputs":[],"source":"vect = TfidfVectorizer(analyzer='word', stop_words='english', max_features = 850, ngram_range=(1, 1), \n                       binary=False, lowercase=True, norm=None, smooth_idf=True, strip_accents=None,\n                       sublinear_tf=True, use_idf=False)\n\nmcl_transformed = vect.fit_transform(X)\n\nnb_SP_Model = MultinomialNB()\nnb_SP_Model.fit(mcl_transformed, y)\nprint (\"Model accuracy within dataset: \", nb_SP_Model.score(mcl_transformed, y))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7a03707-cc37-a02e-8a54-ffb4d8bdbd38"},"outputs":[],"source":"print (\"Model accuracy with cross validation:\", cross_val_score(MultinomialNB(), mcl_transformed.toarray(), \n                                                                y, cv=5, scoring=\"accuracy\").mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"2dc292f5-b462-8e74-eb71-5c3c715decbb"},"source":"---"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c6ba53e-94d9-47fd-aa87-7ccf41a874b0"},"source":"## Test Model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"753de365-199f-4de1-c1be-d69ae4e660f8"},"outputs":[],"source":"# Predict on new text\nnew_text = [\"Well, I guess we'll have to roshambo for it. I'll kick you in the nuts as hard as I can, then you kick me square in the nuts as hard as you can...\"]\nnew_text_transform = vect.transform(new_text)\n\nprint (nb_SP_Model.predict(new_text_transform),\" most likely said it.\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"b2cfd192-f38e-6e71-384d-6d620b1a186d"},"source":"##### Table with Characters' Line likelihood"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52476dbf-148e-601e-e408-59c224230097"},"outputs":[],"source":"SP_prob=pd.DataFrame(nb_SP_Model.predict_proba(new_text_transform))\nSP_prob=pd.DataFrame.transpose(SP_prob)\nSP_prob.columns = ['Likelihood']\n\ntop_speakers_index = top_speakers.reset_index()\ntop_speakers_index.columns = ['Character', 'Lines']\ntop_speakers_index = top_speakers_index.drop('Lines', 1)\n\nResult = pd.concat([top_speakers_index, SP_prob], axis=1)\n\nprint (Result.sort_values('Likelihood',ascending=False))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5cbd0ec8-5036-f6e3-289a-bff8fbf8a976"},"source":"---"},{"cell_type":"markdown","metadata":{"_cell_guid":"ad971a71-8424-3884-9e9f-54565ac0263b"},"source":"---"},{"cell_type":"markdown","metadata":{"_cell_guid":"d843b38d-8ac4-7f8c-8a73-3bb01eab7b1f"},"source":"## Just for fun:\n\n## Calculate \"spamminess\" for the top 3 characters: Cartman, Stan and Kyle\n### Used to test common words pertaining to these characters more than to others"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f2408b2-8a16-6175-1518-065a85d59c26"},"source":"#### Calculate \"spaminess\" for Cartman with detailed coding (top 10 words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1366441-714b-8afc-7cf9-759cf148055b"},"outputs":[],"source":"cartman = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\ndel cartman['Season']\ndel cartman['Episode']\n\ncartman.Character[cartman.Character != 'Cartman'] = 'Not Cartman'\ncartman.Character[cartman.Character == 'Cartman'] = 'Cartman'\nprint (cartman)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1a3952e-3fd3-c9b6-6373-694cb4dfe29f"},"outputs":[],"source":"cartman.Character.value_counts(normalize=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"047b09fb-1dcf-4162-dd8e-d8e41dc6f6fa"},"outputs":[],"source":"X_cartman = cartman.Line\ny_cartman = cartman.Character\nvect_cartman =CountVectorizer(stop_words='english')\nXdtm_cartman = vect_cartman.fit_transform(X_cartman)\nnb_cartman = MultinomialNB()\nnb_cartman.fit(Xdtm_cartman,y_cartman)\nnb_cartman.score(Xdtm_cartman,y_cartman)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71e6617c-7ee2-35e8-e3d6-fe771861f1dd"},"outputs":[],"source":"tokens_cartman = vect_cartman.get_feature_names()\nlen(tokens_cartman)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc195704-275f-de07-f2a5-5c59a6294465"},"outputs":[],"source":"print (vect_cartman.get_feature_names()[:50])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6257a24-8026-e453-50d6-924629ca6d8d"},"outputs":[],"source":"nb_cartman.feature_count_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c012d1d1-3199-9dfc-1ad0-d0e9157f1fd3"},"outputs":[],"source":"nb_cartman.feature_count_.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"686660cf-53ec-eaeb-8800-27dec3751f19"},"outputs":[],"source":"token_count_cartman= nb_cartman.feature_count_[0,:]\ntoken_count_cartman"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e34883a8-d7a1-6e9e-cd70-130db993e7ba"},"outputs":[],"source":"token_count_not_cartman = nb_cartman.feature_count_[1, :]\ntoken_count_not_cartman"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22b452c8-7aab-9d0d-da65-ec9ac9e0fe06"},"outputs":[],"source":"# create a DataFrame of tokens with their separate Not-Cartman and Cartman counts\ncartman_tokens = pd.DataFrame({'token':tokens_cartman, 'Cartman':token_count_cartman, 'Not_Cartman':token_count_not_cartman}).set_index('token')\ncartman_tokens.sample(10, random_state=3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c77c3816-0e21-2f49-2504-0ed0a370a45b"},"outputs":[],"source":"# add 1 to Cartmen and Not Cartman counts to avoid dividing by 0\ncartman_tokens['Cartman'] = cartman_tokens.Cartman + 1\ncartman_tokens['Not_Cartman'] = cartman_tokens.Not_Cartman + 1\ncartman_tokens.sample(10, random_state=3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05bc6e66-bd0d-8e9f-5998-6252b6b1e532"},"outputs":[],"source":"# Naive Bayes counts the number of observations in each class\nnb_cartman.class_count_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04b0dd3b-1d50-6f26-ca38-4ee11cc0e17b"},"outputs":[],"source":"# convert the Cartman and Not Cartman counts into frequencies\ncartman_tokens['Cartman'] = cartman_tokens.Cartman / nb_cartman.class_count_[0]\ncartman_tokens['Not_Cartman'] = cartman_tokens.Not_Cartman / nb_cartman.class_count_[1]\ncartman_tokens.sample(10, random_state=3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc99406d-a538-57aa-60f0-f11d8340a83b"},"outputs":[],"source":"# calculate the ratio of Cartman-to-Not_Cartman for each token\ncartman_tokens['spam_ratio'] = cartman_tokens.Cartman / cartman_tokens.Not_Cartman\ncartman_tokens.sample(10, random_state=3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b90fd0b-7dd2-1ce4-929f-bf8a6d85094d"},"outputs":[],"source":"# examine the DataFrame sorted by spam_ratio\ncartman_tokens.sort_values('spam_ratio', ascending=False).head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcec45ed-6c4c-a13a-3123-e5c14c607d75"},"outputs":[],"source":"#Try looking up scores of different words\nword = \"nyah\"\ncartman_tokens.loc[word, 'spam_ratio']"},{"cell_type":"markdown","metadata":{"_cell_guid":"68268924-a31c-629a-dbc2-546c9674f29e"},"source":"#### \"Spamminess\" for Stan (top 10 words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3bc58ed-2cdb-35b6-21d6-026acb1caddf"},"outputs":[],"source":"stan = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\ndel stan['Season']\ndel stan['Episode']\n\nstan.Character[stan.Character != 'Stan'] = 'Not Stan'\nstan.Character[stan.Character == 'Stan'] = 'Stan'\n\nX_stan = stan.Line\ny_stan = stan.Character\nvect_stan =CountVectorizer(stop_words='english')\nXdtm_stan = vect_stan.fit_transform(X_stan)\nnb_stan = MultinomialNB()\nnb_stan.fit(Xdtm_stan,y_stan)\nnb_stan.score(Xdtm_stan,y_stan)\n\ntokens_stan = vect_stan.get_feature_names()\n\ntoken_count_stan= nb_stan.feature_count_[0,:]\ntoken_count_not_stan = nb_stan.feature_count_[1, :]\n\nstan_tokens = pd.DataFrame({'token':tokens_stan, 'Stan':token_count_stan, 'Not_Stan':token_count_not_stan}).set_index('token')\n\nstan_tokens['Stan'] = stan_tokens.Stan + 1\nstan_tokens['Not_Stan'] = stan_tokens.Not_Stan + 1\n\nstan_tokens['Stan'] = stan_tokens.Stan / nb_stan.class_count_[0]\nstan_tokens['Not_Stan'] = stan_tokens.Not_Stan / nb_stan.class_count_[1]\n\nstan_tokens['spam_ratio'] = stan_tokens.Stan / stan_tokens.Not_Stan\n\n# examine the DataFrame sorted by spam_ratio\nstan_tokens.sort_values('spam_ratio', ascending=False).head(10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"afbaa2c6-7c52-9673-3d80-a5de3618456d"},"source":"#### \"Spamminess\" for Kyle (top 10 words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91a44ee4-e79d-b0f2-b10f-caa2c62be09e"},"outputs":[],"source":"kyle = pd.DataFrame(South_Park_raw.loc[South_Park_raw['Character'].isin(top_speakers.index.values)])\ndel kyle['Season']\ndel kyle['Episode']\n\nkyle.Character[kyle.Character != 'Kyle'] = 'Not Kyle'\nkyle.Character[kyle.Character == 'Kyle'] = 'Kyle'\n\nX_kyle = kyle.Line\ny_kyle = kyle.Character\nvect_kyle =CountVectorizer(stop_words='english')\nXdtm_kyle = vect_kyle.fit_transform(X_kyle)\nnb_kyle = MultinomialNB()\nnb_kyle.fit(Xdtm_kyle,y_kyle)\nnb_kyle.score(Xdtm_kyle,y_kyle)\n\ntokens_kyle = vect_kyle.get_feature_names()\n\ntoken_count_kyle= nb_kyle.feature_count_[0,:]\ntoken_count_not_kyle = nb_kyle.feature_count_[1, :]\n\nkyle_tokens = pd.DataFrame({'token':tokens_kyle, 'Kyle':token_count_kyle, 'Not_Kyle':token_count_not_kyle}).set_index('token')\n\nkyle_tokens['Kyle'] = kyle_tokens.Kyle + 1\nkyle_tokens['Not_Kyle'] = kyle_tokens.Not_Kyle + 1\n\nkyle_tokens['Kyle'] = kyle_tokens.Kyle / nb_kyle.class_count_[0]\nkyle_tokens['Not_Kyle'] = kyle_tokens.Not_Kyle / nb_kyle.class_count_[1]\n\nkyle_tokens['spam_ratio'] = kyle_tokens.Kyle / kyle_tokens.Not_Kyle\n\n# examine the DataFrame sorted by spam_ratio\nkyle_tokens.sort_values('spam_ratio', ascending=False).head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fbd98658-7641-f803-3552-887485b036bf"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}