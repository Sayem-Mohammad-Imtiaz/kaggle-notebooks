{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n\n# Loading datasets and quick look\n\n\nphone_features = pd.read_csv(\"../input/mobile-price/train.csv\", sep=';')\nphone_features_test = pd.read_csv(\"../input/mobile-price/test.csv\", sep=';')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phone_features.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phone_features.info()\n# as shown at the bottom, there are no null-values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can remove correlated features if they appear. Highly correlated features are limitations towards the proper model creation.\n# If appear, then features can be inseparable so whole dependences could be corrupted. As we can see below there is no such problem - most features are not correlated.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = phone_features.corr()\n\nmap = sns.heatmap(correlation, vmin=-1, vmax=1, center=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PF = phone_features.drop(columns='price_range')\nPF_test = phone_features_test.drop(columns='id')\n\n# Division between training data and testing data\n\nX_train, X_test, y_train, y_test = train_test_split(PF, phone_features['price_range'], test_size=0.3, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MODELS - comparison and selection\n\n# 1: random forest\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(X_train, y_train)\nprediction1 = random_forest.predict(X_test)\nprint(\"Random forest:\")\nprint(classification_report(y_test, prediction1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2: logistic regression\nlr = LogisticRegression(multi_class='ovr', solver='liblinear')\nlr.fit(X_train, y_train)\nprediction2 = lr.predict(X_test)\nprint(\"Logistic regression:\")\nprint(classification_report(y_test, prediction2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3: naive bayes\nNaive_bayes = GaussianNB()\nNaive_bayes.fit(X_train, y_train)\nprediction3 = Naive_bayes.predict(X_test)\nprint(\"Naive Bayes:\")\nprint(classification_report(y_test, prediction3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4: support vector machine\nSupport_vector_machine = SVC()\nSupport_vector_machine.fit(X_train, y_train)\nprediction4 = Support_vector_machine.predict(X_test)\nprint(\"Support vector machine:\")\nprint(classification_report(y_test, prediction4))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing 'n' for nearest neighbors\n\nf1_storage = []\n\nfor i in range(1, 10):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn = knn.fit(X_train, y_train)\n    knn_predict = knn.predict(X_test)\n    print(i, f1_score(y_test, knn_predict, average='weighted'))\n    f1_storage.append(f1_score(y_test, knn_predict, average='weighted'))\n\nnumbers = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n\nplt.bar(numbers, f1_storage)\n\nplt.ylim(0.75, 1)\nfor i in range(len(f1_storage)):\n    plt.text(i, f1_storage[i], str(round(f1_storage[i]*100, 3))+'%', size=8, ha='center', va='bottom')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see above the best 'n' value is 7..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5: KNN\nknn = KNeighborsClassifier(n_neighbors=7)\nknn = knn.fit(X_train, y_train)\nprediction5 = knn.predict(X_test)\nprint(\"KNN:\")\nprint(classification_report(y_test, prediction5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Random_forest_results = f1_score(y_test, prediction1, average='weighted')\nLogistic_regression_results = f1_score(y_test, prediction2, average='weighted')\nNaive_Bayes_results = f1_score(y_test, prediction3, average='weighted')\nSupport_vector_machine_results = f1_score(y_test, prediction4, average='weighted')\nKNN_results = f1_score(y_test, prediction5, average='weighted' )\n\nmodels = [Random_forest_results, Logistic_regression_results, Naive_Bayes_results, Support_vector_machine_results, KNN_results]\nmodel_names = ['RF', 'LR', 'NB', 'SVM', 'KNN']\n\nplt.bar(model_names, models)\nplt.ylim(0.5, 1)\nfor i in range(len(models)):\n    plt.text(i, models[i], str(round(models[i]*100))+'%', size=8, ha='center', va='bottom')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally, we can see that Support Vector Machine algorithm gives the best results and is the best model to predict price range.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The predictions are as follows:\nprint(Support_vector_machine.predict(PF_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}