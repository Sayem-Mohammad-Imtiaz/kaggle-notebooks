{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About the dataset\n* This dataset gives you information about a marketing campaign of a financial institution, which you will have to analyze in order to find ways to look for future strategies in order to improve future marketing campaigns for the bank.\n## Input variables explained:\n### bank client data:<br> \n1 - age (numeric)<br> \n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br> \n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br> \n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')<br> \n5 - default: has credit in default? (categorical: 'no','yes','unknown')<br> \n6 - balance: average yearly balance, in euros (numeric)\n7 - housing: has housing loan? (categorical: 'no','yes','unknown')<br> \n8 - loan: has personal loan? (categorical: 'no','yes','unknown')<br> \n### Related with the last contact of the current campaign:\n9 - contact: contact communication type (categorical: 'cellular','telephone')<br> \n10 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br> \n11 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br> \n12 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br> \n### Other attributes:\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)<br> \n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br> \n15 - previous: number of contacts performed before this campaign and for this client (numeric)<br> \n16 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br> \n## Output variable (desired target):\n17 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n* Deposite definition: What is a Term Deposit?\nA Term deposit is a deposit that a bank or a financial institurion offers with a fixed rate (often better  than just opening deposit account) in which your money will be returned back at a specific maturity time. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Approach\nIn order to optimize marketing campaigns with the help of the dataset, we will have to take the following steps:\n* Import data from dataset and perform initial high-level analysis: look at the number of rows, look at the missing values, look at dataset columns and their values respective to the campaign outcome.\n* Clean the data: remove irrelevant columns, deal with missing and incorrect values, turn categorical columns into dummy variables.\n* Here some categorical columns have values \"unknown\". We are considering it as one category which can influence the deposite status. Hence not removing it.\n* Use machine learning techniques to predict the marketing campaign outcome and to find out factors, which affect the success of the campaign.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import linear algebra and data manipulation libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline \n#import standard visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conda install mglearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install mglearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bank-marketing-dataset/bank.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. DATA PREPROCESSING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Checking for Missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Slicing the dataset instances to 1500","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.sample(n=1500,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df1.sort_index()\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inserting missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Have inserted 10% missing values into the dataset as the dataset is clean","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\ndf2 = df1.mask(np.random.random(df1.shape) < .10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values= df2.isna().mean().round(2)\nmissing_values.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* we can see that there is nearly 10% of missing values in the data, hence lets explore data by categorical and numerical column wise.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Check for missing values in outcome variable-deposit\n* If the deposit variable has missing values, then it is better to do row deletion.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df2.dropna(how='all', subset=['deposit'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fill missing values \n* with most frequent values in categorical columns\n* with mean in numerical columns(as we have only less than 10% of the data is missing, filling with average should not decrease the variance much to deviate our predictions.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat_imputed=df3.select_dtypes(include='object').fillna(df3.select_dtypes(include='object').mode().iloc[0])\ndf_cat_imputed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num_imputed=df3.select_dtypes(exclude ='object').fillna(df3.select_dtypes(exclude='object').mean().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imputed = pd.concat([df_cat_imputed, df_num_imputed], axis=1)\ndf_imputed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imputed.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will check how the categorical columns are distributed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month','poutcome']\nfig, axs = plt.subplots(3, 3, sharex=False, sharey=False, figsize=(20, 15))\ncounter = 0\nfor cat_column in cat_columns:\n    value_counts = df_imputed[cat_column].value_counts()\n    trace_x = counter // 3\n    trace_y = counter % 3\n    x_pos = np.arange(0, len(value_counts))\n    \n    axs[trace_x, trace_y].bar(x_pos, value_counts.values, tick_label = value_counts.index)\n    \n    axs[trace_x, trace_y].set_title(cat_column)\n    \n    for tick in axs[trace_x, trace_y].get_xticklabels():\n        tick.set_rotation(90)\n    counter += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will look at the numerical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns = ['age','balance', 'day','duration', 'campaign', 'pdays', 'previous']\ndf3_num=df_imputed[num_columns]\ndf3_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\nax = fig.gca()\ndf3_num.hist(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see from the above graphs, that balance,campaign, duration, pdays and previous variables have some outliers. lets look at those columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df3_num.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Outliers\nWe can see that duration, pdays have outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imputed.groupby('deposit').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_imputed[df_imputed['pdays'] > 400] ) / len(df_imputed) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_imputed[df_imputed['pdays'] == -1.0])/len(df_imputed)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Box plot to show outliers in pdays","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(df_imputed['pdays'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing Outliers from pdays column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_imputed['pdays'].quantile(0.10))\nprint(df_imputed['pdays'].quantile(0.90))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imputed['pdays'] = np.where(df_imputed['pdays'] >185.0, 185.0,df_imputed['pdays'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Box plot to show outliers in duration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len (df_imputed[df_imputed['duration'] > 1700] ) / len(df_imputed) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(df_imputed['duration'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing Outliers from duration column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_imputed['duration'].quantile(0.10))\nprint(df_imputed['duration'].quantile(0.90))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imputed['duration'] = np.where(df_imputed['duration'] >815.1, 815.1,df_imputed['duration'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Response column(y)-Deposit\nOn the diagram we see that counts for 'yes' and 'no' values for 'deposit' are close, so we can use accuracy as a metric for a model, which predicts the campaign outcome.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts = df_imputed['deposit'].value_counts()\n\nvalue_counts.plot.bar(title = 'Deposit value counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imputed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imputed.groupby('deposit').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heatmap to check correlation between variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(data=df_imputed.corr(), annot=True, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build correlation matrix\ncorr = df_imputed.corr()\ncorr.style.background_gradient(cmap='PuBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the above heatmap, it seems like there is no correlation between input numerical variables, hence we do not need to drop any variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Creating dummies for categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique levels in 'job' variable:\", df_imputed.job.nunique())\nprint(\"Unique levels in 'marital' variable:\", df_imputed.marital.nunique())\nprint(\"Unique levels in 'education' variable:\", df_imputed.education.nunique())\nprint(\"Unique levels in 'default' variable:\", df_imputed.default.nunique())\nprint(\"Unique levels in 'housing' variable:\", df_imputed.housing.nunique())\nprint(\"Unique levels in 'loan' variable:\", df_imputed.loan.nunique())\nprint(\"Unique levels in 'contact' variable:\", df_imputed.contact.nunique())\nprint(\"Unique levels in 'month' variable:\", df_imputed.month.nunique())\nprint(\"Unique levels in 'poutcome' variable:\", df_imputed.poutcome.nunique())\nprint(\"Unique levels in 'deposit' variable:\", df_imputed.deposit.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the information above, we will create one-hot encoding for categorical variables with > 2 levels. So, 'job',  'marital', 'education', 'contact', 'month' and 'poutcome' variables have levels >2.\n* For 'default', 'housing', 'loan' and 'deposit' variables, we use create label encoding as they have just 2 unique levels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy1= pd.get_dummies(df_imputed, columns=['job', 'marital', 'education','contact', 'month','poutcome'],\n               drop_first=False, prefix=['job', 'mar', 'edu', 'con', 'mon', 'pout'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy2=dummy1.replace(to_replace = ['yes','no'],value = ['1','0'])\n#dummy2.info()\ndf_conv=dummy2.copy()\ndf_conv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_conv.deposit\nX = df_conv.drop(['deposit'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For selecting feautres we will use random forest method because it is robust, nonlinear","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nscaled_features = scaler.transform(X)\nX_sc = pd.DataFrame(scaled_features,columns=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators = 50, max_depth = 4,random_state=0)\n\nscores = []\nnum_features = len(X_sc.columns)\nfor i in range(num_features):\n    col = X_sc.columns[i]\n    score = np.mean(cross_val_score(clf, X_sc[col].values.reshape(-1,1), y, cv=10))\n    scores.append((float(score*100), col))\n\nprint(sorted(scores, reverse = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the above Cross value scores of the input variables, lets select the top 15 variables with highest scores\n* So the features selected for the classification modeling are: 'duration', 'pdays', 'pout_success', 'pout_unknown', 'previous','age','con_unknown','job_retired','mar_single','housing','con_cellular','mon_apr','mon_may','job_student', 'mon_sep'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final=X[['duration','pdays','pout_success','pout_unknown','previous',\n            'age','con_unknown','job_retired','mar_single','housing', \n            'con_cellular','mon_apr','mon_may','job_student','mon_sep']]\ndf_final.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Split the data into train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_org, X_test_org, y_train, y_test = train_test_split(df_final,y, random_state = 0)\n\nprint(\"Size of training set: {}  size of test set:\"\n      \" {}\\n\".format(X_train_org.shape[0],X_test_org.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\n* As we can see from the graphs of the input varibles, it is clear that they do not have normally distributed data, hence we are using MinMaxScaling. This will be suitable option as we have also removed the outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train2 = pd.DataFrame(scaler.fit_transform(X_train_org))\nX_test2= pd.DataFrame(scaler.transform(X_test_org))\nX_train2.columns = X_train_org.columns.values\nX_test2.columns = X_test_org.columns.values\nX_train2.index = X_train_org.index.values\nX_test2.index = X_test_org.index.values\nX_train = X_train2\nX_test = X_test2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Checking the balance status of y train data set\\n\",y_train.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the above counts of 0 and 1 values for target variable, it looks not that imbalanced. Hence we can proceed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Grid Search & cross validation applied on classification models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method1: KNN-Grid search with Cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\n\nknn_param_grid = {'n_neighbors':[1,2,3,5,7,10,15,25],\n              'leaf_size':[1,3,5],\n              'algorithm':['auto', 'kd_tree'],\n              'n_jobs':[-1]}\n\n#Fit the model 5-fold cross validation\nKNN_grid = GridSearchCV(knn, knn_param_grid, cv=5)\nbest_knn=KNN_grid.fit(X_train, y_train)\ny_pred = best_knn.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_results_knn = pd.DataFrame([['KNN_GridCV', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nResults_all=model_results_knn\n\nprint(\"Best Train Accuracy score: \",KNN_grid.best_score_)\nprint(\"Best parameters:\", KNN_grid.best_estimator_)\nprint(\"Best Test Accuracy score :\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='param_n_neighbors', y='mean_test_score', data=pd.DataFrame(KNN_grid.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method2: Logistic Regression - Grid search with Cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lreg=LogisticRegression(random_state = 0)\npenalty = ['l1', 'l2']\n# Create regularization hyperparameter space\nC = [0.001,0.01,0.1,0.2,0.8,1.2,1.5]\nhyperparameters = dict(C=C, penalty=penalty)\n\n# Create grid search using 5-fold cross validation\nGS_lreg = GridSearchCV(lreg, hyperparameters, cv=5, verbose=0)\n# Fit grid search\nLR_best_model = GS_lreg.fit(X_train, y_train)\ny_pred = GS_lreg.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_results_lr = pd.DataFrame([['Logistic Regression_GridCV', acc, prec, rec, f1]],columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nResults_all = Results_all.append(model_results_lr, ignore_index = True)\n\nprint(\"Best Train Accuracy score: \",GS_lreg.best_score_)\nprint(\"Best parameters:\", GS_lreg.best_estimator_)\nprint(\"Best Test Accuracy score :\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(GS_lreg.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method3: Linear SVC - Gridsearch with Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_LinSVM = LinearSVC()\n# Grid serach for hyperparameter tuning\nparam_grid_svm = {'C': [0.001, 0.01, 0.10, 1, 10,100]}  \n \nLinsvm_grid = GridSearchCV(classifier_LinSVM, param_grid_svm, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \nLinSVM_best_model= Linsvm_grid.fit(X_train, y_train) \n#Predict test data using best model\ny_pred=LinSVM_best_model.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\n\nmodel_results_Linsvm = pd.DataFrame([['LinearSVM_GridCV', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nResults_all = Results_all.append(model_results_Linsvm, ignore_index = True)\n\nprint(\"Best Train Accuracy score: \",Linsvm_grid.best_score_)\nprint(\"Best parameters:\", Linsvm_grid.best_estimator_)\nprint(\"Best Test Accuracy score :\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(Linsvm_grid.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method4: SVM_lin - Grid search with Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_SVM_lin = SVC()\n# Grid serach for hyperparameter tuning \nparam_grid_svm_lin = {'C': [0.001,0.01,0.1, 1, 10, 50,100],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['linear']}  \nsvm_lin_grid = GridSearchCV(classifier_SVM_lin, param_grid_svm_lin, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \nSVMlin_best_model= svm_lin_grid.fit(X_train, y_train) \n#Predict test data using best model\ny_pred = SVMlin_best_model.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_results_svm = pd.DataFrame([['SVM (Linear)_GridCV', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nResults_all = Results_all.append(model_results_svm, ignore_index = True)\n\nprint(\"Best Train Accuracy score: \",svm_lin_grid.best_score_)\nprint(\"Best parameters:\", svm_lin_grid.best_estimator_)\nprint(\"Best Test Accuracy score :\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(svm_lin_grid.cv_results_))\n#pd.DataFrame(svm_lin_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \nimport mglearn\nscores = np.array(pd.DataFrame(svm_lin_grid.cv_results_).mean_test_score).reshape(7, 5)\n\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid_svm_lin['gamma'], ylabel='C', yticklabels=param_grid_svm_lin['C'], cmap=\"viridis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method5: SVM_rbf - Grid Search with Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_SVM_rbf = SVC()\n# Grid search for hyperparameter tuning \nparam_grid_svm_rbf = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \nsvm_rbf_grid = GridSearchCV(classifier_SVM_rbf, param_grid_svm_rbf, refit = True, verbose = 3, cv = 5, scoring='roc_auc') \n  \n# fitting the model for grid search \nSVMrbf_best_model= svm_rbf_grid.fit(X_train, y_train) \n# Predicting test data using best model\ny_pred = SVMrbf_best_model.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_results_SVM_rbf = pd.DataFrame([['SVM(RBF)_GridCV', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nResults_all = Results_all.append(model_results_SVM_rbf, ignore_index = True)\n\nprint(\"Best Train Accuracy score: \",SVMrbf_best_model.best_score_)\nprint(\"Best parameters:\", SVMrbf_best_model.best_estimator_)\nprint(\"Best Test Accuracy score :\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(svm_rbf_grid.cv_results_))\n#pd.DataFrame(svm_rbf_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \nimport mglearn\nscores = np.array(pd.DataFrame(svm_rbf_grid.cv_results_).mean_test_score).reshape(5, 5)\n\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid_svm_rbf['gamma'], ylabel='C', yticklabels=param_grid_svm_rbf['C'], cmap=\"viridis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method6: SVM_poly - Grid Search with Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_SVM_poly = SVC()\n# Grid search for hyperparameter tuning \nparam_grid_svm_poly = {'C': [0.1, 1, 10, 20, 100], 'degree': [2,3,4],'kernel': ['poly']}  \nsvm_poly_grid = GridSearchCV(classifier_SVM_poly, param_grid_svm_poly, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \nSVMpoly_best_model= svm_poly_grid.fit(X_train, y_train) \ny_pred = SVMpoly_best_model.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_results_SVM_poly = pd.DataFrame([['SVM(POLY)_GridCV', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nResults_all = Results_all.append(model_results_SVM_poly, ignore_index = True)\n\nprint(\"Best Train Accuracy score: \",svm_poly_grid.best_score_)\nprint(\"Best parameters:\", svm_poly_grid.best_estimator_)\nprint(\"Best Test Accuracy score :\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(svm_poly_grid.cv_results_))\n#pd.DataFrame(svm_poly_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \nimport mglearn\nscores = np.array(pd.DataFrame(svm_poly_grid.cv_results_).mean_test_score).reshape(3, 5)\n\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='C', xticklabels=param_grid_svm_poly['C'], ylabel='degree', yticklabels=param_grid_svm_poly['degree'], cmap=\"viridis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method7: Decision tree classifier - Grid search with cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_dec = DecisionTreeClassifier(random_state = 0)\nparam_grid_dec = {\"criterion\": [\"gini\", \"entropy\"],\n              \"min_samples_split\": [2, 10],\n              \"max_depth\": [2, 5, 10]\n              }\ndec_grid = GridSearchCV(classifier_dec, param_grid_dec, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \ndec_best_model= dec_grid.fit(X_train, y_train) \n\n# Predicting Test Set\ny_pred = dec_best_model.predict(X_test) \n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_results_dec = pd.DataFrame([['Decision tree_GridCV', acc, prec, rec, f1 ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nResults_all = Results_all.append(model_results_dec, ignore_index = True)\n  \nprint(\"Best Train Accuracy score: \",dec_grid.best_score_)\nprint(\"Best parameters:\", dec_grid.best_estimator_)\nprint(\"Best Test Accuracy score :\", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Comparision1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Results_all)\nProj1_results=Results_all.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case. \n* In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on.\n* We can see from the above scores results that, Logistic Regression has better accuracy score and  F1_Score, which is greater than all other models. \n* We can consider this as the best model after applying Grid search and cross validation techniques.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Performance of Classification models on reduced dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Reducing the dataset using PCA to retain 95% variance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca_red = PCA(n_components=0.95)\nX_train_red = pd.DataFrame(pca_red.fit_transform(X_train))\nX_test_red = pd.DataFrame(pca_red.transform(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_red.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method1: KNN-GridSearchCV on reduced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\n\nknn_param_grid = {'n_neighbors':[1,2,3,5,7,10,15,25],\n              'leaf_size':[1,3,5],\n              'algorithm':['auto', 'kd_tree'],\n              'n_jobs':[-1]}\n\n#Fit the model 5-fold cross validation\nKNN_grid = GridSearchCV(knn, knn_param_grid, cv=5)\nKNN_grid.fit(X_train_red, y_train)\ny_pred=KNN_grid.predict(X_test_red)\n#print(\"Best Accuracy score: \",KNN_grid.best_score_)\n#print(\"Best parameters:\", KNN_grid.best_estimator_)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_knn_grid_red = pd.DataFrame([['KNN classifier(GridSearch)_red', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nprint(\"Accuracy on training set: {:.3f}\".format(KNN_grid.score(X_train_red, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(KNN_grid.score(X_test_red, y_test)))\nprint(\"Best parameters:\", KNN_grid.best_estimator_)\n\n#plot\nsns.lineplot(x='param_n_neighbors', y='mean_test_score', data=pd.DataFrame(KNN_grid.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method2: LogisticRegression-GridSearch on reduced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlreg=LogisticRegression(random_state = 0)\npenalty = ['l1', 'l2']\n# Create regularization hyperparameter space\nC = [0.001,0.01,0.1,0.2,0.8,1.2,1.5]\nhyperparameters = dict(C=C, penalty=penalty)\n\n# Create grid search using 2-fold cross validation\nlreg_grid = GridSearchCV(lreg, hyperparameters, cv=5, verbose=0)\n# Fit grid search\nLR_best_grid = lreg_grid.fit(X_train_red, y_train)\ny_pred=LR_best_grid.predict(X_test_red)\n\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_lreg_grid_red = pd.DataFrame([['LogisticRegression(GridSearch)_red', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nprint(\"Accuracy on training set: {:.3f}\".format(LR_best_grid.score(X_train_red, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(LR_best_grid.score(X_test_red, y_test)))\nprint(\"Best parameters:\", lreg_grid.best_estimator_)\n#PLot\nsns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(lreg_grid.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method3: LinearSVC-GridSearchCV on reduced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nclassifier_LinSVM = LinearSVC()\n# Grid search for hyperparameter tuning\nparam_grid_svm = {'C': [0.001, 0.01, 0.10, 1, 10,100]}  \n \nLinsvm_grid = GridSearchCV(classifier_LinSVM, param_grid_svm, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \nLinSVM_best_model= Linsvm_grid.fit(X_train_red, y_train) \n#Predicting test dataset\ny_pred = LinSVM_best_model.predict(X_test_red) \nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred,pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_linsvm_grid_red = pd.DataFrame([['LinearSVM(GridSearch)_red', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nprint(\"Accuracy on training set: {:.3f}\".format(Linsvm_grid.score(X_train_red, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(Linsvm_grid.score(X_test_red, y_test)))\nprint(\"Best parameters:\", Linsvm_grid.best_estimator_)\n#plot\nsns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(Linsvm_grid.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method4: SVM_kernel=\"linear\"-GridSearchCV on reduced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier_SVM_lin = SVC(random_state = 0)\n# Grid serach for hyperparameter tuning \nparam_grid_svm_lin = {'C': [0.001,0.01,0.1, 1, 10, 50,100],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['linear']}  \nsvm_lin_grid = GridSearchCV(classifier_SVM_lin, param_grid_svm_lin, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \nSVMlin_best_model= svm_lin_grid.fit(X_train_red, y_train) \n#Predicting testset\ny_pred = SVMlin_best_model.predict(X_test_red)  \nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred,pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_svmlin_grid_red = pd.DataFrame([['SVM_lin(GridSearch)_red', acc, prec, rec, f1 ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nprint(\"Accuracy on training set: {:.3f}\".format(svm_lin_grid.score(X_train_red, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svm_lin_grid.score(X_test_red, y_test)))\nprint(\"Best parameters:\", svm_lin_grid.best_estimator_)\n\n#Plot\nsns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(svm_lin_grid.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the mean cross-validation scores\n%matplotlib inline \nimport mglearn\nscores = np.array(pd.DataFrame(svm_lin_grid.cv_results_).mean_test_score).reshape(7, 5)\nmglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid_svm_lin['gamma'], ylabel='C', yticklabels=param_grid_svm_lin['C'], cmap=\"viridis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method5: SVM_kernel=\"rbf\"-GridSearchCV on reduced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier_SVM_rbf = SVC(random_state = 0)\n# Grid search for hyperparameter tuning \nparam_grid_svm_rbf = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \nsvm_rbf_grid = GridSearchCV(classifier_SVM_rbf, param_grid_svm_rbf, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \nSVMrbf_best_model= svm_rbf_grid.fit(X_train_red, y_train) \n# Predicting test set\ny_pred = SVMrbf_best_model.predict(X_test_red) \n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred,pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_svmrbf_grid_red = pd.DataFrame([['SVM_rbf(GridSearch)_red', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nprint(\"Accuracy on training set: {:.3f}\".format(svm_rbf_grid.score(X_train_red, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svm_rbf_grid.score(X_test_red, y_test)))\nprint(\"Best parameters:\", svm_rbf_grid.best_estimator_)\n\n#plot\nsns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(svm_rbf_grid.cv_results_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \nimport mglearn\nscores = np.array(pd.DataFrame(svm_rbf_grid.cv_results_).mean_test_score).reshape(5, 5)\n\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid_svm_rbf['gamma'], ylabel='C', yticklabels=param_grid_svm_rbf['C'], cmap=\"viridis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method6: SVM_kernel=\"poly\"-GridSearchCV on reduced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier_SVM_poly = SVC(random_state = 0)\n# Grid search for hyperparameter tuning \nparam_grid_svm_poly = {'C': [0.1, 1, 10, 20, 100],   \n              'kernel': ['poly'],\n                'degree' :[2,3,4]}  \nsvm_poly_grid = GridSearchCV(classifier_SVM_poly, param_grid_svm_poly, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \nSVMpoly_best_model= svm_poly_grid.fit(X_train_red, y_train) \ny_pred = svm_poly_grid.predict(X_test_red) \nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred,pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_svmpoly_grid_red = pd.DataFrame([['SVM_poly(GridSearch)_red', acc, prec, rec, f1 ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nprint(\"Accuracy on training set: {:.3f}\".format(svm_poly_grid.score(X_train_red, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svm_poly_grid.score(X_test_red, y_test)))\nprint(\"Best parameters:\", svm_poly_grid.best_estimator_)\n\n#PLOT\nsns.lineplot(x='param_C', y='mean_test_score', data=pd.DataFrame(svm_poly_grid.cv_results_))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \nimport mglearn\nscores = np.array(pd.DataFrame(svm_poly_grid.cv_results_).mean_test_score).reshape(3, 5)\n\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='C', xticklabels=param_grid_svm_poly['C'], ylabel='degree', yticklabels=param_grid_svm_poly['degree'], cmap=\"viridis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method7: DecisionTreeClassifier-GridSearchCV on reduced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dec = DecisionTreeClassifier(random_state = 0)\n\nparam_grid_dec = {\"criterion\": [\"gini\", \"entropy\"],\n              \"min_samples_split\": [2,10],\n              \"max_depth\": [2, 5, 10]\n              }\ndec_grid = GridSearchCV(classifier_dec, param_grid_dec, refit = True, verbose = 3, cv = 5) \n  \n# fitting the model for grid search \ndec_best_model= dec_grid.fit(X_train_red, y_train) \n\n# Predicting Test Set\ny_pred = dec_best_model.predict(X_test_red) \n  \nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred,pos_label='1')\nrec = recall_score(y_test, y_pred, pos_label='1')\nf1 = f1_score(y_test, y_pred,pos_label='1')\n\nmodel_dec_grid_red = pd.DataFrame([['DecisionTree(GridSearch)_red', acc, prec, rec, f1 ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nprint(\"Accuracy on training set: {:.3f}\".format(dec_grid.score(X_train_red, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(dec_grid.score(X_test_red, y_test)))\nprint(\"Best parameters:\", dec_grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Models prediction scores comparision on original dataset vs. on reduced data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The models which were developed on full dataset are compared with models developed on PCA reduced dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Project 2 Models results table\nProj2_models= pd.concat([model_knn_grid_red,\n           model_lreg_grid_red,model_linsvm_grid_red,model_svmlin_grid_red,\n           model_svmrbf_grid_red,model_svmpoly_grid_red\n           ,model_dec_grid_red])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparision table\npd.concat([Proj1_results,Proj2_models],ignore_index=True,sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the above results table we can see that, the accuracy scores of the models that were developed on PCA reduced datasets are slightly smaller compared to the accuracy scores of the models developed on full datasets.But in most of the models, the accuracy is approximately same. \n\n* The models have predicted with sligtly better precision on reduced dataset.\n\n* Hence we can say that even after reducing the dataset to retain 95% variance, the models are predicting the output variable \"deposite\" with good accuracy.\n\n* We can conclude from this, that PCA indeed helps in getting good results with faster analysis. But it is always accuracy variance tradeoff, as we loose variance and some information in the data further by reducing the data using PCA.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}