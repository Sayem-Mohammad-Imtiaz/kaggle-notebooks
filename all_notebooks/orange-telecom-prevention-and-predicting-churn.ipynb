{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real-World Project: Orange Telecom Churn Prevention and Prediction\n### The Orange Telecom's Churn Dataset, which consists of cleaned customer activity data (features), along with a churn label specifying whether a customer canceled the subscription, will be used to develop predictive models."},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading Data into Pandas DataFrame\nimport pandas as pd\n\ndf = pd.read_csv('../input/telecom-churn-datasets/churn-bigml-20.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()  # we see that we have 2666 observations and no null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Variable Breakdown\n  \n**STATE**: 51 Unique States in United States of America\n  \n**Account Length**. Length of The Account\n\n**Area Code** 415 relates to San Francisco,408 is  of San Jose and 510 is of City of Okland \n\n**International Plan**  Yes Indicate International Plan is Present and No Indicates no subscription for Internatinal Plan\n\n**Voice Mail Plan**  Yes Indicates Voice Mail  Plan is Present and No Indicates no subscription for Voice Mail Plan\n\n**Number vmail messages** Number of Voice Mail Messages ranging from 0 to 50\n\n**Total day minutes**  Total Number of Minutes Spent By Customers in Morning\n\n**Total  day calls** Total Number of Calls made by Customer in Morning.\n\n**Total day charge** Total Charge to the Customers in Morning.\n\n**Total eve minutes**Total Number of Minutes Spent By Customers in Evening\n\n**Total eve calls** Total Number of Calls made by Customer in Evening.\n\n**Total eve charge**  Total Charge to the Customers in Morning.\n\n**Total night minutes**  Total Number of Minutes Spent By Customers in the Night.\n\n**Total night calls**   Total Number of Calls made by Customer in Night.\n\n**Total night charge** Total Charge to the Customers in Night."},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Checking the Churn Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"import io\nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization\nimport matplotlib.pyplot as plt#visualization\nfrom PIL import  Image\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns#visualization\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#labels\nlab = df[\"Churn\"].value_counts().keys().tolist()\n#values\nval = df[\"Churn\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Customer attrition in data\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This is a Highly-Imbalanced Dataset,Hence we need to use SMOTE techniques for such a data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separating churn and non churn customers\nchurn     = df[df[\"Churn\"] == bool(True)]\nnot_churn = df[df[\"Churn\"] == bool(False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping Account Length as it doesnt make a sense here\ndf = df.drop('Account length',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Area Code\ndf['Area code'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing Yes/No values with 1 and 0\ndf['International plan'] = df['International plan'].replace({\"Yes\":1,\"No\":0}).astype(int)\ndf['Voice mail plan'] = df['Voice mail plan'].replace({\"Yes\":1,\"No\":0}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Checking Voice-Mail Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Voice-Mail Feautre Messages\nprint('Unique vmail messages',df['Number vmail messages'].unique())\ndf['Number vmail messages'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\n\nplt.show()\ndf.boxplot(column='Number vmail messages', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can Notice for Voice-Mail Feature when there are more than 20 voice-mail messages then certainly there is a churn indicating improving the voice-mail feature or setting a limit and check whether a customer is retianed.\n#### According to my hypothesis :\n  ***1.Voice-Mail Service Upgradation\n  2.Setting up a limit on Voice-Mail service strictly no more than 25 voice mails.\n  3.Quality Drop in Voice-Mail after 25 voice mails.****"},{"metadata":{},"cell_type":"markdown","source":"## 1.5  Total-Minutes in Morning Affecting the Churn Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of minutes:',df['Total day minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total day minutes'].max()/60))\nprint('Maximum number of minutes:',df['Total day minutes'].min())\nprint('Average number of minutes:',df['Total day minutes'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\n\nplt.show()\ndf.boxplot(column='Total day minutes', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can infer from above box-plot that with users spending more 225 minutes or more i.e. approx 4hrs tend to switch to other operator.\n<b>According to my hypothesis, following would be the factors that should be implemented:\n    <li>1.Network Disturbance during a Call\n    <li>2.Cracking sound or noise during a call\n    <li>3.Need to Upgrade or make smarter use of technologies like VoLTE to improve Voice Quality.\n    <li>4. Network Upgradation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of calls:',df['Total day calls'].max())\nprint('Minimum number of calls:',df['Total day calls'].min())\nprint('Average number of calls:',df['Total day calls'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.show()\ndf.boxplot(column='Total day calls', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can infere here that on an average a 100 calls are made which is a good indication for the company.But we can also note that for the churn customer the median is slightly higher than 100 which indicates there are call drops which may lead to more calls in a morning."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of charge:',df['Total day charge'].max())\nprint('Minimum number of charge:',df['Total day charge'].min())\nprint('Average number of charge:',df['Total day charge'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average charge is around 30$ which is  a decent pricing strategy!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total day charge', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Well,Here we can clearly indicate a strategy a good strategy to be implemented. As from above infered box-plots we can conclude one thing i.e Customers having more minutes spent on the network tend to leave the it's subscription and from the above box-plot it clearly indicates that there is defect in the pricing startegy of the company.\n<b> According to my Hypothsis:\n    <li>1.Startegy of pricing needs to be re-evaluated.\n    <li> 2. The Clients who have high call minutes and calls need a discount in the end.\n       "},{"metadata":{},"cell_type":"markdown","source":"## 1.5 Evening time Affecting the Churn Rate!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of minutes:',df['Total eve minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total eve minutes'].max()/60))\nprint('Maximum number of minutes:',df['Total eve minutes'].min())\nprint('Average number of minutes:',df['Total eve minutes'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total eve minutes', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of calls:',df['Total eve calls'].max())\nprint('Minimum number of calls:',df['Total eve calls'].min())\nprint('Average number of calls:',df['Total eve calls'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.show()\ndf.boxplot(column='Total eve calls', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of charge:',df['Total eve charge'].max())\nprint('Minimum number of charge:',df['Total eve charge'].min())\nprint('Average number of charge:',df['Total eve charge'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total eve charge', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.6 Night Time Affecting Churn Rate !!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of minutes:',df['Total night minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total night minutes'].max()/60))\nprint('Minimum number of minutes:',df['Total night minutes'].min())\nprint('Average number of minutes:',df['Total night minutes'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total night minutes', by='Churn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In Night, The Loyal Customers are spend more time is what the box-plot indicates!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of calls:',df['Total night calls'].max())\nprint('Minimum number of calls:',df['Total night calls'].min())\nprint('Average number of calls:',df['Total night calls'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total night calls', by='Churn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Still Calls are made more by the churned customers!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of charge:',df['Total night charge'].max())\nprint('Minimum number of charge:',df['Total night charge'].min())\nprint('Average number of charge:',df['Total night charge'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total night charge', by='Churn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Again Optimization of The Charges would lead to a loyal customer!!!"},{"metadata":{},"cell_type":"markdown","source":"## 1.7 International Calls Affecting the Churn Rate "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of minutes:',df['Total intl minutes'].max(),'i.e. Max number of Hours  spent:',round(df['Total intl minutes'].max()/60))\nprint('Minimum number of minutes:',df['Total intl minutes'].min())\nprint('Average number of minutes:',df['Total intl minutes'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total intl minutes', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Users who make the International Call tend to spend more minutes."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of calls:',df['Total intl calls'].max())\nprint('Minimum number of calls:',df['Total intl calls'].min())\nprint('Average number of calls:',df['Total intl calls'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total intl calls', by='Churn')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### These Calls clearly indicat that clients without International Plan Suffer and May Leave the Operator."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum number of charge:',df['Total intl charge'].max())\nprint('Minimum number of charge:',df['Total intl charge'].min())\nprint('Average number of charge:',df['Total intl charge'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.show()\ndf.boxplot(column='Total intl charge', by='Churn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Different Pricing Strategy and International Calling Rate Optimization would lead to lower churn rate"},{"metadata":{},"cell_type":"markdown","source":"## 1.8 Churn According to States"},{"metadata":{"trusted":true},"cell_type":"code","source":"# By Age\nfig, axz = plt.subplots(figsize=(20,15))\n\naxz = sns.countplot(x='State', hue='Churn', data=df, palette='Reds')\n\n\naxz.set_ylabel('COUNTS', rotation=0, labelpad=100,size=20)\naxz.set_xlabel('State', size=20)\naxz.yaxis.set_label_coords(0.05, 0.95)  # (x, y)\naxz.legend(loc=0,fontsize=20);\n\naxz.tick_params(labelsize=15)  # Changes size of the values on the label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Texas,Maryland have the bit more churn rate than usual, A Network Upgradation would be strongly suggested in these areas!."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Customer Calls\nfig, axz = plt.subplots(figsize=(20,15))\n\naxz = sns.countplot(x='Customer service calls', hue='Churn', data=df, palette='Reds')\n\n\naxz.set_ylabel('COUNTS', rotation=0, labelpad=100,size=20)\naxz.set_xlabel('Customer Service Calls', size=20)\naxz.yaxis.set_label_coords(-0.05, 0.95)  # (x, y)\naxz.legend(loc=0,fontsize=20);\n\naxz.tick_params(labelsize=15)  # Changes size of the values on the label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### While some customers are lazy and hence without resolving the issue they have jumped to other network operator,while the customers who have called once also have high churn rate indicating their issue was not solved in first attempt.\n<li><b>A Feedback is neccesary in such situations.\n<li><b> It should given a Confirmation to the Customer that there issue would be solved in first attempt"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Churn'] = df['Churn'].replace({bool(True):1,bool(False):0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freq distribution of all data\nfig, ax = plt.subplots(figsize=(15,15))\npd.DataFrame.hist(df,ax=ax)\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_df = df[['Total day minutes','Total day calls','Total day minutes','Total day charge','Churn']]\neve_df = df[['Total eve minutes','Total eve calls','Total eve minutes','Total eve charge','Churn']]\nnight_df = df[['Total night minutes','Total night calls','Total night minutes','Total night charge','Churn']]\nintl_df = df[['Total intl minutes','Total intl calls','Total intl minutes','Total intl charge','Churn']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.8 Data Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that we have our features, let's plot them on a correlation matrix to remove anything that might \n# cause multi-colinearity within our model\n\nsns.set(style=\"white\")\n# Creating the data\ndata = df.corr()\n\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(data, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(60,50))\n\n\n# Create a custom color palette\ncmap = \\\nsns.diverging_palette(133, 10,\n                      as_cmap=True)  \n# as_cmap returns a matplotlib colormap object rather than a list of colors\n# Green = Good (low correlation), Red = Bad (high correlation) between the independent variables\n\n# Plot the heatmap\ng = sns.heatmap(data=data, annot=True, cmap=cmap, ax=ax, \n                mask=mask, # Splits heatmap into a triangle\n                annot_kws={\"size\":20},  #Annotation size\n               cbar_kws={\"shrink\": 0.8} # Color bar size\n               );\n\n\n# Prevent Heatmap Cut-Off Issue\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\n# Changes size of the values on the label\nax.tick_params(labelsize=25) \n\nax.set_yticklabels(g.get_yticklabels(), rotation=0);\nax.set_xticklabels(g.get_xticklabels(), rotation=80);\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{},"cell_type":"markdown","source":"## 1.9 Training Our Model using Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm\nfrom sklearn.metrics import precision_score,recall_score\nfrom yellowbrick.classifier import DiscriminationThreshold\n#splitting train and test data \ntrain,test = train_test_split(df,test_size = .25 ,random_state = 111)\nX = df.drop(['State','Churn'],axis=1)\ntarget_col = ['Churn']\n##seperating dependent and independent variables\ncols    = X.columns\ntrain_X = train[cols]\ntrain_Y = train[target_col]\ntest_X  = test[cols]\ntest_Y  = test[target_col]\n\n#Function attributes\n#dataframe     - processed dataframe\n#Algorithm     - Algorithm used \n#training_x    - predictor variables dataframe(training)\n#testing_x     - predictor variables dataframe(testing)\n#training_y    - target variable(training)\n#training_y    - target variable(testing)\n#cf - [\"coefficients\",\"features\"](cooefficients for logistic \n                                 #regression,features for tree based models)\n\n#threshold_plot - if True returns threshold plot for model\n    \ndef telecom_churn_prediction(algorithm,training_x,testing_x,\n                             training_y,testing_y,cols,cf,threshold_plot) :\n    \n    #model\n    algorithm.fit(training_x,training_y)\n    predictions   = algorithm.predict(testing_x)\n    probabilities = algorithm.predict_proba(testing_x)\n    #coeffs\n    if   cf == \"coefficients\" :\n        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n    elif cf == \"features\" :\n        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n        \n    column_df     = pd.DataFrame(cols)\n    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n                              right_index= True, how = \"left\"))\n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    \n    print (algorithm)\n    print (\"\\n Classification report : \\n\",classification_report(testing_y,predictions))\n    print (\"Accuracy   Score : \",accuracy_score(testing_y,predictions))\n    #confusion matrix\n    conf_matrix = confusion_matrix(testing_y,predictions)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(testing_y,predictions) \n    print (\"Area under curve : \",model_roc_auc,\"\\n\")\n    fpr,tpr,thresholds = roc_curve(testing_y,probabilities[:,1])\n    \n    #plot confusion matrix\n    trace1 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Not churn\",\"Churn\"],\n                        y = [\"Not churn\",\"Churn\"],\n                        showscale  = False,colorscale = \"Picnic\",\n                        name = \"matrix\")\n    \n    #plot roc curve\n    trace2 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n    trace3 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot coeffs\n    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\",\n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Picnic\",\n                                  line = dict(width = .6,color = \"black\")))\n    \n    #subplots\n    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                            subplot_titles=('Confusion Matrix',\n                                            'Receiver operating characteristic',\n                                            'Feature Importances'))\n    \n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,1,2)\n    fig.append_trace(trace4,2,1)\n    \n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 900,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         margin = dict(b = 195))\n    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n                                        tickangle = 90))\n    py.iplot(fig)\n    \n    if threshold_plot == True : \n        visualizer = DiscriminationThreshold(algorithm)\n        visualizer.fit(training_x,training_y)\n        visualizer.poof()\n        \nlogit  = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit,train_X,test_X,train_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As Dataset is Highly-Imbalance we would like to Over Sample and check results"},{"metadata":{},"cell_type":"markdown","source":"### SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\n\nsmote_X = df[cols]\nsmote_Y = df[target_col]\n\n#Split train and test data\nsmote_train_X,smote_test_X,smote_train_Y,smote_test_Y = train_test_split(smote_X,smote_Y,\n                                                                         test_size = .25 ,\n                                                                         random_state = 111)\n\n#oversampling minority class using smote\nos = SMOTE(random_state = 0)\nos_smote_X,os_smote_Y = os.fit_sample(smote_train_X,smote_train_Y)\nos_smote_X = pd.DataFrame(data = os_smote_X,columns=cols)\nos_smote_Y = pd.DataFrame(data = os_smote_Y,columns=target_col)\n###\n\n\n\nlogit_smote = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\ntelecom_churn_prediction(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                         cols,\"coefficients\",threshold_plot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Clearly SMOTE over sampling increase the recall which is our main metric to correctly predict the churned customer"},{"metadata":{},"cell_type":"markdown","source":"### SMOTE and RFE and Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nlogit = LogisticRegression()\n\nrfe = RFE(logit,10)\nrfe = rfe.fit(os_smote_X,os_smote_Y.values.ravel())\n\nrfe.support_\nrfe.ranking_\n\n#identified columns Recursive Feature Elimination\nidc_rfe = pd.DataFrame({\"rfe_support\" :rfe.support_,\n                       \"columns\" : X.columns,\n                       \"ranking\" : rfe.ranking_,\n                      })\ncols = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist()\n\n\n#separating train and test data\ntrain_rf_X = os_smote_X[cols]\ntrain_rf_Y = os_smote_Y\ntest_rf_X  = test[cols]\ntest_rf_Y  = test[target_col]\n\nlogit_rfe = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n#applying model\ntelecom_churn_prediction(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                         cols,\"coefficients\",threshold_plot = True)\n\ntab_rk = ff.create_table(idc_rfe)\npy.iplot(tab_rk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Applying SMOTE and Logistic Regression using RFE has good impact and gives us the ranking"},{"metadata":{},"cell_type":"markdown","source":"## Checking the Scores with other Models and Choosing the Best One!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\n\ngnb = GaussianNB(priors=None)\nsvc_lin  = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n               decision_function_shape='ovr', degree=3, gamma=1.0, kernel='linear',\n               max_iter=-1, probability=True, random_state=None, shrinking=True,\n               tol=0.001, verbose=False)\nsvc_rbf  = SVC(C=1.0, kernel='rbf', \n               degree= 3, gamma=1.0, \n               coef0=0.0, shrinking=True,\n               probability=True,tol=0.001,\n               cache_size=200, class_weight=None,\n               verbose=False,max_iter= -1,\n               random_state=None)\nfrom lightgbm import LGBMClassifier\n\nlgbm_c = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\nfrom xgboost import XGBClassifier\n\nxgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bytree=1, gamma=0, learning_rate=0.9, max_delta_step=0,\n                    max_depth = 7, min_child_weight=1, missing=None, n_estimators=100,\n                    n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=True, subsample=1)\n#gives model report in dataframe\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    roc_auc      = roc_auc_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    kappa_metric = cohen_kappa_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n#outputs for every model\nmodel1 = model_report(logit,train_X,test_X,train_Y,test_Y,\n                      \"Logistic Regression(Baseline_model)\")\nmodel2 = model_report(logit_smote,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Logistic Regression(SMOTE)\")\nmodel3 = model_report(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n                      \"Logistic Regression(RFE)\")\ndecision_tree = DecisionTreeClassifier(max_depth = 9,\n                                       random_state = 123,\n                                       splitter  = \"best\",\n                                       criterion = \"gini\",\n                                      )\nmodel4 = model_report(decision_tree,train_X,test_X,train_Y,test_Y,\n                      \"Decision Tree\")\nmodel5 = model_report(knn,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"KNN Classifier\")\nrfc = RandomForestClassifier(n_estimators = 1000,\n                             random_state = 123,\n                             max_depth = 9,\n                             criterion = \"gini\")\nmodel6 = model_report(rfc,train_X,test_X,train_Y,test_Y,\n                      \"Random Forest Classifier\")\nmodel7 = model_report(gnb,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"Naive Bayes\")\nmodel8 = model_report(svc_lin,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier Linear\")\nmodel9 = model_report(svc_rbf,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"SVM Classifier RBF\")\nmodel10 = model_report(lgbm_c,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"LGBM Classifier\")\nmodel11 = model_report(xgc,os_smote_X,test_X,os_smote_Y,test_Y,\n                      \"XGBoost Classifier\")\n\n#concat all models\nmodel_performances = pd.concat([model1,model2,model3,\n                                model4,model5,model6,\n                                model7,model8,model9,\n                                model10,model11],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Well no doubt, Boosting and Tree Based Perform Better and XGBoost is the best model to conclude it let's evaluate with other metrics"},{"metadata":{},"cell_type":"markdown","source":"# 6. Model Performances\n## 6.1. model performance metrics</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performances\ndef output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGBoost is the best "},{"metadata":{},"cell_type":"markdown","source":"## Plotting Confusion Matrix of all models"},{"metadata":{"trusted":true},"cell_type":"code","source":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,15))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    plt.subplot(4,3,j+1)\n    predictions = i.predict(test_X)\n    conf_matrix = confusion_matrix(predictions,test_Y)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"not churn\",\"churn\"],\n                yticklabels=[\"not churn\",\"churn\"],\n                linewidths = 2,linecolor = \"w\",cmap = \"Set1\")\n    plt.title(k,color = \"b\")\n    plt.subplots_adjust(wspace = .3,hspace = .3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Well we want our model to predict churn customers correctly and XGBoost is exceptional to our metric evaluation"},{"metadata":{},"cell_type":"markdown","source":"## Evaluating ROC Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"lst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nplt.style.use(\"dark_background\")\nfig = plt.figure(figsize=(12,16))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    fpr,tpr,thresholds = roc_curve(test_Y,probabilities[:,1])\n    plt.plot(fpr,tpr,linestyle = \"dotted\",\n             color = \"royalblue\",linewidth = 2,\n             label = \"AUC = \" + str(np.around(roc_auc_score(test_Y,predictions),3)))\n    plt.plot([0,1],[0,1],linestyle = \"dashed\",\n             color = \"orangered\",linewidth = 1.5)\n    plt.fill_between(fpr,tpr,alpha = .4)\n    plt.fill_between([0,1],[0,1],color = \"k\")\n    plt.legend(loc = \"lower right\",\n               prop = {\"size\" : 12})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xticks(np.arange(0,1,.3))\n    plt.yticks(np.arange(0,1,.3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Precision Recall Curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n\nlst    = [logit,logit_smote,decision_tree,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression(Baseline_model)','Logistic Regression(SMOTE)',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,17))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    \n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    recall,precision,thresholds = precision_recall_curve(test_Y,probabilities[:,1])\n    plt.plot(recall,precision,linewidth = 1.5,\n             label = (\"avg_pcn : \" + \n                      str(np.around(average_precision_score(test_Y,predictions),3))))\n    plt.plot([0,1],[0,0],linestyle = \"dashed\")\n    plt.fill_between(recall,precision,alpha = .2)\n    plt.legend(loc = \"lower left\",\n               prop = {\"size\" : 10})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xlabel(\"recall\",fontsize =7)\n    plt.ylabel(\"precision\",fontsize =7)\n    plt.xlim([0.25,1])\n    plt.yticks(np.arange(0,1,.3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n## We can definitely suggest to prevent churn :\n<b><li> Upgrading network to improve services for long duration users.\n  <b><li> Updating Pricing Strategies.\n    <b><li>Updating and Optimizing Internationall Call Rates.\n     <b><li> Implmenting a better network infrastructure in Maryland and Texas Areas where there is more Churn Rate.\n      <b><li>Upgrading their services when in emegenvy only in evening period as low network traffic.<b>\n"},{"metadata":{},"cell_type":"markdown","source":"## We can Predict using:\n<b><li> If we want to predict churn rate  correctly, then Tree based classification using SMOTE would be recommended.\n <b><li> XGBoost Classifier Performs best and would be a recommended Model.\n"},{"metadata":{},"cell_type":"markdown","source":"## Further for Improvement we can suggest and discuss more strategies to the company by collecting other data and through a domain expert!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}