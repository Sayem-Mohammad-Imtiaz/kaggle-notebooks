{"cells":[{"metadata":{"_uuid":"8551f30c0b0a677494e351cd24fe9e61d7e9615c"},"cell_type":"markdown","source":"### Introduction\nIn this notebook I will build a simple spam filter using naive bayes & labelled dataset of email obtained from [kaggle](https://www.kaggle.com/omkarpathak27/identify-spam-using-emails/data). I will also explain various preprocessing steps involved for text data followed by feature extraction & calssification model."},{"metadata":{"trusted":true,"_uuid":"594b1c0844e59706e112c23ba139ebf6a3dbadcf"},"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport re\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e6af33e1e0f8c93db7c627a22d789c47d547378"},"cell_type":"markdown","source":"### Exploratory Analysis"},{"metadata":{"trusted":true,"_uuid":"184b7c0396d80fb373c90c432abf6f81b1cdf8f5"},"cell_type":"code","source":"emails = pd.read_csv('../input/emails.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"920dd1f4c17ed1563ef2e8520fee8a2c75766c3f"},"cell_type":"code","source":"emails.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"652d97913b55a67362bbee36ccb0d69d343b9e9a"},"cell_type":"code","source":"#Lets read a single email \n\nemails.get_value(58,'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48a442bce7d0bba00f1e858b62b04e878797994e"},"cell_type":"code","source":"emails.shape\n#Total 5728 emails","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09d89ba77ec7e5791119daa5f53b76599e8dc826"},"cell_type":"code","source":"#Checking class distribution\nemails.groupby('spam').count()\n#23.88% emails are spam which seems good enough for our task","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c15fe1c9886f948d51250472291412fa4700bc36"},"cell_type":"code","source":"#Lets see the distribution of spam using beautiful seaborn package\n\nlabel_counts = emails.spam.value_counts()\nplt.figure(figsize = (12,6))\nsns.barplot(label_counts.index, label_counts.values, alpha = 0.9)\n\nplt.xticks(rotation = 'vertical')\nplt.xlabel('Spam', fontsize =12)\nplt.ylabel('Counts', fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8d32282e6c22e85aef6735a5e57ddc3d3b132ac"},"cell_type":"code","source":"#Lets check if email length is coorelated to spam/ham\nemails['length'] = emails['text'].map(lambda text: len(text))\n\nemails.groupby('spam').length.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d375c05f2e7449fdb516d12f7a87c8684b1fe7b2"},"cell_type":"code","source":"#emails length have some extreme outliers, lets set a length threshold & check length distribution\nemails_subset = emails[emails.length < 1800]\nemails_subset.hist(column='length', by='spam', bins=50)\n\n#Nothing much here, lets process the contents of mail now for building spam filter","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d93a172592af8784e60bd428fbff6dda45f97bf2"},"cell_type":"markdown","source":"### Text Data Preprocessing\nMails provided in data are full of unstuctured mess, so its important to preprocess this text before feature extraction & modelling. Thanks to [nltk](https://www.nltk.org/) library, its very easy to do this preprocessing now  with few lines of python code."},{"metadata":{"_uuid":"a74c8131a907930dc7eea632e6c2178f7917622d"},"cell_type":"markdown","source":"#### Tokenization\nTokenization converts continuous stream of words into seprate token for each word."},{"metadata":{"trusted":true,"_uuid":"277058afa7175b2f0bc319c72955332102484da4"},"cell_type":"code","source":"emails['tokens'] = emails['text'].map(lambda text:  nltk.tokenize.word_tokenize(text)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c57cd2576d6d085253dd089dbf03835ab7b7d9a"},"cell_type":"code","source":"#Lets check tokenized text from first email\n\nprint(emails['tokens'][1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eab0fb99550c53cc813d183cb9ca686989f565d"},"cell_type":"markdown","source":"#### Stop Words Removal\nStop words usually refers to the most common words in a language like 'the', 'a', 'as' etc. These words usually do not convey any useful information needed for spam filter so lets remove them."},{"metadata":{"trusted":true,"_uuid":"360ec719c44b0dd2958161b46ed6f9e07fc9b05c"},"cell_type":"code","source":"#Removing stop words\n\nstop_words = set(nltk.corpus.stopwords.words('english'))\nemails['filtered_text'] = emails['tokens'].map(lambda tokens: [w for w in tokens if not w in stop_words]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58f052dcdbcf2f1890b2e6d91b54c50328bd1771"},"cell_type":"code","source":"#Every mail starts with 'Subject :' lets remove this from each mail \n\nemails['filtered_text'] = emails['filtered_text'].map(lambda text: text[2:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf8684104e4d98689c167bd3ee48912956ee107f"},"cell_type":"code","source":"#Lets compare an email with stop words removed\n\nprint(emails['tokens'][3],end='\\n\\n')\nprint(emails['filtered_text'][3])\n\n#many stop words like 'the', 'of' etc. were removed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc33d4d6b3137782ac063f1a02b95370f1987a4a"},"cell_type":"code","source":"#Mails still have many special charater tokens which may not be relevant for spam filter, lets remove these\n#Joining all tokens together in a string\nemails['filtered_text'] = emails['filtered_text'].map(lambda text: ' '.join(text))\n\n#removing apecial characters from each mail \nemails['filtered_text'] = emails['filtered_text'].map(lambda text: re.sub('[^A-Za-z0-9]+', ' ', text))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"746cc355eaf93909a580e83fc31f10d082804664"},"cell_type":"markdown","source":"#### Lemmatization\nIts the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. so word like 'moved' & 'moving' will be reduced to 'move'. "},{"metadata":{"trusted":true,"_uuid":"fd91c453b06d1b5537d11a5d451fafbcdfb09270"},"cell_type":"code","source":"wnl = nltk.WordNetLemmatizer()\nemails['filtered_text'] = emails['filtered_text'].map(lambda text: wnl.lemmatize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"311f1369521c2711d838d52739a53638e2c05581"},"cell_type":"code","source":"#Lets check one of the mail again after all these preprocessing steps\nemails['filtered_text'][4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b853c0aab5f3a5b2a3ba298e325a112a5ba81db7"},"cell_type":"code","source":"#Wordcloud of spam mails\nspam_words = ''.join(list(emails[emails['spam']==1]['filtered_text']))\nspam_wordclod = WordCloud(width = 512,height = 512).generate(spam_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(spam_wordclod)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d938b9b859dd0cc963b8960ae99a1d3437e04fe"},"cell_type":"code","source":"#Wordcloud of non-spam mails\nspam_words = ''.join(list(emails[emails['spam']==0]['filtered_text']))\nspam_wordclod = WordCloud(width = 512,height = 512).generate(spam_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(spam_wordclod)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"131c6d6e6058881f3eb1099723dd95eaf38985a6"},"cell_type":"markdown","source":"### Spam Filtering Models\nAfter preprocessing we have clean enough text, lets convert these mails into vectors of numbers using 2 popular methods: [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) & [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). After getting vectors for each mail we will build our classifier using [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)."},{"metadata":{"_uuid":"8b371e5eaf1854d8da718c6d5dcb4310f186c5a1"},"cell_type":"markdown","source":"### 1. Bag of Words\nIt basically creates a vector with frequency of each word from vocabulary in given mail. Like name suggests bag of words does not treat text as a sequence but a collection of unrelated bag of words. Its easy to create these vectors using CountVectorizer() from scikit learn.\n"},{"metadata":{"trusted":true,"_uuid":"61211de9e9b0cad621822a22e760fb7bc00dbd81"},"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ncounts = count_vectorizer.fit_transform(emails['filtered_text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea19547d3cb75795afdb99855fe4eb8ceb019cf7"},"cell_type":"code","source":"print(counts.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc5addabf2b44a0a3433596f1eca9b3d4aa543cf"},"cell_type":"markdown","source":"### Naive Bayes Classifier"},{"metadata":{"trusted":true,"_uuid":"dd3db7b1446b98932341b5cd068aa6ba720a6358"},"cell_type":"code","source":"classifier = MultinomialNB()\ntargets = emails['spam'].values\nclassifier.fit(counts, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8da5e4259c081da159f5e3537fee26d21996e26"},"cell_type":"code","source":"#Predictions on sample text\nexamples = ['cheap Viagra', \"Forwarding you minutes of meeting\"]\nexample_counts = count_vectorizer.transform(examples)\npredictions = classifier.predict(example_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df4ed511d9ba4672522c5bfb4660f15821bd64f0"},"cell_type":"code","source":"print(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f267423215c78fbb859a3cbb53fa5de222e0505d"},"cell_type":"markdown","source":"### 2. TF-IDF\ntf-idf is a numerical statistic that is intended to reflect how important a word is to a mail in collection of all mails or corpus. This is also a vector with tf-idf values of each word for each mail. To understsnd how tf-fdf values are computed please check my [blog post](https://mohitatgithub.github.io/2018-04-28-Learning-tf-idf-with-tidytext/) on understanding tf-idf. Here we will use TfidfTransformer() from scikit learn to generate this vector."},{"metadata":{"trusted":true,"_uuid":"eae978b1375a7f4aafb238476242edec750296d8"},"cell_type":"code","source":"tfidf_vectorizer = TfidfTransformer().fit(counts)\ntfidf = tfidf_vectorizer.transform(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e8746bc7dc5e6ef31a2adbf845bfa051279acac"},"cell_type":"code","source":"print(tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf5bb45773c7a321961284ac50ffb6c3cbf5df50"},"cell_type":"code","source":"classifier = MultinomialNB()\ntargets = emails['spam'].values\nclassifier.fit(counts, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9c17e3375e9334ab9f8f8a8c4422e77106014a5"},"cell_type":"code","source":"#Predictions on sample text\nexamples = ['Free Offer Buy now',\"Lottery from Nigeria\",\"Please send the files\"]\nexample_counts = count_vectorizer.transform(examples)\nexample_tfidf = tfidf_vectorizer.transform(example_counts)\npredictions_tfidf = classifier.predict(example_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d62984d71ebbf72bf4877980504b613243f6121a"},"cell_type":"code","source":"print(predictions_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a2b08510e69fd439671d596b7879b6ba9072c09"},"cell_type":"markdown","source":"Future Scope: \n1. Steps discussed above for feature vectorization & model building can also be stacked together using [scikit learn pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). \n2. I have not used a train test split here, we can throughly evaluate our model with a seprate text set & further using cross-validation.\n3. Text data can be further processed & new features can be used to build more robust filters using other techniques like N-grams. We can also try other machine learning algortihms like SVM, KNN etc."},{"metadata":{"_uuid":"b31a761c2b77ab4148d7a6a3ac5527455ebcab21"},"cell_type":"markdown","source":"Thanks for reading, I am a novice in text analysis so please share your feedback on improvements & errors."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}