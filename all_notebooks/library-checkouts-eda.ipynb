{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a9980c67fcee3c1239c74ca3ec4a10f7e3eaba2"},"cell_type":"code","source":"# https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else: df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"## Note: Kernel unstable when using all data\ndf = reduce_mem_usage(pd.read_csv(\"../input/checkouts-by-title.csv\",\n                 nrows=15123456,\n                 keep_date_col=True,infer_datetime_format=True\n                 ,parse_dates=[[\"CheckoutYear\",\"CheckoutMonth\"]]).rename(columns={\"CheckoutYear_CheckoutMonth\":\"CheckoutDate\"}).set_index(\"CheckoutDate\"))\n\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01218bad2531fe7ad1592f7bcfeb467c4c36250"},"cell_type":"code","source":"### assuming all publication years are 4 digit years, we could parse with this. \n### It's not, so we get erros. We could coerce, but let's just leave them as numbers for now:\n# df.PublicationYear = pd.to_datetime(df.PublicationYear.str.replace(r\"[^0-9]\",''),format=\"%Y\")\n\ndf.PublicationYear = df.PublicationYear.str.replace(r\"[^0-9]\",'')#.astype(int) # astype int fails due to nans. Parsing as floats would give us \"ugly\" .0 per number","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c88d74123af152761b23d072f6f9dca4670c8321"},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe4326fca49282c771261d15072d5fcf2ffece0e"},"cell_type":"markdown","source":"Let's get the most popular items over all the time-period covered:\n* Ideally we would also normalize titles' text and versions of the same product, but I'll ignore for now.\n* Could also look at distinct counts, (i.e not just # checkouts, but count of distinct months when it was checked out). \n* We could also look at the quarterly level. Or even better, look at items as a % of total checkouts for that month per library. Or reaggregate per quarter"},{"metadata":{"trusted":true,"_uuid":"4e68213525f773a239eae630fb81bb354444aa31"},"cell_type":"code","source":"df[\"title_sum_total\"] = df.groupby(\"Title\")[\"Checkouts\"].transform(\"sum\")\ndf[\"title_MonthCounts_total\"] = df.groupby(\"Title\")[\"Checkouts\"].transform(\"count\")\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7abba31342f918364b87cdc23bc05abd9b203179"},"cell_type":"markdown","source":"* Unsurprisingly, the distribution is very biased with a super long tail, and a \"bump\" for the very popular items! "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8f75e23e00d65b434149ab9217b9044520673d74"},"cell_type":"code","source":"df[\"title_sum_total\"].hist(bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"fc281911418590b6ded22f1e74778fc3262e974f"},"cell_type":"code","source":"df[\"title_MonthCounts_total\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11ff28d17362755c568faf0ef4ed420dde391c33"},"cell_type":"markdown","source":"## Add total library checkouts for the library per month.\n* Relevant: \n    * Distinct checkouts (proxy for # visitors)\n    * Checkouts per MaterialType (books vs music..) "},{"metadata":{"trusted":true,"_uuid":"d7160f5397c2c87f46e8d04c9ef607856f375515"},"cell_type":"code","source":"df[\"total_checkouts_all_monthly_sum\"] = df.groupby([\"CheckoutYear\",\"CheckoutMonth\"])[\"Checkouts\"].transform(\"sum\")\n\ndf[\"total_checkouts_ByMaterial_monthly_sum\"] = df.groupby([\"MaterialType\",\"CheckoutYear\",\"CheckoutMonth\"])[\"Checkouts\"].transform(\"sum\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"591e93ae973ecccdf61fdf99cc226ac8468df897"},"cell_type":"code","source":"# we see a lot of overlap for some \"mixed\" categories\ndf.groupby(\"MaterialType\")[\"total_checkouts_ByMaterial_monthly_sum\"].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51f3c06460a910571de13e41f498a24f97240278"},"cell_type":"code","source":"df[\"total_checkouts_all_monthly_sum\"].plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fea56deb958a97deb0aa43f51e14c9719d3e152"},"cell_type":"markdown","source":"# Let's filter out the \"less popular\" items. \n* While interesting for many uses cases (and specific users, e.g. research textbook), we want to get at least moderately popular items\n\n* We could also look at subsets by subject: i.e get total monthly checkouts per publisher (for popular subjects or publishers) , then  see which checked-out items have a larger proportional share of this. \n\n* the number of distinct months an item was checked out in, is also important for building a model to predict monthly checkouts per item: Our model will \"see\" each item only once per month, so # occurences is going to affect this strongly! "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4be9e667659a540ab4e343f3a227f676d8422954"},"cell_type":"code","source":"print(\"Original shape: %i\" %(df.shape[0]))\ndf = df.loc[(df[\"title_MonthCounts_total\"]>8) & (df[\"title_sum_total\"]>40)]\nprint(\"We keep items that were checked out at least once in 8+ different months, AND checked out at least 30 times in aggregate\")\nprint(\"New shape: %i\" %(df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4e0bb409e8fa543cd18761a18e020a45a59dc1"},"cell_type":"code","source":"# Describe new counts/sum distribution for our \"popular\" subset\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36ea627f51287d0359d21dec2617e51adcde594a"},"cell_type":"markdown","source":"# Let's look at some of the popular items\n\n* We find a data issue: \"<Unknown Title>\" is very common (although we could probably create a new ID using the concatenation of subject+publisher, we may wish to drop it for now, as it creates outliers)\n    \n    * It seems that the popular titles are mostly not what we might expect! This could be a data issue in our process... or just the result of popular romance and historical novels being popular or taken out in batch :) "},{"metadata":{"trusted":true,"_uuid":"6b906373a63d07b767d43c1860dfd92d2908c9ce"},"cell_type":"code","source":"df.nlargest(10, columns=[\"title_MonthCounts_total\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef992a1dcca56a3c5a2f859cb20458160a6bddc4"},"cell_type":"code","source":"df.nlargest(15, columns=[\"total_checkouts_all_monthly_sum\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e444587dcf317f0d28f5db351bf2b74eb928a0d"},"cell_type":"markdown","source":"* ## Drop *Unknown Title* for simplicity\n    * Ideally we would impute this or create a hummy ID. Easier to ignore for now"},{"metadata":{"trusted":true,"_uuid":"40e6a21280611b9fc6f3d6c0228fc3d2f6c40ee2"},"cell_type":"code","source":"df = df.loc[df.Title !=\"<Unknown Title>\"]\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe6c28f0418d7c4b0fb94d9cdf90bfa0da7d517c"},"cell_type":"code","source":"df.nlargest(10, columns=[\"title_MonthCounts_total\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69dafff03c1c21041c9b79ce0ba94d92fcc365b8"},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dac7663715510caea77e26a66f6d5f0258bd381"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e5be47133d2314a1cd9c09836d4eec48cb7d0c"},"cell_type":"code","source":"df.to_csv(\"LibraryCheckouts_WithLeaks_v1.csv.gz\",compression=\"gzip\")\n# df.drop([])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}