{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport sklearn.model_selection as ms\nimport sklearn.metrics as m\nimport sklearn.tree as tree\nimport sklearn.ensemble as ensemble\nimport sklearn.svm as svm\nimport sklearn.linear_model as lm\nimport sklearn.preprocessing as pp\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[:, 0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[:, 10:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum() # check for blank values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-root'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tol_label = 0\nfor col in data.columns:\n    print(col)\n    print(data[col].unique())\n    tol_label += len(data[col].unique())\n    print()\n# seems that only the column - \"stalk-root\" contain ?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tol_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['stalk-root'].value_counts() # we have about 2480 row items with ?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['stalk-root'] == '?']['class'].value_counts() # breakdown of class where stalk-root == ?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].value_counts() # about 45% of the positive class (p) has rows where stalk-root == ?","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing\n* One-hot enncoding\n* Data is splitted such that BOTH the training and the testing dataset contain the same proportion of positive and negative class"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].replace({'p': 1, 'e': 0}, inplace=True)\n\nX = data.copy()\nX.drop('class', inplace=True, axis=1)\n\ny = data['class'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot = pp.OneHotEncoder()\none_hot.fit(X)\nX_transform = one_hot.transform(X)\nX_transform.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_val, X_test, y_train_val, y_test = ms.train_test_split(X_transform, y, train_size=0.75, shuffle= True, stratify= y, random_state= 42)\nX_train_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_validation, y_train, y_validation = ms.train_test_split(\\\n                                                X_train_val, y_train_val, train_size=0.75, shuffle= True, stratify= y_train_val, random_state= 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Size of training set: {X_train.shape[0]}')\nprint(f'Size of validation set: {X_validation.shape[0]}')\nprint(f'Size of testing set: {X_test.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Candidates Models & Ensemble (BASELINE)\n* My approach will be to train individual classifier models as well as an ensemble model (voting classifier)\n* I will assess the individual classifier models based on the default hyperparameter values\n* The voting classifier will be based on all the individual classifier models with their default hyperparameter values\n* ALL features will be used for this baseline models"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = ensemble.RandomForestClassifier(random_state=42)\ndt_clf = tree.DecisionTreeClassifier(random_state=42)\next_clf = ensemble.ExtraTreesClassifier(random_state=42)\nsvc_clf = svm.LinearSVC(random_state=42)\nlog_clf = lm.LogisticRegression(random_state=42)\ngb_clf = ensemble.GradientBoostingClassifier(random_state=42)\n\nvoting_classifier = ensemble.VotingClassifier([\n                    ('rf_clf', ensemble.RandomForestClassifier(random_state=42)),\n                    ('dt_clf', tree.DecisionTreeClassifier(random_state=42)),\n                    ('ext_clf', ensemble.ExtraTreesClassifier(random_state=42)),\n                    ('svc_clf', svm.LinearSVC(random_state=42)),\n                    ('log_clf', lm.LogisticRegression(random_state=42)),\n                    ('gb_clf', ensemble.GradientBoostingClassifier(random_state=42))\n                    ], voting='hard')\n\nestimators = [rf_clf, dt_clf, ext_clf, svc_clf, log_clf, gb_clf, voting_classifier]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance on Training Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n\nfor estimator in estimators:\n    estimator.fit(X_train, y_train)\n    cv_accuracy = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='accuracy')\n    cv_f1 = ms.cross_val_score(estimator, X_train, y_train, cv=cv, n_jobs=-1, scoring='f1')\n    \n    print(estimator.__class__.__name__)\n    print(f'Avg Accuracy: {np.mean(cv_accuracy) * 100}')\n    print(f'Std Accuracy: {np.std(cv_accuracy) * 100}')\n    print(f'Avg F1: {np.mean(cv_f1) * 100}')\n    print(f'Std F1: {np.std(cv_f1) * 100}')\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance on Validation Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most likely, we have overfitted the models\n# Let's see how it perform on the validation set\n\nfor estimator in estimators:\n    print(estimator.__class__.__name__)\n    print(estimator.score(X_validation, y_validation) * 100)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance on Testing Set\n* Seems like all of the individual models performed just as well as the voting model (voting classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for estimator in estimators:\n    print(estimator.__class__.__name__)\n    print(estimator.score(X_test, y_test) * 100)\n    print()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}