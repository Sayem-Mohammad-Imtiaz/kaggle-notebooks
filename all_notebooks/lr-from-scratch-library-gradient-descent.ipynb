{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \npd.set_option('display.max_rows',None)\n\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Importing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/headbrain/headbrain.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Segregating variables: Independent and Dependent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data['Head Size(cm^3)']\ny = data['Brain Weight(grams)']\nn = len(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data into train set and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.3)\ntrain_x.shape , train_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing Linear Regression -Manual"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train_x, train_y)\nplt.xlabel('Head Size(cm^3)')\nplt.ylabel('Brain Weight(grams)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating m and c manually"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_x = np.mean(train_x)\nmean_y = np.mean(train_y)\nnum = 0\ndenom = 0\n\n# for i in range(n):\n#     num = num + (x[i] - mean_x)*(y[i] - mean_y)\n#     denom = denom + ((x[i] - mean_x))**2\n    \nnum = np.dot(np.subtract(train_x,mean_x), np.subtract(train_y,mean_y))\ndenom = np.dot(np.subtract(train_x,mean_x), np.subtract(train_x,mean_x))\n\nm = num/denom\nc = mean_y - (m*mean_x)\nprint(m,c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating dummy dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_x = np.min(train_x)-100\nmax_x = np.max(train_x)+100\nx_dummy = np.linspace(min_x,max_x,1000)\ny_dummy = m * x_dummy + c\n\nplt.scatter(train_x,train_y,color='g')\nplt.plot(x_dummy,y_dummy,color='r')\nplt.title('Simple Linear Regression')\nplt.xlabel('Head size cm^3')\nplt.ylabel('Brain weight in grams')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating R Square"},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_pred = 0\nsum_act = 0\n\nfor xi,yi in zip(train_x, train_y):\n    y_pred = (m * xi + c)\n    sum_pred += (y_pred - mean_y)**2\n    sum_act += (yi - mean_y)**2\n\n# r2 = 1-(sum_pred/sum_act)\nr2 = sum_pred/sum_act\nprint(r2)\n\n# Here we can observe that we got R**2> 0.5 . so we have good model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(x):\n    return m*x+c\n\nprint(predict(4177))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Implementing Linear Regression - Library**"},{"metadata":{},"cell_type":"markdown","source":"**Segregating variables: Independent and Dependent variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data['Head Size(cm^3)'].values\ny = data['Brain Weight(grams)'].values\nn = len(y)\n\nx = x.reshape((len(x),1)) # Converting into 2d array\ny = y.reshape((len(y),1))\n\ntrain_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.3)\n\ntrain_x.shape , train_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building model for training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\nreg = linear_model.LinearRegression(normalize=True)\nreg.fit(train_x, train_y) # accepts 2d array\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Predicting for testing dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = reg.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing predicted and guessed value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'Actual': test_y.flatten(), 'Predicted': y_predict.flatten()})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing test dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(test_x,test_y,color='g')\nplt.plot(test_x,y_predict,color='r')\nplt.title('Simple Linear Regression')\nplt.xlabel('Head size cm^3')\nplt.ylabel('Brain weight in grams')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating Mean Absolute Error"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\naccuracy2 = mean_absolute_error(test_y, y_predict)\naccuracy2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating R Square"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\naccuracy = r2_score(test_y, y_predict)\nprint(accuracy)\n\nweights = reg.coef_\nintercept = reg.intercept_\nprint(weights, intercept)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Calculating Linear Regression Using Gradient Descent**"},{"metadata":{},"cell_type":"markdown","source":"**Collecting Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data['Head Size(cm^3)']\ny = data['Brain Weight(grams)']\nx.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(x, y, m, c, alpha, iterations, n):\n\n    # Performing Gradient Descent \n    for i in range(iterations): \n        y_guess = m*x + c  # The current predicted value of Y\n        cost = 1/n * np.sum((y - y_guess)**2) # Cost function to check convergence of theta\n        D_m = (-2/n) * np.sum(x * (y- y_guess))  # Derivative wrt m\n        D_c = (-2/n) * np.sum(y - y_guess)  # Derivative wrt c\n        m = m - alpha * D_m  # Update m\n        c = c - alpha * D_c  # Update c\n        costs.append(cost)\n    return m,c, costs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(x)\nm = 0\nc = 0\ncosts = []\nalpha = 0.000000009 # The learning Rate\niterations = 30 # The number of iterations to perform gradient descent\n\nm,c, costs = gradient_descent(x, y, m, c, alpha, iterations, n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Debugging Theta"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title('Cost reduction over time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It shows, that we have learning rate neighter too small nor too large, and after some iterations, m and c are constant.**"},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_guess = m*x+c\n\nplt.scatter(x,y)\nplt.xlabel('Head Size(cm^3)')\nplt.ylabel('Brain Weight(grams)')\nplt.plot([min(x), max(x)], [min(y), max(y)], color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Calculating R Square**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\naccuracy = r2_score(y, y_guess)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R Square > 0.5, thus we can go with our model"},{"metadata":{},"cell_type":"markdown","source":"# Predicting Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(x_):\n    return m*x_+c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predict(4747))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Head Size(cm^3)'] == 4747]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}