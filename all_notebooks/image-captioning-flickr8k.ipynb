{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nHello community, welcome to this kernel. In this kernel I am going to create a Feature Extractor - Decoder model that creates captions from images. \n\nI'll use a pretrained model (VGG16) as image feature extractor and RNNs (GRU) as decoder. I will explain everything as much as I can.\n\nSo let's start."},{"metadata":{},"cell_type":"markdown","source":"# Importing The Libraries"},{"metadata":{},"cell_type":"markdown","source":"* In this section we'll import the libraries we'll use. In this kernel we'll build our model in tensorflow, (also Keras API) but after this kernel you can make a similar model using Pytorch.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os,sys,time\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport warnings as wrn\nwrn.filterwarnings('ignore')\n\ntf.compat.v1.disable_eager_execution()\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input,Dense,GRU,CuDNNGRU,Embedding\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's check our data description dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataDesc = pd.read_csv(\"../input/flickr8kimagescaptions/flickr8k/captions.txt\")\ndataDesc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Hmm, it looks like we have captions more than one per image, let's check it."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataDesc[dataDesc[\"image\"] == \"1000268201_693b08cb0e.jpg\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data Using VGG16\n\nIn this section we'll prepare our dataset to use in our decoder using VGG16. But before this I wanna explain why we won't connect VGG16's output layer to decoder's initial state.\n\nYou know, VGG16 is a really huge model that has millions of parameters and it was trained on ImageNet, a dataset that contains 1000 class and hundreds of images.\n\nIn transfer learning we don't generally train feature extractors (Convolution Operators not fully connected layers) because they work properly without training them again.\n\nSo, if we don't need to train VGG16 again, we can use it as a seperate model. We can extract features of all the images once and use them when we train model.\n\nThis will help us to save memory and time.\n\n\nFirst, we'll a define function that reads the dataset and returns images and captions."},{"metadata":{"trusted":true},"cell_type":"code","source":"ABSOLUTE_PATH = \"../input/flickr8kimagescaptions/flickr8k/images/\"\n\ndef loadDataset(resize=None):\n    images = []\n    for index,image in enumerate(np.unique(dataDesc[\"image\"])):\n        sys.stdout.write(f\"\\r Reading Image {index}\")\n        sys.stdout.flush()\n        # Reading Image\n        img = Image.open(ABSOLUTE_PATH + image)\n        # If we specified resize, resize the image.\n        if resize != None:\n            img = img.resize(resize,resample=Image.LANCZOS)\n        \n        images.append(np.asarray(img))\n            \n    captions = []\n    cnt = 0\n    for image in np.unique(dataDesc[\"image\"]):\n        sys.stdout.write(f\"\\rReading Caption {cnt+1}\")\n        cnt += 1\n        chck = []\n        # Each image has captions more than one, so we'll store captions in a list of lists.\n        for cap in dataDesc[dataDesc[\"image\"] == image][\"caption\"].values:\n            chck.append(cap)\n        captions.append(chck)\n        \n    return np.asarray(images),captions  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's download our VGG16 from google cloud storage."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = VGG16()\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see VGG16 has 138M parameters.\n* Also VGG16 has a prediction layer with softmax and 1000 neurons, but we won't make predictions, we just wanna extract features. So we'll take the fully connected layer 2 and connect the model using Keras Functional API."},{"metadata":{"trusted":true},"cell_type":"code","source":"transfer_layer = base_model.get_layer(\"fc2\")\nimage_model_transfer = Model(inputs=[base_model.input],outputs=[transfer_layer.output])\n\nimg_size = K.int_shape(image_model_transfer.input)[1:3]\nprint(img_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ImageNet has images with shape 224,224,3. And our last dense layer has 4096 neurons."},{"metadata":{"trusted":true},"cell_type":"code","source":"transfer_values_size = K.int_shape(image_model_transfer.output)[1]\ntransfer_values_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we'll define a new function. This function will read the data using the previous function and give the images to the VGG16 and return extracted features and captions."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef processDataset():\n    \n    images,captions = loadDataset(resize=img_size)\n    print(\"Reading images finished\")\n    vecs = image_model_transfer.predict(images)\n    return vecs,captions\n\ntransfer_values,captions = processDataset()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions[0][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transfer_values.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our last layer has 4096 neurons and we have 8091 images in our set, so our transfer values array has shape (8091,4096)\n\n* Now we'll add start and end marks to the captions. We're adding these marks because model has to understand where sentence starts and finishes.\n\n* Also when we create captions we'll finish to select words when we see the end mark because end mark means *Oh please stop, I finished*"},{"metadata":{"trusted":true},"cell_type":"code","source":"mark_start = \"ssss \"\nmark_end = \" eeee\"\n\ndef markCaptions(captions_listlist):\n    return [[mark_start + caption + mark_end for caption in captions_list]\n           for captions_list in captions_listlist]\n\n\nmarked_captions = markCaptions(captions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"marked_captions[0][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flattened_captions = [caption for cap_list in marked_captions for caption in cap_list]\nprint(len(flattened_captions))\nprint(flattened_captions[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we'll define a tokenizer wrap. This tokenizer wrap will have some extra functions such as a function that converts a token to it's represantation."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TokenizerWrap(Tokenizer): \n    \n    def __init__(self, texts, num_words=None):\n        Tokenizer.__init__(self, num_words=num_words)\n        \n        self.fit_on_texts(texts)\n\n        self.index_to_word = dict(zip(self.word_index.values(),\n                                      self.word_index.keys()))\n\n    def token_to_word(self, token):\n        word = \" \" if token == 0 else self.index_to_word[token]\n        return word \n\n    def tokens_to_string(self, tokens):\n        words = [self.index_to_word[token] for token in tokens if token != 0]\n        \n        text = \" \".join(words)\n\n        return text\n    \n    def captions_to_tokens(self, captions_listlist):\n        tokens = [self.texts_to_sequences(captions_list)\n                  for captions_list in captions_listlist]\n        return tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's create our tokenizer and tokenize our captions."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = 10000\ntokenizer = TokenizerWrap(texts=flattened_captions,num_words=num_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_train = tokenizer.captions_to_tokens(marked_captions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_start = tokenizer.word_index[mark_start.strip()]\ntoken_end = tokenizer.word_index[mark_end.strip()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(token_start)\nprint(token_end)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We had realised there are captions more than one per image, but we can just use one caption per image in our deep neural network.\n* There are several strategies, such as we can use data augmentation to create more image.\n* But if we do it, we can encounter with memory problems\n\n* So we'll use a different strategy, we'll choose a random caption per image and change the selected caption in each batch."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function will return random captions for specified indices.\ndef get_random_captions_tokens(idx):\n    results = [] \n    for i in idx:\n        j = np.random.choice(len(np.asarray(tokens_train[i])))\n        tokens = tokens_train[i][j]\n        results.append(tokens)\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* And now we'll use a generator that generates one batch data per iteration. This generator will randomly select images and randomly select captions for the selected images. Then it will return the data in the true format."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_images = 8091\ndef batch_generator(batch_size):\n    while True:\n        idx = np.random.randint(num_images,size=batch_size)\n        \n        t_values = np.asarray(list(map(transfer_values.__getitem__,idx)))\n        tokens = get_random_captions_tokens(idx)\n\n        num_tokens = [len(t) for t in tokens]\n        max_tokens = np.max(num_tokens)\n\n        tokens_padded = pad_sequences(tokens,\n                                      maxlen=max_tokens,\n                                      padding=\"post\",\n                                      truncating=\"post\"\n                                     )\n\n        decoder_input_data = tokens_padded[:,0:-1]\n        decoder_output_data = tokens_padded[:,1:]\n\n        x_data = {\"decoder_input\":decoder_input_data,\n                  \"transfer_values_input\":t_values\n                 }\n\n        y_data = {\"decoder_output\":decoder_output_data}\n        \n        yield (x_data,y_data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decoder Modeling\nEverything about data is ready, now we can build our decoder. In decoder we'll use a developed version of RNN. Gated Recurrent Unit. You should learn details if you don't know but knowing this is enough for starting:\n\n***Simple Recurrent Neural Networks store all the data without checking it whether is it relevant or unrelevant. And this cause vanishing gradient problem when we backpropagate the model.***\n\n***But other variants of RNNs such as GRUs and LSTMs don't store all the data, they forget redundant data using forget gates, and this prevents vanishing gradient***"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_num_captions = len(dataDesc[\"caption\"])\ntotal_num_captions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps_per_epoch = int(total_num_captions / BATCH_SIZE)\nsteps_per_epoch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's create our layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"state_size = 256\nembedding_size = 100\n\ntransfer_values_input = Input(shape=(transfer_values_size,),\n                              name=\"transfer_values_input\"\n                             )\n\ndecoder_transfer_map = Dense(state_size,\n                             activation=\"tanh\",\n                             name=\"decoder_transfer_map\"\n                            )\n\ndecoder_input = Input(shape=(None,),name=\"decoder_input\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you know when we work with texts in deep learning we generally use word vectors representation to make sense of the texts. We can use other primitive methods such as BoW but word vectors works better than them.\n\n* In this kernel we won't train word embeddings from scratch, we'll use pretrained glove vectors because our dataset is small to train great word vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec = {}\nfor line in open(\"../input/glove6b100dtxt/glove.6B.100d.txt\",encoding=\"utf-8\"):\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:],dtype=\"float32\")\n    word2vec[word] = vec\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We've created a dict that contains words and their vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"replaced_count = 0\nembedding_matrix = np.random.uniform(-1,1,(num_words,embedding_size))\nfor word,i in tokenizer.word_index.items():\n    vec = word2vec.get(word)\n    if vec is not None:\n        embedding_matrix[i] = vec\n        replaced_count += 1\n        \nprint(replaced_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* And we created a random embedding matrix and changed their values with pre trained ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_embedding = Embedding(input_dim=num_words,\n                              output_dim=embedding_size,\n                              weights=[embedding_matrix],\n                              trainable=True,\n                              name=\"decoder_embedding\"\n                             )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_gru1 = CuDNNGRU(state_size,return_sequences=True,name=\"decoder_gru1\")\ndecoder_gru2 = CuDNNGRU(state_size,return_sequences=True,name=\"decoder_gru2\")\ndecoder_gru3 = CuDNNGRU(state_size,return_sequences=True,name=\"decoder_gru3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We'll use GRU in Decoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_dense = Dense(num_words,\n                      activation=\"linear\",\n                      name=\"decoder_output\"\n                     )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's define a function that connect decoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"def connectDecoder(transfer_values):\n    initial_state = decoder_transfer_map(transfer_values)\n\n    net = decoder_input\n    net = decoder_embedding(net)\n    net = decoder_gru1(net,initial_state=initial_state)\n    net = decoder_gru2(net,initial_state=initial_state)\n    net = decoder_gru3(net,initial_state=initial_state)\n    decoder_output = decoder_dense(net)\n    \n    return decoder_output\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Everything is ready, we can create our model using Functional API."},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_output = connectDecoder(transfer_values_input)\n\ndecoder_model = Model(inputs=[transfer_values_input,decoder_input],outputs=[decoder_output])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Also we'll use a loss function that has softmax in its own, this is why our last dense layer has linear activation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sparse_cross_entropy(y_true,y_pred):\n    \n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n                                                          logits=y_pred)\n    \n    loss_mean = tf.reduce_mean(loss)\n    \n    return loss_mean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we'll define optimizer and compile the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = RMSprop(lr=1e-3)\ndecoder_target = tf.compat.v1.placeholder(dtype=\"int32\",shape=(None,None))\ndecoder_model.compile(optimizer=optimizer,\n                      loss=sparse_cross_entropy,\n                      target_tensors=[decoder_target]\n                     )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_checkpoint = \"model.keras\"\ncheckpoint = ModelCheckpoint(filepath=path_checkpoint,\n                             save_weights_only=True\n                            )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    decoder_model.load_weights(path_checkpoint)\nexcept Exception as E:\n    print(\"Something went wrong when loading the checkpoint, training will start from scratch\")\n    print()\n    print(E)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.fit_generator(generator=batch_generator(BATCH_SIZE),\n                            steps_per_epoch=steps_per_epoch,\n                            epochs=10,\n                            callbacks=[checkpoint]\n                           )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We've trained our model, but we need a function to use it. Let's define it.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_caption(image_path,max_tokens=30):\n    \n    # Reading and extracting features using VGG16.\n    image = np.asarray(Image.open(image_path).resize((224,224)))\n    transfer_values = image_model_transfer.predict(np.expand_dims(image,axis=0))\n    \n    # We'll create our input text, it will have just spaces and model will fill them.\n    decoder_input_data = np.zeros(shape=(1,max_tokens),dtype=np.int)\n    \n    token_int = token_start\n    output_text = \" \"\n    count_tokens = 0\n    \n    # While our model don't create finish token \n    while token_int != token_end and count_tokens < max_tokens:\n        \n        decoder_input_data[0,count_tokens] = token_int\n        x_data = {\"transfer_values_input\":transfer_values,\n                  \"decoder_input\":decoder_input_data\n                 }\n        \n        # Model will predict the next word.\n        decoder_output = decoder_model.predict(x_data)\n        \n        token_onehot = decoder_output[0,count_tokens,:]\n        token_int = np.argmax(token_onehot)\n        \n        sampled_word = tokenizer.token_to_word(token_int)\n        output_text = output_text + \" \" + sampled_word\n        count_tokens += 1\n        \n    plt.imshow(image)\n    plt.axis(\"off\")\n    print()\n        \n    print(\"Predicted Caption:\")\n    print(output_text.replace(\"ssss\",\" \").replace(\"eeee\",\" \").strip())\n    print()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_caption(\"../input/clothing-dataset-full/images_original/00143901-a14c-4600-960f-7747b4a3a8cd.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_caption(\"../input/clothing-dataset-full/images_original/00f6e504-7c27-438e-a5d7-bc65e557bb2b.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see our models works a bit weird because our dataset is soo soo small to create a great image captioning model, but if you're interested in image captioning you must try COCO set."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nThanks for your attention, if you have questions in your mind, feel free to ask in comment section."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}