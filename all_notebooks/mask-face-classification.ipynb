{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mask Face Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom skimage import io, transform\nimport matplotlib.pyplot as plt\nimport random\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_WIDTH=200\nIMAGE_HEIGHT=200\nIMAGE_CHANNELS=3\nEPOCHS=30\nIMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\nPATH='../input/dataset-for-mask-detection/dataset/'\nPATH2= '../input/face-mask-detection/dataset/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 불러오기\nwith_mask = os.listdir(PATH+\"with_mask\")\nwithout_mask = os.listdir(PATH+\"without_mask\")\nwith_mask2 = os.listdir(PATH2+\"with_mask\")\nwithout_mask2 = os.listdir(PATH2+\"without_mask\")\n\n\ndef add_path1(filename):\n    return PATH +'with_mask/' + filename\ndef add_path2(filename):\n    return PATH + 'without_mask/' + filename\ndef add_path3(filename):\n    return PATH2 +'with_mask/' + filename\ndef add_path4(filename):\n    return PATH2 + 'without_mask/' + filename\n\nw_mask = list(map(add_path1, with_mask))\nwo_mask = list(map(add_path2, without_mask))\nw_mask2 = list(map(add_path3, with_mask2))\nwo_mask2 = list(map(add_path4, without_mask2))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 preprocessing & label\n\ndef dataset(file_list_with, file_list_without,file_list_with2, file_list_without2,size=IMAGE_SIZE,flattened=False):\n    data = []\n    labels = []\n    sum_1 = 0\n    sum_2 = 0\n    for i, file in enumerate(file_list_with):\n        if(file == PATH + \"with_mask/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        data.append(image)\n        labels.append(1)\n    for i, file in enumerate(file_list_without):\n        if(file == PATH + \"without_mask/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        data.append(image)\n        labels.append(0)\n    for i, file in enumerate(file_list_with2):\n        if(file == PATH2 + \"with_mask/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        if(image.shape == (200,200,4)):\n            sum_1 += 1\n            continue\n        data.append(image)\n        labels.append(1)\n    for i, file in enumerate(file_list_without2):\n        if(file == PATH2 + \"without_mask/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        if(image.shape == (200,200,4)):\n            sum_2 += 1\n            continue\n        data.append(image)\n        labels.append(0)\n    \n    print(sum_1, sum_2)\n    return np.array(data), np.array(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# skimage 의 transform.resize 가 auto scale 되서 나오는듯합니다.\n# 0-1 의 범위를 가지고 있습니다.\n\nX, y = dataset(w_mask, wo_mask,w_mask2, wo_mask2)\nprint(X.shape,y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 확인하기\nsample_1 = random.choice(X)\n\nf = plt.figure()\nplt.imshow(sample_1)\nplt.show(block=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# create model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, Activation, BatchNormalization, MaxPooling2D, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Conv2D(64, (3,3), activation='relu', strides=(2,2), input_shape=(IMAGE_WIDTH,IMAGE_HEIGHT,IMAGE_CHANNELS)))\n    model.add(Conv2D(64, (3,3), activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(128, (3,3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(128, (3,3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(256, (3,3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(256, (3,3), activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = create_model()\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.10,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"partial_x_train, validation_x_train, partial_y_train, validation_y_train = train_test_split(x_train, y_train, test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(partial_x_train.shape,validation_x_train.shape,partial_y_train.shape,validation_y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The size of the training set: ',len(x_train))\nprint('The size of the partial training set: ',len(partial_x_train))\nprint('The size of the validation training set: ',len(validation_x_train))\nprint('The size of the testing set: ',len(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\ncallbacks = [learning_rate_reduction]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model1.fit(\n    partial_x_train, \n    partial_y_train,\n    validation_data=(validation_x_train, validation_y_train),\n    epochs=EPOCHS, \n    batch_size=32,\n    verbose =1,\n    callbacks=callbacks\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def smooth_curve(points, factor=0.8): #this function will make our plots more smooth\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous*factor+point*(1-factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = range(1, len(acc)+1)\nplt.plot(epochs, smooth_curve(acc), 'bo', label='Training acc')\nplt.plot(epochs, smooth_curve(val_acc), 'r-', label='Validation acc')\nplt.legend()\nplt.title('Training and Validation Acc')\nplt.figure()\n\nplt.plot(epochs, smooth_curve(loss), 'bo', label='Training loss')\nplt.plot(epochs, smooth_curve(val_loss), 'r-', label='Validation loss')\nplt.legend()\nplt.title('Training and Validation loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model1.evaluate(x_test, y_test, steps=32)\nprint('The final test accuracy: ',test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model1.predict(x_test)     # Vector of probabilities\npred_labels = np.argmax(predictions, axis = 1) # We take the highest probability","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mislabeled_images(test_images, test_labels, pred_labels):\n    \"\"\"\n        Print 25 examples of mislabeled images by the classifier, e.g when test_labels != pred_labels\n    \"\"\"\n    BOO = (test_labels == pred_labels)\n    mislabeled_indices = random.choice(np.where(BOO == 0)[0])\n    mislabeled_images = test_images[mislabeled_indices]\n    mislabeled_labels = pred_labels[mislabeled_indices]\n    print(mislabeled_labels)\n    title = \"Some examples of mislabeled images by the classifier:\"\n    f = plt.figure()\n    plt.imshow(mislabeled_images)\n    plt.show(block=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_mislabeled_images(x_test, y_test, pred_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}