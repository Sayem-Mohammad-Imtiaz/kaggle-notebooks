{"metadata":{"language_info":{"version":"3.6.3","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"a1d7df90-e823-4164-8119-e03130e64093","_uuid":"fda389deaabd50e04e58b7a6a0e1feb1515ab955"},"cell_type":"markdown","source":"> Elon Musk's Tweets"},{"metadata":{"_cell_guid":"3b8604e9-5c00-4d1a-a827-934e8cf8ffab","collapsed":true,"_uuid":"00ce449e2624e1bbf042520f92e665a4665417c7"},"cell_type":"code","outputs":[],"source":"# Import Modules\n\nimport pandas as pd\nimport re, string\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport networkx as nx\nimport re, string\nimport nltk\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\nfrom scipy.misc import imread\nfrom subprocess import check_output\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":1},{"metadata":{"_cell_guid":"984955ac-de44-45b1-aed7-35fb943509cf","collapsed":true,"_uuid":"e5c0e178c345fe6bcc3a694e30feb9c433a3adc3"},"cell_type":"code","outputs":[],"source":"# Inputfile\n\ndef load_tweets(tweet_file):\n\n    \"\"\" Load and process a Twitter analytics data file \"\"\"\n    # Read tweet data (obtained from Twitter Analytics)\n    tweet_df = pd.read_csv(tweet_file,sep = ',', low_memory = False,encoding=\"L5\")\n    return tweet_df\n\ntweet_df = load_tweets('../input/dataelonmuskstweet/data_elonmusk.csv')\n","execution_count":2},{"metadata":{"_cell_guid":"cb556089-797b-481a-951c-58a91c65f97c","_uuid":"2a27560cf33d2d28e932467656e2578cd5fb09a5"},"cell_type":"markdown","source":"> Data visualization"},{"metadata":{"_cell_guid":"0305a7ea-2b29-40d5-9194-f95e8e4cbf4c","_uuid":"a2d910140e586e202f47304e198c3f2c9f68cfc2"},"cell_type":"code","outputs":[],"source":"#-----------------------------------------------------------------#\n\ndf = pd.DataFrame(tweet_df)\ndf['Time'] = pd.to_datetime(df['Time'])\ndf['Time'] = pd.to_datetime(df['Time'], format='%y-%m-%d %H:%M:%S')\ndf['Time'].hist(label=\"Frequency\",alpha=0.7)\nplt.legend()\nplt.title(\"Tweet Activty Over The Years\")\nplt.show()\n","execution_count":3},{"metadata":{"_cell_guid":"61a62396-b4a5-4e81-9813-2289c70ffc69","_uuid":"2a40dcc9d5cb1c9abc075bbe85df2eb823f05fb3"},"cell_type":"code","outputs":[],"source":"#-----------------------------------------------------------------#\n\ndf['Retweet from'].value_counts(dropna=True)[:5].plot(kind='bar')\nplt.title('Retweet from')\nplt.tight_layout()\nplt.show()\n","execution_count":4},{"metadata":{"_cell_guid":"6cd58c48-662b-4a60-9ca1-46c97c9fa576","_uuid":"ab4d67ed542a7341f0dde4f99b3d53a1ed6b9e20"},"cell_type":"markdown","source":"> Cleaning Tweets"},{"metadata":{"_cell_guid":"0d52d44f-f1d6-4697-8e32-e8607b58b9b9","_uuid":"8f01e62a0aad72274ce112759f5a38389b0fa3c8"},"cell_type":"code","outputs":[],"source":"# Total tweets\nprint('Total tweets this period:', len(tweet_df.index), '\\n')\ndf = df.drop('row ID',1)\ndf = df.drop('User',1)\ntweet = df['Tweet'].tolist()\n\nclean = [i.replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ').replace('(',' ').replace(')',' ').replace('\\n',' ').replace('http',' ') for i in tweet]\nclean = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", str(clean)).lower()\nclean = re.sub(r'https?:\\/\\/.*\\/\\w*',' ',clean) # Remove hyperlinks\nclean = re.sub(r'['+string.punctuation+']+', ' ',clean) # Remove puncutations like 's\n\n\nstop_words = set(stopwords.words('english'))\ntext_=[]\nfor w in clean.split():\n    if w not in stop_words and len(w) > 2:\n        ps = PorterStemmer()\n        text_.append(ps.stem(w)) # stemming words\n#print(text_)\n","execution_count":5},{"metadata":{"_cell_guid":"9ae0cda9-be09-48f5-abf3-cf87d18e55bd","_uuid":"0ff166683aa3ce23fa9a653b836b9dd57f870387"},"cell_type":"code","outputs":[],"source":"def word_list(text_):\n    wordfreq = {}\n    for word in text_:\n        if word in wordfreq.keys():\n            wordfreq[word] += 1\n        else:\n            wordfreq[word] = 1\n    return wordfreq\n\nwordfreq = word_list(text_)\n\nrslt = pd.DataFrame(Counter(wordfreq).most_common(15),columns=['Word', 'Frequency']).set_index('Word')\nprint('All frequencies, including STOPWORDS:')\nprint('=' * 60)\nprint(rslt)\nprint('=' * 60)\n\nax1 = rslt.plot.bar(rot=0,  width=0.8)\nplt.xticks(fontsize=9,rotation=25)\nplt.title('All frequencies, including stopwords')\nplt.tight_layout()\nplt.show()\n","execution_count":6},{"metadata":{"_cell_guid":"ef4f1b90-7c65-47bf-bdb4-6de9721217bc","_uuid":"17c77737e8296db2806dac14149047ba219d4b27"},"cell_type":"code","outputs":[],"source":"# Create bigrams\nbgs2 = nltk.bigrams(text_)\n\n# compute frequency distribution for all the bigrams \nfdist2 = nltk.FreqDist(bgs2)\n\n# for k,v in sorted(fdist2.items(),key = operator.itemgetter(1)):\n#    print(k,v)\n\nrslt2 = pd.DataFrame(Counter(fdist2).most_common(10),columns=['Word', 'Frequency']).set_index('Word')\nprint('All frequencies, including STOPWORDS:')\nprint('=' * 60)\nprint(rslt2)\nprint('=' * 60)\n\nax2 = rslt2.plot.bar(rot=0,  width=0.8)\nplt.xticks(fontsize=9, rotation=35)\nplt.tight_layout()\nplt.show()","execution_count":7},{"metadata":{"_cell_guid":"d35466fe-08a2-4e0f-8e15-58314aa0b5bf","_uuid":"e1d73e9b10489acbbb4257fc16d9fae06719e568"},"cell_type":"markdown","source":"> Word Cloud"},{"metadata":{"_cell_guid":"ba12bf01-3468-400d-8f6c-e3a277c4dbfb","_uuid":"4407c923fb288e16a57dc95b23b338dc2d14ec9a"},"cell_type":"code","outputs":[],"source":"#----------------------------------------------------------------#\n\nlogo_mask = imread('../input/tweet-mask/tweet_mask.png', flatten=True)\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(clean)\n\n# adding movie script specific stopwords\nstopwords = set(STOPWORDS)\nstopwords.add(\"co\")\nstopwords.add(\"rt\")\nstopwords.add(\"dr\")\nstopwords.add(\"et\")\nstopwords.add(\"will\")\nstopwords.add(\"re\")\n\n# lower max_font_size\nwordcloud = WordCloud(max_font_size=40, stopwords=stopwords, background_color='black',\n                      width=1800,\n                      height=1400,\n                      mask=logo_mask, normalize_plurals=bool).generate(clean)\n\nplt.figure()\nplt.imshow(wordcloud,cmap=plt.cm.gray)\nplt.axis(\"off\")\nplt.show()","execution_count":8},{"metadata":{"_cell_guid":"bbd4b652-f121-47a4-9921-c1fc497c3e07","_uuid":"3773c0ea740adae4da5933c2d4f5c34ac51b3586"},"cell_type":"markdown","source":"> NetworkX"},{"metadata":{"_cell_guid":"36be61b6-cafe-436a-887b-7128d310dd38","_uuid":"090ad308729ef803e3f0ff15f6584d889ab84ef4"},"cell_type":"markdown","source":"... to be continued "}],"nbformat_minor":1}