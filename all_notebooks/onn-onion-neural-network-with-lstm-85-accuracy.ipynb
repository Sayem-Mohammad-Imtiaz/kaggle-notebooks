{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WHY?\n\nFirst of all, I want to introduce what's so called 'onion'. Of course, a kind of cooking ingredient (although one of my friend treat it like a fruit). A story could be so utopiad, then crowds don't believe it, in spite of the fact that it happened. Later, I will show you about the story. \n\nThe onion articles labeled as 1, and the r/NotTheOnion articles are labeled 0."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/onion-or-not/OnionOrNot.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's have a look"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets see this onion"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data['text'][0])\ndisplay(data['text'][2])\ndisplay(data['text'][5])\ndisplay(data['text'][9])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And this is r/NotTheOnion"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data['text'][1])\ndisplay(data['text'][6])\ndisplay(data['text'][7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### See the differences? they were awful, right? "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nlabels = [1,0]\nplt.pie(data['label'].value_counts() )\nplt.legend(labels)\nplt.title('Onions or not')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the library and Preprocessing\n\nThe ONN was based on NLP with DNN approach.\n\n1. First, we will remove punctuation.\n2. Second, decapitalize letters\n3. Tokenization"},{"metadata":{},"cell_type":"markdown","source":"### 1. Remove Puctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_process = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_process.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_process[\"text\"] = data_process[\"text\"].str.replace('[^a-zA-Z]', ' ', regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data['text'][2])\ndisplay(data_process['text'][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Turn to lowercase"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_process[\"text\"] = data_process[\"text\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data['text'][2])\ndisplay(data_process['text'][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = 20000\nmax_len = 150\nemb_size = 128\nX = data_process[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token = Tokenizer(num_words = num_words)\ntoken.fit_on_texts(list(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = token.texts_to_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data['text'][2])\ndisplay(data_process['text'][2])\ndisplay(X[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(X[2], label = 'sample text spectra')\nplt.title(str(data['text'][2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence\n\nX = sequence.pad_sequences(X, maxlen = 150)\ny = pd.get_dummies(data_process['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I got the feature extraction notes on https://www.kaggle.com/nihalbey/spam-detection-and-deep-nlp . I was stuck before wondering how to be done with those dataframe"},{"metadata":{},"cell_type":"markdown","source":"## NLP\n\nFinally this part came, lets start!\n\nThe traditional LSTM was time consuming, therefore the bidirectional used and combined with Dense layer for time saving, thanks to this kernel https://www.kaggle.com/lsjsj92/toxic-nlp-with-keras-lstm#648911 . "},{"metadata":{},"cell_type":"markdown","source":"### Separating train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom keras.models import Sequential\nfrom keras.layers import Input,Dense, LSTM, Dropout, Flatten, Embedding, Bidirectional, GlobalMaxPool1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model():\n    \n    inp = Input(shape = (max_len, ))\n    layer = Embedding(num_words, emb_size)(inp)\n    layer = Bidirectional(LSTM(50, return_sequences = True, recurrent_dropout = 0.1))(layer)\n    \n    layer = GlobalMaxPool1D()(layer)\n    layer = Dropout(0.2)(layer)\n    \n    \n    layer = Dense(50, activation = 'relu')(layer)\n    layer = Dropout(0.2)(layer)\n    layer = Dense(50, activation = 'relu')(layer)\n    layer = Dropout(0.2)(layer)\n    layer = Dense(50, activation = 'relu')(layer)\n    layer = Dropout(0.2)(layer)\n    \n    \n    layer = Dense(2, activation = 'softmax')(layer)\n    model = Model(inputs = inp, outputs = layer)\n    \n    \n    model.compile(loss = 'binary_crossentropy', optimizer = 'nadam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.utils import plot_model\nimport matplotlib.image as mpimg\n\nmodel = model()\nmodel.summary()\n\nplot_model(model, to_file='onion.png',show_shapes=True, show_layer_names=True)\nplt.figure(figsize = (30,20))\nimg = mpimg.imread('/kaggle/working/onion.png')\nimgplot = plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfile_path = 'save.hd5'\ncheckpoint = ModelCheckpoint(file_path, monitor = 'val_loss', save_best_only=True)\nearly_stop = EarlyStopping(monitor = 'loss', patience = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size = 32, epochs = 3, validation_split = 0.1, callbacks = [checkpoint,early_stop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Limit the time, and save the model. \n\nHow to save and load if interrupted: https://www.kaggle.com/danmoller/make-best-use-of-a-kernel-s-limited-uptime-keras"},{"metadata":{},"cell_type":"markdown","source":"#### Prepare yourself, it will be long, so its better to save the model"},{"metadata":{},"cell_type":"markdown","source":"## Lets see the prediction result!"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loss = history.history['val_loss']\nloss = history.history['loss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('validation loss: ', val_loss[-1])\nprint('training loss: ', loss[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test, y_test)\nprint(model.metrics_names)\nprint(score)\nprint('test loss: ', score[0])\nprint('test accuracy: ', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}