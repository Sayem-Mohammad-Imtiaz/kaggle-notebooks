{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Import library \n\n필요한 library를 수집합니다.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport missingno as msno\n\nfrom glob import glob\nimport os, random, time, gc, warnings\n\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgbm\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.feature_selection import RFECV\n\n\nfrom sklearn.cluster import KMeans\n\nfrom datetime import datetime\n\nfrom math import sqrt\n\nimport folium\nfrom folium import Marker, Icon, CircleMarker\n\nfrom pdpbox import pdp, info_plots\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 500)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Dataset\n\n주최 측에서 제공한 데이터를 확인 & Load 합니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 상위 directory인 input에 무엇이 들어있나 확인합니다.\nos.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# glob 함수를 이용하여 주최 측이 제공한 데이터셋을 확인합니다.\nglob('../input/dacon-bus-dataset/*.*')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 첫 10개의 row를 출력해보도록 합시다.\n!head ../input/dacon-bus-dataset/train.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터를 load합니다.\n# train/test --> string형식으로 저장되어 있는 `date` column은 datetime형식으로 수집합니다.\n# bus_bts    --> string형식으로 저장되어 있는 `geton_date`, `getoff_date` columns은 datetime형식으로 수집합니다.\ndef load_dataset(path):\n    train = pd.read_csv(path + 'train.csv', parse_dates=['date'])\n    test  = pd.read_csv(path + 'test.csv', parse_dates=['date'])\n    df_bus = pd.read_csv(path + 'bus_bts.csv', parse_dates = ['geton_date', 'getoff_date'] )\n    sample_submission = pd.read_csv(path + 'submission_sample.csv')\n    return train, test, df_bus, sample_submission\n\npath = '../input/dacon-bus-dataset/'\n%time train, test, df_bus, sample_submission = load_dataset(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 데이터 이해하기\n\n1. 데이터의 사이즈는 ?\n 1.1 모델 학습에 적합한 형태인가?\n2. Train/Test는 어떻게 분리되어 있는가?\n3. Missing Value는?\n4. Target Variable의 분포는?\n5. 데이터의 특이한/주목해야할 부분은?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/Test-set을 각각 체크해봅니다.\ndisplay(train.head(3))\n\ndisplay(test.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. 데이터의 사이즈는?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-- Size -- \")\nprint(f\"Train-set : {train.shape}\")\nprint(f\"Test-set  : {test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-set에만 있는 칼럼은?\ntrain.columns.difference( test.columns )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/Test-set은 어떻게 분리되었을까?\ndisplay(train.head(3))\ndisplay(test.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Train/Test는 어떻게 분리되어 있는가?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### (1) id에 차이가 있는가?\n\n--> id는 Train/Test-set 각각의 key로 사용됨. 특별한 의미를 지니지 않고 테이블의 각 row를 구분짓는데만 사용되기에 모델링 시 제거해줘야하는 column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-set의 id는?\nprint(\"Min/Max of id in Train-set\")\ndisplay( train['id'].agg(['min','max']) )\n\nprint('='* 80)\nprint(f'Size : {len(train)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test-set의 id는?\nprint(\"Min/Max of id in Test-set\")\ndisplay( test['id'].agg(['min','max']) )\n\nprint('='* 80)\nprint(f'Size : {len(test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (2) date에 차이가 있는가?\n\n--> 맞다. 전체 데이터는 시간을 기준으로 Train/Test-set으로 나뉘어졌다.\n\n- 2019-09-01 ~ 2019-09-30에 해당하는 데이터는 Train-set\n- 2019-10-01 ~ 2019-10-16에 해당하는 데이터는 Test-set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-set의 date는?\nprint(\"Min/Max of date in Train-set\")\ndisplay( train['date'].agg(['min','max']) )\n\nprint('='* 80)\nprint(f'Size : {len(train)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-set의 date는?\nprint(\"Min/Max of date in Test-set\")\ndisplay( test['date'].agg(['min','max']) )\n\nprint('='* 80)\nprint(f'Size : {len(test)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/Test의 date 분포는?\n\n# Figure을 정의\nplt.figure(figsize = (12,8))\n\n# Train/Test-set 각각에서 특정 date가 몇 번 등장했는지 시각화 시킴\ntrain['date'].value_counts().sort_index().plot(color='b', lw=2, label='train')\ntest['date'].value_counts().sort_index().plot(color='r',  lw=2, label='test')\n\nplt.legend()\nplt.xlabel(\"date\")\nplt.ylabel(\"# of rows\")\nplt.title(\"Distribution of date in Train/Test-set\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. 정류장에 차이가 있는가?\n\n--> 큰 차이는 없다. \n\n다만,\n- Train-set에는 있지만, Test-set에는 없는 bus_route_id가 30개 존재하며\n- Test-set에는 있지만,  Train-set에는 없는 bus_route_id가 18개 존재한다.\n\nTrain-set에 등장하지 않으나, Test-set에 등장하는 경우 모델은 학습되지 않은 데이터를 가지고 예측을 해야하는 문제가 있다.\n\n해당 부분들을 어떻게 보완할 지 생각보는 것도 좋을 듯하다.\n\n--> 아래에서 확인할 수 있듯이, Test-set에만 등장하는 bus_route_id는 Train/Test-set 모두에 등장하는 bus_route_id보다 평균 탑승승객수가 적은 경향이 있는 듯이 보인다.\n\n--> 해결책 중 하나로, Train-set기간동안에 해당 bus_route_id의 퇴근 시간의 탑승 승객수를 \"0\"으로 레이블링하여 새로운 row를 추가할 수 있을 것이다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/Test-set의 고유한 bus_route_id를 구함.\ntrain_bus_route_id_set = set(train['bus_route_id'])\ntest_bus_route_id_set  = set(test['bus_route_id'])\n\n\n# Train/Test-set 고유한 bus_route의 개수를 구함.\nprint(f\"Train-set에 있는 고유한 bus_route의 개수 : { len(train_bus_route_id_set) }\")\nprint(f\"Test-set에 있는 고유한 bus_route의 개수 : { len(test_bus_route_id_set) }\")\n\n# Train/Test-set 모두에 포함되어있는 bus_route를 구함.\nprint('='* 80)\ncommon_bus_route_id = train_bus_route_id_set.intersection(test_bus_route_id_set)\nprint(f\"Train/Test-set에 공통으로 포함되어 있는 bus_route 개수 : {len(common_bus_route_id)}\")\n\n# Train-set에만 있는 bus_route를 구함.\nprint('='* 80)\nonly_train_bus_route = train_bus_route_id_set.difference(test_bus_route_id_set)\nprint(f\"Train-set에만 있는 bus_route는 총 { len(only_train_bus_route) }개 입니다.\")\nprint(f\"Train-set에만 있는 bus_route는 : { sorted(only_train_bus_route ) }\")\n\n# Test-set에만 있는 bus_route를 구함.\nprint('='* 80)\nonly_test_bus_route = test_bus_route_id_set.difference(train_bus_route_id_set)\nprint(f\"Test-set에만 있는 bus_route는 총 { len(only_test_bus_route) }개 입니다.\")\nprint(f\"Test-set에만 있는 bus_route는 : { sorted( only_test_bus_route ) }\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test-set에만 있는 bus_route_id와 Train/Test-set모두에 등장하는 bus_route_id의 탑승/하차 칼럼들의 합을 비교해보자\n\nprint(\"오직 Test-set에만 존재하는 bus_route_id\")\ndisplay(test[test['bus_route_id'].isin(only_test_bus_route)].head() )\n\nprint(\"=\"*80)\nprint(\"Train/Test-set 모두에 존재하는 bus_route_id\")\ndisplay(test[test['bus_route_id'].isin(common_bus_route_id)].head() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 탑승 관련 columns & 하차 관련 columns\nride_columns = [col for col in test.columns if '_ride' in col] + ['bus_route_id','date']\ntake_off_columns = [col for col in test.columns if '_takeoff' in col] + ['bus_route_id','date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 두 경우의 탑승 관련 columns 비교\nplt.figure(figsize=(12,5))\n\ntest[test['bus_route_id'].isin(only_test_bus_route)][ride_columns].groupby(['date','bus_route_id'])['8~9_ride'].sum().groupby('date').mean().plot(color='b', lw=2, label='only in Test-set')\ntest[test['bus_route_id'].isin(common_bus_route_id)][ride_columns].groupby(['date','bus_route_id'])['8~9_ride'].sum().groupby('date').mean().plot(color='r', lw=2, label='Both in Train/Test-set')\nplt.legend()\nplt.title(\"Average number of passengers\\nbus_route_id only in Test-set VS bus_route_id both in Train/Test-set \");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Missing Value는 존재하는가?\n\n--> 존재하지 않는다","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Values\nmsno.matrix(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Value 확인\nprint(\"Train-set\")\ndisplay( train.isnull().sum() )\n\nprint('=' * 80)\n\nprint(\"Test-set\")\ndisplay( test.isnull().sum() )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Target Variable의 분포는?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Variable의 분포를 살펴보자\ntarget_col = '18~20_ride'\n\ntrain[target_col].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dist-plot을 그려보도록 한다.\n# --> (1) 0이 굉장히 많다. \n# --> (2) right-skewed된 형태이며, 값이 매우 큰 outlier들이 존재한다.\nsns.distplot( train[target_col] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# log1p transformation을 적용해봐도 정규분포에 근사한 모양을 보이지 않는다.\nsns.distplot( np.log1p( train[target_col] ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Target Variable이 0인 데이터는 어떤 특징을 가지고 있는가?\n\n--> 테이블에 적재된 row들은 해당 승차/하차 관련 칼럼들의 rowsum이 1이상인 데이터\n--> 퇴근시간에 승차한 승객이 있더라도 승차/하차 관련 칼럼들의 rowsum이 0인 경우는 우리가 볼 수 있는 테이블에 적재되지 못하였다.\n\n--> 나중에 data-augmentation을 시도하고자 하는 경우 유용한 정보가 될 수도..?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 탑승 관련 columns & 하차 관련 columns\nride_columns = [col for col in test.columns if '_ride' in col]\ntake_off_columns = [col for col in test.columns if '_takeoff' in col] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-set의 승차관련 칼럼들의 rowsum\ndisplay( train[train[target_col]==0][ride_columns].sum(axis=1).agg(['min','max']) )\n\n# Train-set의 하차관련 칼럼들의 rowsum\ndisplay( train[train[target_col]==0][take_off_columns].sum(axis=1).agg(['min','max']) )\n\n# Train-set의 승하차관련 칼럼들의 rowsum\ndisplay( train[train[target_col]==0][ride_columns + take_off_columns].sum(axis=1).agg(['min','max']) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. 데이터의 특이한/주목해야할 부분은?\n\n1. 해당 버스정류장에 대한 각각의 위도, 경도가 제공이 되어있는 상태로 같은 정류장 이름이지만 위도와 경도가 서로 다른 경우가 존재합니다. 해당 경우는, 같은 정류장 이름을 가지고 있는 길 건너편의 정류장에 해당이 됩니다.\n\n1. 해당 데이터에는 버스카드를 통해 결제를 한 경우에 대한 정류소 승, 하차 데이터로 모든 승차정보의 경우는 기록이 되어있지만, 버스에서 하차를 할 때, 버스카드를 찍지 않는 경우, 해당 기록이 비어 있는 상태입니다. 따라서, 승차 인원수와 하차 인원수가 동일하지 않고 다소 차이가 있음을 미리 알려드립니다.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1. 같은 정류장 이름이 여러 번 나오는 경우?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# (1)의 경우에는 어떤 것들이 있나 예시를 통해 살펴보도록 하자\n# 하나의 station_name에 여러 개의 station_code가 기록되어 있는 경우는 어떤 상황인가?\nmultiple_station_name = train.groupby('station_name')['station_code'].nunique()\nmultiple_station_name = multiple_station_name[multiple_station_name>=7]\nprint(multiple_station_name)\n\ndf_sample = train[train['station_name'].isin(multiple_station_name.index)][['station_code','station_name','latitude','longitude']]\ndf_sample = df_sample.drop_duplicates().reset_index(drop=True)\ndf_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generateMap(default_location=[33.35098, 126.79807], default_zoom_start=10):\n    base_map = folium.Map(location=default_location, \n                          control_scale=True, \n                          zoom_start=default_zoom_start)\n    \n    # 여러 개의 정거장에 대해서 Icon 생성하기\n    for row in df_sample.itertuples():\n        station_code, station_name, latitude, longitude = row[1:]\n        \n        # Create Icon\n        if station_name == '금악리':\n            icon = Icon(color='red',icon='station')\n        else:\n            icon = Icon(color='blue',icon='station')\n                \n        # Add Marker\n        Marker(location=[ latitude , longitude], \n               popup=f'station_code : {station_code} station_name : {station_name}',\n               icon = icon).add_to(base_map)\n        \n    \n    base_map.save('하나의 station_name에 여러개의 station_code.html')\n    return base_map\n\ngenerateMap()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.1 고유한 정거장의 기준은?\n\n다음으로 `고유한 정거장`을 어떻게 분리할 수 있는지 알아보도록 하겠습니다.\n\n먼저 `고유한 정거장`은 아래와 같은 조건을 만족해야한다고 생각합니다.\n    1. 위경도가 1개여야한다.\n    2. 시내버스 혹은 시내버스 1개의 노선만 있어야 한다.\n    3. 특정 날짜에 특정 노선에서 특정 정거장의 승객 수를 기록한 row는 1개여야 한다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"####  **station_name를 기준으로 삼는다면?**\n\n--> station_name에는 여러개의  station_code가 매핑되어있음 & 여러 개의 위치 정보를 갖는 것으로 보임\n\n--> 고유한 정거장의 기준을 station_name으로 잡기에는 어려워보임","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display( train.groupby('station_name')['station_code'].nunique().value_counts() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# station_name를 기준으로 삼는다면?\n# --> 하나의 station_name가 여러 개의 latitude, longitude를 갖는 것으로 보임\ndisplay( train.groupby('station_name')['latitude'].nunique().value_counts() )\ndisplay( train.groupby('station_name')['longitude'].nunique().value_counts() )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### station_code는 어떨까?\n\n--> 적합하다고 생각된다.\n\n따라서 \"특정 날짜\"에 \"특정 버스 노선\"의 \"특정 정거장\"에서 \"몇 명의 승객이 승하차했는지\"를 파악하기 위해선\n`date, bus_route_id, station_code`로 그룹화를 시켜서 파악해야 한다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# station_code를 기준으로 삼는다면?\n# --> station_code에는 1개의  station_name이 매핑되어있음.\ndisplay( train.groupby('station_code')['station_name'].nunique().value_counts() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# station_code를 기준으로 삼는다면?\n# --> station_code는 latitude, longitude와 1대1 관계를 만족함\ndisplay( train.groupby('station_code')['latitude'].nunique().value_counts() )\ndisplay( train.groupby('station_code')['longitude'].nunique().value_counts() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# station_code를 기준으로 삼는다면?\n# --> station_code는 in_out와 1대1 관계를 만족함\ndisplay( train.groupby('station_code')['in_out'].nunique().value_counts() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# date, bus_route_id, station_code이 특정 날짜에 몇번 등장했는지 재확인하기 \ndisplay( train.groupby(['date','bus_route_id','station_code']).size().value_counts() )\nprint('='* 80)\nprint(f'Train-set size : {len(train)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation 전략\n\n시간을 기준으로 앞의 3주를 local_train으로, 뒤의 1주를 local_test를 만들어 본다.\n\n1. local_train을 통하여 validation 전략을 실험해보고\n2. 해당 모델로 local_test의 값을 예측한 값이 어느정도 차이가 나는지 살펴본다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# local_train/local_test를 만든다.\nlocal_train = train[train['date']<='2019-09-24'].reset_index(drop=True)\nlocal_test  = train[train['date']>'2019-09-24'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical variable인 'bus_route_id','in_out','station_code','station_name' 에 대해선 label_encoding을 적용해주고,\n# numeric variable들에 대해선 있는 그대로 학습을 시켜보도록 한다.\nlbl = LabelEncoder()\n\n# Implement Label Encoding \ncat_cols = ['bus_route_id','in_out','station_code','station_name']\nfor col in tqdm_notebook( cat_cols ):\n    # local_train과 local_test를 concat하여 temp_df에 저장\n    temp_df = pd.concat([ local_train[[col]], local_test[[col]] ] , axis=0)\n    \n    # Label-Encoding을 fitting함\n    lbl.fit( temp_df[col] )\n    \n    # local_train/local_test에 label_encoding한 값을 대입함\n    local_train[col] = lbl.transform(local_train[col])\n    local_test[col] = lbl.transform(local_test[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM을 통하여 모델링 하기","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"local_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델에 쓰일 parameter 정의하기\nn_splits= 5\nNUM_BOOST_ROUND = 100000\nSEED = 1993\nlgbm_param = {'objective':'rmse',\n              'boosting_type': 'gbdt',\n              'random_state':1993,\n              'learning_rate':0.3,\n              'subsample':0.7,\n              'tree_learner': 'serial',\n              'colsample_bytree':0.78,\n              'early_stopping_rounds':50,\n              'subsample_freq': 1,\n              'reg_lambda':7,\n              'reg_alpha': 5,\n              'num_leaves': 96,\n              'seed' : SEED\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 제거해야하는 columns들 정의\ndrop_cols = ['id','date', target_col]\n\n# local_train/local_test에 대한 label 정의\nlocal_train_label = local_train[target_col]\nlocal_test_label  = local_test[target_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# local_train/local_test의 예측값을 저장하기 위한 OOF 만들기 & CV를 저장할 list 정의\noof_train = np.zeros((local_train.shape[0], ))\noof_test = np.zeros((local_test.shape[0], ))\n\ncv_list = []\n\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = local_train, y = local_train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = local_train.iloc[trn_ind].drop(drop_cols, 1), local_train_label[trn_ind]\n    X_valid , y_valid = local_train.iloc[val_ind].drop(drop_cols, 1), local_train_label[val_ind]\n    \n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    valid_pred = model.predict(X_valid)\n    test_pred  = model.predict( local_test.drop(drop_cols,1) )\n    \n    # CV를 저장\n    cv_list.append( sqrt( mean_squared_error( y_valid, valid_pred )  ) )\n    \n    # OOF에 예측값을 저장\n    oof_train[val_ind] = valid_pred\n    oof_test += test_pred/n_splits\n    print('='*80)\n    \nprint(f\"<LOCAL_TRAIN> OVERALL RMSE : {sqrt( mean_squared_error( local_train_label, oof_train ) )}\")\nprint(f\"<LOCAL_TEST>  OVERALL RMSE : {sqrt( mean_squared_error( local_test_label, oof_test ) )}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 실제값과 예측값의 분포 비교\nfig, axes = plt.subplots( 1, 2, figsize=(20, 8), sharex=True, sharey=True)\n\n# y=x를 그리기 위하여\nx_range = np.linspace(0, 300, 1000)\n\n# <SUBPLOT 1> : local_train에 대한 예측/실제값 비교\naxes[0].scatter( local_train_label, oof_train )\naxes[0].set_xlabel(\"Prediction\")\naxes[0].set_ylabel(\"Real\")\n\n# y=x 그리기\naxes[0].plot(x_range, x_range, color='r')\n\n# <SUBPLOT 2> : local_test에 대한 예측/실제값 비교\naxes[1].scatter( local_test_label, oof_test )\naxes[1].set_xlabel(\"Prediction\")\naxes[1].set_ylabel(\"Real\")\n\n# y=x 그리기\naxes[1].plot(x_range, x_range, color='r');\n\n# Super Title \nplt.suptitle('Comparison between Prediction VS Real');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 실제값 vs 예측값 비교\nplt.figure(figsize=(12,6))\n\nsns.distplot( oof_train, color='r' , label='Prediction for Local-Train')\nsns.distplot( local_train_label, color='b', label='Real' )\nplt.legend()\nplt.title(\"Comparing the real vs prediction in Local-Train\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 실제값 vs 예측값 비교\nplt.figure(figsize=(12,6))\n\nsns.distplot( oof_test, color='r' , label='Prediction for Local-Test')\nsns.distplot( local_test_label, color='b', label='Real' )\nplt.legend()\nplt.title(\"Comparing the real vs prediction in Local-Test\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- LOCAL_TRAIN에 해당하는 부분보다 LOCAL_TEST에 해당하는 부분의 예측력이 떨어지는 모습을 보이긴 한다.\n- 실제 값보다 예측 값이 지나치게 큰 경우들이 존재하는데, 해당 경우들이 어떤 것들인지 살펴봐야겠다.\n- 아래와 같은 다른 validation 기법들도 고려해볼 수 있을 것이다.\n - HOLD OUT\n - GROUP FOLD BY weekofmonth \n- 본 강의에서는 KFOLD를 가지고 baseline model을 만들어 보기로 한다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del local_train, local_test, local_train_label, local_test_label; gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BASELINE MODEL 만들기\n\n아래와 같은 모델들을 만들어보고, 어떤 타입의 모델이 본 문제를 풀기 적합한가 살펴보도록 하자.\n추가적으로 linear model에서는 coefficient를 통하여, tree-based model에서는 feature_importance를 통하여 변수의 중요도를 살펴보며,\n향후 모델의 성능을 높이기 위해선 어떤 방법이 있을까 생각해보자.\n\n- Linear Regression \n    - Ridge\n    - Lasso\n- Tree-Based \n    - Decision Tree\n    - Random Forest\n    - Light GBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_label 정의\ntrain_label = train[target_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical variable에 대해서는 Label-Encoding을 수행 \n# --> One-Hot Encoding가 바람직하다고 생각되나 메모리 문제로 실행할 수 없음.\nlbl = LabelEncoder()\n\n# Implement Label Encoding \ncat_cols = ['bus_route_id','in_out','station_code','station_name']\nfor col in tqdm_notebook( cat_cols ):\n    \n    # Label-Encoding을 fitting함\n    lbl.fit( train[col] )\n    \n    # local_train/local_test에 label_encoding한 값을 대입함\n    train[col] = lbl.transform(train[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nridge_oof_train = np.zeros((train.shape[0]))\nlasso_oof_train = np.zeros((train.shape[0]))\ndt_oof_train = np.zeros((train.shape[0]))\nrf_oof_train = np.zeros((train.shape[0]))\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # (1) Ridge\n    print(\"---TRAINING RIDGE---\")\n    ridge = Ridge(random_state = 1993)\n    \n    ridge.fit(X_train, y_train)\n    \n    ridge_valid_pred = ridge.predict(X_valid)\n    ridge_oof_train[val_ind] = ridge_valid_pred\n    \n    # (2) Lasso\n    print(\"---TRAINING LASSO---\")\n    lasso = Lasso(random_state = 1993)\n    \n    lasso.fit(X_train, y_train)\n    \n    lasso_valid_pred = lasso.predict(X_valid)\n    lasso_oof_train[val_ind] = lasso_valid_pred\n    \n    # (3) Decision Tree\n    print(\"---TRAINING DECISION TREE---\")\n    dt = DecisionTreeRegressor(random_state=231)\n    \n    dt.fit(X_train, y_train)\n    \n    dt_valid_pred = dt.predict(X_valid)\n    dt_oof_train[val_ind] = dt_valid_pred\n    \n    \n    # (4) Random Forest\n    print(\"---TRAINING RANDOM FOREST---\")\n    rf = RandomForestRegressor(random_state=231, n_estimators=20 )\n    \n    rf.fit(X_train, y_train)\n    \n    rf_valid_pred = rf.predict(X_valid)\n    rf_oof_train[val_ind] = rf_valid_pred\n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       verbose_eval= 0)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Ridge> OVERALL RMSE         : {sqrt( mean_squared_error( train_label, ridge_oof_train ) )}\")\nprint(f\"<Lasso> OVERALL RMSE         : {sqrt( mean_squared_error( train_label, lasso_oof_train ) )}\")\nprint(f\"<Decision-Tree> OVERALL RMSE : {sqrt( mean_squared_error( train_label, dt_oof_train ) )}\")\nprint(f\"<Random-Forest> OVERALL RMSE : {sqrt( mean_squared_error( train_label, rf_oof_train ) )}\")\nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 모델 학습에 있어 중요한 피쳐는 무엇인가?!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### linear model\n\nlinear model에서 coef를 통하여 어떤 칼럼의 weight가 높은지 파악할 수 있다.\n\n- Ridge \n    - latitude와 longitude의 weight를 통해서 동쪽에 위치한 버스 정류장일수록 탑승 승객이 많으며, 북쪽에 위치한 버스 정류장일수록 탑승 승객이 적다고 판단한다.\n        -> 납득이 가지 않는 부분일수도..?\n    - 하차 승객 수 보다는 승차 승객 수가 \"퇴근 시간 탑승 승객\"에 보다 큰 영향을 미침\n    - 출퇴근 시간보다는 정오 즈음 승객이 얼마나 탔는지에 대한 정보가 중요할 수도?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure을 정의한다.\nplt.figure(figsize=(24,5))\n\n# Ridge의 Coef를 barplot으로 그린다.\nplt.bar( train.drop(drop_cols,1).columns,  ridge.coef_ )\n\n# y=0인 horizental한 선을 그린다.\nplt.axhline(y=0, color='r', linestyle='-')\n\nplt.xticks(rotation=45)\nplt.title(\"Coef of Ridge Model\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure을 정의한다.\nplt.figure(figsize=(24,5))\n\n# lasso의 Coef를 barplot으로 그린다.\nplt.bar( train.drop(drop_cols,1).columns,  lasso.coef_ )\n\n# y=0인 horizental한 선을 그린다.\nplt.axhline(y=0, color='r', linestyle='-')\n\nplt.xticks(rotation=45)\nplt.title(\"Coef of lasso Model\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 상관관계를 살펴보도록 하자.\ntrain.corr()[target_col].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tree-based Model\n\n- random forest \n    - 선형 모델과 비슷하게 정오 즈음의 탑승 승객수가 중요한 변수라고 판단\n- light gbm\n    - 가장 좋은 성능을 보인 모델\n    - 공간적 정보를 갖는 bus_route_id, station_code, station_name, latitude, longitude의 중요도가 다른 모델보다 높은 것을 확인 가능\n    - 하차 승객수 보다는 승차 승객수의 변수 중요도가 더 높은 것으로 보임","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure을 정의한다.\nplt.figure(figsize=(24,5))\n\n# Ridge의 Coef를 barplot으로 그린다.\nplt.bar( train.drop(drop_cols,1).columns,  rf.feature_importances_ )\n\n# y=0인 horizental한 선을 그린다.\nplt.axhline(y=0, color='r', linestyle='-')\n\nplt.xticks(rotation=45)\nplt.title(\"Coef of Random Forest Model\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure을 정의한다.\nplt.figure(figsize=(24,5))\n\n# Ridge의 Coef를 barplot으로 그린다.\nplt.bar( train.drop(drop_cols,1).columns,  model.feature_importance() )\n\n# y=0인 horizental한 선을 그린다.\nplt.axhline(y=0, color='r', linestyle='-')\n\nplt.xticks(rotation=45)\nplt.title(\"Coef of Random Forest Model\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PDP PLOT\n\n다음으로 PDP(Partial Dependency Plot)을 통하여 특정 칼럼의 값이 변할 때 Target Variable이 어떤 식으로 영향을 받는지 살펴보도록 합시다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 전체 데이터를 사용할 시 너무 많은 시간이 소요되어 일부 샘플만 사용하도록 하겠습니다.\nsample = train.drop(drop_cols,1).sample(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PDP Plot For 11~12_ride\npdp_ = pdp.pdp_isolate(\n    model= model, dataset=sample, model_features=list(sample), feature='11~12_ride'\n)\nfig, axes = pdp.pdp_plot(pdp_, '11~12_ride')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PDP Plot For 8~9_takeoff\npdp_ = pdp.pdp_isolate(\n    model= model, dataset=sample, model_features=list(sample), feature='8~9_takeoff'\n)\nfig, axes = pdp.pdp_plot(pdp_, '8~9_takeoff')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PDP Plot For latitude\npdp_ = pdp.pdp_isolate(\n    model= model, dataset=sample, model_features=list(sample), feature='latitude'\n)\nfig, axes = pdp.pdp_plot(pdp_, 'latitude')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PDP Plot For longitude\npdp_ = pdp.pdp_isolate(\n    model= model, dataset=sample, model_features=list(sample), feature='longitude'\n)\nfig, axes = pdp.pdp_plot(pdp_, 'longitude')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interactive PDP Plot For latitude,longitude\npdp_ = pdp.pdp_interact(\n    model= model, dataset=sample, model_features=list(sample), features=['latitude','longitude']\n)\n\nfig, axes = pdp.pdp_interact_plot(pdp_interact_out=pdp_,\n                                  feature_names=['latitude','longitude'],\n                                  plot_type='grid',\n                                  x_quantile=True,\n                                  plot_pdp=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 엔티티 개념을 활용한 피쳐 생성\n\n- 버스 노선\n    - 시간대별로 승객들이 얼마나 버스를 이용하였는가?(오전의 승하차 패턴 - Target Mean Encoding)\n    - 날짜별로 오전 시간 승객들이 얼마나 버스를 이용하였는가?\n- 버스 \n    - 배차시간\n- 승객\n    - 승하차를 1시간 단위로 해야하는가?\n- 정류장\n    - 정류장의 순서 \n    - 특정 정류장이 데이터에 얼마나 자주 등장하였는가?(고정적으로 타는 승객이 있는가 - Frequency Encoding)\n    - 지역적 특성(상업 지구, 주거 지구, 학원가)등의 특징을 파악할 수 있는가?\n        - 출근 시간에 승차가 많고 퇴근 시간에 하차가 많다면? -> 주거지구\n        - 출근 시간엔 하차 내역이 많고 퇴근 시간에 승차가 많다면? -> 학교, 직장 근처 사무 지구\n- 특정 날짜\n    - 요일 정보\n        - 평일과 주말의 승하차 패턴이 다르지 않을까?\n            - 초중고 근처 정류장은 주말의 하차 승객 수가 급격히 줄어들 것\n            - 같은 주중이라고 할지라도 금요일 밤의 패턴은 다르지 않을까?\n    - 공휴일 정보\n        - 추석의 영향은?\n        - 10월에는 2개의 공휴일이 존재함(개천절, 한글날)\n            - 같은 정도의 휴일이라고 할 수 있을까?\n    - 날씨 정보\n    - 시간대별로 승객들이 얼마나 버스를 이용하였는가?(오전의 승하차 패턴 - Mean Encoding / 퇴근 시간의 하차 패턴 - Target Mean Encoding)\n- 제주도   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델에 쓰일 parameter 정의하기\nn_splits= 5\nNUM_BOOST_ROUND = 100000\nSEED = 1993\nlgbm_param = {'objective':'rmse',\n              'boosting_type': 'gbdt',\n              'random_state':1993,\n              'learning_rate':0.1,\n              'subsample':0.7,\n              'tree_learner': 'serial',\n              'colsample_bytree':0.78,\n              'early_stopping_rounds':50,\n              'subsample_freq': 1,\n              'reg_lambda':7,\n              'reg_alpha': 5,\n              'num_leaves': 96,\n              'seed' : SEED\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터를 load합니다.\n# train/test --> string형식으로 저장되어 있는 `date` column은 datetime형식으로 수집합니다.\n# bus_bts    --> string형식으로 저장되어 있는 `geton_date`, `getoff_date` columns은 datetime형식으로 수집합니다.\ndef load_dataset(path):\n    train = pd.read_csv(path + 'train.csv', parse_dates=['date'])\n    test  = pd.read_csv(path + 'test.csv', parse_dates=['date'])\n    df_bus = pd.read_csv(path + 'bus_bts.csv', parse_dates = ['geton_date', 'getoff_date'] )\n    sample_submission = pd.read_csv(path + 'submission_sample.csv')\n    return train, test, df_bus, sample_submission\n\npath = '../input/dacon-bus-dataset/'\n%time train, test, df_bus, sample_submission = load_dataset(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical variable에 대해서는 Label-Encoding을 수행 \n# --> One-Hot Encoding가 바람직하다고 생각되나 메모리 문제로 실행할 수 없음.\nlbl = LabelEncoder()\n\n# Implement Label Encoding \ncat_cols = ['bus_route_id','in_out','station_code','station_name']\nfor col in tqdm_notebook( cat_cols ):\n    \n    # Label-Encoding을 fitting함\n    lbl.fit( train[[col]].append(test[[col]]) )\n    \n    # train/test label_encoding한 값을 대입함\n    train[col] = lbl.transform(train[col])\n    test[col] = lbl.transform(test[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 피쳐 중요도 확인\ndf_imp = pd.DataFrame(data = {'col': model.feature_name(),\n                              'imp': model.feature_importance()})\ndf_imp = df_imp.sort_values(by='imp', ascending=False).reset_index(drop=True)\ndf_imp ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 승하차 간격을 2시간 간격으로 설정할 수는 없는가? (3시간으로 설정해도 ok -> 결국 실험의 영역)\ndawn_ride_cols, dawn_takoff_cols = ['6~7_ride','7~8_ride'], ['6~7_takeoff','7~8_takeoff']\nmorning_ride_cols, morning_takeoff_cols = ['8~9_ride','9~10_ride'], ['8~9_takeoff','9~10_takeoff']\nnoon_ride_cols, noon_takeoff_cols = ['10~11_ride','11~12_ride'], ['10~11_takeoff','11~12_takeoff']\n\n# df 가공\ndef modify_terms(df):\n    # ride columns\n    df['dawn_ride'] = df[dawn_ride_cols].sum(axis=1)\n    df['morning_ride'] = df[morning_ride_cols].sum(axis=1)\n    df['noon_ride'] = df[noon_ride_cols].sum(axis=1)\n    \n    # takeoff columns\n    df['dawn_takeoff'] = df[dawn_takoff_cols].sum(axis=1)\n    df['morning_takeoff'] = df[morning_takeoff_cols].sum(axis=1)\n    df['noon_takeoff'] = df[noon_takeoff_cols].sum(axis=1)\n    \n    # drop columns\n    drop_cols = dawn_ride_cols + morning_ride_cols + noon_ride_cols + dawn_takoff_cols + morning_takeoff_cols + noon_takeoff_cols\n    df = df.drop(drop_cols, 1)\n    \n    return df\n    \n\ntrain = modify_terms(train)\ntest = modify_terms(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 피쳐 중요도 확인\ndf_imp = pd.DataFrame(data = {'col': model.feature_name(),\n                              'imp': model.feature_importance()})\ndf_imp = df_imp.sort_values(by='imp', ascending=False).reset_index(drop=True)\ndf_imp ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 날짜\n\n- 요일 정보\n    - 평일과 주말의 승하차 패턴이 다르지 않을까?\n        - 초중고 근처 정류장은 주말의 하차 승객 수가 급격히 줄어들 것\n        - 같은 주중이라고 할지라도 금요일 밤의 패턴은 다르지 않을까?\n    - 공휴일 정보\n        - 추석의 영향은?\n        - 10월에는 2개의 공휴일이 존재함(개천절, 한글날)\n            - 같은 정도의 휴일이라고 할 수 있을까?\n    - 날씨 정보\n    - 시간대별로 승객들이 얼마나 버스를 이용하였는가?(오전의 승하차 패턴 - Mean Encoding / 퇴근 시간의 하차 패턴 - Target Mean Encoding)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 요일 정보 추가\ntrain['weekday'] = train['date'].dt.weekday\ntest['weekday']  = test['date'].dt.weekday\n\n# 공휴일 정보 추가\n# -> EDA필요\nholidays = [datetime(2019, 9 ,12), datetime(2019, 9, 13), datetime(2019, 9 ,14), datetime(2019, 10,3), datetime(2019, 10,9) ]\ntrain['is_holiday'] = train['date'].apply(lambda x: x in holidays).astype(np.int8)\ntest['is_holiday']  = test['date'].apply(lambda x: x in holidays).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Encoding\n# (1) 일자별로 dawn, morning, noon에 각각 몇몇의 승객이 탑승하였는가\n# (2) 일자별로 dawn, morning, noon에 각각 몇몇의 승객이 하차하였는가\n# - 기준 :\n# - (1) bus_route_id\n# - (2) bus_route_id , station_code\n# - (3) station_code\n\n# (1) bus_route_id 기준\n\n# 탑승\ntrain['avg_dawn_ride_bus_route_id'] = train.groupby(['date','bus_route_id'])['dawn_ride'].transform('mean') \ntrain['avg_morning_ride_bus_route_id'] = train.groupby(['date','bus_route_id'])['morning_ride'].transform('mean') \ntrain['avg_noon_ride_bus_route_id'] = train.groupby(['date','bus_route_id'])['noon_ride'].transform('mean') \n\ntest['avg_dawn_ride_bus_route_id'] = test.groupby(['date','bus_route_id'])['dawn_ride'].transform('mean') \ntest['avg_morning_ride_bus_route_id'] = test.groupby(['date','bus_route_id'])['morning_ride'].transform('mean') \ntest['avg_noon_ride_bus_route_id'] = test.groupby(['date','bus_route_id'])['noon_ride'].transform('mean') \n\n# 하차\ntrain['avg_dawn_takeoff_bus_route_id'] = train.groupby(['date','bus_route_id'])['dawn_takeoff'].transform('mean') \ntrain['avg_morning_takeoff_bus_route_id'] = train.groupby(['date','bus_route_id'])['morning_takeoff'].transform('mean') \ntrain['avg_noon_takeoff_bus_route_id'] = train.groupby(['date','bus_route_id'])['noon_takeoff'].transform('mean') \n\ntest['avg_dawn_takeoff_bus_route_id'] = test.groupby(['date','bus_route_id'])['dawn_takeoff'].transform('mean') \ntest['avg_morning_takeoff_bus_route_id'] = test.groupby(['date','bus_route_id'])['morning_takeoff'].transform('mean') \ntest['avg_noon_takeoff_bus_route_id'] = test.groupby(['date','bus_route_id'])['noon_takeoff'].transform('mean') \n\n# (2) bus_route_id, station_code 기준\n# train['avg_dawn_ride_bus_route_id_station_code'] = train.groupby(['date','bus_route_id','station_code'])['dawn_ride'].transform('mean') \n# train['avg_morning_ride_bus_route_id_station_code'] = train.groupby(['date','bus_route_id','station_code'])['morning_ride'].transform('mean') \n# train['avg_noon_ride_bus_route_id_station_code'] = train.groupby(['date','bus_route_id','station_code'])['noon_ride'].transform('mean') \n\n# test['avg_dawn_ride_bus_route_id_station_code'] = test.groupby(['date','bus_route_id','station_code'])['dawn_ride'].transform('mean') \n# test['avg_morning_ride_bus_route_id_station_code'] = test.groupby(['date','bus_route_id','station_code'])['morning_ride'].transform('mean') \n# test['avg_noon_ride_bus_route_id_station_code'] = test.groupby(['date','bus_route_id','station_code'])['noon_ride'].transform('mean') \n\n# (3) station_code 기준\n# train['avg_dawn_ride_station_code'] = train.groupby(['date','station_code'])['dawn_ride'].transform('mean') \n# train['avg_morning_ride_bus_station_code'] = train.groupby(['date','station_code'])['morning_ride'].transform('mean') \n# train['avg_noon_ride_station_code'] = train.groupby(['date','station_code'])['noon_ride'].transform('mean') \n\n# test['avg_dawn_ride_station_code'] = test.groupby(['date','station_code'])['dawn_ride'].transform('mean') \n# test['avg_morning_ride_bus_station_code'] = test.groupby(['date','station_code'])['morning_ride'].transform('mean') \n# test['avg_noon_ride_station_code'] = test.groupby(['date','station_code'])['noon_ride'].transform('mean') \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # Target- Mean Encoding\n    X_train['label'] = y_train\n    d = X_train.groupby(['station_code'])['label'].mean().to_dict()\n    X_train['station_code_te'] = X_train['station_code'].apply(lambda x: d.get(x))\n    X_valid['station_code_te'] = X_valid['station_code'].apply(lambda x: d.get(x))\n    \n    X_train= X_train.drop('label',1)\n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 날씨 정보\ndf_weather = pd.read_csv('../input/dacon-bus-dataset/jeju_weather_dataset', encoding='cp949')\ndf_weather = df_weather[['일시','강수량(mm)']]\ndf_weather.columns = ['date','precipitation']\n\n# date의 type을 string에서 datetime으로 변환\ndf_weather['date'] = pd.to_datetime( df_weather['date'] )\n\n# 대회 기간에 해당하는 데이터만 사용하도록 함\ndf_weather = df_weather[(df_weather['date']>='2019-08-31 00:00:00')&(df_weather['date']<='2019-10-16 23:00:00')].reset_index(drop=True)\n\n# 대회 규정상 해당 날짜의 15시까지 정보만 사용할 수 있음\ndf_weather['hour'] = df_weather['date'].dt.hour\ndf_weather['date'] = df_weather['date'].dt.date\n\n# 전날의 강수량을 정보를 대입할 때 사용\ndf_prevday_weather = df_weather.groupby('date')['precipitation'].sum().reset_index()\ndf_prevday_weather.columns = ['prev_date', 'prevday_precipitation']\n\n# 해당 날짜의 강수량을 구함\ndf_weather = df_weather[df_weather['hour']<=15].reset_index(drop=True)\n\n# 00~15시까지의 강수량을 피쳐로 사용\ndf_weather = df_weather.groupby('date')['precipitation'].sum().reset_index()\n\n# Train/Test-set과 join하기 위하여 column의 타입을 datetime으로 변환한다.\ndf_prevday_weather['prev_date'] = pd.to_datetime( df_prevday_weather['prev_date']  )\ndf_weather['date'] = pd.to_datetime( df_weather['date']  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 전날짜에 대하여 Train/Test-set과 강수량 정보를 join\n\n# Train/Test-set에 대하여 전날을 구함\ntrain['prev_date'] = train['date'] - pd.Timedelta('1 day')\ntest['prev_date'] = test['date'] - pd.Timedelta('1 day')\n\ntrain = pd.merge(train, df_prevday_weather , on ='prev_date',  how ='left')\ntest = pd.merge(test, df_prevday_weather , on ='prev_date',how ='left')\n\n# prev_date 칼럼은 삭제해줌\ntrain = train.drop('prev_date',1)\ntest = test.drop('prev_date',1)\n\n\n# 해당날짜에 대하여 Train/Test-set과 강수량 정보를 join\ntrain = pd.merge( train, df_weather , on ='date', how='left')\ntest = pd.merge( test, df_weather , on ='date', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # Target- Mean Encoding\n    X_train['label'] = y_train\n    d = X_train.groupby(['station_code'])['label'].mean().to_dict()\n    X_train['station_code_te'] = X_train['station_code'].apply(lambda x: d.get(x))\n    X_valid['station_code_te'] = X_valid['station_code'].apply(lambda x: d.get(x))\n    \n    X_train= X_train.drop('label',1)\n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 버스 노선\n\n- 해당 정거장이 특정 노선 중 몇 번째 정거장인가?\n    - 해당 정거장에는 몇 개의 정거장이 있는가?\n    - bus_route_id 별 station_code 순으로 버스가 정차하는 것처럼 보임\n- 노선을 numeric type으로 학습시키는 것이 적합한가?\n    - categorical vairable로 학습시킨다면?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 해당 딕셔너리에 bus_route_id 별 정차 순서를 구하도록 함\nbus_route_sequence = {}\n\n# 모든 bus_route_id 수집\ncombined = train.append(test, ignore_index=True)\nall_bus_route_ids = set(combined['bus_route_id'])\n\nfor bus_route_id in tqdm_notebook( all_bus_route_ids ) :\n    # bus_route_id별 station_code를 오름차순으로 순서매김함\n    df_bus_route = combined[combined['bus_route_id']==bus_route_id]\n    sorted_station_codes = np.unique(df_bus_route['station_code'])\n    \n    # dictionary에 해당 정류장이 몇번째 정차 정류장인지 기입\n    bus_route_sequence[bus_route_id] = {station_code: ind for ind, station_code in enumerate( list(sorted_station_codes) )}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 몇 번째 정류장인지를 피쳐로 생성\ntrain['nth_station']= train[['bus_route_id','station_code']].apply(lambda x: bus_route_sequence.get(x[0]).get(x[1]), axis=1)\ntest['nth_station'] = test[['bus_route_id','station_code']].apply(lambda x: bus_route_sequence.get(x[0]).get(x[1]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 해당 bus_route_id에는 몇 개의 정류장이 있는지\nbus_route_id_total_station_count_dict = combined.groupby('bus_route_id')['station_code'].nunique().to_dict()\n\ntrain['bus_route_id_total_staion_count'] = train['bus_route_id'].apply(lambda x: bus_route_id_total_station_count_dict.get(x) )\ntest['bus_route_id_total_staion_count']  = test['bus_route_id'].apply(lambda x: bus_route_id_total_station_count_dict.get(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 뒤에서부터 몇 번째 정류정인지\ntrain['nth_station_backward']= train['nth_station'] - train['bus_route_id_total_staion_count']\ntest['nth_station_backward'] = test['nth_station'] - test['bus_route_id_total_staion_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # Target- Mean Encoding\n    X_train['label'] = y_train\n    d = X_train.groupby(['station_code'])['label'].mean().to_dict()\n    X_train['station_code_te'] = X_train['station_code'].apply(lambda x: d.get(x))\n    X_valid['station_code_te'] = X_valid['station_code'].apply(lambda x: d.get(x))\n    \n    X_train= X_train.drop('label',1)\n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       categorical_feature= ['bus_route_id','station_code'],\n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 정류장\n\n- Kmeans등을 이용하여 정류장을 군집화 시킨다면?\n- 위경도 좌표를 통하여 행정동 or 법정동 정보를 수집할 수 있다면?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 중복되지 않는 위경도 값들을 수집함\ncombined = train[['latitude','longitude']].append(test[['latitude','longitude']])\ncombined = combined.drop_duplicates()\n\n# kmeans를 통하여 군집화\nkmeans = KMeans(n_clusters= int(sqrt(len(combined)) ), random_state=1993)\nkmeans.fit( combined )\n\ntrain['station_code_kmeans'] = kmeans.predict(train[['latitude','longitude']])\ntest['station_code_kmeans']  = kmeans.predict(test[['latitude','longitude']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # Target- Mean Encoding\n    X_train['label'] = y_train\n    d = X_train.groupby(['station_code'])['label'].mean().to_dict()\n    X_train['station_code_te'] = X_train['station_code'].apply(lambda x: d.get(x))\n    X_valid['station_code_te'] = X_valid['station_code'].apply(lambda x: d.get(x))\n    \n    X_train= X_train.drop('label',1)\n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       categorical_feature= ['bus_route_id','station_code', 'station_code_kmeans'],\n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature 선택/제거","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_param = {'objective': 'rmse',\n             'boosting_type': 'gbdt',\n             'random_state': 1993,\n             'learning_rate': 0.1,\n             'subsample': 0.7,\n             'tree_learner': 'serial',\n             'colsample_bytree': 0.78,\n#              'early_stopping_rounds': 50,\n             'subsample_freq': 1,\n             'reg_lambda': 7,\n             'reg_alpha': 5,\n             'num_leaves': 96,\n             'seed': 1993}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_model = lgbm.LGBMRegressor(**lgbm_param)\nrfe = RFECV(estimator=reg_model, step=1, cv=KFold(n_splits=5, shuffle=False, random_state=231), scoring='neg_mean_squared_error', verbose=2)\nrfe.fit(train.drop(drop_cols,1), train_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rank = pd.DataFrame(data = {'col': list(train.drop(drop_cols,1)) , 'imp': rfe.ranking_})\nuse_cols = list(df_rank[df_rank['imp']==1]['col'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_param = {'objective': 'rmse',\n             'boosting_type': 'gbdt',\n             'random_state': 1993,\n             'learning_rate': 0.1,\n             'subsample': 0.7,\n             'tree_learner': 'serial',\n             'colsample_bytree': 0.78,\n             'early_stopping_rounds': 50,\n             'subsample_freq': 1,\n             'reg_lambda': 7,\n             'reg_alpha': 5,\n             'num_leaves': 96,\n             'seed': 1993}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # Target- Mean Encoding\n    X_train['label'] = y_train\n    d = X_train.groupby(['station_code'])['label'].mean().to_dict()\n    X_train['station_code_te'] = X_train['station_code'].apply(lambda x: d.get(x))\n    X_valid['station_code_te'] = X_valid['station_code'].apply(lambda x: d.get(x))\n    \n    X_train= X_train.drop('label',1)\n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train[use_cols], y_train)\n    dvalid = lgbm.Dataset(X_valid[use_cols], y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       categorical_feature= ['bus_route_id','station_code', 'station_code_kmeans'],\n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid[use_cols])\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test-set 값 예측 (Ensemble)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델에 쓰일 parameter 정의하기\nn_splits= 5\nNUM_BOOST_ROUND = 100000\nSEED = 1993\nlgbm_param = {'objective':'rmse',\n              'boosting_type': 'gbdt',\n              'random_state':1993,\n              'learning_rate':0.01,\n              'subsample':0.7,\n              'tree_learner': 'serial',\n              'colsample_bytree':0.68,\n              'early_stopping_rounds':50,\n              'subsample_freq': 1,\n              'reg_lambda':7,\n              'reg_alpha': 5,\n              'num_leaves': 96,\n              'seed' : SEED\n            }\n\nn_rounds = 100000\ncat_params = {\n        'n_estimators': n_rounds,\n        'learning_rate': 0.08,\n        'eval_metric': 'RMSE',\n        'loss_function': 'RMSE',\n        'random_seed': 42,\n        'metric_period': 500,\n        'od_wait': 500,\n        'task_type': 'GPU',\n       'l2_leaf_reg' : 3,\n        'depth': 8,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = '18~20_ride'\ndrop_cols = ['date','id',target_col]\ntrain_label = train[target_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 형식을 맞춰주기 위해서 Test-set에 '18~20_ride' columns을 만들어줌\ntest[target_col] = np.NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\nlgbm_oof_train = np.zeros((train.shape[0]))\nlgbm_oof_test = np.zeros((test.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # Target- Mean Encoding\n    X_train['label'] = y_train\n    d = X_train.groupby(['station_code'])['label'].mean().to_dict()\n    X_train['station_code_te'] = X_train['station_code'].apply(lambda x: d.get(x))\n    X_valid['station_code_te'] = X_valid['station_code'].apply(lambda x: d.get(x))\n    test['station_code_te'] = test['station_code'].apply(lambda x: d.get(x))\n    \n    X_train= X_train.drop('label',1)\n    \n    \n    # (5) Light GBM\n    print(\"---TRAINING LIGHT GBM---\")\n    # dtrain/dvalid 정의\n    dtrain = lgbm.Dataset(X_train, y_train)\n    dvalid = lgbm.Dataset(X_valid, y_valid)\n    \n    # model 정의&학습\n    model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, \n                       valid_sets=(dtrain, dvalid), \n                       valid_names=('train','valid'), \n                       categorical_feature= ['bus_route_id','station_code', 'station_code_kmeans'],\n                       verbose_eval= 100)\n    \n    # local_valid/local_test에 대한 예측\n    lgbm_valid_pred = model.predict(X_valid)\n    lgbm_test_pred = model.predict(test.drop(drop_cols, 1))\n        \n    lgbm_oof_train[val_ind] = lgbm_valid_pred\n    lgbm_oof_test += lgbm_test_pred/ n_splits\n    print('='*80)\n    \nprint(f\"<Light-GBM> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, lgbm_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 모델에 대한 oof 정의\ncat_oof_train = np.zeros((train.shape[0]))\ncat_oof_test = np.zeros((test.shape[0]))\n\n# Kfold 정의\nkfolds = KFold(n_splits=n_splits, random_state=1993, shuffle=True)\n\n\n# Fold별로 학습진행\nfor ind, (trn_ind, val_ind) in tqdm_notebook( enumerate( kfolds.split( X = train, y = train_label ) ) ):\n    \n    # Train/Valid-set을 정의하기\n    X_train , y_train = train.iloc[trn_ind].drop(drop_cols, 1), train_label[trn_ind]\n    X_valid , y_valid = train.iloc[val_ind].drop(drop_cols, 1), train_label[val_ind]\n    \n    # Target- Mean Encoding\n    X_train['label'] = y_train\n    d = X_train.groupby(['station_code'])['label'].mean().to_dict()\n    X_train['station_code_te'] = X_train['station_code'].apply(lambda x: d.get(x))\n    X_valid['station_code_te'] = X_valid['station_code'].apply(lambda x: d.get(x))\n    test['station_code_te'] = test['station_code'].apply(lambda x: d.get(x))\n    \n    X_train= X_train.drop('label',1)\n    \n    \n    # (5) CATBOOST\n    print(\"---TRAINING CATBOOST---\")\n    \n    # model 정의&학습\n    model = CatBoostRegressor(**cat_params)\n    \n    model.fit( X_train, y_train, eval_set = (X_valid, y_valid), \n              cat_features  = ['bus_route_id','station_code', 'station_code_kmeans'],\n              use_best_model=True,\n              verbose=True)\n    \n    # local_valid/local_test에 대한 예측\n    cat_valid_pred = model.predict(X_valid)\n    cat_test_pred = model.predict(test.drop(drop_cols, 1))\n        \n    cat_oof_train[val_ind] = cat_valid_pred\n    cat_oof_test += cat_test_pred/ n_splits\n    print('='*80)\n    \nprint(f\"<CATBOOST> OVERALL RMSE     : {sqrt( mean_squared_error( train_label, cat_oof_train ) )}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 제출 파일 만들기","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 제출 파일 만들기\nensemble_pred = 0.5 * ( lgbm_oof_test+ cat_oof_test )\nsample_submission[target_col] = np.clip( ensemble_pred, 0 , max(ensemble_pred) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-set의 실제값과 예측값 비교\nplt.figure(figsize=(12,6))\n\nsns.distplot( train_label, color='r' , label='real')\nsns.distplot( 0.5*(lgbm_oof_train + cat_oof_train), color='b', label='prediction' )\nplt.legend()\nplt.title(\"Real Vs Prediction\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-set/Test-set의  예측값 비교\nplt.figure(figsize=(12,6))\n\nsns.distplot( 0.5*(lgbm_oof_train + cat_oof_train), color='r' , label='Train')\nsns.distplot( ensemble_pred, color='b', label='Test' )\nplt.legend()\nplt.title(\"Prediction for Train/Test-set\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\n\nsample_submission.to_csv('lgbm_catboost_ensemble.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FileLink('lgbm_catboost_ensemble.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}