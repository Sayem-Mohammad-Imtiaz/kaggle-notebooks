{"cells":[{"metadata":{"id":"I-P5m4iA_xe7"},"cell_type":"markdown","source":"# Bounding box detection - Racoon data\n\n\n## Data files\n- images: images of racoons\n- train_labels.cv: contains coordinates for bounding box for every image"},{"metadata":{"id":"paKO7ERla7Hk"},"cell_type":"markdown","source":"### Load the training data from train.csv file"},{"metadata":{"id":"Srk9fmLN-oEi","executionInfo":{"status":"ok","timestamp":1604508293677,"user_tz":-330,"elapsed":2413,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"31030f9d-57a0-45d5-c939-739195fa3fdc","trusted":true},"cell_type":"code","source":"import tensorflow\nimport random\nrandom.seed(0)\nimport warnings\nwarnings.filterwarnings('ignore')\ntensorflow.__version__","execution_count":null,"outputs":[]},{"metadata":{"id":"YiE2kyBX-aIg","executionInfo":{"status":"ok","timestamp":1604508357988,"user_tz":-330,"elapsed":1078,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"trusted":true},"cell_type":"code","source":"project_path = '../input/racoon-detection'\ndata_path = project_path + '/Racoon Images/images'\ntrain_csv_path = project_path + '/train_labels_.csv'","execution_count":null,"outputs":[]},{"metadata":{"id":"4UweyCk1a-73"},"cell_type":"markdown","source":"### Print the shape of the train dataset"},{"metadata":{"id":"3EmVHq8PbEGU"},"cell_type":"markdown","source":"### Declare a variable IMAGE_SIZE = 128 as we will be using MobileNet which will be taking Input shape as 128 * 128 "},{"metadata":{"id":"yAZhqikIbEQz","executionInfo":{"status":"ok","timestamp":1604508359091,"user_tz":-330,"elapsed":950,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"trusted":true},"cell_type":"code","source":"IMAGE_SIZE=128","execution_count":null,"outputs":[]},{"metadata":{"id":"8xCLu4gDbKV0"},"cell_type":"markdown","source":"### With the help of csv.reader write a for loop which can load the train.csv file and store the path, width, height, x0,y0,x1,y1 in induvidual variables. <br>\n1. Create a list variable known as 'path' which has all the path for all the training images\n2. Create an array 'coords' which has the resized coordinates of the bounding box for the training images\n\n<u>Note:</u> All the training images should be downsampled to 128 * 128 as it is the input shape of MobileNet (which we will be using for Object detection). Hence the corresponding coordinates of the bounding boxes should be changed to match the image dimension of 128 * 128 "},{"metadata":{"id":"M6e4ANWEaNPA","executionInfo":{"status":"ok","timestamp":1604508360822,"user_tz":-330,"elapsed":672,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"trusted":true},"cell_type":"code","source":"import csv\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"id":"wajkLXEuaqzz","executionInfo":{"status":"ok","timestamp":1604508368492,"user_tz":-330,"elapsed":4647,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"trusted":true},"cell_type":"code","source":"IMAGE_SIZE=128\n\nnormalize = lambda coordinate, value: (coordinate * IMAGE_SIZE)/value \n\nwith open (train_csv_path, 'r') as csvfile:\n  y_train = np.zeros((sum(1 for line in csvfile)-1,4))\n  X_train=[]\n  csvfile.seek(0)\n  data = csv.reader(csvfile, delimiter=',')\n  next(data)\n\n# int(data[1]) for row in data\n  for index, row in enumerate(data):\n    for i, r in enumerate(row[x] for x in [1,2,4,5,6,7]):\n      row[i+1] = int(r)\n# read the required values\n    path, image_width, image_height, x0, y0, x1, y1,_ = row\n    path = data_path + '/' + path\n\n    y_train[index, 0] = normalize(x0, image_width)\n    y_train[index, 1] = normalize(y0, image_height)\n    y_train[index, 2] = normalize((x1-x0), image_width)\n    y_train[index, 3] = normalize((y1-y0), image_height)\n\n    X_train.append(path)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"frwe809Y4SwE","executionInfo":{"status":"ok","timestamp":1604508375992,"user_tz":-330,"elapsed":1104,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"5ea0fe3f-7c35-4631-e59a-c5325245c779","trusted":true},"cell_type":"code","source":"X_train[0:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"hzEj2Oft4Yc7","executionInfo":{"status":"ok","timestamp":1604508376473,"user_tz":-330,"elapsed":697,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"0a41c416-6609-4e73-b2a4-58b16634fe25","trusted":true},"cell_type":"code","source":"y_train[0:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"Jon-cgDX9H2G","executionInfo":{"status":"ok","timestamp":1604508384877,"user_tz":-330,"elapsed":7737,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"trusted":true},"cell_type":"code","source":"# resize the image to 128*128\nimport cv2\n\n#pick up a random image\nfilename = X_train[1]\nunscaled = cv2.imread(filename)\nregion = y_train[1]\n\nimage_height, image_width, _ = unscaled.shape\n\nx0= int(region[0] * image_width / IMAGE_SIZE)\ny0= int(region[1] * image_height / IMAGE_SIZE)\n\nx1= int((region[0] + region[2])* image_width / IMAGE_SIZE)\ny1= int((region[1] + region[3])* image_height / IMAGE_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"xOZ4tQboBFRV","executionInfo":{"status":"ok","timestamp":1604508384878,"user_tz":-330,"elapsed":4350,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"5401bbda-1dc9-4688-a680-28afb2c7069c","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfig, axis = plt.subplots(1)\naxis.imshow(unscaled)\n\nrect = patches.Rectangle((x0, y0), x1-x0, y1-y0, linewidth=2, edgecolor='r', facecolor='none')\n\naxis.add_patch(rect)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"-CMdmgbvbuXg"},"cell_type":"markdown","source":"### Write a for loop which can load all the training images into a variable 'batch_images' using the paths from the 'paths' variable\n<u>Note:</u> Convert the image to RGB scale as the MobileNet accepts 3 channels as inputs   "},{"metadata":{"id":"yLckLOBObujI","executionInfo":{"status":"ok","timestamp":1604508436684,"user_tz":-330,"elapsed":49295,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\n\nfor i, f in enumerate(X_train):\n  img = Image.open(f)\n  img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n  img = img.convert('RGB')\n\n  X_train[i] = preprocess_input(np.array(img, dtype=np.float32))\n  img.close()","execution_count":null,"outputs":[]},{"metadata":{"id":"IJNXSY98DylD","executionInfo":{"status":"ok","timestamp":1604508436686,"user_tz":-330,"elapsed":47782,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"8c3d8c9b-c010-4f6d-9c7b-1203ffb03109","trusted":true},"cell_type":"code","source":"X_train = np.array(X_train)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"rWK9lxzOEHYW","executionInfo":{"status":"ok","timestamp":1604508436686,"user_tz":-330,"elapsed":46423,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"b537e73e-f203-476d-fe6a-3b146fdb57a5","trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"DZp1nd6_bylk"},"cell_type":"markdown","source":"### Import MobileNet and load MobileNet into a variable named 'model' which takes input shape of 128 * 128 * 3. Freeze all the layers. Add convolution and reshape layers at the end to ensure the output is 4 coordinates"},{"metadata":{"id":"IL7Vxs_7by0k","trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, Reshape\n\nALPHA=1\n\ndef create_model(trainable=True):\n  model = MobileNet(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), include_top=False, alpha=ALPHA)\n\n  # freez the layers which we have till now from training\n  for layer in model.layers:\n    layer.trainable = trainable\n\n  x0 = model.layers[-1].output\n  x1 = Conv2D(4, kernel_size=4, name='coords')(x0)\n\n  x2 = Reshape((4,))(x1)\n\n  return Model(inputs = model.input, outputs=x2)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Hdy9wEe8b3Ub"},"cell_type":"markdown","source":"### Define a custom loss function IoU which calculates Intersection Over Union"},{"metadata":{"id":"KgEL4Mwub3do","trusted":true},"cell_type":"code","source":"from tensorflow.keras.backend import epsilon\ndef loss(gt,pred):\n    intersections = 0\n    unions = 0\n    diff_width = np.minimum(gt[:,0] + gt[:,2], pred[:,0] + pred[:,2]) - np.maximum(gt[:,0], pred[:,0])\n    diff_height = np.minimum(gt[:,1] + gt[:,3], pred[:,1] + pred[:,3]) - np.maximum(gt[:,1], pred[:,1])\n    intersection = diff_width * diff_height\n    \n    # Compute union\n    area_gt = gt[:,2] * gt[:,3]\n    area_pred = pred[:,2] * pred[:,3]\n    union = area_gt + area_pred - intersection\n\n#     Compute intersection and union over multiple boxes\n    for j, _ in enumerate(union):\n        if union[j] > 0 and intersection[j] > 0 and union[j] >= intersection[j]:\n            intersections += intersection[j]\n            unions += union[j]\n\n    # Compute IOU. Use epsilon to prevent division by zero\n    iou = np.round(intersections / (unions + epsilon()), 4)\n    iou = iou.astype(np.float32)\n    return iou\n\ndef IoU(y_true, y_pred):\n    iou = tensorflow.py_function(loss, [y_true, y_pred], tensorflow.float32)\n    return iou","execution_count":null,"outputs":[]},{"metadata":{"id":"KCHmzs3Zb58T"},"cell_type":"markdown","source":"### Write model.compile function & model.fit function with: <br>\n1. Optimizer = Adam, Loss = 'mse' and metrics = IoU\n2. Epochs = 30, batch_size = 32, verbose = 1"},{"metadata":{"id":"xV4sVCivb6FJ","executionInfo":{"status":"ok","timestamp":1604500067039,"user_tz":-330,"elapsed":91959,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"ba6c14f9-58ab-4005-d74a-407034e238a6","trusted":true},"cell_type":"code","source":"model = create_model(False)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"pGe1ZUe0_7Jv","trusted":true},"cell_type":"code","source":"model.compile(loss='mean_squared_error', optimizer='adam', metrics=[IoU])","execution_count":null,"outputs":[]},{"metadata":{"id":"cuYYO1i-Ai-G","executionInfo":{"status":"ok","timestamp":1604500093682,"user_tz":-330,"elapsed":118595,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"92a2940c-d350-4648-ea68-3206b1f9f360","trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearlyS = EarlyStopping(monitor='IoU', patience=5, min_delta=0.01)\n\nmodel.fit(X_train, y_train, epochs=50, batch_size=32, callbacks=[])","execution_count":null,"outputs":[]},{"metadata":{"id":"BWrj3s6Rb-C8"},"cell_type":"markdown","source":"### Pick a test image from the given data"},{"metadata":{"id":"j5zeEuTFb-M0","executionInfo":{"status":"ok","timestamp":1604500094063,"user_tz":-330,"elapsed":118971,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"6a3fd0d3-8492-4ebd-a8b2-e2a1967fbaa9","trusted":true},"cell_type":"code","source":"filepath = data_path + '/raccoon-62.jpg'\nunscaled = cv2.imread(filepath)\nimage_height, image_width, _ = unscaled.shape\nunscaled.shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_TdRTlbfcAHr"},"cell_type":"markdown","source":"### Resize the image to 128 * 128 and preprocess the image for the MobileNet model"},{"metadata":{"id":"ACVNzBCAcARH","executionInfo":{"status":"ok","timestamp":1604500094064,"user_tz":-330,"elapsed":118967,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"1263eab7-7afa-4d3c-c76a-970b4d2322a8","trusted":true},"cell_type":"code","source":"img = cv2.resize(unscaled, (IMAGE_SIZE,IMAGE_SIZE))\nfeat_scaled = preprocess_input(np.array(img, dtype=np.float32))\nprint(f'Before preporcessing image size was {unscaled.shape}')\nprint(f'After preprocess image size is {feat_scaled.shape}')","execution_count":null,"outputs":[]},{"metadata":{"id":"j7Wc1mXlcFT2"},"cell_type":"markdown","source":"### Predict the coordinates of the bounding box for the given test image"},{"metadata":{"id":"sw2L2wtXcFcH","trusted":true},"cell_type":"code","source":"region = model.predict(np.array([feat_scaled]))[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"eF1iqyc4cHm-"},"cell_type":"markdown","source":"### Plot the test image using .imshow and draw a boundary box around the image with the coordinates obtained from the model"},{"metadata":{"id":"BE0l5FCQcH6p","executionInfo":{"status":"ok","timestamp":1604500096039,"user_tz":-330,"elapsed":120934,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNkrS1-xYXX-SJb4SSDdrDAZnH8_u9tgzyQXEWbQ=s64","userId":"09536137637758786577"}},"outputId":"6733d27e-c360-4ab7-a2be-b43d24b7abf8","trusted":true},"cell_type":"code","source":"x0 = int(region[0] * image_width / IMAGE_SIZE) # Scale the BBox\ny0 = int(region[1] * image_height / IMAGE_SIZE)\n\nx1 = int((region[2]) * image_width / IMAGE_SIZE)\ny1 = int((region[3]) * image_height / IMAGE_SIZE)\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport numpy as np\n\n\n# Create figure and axes\nfig,ax = plt.subplots(1)\n\n# Display the image\nax.imshow(unscaled)\n\n# Create a Rectangle patch\nrect = patches.Rectangle((x0, y0), (x1 - x0) , (y1 - y0) , linewidth=2, edgecolor='r', facecolor='none')\n\n# Add the patch to the Axes\nax.add_patch(rect)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}