{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About Dataset:\ndataset has data collected from New York, California and Florida about 50 business Startups \"17 in each state\". The variables used in the dataset are Profit, R&D spending, Administration Spending, and Marketing Spending.\n- We have to  make a Model that can predict the profit based on the comapanies data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Multiple Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.1 importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Load dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataset = pd.read_csv('../input/50-startups/50_Startups.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2 EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 identifying  the missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There is no null values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2.2 checking the datatype","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Here __state__ column is object type. later we will convert this column  into dummy varible.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2.3 Descriptive Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.4  Checking the distribution of  'R&D Spend',    'Administration'   &  'Marketing Spend'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(dataset['R&D Spend'], color = 'green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(dataset['Administration'], color = 'red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(dataset['Marketing Spend'], color = 'orange')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  2.5 Checking the relation b/w the features and o/p variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Here above we can see that R&D Spend have linear relationship with Profit.\n- So here it's most significant feature compare to others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2.6 find the correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(dataset.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Here above we can see that __R&D Spend__ is highly correlated to __Profit__.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Preparing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.1 splitting  data into dependent &  independent varibles ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = dataset.iloc[:,:-1].values\n\ny = dataset.iloc[:,4].values\n\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2 Encoding categorical data :","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" To encode the categorical variable into numbers, we will use the LabelEncoder class. But it is not sufficient because it still has some relational order, which may create a wrong model. So in order to remove this problem, we will use OneHotEncoder, which will create the dummy variables. Below is code for it:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nlabelencoder_X = LabelEncoder()\nX[:,3] = labelencoder_X.fit_transform(X[:,3])\n\n# Country column\nct = ColumnTransformer([(\"Country\", OneHotEncoder(), [3])], remainder = 'passthrough')\n                                # creating dummy var(for states means 3 diff. column ) \nX = ct.fit_transform(X)\nprint(X)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  3.3  Avoiding the dummy variable trap:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If we do not remove the first dummy variable, then it may introduce multicollinearity in the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = X[:,1:]\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.4 Splitting the dataset into the Training set and Test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(X,y,test_size = 1/3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4 Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 4.1 Training the Multiple Linear Regression model on the Training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 5. Making the predictions and evaluating the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5.1 Predicting the Test set results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2 comparing the actual_price with predicted_price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i,j in np.nditer((y_test, y_pred)):\n    print(i,\"      \", j)  # compare actual price vs predicted price\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3 evaluate the train & test score performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(regressor.score(X_train, y_train))\nprint(regressor.score(X_test,y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. find optimal Model using backward elimination","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Backward elimination:\nBackward elimination is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Unnecessary features increase the complexity of the model. Hence it is good to have only the most significant features and keep our model simple to get the better result.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport statsmodels.api as sm\n# (bydefault it's not take constant(thetas 0 ,we have to put theta_0 * X0 = 1\n# that's why we are creating col. of 1's and trying to put in the starting of X)\nX = np.append(arr  = np.ones((50,1)).astype(int),values = X, axis = 1)# we are adding 1 extra col. in the starting  of X\nprint(X)\n\n\n# np.append(values = X, np.ones((50,1)), axis = 1) # it will add col. at the last of X dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.1 applying backward elimination","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt =X[:, [0,1,2,3,4,5]].astype(float)\nregressor_OLS = sm.OLS(y,X_opt).fit()  \nregressor_OLS.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_opt =X[:, [0,1,3,4,5]].astype(float)   # removed X2(dummmy var) cause p>SL(significant level =0.5)\nregressor_OLS = sm.OLS(y,X_opt).fit()           #    p>SL (0.990 > 0.05)\nregressor_OLS.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt =X[:, [0,3,4,5]].astype(float)   # removed X1(dummy var) cause P>SL(0.953 > 0.05)\nregressor_OLS = sm.OLS(y,X_opt).fit()\nregressor_OLS.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt =X[:, [0,3,5]].astype(float)   #     removed X4(administration) cause P>SL (0.608 > 0.05)\nregressor_OLS = sm.OLS(y,X_opt).fit()\nregressor_OLS.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt =X[:, [0,3]].astype(float)   # removed X5 (marketing spend) cause P>SL(0.060 > 0.05)\nregressor_OLS = sm.OLS(y,X_opt).fit()\nregressor_OLS.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Apply  optimal Multiple Linear Regression model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"####  7.1 Extracting Independent and dependent Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"  \nx_BE= dataset.iloc[:,[0]].values  \ny_BE= dataset.iloc[:, -1].values  \n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.2 Splitting the dataset into training and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" \nfrom sklearn.model_selection import train_test_split  \nx_BE_train, x_BE_test, y_BE_train, y_BE_test= train_test_split(x_BE, y_BE, test_size= 0.20, random_state=0)  \n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  7.3 Fitting the MLR model to the training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"  \nfrom sklearn.linear_model import LinearRegression  \nregressor= LinearRegression()  \nregressor.fit(np.array(x_BE_train).reshape(-1,1), y_BE_train)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  7.4 Predicting the Test set result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_pred= regressor.predict(x_BE_test)  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  7.5 Cheking the score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nprint('Train Score: ', regressor.score(x_BE_train, y_BE_train))  \nprint('Test Score: ', regressor.score(x_BE_test, y_BE_test))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.6 Comparision b/w actual price and predicted price ","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for i,j in np.nditer((y_BE_test,y_pred)):\n    print(i,\"      \", j)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Visualizing the final result ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"####  8.1 Visualizing the R&d spend with Profits","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"R&D independent variable is a significant variable for the prediction. So we  predicted efficiently using this variable.\nWe can see below the relation of R&d spend with Profits.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(dataset.iloc[:,0], dataset.iloc[:, 4], color = 'green')\nplt.xlabel('R&D Spends')\nplt.ylabel('Profits')\nplt.title('Relation b/w the R&D spend and Profits')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.2 Visualizing the train set result ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x_BE_train, y_BE_train, color = 'red')\nplt.plot(x_BE_train, regressor.predict(x_BE_train), color = 'blue')\nplt.title('R&D spend vs Profit (Training set)')\nplt.xlabel('R&D spend')\nplt.ylabel('Profit')\nplt.grid(color='gold', linestyle='-.', linewidth=0.7)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  8.3 Visualizing the test set result ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x_BE_test, y_BE_test, color = 'red')\nplt.plot(x_BE_test, regressor.predict(x_BE_test), color = 'blue')\nplt.title('R&D spend vs Profit (Test set)')\nplt.xlabel('R&D spend')\nplt.ylabel('Profit')\nplt.grid(color = 'green', linestyle='-.', linewidth=0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Conclusion :","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nWe got this result by using one independent variable (R&D spend) only instead of four variables. Hence, now our model is simple and accurate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}