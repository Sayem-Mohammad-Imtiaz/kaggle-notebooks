{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Task Details\nYou're a marketing analyst and you've been told by the Chief Marketing Officer that recent marketing campaigns have not been as effective as they were expected to be. You need to analyze the data set to understand this problem and propose data-driven solutions.\n\nExpected Submission\nSubmit a well documented notebook with these three sections:\n\nSection 01: Data Exploration\nAre there any null values or outliers? How will you wrangle/handle them?\nAre there any variables that warrant transformations?\nAre there any useful variables that you can engineer with the given data?\nDo you notice any patterns or anomalies in the data? Can you plot them?\n"},{"metadata":{},"cell_type":"markdown","source":"First thing first! lets import our data and quickly check it:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/marketing-data/marketing_data.csv')\npd.set_option('display.max_columns', 30)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there any null values or outliers? "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, in the \"Income\" column we have some null values. Lets check it closer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data[' Income '].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sometimes, NaN value means there is no value for that specific feature. For instance, when someone does not have any income, they may write NaN for that, which means NaN in this case is zero (no income); or, when a house does not have a pool, we may see NaN for the pool record, which means no pool. However, in this case, NaN values seems to be missed and it does not mean \"there is no income\" or \"income is zero\". "},{"metadata":{},"cell_type":"markdown","source":"Anyway, lets check how much missed data we have for \" Income \" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"(data[' Income '].isna().sum()/data.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only about one percent of the data are null values, so we may want to drop them. However, before complate dropping of null values, lets check the relation btw different features and the income value. We may find a way to guess the missing values. Hence, save the non-NaN values in a clean_data variable."},{"metadata":{},"cell_type":"markdown","source":"# Method1: Deleting the NaN values"},{"metadata":{},"cell_type":"markdown","source":"We do not want to change the original data, so we are copying non-NaN values in a new dataset named clean_data"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data=data.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fist we need to clean the Income column. We have \"$\" and \",\" in the data which should be removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_income=clean_data[' Income '].str.replace('$', '').str.replace(',', '').astype('float')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are going to replace \" Income \" column with \"clean_income\""},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data['clean_income']=clean_income","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.drop(' Income ', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lest check the most affected parameters on Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"newd=data.dropna()\nimport seaborn as sns\nsns.boxplot(x=newd.Education, y=clean_data.clean_income)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, it seems some people get more than 600000 after graduation. Good for them :). Lets see who is this lucky person :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.loc[clean_data.clean_income>600000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this is just one record and the income is not the norm of income for graduate we can delete it as an outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.drop(index=527, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs=plt.subplots(1,2, figsize=(10,5))\nsns.boxplot(x=newd.Education, y=clean_data.clean_income, ax=axs[0])\nsns.boxplot(x=newd.Country, y=clean_data.clean_income, ax=axs[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, it seems the country does not have too much effect on the income. On the other hand, as expected, PhD on the average geeting more money than Basic!! One way to replace the NaN values can be based on the education. For example, if someone have graduation, we can replace income with the average of graduaction income which is 50000. "},{"metadata":{},"cell_type":"markdown","source":"# Method 2: Use average values, based on \"Education\" columns"},{"metadata":{},"cell_type":"markdown","source":"We found a better way to deal with the NaN value so lets replace them with the average values based on the Eductaion"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[' Income '].loc[data.loc[(data.Education=='PhD') & (data[' Income '].isna())].index]=58000\ndata[' Income '].loc[data.loc[(data.Education=='Graduation') & (data[' Income '].isna())].index]=50000\ndata[' Income '].loc[data.loc[(data.Education=='Master') & (data[' Income '].isna())].index]=55000\ndata[' Income '].loc[data.loc[(data.Education=='2n Cycle') & (data[' Income '].isna())].index]=48000\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now do the same cleaning procedure on \"Income\" column: "},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_income=data[' Income '].str.replace('$', '').str.replace(',', '').astype('float')\ndata['clean_income']=clean_income\ndata.drop(' Income ', axis=1, inplace=True)\n#data.drop(index=527, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"#  Dealing with Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Education', 'Marital_Status','Dt_Customer','Country']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets convert these categorical objects to some meaningful data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nCat_c=['Education', 'Marital_Status','Country']\n#Cat_c=['Education']\nfor items in Cat_c:\n    le=OneHotEncoder()\n    t=le.fit_transform(data[[items]]).toarray()\n    a=data[items].unique()\n    indexs=np.unique(a, return_index=True)[1]\n    col=[a[indexs] for index in sorted(indexs)]\n    data=pd.concat([data, pd.DataFrame(t, columns=col[1])], axis=1).drop([items],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 50)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are going to change the DT_Customer to a date_times which python can read:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Dt_Customer_n']=pd.to_datetime(data.Dt_Customer)\ndata=data.drop(['Dt_Customer'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our cleaned data is:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_column', 50)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data.clean_income>600000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(index=527)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have fairly normal distribution for income and Year_birth:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,2)\nsns.distplot(data.clean_income, ax=axs[0])\nsns.distplot(data.Year_Birth, ax=axs[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do we have any Year_Birth before 1910?! wow"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data.Year_Birth<1910]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(index=[513, 827, 2233])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fid, axs=plt.subplots(2,3, figsize=(14,14))\nsns.scatterplot( x='clean_income',y='MntMeatProducts', data=data, ax=axs[0, 0])\nsns.scatterplot( x='clean_income',y='MntWines', data=data, ax=axs[0, 1])\nsns.scatterplot( x='clean_income',y='MntFishProducts', data=data, ax=axs[0, 2])\nsns.scatterplot( x='clean_income',y='MntSweetProducts', data=data, ax=axs[1, 0])\nsns.scatterplot( x='clean_income',y='MntGoldProds', data=data, ax=axs[1, 1])\nsns.scatterplot( x='clean_income',y='MntFruits', data=data, ax=axs[1, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen from the figures, amount of different products purchasing is exponentially increasing by the incom. There are a few obvious outliners, which must be removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(index=data[data.clean_income>150000].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fid, axs=plt.subplots(2,3, figsize=(14,14))\nsns.scatterplot( x='clean_income',y='MntMeatProducts', data=data, ax=axs[0, 0])\nsns.scatterplot( x='clean_income',y='MntWines', data=data, ax=axs[0, 1])\nsns.scatterplot( x='clean_income',y='MntFishProducts', data=data, ax=axs[0, 2])\nsns.scatterplot( x='clean_income',y='MntSweetProducts', data=data, ax=axs[1, 0])\nsns.scatterplot( x='clean_income',y='MntGoldProds', data=data, ax=axs[1, 1])\nsns.scatterplot( x='clean_income',y='MntFruits', data=data, ax=axs[1, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs=plt.subplots(1,2, figsize=(14,7))\nsns.boxplot(x='Kidhome',y='clean_income', data=data, ax=axs[0])\nsns.boxplot(x='Teenhome',y='clean_income', data=data, ax=axs[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that no kids at home families have an average income more than one and two kids. On the other hand, no teenhome has an average lower income than one and two teenhome."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}