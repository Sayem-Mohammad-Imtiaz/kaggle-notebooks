{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Human Resources Data Science Project**\n*(This project is based off of the [IBM HR Analytics Competition](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset))*  \n  \nAuthor: Malik R. Booker  \nCreated: 19-MAY-2020  \nPublished: 22-MAY-2020  \nEdited: 24-MAY-2020","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Quick Navigation**\n- [Introduction](#Introduction)\n- [Hypotheses](#Hypotheses)\n- [Data Analysis](#Data-Analysis)\n    - [Feature Distribution](#Feature-Distribution)\n    - [Attrition by Age](#Attrition-by-Age)\n    - [Kernel Density Estimation](#Kernel-Density-Estimation)\n- [Preprocessing](#Preprocessing)\n- [Modeling](#Modeling)\n    - [Linear Model (Gradient Boosted Classifier)](#Linear-Model)\n    - [Neural Network](#Neural-Network)\n    - [Results](#Results)\n- [Actionble Insight](#Actionable-Insight)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThe Human Resources Department is responsible for a large portion of a  \ncompany's internal operations.  \nThese operations include:\n- Hiring and Recruiting\n- Training and Development\n- Employee Compensation\n- Employee Benefits\n- Employee Relations\n- Legal Responsibilities\n\nHiring employees is an expensive task. The final cost of hiring a new  \nemployee is the sum of training, briefing, recruiting, and of course  \nlost profit from not having an open spot on a team filled.  \n\nRetention, on the other hand, is much less expensive with respect to  \nthe hiring process. We can circumvent the affect that the hiring process  \nwould cost a company by simply making it more desirable to stay with the  \ncompany longer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Hypotheses \n1. Employee attrition increases as distance from home increases.  \n2. Employee attrition is strongly correlated with Department/Job Title.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/malikrb/HumanResourcesDemonstration/master/data/hr_data.csv'\ndf = pd.read_csv(url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context('display.max_columns', 35):\n    display(df.head(6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bin the Data into ['Close', 'Middle', 'Far']\ntemp = df.copy()\ntemp['DistanceFromHome'] = pd.cut(temp['DistanceFromHome'], 3, labels=['Closest', 'Middle', 'Farthest'])\n\ntemp = pd.concat([temp, pd.get_dummies(temp['Attrition'], prefix='Attrition')], axis=1)\n\n# Visualize Attrition\ndisplay(temp.groupby(['DistanceFromHome'])[['Attrition_No', 'Attrition_Yes']].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This frame shows that the *raw attrition count* was higher for  \nthose who live the closest to the company, but suprisingly the  \n*Middle* group had the highest attrition rate at *44:215*.\n\nHypothesis 1 seems to be debunk by this observation but we won't  \ncompletely accept this until further investigation is conducted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display(temp.groupby(['Department'])[['Attrition_No', 'Attrition_Yes']].sum())\nprint('--------------------------------------------')\ndisplay(temp.groupby(['JobRole'])[['Attrition_No', 'Attrition_Yes']].sum())\nprint('-------------------------------------------------------------------')\ndisplay(temp.groupby(['Department', 'JobRole'])[['Attrition_No', 'Attrition_Yes']].sum())\n\ndel temp\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest and lowest raw attrition counts were for the positions  \nLaboratory Technition and (Human Resources) Manager at *62* and *0,*  \nrespectively.  \n\nThe highest and lowest rate of attrition were for the positions  \nSales Representative and (Human Resources) Manager at *50:33* and  \n*11:0*, respectively.\n\nSales Representatives hold a significantly large *50:33* No/Yes  \nattrition ratio while the next closest ratio belongs to Sales  \nExecutives at *269:57*\n\nThe data above somewhat supports the claim that Hypothesis 2  \nstates that attrition is correlated, however, more analysis is  \nneeded to determine if the two are strongly correlated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndf['Attrition'] = LabelEncoder().fit_transform(df['Attrition'])\ndf = df.drop(['EmployeeNumber', 'EmployeeCount', 'Over18', 'StandardHours'], axis=1)\n\nwith pd.option_context('display.max_columns', 35):\n    display(df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Feature Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\n\ndf.hist(bins=30, figsize=(22,22), edgecolor='w')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graphs above, we can see that the distributions of  \nnumerous features are positively skewed. In this context, the graphs  \nsuggest that employees tend to spent quite a large amount of years  \nwith the company, however, the majority of categories seem to have  \na diminishing pattern as they approach their respective maximum.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Attrition by Age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(18,8))\nsns.countplot(x='Age', hue='Attrition', data=df,\n              edgecolor='w', linewidth=1.15)\nplt.title('Attrition by Age', size=26, y=1.05)\nplt.xlabel('Age', size=20)\nplt.ylabel('Count', size=20)\n\nplt.legend(['Stayed', 'Left'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The countplot seems to suggest that the ages with the highest  \n*raw attrition count* are those within the range of *26-35*.\n\nThe highest *rate of attrition* however seems to be those within  \nthe age range of *18-21*.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(4, 1, figsize=(18,18), tight_layout=True)\n\nx_vars = ['JobRole', 'MaritalStatus', 'JobLevel', 'StockOptionLevel']\nfor ax, x in zip(axes, x_vars):\n    sns.countplot(x=x, hue='Attrition', data=df, ax=ax)\n    ax.set_xlabel(x, size=16)\n    ax.set_ylabel(\"\")\n    ax.legend(['Stayed', 'Left'])\n\nf.text(x=-0.0275, y=0.5, s='Count', rotation=90, size=22)\n\nplt.show()\n\ndel x_vars\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, we have visualized the relationship of job title/role and attrition  \nand Sales Representative again appears to have the highest attrition ratio.\n\nThe numeric Values like: Job Involvement and Job Level actually seem to have a linear  \nrelationship with the attrition rate. Of course, further investigation will be done in  \nthe modeling step to see if this hypothesis is supported.\n\nAs far as we are concerned from a obervational standpoint, we will take the claim from  \nhypothesis 2 and formulate a controlled study to further investigate the correlation  \nbetween job title/role and attrition.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Kernel Density Estimation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), tight_layout=True)\n\n# Split the DataFrame by Attrition\nattrition_0 = df.loc[df['Attrition'] == 0]\nattrition_1 = df.loc[df['Attrition'] == 1]\n\nsns.kdeplot(attrition_0['DistanceFromHome'], label='Stayed',\n            shade=True, ax=ax1, alpha=0.6)\nsns.kdeplot(attrition_1['DistanceFromHome'], label='Left',\n            shade=True, ax=ax1, alpha=0.6)\nax1.set_xlabel('Distance From Home', size=18)\n\nsns.kdeplot(attrition_0['YearsWithCurrManager'], label='Stayed',\n            shade=True, ax=ax2, alpha=0.6)\nsns.kdeplot(attrition_1['YearsWithCurrManager'], label='Left',\n            shade=True, ax=ax2, alpha=0.6)\nax2.set_xlabel('Years With Current Manager', size=18)\n\nplt.show()\n\ndel attrition_0, attrition_1\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With Reference to the groupby frames earlier, this plot further supports the sentiment  \nthat the majority of the attrition comes from those who live in the shortest of distances  \nfrom the company.\n\nIt's safe to say that hypothesis 1 is no longer worth pursuing with futher resources.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode all 'object' columns\ncolumns = df.select_dtypes(include='object').columns\nfor col in columns:\n    df[col] = LabelEncoder().fit_transform(df[col])\n    \ndf = pd.get_dummies(df)\n    \ndel columns\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the DataFrame into train test\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('Attrition', axis=1)\ny = df['Attrition']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom multiprocessing import cpu_count\n\ndef algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n                       model, param_grid, cv=10, scoring_fit='accuracy',\n                       do_probabilities=False):\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv,  \n        scoring=scoring_fit,\n        verbose=2,\n        n_jobs=cpu_count()//2,\n    )\n    fitted_model = gs.fit(X_train_data, y_train_data)\n    \n    if do_probabilities:\n      pred = fitted_model.predict_proba(X_test_data)\n    else:\n      pred = fitted_model.predict(X_test_data)\n    \n    score = accuracy_score(pred, y_test)\n    \n    return fitted_model, pred, score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nparam_grid = {\n    'colsample_bytree': [0.7],\n    'learning_rate': [0.01],\n    'max_depth': [5],\n    'n_estimators': [500],\n    'reg_alpha': [1.1],\n    'reg_lambda': [1.2],\n    'subsample': [0.8],\n#     'colsample_bytree': [0.7, 0.8],\n#     'learning_rate': [0.01, 0.05],\n#     'n_estimators': [500, 1000],\n#     'max_depth': [5, 10],\n#     'reg_alpha': [1.1, 1.2, 1.3],\n#     'reg_lambda': [1.1, 1.2, 1.3],\n#     'subsample': [0.7, 0.8, 0.9]\n}\n\nmodel = XGBClassifier()\n\nxgb_model, xgb_pred, xgb_score = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                        param_grid, cv=5, scoring_fit='accuracy')\n\nprint(xgb_model.best_score_)\nprint(xgb_model.best_params_)\nprint(f'xgb_score: {xgb_score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef build_model(activation='relu', optimizer='adam', dropout_rate=0.2):\n    model = Sequential()\n    model.add(Dense(500, activation=activation, input_shape=(30, )))\n    model.add(Dense(500, activation=activation))\n    model.add(Dense(500, activation=activation))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=optimizer,\n        metrics=['accuracy']\n    )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nparam_grid = {\n    'batch_size':   [50],\n    'epochs':       [125],\n    # 'dropout_rate': [0.2, 0.3],\n    # 'activation':   ['relu', 'elu'],\n    # 'batch_size':   [50, 100, 150],\n    # 'optimizer':    ['Adam', 'Nadam'],\n    # 'epochs':       [25, 75, 125],\n}\n\nmodel = KerasClassifier(build_fn=build_model, verbose=0)\n\nnn_model, nn_pred, nn_score = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                        param_grid, cv=5, scoring_fit='accuracy')\n\nprint(nn_model.best_score_)\nprint(nn_model.best_params_)\nprint(f'nn_score: {nn_score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results\n\nThe xgb_boost model performed slightly better than the neural network  \nmodel, so we will use this one to estimate the feature importances.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = xgb_model.best_estimator_.feature_importances_\ncolumns = X.columns\nsorted_features = sorted(zip(columns, features), key=lambda x: x[1], reverse=True)\n\n## Uncomment lines below to see feature importances\n## with respective column\n# for col, feature in sorted_features:\n#     print(f'{col}: {feature}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(1, 1, figsize=(14,6), tight_layout=True)\n\nx = [x[0] for x in sorted_features]\nheight = [x[1] for x in sorted_features]\n\nplt.bar(x=x, height=height)\nplt.title('Feature Importances', size=22, y=1.05)\nplt.xticks(rotation=90, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nxgb_prob = xgb_model.predict_proba(X_test)[:,1]\nfpr, tpr, thresh = roc_curve(y_test, xgb_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.subplots(1, 1, figsize=(6,6), tight_layout=True)\n\nplt.title(f'Recieving Operator Characteristics', size=16, y=1.05)\nplt.plot(fpr, tpr, label=f'XBGClassifier: {roc_auc:.2f}')\nplt.plot([0,1], [0,1], 'k--', alpha=0.6)\n\nplt.legend(frameon=1, facecolor='w')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not the greatest ROC curve, but that can be more finely tuned as   \nmore data comes in.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nmxN = confusion_matrix(y_test, xgb_pred)\nplt.subplots(1, 1, figsize=(6,6), tight_layout=True)\n\nplt.title('Confusion Matrix', size=18, y=1.025)\nsns.heatmap(mxN, annot=True, fmt='d',\n            cmap='Blues', cbar=False,\n            xticklabels=['Stayed', 'Left'],\n            yticklabels=['Stayed', 'Left']\n           )\n\nplt.xlabel('Actual', size=16)\nplt.ylabel('Predicted', size=16)\nplt.yticks(rotation=0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix seems to suggest that our XGB model has taken an aggressive stance on  \npredicting that employees have left, even if they haven't.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Actionable Insight**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The data shown suggests that a company should focus on at the following 3 key  \nfactors when planning to maximize employee retention while minimizing adverse  \naffection on other factors of influence.\n\n**Over time**  \nThose with higher overtime hours logged tended to be be much more likely to quit  \nor be fired than those with less overtime hours. [Occupational burnout](https://en.wikipedia.org/wiki/Occupational_burnout) is a legitimate  \nconcern that should be addressed when considering employee retention.\n\nPossible  \nsteps to consider include *improved distribution of responsibility* and encouragement  \nto not work to such an extent that an employee may evetually suffer from  [occupational burnout.](https://en.wikipedia.org/wiki/Occupational_burnout)  \n\n**Job Level**  \nIn an almost linear fashion, those with a lower job level saw an increased rate  \nof attrition.\n\nThe employer should aim to focus on promoting job advancement opportunities when  \npossible to get employees out of that stage OR they should improve the quality of  \nlife for the employees who are in that stage.\n\n**Stock Options Level**  \nThose with a 0 Level stock option had not only the highest raw attrition count  \nat nearly 150,  but also held the highest attrition ratio with respect to the  \nother stock options levels. \n\nAn employee's stock options satisfaction should be considered as another  \npossible metric to further investigate as a factor retention influence.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}