{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dostoevsky or Tolstoy?\n\nWhen the topic of a conversation is the Russian Literature, \"wo/men of letters\" quite frequently encounter the question \"Who do you think is greater, Tolstoy or Dostoevsky?\", \"Are you more Tolstoy or Dostoevsky?\", or rather briefly: \"Tolstoy or Dostoevsky?\". It is obvious that the answer to this question is rather \"one (or the other) is greater for me\" than \"one (or the other) is greater than the other\", just as one literary person once noted:\n___\n                   \"I loved them both: Tolstoy, for the story he told, \n                                and Dostoevsky, for the thoughts he provoked.\"\n                                                              (Raquel Chanto)\n                                               \n  <img style=\"width: 600px; height:600 px;\" src=\"https://bit.ly/38EWEwM\" class=center>\n\n___\nThis Kaggle notebook will firstly try to develop a machine learning model to identify the author of a quote from the works of either of the two Russian writers. It will later repeat the same model for more Russian writers with the addition of Turgenev, which I believe will make the task even more interesting).\n\nFor this purpose, I'll use the relevant folders of [the Russian Literature Dataset](https://www.kaggle.com/d0rj3228/russian-literature) and create training and testing subdatasets out of them. \n\nA final note in the intro: I will be very happy if you could help me with your corrections and advice. \nThank you in advance."},{"metadata":{},"cell_type":"markdown","source":"## Libraries and Modules"},{"metadata":{},"cell_type":"markdown","source":"Just a quick initial note: WordNetLemmatizer did not work well with the Russian texts. So, I searched for a proper lemmatizer and came across  MorphAnalyzer from pymorphy2. I'm very satisfied with the result and I believe you'll also agree. "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"pip install pymorphy2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport random \nimport glob \nimport pickle\nfrom collections import Counter\nimport re\n\nfrom pymorphy2 import MorphAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk import tokenize\n\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlabelencoder=LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_text(filepath, min_len=50):\n    \n    text = str()\n    with open(filepath, \"r\", encoding=\"utf8\") as file:\n        sentences = tokenize.sent_tokenize(file.read())\n  \n    sentences = [sentence for sentence in sentences if len(sentence) >= min_len]\n        \n    return list(sentences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*split_text* will simply split texts into sentences of at least 50 characters each, which will somehow ensure meaningful fragments remain after the lemmatization."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(auth):\n    \n    text=[]\n    for path in glob.glob('../input/russian-literature/prose/{}/*.txt'.format(auth)):\n        text += split_text(path)  \n    \n    return text\n\nauthors=[\"Dostoevsky\", \"Tolstoy\", \"Turgenev\"]\nall_texts={}\nall_texts_train={}\nall_texts_test={}\n\nfor author in authors:\n    all_texts[author]=prepare_data(author)\n    all_texts_train[author]=all_texts[author][:10000]\n    all_texts_test[author]=all_texts[author][-2000:]\n    \nnp.random.seed(1)\n\nfor author in authors:\n    all_texts_train[author]=np.random.choice(all_texts_train[author], 10000, replace=False)\n    all_texts_test[author]=np.random.choice(all_texts_test[author], 1000, replace=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is where we get the requested data read from the files by author and then store it in a dictionary. For the sake of data consistency we take 10,000 sentences randomly chosen from each of the three authors. Please note that I prepared two sets: one for training and validation and another smaller one for post-testing purpose. In order for the post-test set not to be too similar to the training/validation set, I fetched it from the other end of the data. To summarize, we'll have 10,000 sentences per author in the training/validation set and another 1,000 per author in the post-test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in all_texts_train.keys():\n    print(key, ':', len(all_texts_train[key]), 'sentences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is some simple code to double-check if we have an equal number of sentences from each author."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(5)\n\ntmp=[]\n\nfor key, value in all_texts_train.items():    \n    for v in value:\n        txt=\"\".join(v) \n        zipped=(txt,key)\n        tmp.append(zipped)\nrandom.shuffle(tmp)\nunv=pd.DataFrame(tmp,columns=[\"text\",\"author\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We convert the \"author:text list\" dictionary into a dataframe, which will hold all the texts for the three authors in the training/validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"td=unv[unv[\"author\"] == \"Dostoevsky\"].append(unv[unv[\"author\"] == \"Tolstoy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll firstly work on \"Tolstoy or Dostoevsky\", so, for now, we'll need the data for the two authors only. Alternatively, we could first prepare the data for these two authors, and later repeat the function in the kernel, when the three authors are compared."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.shuffle(td.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It will be a good idea to make sure the data is more or less random rather than all Dostoevsky first and Tolstoy next."},{"metadata":{"trusted":true},"cell_type":"code","source":"td.index=range(20000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This will reset the indices inherited from the main dataframe above."},{"metadata":{"trusted":true},"cell_type":"code","source":"td.head(7) #7 is my lucky number :-P","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's convert the smaller post-testing data into a dataframe as well:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp=[]\nfor key, value in all_texts_test.items():    \n    for v in value:\n        txt=\"\".join(v) \n        zipped=(txt,key)\n        tmp.append(zipped)\nrandom.shuffle(tmp)\nunvt=pd.DataFrame(tmp,columns=[\"text\",\"author\"])\n\ntdt=unvt[unvt[\"author\"] == \"Dostoevsky\"].append(unvt[unvt[\"author\"] == \"Tolstoy\"])\nnp.random.shuffle(tdt.values)\ntdt.index=range(2000)\n\ntdt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"character_set = \"[!#$%&'()*+,./:;<=>?@[\\]^_`{|}„“~—\\\"\\-]–+«»…\"\nstopwords_ru = set(stopwords.words(\"russian\")+ [\"это\", \"твой\",\"свой\",\"всё\", \"который\", \"ещё\"])\n\nwith open (\"../input/toldostur/romans.csv\", \"r\") as f:\n    roman_nums=[item.strip() for item in f]\n\nmorph = MorphAnalyzer()\n\ndef lemmatize(sent):\n    \n    sent = re.sub('\\w*\\d\\w*', '', sent)\n    \n    punct_free=[character for character in sent if character not in character_set]\n    punct_free=''.join(punct_free)\n    \n    lemmas = []\n    for lemma in punct_free.split():\n        lemma = lemma.strip()\n        if lemma and lemma.upper() not in roman_nums:\n            lemma = morph.normal_forms(lemma)[0]\n            if lemma not in stopwords_ru and len(lemma)!=1:\n                lemmas.append(lemma)\n\n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some important remarks about the code above:**\n1. Initially I used string.punctuation to clean the punctuation marks from the text, but it was ineffective with, for example, \"«»\", the punctuation marks used in place of the quotation marks in Russian texts, or, for example, when the punctuation mark was attached to a word. I added, for the same reason, several other punctuation marks to the character_set string as I came across several exceptions by trial and error.\n2. 0-9 was initially included in the character set, but it didn't help get rid of numbers in the text, either, for which I used a simple regexp operation.\n3. I added several items to the NLTK Russian stopwords.\n4. There were Roman numerals in the texts, so I removed them by reading the numbers from a separate file. It is, of course, possible to write a separate code to identify and eliminate them, but that is not the job of this kernel."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"'''\ntd[\"lems\"]=''\ntd[\"lemphrases\"]=''\nfor i,j in enumerate(td['text']):\n    td['lems'][i]=lemmatize(j)\n    \n    td[\"lemphrases\"][i]=\" \".join(td[\"lems\"][i])\n'''\n\n#with open('td.pickle', 'wb') as p:\n#     pickle.dump(td, p, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('../input/toldostur/td.pickle', 'rb') as p:\n    td = pickle.load(p)  \n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Here we've created two more columns: one with the lemmatized lists of the texts and another with the lemmatized sentences put together as phrases.\n\nAnd we repeat the same for the smaller post-testing data:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tdt[\"lems\"]=''\ntdt[\"lemphrases\"]=''\nfor i,j in enumerate(tdt['text']):\n    tdt['lems'][i]=lemmatize(j)\n    tdt[\"lemphrases\"][i]=\" \".join(tdt[\"lems\"][i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"np.random.shuffle(tdt.values);    \ntdt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cv.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection and Model Creation\n\nThe answer to the question what features make an author different than the others is the most important step for this task. Style and content are certainly two of the most important factors: Author's choice of words, the way s/he orders them, use of unique vocabulary, etc., etc. Here I'm going to experiment with the original text (without any preprocesses) and also the lemmatized phrases. Firstly, I'll train the original texts and test them and later repeat the experiment with the lemmatized phrases. Next, I'll create a pipeline to combine the two features.\n\nIn my previous versions the code was rather messy and repetitive. I've decided to make it all into a tidier, though in a bigger chunk, format. In this new version I'll test the models for the training/validation and post-testing sets and the results will be shown in a table."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"training_scores=[]\nvalidation_scores=[]\npost_test_scores=[]\n\ndef model_test(d1,d2):\n\n    y=d1['author']\n    yt=d2['author']\n    \n    y = labelencoder.fit_transform(y)\n    yt = labelencoder.fit_transform(yt)\n    \n    for i in ['text', 'lemphrases']:\n        \n        X=d1[i]\n        Xt=d2[i]\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2294)\n\n        cv=CountVectorizer(ngram_range=(1,4), min_df=2)\n        transformer=cv.fit(X_train)\n\n        text_train=transformer.transform(X_train)\n        text_test=transformer.transform(X_test)\n        model = MultinomialNB()\n        model = model.fit(text_train, y_train)\n        \n        score=model.score(text_train, y_train)\n        training_scores.append(score)        \n \n        score=model.score(text_test, y_test)\n        validation_scores.append(score)\n\n        Xt=transformer.transform(Xt)\n        preds1=model.predict(Xt)\n        score=model.score(Xt, yt)\n        post_test_scores.append(score)\n        \n    \n    Xp=d1.drop(['author', 'lems'], axis=1)\n    yp=d1['author']\n    yp = labelencoder.fit_transform(yp)\n    Xp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, yp, test_size=0.2, random_state=1887)\n    \n    p_transformer=make_column_transformer((CountVectorizer(ngram_range=(1,4), min_df=2), \"text\"),\n                                     (CountVectorizer(ngram_range=(1,3), min_df=2), \"lemphrases\"))\n\n    p_model=make_pipeline(p_transformer, MultinomialNB())\n    p_model.fit(Xp_train, yp_train)\n    \n    score=p_model.score(Xp_train, yp_train)\n    training_scores.append(score)\n    \n    score=p_model.score(Xp_test, yp_test)\n    validation_scores.append(score)\n    \n    Xpt=d2.drop(['author', 'lems'], axis=1)\n    ypt=d2['author']\n    ypt = labelencoder.fit_transform(ypt)\n    score=p_model.score(Xpt, ypt)\n    post_test_scores.append(score)\n \n\nmodel_test(td, tdt) \n\nresults=pd.DataFrame()\nresults[\"Features\"]=[\"Original Text\", \"Lemmatized\" , \"Pipeline\"]\nresults[\"Training\"]=training_scores\nresults[\"Validation\"]=validation_scores\nresults[\"Test\"]=post_test_scores\n\nresults\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we get better results with the two features combined. Of course, it's possible to create a more complex pipeline with more detailed parameters and also think of some other features like the unique vocabulary used by each author.\n\nWe also see that the model isn't that good with totally new data, the data which the machine hasn't seen at all. It may be possible to get much better results for the totally new, untrained data, by training much bigger data.\nHowever, I must add that I get much better test_set results on my personal notebook - by about 10% - (using the same data and the same code, of course :)) I'm sharing a screenshot here:\n\n<img src=\"https://www.linkpicture.com/q/pipe_results.png\" class=center>\n\n---\nNow we'll repeat almost everything for the three great Russian authors: Dostoevsky, Tolstoy and Turgenev.\n"},{"metadata":{},"cell_type":"markdown","source":"## Your favorite Russian author: Tolstoy, Dostoevsky or Turgenev?\n---\n<img src=\"https://www.linkpicture.com/q/dtt.png\" class=center>\n\n---\nIt may be important to note in advance that those interested in the Russian Literature usually compare Dostoevsky and Tolstoy or Tolstoy and Turgenev. Dostoevsky and Turgenev are not usually compared as they are quite different from each other. Yet, the results of this task are a bit different as we see that the relative difference between the authors is pretty much the same."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"'''\nunv[\"lems\"]=''\nunv[\"lemphrases\"]=''\nfor i,j in enumerate(unv['text']):\n    unv['lems'][i]=lemmatize(j)\n    unv[\"lemphrases\"][i]=\" \".join(unv[\"lems\"][i])\n'''\n    \n#with open('unv.pickle', 'wb') as pu:\n#    pickle.dump(unv, pu, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('../input/toldostur/unv.pickle', 'rb') as pu:\n    unv = pickle.load(pu)\n    \nunv.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we repeat the same and create two more columns: one with the lemmatized lists of the texts and another with the lemmatized sentences put together as phrases.\n\nAnd we repeat the same for the smaller post-testing data:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"unvt[\"lems\"]=''\nunvt[\"lemphrases\"]=''\nfor i,j in enumerate(unvt['text']):\n    unvt['lems'][i]=lemmatize(j)\n    unvt[\"lemphrases\"][i]=\" \".join(unvt[\"lems\"][i])\n    \nunvt.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although I'm not going to use unique vocabulary by \"one and only one\" author as a feature at this moment, I'd like to share some plots to show the uniqueness of the vocabulary each of the three authors uses:"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"'''\nall_words={\"Dostoevsky\":[], \"Tolstoy\":[], \"Turgenev\":[]}\n\nfor auth in all_words.keys():\n    for line in unv[unv[\"author\"]==auth][\"lems\"]:\n        for w in line:\n            all_words[auth].append(w)\n\nall_words_unique={}\nall_words_unique[\"Dostoevsky\"]=[ x for x in all_words['Dostoevsky'] if x not in (all_words['Tolstoy'] + all_words['Turgenev'])]\nall_words_unique[\"Tolstoy\"]=[ x for x in all_words['Tolstoy'] if x not in (all_words['Dostoevsky'] + all_words['Turgenev'])]\nall_words_unique[\"Turgenev\"]=[ x for x in all_words['Turgenev'] if x not in (all_words['Dostoevsky'] + all_words['Tolstoy'])]\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with open('all_words_unique.pickle', 'wb') as p:\n#    pickle.dump(all_words_unique, p, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('../input/toldostur/all_words_unique.pickle', 'rb') as p:\n    all_words_unique = pickle.load(p)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(all_words_unique[\"Tolstoy\"]).most_common()[:10]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"auth_counts={\"Dostoevsky\": len(all_words_unique[\"Dostoevsky\"]),\n             \"Tolstoy\": len(all_words_unique[\"Tolstoy\"]),\n             \"Turgenev\":len(all_words_unique[\"Turgenev\"])\n             }\n\nfig_sizes = {'S' : (6.5,4),\n             'M' : (9.75,6),\n             'L' : (13,8)}\n\ndef show_plot(f_size=(6.5,4),plot_title=\"\",x_title=\"\",y_title=\"\"):\n    plt.figure(figsize=f_size)\n    plt.xlabel(x_title)\n    plt.ylabel(y_title)\n    plt.title(plot_title)\n\nax_bp = show_plot((6.5,4),'Unique Vocabulary by Author','Author','Count')\n#sns.barplot(x=list(auth_counts.keys()), y=list(auth_counts.values()), ax=ax_bp)\nsns.barplot(x=list(all_words_unique.keys()), y=list(len(i) for i in all_words_unique.values()), ax=ax_bp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that Turgenev has got the most unique vocabulary among the three Russian authors. Unique vocabulary by author may be added as a feature to the models in future versions."},{"metadata":{"trusted":true},"cell_type":"code","source":"awu=set(all_words_unique['Turgenev'])| set(all_words_unique['Dostoevsky']) | set(all_words_unique['Tolstoy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_scores=[]\nvalidation_scores=[]\npost_test_scores=[]\n\ndef model_test(d1,d2):\n\n    y=d1['author']\n    yt=d2['author']\n    \n    y = labelencoder.fit_transform(y)\n    yt = labelencoder.fit_transform(yt)\n    \n    for i in ['text', 'lemphrases']:\n        \n        X=d1[i]\n        Xt=d2[i]\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n        CV=CountVectorizer(ngram_range=(1,4), min_df=2)\n        transformer=CV.fit(X_train)\n\n        text_train=transformer.transform(X_train)\n        text_test=transformer.transform(X_test)\n\n        model = LogisticRegression(penalty=\"l2\", max_iter=2000, solver=\"newton-cg\") #MultinomialNB() \n        model = model.fit(text_train, y_train)\n        \n        score=model.score(text_train, y_train)\n        training_scores.append(score)        \n \n        score=model.score(text_test, y_test)\n        validation_scores.append(score)        \n\n        Xt=transformer.transform(Xt)  \n        score=model.score(Xt, yt)\n        post_test_scores.append(score)\n        \n\n    yp=d1['author']\n    Xp=d1.drop(['author', 'lems'], axis=1)\n\n    yp = labelencoder.fit_transform(yp)\n    Xp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, yp, test_size=0.2, random_state=1567)\n    \n    p_transformer=make_column_transformer((CountVectorizer(ngram_range=(1,4), min_df=2), \"text\"),\n                                     (CountVectorizer(ngram_range=(1,3), min_df=2), \"lemphrases\"))                                      \n\n    p_model=make_pipeline(p_transformer, LogisticRegression(penalty=\"l2\", max_iter=2000, solver=\"newton-cg\")) #MultinomialNB())\n    \n    p_model.fit(Xp_train, yp_train)\n    \n    score=p_model.score(Xp_train, yp_train)\n    training_scores.append(score)\n    \n    score=p_model.score(Xp_test, yp_test)\n    validation_scores.append(score)\n    \n    \n    ypt=d2['author']\n    Xpt=d2.drop(['author', 'lems'], axis=1)\n    \n    ypt = labelencoder.fit_transform(ypt)\n    score=p_model.score(Xpt, ypt)\n    post_test_scores.append(score)    \n    \n\nmodel_test(unv, unvt) \n\nresults=pd.DataFrame()\nresults[\"Features\"]=[\"Original Text\", \"Lemmatized\" , \"Pipeline\"]\nresults[\"Training\"]=training_scores\nresults[\"Validation\"]=validation_scores\nresults[\"Test\"]=post_test_scores\n\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score with the pipeline is again better. Besides, the LogisticRegression model gives much better results for the multiclass dataset. I should repeat that this pipeline is pretty simple and with more complex ones it may be possible to achieve much better results. Also, the scores with totally new data aren't very good and the model requires training with much bigger data, more complex pipelines and some further feature engineering. I'll just check the pipeline model with Kfold cross validation below, which gives a slightly better result.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\n\np_transformer=make_column_transformer((CountVectorizer(ngram_range=(1,4), min_df=2), \"text\"),\n                                     (CountVectorizer(ngram_range=(1,3), min_df=2), \"lemphrases\"))                                      \n\np_model=make_pipeline(p_transformer, LogisticRegression(penalty=\"l2\", max_iter=2000, solver=\"newton-cg\"))\n\nyp=unv['author']\nXp=unv.drop(['author', 'lems'], axis=1)\n\nkfold = model_selection.KFold(n_splits=3, shuffle=True, random_state=2323)\nresults = model_selection.cross_val_score(p_model, Xp, yp, cv=kfold)\nprint(\"Accuracy: %.1f%%\" % (results.mean()*100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_model=p_model.fit(Xp, yp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypt=unvt['author']\nXpt=unvt.drop(['author', 'lems'], axis=1)\n\nf_model.score(Xpt,ypt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bonus :)\n\nI'm aware of the fact that some of us don't like wordclouds much and I must admit I also don't know to what extent they could be useful, but they \"kinda\" look nice :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nfor author, color in [(\"Dostoevsky\",\"orange\"), (\"Tolstoy\", \"lightgreen\"), (\"Turgenev\",\"lightblue\")]:\n    wc=WordCloud(font_path='../input/toldostur/a_RussDecor.ttf', random_state=42, \n                         background_color=color,  width=1200, height=900, collocations=False,\n                         max_words=200) \n    wordcloud=wc.generate(' '.join(all_words_unique[author]))\n    plt.figure(figsize=(9, 6))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.suptitle(author, size=\"x-large\", weight=\"bold\")\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}