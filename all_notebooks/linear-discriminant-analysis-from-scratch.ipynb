{"cells":[{"metadata":{},"cell_type":"markdown","source":"## LDA \n#### Supervised Dimensionality Reduction Technique\n###### It increases class separability within class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Refer to PCA for explanation and dimensions\n## Link https://www.kaggle.com/ankan1998/pca-from-scratch\n## Details on Eigenvectors\n## For more resources visit https://www.kaggle.com/getting-started/176613","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class LDA:\n    \n    # Compare LDA with my PCA graph and see what is the difference between two\n    # number_of_important_feature is the component axes in mathematical terms\n    def __init__(self, number_of_important_features=2):\n        self.number_of_important_features=number_of_important_features\n        self.LDs=None\n    \n    def fit(self, X,y):\n        feature_count=X.shape[1]\n        # Getting unique classes in y\n        type_of_class_in_y=np.unique(y)\n        # Calculating mean of all samples\n        mean_all=np.mean(X,axis=0)\n        # Initialising with zeros these below matrix\n        separation_within_class=np.zeros((feature_count,feature_count))\n        separation_between_class=np.zeros((feature_count,feature_count))\n        # Iterating over each type of unique classes of y\n        for c in type_of_class_in_y:\n            X_of_each_class=X[y==c]\n            # Calculating the mean of each unique class\n            mean_of_each_class=np.mean(X_of_each_class,axis=0)\n            # Calculating separation within class(squared) and summing over it\n            separation_within_class=separation_within_class+np.dot((X_of_each_class-mean_of_each_class).T,(X_of_each_class-mean_of_each_class))\n            # Calculating difference between mean of each class with mean of overall samples\n            mean_difference_with_overall_mean=(mean_of_each_class-mean_all).reshape(feature_count,1)\n            # Calculating and summing over separation between classes\n            separation_between_class=separation_between_class+(X.shape[0]*np.dot(mean_difference_with_overall_mean,mean_difference_with_overall_mean.T))\n            # calculating these formula (d1(squared)+d2(squared)+d3(squared)..)/s1(squared)+s2(squared)+s3(squared)\n            # separation_within_class(inverse)xseparation_between_class==>mat_trans\n            mat_trans=np.dot(np.linalg.inv(separation_within_class),separation_between_class)\n            # Same as PCA \n            # Refer to PCA for explanation and dimensions\n            # Link https://www.kaggle.com/ankan1998/pca-from-scratch\n            # Details on Eigenvectors\n            # For more resources visit https://www.kaggle.com/getting-started/176613\n            eigenvalues,eigenvector=np.linalg.eig(mat_trans)\n            eigenvector=eigenvector.T\n            indexs=np.argsort(eigenvalues)[::-1]\n            eigenvector=eigenvector[indexs]\n            eigenvalues=eigenvalues[indexs]\n            self.LDs=eigenvector[:self.number_of_important_features]\n            print(indexs)\n            \n    def apply(self,X):\n        # Projecting on New Axis\n        return np.dot(X,self.LDs.T)\n    \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### loading Iris Dataset\n#### 4 features and 1 output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=pd.read_csv(\"../input/wine-pca/Wine.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=dataset.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=dataset.iloc[:,:-1]\ny=dataset.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting\nX_train=dataset.iloc[:150,:-1]\nX_test=dataset.iloc[150:,:-1]\ny_train=dataset.iloc[:150,-1]\ny_test=dataset.iloc[150:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardizing\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\nprint(X[0:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply LDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lda=LDA(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Projecting\nprojected=lda.apply(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x0=projected[:,0]\nx1=projected[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x0,x1,c=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(x0,x1,shade=True,cmap=\"Purples_d\",cbar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This kernel density distribution tells about the probablity density of two features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn=KNeighborsClassifier(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cn=confusion_matrix(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}