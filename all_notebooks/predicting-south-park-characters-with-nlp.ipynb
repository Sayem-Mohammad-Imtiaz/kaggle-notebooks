{"cells":[{"metadata":{},"cell_type":"markdown","source":"# South Park Character Predictor\nWelcome to the South Park Character Predictor, where we will be using a dataset of South Park dialogues to predict which character in the show is speaking."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler, Normalizer\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/southparklines/All-seasons.csv')\nseasons = data['Season']\nepisodes = data['Episode']\nlines = data['Line']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\nFirstly, we will do feature engineering. Below is the dataset which we given. It consists of over 70,000 lines in the show, along with what season and episode it is in and which character speaks it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the initial data, there is a '\\n' at the end of every line. The function this performs is that it creates a new line when printed out. The next cell removes all instances of them from every point in the input."},{"metadata":{"trusted":true},"cell_type":"code","source":"d = []\nfor i in data['Line']:\n    d.append(str(i)[:-1])\ndata['Line'] = d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we define X to be the 'Line' feature and y to be the 'Character' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['Line']\ny = data['Character']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = Counter(y)\nnames = []\nX = []\ny = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we remove every person which has spoken less than 1,000 times. This is done so that we can remove all the unimportant people in the series, focusing only on the main characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor j in count.keys():\n    if list(count.values())[i] > 1000:\n        names.append(list(count.keys())[i])\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nwhile i < len(data['Character']):\n    if data['Character'][i] in names:\n        X.append(data['Line'][i])\n        y.append(data['Character'][i])\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, the target (y) input is categorical, so we need to convert it into numerical. This is performed by the LabelEncoder function, which assigns a number to every unique value in the array."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = y\nle = LabelEncoder()\ny = le.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the most necessary pieces of engineering we can do is splitting the X and y into train and test sets. We accomplish this task through the train_test_split function. This splits our data to be 80% train (which is used to train the model) and 20% test (which is used to evaluate how well the model has done)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the dialogue lines in our X data are textual, we must convert them into numerical. This is done using a Bag of Words method and subsequently a TF-IDF method."},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer()\ntfidf = TfidfTransformer()\nnormalizer = Normalizer()\n\nXtr_bow = cv.fit_transform(X_train)\nXte_bow = cv.transform(X_test)\n\nX_train = tfidf.fit_transform(Xtr_bow)\nX_test = tfidf.fit_transform(Xte_bow)\n\nX_train = normalizer.fit_transform(X_train)\nX_test = normalizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualisation\nNext, we will visualise the data."},{"metadata":{},"cell_type":"markdown","source":"The first part will be using a word cloud to determine which words are the most common. As we can see below, the most frequently used words are 'know', 'now', 'right', 'well' and 'oh'."},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(background_color='white').generate(' '.join(lines))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afterwards, we will examine three bar graphs which study how much dialogue there was in relation to the characters, season and episode number.\n* The first graph shows that the character which speaks the most is Cartman, with him speaking almost 10,000 times. Stan and Kyle follow with over 7,000, then Butters, Randy and Mr Garrison.\n* The second graph shows that the amount of lines dropped as the seasons went by, with the decline beginning at season 7. Seasons 2-7 had all above 4,000 lines, reaching a maximum of over 6,000, however after season 7 the average was around 3,000.\n* The third graph shows that episodes 1-14 in per season had roughly 4-5 thousand lines, however episodes 15-18 had less than 2,000."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lines per character\ncountY = Counter(target)\nplt.bar(countY.keys(), countY.values(), color='blue')\nplt.title('Distribution of character lines')\nplt.ylabel('Number of lines')\nplt.xlabel('Character')\nplt.show()\n\n#lines per season\ncountS = Counter(seasons)\ndel countS['Season']\n\nvals = list(countS.values())[9:]+list(countS.values())[:9]\nkeys = list(countS.keys())[9:]+list(countS.keys())[:9]\ncountS = dict(zip(keys, vals))\n\nplt.bar(countS.keys(), countS.values(), color='red')\nplt.title('Distribution of line per season')\nplt.ylabel('Number of lines')\nplt.xlabel('Season')\nplt.show()\n\n#lines per episode\ncountE = Counter(episodes)\ndel countE['Episode']\n\nplt.bar(countE.keys(), countE.values(), color='green')\nplt.title('Distribution of lines per episode')\nplt.ylabel('Number of lines')\nplt.xlabel('Episode in season')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification\nThe final part of this notebook will be creating a model which predicts the characters speaking in the show. Here, we test three different classifiers: Naive Bayes, Linear SVC and Passive Agressive."},{"metadata":{},"cell_type":"markdown","source":"We loop over all these predictors and fit them to X and y train. Following, we evaluate their scores using accuracy score, model score and cross val score. As we can see below, the Linear SVC does the best, as it reaches scores of 45%, while the others perform at around 41%."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [MultinomialNB(), LinearSVC(), PassiveAggressiveClassifier()]\n\nfor model in models:\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    score = model.score(X_test, y_test)\n    cross_val = cross_val_score(model, X_test, y_test).mean()\n\n    print(model, 'accuracy:', accuracy, ' score:', score, ' cross_val:', cross_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After tuning it with a Grid Search CV, the scores of the Linear SVC are shown below, with accuracy and model scores being 45% and cross val score being 41%."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearSVC(C=0.4, loss='squared_hinge', penalty='l2', tol=0.1, multi_class='ovr')\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nscore = model.score(X_test, y_test)\ncross_val = cross_val_score(model, X_test, y_test).mean()\n\nprint('accuracy:', accuracy, ' score:', score, ' cross_val:', cross_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you for reading my notebook."},{"metadata":{},"cell_type":"markdown","source":"## If you enjoyed this notebook and found it useful, please upvote it as it will help me make more of these."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}