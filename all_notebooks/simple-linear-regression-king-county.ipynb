{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#!pip install uszipcode\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import regular packages\n#Step to import the packages\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the Input file and go through the data.\ndf = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check any null values are there. \ndf.isnull().count()\n# NO null values are found. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we are supposed to predict the house price, there should be strong correlation between Square feet\n# and house price. Lets try summing up square feet price and try to plot it against price to get a view over it\n\n# Let us sum all the square feets and try to correlate against Price. \n\ndf['sum'] = df['sqft_living']+df['sqft_above']+df['sqft_basement']\ny = df['price']\nx=df['sum']\nplt.scatter(x,y)\nplt.xlabel('Sqft Sum')\nplt.ylabel('Price in  $100k')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The above graph Sqft Sum vs Price seems to be linearly correlated. We can see strong corelationship. \n# Lets check on other possible way to visualize the data. \n# Going through the data, zipcode plays an important role as prices of certian codes (downtown/Near commerical areas)\n# may have higher rates. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### let us find the top 10 count of houses available based on zipcode\n\nk=df[['zipcode','id']].groupby(['zipcode']).count().sort_values(by='id',ascending=False).head(10).plot.bar()\nplt.figure(figsize=(40,5))\nplt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\nplt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n\n# set individual bar labels using above list\nfor i in k.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    k.text(i.get_x()-.03, i.get_height()+.75, \\\n            str(round((i.get_height()), 2)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us find Per Sqft Price based on total sum arrived for indivudal houses.  \ndf['Per_Sqft'] =  (df['price']/df['sum'])\n# Let us correlate per sqft price against total sum\n\ny=df['Per_Sqft']\nx=df['sum']\nplt.scatter(x,y)\n\n#on similar note, we will compare it against other parameters as well \nx=df['sqft_living']\nplt.scatter(x,y,color='green')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#However the above model is not right as Per_sqft price vary from house to house depending on the size of the house \n#and facilities it might have. Lets try to correlate the persqft averaged out against the zipcode. \n#Lets group zipcodes and find out average sqft price. \n\nzipgrp=pd.DataFrame(df.groupby('zipcode').mean())\nzipgrp.sort_values(by=['Per_Sqft'],inplace=True, ascending=False)\nzipgrp.reset_index(level=0, inplace=True)\n \n# Moving top 10 data for graphical representation     \nzipgrp1 = zipgrp.head(10)\nzipgrp1.filter(['zipcode','Per_Sqft']).head(5)\n\n# the top zipcodes which has highest sellig prices are below. Please scroll right to see the ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find out locations with highest selling units with high per square feet price\nplt.figure(figsize=(35,5))\nsns.set(style=\"whitegrid\")\nplot_order = zipgrp1.sort_values(by='Per_Sqft', ascending=False).zipcode.values\ng=sns.barplot(x=df['zipcode'],y=df['Per_Sqft'],data=df,order=plot_order)\nplt.ylabel('Per_Sqft in ($)',fontsize=18)\nplt.xlabel('Zipcode',fontsize=18)\nfor i in g.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    g.text(i.get_x()+.3, i.get_height()+1, \\\n            str(round((i.get_height()), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us try to do few correlations as well. A strong correlation against 'Price' should help us to understand\n# how other variables are related to it.\ncorrelation_matrix = df.corr()\nplt.figure(figsize = (10,10))\ns = correlation_matrix['price'].sort_values(ascending = False)\nprint(s)\ns.plot.bar()\nplt.xlabel('Zipcode',fontsize=18)\nplt.ylabel('Correlation factor',fontsize=18)\n#for i in s.patches:\n#    # get_x pulls left or right; get_height pushes up or down\n#    s.text(i.get_x()+.3, i.get_height()+1, \\\n#            str(round((i.get_height()), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting training and test data \n# Considered 'Per_Sqft'data as well as it improves covariance to a greater extent.\ny = df['price']\nX = df[['sqft_living','grade','sqft_above','sqft_living15','bathrooms','view','bedrooms','lat','floors','waterfront',\n        'sqft_basement','sqft_lot','yr_renovated','yr_built','condition','long','Per_Sqft']]\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, \n                                                    random_state=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing linear regressionmodel and predicting output\n\nX_test.count()\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression() \nreg.fit(X_train, y_train)\nprediction = reg.predict(X_test)\nprint('Prediction', prediction,sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To retrieve the intercept:\nprint('Intercept', reg.intercept_)\n#For retrieving the slope:\nprint('Regression Coefficients: \\n', reg.coef_) \n# variance score: 1 means perfect prediction \nprint('Variance score: {}'.format(reg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint('Mean Absolute error',metrics.mean_absolute_error(y_test,prediction))\nprint('Mean Squared Error',metrics.mean_squared_error(y_test,prediction))\nprint('Rootmean squared Error',np.sqrt(metrics.mean_squared_error(y_test,prediction)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, prediction)\nplt.xlabel(\"Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check on residuals\ndf2= X_test\ndf1 = pd.DataFrame({'Actual': y_test, 'Predicted': prediction})\ndf1['Residuals'] = df1['Actual'] - df1['Predicted']\ndf1.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see residuals are distributed randomly or does it follow a pattern. \n# let us try to plot a graph between predicted values and residuals\n\nx=df1['Predicted']\ny=df1['Residuals']\n\nplt.scatter(x,y,color='blue')\n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.figure(figsize=(100,10))\nplt.grid(True)\n\n#Residuals were not exactly randomly distributed due to mulitcolinearity factors. Need to transform the data with diffrent\n# regrression models to match the best fit. Will keep posted the second version on this. Few more residual visualization for reference. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.regressor import residuals_plot\nviz = residuals_plot(LinearRegression(), X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.regressor import ResidualsPlot\nfrom sklearn.linear_model import Ridge\nmodel = ResidualsPlot(Ridge())\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\nmodel.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}