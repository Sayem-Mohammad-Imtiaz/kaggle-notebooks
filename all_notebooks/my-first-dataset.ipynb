{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plots\nfrom sklearn.neighbors import KNeighborsClassifier # Closest neighbors \nfrom sklearn.metrics import accuracy_score # Accuracy metrics\nfrom sklearn.neural_network import MLPClassifier\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":22,"outputs":[{"output_type":"stream","text":"['mnist_train.csv', 'mnist_test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading data\nmnist_train = pd.read_csv(\"../input/mnist_train.csv\")\nmnist_test  = pd.read_csv(\"../input/mnist_test.csv\")\n\n# Initialising column headers\ncols = [\"label\"]\nfor i in range(784):\n    cols.append(\"px_{}\".format(i + 1))\n\n# Labeling the columns\nmnist_train.columns = cols\nmnist_test.columns  = cols","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get image from row of numbers\nimage_row_id  = 199\nimage_row = mnist_train.values[image_row_id, 1:] # image_row.shape = (784,)\n# Reshape the row into 28x28 matrix\nimage_shaped = image_row.reshape(28, 28)\n# Show image\nplt.imshow(image_shaped, cmap=\"Greys\")","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f8de10b9208>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADbdJREFUeJzt3X+IHHWax/HPY34Qko0QTZsEV524qCgDZo8miJEjh5fFDMG4fygbdM2BXKJsyC3mj9MY0H8EPS4JCx4LWR03keju4a4YNd7FCwdh4VjSimd0PU9PJm7CTDJBJeavZCbP/TEVmdXpb7ddVV09ed4vGKa7nqr+PlTmk+ru6uqvubsAxHNJ1Q0AqAbhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1MxuDrZw4ULv6+vr5pBAKENDQzp16pS1s26u8JvZHZJ+IWmGpGfd/anU+n19fWo0GnmGBJBQr9fbXrfjp/1mNkPSv0haLekmSevM7KZOHw9Ad+V5zb9c0ifu/qm7n5X0G0lri2kLQNnyhP9KSX+edP9YtuwvmNkGM2uYWWN0dDTHcACKVPq7/e6+y93r7l6v1WplDwegTXnCf1zSVZPufz9bBmAayBP+w5KuM7OlZjZb0k8k7SumLQBl6/hUn7uPmdkmSf+uiVN9g+7+QWGdAShVrvP87r5f0v6CegHQRXy8FwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaByzdJrZkOSvpI0LmnM3etFNAWgfLnCn/kbdz9VwOMA6CKe9gNB5Q2/SzpgZm+b2YYiGgLQHXmf9t/m7sfN7ApJb5nZ/7j7ockrZP8pbJCkq6++OudwAIqS68jv7sez3yclvSJp+RTr7HL3urvXa7VanuEAFKjj8JvZPDObf+G2pB9Jer+oxgCUK8/T/kWSXjGzC4/zorv/WyFdAShdx+F3908l3VxgLwC6iFN9QFCEHwiK8ANBEX4gKMIPBEX4gaCKuKoPLZw9ezZZP3fuXK7HHx8fb1obHBzM9dg7d+5M1t09WX/44Yc7HnvlypXJen9/f7I+cyZ/3ikc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKE6EdsG2bduS9e3bt3epk+K1Os+/ZcuW0sZevXp1sj4wMNC0tnHjxuS2M2bM6Kin6YQjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EZa3O0xapXq97o9Ho2njdMjIykqwvXbo0WW91vX+ZrrnmmmR9zZo1pY394osvJutffPFFaWOfPn06WZ83b15pY5epXq+r0WhYO+ty5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoFpez29mg5LWSDrp7v3Zsssk/VZSn6QhSfe4e3knZXvc4sWLk/UdO3Yk62NjY8n64cOHk/XNmzc3rd14443JbVtdtz5nzpxkPY+nn346Wb/77ruT9TfffLPIdsJp58j/a0l3fGPZI5IOuvt1kg5m9wFMIy3D7+6HJH3+jcVrJe3Obu+WdFfBfQEoWaev+Re5+3B2e0TSooL6AdAlud/w84mLA5peIGBmG8ysYWaN0dHRvMMBKEin4T9hZkskKft9stmK7r7L3evuXq/Vah0OB6BonYZ/n6T12e31kl4tph0A3dIy/Gb2kqT/knSDmR0zswckPSVplZl9LOlvs/sAppGW5/ndfV2T0u0F93LReuihh6puoSc9//zzyfqBAwdyPf6qVaua1mbPnp3rsS8GfMIPCIrwA0ERfiAowg8ERfiBoAg/EBRTdCOXVl87vn///qa1rVu3JrcdHx9P1hcuXJis33///U1rs2bNSm4bAUd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8/xIOnbsWLK+bdu2ZP2FF17oeOy5c+cm64cOHUrWb7jhho7HjoAjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExXn+i8DEjGlT++yzz5LbPv7448n6nj17kvVWU3xff/31TWubNm1Kbvvggw/mGhtpHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiW5/nNbFDSGkkn3b0/W/aEpL+XNJqtttXdm39BO3I5c+ZMsr5ly5amtWeffTbX2GaWrN97773J+s6dO5vWFixY0FFPKEY7R/5fS7pjiuU73X1Z9kPwgWmmZfjd/ZCkz7vQC4AuyvOaf5OZvWdmg2bG8zdgmuk0/L+U9ANJyyQNS9rebEUz22BmDTNrjI6ONlsNQJd1FH53P+Hu4+5+XtKvJC1PrLvL3evuXq/Vap32CaBgHYXfzJZMuvtjSe8X0w6AbmnnVN9LklZKWmhmxyQ9LmmlmS2T5JKGJG0ssUcAJWgZfndfN8Xi50ro5aJ19OjRZP3JJ59M1g8ePJisDw0NfdeWvnbfffcl662u97/22ms7HruVkZGRZP3LL79M1vPsl7yeeeaZZP3111/vUifN8Qk/ICjCDwRF+IGgCD8QFOEHgiL8QFB8dXebhoeHm9Yee+yx5LZ79+5N1sfGxjrq6YKBgYGmtVZTZM+cmf4TePnll5P1Vl/tvWPHjmQ95dy5c8n6+Ph4rnpKq68Fv+KKK5L11GXWvYIjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExXn+zJEjR5L1O++8s2mt1TTYZdu/v/mXJ19++eWljn3+/Plk/ZJLyju+XHrppcn6Lbfc0rR26623JrdN/XtL0s0335ysTwcc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKM7zZ3bv3p2sV30uP6VerzetrVmzJrnt3Llzk/U33ngjWV+xYkWynrruvdXYt99+e7I+Z86cZH3x4sXJenQc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqJbn+c3sKkl7JC2S5JJ2ufsvzOwySb+V1CdpSNI97v5Fea2W66OPPkrW58+f3/Fjtzqf/eijjybr/f39yXrqXPvs2bOT27ayefPmZH3WrFm5Hh/VaefIPyZpi7vfJOkWST8zs5skPSLpoLtfJ+lgdh/ANNEy/O4+7O7vZLe/kvShpCslrZV04WNxuyXdVVaTAIr3nV7zm1mfpB9K+qOkRe5+YQ6rEU28LAAwTbQdfjP7nqTfSfq5u5+eXHN318T7AVNtt8HMGmbWGB0dzdUsgOK0FX4zm6WJ4O91999ni0+Y2ZKsvkTSyam2dfdd7l5393qtViuiZwAFaBl+MzNJz0n60N0nT7m6T9L67PZ6Sa8W3x6AsrRzSe8KST+VdMTM3s2WbZX0lKR/NbMHJB2VdE85LXbHa6+9VnULPYlTeRevluF39z9Isibl9AXXAHoWn/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBNUy/GZ2lZn9p5n9ycw+MLN/yJY/YWbHzezd7Geg/HYBFGVmG+uMSdri7u+Y2XxJb5vZW1ltp7v/c3ntAShLy/C7+7Ck4ez2V2b2oaQry24MQLm+02t+M+uT9ENJf8wWbTKz98xs0MwWNNlmg5k1zKwxOjqaq1kAxWk7/Gb2PUm/k/Rzdz8t6ZeSfiBpmSaeGWyfajt33+XudXev12q1AloGUIS2wm9mszQR/L3u/ntJcvcT7j7u7ucl/UrS8vLaBFC0dt7tN0nPSfrQ3XdMWr5k0mo/lvR+8e0BKEs77/avkPRTSUfM7N1s2VZJ68xsmSSXNCRpYykdAihFO+/2/0GSTVHaX3w7ALqFT/gBQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCMnfv3mBmo5KOTlq0UNKprjXw3fRqb73al0RvnSqyt2vcva3vy+tq+L81uFnD3euVNZDQq731al8SvXWqqt542g8ERfiBoKoO/66Kx0/p1d56tS+J3jpVSW+VvuYHUJ2qj/wAKlJJ+M3sDjP7yMw+MbNHquihGTMbMrMj2czDjYp7GTSzk2b2/qRll5nZW2b2cfZ7ymnSKuqtJ2ZuTswsXem+67UZr7v+tN/MZkj6X0mrJB2TdFjSOnf/U1cbacLMhiTV3b3yc8Jm9teSzkja4+792bJ/kvS5uz+V/ce5wN3/sUd6e0LSmapnbs4mlFkyeWZpSXdJ+jtVuO8Sfd2jCvZbFUf+5ZI+cfdP3f2spN9IWltBHz3P3Q9J+vwbi9dK2p3d3q2JP56ua9JbT3D3YXd/J7v9laQLM0tXuu8SfVWiivBfKenPk+4fU29N+e2SDpjZ22a2oepmprAomzZdkkYkLaqymSm0nLm5m74xs3TP7LtOZrwuGm/4fdtt7v5XklZL+ln29LYn+cRrtl46XdPWzM3dMsXM0l+rct91OuN10aoI/3FJV026//1sWU9w9+PZ75OSXlHvzT584sIkqdnvkxX387Vemrl5qpml1QP7rpdmvK4i/IclXWdmS81stqSfSNpXQR/fYmbzsjdiZGbzJP1IvTf78D5J67Pb6yW9WmEvf6FXZm5uNrO0Kt53PTfjtbt3/UfSgCbe8f8/SY9V0UOTvq6V9N/ZzwdV9ybpJU08DTynifdGHpB0uaSDkj6W9B+SLuuh3l6QdETSe5oI2pKKertNE0/p35P0bvYzUPW+S/RVyX7jE35AULzhBwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqP8HjkowtGQReHQAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting train and test data values\n# train - data from which the model will train,             we will train from it\n# test  - data from which the model will test its accuracy, we will check if true values and predicted values are correct from it\ntrain_data = mnist_train.values[:, 1:] # (60000, 784) 60000x784 matrix\ntest_data  = mnist_test.values[:, 1:]  # (10000, 784) 10000x784 matrix\n\n# Getting labels for train and test data values\n# Labels are the real values of data, the representation of what the data holds in itself\n# Labels here are the digits from 0 to 9, and the data is 28x28 grid with grayscale numbers from 0 to 255\ntrain_label = mnist_train.values[:, 0] # (60000,) 60000x1 matrix / vector column\ntest_label  = mnist_test.values[:, 0]  # (10000,) 10000x1 matrix / vector column","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape, train_label.shape)\nprint(test_data.shape, test_label.shape)","execution_count":26,"outputs":[{"output_type":"stream","text":"(60000, 784) (60000,)\n(10000, 784) (10000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Closest neighbors algorithm usage\n# n_jobs = how many cpu cores will be used\n#     -1 = all you have\n# In these steps we train the model using TRAIN DATA (train_data)\n# by giving it our dataset with values (28x28) and giving it correct values as labels so they map like this:\n# label => 28x28 pic\n# 7     => 1x784 matrix / vector row of pixels that has shape of a 7\n# 3     => 1x784 shape of 3\n# 2     => 1x784 shape of 2\n# etc\n# And then we will test what our trained model will predict if we give her\n# some test data she hadn't seen before\n# So the model will take her knowledge and tries to PREDICT what it will be\nkn_classifier = KNeighborsClassifier(n_jobs=-1)\n# .fit(x, y): fit the model using X as train data and Y as target values\nkn_classifier = kn_classifier.fit(train_data, train_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We should look up onto our test data and check by ourselves \n# what the random-picked row (digit) contains\ntest_row_id = 199\ntest_row_matrix = test_data[test_row_id, :].reshape(28, 28) # We taking the test_row_id ROW, and ALL the COLUMNS the test_data row contains\n                                                            # and reshape it so it will like like 28x28 matrix of colormapped color brightness\nplt.imshow(test_row_matrix, cmap=\"Greys\")\nprint('The digit on a plot is: {}'.format(test_label[test_row_id]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then, we are trying to predict with Closest neighbors classifier on a same number \n# predict(labels) - predict the class LABELS for provided data\nkn_classifier.predict(test_data[test_row_id, :].reshape(1, 784))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So on, using metrics we doing the same stuff with all test data set\nkn_predictions = kn_classifier.predict(test_data)\n# And output of a total score\ntotal_score = accuracy_score(test_label, kn_predictions)\nprint(\"Точность модели: {}\".format(total_score * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# Neural network\n# Same as with closest neighbors, we fit, we predict\n# verbose = Do output\n# max_iter = max iterations\n# n_iter_no_change = how many iterations without breaking the delta there should be \nmlp_classifier = MLPClassifier(verbose=True, max_iter=750, n_iter_no_change=700)\nmlp_classifier = mlp_classifier.fit(train_data, train_label)","execution_count":27,"outputs":[{"output_type":"stream","text":"Iteration 1, loss = 2.40887741\nIteration 2, loss = 0.83834367\nIteration 3, loss = 0.46925628\nIteration 4, loss = 0.33188863\nIteration 5, loss = 0.24610761\nIteration 6, loss = 0.20453632\nIteration 7, loss = 0.17623661\nIteration 8, loss = 0.15055558\nIteration 9, loss = 0.13902115\nIteration 10, loss = 0.12824005\nIteration 11, loss = 0.12335654\nIteration 12, loss = 0.11821255\nIteration 13, loss = 0.12211546\nIteration 14, loss = 0.10948673\nIteration 15, loss = 0.10321836\nIteration 16, loss = 0.10271990\nIteration 17, loss = 0.09101988\nIteration 18, loss = 0.09576018\nIteration 19, loss = 0.08981042\nIteration 20, loss = 0.08540228\nIteration 21, loss = 0.08174301\nIteration 22, loss = 0.07382168\nIteration 23, loss = 0.07428456\nIteration 24, loss = 0.07569388\nIteration 25, loss = 0.07544307\nIteration 26, loss = 0.06834083\nIteration 27, loss = 0.06387472\nIteration 28, loss = 0.06696220\nIteration 29, loss = 0.06384428\nIteration 30, loss = 0.06886659\nIteration 31, loss = 0.07061539\nIteration 32, loss = 0.05862419\nIteration 33, loss = 0.05069168\nIteration 34, loss = 0.05141473\nIteration 35, loss = 0.05917654\nIteration 36, loss = 0.06416412\nIteration 37, loss = 0.04962507\nIteration 38, loss = 0.05004129\nIteration 39, loss = 0.05501719\nIteration 40, loss = 0.05252299\nIteration 41, loss = 0.04538107\nIteration 42, loss = 0.05600248\nIteration 43, loss = 0.04658496\nIteration 44, loss = 0.03960451\nIteration 45, loss = 0.04760871\nIteration 46, loss = 0.05138837\nIteration 47, loss = 0.04993789\nIteration 48, loss = 0.03973890\nIteration 49, loss = 0.04899459\nIteration 50, loss = 0.04311043\nIteration 51, loss = 0.04755256\nIteration 52, loss = 0.04843278\nIteration 53, loss = 0.03753629\nIteration 54, loss = 0.04192171\nIteration 55, loss = 0.04213136\nIteration 56, loss = 0.03880888\nIteration 57, loss = 0.03866342\nIteration 58, loss = 0.03305922\nIteration 59, loss = 0.04414033\nIteration 60, loss = 0.03756387\nIteration 61, loss = 0.03871560\nIteration 62, loss = 0.04428671\nIteration 63, loss = 0.04422390\nIteration 64, loss = 0.03374754\nIteration 65, loss = 0.02890577\nIteration 66, loss = 0.04314853\nIteration 67, loss = 0.04300307\nIteration 68, loss = 0.03661209\nIteration 69, loss = 0.03506460\nIteration 70, loss = 0.03405127\nIteration 71, loss = 0.04336590\nIteration 72, loss = 0.02707894\nIteration 73, loss = 0.03560562\nIteration 74, loss = 0.03052978\nIteration 75, loss = 0.04252317\nIteration 76, loss = 0.03215136\nIteration 77, loss = 0.03436849\nIteration 78, loss = 0.03484160\nIteration 79, loss = 0.03831279\nIteration 80, loss = 0.04594769\nIteration 81, loss = 0.03021586\nIteration 82, loss = 0.02997476\nIteration 83, loss = 0.03402080\nIteration 84, loss = 0.03578352\nIteration 85, loss = 0.03501053\nIteration 86, loss = 0.03045399\nIteration 87, loss = 0.02097091\nIteration 88, loss = 0.02973019\nIteration 89, loss = 0.02797086\nIteration 90, loss = 0.04085159\nIteration 91, loss = 0.03822980\nIteration 92, loss = 0.02763485\nIteration 93, loss = 0.02946457\nIteration 94, loss = 0.04546509\nIteration 95, loss = 0.03331835\nIteration 96, loss = 0.03112341\nIteration 97, loss = 0.03621192\nIteration 98, loss = 0.03220509\nIteration 99, loss = 0.03101380\nIteration 100, loss = 0.03565841\nIteration 101, loss = 0.02750177\nIteration 102, loss = 0.02588224\nIteration 103, loss = 0.01915763\nIteration 104, loss = 0.03309789\nIteration 105, loss = 0.03824245\nIteration 106, loss = 0.03995920\nIteration 107, loss = 0.03345701\nIteration 108, loss = 0.02464829\nIteration 109, loss = 0.03860791\nIteration 110, loss = 0.03539313\nIteration 111, loss = 0.03079579\nIteration 112, loss = 0.02748923\nIteration 113, loss = 0.02224135\nIteration 114, loss = 0.01765944\nIteration 115, loss = 0.02102684\nIteration 116, loss = 0.02683254\nIteration 117, loss = 0.03687968\nIteration 118, loss = 0.04423674\nIteration 119, loss = 0.03108417\nIteration 120, loss = 0.02918960\nIteration 121, loss = 0.02619234\nIteration 122, loss = 0.02869341\nIteration 123, loss = 0.03007074\nIteration 124, loss = 0.02453083\nIteration 125, loss = 0.02043291\nIteration 126, loss = 0.02422430\nIteration 127, loss = 0.02745178\nIteration 128, loss = 0.03660454\nIteration 129, loss = 0.02840478\nIteration 130, loss = 0.02009369\nIteration 131, loss = 0.02708887\nIteration 132, loss = 0.03510869\nIteration 133, loss = 0.03917234\nIteration 134, loss = 0.02790227\nIteration 135, loss = 0.02585757\nIteration 136, loss = 0.01832092\nIteration 137, loss = 0.02387652\nIteration 138, loss = 0.03556890\nIteration 139, loss = 0.02259587\nIteration 140, loss = 0.01702132\nIteration 141, loss = 0.02774912\nIteration 142, loss = 0.02758820\nIteration 143, loss = 0.02692622\nIteration 144, loss = 0.03040604\nIteration 145, loss = 0.03388302\nIteration 146, loss = 0.03044549\nIteration 147, loss = 0.03093589\nIteration 148, loss = 0.02624958\nIteration 149, loss = 0.01973735\nIteration 150, loss = 0.01846373\nIteration 151, loss = 0.02373453\nIteration 152, loss = 0.02286761\nIteration 153, loss = 0.03068290\nIteration 154, loss = 0.03509681\nIteration 155, loss = 0.03088565\nIteration 156, loss = 0.01900251\nIteration 157, loss = 0.02683620\nIteration 158, loss = 0.02567040\nIteration 159, loss = 0.01989897\nIteration 160, loss = 0.01791232\nIteration 161, loss = 0.03505321\nIteration 162, loss = 0.03916585\nIteration 163, loss = 0.02161168\nIteration 164, loss = 0.02668295\nIteration 165, loss = 0.02088819\nIteration 166, loss = 0.02350875\nIteration 167, loss = 0.02742603\nIteration 168, loss = 0.03679444\nIteration 169, loss = 0.02210442\nIteration 170, loss = 0.02318377\nIteration 171, loss = 0.02095931\nIteration 172, loss = 0.02383310\nIteration 173, loss = 0.02867381\nIteration 174, loss = 0.01631009\nIteration 175, loss = 0.02147320\nIteration 176, loss = 0.02564370\nIteration 177, loss = 0.02551804\nIteration 178, loss = 0.02831153\nIteration 179, loss = 0.02348930\nIteration 180, loss = 0.01860585\nIteration 181, loss = 0.01578570\nIteration 182, loss = 0.02610630\nIteration 183, loss = 0.02260067\nIteration 184, loss = 0.02179013\nIteration 185, loss = 0.02173386\nIteration 186, loss = 0.02221052\nIteration 187, loss = 0.03877773\nIteration 188, loss = 0.03652384\nIteration 189, loss = 0.02816053\nIteration 190, loss = 0.01472550\nIteration 191, loss = 0.01587077\nIteration 192, loss = 0.02145123\nIteration 193, loss = 0.02967516\nIteration 194, loss = 0.02381248\nIteration 195, loss = 0.01885375\nIteration 196, loss = 0.02370265\nIteration 197, loss = 0.02674207\nIteration 198, loss = 0.02174366\nIteration 199, loss = 0.02105560\nIteration 200, loss = 0.02667056\nIteration 201, loss = 0.01987229\nIteration 202, loss = 0.01855393\nIteration 203, loss = 0.02655398\nIteration 204, loss = 0.02037571\nIteration 205, loss = 0.01663787\nIteration 206, loss = 0.02198759\nIteration 207, loss = 0.02534442\nIteration 208, loss = 0.02500874\nIteration 209, loss = 0.01716017\nIteration 210, loss = 0.02091008\nIteration 211, loss = 0.02311104\nIteration 212, loss = 0.02272622\nIteration 213, loss = 0.02108811\nIteration 214, loss = 0.02342562\nIteration 215, loss = 0.02908395\nIteration 216, loss = 0.02928996\nIteration 217, loss = 0.03271266\nIteration 218, loss = 0.02080312\nIteration 219, loss = 0.01668872\nIteration 220, loss = 0.01549863\nIteration 221, loss = 0.02151616\nIteration 222, loss = 0.02151557\nIteration 223, loss = 0.02461328\nIteration 224, loss = 0.01948087\nIteration 225, loss = 0.01972450\nIteration 226, loss = 0.02426720\nIteration 227, loss = 0.02193682\nIteration 228, loss = 0.02918633\nIteration 229, loss = 0.03032965\nIteration 230, loss = 0.02412996\nIteration 231, loss = 0.02259985\nIteration 232, loss = 0.02179860\nIteration 233, loss = 0.01483119\nIteration 234, loss = 0.01266573\nIteration 235, loss = 0.02742038\nIteration 236, loss = 0.02568143\nIteration 237, loss = 0.01613877\nIteration 238, loss = 0.01990955\nIteration 239, loss = 0.02250661\nIteration 240, loss = 0.02457459\nIteration 241, loss = 0.03331198\nIteration 242, loss = 0.02092691\nIteration 243, loss = 0.01567920\nIteration 244, loss = 0.01922045\nIteration 245, loss = 0.01430264\nIteration 246, loss = 0.01057244\nIteration 247, loss = 0.02300091\nIteration 248, loss = 0.02264381\nIteration 249, loss = 0.02665184\nIteration 250, loss = 0.03257863\nIteration 251, loss = 0.02802505\nIteration 252, loss = 0.01519703\n","name":"stdout"},{"output_type":"stream","text":"Iteration 253, loss = 0.01662432\nIteration 254, loss = 0.02348027\nIteration 255, loss = 0.01848896\nIteration 256, loss = 0.01496719\nIteration 257, loss = 0.01944046\nIteration 258, loss = 0.02265420\nIteration 259, loss = 0.02459019\nIteration 260, loss = 0.01387690\nIteration 261, loss = 0.01235397\nIteration 262, loss = 0.02599904\nIteration 263, loss = 0.02604495\nIteration 264, loss = 0.01371631\nIteration 265, loss = 0.02627044\nIteration 266, loss = 0.01674155\nIteration 267, loss = 0.01828384\nIteration 268, loss = 0.02992885\nIteration 269, loss = 0.01787442\nIteration 270, loss = 0.01678338\nIteration 271, loss = 0.01326467\nIteration 272, loss = 0.01698978\nIteration 273, loss = 0.02677139\nIteration 274, loss = 0.02863138\nIteration 275, loss = 0.01906847\nIteration 276, loss = 0.01081863\nIteration 277, loss = 0.01416752\nIteration 278, loss = 0.01693183\nIteration 279, loss = 0.02732817\nIteration 280, loss = 0.02723809\nIteration 281, loss = 0.01841862\nIteration 282, loss = 0.01710279\nIteration 283, loss = 0.01653490\nIteration 284, loss = 0.02956793\nIteration 285, loss = 0.02487995\nIteration 286, loss = 0.02584616\nIteration 287, loss = 0.01675822\nIteration 288, loss = 0.01484915\nIteration 289, loss = 0.01598189\nIteration 290, loss = 0.01631418\nIteration 291, loss = 0.03294706\nIteration 292, loss = 0.02756510\nIteration 293, loss = 0.01150603\nIteration 294, loss = 0.01369300\nIteration 295, loss = 0.01515087\nIteration 296, loss = 0.02621173\nIteration 297, loss = 0.00953729\nIteration 298, loss = 0.01404781\nIteration 299, loss = 0.02153252\nIteration 300, loss = 0.02192609\nIteration 301, loss = 0.02120401\nIteration 302, loss = 0.02346871\nIteration 303, loss = 0.01660240\nIteration 304, loss = 0.00920332\nIteration 305, loss = 0.01146739\nIteration 306, loss = 0.02335553\nIteration 307, loss = 0.03682874\nIteration 308, loss = 0.01556160\nIteration 309, loss = 0.01266514\nIteration 310, loss = 0.02019992\nIteration 311, loss = 0.01626786\nIteration 312, loss = 0.02979244\nIteration 313, loss = 0.03442385\nIteration 314, loss = 0.02317749\nIteration 315, loss = 0.01724793\nIteration 316, loss = 0.01443950\nIteration 317, loss = 0.01486147\nIteration 318, loss = 0.02857667\nIteration 319, loss = 0.01773122\nIteration 320, loss = 0.01131150\nIteration 321, loss = 0.01351214\nIteration 322, loss = 0.02682468\nIteration 323, loss = 0.01768772\nIteration 324, loss = 0.01779897\nIteration 325, loss = 0.01678889\nIteration 326, loss = 0.02352706\nIteration 327, loss = 0.01334371\nIteration 328, loss = 0.01220639\nIteration 329, loss = 0.01189909\nIteration 330, loss = 0.01914233\nIteration 331, loss = 0.02904815\nIteration 332, loss = 0.02519264\nIteration 333, loss = 0.01706997\nIteration 334, loss = 0.02106615\nIteration 335, loss = 0.01551642\nIteration 336, loss = 0.01587119\nIteration 337, loss = 0.01932640\nIteration 338, loss = 0.01095490\nIteration 339, loss = 0.01448275\nIteration 340, loss = 0.01146253\nIteration 341, loss = 0.01928491\nIteration 342, loss = 0.01638752\nIteration 343, loss = 0.02277894\nIteration 344, loss = 0.01847244\nIteration 345, loss = 0.01916187\nIteration 346, loss = 0.01209453\nIteration 347, loss = 0.01974494\nIteration 348, loss = 0.02623309\nIteration 349, loss = 0.00955764\nIteration 350, loss = 0.01114690\nIteration 351, loss = 0.02294249\nIteration 352, loss = 0.01858617\nIteration 353, loss = 0.02539477\nIteration 354, loss = 0.01315772\nIteration 355, loss = 0.01806039\nIteration 356, loss = 0.01098802\nIteration 357, loss = 0.01799872\nIteration 358, loss = 0.01939238\nIteration 359, loss = 0.01986171\nIteration 360, loss = 0.01685711\nIteration 361, loss = 0.01280365\nIteration 362, loss = 0.01567544\nIteration 363, loss = 0.02107413\nIteration 364, loss = 0.02745394\nIteration 365, loss = 0.02372769\nIteration 366, loss = 0.02006119\nIteration 367, loss = 0.01408501\nIteration 368, loss = 0.01448641\nIteration 369, loss = 0.01807970\nIteration 370, loss = 0.00925097\nIteration 371, loss = 0.02064180\nIteration 372, loss = 0.02078656\nIteration 373, loss = 0.01478413\nIteration 374, loss = 0.01548008\nIteration 375, loss = 0.02275508\nIteration 376, loss = 0.02658281\nIteration 377, loss = 0.01229370\nIteration 378, loss = 0.00979987\nIteration 379, loss = 0.01383980\nIteration 380, loss = 0.01237343\nIteration 381, loss = 0.01313057\nIteration 382, loss = 0.01811796\nIteration 383, loss = 0.01791101\nIteration 384, loss = 0.01629968\nIteration 385, loss = 0.02305010\nIteration 386, loss = 0.02038952\nIteration 387, loss = 0.02262790\nIteration 388, loss = 0.01622774\nIteration 389, loss = 0.01401011\nIteration 390, loss = 0.01741261\nIteration 391, loss = 0.01540667\nIteration 392, loss = 0.01308137\nIteration 393, loss = 0.01702742\nIteration 394, loss = 0.01527720\nIteration 395, loss = 0.01760662\nIteration 396, loss = 0.01457602\nIteration 397, loss = 0.01024662\nIteration 398, loss = 0.01398786\nIteration 399, loss = 0.03185968\nIteration 400, loss = 0.01248387\nIteration 401, loss = 0.01477977\nIteration 402, loss = 0.01429170\nIteration 403, loss = 0.01782836\nIteration 404, loss = 0.02266502\nIteration 405, loss = 0.01707399\nIteration 406, loss = 0.01069121\nIteration 407, loss = 0.00611713\nIteration 408, loss = 0.00975618\nIteration 409, loss = 0.01948515\nIteration 410, loss = 0.02676424\nIteration 411, loss = 0.01736486\nIteration 412, loss = 0.01680827\nIteration 413, loss = 0.02157312\nIteration 414, loss = 0.02371133\nIteration 415, loss = 0.01855002\nIteration 416, loss = 0.01529868\nIteration 417, loss = 0.00926882\nIteration 418, loss = 0.00826827\nIteration 419, loss = 0.00733741\nIteration 420, loss = 0.02028919\nIteration 421, loss = 0.02802885\nIteration 422, loss = 0.02515706\nIteration 423, loss = 0.01799160\nIteration 424, loss = 0.01433007\nIteration 425, loss = 0.02379768\nIteration 426, loss = 0.01653035\nIteration 427, loss = 0.01268610\nIteration 428, loss = 0.00941827\nIteration 429, loss = 0.01107866\nIteration 430, loss = 0.01107873\nIteration 431, loss = 0.02254208\nIteration 432, loss = 0.02329683\nIteration 433, loss = 0.01781654\nIteration 434, loss = 0.01531934\nIteration 435, loss = 0.01633018\nIteration 436, loss = 0.01537819\nIteration 437, loss = 0.01106189\nIteration 438, loss = 0.01465861\nIteration 439, loss = 0.02573471\nIteration 440, loss = 0.01340170\nIteration 441, loss = 0.01376192\nIteration 442, loss = 0.01574664\nIteration 443, loss = 0.01233111\nIteration 444, loss = 0.01720992\nIteration 445, loss = 0.02325749\nIteration 446, loss = 0.01789067\nIteration 447, loss = 0.01643541\nIteration 448, loss = 0.01351592\nIteration 449, loss = 0.01231598\nIteration 450, loss = 0.01360087\nIteration 451, loss = 0.01090498\nIteration 452, loss = 0.01105980\nIteration 453, loss = 0.01313376\nIteration 454, loss = 0.02058614\nIteration 455, loss = 0.01592669\nIteration 456, loss = 0.01053355\nIteration 457, loss = 0.01143482\nIteration 458, loss = 0.01164831\nIteration 459, loss = 0.01746660\nIteration 460, loss = 0.02077071\nIteration 461, loss = 0.02377822\nIteration 462, loss = 0.01787151\nIteration 463, loss = 0.01101660\nIteration 464, loss = 0.01454855\nIteration 465, loss = 0.02038283\nIteration 466, loss = 0.01530609\nIteration 467, loss = 0.01622480\nIteration 468, loss = 0.01130817\nIteration 469, loss = 0.01421725\nIteration 470, loss = 0.01599089\nIteration 471, loss = 0.01507085\nIteration 472, loss = 0.01011154\nIteration 473, loss = 0.02642988\nIteration 474, loss = 0.01318810\nIteration 475, loss = 0.02640570\nIteration 476, loss = 0.01894406\nIteration 477, loss = 0.01159152\nIteration 478, loss = 0.01220083\nIteration 479, loss = 0.00814077\nIteration 480, loss = 0.01255043\nIteration 481, loss = 0.01015027\nIteration 482, loss = 0.01162186\nIteration 483, loss = 0.01883956\nIteration 484, loss = 0.02498616\nIteration 485, loss = 0.02409188\nIteration 486, loss = 0.01217582\nIteration 487, loss = 0.01476178\nIteration 488, loss = 0.01503293\nIteration 489, loss = 0.01430146\nIteration 490, loss = 0.02127342\nIteration 491, loss = 0.01874485\nIteration 492, loss = 0.01415068\nIteration 493, loss = 0.01062249\nIteration 494, loss = 0.01620240\nIteration 495, loss = 0.01663184\nIteration 496, loss = 0.01302667\nIteration 497, loss = 0.01438822\nIteration 498, loss = 0.01603180\nIteration 499, loss = 0.00955288\nIteration 500, loss = 0.01888300\nIteration 501, loss = 0.01750878\n","name":"stdout"},{"output_type":"stream","text":"Iteration 502, loss = 0.01451635\nIteration 503, loss = 0.00994410\nIteration 504, loss = 0.01419540\nIteration 505, loss = 0.01539038\nIteration 506, loss = 0.01426154\nIteration 507, loss = 0.01318761\nIteration 508, loss = 0.01401938\nIteration 509, loss = 0.01601212\nIteration 510, loss = 0.01124595\nIteration 511, loss = 0.00832918\nIteration 512, loss = 0.01688224\nIteration 513, loss = 0.02102772\nIteration 514, loss = 0.01803640\nIteration 515, loss = 0.01337559\nIteration 516, loss = 0.02088762\nIteration 517, loss = 0.01778825\nIteration 518, loss = 0.00907430\nIteration 519, loss = 0.00847518\nIteration 520, loss = 0.00981502\nIteration 521, loss = 0.01900952\nIteration 522, loss = 0.00888569\nIteration 523, loss = 0.01324906\nIteration 524, loss = 0.03088002\nIteration 525, loss = 0.01905275\nIteration 526, loss = 0.00910279\nIteration 527, loss = 0.01266393\nIteration 528, loss = 0.00665144\nIteration 529, loss = 0.00626488\nIteration 530, loss = 0.01519974\nIteration 531, loss = 0.02009053\nIteration 532, loss = 0.01173047\nIteration 533, loss = 0.01513765\nIteration 534, loss = 0.01611843\nIteration 535, loss = 0.01485916\nIteration 536, loss = 0.01988093\nIteration 537, loss = 0.01943822\nIteration 538, loss = 0.01643327\nIteration 539, loss = 0.01009380\nIteration 540, loss = 0.00979445\nIteration 541, loss = 0.01530111\nIteration 542, loss = 0.01200001\nIteration 543, loss = 0.00792496\nIteration 544, loss = 0.01643136\nIteration 545, loss = 0.02840934\nIteration 546, loss = 0.01645541\nIteration 547, loss = 0.00887798\nIteration 548, loss = 0.01033996\nIteration 549, loss = 0.01396262\nIteration 550, loss = 0.00583173\nIteration 551, loss = 0.01570472\nIteration 552, loss = 0.02136073\nIteration 553, loss = 0.02161725\nIteration 554, loss = 0.01056967\nIteration 555, loss = 0.01356559\nIteration 556, loss = 0.01378595\nIteration 557, loss = 0.01039378\nIteration 558, loss = 0.01160557\nIteration 559, loss = 0.01853938\nIteration 560, loss = 0.01230950\nIteration 561, loss = 0.01213423\nIteration 562, loss = 0.00843602\nIteration 563, loss = 0.01925636\nIteration 564, loss = 0.02235132\nIteration 565, loss = 0.01100166\nIteration 566, loss = 0.01152282\nIteration 567, loss = 0.01239003\nIteration 568, loss = 0.01513859\nIteration 569, loss = 0.01581477\nIteration 570, loss = 0.02102359\nIteration 571, loss = 0.01700184\nIteration 572, loss = 0.01034188\nIteration 573, loss = 0.01564658\nIteration 574, loss = 0.01652514\nIteration 575, loss = 0.01456886\nIteration 576, loss = 0.01893442\nIteration 577, loss = 0.01265295\nIteration 578, loss = 0.02315585\nIteration 579, loss = 0.01378743\nIteration 580, loss = 0.01077114\nIteration 581, loss = 0.01614782\nIteration 582, loss = 0.01686555\nIteration 583, loss = 0.01287265\nIteration 584, loss = 0.00769570\nIteration 585, loss = 0.00663241\nIteration 586, loss = 0.00995138\nIteration 587, loss = 0.00889720\nIteration 588, loss = 0.02234826\nIteration 589, loss = 0.01875225\nIteration 590, loss = 0.02425984\nIteration 591, loss = 0.02067277\nIteration 592, loss = 0.00893562\nIteration 593, loss = 0.00529369\nIteration 594, loss = 0.00870904\nIteration 595, loss = 0.01019126\nIteration 596, loss = 0.02258643\nIteration 597, loss = 0.02182832\nIteration 598, loss = 0.01788182\nIteration 599, loss = 0.01460089\nIteration 600, loss = 0.01177556\nIteration 601, loss = 0.01175540\nIteration 602, loss = 0.01527430\nIteration 603, loss = 0.01540020\nIteration 604, loss = 0.00971940\nIteration 605, loss = 0.01219881\nIteration 606, loss = 0.01915781\nIteration 607, loss = 0.01191323\nIteration 608, loss = 0.01777684\nIteration 609, loss = 0.01430359\nIteration 610, loss = 0.01024332\nIteration 611, loss = 0.00990401\nIteration 612, loss = 0.01408636\nIteration 613, loss = 0.02023468\nIteration 614, loss = 0.01157026\nIteration 615, loss = 0.00782211\nIteration 616, loss = 0.02056687\nIteration 617, loss = 0.01853426\nIteration 618, loss = 0.01293032\nIteration 619, loss = 0.00891053\nIteration 620, loss = 0.00807620\nIteration 621, loss = 0.01060371\nIteration 622, loss = 0.01781396\nIteration 623, loss = 0.01546745\nIteration 624, loss = 0.01858116\nIteration 625, loss = 0.01579652\nIteration 626, loss = 0.01047954\nIteration 627, loss = 0.01108339\nIteration 628, loss = 0.01993205\nIteration 629, loss = 0.01676126\nIteration 630, loss = 0.01510597\nIteration 631, loss = 0.00862785\nIteration 632, loss = 0.01772515\nIteration 633, loss = 0.00833691\nIteration 634, loss = 0.01098307\nIteration 635, loss = 0.01206055\nIteration 636, loss = 0.01010394\nIteration 637, loss = 0.01311135\nIteration 638, loss = 0.01425479\nIteration 639, loss = 0.02232199\nIteration 640, loss = 0.02085335\nIteration 641, loss = 0.00916319\nIteration 642, loss = 0.01067642\nIteration 643, loss = 0.01141292\nIteration 644, loss = 0.01494628\nIteration 645, loss = 0.01225211\nIteration 646, loss = 0.02573319\nIteration 647, loss = 0.01057674\nIteration 648, loss = 0.00573157\nIteration 649, loss = 0.01099154\nIteration 650, loss = 0.02565560\nIteration 651, loss = 0.01763791\nIteration 652, loss = 0.01075782\nIteration 653, loss = 0.00823691\nIteration 654, loss = 0.01324542\nIteration 655, loss = 0.01415229\nIteration 656, loss = 0.01396917\nIteration 657, loss = 0.01147136\nIteration 658, loss = 0.01180490\nIteration 659, loss = 0.01304814\nIteration 660, loss = 0.02037337\nIteration 661, loss = 0.01538479\nIteration 662, loss = 0.01028590\nIteration 663, loss = 0.00824802\nIteration 664, loss = 0.01185292\nIteration 665, loss = 0.01276359\nIteration 666, loss = 0.01480893\nIteration 667, loss = 0.00712449\nIteration 668, loss = 0.00611126\nIteration 669, loss = 0.01518680\nIteration 670, loss = 0.02913787\nIteration 671, loss = 0.01217238\nIteration 672, loss = 0.01646054\nIteration 673, loss = 0.01435813\nIteration 674, loss = 0.00752475\nIteration 675, loss = 0.01193635\nIteration 676, loss = 0.01602244\nIteration 677, loss = 0.01501383\nIteration 678, loss = 0.01035360\nIteration 679, loss = 0.01576092\nIteration 680, loss = 0.01196190\nIteration 681, loss = 0.01871183\nIteration 682, loss = 0.01054305\nIteration 683, loss = 0.00610322\nIteration 684, loss = 0.00598789\nIteration 685, loss = 0.00624017\nIteration 686, loss = 0.01119080\nIteration 687, loss = 0.02508431\nIteration 688, loss = 0.01454569\nIteration 689, loss = 0.02233561\nIteration 690, loss = 0.01189124\nIteration 691, loss = 0.01146558\nIteration 692, loss = 0.01239628\nIteration 693, loss = 0.01638158\nIteration 694, loss = 0.01170497\nIteration 695, loss = 0.00532411\nIteration 696, loss = 0.00561808\nIteration 697, loss = 0.00401931\nIteration 698, loss = 0.01892137\nIteration 699, loss = 0.01494733\nIteration 700, loss = 0.02982853\nIteration 701, loss = 0.01489943\nIteration 702, loss = 0.00789179\nIteration 703, loss = 0.00720934\nIteration 704, loss = 0.00397093\nIteration 705, loss = 0.00346333\nIteration 706, loss = 0.00915535\nIteration 707, loss = 0.03346070\nIteration 708, loss = 0.01593388\nIteration 709, loss = 0.00769575\nIteration 710, loss = 0.01661579\nIteration 711, loss = 0.01305594\nIteration 712, loss = 0.01483922\nIteration 713, loss = 0.01099995\nIteration 714, loss = 0.01074412\nIteration 715, loss = 0.01294599\nIteration 716, loss = 0.01496576\nIteration 717, loss = 0.01524356\nIteration 718, loss = 0.01432588\nIteration 719, loss = 0.00930038\nIteration 720, loss = 0.00651792\nIteration 721, loss = 0.01013726\nIteration 722, loss = 0.01275558\nIteration 723, loss = 0.01832112\nIteration 724, loss = 0.01624045\nIteration 725, loss = 0.01225507\nIteration 726, loss = 0.01428617\nIteration 727, loss = 0.01279312\nIteration 728, loss = 0.01966931\nIteration 729, loss = 0.01116368\nIteration 730, loss = 0.00785467\nIteration 731, loss = 0.00779474\nIteration 732, loss = 0.00367019\nIteration 733, loss = 0.00720068\nIteration 734, loss = 0.01084463\nIteration 735, loss = 0.02116323\nIteration 736, loss = 0.01795192\nIteration 737, loss = 0.01305116\nIteration 738, loss = 0.00961888\nIteration 739, loss = 0.02055817\nIteration 740, loss = 0.01452388\nIteration 741, loss = 0.01322544\nIteration 742, loss = 0.01075872\nIteration 743, loss = 0.00906800\nIteration 744, loss = 0.00497590\nIteration 745, loss = 0.00454870\nIteration 746, loss = 0.01555417\nIteration 747, loss = 0.02181201\nIteration 748, loss = 0.02376551\nIteration 749, loss = 0.01352816\nIteration 750, loss = 0.01380532\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (750) reached and the optimization hasn't converged yet.\n  % self.max_iter, ConvergenceWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we predict the number using neural network Multi-layer perceptron\nmlp_classifier.predict(test_data[test_row_id, :].reshape(1, 784))","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"array([2])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we predict all the numbers!\n# And check the accuracy score!\nmlp_predictions = mlp_classifier.predict(test_data)\nmlp_total_score = accuracy_score(test_label, mlp_predictions)\nprint(\"Точность модели MLP: {}\".format(mlp_total_score * 100))","execution_count":29,"outputs":[{"output_type":"stream","text":"Точность модели MLP: 96.99\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}