{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from scipy import spatial\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers import Input, Dense\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/atis-airlinetravelinformationsystem/atis_intents_train.csv\", header=None)\ntest = pd.read_csv(\"../input/atis-airlinetravelinformationsystem/atis_intents_test.csv\", header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Pre-Processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"words = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor elem in iter(words):\n    count = count + 1\n    if count == 20:\n        break\n    print (elem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stopwords Corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (words)]))\ntest['text'] = test[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (words)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Digits Removal \\d+"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].str.replace('\\d+', '')\ntest['text'] = test['text'].str.replace('\\d+', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train['text']\nlabels = train[0]\ntest_text = test['text']\ntest_labels = test[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize and Padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntok = Tokenizer()\ntok.fit_on_texts(text)\nword_index = tok.word_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indexed each words as there are 631 chars, words are listed to 0-631"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_vocab_size = len(word_index) + 1\ninput_length = 25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_tokens = tok.texts_to_sequences(text)\ntest_data_tokens = tok.texts_to_sequences(test_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenized each word based off of word index"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = pad_sequences(train_data_tokens, input_length)\ntest_input = pad_sequences(test_data_tokens, input_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Padded each sentence (text) to the same size of 25"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LabelEncoder\nWhat this does is that it encodes the text in the data into numbers\nFor example if we have a columnb in our data named: Gender which takes on the values: Male, Female, Other, then the Label Encoder converts these values to 1, 2,  and 3 becasue only numeric data can be operated upon by a computer.\nHere, Male-1, Female-2, Other-3\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_transformer = preprocessing.LabelEncoder()\nlabel_transformer.fit(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.externals import joblib\n# joblib.dump(label_transformer, 'atis-airlinetravelinformationsystem/label_encoder.pk1')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = label_transformer.transform(labels)\ntest_labels = label_transformer.transform(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = to_categorical(np.asarray(labels))\ntest_labels = to_categorical(np.asarray(test_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Validation Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train_input, labels, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Embeddings "},{"metadata":{},"cell_type":"markdown","source":"Embedded Index saves the info from pretrained GloVe model which can be later used for word embedding in terms of its\napplication to our specific model. Our embedded matrix is first matrix of zeros, and then updated according to the \nour dataset-GloVe dataset comparison."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_dim = 300\nembedded_index = dict()\n\nwith open('../input/glove42b300dtxt/glove.42B.300d.txt', 'r', encoding='utf-8') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embedded_index[word] = vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove.close","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_matrix = np.zeros((max_vocab_size, embedded_dim))\nfor x, i in word_index.items():\n    vector = embedded_index.get(x)\n    if vector is not None:\n        embedded_matrix[i] = vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN for NLP task"},{"metadata":{},"cell_type":"markdown","source":"As words and their sequence are important for NLP solutions, pixels and their order are also essential and something valubale to keep in mind while training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_vocab_size, 300, input_length=input_length, weights=[embedded_matrix], trainable=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Conv1D(filters=32, kernel_size=8, activation='selu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='selu'))\nmodel.add(Dense(8, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc(y_true, y_pred):\n    return np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc(test_labels, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Thanks for reading it to the end. Credits to the OpenSourceCommunity"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}