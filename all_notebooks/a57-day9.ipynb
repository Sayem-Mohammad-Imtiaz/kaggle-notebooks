{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#using sklearn module\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/apndcts/apndcts.csv\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nX = df.iloc[:,0:7]\ny = df.iloc[:,7]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\nmodel = DecisionTreeClassifier(criterion = 'entropy', random_state = 68, max_depth = 3, min_samples_leaf = 5)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Predicted array is:\")\nprint(y_pred)\nprint(\"Accuracy score of our model is:\",(accuracy_score(y_pred, y_test)*100).round(3),'%')\nprint(\"F1 score of our model is:\",(f1_score(y_pred, y_test)*100).round(3),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#doing from scratch\nimport pandas as pd\nimport numpy as np\nimport math as m\nfrom sklearn.model_selection import train_test_split\n\n#reading the file into pandas dataframe\ndf = pd.read_csv(\"../input/apndcts/apndcts.csv\")\n\n#function to split a dataset in two parts\ndef group_split(index, value, dataset):\n    left, right = list(), list()\n    for row in dataset:\n        if row[index] <= value:\n            left.append(row)\n        else:\n            right.append(row)\n    return left, right\n\n#finding information gain by entropy over a split of dataset\ndef information_gain(groups): #i should have used gini instead of entropy but...\n    entropy_value = []\n    for group in groups:\n        size = float(len(group))\n        if size == 0:\n            p = 0.5\n        else:\n            one = 0\n            for row in group:\n                if(row[7] == 1.0):\n                    one += 1\n            p = one/size\n        if (p == 0 or p == 1):\n            entropy = 0\n        else:\n            entropy = -(p*m.log2(p)+(1-p)*m.log2(1-p)) #using log is troublesome\n        entropy_value.append(round(entropy,3)) #entropy for both left and right group\n    one = 0\n    s1, s2 = len(groups[0]), len(groups[1])\n    for group in groups:\n        for row in group:\n            if(row[-1] == 1): one += 1\n    p = one/(s1+s2)\n    entropy_set = -(p*m.log2(p)+(1-p)*m.log2(1-p)) #entropy for whole dataset\n    ig = entropy_set - (s1*entropy_value[0]+s2*entropy_value[1])/(s1+s2) #information gain on split\n    return round(ig,5)\n\n#finding best possible split in a dataset based on information gain\ndef database_split(ds):\n    b_in, b_value, b_ig, b_groups = 0, 0, 0, None #variables to return the best split\n    for index in range(len(ds[0])-1):\n        for row in ds:\n            groups = group_split(index, row[index], ds) #spliting in groups\n            inform_gain = information_gain(groups) #caluculating information gain\n            if inform_gain > b_ig: #getting biggest information gain possible\n                b_ig, b_in, b_value, b_groups = inform_gain, (index+1), row[index].round(3), groups\n    #returning in form of dict to use less variables  and easy access\n    return {'index':b_in, 'value':b_value, 'groups':b_groups}\n\n#converting a node to a leaf node and declaring its value\ndef make_leaf(group):\n    targets = [row[-1] for row in group] #getting all values for rows in group\n    return max(set(targets), key=targets.count) #finding maximum occured value and assigning it to test\n\n#recursively creating nodes until pruning conditions are met\ndef create_nodes(node, max_depth, min_size, depth):\n    left, right = node['groups']\n    del(node['groups'])\n    #if any group is empty\n    if not left or not right:\n        node['left'] = node['right'] = make_leaf(left+right)\n        return\n    #if depth of node is reached to maximum\n    if depth >= max_depth:\n        node['left'], node['right'] = make_leaf(left), make_leaf(right)\n        return\n    #processing left group\n    if len(left) <= min_size:\n        #if group size limit is reached\n        node['left'] = make_leaf(left)\n    else:\n        #create new children nodes for this node\n        node['left'] = database_split(left)\n        create_nodes(node['left'], max_depth, min_size, depth+1)\n    #processing right group\n    if len(right) <= min_size:\n        #if group size limit is reached\n        node['right'] = make_leaf(right)\n    else:\n        #create new children nodes for this node\n        node['right'] = database_split(right)\n        create_nodes(node['right'], max_depth, min_size, depth+1)\n    \n#finding the best first split and then processing each group for further split creating a tree of nodes\ndef build_tree(ds_train, max_depth, min_size):\n    root = database_split(ds_train) #creating the first split at root\n    create_nodes(root, max_depth, min_size, 1) #building rest of the tree\n    return root #returning root for access\n\n#predicting for a row based on the training dataset\ndef predict(node, row):\n    if row[node['index']] < node['value']:\n        if isinstance(node['left'], dict):#to check if current node is a children node or not\n            return predict(node['left'], row) #if a children node we go further\n        else:\n            return node['left']#if a leaf node we take the leaf value\n    else: #same with right side\n        if isinstance(node['right'], dict):\n            return predict(node['right'], row)\n        else:\n            return node['right']\n\n#finding scores to evaluate a given predicted array or to score the model\ndef check_score(list1, list2):\n    TP = 0 #true positive\n    for i in range(list2.size):\n        if((list1[i]==list2[i])and(list1[i]==1)):\n            TP += 1\n\n    FP = np.sum(list1) - TP #false positive\n    TN = (list1==list2).sum() - TP #true negative\n    FN = list2.size - (TP + FP + TN) #false negative\n\n    accuracy = (TP+TN)/(TP+TN+FP+FN)\n    precision = TP/(TP+FP)\n    recall = TP/(TP+FN)\n    f1_score = 2*precision*recall/(precision+recall)\n\n    #returning a dict so dont have to use multiple variables\n    return {'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1_score':f1_score}\n\n#spliting the whole dataset into training and testing group\ndf_train, df_test = train_test_split(df, test_size=0.25, random_state=100)\n\n#creating a list of outcomes for test dataset to evaluate the model later\ny_test = df_test.iloc[:,7].to_numpy()\n\n#converting each dataset to numpy 2d arrays for simple iteration\nnp_df_train = df_train.to_numpy()\nnp_df_test = df_test.to_numpy()\n\nmax_depth = 2\nmin_size = 5\n#building the model\ntree = build_tree(np_df_train, max_depth, min_size)\ny_pred = [] #creating the prediction list\n#testing each row of test dataset in the build model and storing the predicted outcome\nfor row in np_df_test:\n    output = predict(tree, row)\n    y_pred.append(int(output))\n\n#here is your prediction\nprint(\"predicted array is:\")\nprint(y_pred)\n\n#here is your evaluation of model\nscore = check_score(y_pred, y_test)\n\n#here is outcome\nprint('accuracy of our model is:',score['accuracy']*100,'%')\nprint('f1_score of our model is:',score['f1_score']*100,'%')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}