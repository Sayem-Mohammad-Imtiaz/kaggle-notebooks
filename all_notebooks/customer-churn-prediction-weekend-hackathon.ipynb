{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"Insurance companies around the world operate in a very competitive environment. With various aspects of data collected from millions of customers, it is painstakingly hard to analyze and understand the reason for a customer’s decision to switch to a different insurance provider.\n\nFor an industry where customer acquisition and retention are equally important, and the former being a more expensive process, insurance companies rely on data to understand customer behavior to prevent retention. Thus knowing whether a customer is possibly going to switch beforehand gives Insurance companies an opportunity to come up with strategies to prevent it from actually happening.\n\nGiven are 16 distinguishing factors that can help in understanding the customer churn, your objective as a data scientist is to build a Machine Learning model that can predict whether the insurance company will lose a customer or not using these factors.\n\nYou are provided with 16 anonymized factors (feature_0 to feature 15) that influence the churn of customers in the insurance industry\n"},{"metadata":{},"cell_type":"markdown","source":"# Phase1: Model Building On Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step1: Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/Train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step2: Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Below are the steps involved to understand, clean and prepare your data for building your predictive model:\n\n1. Variable Identification\n2. Univariate Analysis\n3. Bi-variate Analysis\n4. Missing values treatment\n5. Outlier treatment\n6. Variable transformation\n7. Variable creation"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Missing Data Analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Data Type Analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let’s look at these methods and statistical measures for categorical and continuous variables individually:\n\n<b> Continuous Variables:- </b> In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics such as Histogram and Bar plots: "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 Box Plot of CONTINUOUS variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ntrain.boxplot(column=['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n                       'feature_5', 'feature_6', 'feature_7'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\ntrain.boxplot(column=['feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', \n                                   'feature_13', 'feature_14', 'feature_15'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> From the plots we can see that, there are lots of outliers in each varibale. </b>"},{"metadata":{},"cell_type":"markdown","source":"### 2.3.2 Plot for Continuous variables"},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3 Histogram Plots Of Continuous Variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,10))\nclr=['red','blue','green','pink','lime','orange','yellow','violet','indigo','teal','red','blue','green','pink','lime','orange']\nfor i,j in zip(range(1,17),train.columns[:-1]):\n    plt.subplot(4,4,i)\n    train[j].hist(color = clr[i-1], label=j)\n    plt.legend()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.4 Density Plots Of Continuous Variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15']].plot(kind='density', subplots=True, \n                                                    layout=(4,4), sharex=False,\n                                                    sharey=False, figsize=(14,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.5 Target Variable Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.labels.value_counts().plot(kind='bar', colors=['green', 'orange'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.4 Bi-variate Analysis"},{"metadata":{},"cell_type":"markdown","source":"Bi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process."},{"metadata":{},"cell_type":"markdown","source":"### 2.4.1 Correlation Matrix Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4.2 Scatterplot Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"ticks\")\n\nsns.pairplot(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step4: Separating X and Y"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Re-setting Index Before Splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Split Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.drop(['labels'], axis=1)\ny = train['labels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step5: Creating Train and Test Set In Ratio 80:20"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step6: Model Building"},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Identification Of Best Features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# feature extraction\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 3)\nfit = rfe.fit(x, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n\n\ndf_feat = pd.DataFrame(fit.ranking_, x.columns)\ndf_feat.rename(columns = {0:\"Feature_Ranking\"}, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat.sort_values(by=\"Feature_Ranking\").plot(kind='bar', figsize=(18,7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Importing and Model Fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.metrics import classification_report\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2.1 Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n#making the instance\nmodel= DecisionTreeClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n          'random_state':[123]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_dt = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_dt.predict(X_test)\n\nprint(\"*******************ACCURACY***************************************************************\")\n#Check Prediction Score\nprint(\"Accuracy of Decision Trees: \",accuracy_score(y_test, predictions))\n\nprint(\"*******************CLASSIFICATION - REPORT***************************************************************\")\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2.2 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_rf.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of Random Forest: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2.3 XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgbmodel=xgb.XGBClassifier(learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\n# Fit on data\nbest_clf_xgb = xgbmodel.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_xgb.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of XGBoost: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(20, 3), max_iter=150, alpha=1e-4,\n                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n                    learning_rate_init=.1)\n\nmlp.fit(X_train,y_train)\nprint(\"Training set score: %f\" % mlp.score(X_train,y_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict\npredictions = mlp.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of MLP: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Light GBM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm_c = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\n\n# Fit on data\nbest_clf_lgbm = lgbm_c.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_lgbm.predict(X_test)\n\nprint(\"*******************ACCURACY***************************************************************\")\n#Check Prediction Score\nprint(\"Accuracy of LGBM: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phase2: Applying Model On Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/Test.csv')\n\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_for_prediction = test[['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprediction_from_dt  = best_clf_dt.predict(test_for_prediction)\ndf_prediction_from_dt = pd.DataFrame({'labels': prediction_from_dt})\ndf_prediction_from_dt.to_excel(\"Final_output_prediction_from_dt.xlsx\")\n\nprediction_from_rf  = best_clf_rf.predict(test_for_prediction)\ndf_prediction_from_rf = pd.DataFrame({'labels': prediction_from_rf})\ndf_prediction_from_rf.to_excel(\"Final_output_prediction_from_rf.xlsx\")\n\nprediction_from_xgb  = best_clf_rf.predict(test_for_prediction)\nprediction_from_xgb = pd.DataFrame({'labels': prediction_from_xgb})\nprediction_from_xgb.to_excel(\"Final_output_prediction_from_xgb.xlsx\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_prediction(model_name, model, test_file):    \n    prediction_file_name = \"Final_output_prediction_from_\" + model_name +\".xlsx\"\n    prediction_from_model  = model.predict(test_file)\n    prediction_from_model = pd.DataFrame({'labels': prediction_from_model})\n    prediction_from_model.to_excel(prediction_file_name)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_prediction(\"lgbm\", best_clf_lgbm, test_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_prediction(\"mlp\", mlp, test_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":4}