{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Import Libraries <a class = \"anchor\" id = \"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np #linear algebra\nimport pandas as pd #data processing\n\n#import libraries for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# importing data\ndf = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"markdown","source":"## 2. Exploratory data analysis<a class = \"anchor\" id = \"2\"></a>\n* As we have imported the data.\n* Now, its time to explore the data to get insigths about it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#preview the dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dimentions of dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viewing column names\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop RISK_MM variable (it is given in the description to drop the feature)\ndf.drop([\"RISK_MM\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viewing the summary of dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view statistical properties of dataset\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.Univariate Analysis<a class = \"anchor\" id = \"3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explore \"RainTomorrow\" target variable\n#check for missing values\ndf[\"RainTomorrow\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view number of unique values\ndf[\"RainTomorrow\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view the unique values\ndf[\"RainTomorrow\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view the frequency distribution of values\ndf['RainTomorrow'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view percentage of frequency distribution of values\ndf[\"RainTomorrow\"].value_counts()/len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Findings of Univariate Analysis\n* The number of unique values in \"RainTomorrow\" is 2 ie \"Yes\" or \"No\".\n* Out of total number of \"RainTomorrow\" values, No appears 77.58% times and Yes appears 22.42% times."},{"metadata":{},"cell_type":"markdown","source":"## 6.Bivariate Analysis<a class = \"anchor\" id = \"6\"></a>\nIn this section we explore two categories : Categorial Variables and Numerical Variables. "},{"metadata":{},"cell_type":"markdown","source":"### Exploring Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#find categorical values\n\ncategorical = [var for var in df.columns if df[var].dtype=='O']\nprint(\"There are {} categorical values\\n\".format(len(categorical)))\nprint(\"The categorical variavles are : \", categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view categorical variables\ndf[categorical].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check missing values in categorical variables\ndf[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view frequency count of categorical variables\nfor var in categorical:\n    print(df[var].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for cardinality in categorical variables\nfor var in categorical:\n    print(var, \" contains \",len(df[var].unique()), \" labels\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Date variable needs to be preprocessed as it has **High cardinality**. All the other variables contain relatively smaller number of variables.\n\n**Feature Engineering of Date variable.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'].dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the data type of Date variable is object. I will parse the Date currently coded as object into datetime format."},{"metadata":{"trusted":true},"cell_type":"code","source":"#parse the dates, currently coded as strings, into datetime format\ndf[\"Date\"] = pd.to_datetime(df['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract year from date\ndf['Year'] = df['Date'].dt.year\ndf['Year'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract month from date\ndf['Month'] = df['Date'].dt.month\ndf['Month'].head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract day from date\ndf['Day'] = df['Date'].dt.day\ndf['Day'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#again viewing the summary of the dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As there are three additional columns from Date variable, I will drop the original Date variable.\ndf.drop('Date', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find categorical values\ncategorical = [var for var in df.columns if df[var].dtype=='O']\ndf[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore `Location` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print number of labels in Location variable\nprint('Location contains', len(df.Location.unique()), 'labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check labels in location variable\ndf.Location.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check frequency distribution of values in Location variabe\ndf.Location.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do One Hot Encoding of Location variable\n# get k-1 dummy variables after One Hot Encoding \n# preview the dataset with head() method\n\npd.get_dummies(df.Location, drop_first = True).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore `WindGustDir` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print number of labels in WindGustDir variable\nprint('WindGustDir contains',len(df.WindGustDir.unique()),'labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check labels in WindGustDir variable\ndf['WindGustDir'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check frequency distribution of values in WindGustDir variable\ndf.WindGustDir.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do One Hot Encoding of WindGustDir variable\n# get k-1 dummy variables after One Hot Encoding \n# also add an additional dummy variable to indicate there was missing data\n# preview the dataset with head() method\npd.get_dummies(df.WindGustDir, drop_first = True, dummy_na = True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum the number of 1s per boolean variable over the rows of the dataset\n# it will tell us how many observations we have for each category\npd.get_dummies(df.WindGustDir, drop_first = True, dummy_na = True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore `WindDir9am` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check number of labels in WindDir9am variable\nprint('WindDir9am contains', len(df.WindDir9am.unique()),'labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check lables in WindDir9am variable\ndf['WindDir9am'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check frequency distribution of values in WindDir9am variable\ndf['WindDir9am'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do One Hot Encoding of WindDir9am variable\n# get k-1 dummy variables after One Hot Encoding \n# also add an additional dummy variable to indicate there was missing data\n# preview the dataset with head() method\n\npd.get_dummies(df.WindDir9am, drop_first=True, dummy_na=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum the number of 1s per boolean variable over the rows of the dataset\n# it will tell us how many observations we have for each category\n\npd.get_dummies(df.WindDir9am, drop_first=True, dummy_na=True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore `WindDir3pm` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print number of lables in WindDir3pm variable\nprint('WindDir3pm contains',len(df.WindDir3pm.unique()),'labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check labels in WindDir3pm variable\ndf['WindDir3pm'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for frequency distribution of values in WindDir3pm variable\ndf['WindDir3pm'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do One Hot Encoding of WindDir3pm variable\n# get k-1 dummy variables after One Hot Encoding \n# also add an additional dummy variable to indicate there was missing data\n# preview the dataset with head() method\n\npd.get_dummies(df.WindDir3pm, drop_first=True, dummy_na=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum the number of 1s per boolean variable over the rows of the dataset\n# it will tell us how many observations we have for each category\n\npd.get_dummies(df.WindDir3pm, drop_first=True, dummy_na=True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore `RainToday` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print number of labels in RainToday variable\nprint('RainToday contains', len(df['RainToday'].unique()), 'labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check labels in WindGustDir variable\ndf['RainToday'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check frequency distribution of values in WindGustDir variable\ndf.RainToday.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do One Hot Encoding of RainToday variable\n# get k-1 dummy variables after One Hot Encoding \n# also add an additional dummy variable to indicate there was missing data\n# preview the dataset with head() method\n\npd.get_dummies(df.RainToday, drop_first=True, dummy_na=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum the number of 1s per boolean variable over the rows of the dataset\n# it will tell us how many observations we have for each category\n\npd.get_dummies(df.RainToday, drop_first=True, dummy_na=True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore Numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find numerical variables\nnumerical = [var for var in df.columns if df[var].dtype!='O']\nprint('There are {} numerical variables\\n'.format(len(numerical)))\nprint('The numerical variables are :', numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view the numberical variables\ndf[numerical].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Explore problems within numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check missing values in numerical variable\ndf[numerical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 16 numerical variable containing missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#view summary statistics in numerical variables\nprint(round(df[numerical].describe()),2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On closer inspection, we can see that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns may contain outliers.\n\nLet's draw boxplot to visualize outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nfig = df.boxplot(column='Rainfall')\nfig.set_title('')\nfig.set_label('Rainfall')\n\nplt.subplot(2,2,2)\nfig = df.boxplot(column='Evaporation')\nfig.set_title('')\nfig.set_label('Evaporation')\n\nplt.subplot(2,2,3)\nfig = df.boxplot(column='WindSpeed9am')\nfig.set_title('')\nfig.set_label('WindSpeed9am')\n\nplt.subplot(2,2,4)\nfig = df.boxplot(column='WindSpeed3pm')\nfig.set_title('')\nfig.set_label('WindSpeed3pm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The above boxplot confirms that there are lot of outliers in these variables."},{"metadata":{},"cell_type":"markdown","source":"## Check the distribution of variables\n* Now, I will plot histograms to check distributions to find out if the are normal or skewed.\n* If the variable follows normal distribution, then I will do `Extreme value Analysis` otherwise if they are skewed, i will find IQR(Interquantile range)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot historams to check distribution\nplt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nfig = df.Rainfall.hist(bins=10)\nfig.set_xlabel('Rainfall')\nfig.set_ylabel('RainTomorrow')\n\nplt.subplot(2,2,2)\nfig = df.Evaporation.hist(bins=10)\nfig.set_xlabel('Evaporation')\nfig.set_ylabel('RainTomorrow')\n\nplt.subplot(2,2,3)\nfig = df.WindSpeed9am.hist(bins=10)\nfig.set_xlabel('WindSpeed9am')\nfig.set_ylabel('RainTomorrow')\n\nplt.subplot(2,2,4)\nfig = df.WindSpeed3pm.hist(bins=10)\nfig.set_xlabel('WindSpeed3pm')\nfig.set_ylabel('RainTomorrow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all the four variables are skewed. So, I will use IQR to find outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find outliers for Rainfall variable\n\nIQR = df.Rainfall.quantile(0.75) - df.Rainfall.quantile(0.25)\nlower_fence = df.Rainfall.quantile(0.25) - (IQR * 3)\nupper_fence = df.Rainfall.quantile(0.75) + (IQR * 3)\n\nprint('Rainfall outlier values are < {} or > {}'.format(lower_fence, upper_fence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For `Rainfall`, the minimum and maximum values are 0.0 and 371.0. So, the outliers are values > 3.2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find outliers for Evaporation variable\n\nIQR = df.Evaporation.quantile(0.75) - df.Evaporation.quantile(0.25)\nlower_fence = df.Evaporation.quantile(0.25) - (IQR * 3)\nupper_fence = df.Evaporation.quantile(0.75) + (IQR * 3)\n\nprint('Evaporation outliers values are < {} or > {}'.format(lower_fence, upper_fence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For `Evaporation`, the minimum and maximum values are 0.0 and 145.0. so, the outliers are values > 21.8"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find outliers for WindSpeed9am variable\n\nIQR = df.WindSpeed9am.quantile(0.75) - df.WindSpeed9am.quantile(0.25)\nlower_fence = df.WindSpeed9am.quantile(0.25) - (IQR * 3)\nupper_fence = df.WindSpeed9am.quantile(0.75) + (IQR * 3)\nprint('WindSpeed9am outlier values are < {} or > {}'.format(lower_fence, upper_fence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For `WindSpeed9am`, the minimum and maximum values are 0.0 and 130.0. so, the outiers are values > 55.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find outliers for WindSpeed3pm variable\n\nIQR = df.WindSpeed3pm.quantile(0.75) - df.WindSpeed3pm.quantile(0.25)\nlower_fence = df.WindSpeed3pm.quantile(0.25) - (IQR * 3)\nupper_fence = df.WindSpeed3pm.quantile(0.75) + (IQR * 3)\nprint('WindSpeed3pm outliers values are < {} or > {}'.format(lower_fence, upper_fence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For `WindSpeed3pm`, the minimum and maximum values are 0.0 and 87.0. so, the outliers are values > 57.0"},{"metadata":{},"cell_type":"markdown","source":"## 7. Multivariate Analysis\n* An important spet in EDA is to discover patterns and relationships between variables in the dataset.\n* I will use heat map and pair plt to discover the patterns and relationsip in the dataset.\n* First of all, I will draw a heat map."},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HeatMap"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 12))\nplt.title('Correlation Heatmap of Rain in Australia Dataset')\nax = sns.heatmap(correlation, square=True, annot=True, fmt='.2f',linecolor='white')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Interpretation** : \nFrom the above correlation map, we can conclude that the variables which are highly positively correlated are - \n* `MinTemp` and `MaxTemp` (cc = 0.74)\n* `MinTemp` and `Temp3pm` (cc = 0.71)\n* `MinTemp` and `Temp9am` (cc = 0.90)\n* `MaxTemp` and `Temp9am` (cc = 0.89)\n* `MaxTemp` and `Temp3pm` (cc = 0.98)\n* `WindGustSpeed` and `WindSpeed3pm` (cc = 0.69)\n* `Pressure9am` and `Pressure3pm` (cc = 0.96)\n* `Temp9am` and `Temp3pm` (cc = 0.86)"},{"metadata":{},"cell_type":"markdown","source":"### Pair Plot\nFirst of all, I will define extract the variables which are highly positivety correlated."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_var = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'WindGustSpeed', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(df[num_var], kind='scatter', diag_kind='hist', palette='Rainbow')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Interpretation** \n* I have defined a variable num_var which consists of `MinTemp`, `MaxTemp`, `Temp9am`, `Temp3pm`, `WindGustSpeed`, `WindSpeed3pm`, `Pressure9am` and `Pressure3pm` variables."},{"metadata":{},"cell_type":"markdown","source":"## 8. Declare feature vector and target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['RainTomorrow'], axis=1)\ny = df['RainTomorrow']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Split data into seprate training and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Feature Engineering\n**Feature Engineering** is the process of transforming raw data into useful features that helps us to understand our model better and increase its predictive power. I will carry out our feature engineering on different types of variables.\nFirst, I will display the categorical and numberical variables again separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data types in x_train\nX_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display categorical variables\ncategorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\ncategorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display numerical variables\nnumerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\nnumerical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Engineering missing values in numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[numerical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[numerical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print percentage of missing values in the numerical variables in train set\nfor col in numerical:\n    if X_train[col].isnull().mean()>0:\n        print(col, round(X_train[col].isnull().mean(), 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Assumption**\n> I assume that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers.\n\n> I will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# inpute missing values in X_train and X_test with respective column meadian in X_train\nfor df1 in [X_train, X_test]:\n    for col in numerical:\n        col_median = X_train[col].median()\n        df1[col].fillna(col_median, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[numerical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[numerical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can see that there are no missing values in the numerical columns of training and test set."},{"metadata":{},"cell_type":"markdown","source":"### Engineering missing values in categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print percentage of missing values in the categorical variables in training set\nX_train[categorical].isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inpute missing categorical variables with most frequent value\nfor df2 in [X_train, X_test]:\n    df2['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace=True)\n    df2['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace=True)\n    df2['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace=True)\n    df2['RainToday'].fillna(X_train['RainToday'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that are no missing values in X_train and X_test."},{"metadata":{},"cell_type":"markdown","source":"### Engineering outliers in numerical variables\n> We have seen that the `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns contain outliers. I will use top-coding approach to cap maximum values and remove outliers from the above variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_value(df3, variable, top):\n    return np.where(df3[variable]>top, top, df3[variable])\n\nfor df3 in [X_train, X_test]:\n    df3['Rainfall'] = max_value(df3, 'Rainfall', 3.2)\n    df3['Evaporation'] = max_value(df3, 'Evaporation', 21.8)\n    df3['WindSpeed9am'] = max_value(df3, 'WindSpeed9am', 55)\n    df3['WindSpeed3pm'] = max_value(df3, 'WindSpeed3pm', 57)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.Rainfall.max(), X_test.Rainfall.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.Evaporation.max(), X_test.Evaporation.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.WindSpeed9am.max(), X_test.WindSpeed9am.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.WindSpeed3pm.max(), X_test.WindSpeed3pm.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[numerical].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now see that the outliers in `Rainfall`, `Evaporation`, `WindSpeed9am` and `WindSpeed3pm` columns are capped."},{"metadata":{},"cell_type":"markdown","source":"### Encode categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print categorical variables\ncategorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[categorical].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode RainToday variable\nimport category_encoders as ce\nencoder = ce.BinaryEncoder(cols=['RainToday'])\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train[numerical], X_train[['RainToday_0','RainToday_1']],pd.get_dummies(X_train.Location), pd.get_dummies(X_train.WindGustDir), pd.get_dummies(X_train.WindDir9am), pd.get_dummies(X_train.WindDir3pm)],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.concat([X_test[numerical], X_test[['RainToday_0','RainToday_1']],pd.get_dummies(X_test.Location), pd.get_dummies(X_test.WindGustDir), pd.get_dummies(X_test.WindDir9am), pd.get_dummies(X_test.WindDir3pm)],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same sacle. It is call `Feature Scaling`. I will do it as follows:"},{"metadata":{},"cell_type":"markdown","source":"## 11. Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(X_train, columns=[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.DataFrame(X_test, columns=[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have `X_train` dataset ready to be fed into the Logistic Regression classifier. I will do it as follows:"},{"metadata":{},"cell_type":"markdown","source":"## 12. Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}