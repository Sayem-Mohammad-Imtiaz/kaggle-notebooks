{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multi-Class Text Classification with Turkish Dataset\n\n<h3>In this kernel, I will try to classify \"comments\"(text) with \"categories\"(text) using CNN</h3>\n<p style=\"font-size:20px\">Table of Content</p>\n\n* [Data Overview](#1)\n* [Word Overview](#2)\n* [NLP Processing](#3)\n* [ANN Building and Fitting](#4)\n* [CNN Building and Fitting](#5)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport keras\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(\"Added shopping_cart.png for wordcloud\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Overview** <a id=\"1\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/multiclass-classification-data-for-turkish-tc32/ticaret-yorum.csv')\npd.set_option('max_colwidth', 500)\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let me explain categories:\n* Kamu Hizmetleri: Public Service\n* Finans: Finance\n* Cep Telefon Kategori: Mobile Phone Category\n* Enerji: Energy\n* Ulaşım: Transportation\n* Kargo-Nakliyat: Cargo-Shipping\n* Medya: Media\n* Mutfak Araç-Gereç: Kitchen Tools\n* Alışveriş: Shopping\n* Mekan ve Eğlence: Venue and Entertainment\n* Elektronik: Electronic\n* Beyaz Eşya: Home Appliance\n* Küçük Ev Aletleri: Small Appliances\n* İnternet: Internet\n* Giyim: Clothing\n* Etkinlik ve Organizasyon: Event and Organization\n* İçecek: Beverage\n* Sağlık: Medical\n* Sigortacılık: Insurance\n* Spor: Sport\n* Mobilya-Ev Tekstili: Furniture-Home Textile\n* Otomotiv: Automotive\n* Turizm: Tourism\n* Eğitim: Education\n* Gıda: Food\n* Temizlik: Cleaning\n* Hizmet Sektörü: Service Industry\n* Mücevher-Saat-Gözlük: Jewel-Watch-Glasses\n* Bilgisayar: Computer\n* Kişisel Bakım ve Kozmetik: Personal Care and Cosmetics\n* Anne-Bebek: Mother-Baby\n* Emlak ve İnşaat: Real Estate and Construction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>So data contains 431306 lines and 32 unique categories and it seems there is no NaN, right?</h2>\n<h3>But why text label has 4075 (431306-427231) non-unique comments? And we have a \"top\" comment starts with \"Cinemaximum 4 Aydır\" so let's check what it is  </h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\ncinemaximum4aydir = \"Cinemaximum 4 Aydır Bilet Paralarını Bir Türlü İade Etmiyor,\"\nfor text in data.text:\n    if cinemaximum4aydir in text[:len(cinemaximum4aydir)]:\n        count += 1\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So they really are duplicated...\n\nWe don't need to but I want to get rid of every non-unique comment so I will use pandas' 2 functions named \"duplicated\" and \"drop_duplicates\"\n\n\"duplicated\" gets keep={‘first’, ‘last’, False} parameter and can be used both \"pandas.DataFrame\" and \"pandas.Series\" objects and returns \"pandas.Series\" object with True-False statements \n\n\"drop_duplicates\" works like \"dropna\" so it gets inplace={True,False} and subset={column label} with keep={‘first’,‘last’ False}\nThere is also another parameter named ignore_index={True,False} let me explain this\nIn default ignore_index=False it means if it remove a duplicated, index will not change so 1-a,2-a,3-b will be 1-a,3-b and it breaks for loop with len(data) because of that we need to change ignore_index\n\n\nIn default keep =\"first\"\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"exampleArray = np.array([[1,1],[1,2],[4,5]])\nexampleFrame = pd.DataFrame(exampleArray,columns=[\"ex1\",\"ex2\"])\nexampleFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ex1 = exampleFrame.drop_duplicates(subset=\"ex1\",keep=\"first\")\nprint(\"Without ignore_index\")\nprint(ex1)\nex2 = exampleFrame.drop_duplicates(subset=\"ex1\",keep=\"first\",ignore_index=True)\nprint(\"With ignore_index\")\nprint(ex2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.text.duplicated(keep=\"first\").value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop_duplicates(subset=\"text\",keep=\"first\",inplace=True,ignore_index=True)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Now we can visualize Category</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.express as px\nimport plotly.io as pio\n\ninit_notebook_mode(True)\n\nfig = px.bar(x=data.category.value_counts().index,y=data.category.value_counts(),color=data.category.value_counts().index,text=data.category.value_counts())\nfig.update_traces(hovertemplate=\"Category:'%{x}' Counted: %{y}\")\nfig.update_layout(title={\"text\":\"Category Counts\",\"x\":0.5,\"font\":{\"size\":35}},xaxis={\"title\":\"Category\",\"showgrid\":False},yaxis={\"title\":\"Value\",\"showgrid\":False},plot_bgcolor=\"white\",width=800,height=500,showlegend=False)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = px.pie(data,values=data.category.value_counts(),names=data.category.value_counts().index)\nfig1.update_traces(textposition='auto', textinfo='percent+label',marker={\"line\":{\"width\":1}},hoverinfo='label+percent',hole=0.4)\nfig1.update_layout(annotations=[{\"text\":\"Percentages\",\"showarrow\":False,\"font_size\":17}])\niplot(fig1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Word Overview** <a id=\"2\"></a>\n<h3> Let's start with word list</h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nwordList = list()\nfor i in range(len(data)):\n    temp = data.text[i].split()\n    for k in temp:\n        k = re.sub(\"[^a-zA-ZğĞüÜşŞıİöÖçÇ]\",\"\",k)\n        if k != \"\":\n            wordList.append(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nwordCount = Counter(wordList)\ncountedWordDict = dict(wordCount)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\n\nprint(\"Most Used 20 Words\")\nfor word,counted in sortedWordDict[0:20]:\n    print(\"{} : {}\".format(word,counted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>So they are most used 20 words... </h2>\n<h3>But why is \"oku\" -\"read\" in english- used 386776 times even if there are 427231\ttexts? Let's see...</h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data[\"text\"][7:10]:\n    if \"oku\" in i:\n        print(i)\n        print(\"*\"*20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>As we can see 'oku' and 'oku\"' used with \"Devamını\" -\"more\" in english-</h2>\n<h3>We know this dataset contains \"comments\" so \"... Devamını oku\" is used to see rest of the comment. It means we can clear it too for better visualization!</h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dontReadMore(text):\n    temptext = text.split(\".\")\n    if \"Devamını\" in temptext[-1]:\n        text = temptext[:-1]\n    return \"\".join(text)\n\ndata[\"text\"] = data[\"text\"].apply(dontReadMore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data[\"text\"][200:500]:\n    if \"oku\" in i:\n        print(i)\n        print(\"*\"*20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Now we can visualize with REAL most used words </h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wordList = list()\nfor i in range(len(data)):\n    temp = data.text[i].split()\n    for k in temp:\n        k = re.sub(\"[^a-zA-ZğĞüÜşŞıİöÖçÇ]\",\"\",k)\n        if k != \"\":\n            wordList.append(k)\nwordCount = Counter(wordList)\ncountedWordDict = dict(wordCount)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\nprint(\"REAL Most Used 20 Words\")\nfor word,counted in sortedWordDict[0:20]:\n    print(\"{} : {}\".format(word,counted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num = 75 # For using most used 75 words\nlist1 = list()\nlist2 = list()\nfor i in range(num):\n    list1.append(wordCount.most_common(num)[i][0])\n    list2.append(wordCount.most_common(num)[i][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = px.bar(x=list1,y=list2,color=list2,hover_name=list1,hover_data={'Word':list1,\"Count\":list2})\nfig2.update_traces(hovertemplate=\"Word:'%{x}' Value: %{y}\")\nfig2.update_layout(title={\"text\":\"Word Values\",\"x\":0.5,\"font\":{\"size\":30}},xaxis={\"title\":\"Words\",\"showgrid\":False},yaxis={\"title\":\"Value\",\"showgrid\":False},plot_bgcolor=\"white\")\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\nshopping_cart = np.array(Image.open(\"/kaggle/input/shopping-cart/shopping_cart.png\"))\nplt.imshow(shopping_cart)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\n\ndef grey_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n    return(\"hsl(0,0%%, %d%%)\" % np.random.randint(50,55))\n\nstopwordCloud = set(stopwords.words(\"turkish\"))\n\nwordcloud = WordCloud(stopwords=stopwordCloud,max_words=1000,background_color=\"white\",min_font_size=3,mask=shopping_cart).generate_from_frequencies(countedWordDict)\nwordcloud.recolor(color_func = grey_color_func)\nplt.figure(figsize=[13,10])\nplt.axis(\"off\")\nplt.title(\"Word Cloud\",fontsize=20)\nplt.imshow(wordcloud)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **NLP Processing** <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\nimport time\n\nps = PorterStemmer()\nstopwordSet = set(stopwords.words('turkish'))\n\nt = time.time()\n\ndef leadMyWord(text):\n    text = re.sub('[^a-zA-ZğĞüÜşŞıİöÖçÇ]',\" \",text)\n    text = text.lower()\n    text = word_tokenize(text,language='turkish')\n    text = [word for word in text if not word in stopwordSet]\n    text = \" \".join(text)\n    return text   \n\ntextList = data.text.apply(leadMyWord)\ntextList = list(textList)\n\nprint(\"Before\")\nprint(data[\"text\"][2])\nprint(\"After\")\nprint(textList[2])\nprint(\"Time Passed\")\nprint(time.time()-t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparing y\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\nle = LabelEncoder()\nlabelEncode = le.fit_transform(data[\"category\"])\nprint(\"LabelEncode\")\nprint(labelEncode)\ncategorical_y = to_categorical(labelEncode)\nprint(\"To_Categorical\")\nprint(categorical_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **ANN Building and Fitting** <a id=\"4\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n#preparing x for ANN\ntfidv = TfidfVectorizer(max_features=20001)\nx = tfidv.fit_transform(textList)\nx.sort_indices()\n\nx_train,x_test,y_train,y_test = train_test_split(x,categorical_y,test_size=0.33,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom keras.losses import categorical_crossentropy\n\ndef build_ann_model():\n    model = Sequential()\n    \n    model.add(Dense(units=1024,activation=\"relu\",input_dim=x_train.shape[1]))\n    model.add(Dense(units=512,activation=\"relu\"))\n    model.add(Dense(units=256,activation=\"relu\"))\n    model.add(Dense(units=y_train.shape[1],activation=\"softmax\"))\n    \n    optimizer = Adam(lr=0.000015,beta_1=0.9,beta_2=0.999)\n    \n    model.compile(optimizer=optimizer,metrics=[\"accuracy\"],loss=categorical_crossentropy)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model = build_ann_model()\nplot_model(ann_model,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_history = ann_model.fit(x_train,y_train,epochs=10,batch_size=256,shuffle=True)\nypred = ann_model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\n\nann_accuracy = accuracy_score(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n#print(\"ANN Accuracy:\",ann_accuracy)\nann_cn = confusion_matrix(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\nplt.subplots(figsize=(18,14))\nsns.heatmap(ann_cn,annot=True,fmt=\"1d\",cbar=False,xticklabels=le.classes_,yticklabels=le.classes_)\nplt.title(\"ANN Accuracy: {}\".format(ann_accuracy),fontsize=50)\nplt.xlabel(\"Predicted\",fontsize=15)\nplt.ylabel(\"Actual\",fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxe1[0].plot(ann_history.history[\"accuracy\"],label=\"accuracy\",color=\"blue\")\naxe1[1].plot(ann_history.history[\"loss\"],label=\"loss\",color=\"red\")\naxe1[0].title.set_text(\"ANN Accuracy\")\naxe1[1].title.set_text(\"ANN Loss\")\naxe1[0].set_xlabel(\"Epoch\")\naxe1[1].set_xlabel(\"Epoch\")\naxe1[0].set_ylabel(\"Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **CNN Building and Fitting** <a id=\"5\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before starting, I'm very grateful to Raj Mehrotra for sharing [\"A Detailed Explanation of Keras Embedding Layer\"](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\n\n#preparing x for CNN\nMAX_FEATURES = 20001\n\nonehot_corpus = []\nfor text in textList:\n    onehot_corpus.append(one_hot(text,MAX_FEATURES))\n    \nmaxTextLen = 0\nfor text in textList:\n    word_token=word_tokenize(text)\n    if(maxTextLen < len(word_token)):\n        maxTextLen = len(word_token)\n        \nprint(\"Max number of words : \",maxTextLen)\n\npadded_corpus=pad_sequences(onehot_corpus,maxlen=maxTextLen,padding='post')\nx_train2,x_test2,y_train2,y_test2 = train_test_split(padded_corpus,categorical_y,test_size=0.33,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\ndef build_cnn_model():\n    model = Sequential()\n    \n    model.add(Embedding(MAX_FEATURES, 100, input_length=maxTextLen))\n\n\n    model.add(Conv1D(64, 2, padding='same', activation='relu'))\n    model.add(MaxPooling1D(2))\n    #model.add(MaxPooling1D(2))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(units=1024,activation=\"relu\"))\n    model.add(Dense(units=512,activation=\"relu\"))\n    \n    model.add(Dense(units=y_train2.shape[1],activation=\"softmax\"))\n    \n    optimizer = Adam(lr=0.000055,beta_1=0.9,beta_2=0.999)\n    \n    model.compile(optimizer=optimizer,metrics=[\"accuracy\"],loss=categorical_crossentropy)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = build_cnn_model()\nplot_model(cnn_model,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_history = cnn_model.fit(x_train2,y_train2,epochs=10,batch_size=1280,shuffle=True)\nypred2 = cnn_model.predict(x_test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_accuracy = accuracy_score(y_test2.argmax(axis=-1),ypred2.argmax(axis=-1))\n#print(\"CNN Accuracy:\",cnn_accuracy)\ncnn_cn = confusion_matrix(y_test2.argmax(axis=-1),ypred2.argmax(axis=-1))\nplt.subplots(figsize=(18,14))\nsns.heatmap(cnn_cn,annot=True,fmt=\"1d\",cbar=False,xticklabels=le.classes_,yticklabels=le.classes_)\nplt.title(\"CNN Accuracy: {}\".format(cnn_accuracy),fontsize=50)\nplt.xlabel(\"Predicted\",fontsize=15)\nplt.ylabel(\"Actual\",fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxe1[0].plot(cnn_history.history[\"accuracy\"],label=\"accuracy\",color=\"blue\")\naxe1[1].plot(cnn_history.history[\"loss\"],label=\"loss\",color=\"red\")\naxe1[0].title.set_text(\"CNN Accuracy\")\naxe1[1].title.set_text(\"CNN Loss\")\naxe1[0].set_xlabel(\"Epoch\")\naxe1[1].set_xlabel(\"Epoch\")\naxe1[0].set_ylabel(\"Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ann_predict(text):\n    puretext = leadMyWord(text)\n    vector = tfidv.transform([puretext])\n    vector.sort_indices()\n    predicted = ann_model.predict(vector)\n    predicted_category = predicted.argmax(axis=1)\n    return le.classes_[predicted_category]\ndef cnn_predict(text):\n    puretext = leadMyWord(text)\n    onehottext = one_hot(puretext,MAX_FEATURES)\n    text_pad = pad_sequences([onehottext],maxlen=maxTextLen,padding='post')\n    predicted = cnn_model.predict(text_pad)\n    predicted_category = predicted.argmax(axis=1)\n    return le.classes_[predicted_category]\n    \nfor _ in range(10):\n    randint = np.random.randint(len(data))\n    text = data.text[randint]  \n    print(\"  Text\")\n    print(\"-\"*8)\n    print(text)\n    print(\"-\"*20)\n    print(\"Actual Category: {}\".format(data.category[randint]))\n    print(\"ANN Predicted Category: {}\".format(ann_predict(text)[0]))\n    print(\"CNN Predicted Category: {}\".format(cnn_predict(text)[0]))\n    print(\"*\"*50)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let me try it too\ndef predict_print(text):\n    print(\"  Text\")\n    print(\"-\"*8)\n    print(text)\n    print(\"-\"*20)\n    print(\"ANN Predicted Category: {}\".format(ann_predict(text)[0]))\n    print(\"CNN Predicted Category: {}\".format(cnn_predict(text)[0]))\n    print(\"*\"*50)\nmyText = \"Yemeğin içinden kıl çıktı, gitmenizi önermiyorum.\" # hair came out of the dish, I don't suggest you go\npredict_print(myText)\nmyText = \"Tuş bozuk.\" # Key Broken\npredict_print(myText)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 >Thanks for reading, I'm open to your advices.</h1>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}