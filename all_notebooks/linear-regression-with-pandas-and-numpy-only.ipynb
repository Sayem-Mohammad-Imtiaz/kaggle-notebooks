{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Prediction of house prices using multivariate linear regression**\n## *By utitilizing only Pandas and NumPy libraries*\n\n# **1. Introduction**\n\nThis is a classic example for beginners in machine learning. The dataset contains the **house sales for King County, USA, between May 2014 and May 2015**. You can find the full dataset on [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction). Before continuing, it is highly recommended that you click the aforementioned link and get to know the original dataset.\n\nOur aim is to apply multivariate linear regression so as to predict the price of any given house in that area, given a number of its features (e.g. number of bathrooms, year on which the house was built, etc.). For this purpose however, **only NumPy and Pandas libraries are used** (as well as Matplotlib and Seaborn for the visualizations). Although some may find the code to be unnecessarily lengthy and detailed, I believe that it can provide the reader with a good grasp of what is really happening in such models and thus, can have great **educational value** (especially for beginners). The code is largely based on Stanford's Machine Learning course on [Coursera](https://www.coursera.org/learn/machine-learning/) by Andrew Ng and can be very helpful for those who have taken it and wish to see an implementation in Python.\n\nIf you have any questions or feedback, please feel free to contact me. Please do not forget to **$\\color{magenta}{\\text{upvote}}$** if you like this kernel!\n\n# **2. Initial exploration of the dataset**\n\nIn this section, we are going to set our environment up by importing the necessary libraries, as well as loading our data into a Pandas dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Read the house data into a data frame\ndf = pd.read_csv('../input/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can go ahead and see how our dataset looks like, while also getting an idea of its statistical properties. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the first five observations\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe the dataset\ndf.describe().round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that each entry has an id, i.e. an identification number, as well as a date, i.e. the date on which the transaction took place. These two columns are irrelevant to our analysis; so we will go ahead and remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the id and date columns\ndf = df.drop(['id', 'date'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The attentive reader will already have noticed that there are 21613 observations or entries. Let's confirm it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the number of data observations\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, we also need to see how many features there are in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the number of starting features\nlen(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the number of features is small, it would probably be a good idea to try an alternative implementation, using the normal equation method. Nevertheless, we will go ahead and check the data types and whether they have correctly been interpreted."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the types of data\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, it is always a good idea to check for any missing values or \"NaN\" in the dataset. However, our data is very clean."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the number of null data observations\ndf.isnull().values.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3. Feature selection**\n\nIn this section, we are going to use a filter method so as to select which features are going to actually be used by the linear regression algorithm. First of all though, we have to specify which is the target variable we want to predict (price) and which are the features we are going to use (all others)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify target and features\ntarget = df.iloc[:, 0].name\nfeatures = df.iloc[:, 1:].columns.tolist()\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we are going to check the correlations between our target variable (price) and the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlations of features with target variable\ncorrelations = df.corr()\ncorrelations['price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a filter method, we are going to select which features are to be used subsequently. Although this is arbitrary and contestable, we will remove all features with an absolute correlation with the target variable which is *smaller than 0.2*."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlations with target variable\ncor_target = abs(correlations['price'])\n\n# Display features with correlation < 0.2\nremoved_features = cor_target[cor_target < 0.2]\nremoved_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove features with correlation < 0.2\ndf = df.drop(['sqft_lot', 'condition', 'yr_built', 'yr_renovated', 'zipcode', 'long',\n              'sqft_lot15'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Furthermore, we also have to ensure that the remaining features are not very highly correlated with each other, i.e. that they are -more or less- independent variables. Let us plot the **Pearson correlation matrix** using Matplotlib and Seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Pearson correlation matrix\nfig_1 = plt.figure(figsize=(12, 10))\nnew_correlations = df.corr()\nsns.heatmap(new_correlations, annot=True, cmap='Greens', annot_kws={'size': 8})\nplt.title('Pearson Correlation Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above figure, we can see that some features are highly correlated with each other. We will -arbitrarily- search for correlations *above 0.75* and remove the features with the **lowest** correlation with the target variable (price)."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Determine the highest intercorrelations\nhighly_correlated_features = new_correlations[new_correlations > 0.75]\nhighly_correlated_features.fillna('-')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once more, this is contestable and the result of my own interpretation, but I am of the opinion that although there are sets of variables which are indeed highly correlated, they ultimately represent two independent variables and should be kept as they are. I believe this to be the case with the following sets of variables:\n- (\"bathrooms\", \"sqft_living\") \n- (\"grade\", \"sqft_living\")\n- (\"grade\", sqft_above\")\n\nIn contrast, I deem that the variable \"sqft_living\" is not only highly correlated with \"sqft_above\" (0.877) and \"sqft_living15\" (0.756), but that it really represents most of the information we need for the linear regression. Indeed, according to the description of the original dataset found on [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction), \"sqft_above\" is the square footage of the house apart from the basement and \"sqft_living15\" is the square footage of the living room area in 2015 (which implies renovations). It is also useful to note that the variable \"grade\" is given by the King County grading system and refers to the types of materials used and the quality of workmanship (more info can be found [here](https://info.kingcounty.gov/assessor/esales/Glossary.aspx?type=r#g))\n\nThus, I chose to remove two more features (\"sqft_above\" and \"sqft_living15\") and keep \"sqft_living\" which has the highest correlation with the target variable (0.702)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove features which are highly correlated with \"sqft_living\"\ndf = df.drop(['sqft_above', 'sqft_living15'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are ultimately left with **9 features** that will be used for the linear regression. Let us update the features and store their number."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update features and store their length\nfeatures = df.iloc[:, 1:].columns.tolist()\nlen_of_features = len(features)\nlen_of_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **4. Multivariate linear regression**\n\nIn this section, we are going to implement our linear regression algorithm. At the very beggining, we are going to **normalize the features** so as to ensure an efficient convergence of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the features\ndf.iloc[:, 1:] = (df - df.mean())/df.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to **create the X, y and theta (Θ) matrices**. We could also use for-loops; however, the NumPy library is able to handle numerical computations more efficiently if the operations are vectorized."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create X, y and theta\nX = df.iloc[:, 1:]\nones = np.ones([len(df), 1])\nX = np.concatenate((ones, X), axis=1)\ny = df.iloc[:, 0:1].values\ntheta = np.zeros([1, len_of_features + 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are also going to store the target variable and check the size of the matrices."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store target\ntarget = y\n\n# Display the size of the matrices\nX.shape, y.shape, theta.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, let's define the function which computes the **cost function J(Θ)**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define computecost function\ndef computecost(X, y, theta):\n    H = X @ theta.T\n    J = np.power((H - y), 2)\n    sum = np.sum(J)/(2 * len(X))\n    return sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is necessary to set the **parameters** of the model, i.e. the number of the total iterations as well as the learning rate  alpha (α). Feel free to *experiment by changing these parameters* and checking if and how quickly does the model converge."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set iterations and alpha (learning rate)\nalpha = 0.01\niterations = 500","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also define the function which does the **gradient descent**, by minimizing the cost function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define gradientdescent function\ndef gradientdescent(X, y, theta, iterations, alpha):\n    cost = np.zeros(iterations)\n    for i in range(iterations):\n        H = X @ theta.T\n        theta = theta - (alpha/len(X)) * np.sum(X * (H - y), axis=0)\n        cost[i] = computecost(X, y, theta)\n    return theta, cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now is the most important step in which we will actually do the gradient descent and print the final theta. This is where the \"learning\" part takes place."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do Gradient Descent and display final theta\nfinal_theta, cost = gradientdescent(X, y, theta, iterations, alpha)\nfinal_theta.round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is more, we will compute and print the final cost."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute and display final cost\nfinal_cost = computecost(X, y, final_theta)\nfinal_cost.round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to check whether our algorithm converges, we will plot the **Iterations vs. Cost** figure."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Iterations vs. Cost figure\nfig_2, ax = plt.subplots(figsize=(10, 8))\nax.plot(np.arange(iterations), cost, 'r')\nax.set_xlabel('Iterations')\nax.set_ylabel('Cost')\nax.set_title('Iterations vs. Cost')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Furthermore, we will define a function which computes the **Root Mean Squared Error (RMSE)** so as to measure the differences  between the values predicted by our model and the observed values we had in the beggining of our analysis. RMSE is an absolute measure of fit and it is in the same units as our response variable, i.e. the price in US dollars."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define rmse function\ndef rmse(target, final_theta):\n    predictions = X @ final_theta.T\n    return np.sqrt(((predictions[:, 0] - target[:, 0]) ** 2).mean())\n\n# Compute and display Root Mean Squared Error\nrmse_val = rmse(target, final_theta)\nrmse_val.round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the RMSE value is high (~216,000 USD). However, if we have in mind that the prices in the original dataset ranged from 75,000 USD to 7,700,000 USD and had a standard deviation of approximately 367,000 USD (see section 2), this RMSE value is not unexpected at all. The best approach would be to implement various machine learning algorithms, compare their results and decide which one works the best for our case.\n\nFinally, let's make a sample prediction in order to check whether our algorithm works as intended. We use the -normalized- features of the first observation in our data which originally had a price of 221,900 USD. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display sample prediction for first observation\npredictions = X @ final_theta.T\nstr(predictions[0].round(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusion\n\nIn this notebook, we applied a multivariate linear regression algorithm so as to predict the house sale prices in King County, USA. The code was greatly detailed and included an exploratory analysis of the dataset and a comprehensive feature selection. In order to get a closer look on what is going on behind the scenes, we used only Pandas and NumPy libraries. \n\nAs I said in the beggining, I am open to any questions, comments and ideas on how to improve the kernel. I hope that it proved useful and that you enjoyed it.\n\nIf you want to access the original code and Jupyter Notebook, you can find them on [GitHub](https://github.com/droussis/linear-regression-house-prices).\n\nApart from Andrew Ng's course (and the contents of Week 1 and 2, in particular), my implementation made use and was influenced by the following websites:\n- https://medium.com/we-are-orb/multivariate-linear-regression-in-python-without-scikit-learn-7091b1d45905\n- https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137\n- https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n- https://www.dataquest.io/blog/understanding-regression-error-metrics/\n- https://www.statisticshowto.datasciencecentral.com/rmse/\n- https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}