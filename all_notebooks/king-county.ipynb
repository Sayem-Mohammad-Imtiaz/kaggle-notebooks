{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Data Science using OSEMN\n\nThe first main step is to obtain all needed informations and import all needed libraries. Here I included the geojson data file from King County and extracted the needed data to help me visuzlize housing data. Next comes the handling of data which is either missing or just not clean. By going through the data and addressing the bits and peices ie NaN etc, helps me understand the data and hopfully can derive some statistics and visualizations. this follows a classifications and scaling of features. Finishing with the interpretation of what I can find.\n\n1. Obtaining data\n2. Scrubbing data\n3. Exploring data\n4. Modeling data \n5. Interpreting results\n"},{"metadata":{},"cell_type":"markdown","source":"### Import all needed libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport math\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport json\n\nfrom scipy import stats\nfrom scipy import linalg\n\nimport statsmodels.api as sm\nimport statsmodels.stats.stattools as sms\nfrom statsmodels.formula.api import ols\n\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn import neighbors\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nimport missingno as msno\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n# set style\nsns.set_style('whitegrid')\n# overriding font size and line width\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n# map visualization\nimport folium\nfrom folium.plugins import HeatMap\n\n# don't print matching warnings\nimport warnings\nwarnings.filterwarnings('ignore') \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# local functions which are in a seperate python file\n#\ndef stepwise_selection(X, y, initial_list=[], threshold_in=0.01, threshold_out=0.05, verbose=True):\n    \"\"\"\n    Perform a forward-backward feature selection based on p-value from statsmodels.api.OLS\n\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features\n\n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    \"\"\"\n\n    included = list(initial_list)\n    while True:\n        changed = False\n        # forward step\n        excluded = list(set(X.columns) - set(included))\n        new_pval = pd.Series(index=excluded)\n\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n\n        best_pval = new_pval.min()\n\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed = True\n\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        # null if pvalues is empty\n        worst_pval = pvalues.max()\n\n        if worst_pval > threshold_out:\n            changed = True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n\n    return included\n\n\ndef display_heatmap(data):\n    \"\"\"\n    Display a heatmap from a given dataset\n\n    :param data: dataset\n    :return: g (graph to display)\n    \"\"\"\n\n    # Set the style of the visualization\n    # sns.set(style = \"white\")\n    sns.set_style(\"white\")\n\n    # Create a covariance matrix\n    corr = data.corr()\n\n    # Generate a mask the size of our covariance matrix\n    mask = np.zeros_like(corr)\n    mask[np.triu_indices_from(mask)] = None\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(15, 12))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(240, 10, sep=20, n=9, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    g = sns.heatmap(corr, cmap=cmap, mask=mask, square=True)\n\n    return g\n\n\ndef display_jointplot(data, columns):\n    \"\"\"\n    Display seaborn jointplot on given dataset and feature list\n\n    :param data: dataset\n    :param columns: feature list\n    :return: g\n    \"\"\"\n\n    sns.set_style('whitegrid')\n\n    for column in columns:\n        g = sns.jointplot(x=column, y=\"price\", data=data, dropna=True,\n                          kind='reg', joint_kws={'line_kws': {'color': 'red'}})\n\n    return g\n\n\ndef display_plot(data, vars, target, plot_type='box'):\n    \"\"\"\n    Generates a seaborn boxplot (default) or scatterplot\n\n    :param data: dataset\n    :param vars: feature list\n    :param target: feature name\n    :param plot_type: box (default), scatter, rel\n    :return: g\n    \"\"\"\n\n    # pick one dimension\n    ncol = 3\n    # make sure enough subplots\n    nrow = math.floor((len(vars) + ncol - 1) / ncol)\n    # create the axes\n    fig, axarr = plt.subplots(nrows=nrow, ncols=ncol, figsize=(20, 20))\n\n    # go over a linear list of data\n    for i in range(len(vars)):\n        # compute an appropriate index (1d or 2d)\n        ix = np.unravel_index(i, axarr.shape)\n\n        feature_name = vars[i]\n\n        if plot_type == 'box':\n            g = sns.boxplot(y=feature_name, x=target, data=data, width=0.8,\n                            orient='h', showmeans=True, fliersize=3, ax=axarr[ix])\n\n        # elif plot_type == 'scatter':\n        else:\n            g = sns.scatterplot(x=feature_name, y=target, data=data, ax=axarr[ix])\n\n        # else:\n        #     col_name = vars[i]\n        #     g = sns.relplot(x=feature_name, y=target, hue=target, col=col_name,\n        #                     size=target, sizes=(5, 500), col_wrap=3, data=data)\n\n    return g\n\n\ndef map_feature_by_zipcode(zipcode_data, col):\n    \"\"\"\n    Generates a folium map of Seattle\n    :param zipcode_data: zipcode dataset\n    :param col: feature to display\n    :return: m\n    \"\"\"\n\n    # read updated geo data\n    king_geo = \"cleaned_geodata.json\"\n\n    # Initialize Folium Map with Seattle latitude and longitude\n    m = folium.Map(location=[47.35, -121.9], zoom_start=9,\n                   detect_retina=True, control_scale=False)\n    # tiles='stamentoner')\n\n    # Create choropleth map\n    m.choropleth(\n        geo_data=king_geo,\n        name='choropleth',\n        data=zipcode_data,\n        # col: feature of interest\n        columns=['zipcode', col],\n        key_on='feature.properties.ZIPCODE',\n        fill_color='OrRd',\n        fill_opacity=0.9,\n        line_opacity=0.2,\n        legend_name='house ' + col\n    )\n\n    folium.LayerControl().add_to(m)\n\n    # Save map based on feature of interest\n    m.save(col + '.html')\n\n    return m\n\n\ndef measure_strength(data, feature_list, target):\n    \"\"\"\n    Calculate a Pearson correlation coefficient and the p-value to test for non-correlation.\n\n    :param data: dataset\n    :param feature_list: feature list\n    :param target: feature name\n    :return:\n    \"\"\"\n\n    print(\"Pearson correlation coefficient R and p-value \\n\\n\")\n\n    for k, v in enumerate(feature_list):\n        r, p = stats.pearsonr(data[v], data[target])\n        print(\"{0} <=> {1}\\t\\tR = {2} \\t\\t p = {3}\".format(target, v, r, p))\n\n\ndef heatmap_features_by_loc(data, feature):\n    \"\"\"\n    Generates a heatmap based on lat, long and a feature\n\n    :param data: dataset\n    :param feature: feature name\n    :return:\n    \"\"\"\n    max_value = data[feature].max()\n\n    lat = np.array(data.lat, dtype=pd.Series)\n    lon = np.array(data.long, dtype=pd.Series)\n    mag = np.array(data[feature], dtype=pd.Series) / max_value\n\n    d = np.dstack((lat, lon, mag))[0]\n    heatmap_data = [i for i in d.tolist()]\n\n    hmap = folium.Map(location=[47.55, -122.0], zoom_start=10, tiles='stamentoner')\n\n    hm_wide = HeatMap(heatmap_data,\n                      min_opacity=0.7,\n                      max_val=max_value,\n                      radius=1, blur=1,\n                      max_zoom=1,\n                      )\n\n    hmap.add_child(hm_wide)\n\n    return hmap\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can import the above function from a seperate python file:\n#\n# import function_filename as f\n#\n# you can check out the the documentation for the rest of the autoreaload modes\n# by apending a question mark to %autoreload, like this:\n# %autoreload?\n#\n# %load_ext autoreload\n# %autoreload 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Description of what can be found in the dataset\n\n+ **ida** notation for a house\n+ **date** Date house was sold\n+ **price** Price is prediction target\n+ **bedrooms** Number of Bedrooms/House\n+ **bathrooms** Number of bathrooms/bedrooms\n+ **sqft_living** square footage of the home\n+ **sqft_lot** square footage of the lot\n+ **floors** Total floors (levels) in house\n+ **waterfront** House which has a view to a waterfront\n+ **view** Has been viewed\n+ **condition** How good the condition is ( Overall )\n+ **grade** overall grade given to the housing unit, based on King County grading system (see below)\n+ **sqft_above** square footage of house apart from basement\n+ **sqft_basement** square footage of the basement\n+ **yr_built** Built Year\n+ **yr_renovated** Year when house was renovated\n+ **zipcode** zip\n+ **lat** Latitude coordinate\n+ **long** Longitude coordinate\n+ **sqft_living15** Living room area in 2015 (implies-- some renovations) This might or might not have affected the lot size area\n+ **sqft_lot15** lotSize area in 2015 (implies-- some renovations)\n\nhttp://www5.kingcounty.gov/sdc/FGDCDocs/resbldg_extr_faq.htm\n##### BLDGGRADE\nBuildling grade (Source: King County Assessments)\n+ Value - Definition\n+ 0     - Unknown\n+ 1     - Cabin\n+ 2     - Substandard\n+ 3     - Poor\n+ 4     - Low\n+ 5     - Fair\n+ 6     - Low Average\n+ 7     - Average\n+ 8     - Good\n+ 9     - Better\n+ 10    - Very Good\n+ 11    - Excellent\n+ 12    - Luxury\n+ 13    - Mansion\n+ 20    - Exceptional Properties\n"},{"metadata":{},"cell_type":"markdown","source":"# Questions:\n\n\n1. Is location of a house by zipcode/neighborhood an indicator for the house price? \n2. Do have zipcodes (neighborhoods) with the higher housing density an effect on selling price?\n3. Does grade, condition and renovation of a house reflect in the price?"},{"metadata":{},"cell_type":"markdown","source":"## Obtaining Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data and read date correctly\n#\ndataset = pd.read_csv(\"../input/kc_house_data.csv\", parse_dates = ['date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Collecting basic informations about the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n\n# Scrubbing Data\n\n\n## Cleaning Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display all missing data\n#\nmsno.matrix(dataset);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handling Null values for view\n#\ndataset.view.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handling yr_renovated\n# - create new column 'is_renovated' and 'yr_since_renovation'\n# - if sqft_living15 > sqft_living set renovated\n# - drop yr_renovated\n#\nimport datetime\ncur_year = datetime.datetime.now().year\n\ndef calc_years(row):\n    return cur_year - row['yr_renovated'] if row['yr_renovated'] > 0 else 0\n\ndef set_renovated(row):\n    return 1 if row['yr_since_renovation'] > 0 or row['sqft_living'] != row['sqft_living15'] else 0\n\n# Set yr_renovated to int\ndataset.yr_renovated.fillna(0, inplace = True)\n# now I can convert yr_renovated to int\ndataset.yr_renovated = dataset.yr_renovated.astype('int64')\n\ndataset['yr_since_renovation'] = dataset.apply(calc_years, axis = 1)\n\n# Create category 'is_renovated'\ndataset['is_renovated'] = dataset.apply(set_renovated, axis=1)\n# Binning\nbins = [0., 1950., 1980., 1990., 2000., 2015.]\nnames = ['never', 'before 1980', '1980-1989', '1990-1999', '2000-2015']\ndataset['yr_renov_bins'] = pd.cut(dataset['yr_renovated'], bins, labels=names, right=False)\ndataset.yr_renov_bins.fillna('never', inplace=True)\n\ndataset.drop(columns=['yr_renovated'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cur_year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.yr_built.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.yr_built.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# While are at it, lets convert yr_built to house_age and drop yr_built\n#\ndataset['house_age'] = cur_year - dataset.yr_built\n# dataset.drop(columns=['yr_built'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.house_age.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To answer this question, it's best to build a new variable (feature engineering) ...\ndataset['yr_built_cat'] = dataset['house_age'].apply(lambda x: ('old' if x >= 50 else 'middle-aged') if x >= 15 else 'modern')\n\n# ... and turn it into a category\ndataset['yr_built_cat'] = pd.Categorical(dataset['yr_built_cat'], categories = ['old', 'middle-aged', 'modern'])\ndataset.head(2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"dataset.yr_built_cat.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"msno.matrix(dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is the percential of NaN in waterfront?\n#\nprint(dataset.waterfront.isnull().sum() / dataset.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because the percential is about 10% we set the NaN values to zero\n#\ndataset.waterfront.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# Waterfront - Level Up:\n# We could try to determine by lat/long if a house is at the waterfront or not by \n# implementing k-nearest neighbor \n# https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(dataset);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning basement feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handling sqft_basement\n#\ndef calc_basement(row):\n    \"\"\"\n    Calculate basement sqft based on difference sqft_living and sqft_above\n    Deals at the same time with the '?' string\n    \n    :param row: feature (column)\n    :return: value (sqft)\n    \"\"\"\n    return row['sqft_living'] - row['sqft_above'] if row['sqft_above'] < row['sqft_living']  else 0\n\ndataset.sqft_basement = dataset.apply(calc_basement, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort dataset by date and reset index (Do I have a good reason for it? No.)\n#\ndataset = dataset.sort_values(by = ['date'])\ndataset = dataset.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get the big picture"},{"metadata":{},"cell_type":"raw","source":"# Correlation Matrix\n#\ndataset.corr()"},{"metadata":{},"cell_type":"raw","source":"abs(dataset.corr()) > 0.7"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_heatmap(dataset);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initial observation (based on the darker colours/higher and lower values):\n    * In general there are very few strong correlations (around +/-0.7 and beyond)\n    * price correlates to sqft_living/15 and grade\n    * grade correlates with sqft_above\n    * house_age correlates with bathrooms, floors, grade, sqft_above\n    * we can consolidate sqft_living, sqft_living15 and sqft_above"},{"metadata":{},"cell_type":"markdown","source":"___\n\n### Get a gerneral overview via scatter plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['zipcode'] = dataset['zipcode'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['bedrooms', 'bathrooms', 'sqft_above', 'sqft_basement', 'sqft_living15', \n        'sqft_lot15', 'yr_since_renovation', 'house_age', 'zipcode']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncol = 3 # pick one dimension\nnrow = math.floor((len(cols)+ ncol-1) / ncol) # make sure enough subplots\nfig, axarr = plt.subplots(nrows=nrow, ncols=ncol, figsize=(20, 20)) # create the axes\n\nfor i in range(len(cols)): # go over a linear list of data\n    ix = np.unravel_index(i, axarr.shape) # compute an appropriate index (1d or 2d)\n\n    name = cols[i]\n    dataset.plot(kind='scatter', x=name, y='price', ax=axarr[ix], label=name) \n\nplt.tight_layout()\nplt.show();\n# plt.savefig('pics/scatter_plot_1.png', dpi = 320)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes:** \n    * Cross referencing on trulia, there are houses with a high sqft_living15 as well a price at $24,000,000\n    * And as it turns out the number of bathrooms can be 8 or even 9, and it looks like that it might have some effect on the price\n    * But 30+ bedrooms is an outlier, they as well looks like can have some effect on the price \n    * How does age (yr_built) and sqft_living coerlate?\n    * It looks like that zipcodes are coerlated to price"},{"metadata":{},"cell_type":"markdown","source":"## Investigating some of the outliers in numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.sqft_lot15.value_counts(bins=10, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.sqft_living15.value_counts(bins=10, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.bedrooms.value_counts(bins=10, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.price.value_counts(bins=10, sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regarding to [trulia](https://www.trulia.com/for_sale/53033_c/price;d_sort/) most of the so called outliers seen in the plot above seem legit (as of 11/02/2018).\n"},{"metadata":{},"cell_type":"markdown","source":"---\n### Investigating Continuos Variables in relationship to price"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'house_age', 'sqft_basement', 'sqft_above', 'sqft_living15',  'sqft_lot15', 'yr_since_renovation'\n#\ncontinous_vars = ['sqft_living15',  'sqft_lot15', 'house_age', 'yr_since_renovation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_jointplot(dataset, continous_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"measure_strength(dataset, continous_vars, 'price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Investigate Discrete Variables\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_vars = ['grade', 'condition', 'view', 'floors', 'bedrooms', 'bathrooms']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"f.display_jointplot(dataset, discrete_vars)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display box-and-whisker plot\n#\ndisplay_plot(dataset, discrete_vars, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"measure_strength(dataset, discrete_vars, 'price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n\n# Answer #1\n\n### Visualize house prices and density by zipcode\n\nDue to missing data we can't run the cells below.\n"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# # Set zipcode type to string (folium)\n# dataset['zipcode'] = dataset['zipcode'].astype('str')\n\n# # get the mean value across all data points\n# zipcode_data = dataset.groupby('zipcode').aggregate(np.mean)\n# zipcode_data.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # count number of houses grouped by zipcode\n# #\n# dataset['count'] = 1\n# t = dataset.groupby('zipcode').sum()\n# t.reset_index(inplace = True)\n# t = t[['zipcode', 'count']]\n# zipcode_data = pd.merge(zipcode_data, t, on='zipcode')\n\n# # drop count from org dataset\n# dataset.drop(['count'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Get geo data file path\n# geo_data_file = os.path.join('data', '../input/king_county_wa_zipcode_area.geojson')\n\n# # load GeoJSON\n# with open(geo_data_file, 'r') as jsonFile:\n#     geo_data = json.load(jsonFile)\n    \n# tmp = geo_data\n\n# # remove ZIP codes not in geo data\n# geozips = []\n# for i in range(len(tmp['features'])):\n#     if tmp['features'][i]['properties']['ZIPCODE'] in list(zipcode_data['zipcode'].unique()):\n#         geozips.append(tmp['features'][i])\n\n# # creating new JSON object\n# new_json = dict.fromkeys(['type','features'])\n# new_json['type'] = 'FeatureCollection'\n# new_json['features'] = geozips\n\n# # save uodated JSON object\n# open(\"../input/cleaned_geodata.json\", \"w\").write(json.dumps(new_json, sort_keys=True, indent=4, separators=(',', ': ')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# map_feature_by_zipcode(zipcode_data, 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# map_feature_by_zipcode(zipcode_data, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the top 5 zipcode by price\n#\n# zipcode_data.nlargest(5, 'price')['zipcode']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Observation:\n    * The most pricey zipcode 98039 seems to be also one of the less densly populated zipcode.\n    * The housing density is focused around Seattle "},{"metadata":{},"cell_type":"markdown","source":"# Answer #2\n\nLocation, location, location. Waterfront properties are by far the most expensive once."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize Folium Map with Seattle latitude and longitude\n\n# from folium.plugins import HeatMap\n\n# max_val = dataset.price.max()\n\n# lat = np.array(dataset.lat, dtype=pd.Series)\n# lon = np.array(dataset.long, dtype=pd.Series)\n# mag = np.array(dataset.price, dtype=pd.Series)\n\n# d = np.dstack((lat, lon, mag))[0]\n# heatmap_data = [i for i in d.tolist()]\n\n# m = folium.Map(location=[47.35, -121.9], zoom_start=9, detect_retina=True, control_scale=False)\n# HeatMap(heatmap_data, radius=1, blur=1).add_to(m)\n# m","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dataset.plot(kind=\"scatter\", x=\"long\", y=\"lat\", figsize=(16, 8), c=\"price\", \n             cmap=\"gist_heat_r\", colorbar=True, sharex=False);\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simpler representation, but in this cae more effective. And brings the point across."},{"metadata":{},"cell_type":"markdown","source":"---\n# Answering question  #3\n\n### Relational plots\nLets visualize the relationship of price and sqft_living15 by grade and condition to hopefuly get a deeper inside"},{"metadata":{},"cell_type":"raw","source":"sns.catplot(x=\"is_renovated\", y=\"price\", data=dataset, height=4, aspect=2)\nplt.title('\\nIs Renovated vs. Price\\n', fontweight='bold')\nplt.xlabel('Is Renovated')\nplt.ylabel('Price');"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"sqft_living15\", y=\"price\", hue=\"price\", col=\"is_renovated\", \n            size=\"price\", sizes=(5, 500), col_wrap=3, data=dataset);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It lloks like that renovation will affect the price"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.is_renovated.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"dataset.info()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get statistics for houses which are renovated\ndf_is_renovated = dataset[dataset['is_renovated'] == 1.0]\n\nsubset = ['price', 'bedrooms', 'floors', 'sqft_living15', 'sqft_lot15']\nis_renovated_descriptives = round(df_is_renovated[subset].describe(), 2)\nis_renovated_descriptives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_not_renovated = dataset[dataset['is_renovated'] == 0.0]\n\nsubset = ['price', 'bedrooms', 'floors', 'sqft_living15', 'sqft_lot15']\nnot_renovated_descriptives = round(df_not_renovated[subset].describe(), 2)\nnot_renovated_descriptives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_renovated_descriptives.price.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_renovated_descriptives.price.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x='is_renovated', y='price', hue='is_renovated', data=dataset, palette=\"PuBu_r\")\n\n# add title, legend and informative axis labels\nax.set_title('\\nMedian Prices depending on if House is renovated\\n', fontsize=14, fontweight='bold')\nax.set(ylabel='Price', xlabel='Is Renovated')\nax.legend(loc=2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['price'][dataset.is_renovated.max()] - dataset['price'][dataset.is_renovated.min()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like that renovating can pay off by about $120,000, but there is no garantie for it.\n\n---"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"sqft_living15\", y=\"price\", hue=\"price\", col=\"condition\",\n            size=\"price\", sizes=(5, 500), col_wrap=3, data=dataset);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot this dataframe with seaborn\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x='condition', y='price', hue='yr_built_cat', data=dataset, palette=\"PuBu_r\")\n\n# add title, legend and informative axis labels\nax.set_title('\\nMedian Prices depending on Condition and Age of Houses\\n', fontsize=14, fontweight='bold')\nax.set(ylabel='Price', xlabel='Condition')\nax.legend(loc=2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Houses which lay in the 3-5 catergory of condition (especially condition 4 for modern homes) seem to have higher price than older homes.\n\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"sqft_living15\", y=\"price\", hue=\"price\", col=\"grade\", \n            size=\"price\", sizes=(5, 500), col_wrap=3, data=dataset);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot this dataframe with seaborn\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x='grade', y='price', hue='yr_built_cat', data=dataset, palette=\"PuBu_r\")\n\n# add title, legend and informative axis labels\nax.set_title('\\nMedian Prices depending on Condition and Age of Houses\\n', fontsize=14, fontweight='bold')\nax.set(ylabel='Price', xlabel='Grade')\nax.legend(loc=2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grade reflects in the price more in older houses, especially houses older than 50 years.\n\n**Conclusion:**\n- Whether you renovate or not is a matter of the outcome you desire. But a simple home improvement seems to help with the selling price.\n- The condition your house is in is important, especially you want to make sure you are in the category 3-5.\n- The grade given for your house reflects in the price on older houses and therefore especially important."},{"metadata":{},"cell_type":"markdown","source":"___\n\n### Categorize Data\n\nWe need to create dummy vars for our categorical variables. **One-hot encoding** shall do the trick. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['condition'] = dataset['condition'].astype('category', ordered = True)\ndataset['waterfront'] = dataset['waterfront'].astype('category', ordered = True)\ndataset['is_renovated'] = dataset['is_renovated'].astype('category', ordered = False)\ndataset['view'] = dataset['view'].astype('category', ordered = False)\n\n# Create category 'has_basement'\ndataset['has_basement'] = dataset.sqft_basement.apply(lambda x: 1 if x > 0 else 0)\ndataset['has_basement'] = dataset.has_basement.astype('category', ordered = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set dummies (we may want to add zipcode as well)\ncat_columns = ['floors', 'view', 'condition', 'waterfront', 'is_renovated', 'has_basement']\n\nfor col in cat_columns:\n    dummies = pd.get_dummies(dataset[col])\n    dummies = dummies.add_prefix(\"{}_\".format(col))\n    \n    dataset.drop(col, axis=1, inplace=True)\n    dataset = dataset.join(dummies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace the '.' in the column name\nfor col in dataset.columns:\n    if col.find('.') != -1: \n        dataset.rename(columns={col: col.replace('.', '_')}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping features\n\nFinally we shall drop eature we are still carrying around but for sure are not needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping id and date\ndataset.drop(['id', 'date', 'lat', 'long'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using MinMax\n#\nminmax_df = dataset[['house_age', 'yr_since_renovation', 'zipcode']]\n\nscaler = preprocessing.MinMaxScaler()\nminmax_scaled_df = scaler.fit_transform(minmax_df)\nminmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['house_age', 'yr_since_renovation', 'zipcode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Robust for price and sqft\n#\nrobust_df = dataset[['price', 'sqft_above', 'sqft_living15', 'sqft_lot15']]\n\nscaler = preprocessing.RobustScaler()\nrobust_scaled_df = scaler.fit_transform(robust_df)\nrobust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['price', 'sqft_above', 'sqft_living15', 'sqft_lot15'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concat normalized data and selected feature into new dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_ols = pd.concat([dataset[['grade', 'bedrooms', 'bathrooms', 'condition_3', 'condition_4', \n                                  'condition_5']], minmax_scaled_df, robust_scaled_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_ols.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n\n## Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ols_results = []\nif len(ols_results) != 1:\n    ols_results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value', 'normality (JB)']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['grade', 'bedrooms', 'bathrooms', 'house_age', 'yr_since_renovation', 'sqft_above',\n            'sqft_living15', 'sqft_lot15', 'zipcode', 'condition_3', 'condition_4', 'condition_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_ols_regression(store_results, data, target, feature, show_plots=False):\n    \"\"\"\n    Run ols model, prints model summary, displays plot_regress_exog and qqplot\n    \n    :param data: dataset\n    :param target: target feature name\n    :param feature: feature name\n    :return:\n    \"\"\"\n    \n    formula = target + '~' + feature\n    model = ols(formula=formula, data=data).fit()\n\n    df = pd.DataFrame({feature: [data[feature].min(), data[feature].max()]})\n    pred = model.predict(df)\n\n    if show_plots:\n        print('Regression Analysis and Diagnostics for formula: ', formula)\n        print('\\n')\n\n        fig = plt.figure(figsize=(16, 8))\n        fig = sm.graphics.plot_regress_exog(model, feature, fig=fig)\n        plt.show();\n\n        residuals = model.resid\n        fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n        fig.show();\n    \n    # append all information to results\n    store_results.append([feature, model.rsquared, model.params[0], model.params[0],\n                        model.pvalues[1], sms.jarque_bera(model.resid)[0]])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check out each feature by itself"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for feature in features:\n    run_ols_regression(ols_results, dataset_ols, 'price', feature)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"pd.DataFrame(ols_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Go through the selection process"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset_ols['price']\nX = dataset_ols.drop(['price'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = stepwise_selection(X, y, verbose = True)\nprint('resulting features:')\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do your regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = '+'.join(features)\nformula = 'price~' + pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ols(formula=formula, data=dataset_ols).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall drop 'yr_since_renovation', 'sqft_above', 'bedrooms', 'condition_3', 'condition_4' \nfrom the feature list"},{"metadata":{},"cell_type":"markdown","source":"## Regression Model Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset_ols['price']\nX = dataset_ols.drop(['price', 'yr_since_renovation', 'sqft_above', 'condition_3', 'condition_4'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_int = sm.add_constant(X)\nmodel = sm.OLS(y, X_int).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\nIterating over the feature seem not to make any improvements on our R-squared value.\n\n##### Furthermore:\n- The regression output shows that most variables are statistically significant with **p-values** less than 0.05. \n- With regards to the **coefficients**, most variables are positively correlated with the price, only (lower) grades, a renovation status years back and the building year are negatively correlated.\n\n#### Final thoughts:\nThe grade of a house has more impact on the price if a house is older than 15 years and even more so if older than 50 years. That said the grade reflects to some extend the size of a house (cabin vs mansoin) but mostly the condition the house is in (poor, average all the way to luxury). The condition of a house has some effect on the price if it is in the range 3 - 5 on all houses no matter the age. One can make the argument that the conidition of a house is coralated wheather or not it is renovated. Renovation has a effect on the price of the house of a median average of $120,000. It is not known what kind of investment one has to make in order to gain such return.\n"},{"metadata":{},"cell_type":"markdown","source":"## Sanity Check using sklearn"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model to the training data\nlinreg = LinearRegression().fit(X_train, y_train)\n\n# Calc preditors on the train and test set\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calc residuals\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calc MSE (Mean Squared Error)\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint('Train Mean Squarred Error:', train_mse)\nprint('Test Mean Squarred Error:', test_mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual Histogram"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16, 8))\nsns.distplot(y_test - y_hat_test, bins=100);\n# sns.distplot(test_residuals, bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation"},{"metadata":{},"cell_type":"raw","source":"num = 100\ntrain_err = []\ntest_err = []\n\nfor i in range(num):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n    linreg.fit(X_train, y_train)\n\n    y_hat_train = linreg.predict(X_train)\n    y_hat_test = linreg.predict(X_test)\n    \n    train_err.append(mean_squared_error(y_train, y_hat_train))\n    test_err.append(mean_squared_error(y_test, y_hat_test))\n    \nplt.scatter(list(range(num)), train_err, label='Training Error')\nplt.scatter(list(range(num)), test_err, label='Testing Error')\nplt.legend();"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_error = []\ntest_error = []\n\nfor t in range(5, 95):\n    train_temp = []\n    test_temp = []\n    for i in range(100):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t/100)\n        linreg.fit(X_train, y_train)\n\n        y_hat_train = linreg.predict(X_train)\n        y_hat_test = linreg.predict(X_test)\n\n        train_temp.append(mean_squared_error(y_train, y_hat_train))\n        test_temp.append(mean_squared_error(y_test, y_hat_test))\n    \n    # save average train/test errors\n    train_error.append(np.mean(train_temp))\n    test_error.append(np.mean(test_temp))\n\nfig = plt.figure(figsize=(16, 12))\nplt.scatter(range(5, 95), train_error, label='training error')\nplt.scatter(range(5, 95), test_error, label='testing error')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ncv_5_results  = np.mean(cross_val_score(linreg, X, y, cv=5, scoring=\"neg_mean_squared_error\"))\ncv_10_results = np.mean(cross_val_score(linreg, X, y, cv=10, scoring=\"neg_mean_squared_error\"))\ncv_20_results = np.mean(cross_val_score(linreg, X, y, cv=20, scoring=\"neg_mean_squared_error\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cv_5_results, cv_10_results, cv_20_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Measure of the quality of an estimator - values closer to zero are better\\n\\n')\nprint('MAE: ', metrics.mean_absolute_error(y_test, y_hat_test))\nprint('MSE: ', metrics.mean_squared_error(y_test, y_hat_test))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_hat_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## Answers\n1. Zip Code (neighborhood) can be an indicator for house prices \n    (see the top 5 zip codes 98039, 98004, 98040, 98112, 98102).\n2. Housing density in condery is less an indicator for the house price.\n3. Regards grade and condition of the house I believe that with the data given we have too little informations and therefore is inconclusive."},{"metadata":{},"cell_type":"raw","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}