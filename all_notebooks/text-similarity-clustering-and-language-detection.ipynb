{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Word2vec Similarity, KMeans Clustering and Fasttext Language detection in tweets about Asunción (Paraguay) city council"},{"metadata":{},"cell_type":"markdown","source":"[Asunción](https://en.wikipedia.org/wiki/Asunci%C3%B3n) is located on the left bank of the Paraguay River, almost at the confluence of this river with the River Pilcomayo, on the South American continent ([Wikipedia](https://en.wikipedia.org/wiki/Asunci%C3%B3n))."},{"metadata":{},"cell_type":"markdown","source":"## Load data & packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re, string \nimport pandas as pd \nimport numpy as np\nfrom time import time  \nimport re, itertools, random\nfrom collections import defaultdict\nimport spacy\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('spanish'))\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom scipy.spatial.distance import cdist\n# uncomment for first run, before put on Internet kernel (settings)\n!python -m spacy download es_core_news_md\n!python -m spacy link es_core_news_md es_md\nnlp = spacy.load('es_md', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/tweets-municipalidad-asuncion/tweets_municipalidad.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    if text is None:\n        return ''\n    text = str(text).replace(\"nan\",'').lower()\n    text = re.sub(r'\\[.*?\\]', '', text)\n    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    # Remove a sentence if it is only one word long\n    if len(text) >= 2:\n        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n\ndf_clean = pd.DataFrame(df.tweet.apply(lambda x: clean_text(x)))\ndf_clean = df_clean.dropna()\ndf_clean = df_clean.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another option is [tweet-preprocessor](https://pypi.org/project/tweet-preprocessor/)..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatizer(text):        \n    sent = []\n    doc = nlp(text)\n    for word in doc:\n        sent.append(word.lemma_)\n    return \" \".join(sent)\n\ndf_clean[\"text_lemmatize\"] = df_clean.apply(lambda x: lemmatizer(x['tweet']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can report any issues with spanish lemmatizer to [spaCy.io](https://spacy.io/), e.g., this [post](https://github.com/explosion/spaCy/issues/2710)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['text_lemmatize_clean'] = df_clean['text_lemmatize'].str.replace('-PRON-', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [row.split() for row in df_clean['text_lemmatize_clean']]\nword_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Top 10 most frequent words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# min_count: minimum number of occurrences of a word in the corpus to be included in the model.\n# window: the maximum distance between the current and predicted word within a sentence.\n# size: the dimensionality of the feature vectors\n# workers: I know kaggle system is having 4 cores without gpu and 2 with gpu, \nw2v_model = Word2Vec(min_count=100,\n                     window=3,\n                     size=200,\n                     workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this line of code to prepare the model vocabulary\nw2v_model.build_vocab(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train word vectors\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(w2v_model.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we do not plan to train the model any further, \n# we are calling init_sims(), which will make the model much more memory-efficient\nw2v_model.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lookup some words: *dengue*"},{"metadata":{},"cell_type":"markdown","source":"> Dengue is a mosquito-borne viral disease that has rapidly spread in all regions of WHO in recent years. Dengue virus is transmitted by female mosquitoes mainly of the species Aedes aegypti and, to a lesser extent, Ae. albopictus. These mosquitoes are also vectors of chikungunya, yellow fever and Zika viruses. Dengue is widespread throughout the tropics, with local variations in risk influenced by rainfall, temperature, relative humidity and unplanned rapid urbanization. [For more info](https://www.who.int/news-room/fact-sheets/detail/dengue-and-severe-dengue)"},{"metadata":{"trusted":true},"cell_type":"code","source":"'dengue' in w2v_model.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most similar words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=['dengue','mosquito','criadero'])\n#w2v_model.wv.most_similar(negative=['dengue','mosquito','criadero','minga'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how similar are these two words to each other \nw2v_model.wv.similarity('mosquito','dengue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*What does not match?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.doesnt_match(['dengue','mosquito','criadero','minga'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which word is to *cateura* (garbage dump) as *dengue* is to *mburicao* (stream)?"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=['cateura','dengue'], negative=['mburicao'], topn=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(model, perplexity=10, n_iter=1000):\n    \"Create TSNE model and plot it\"\n    labels = []\n    tokens = []\n\n    i = 0\n    for word in sorted(model.wv.vocab.keys(), reverse=True):\n        tokens.append(model[word])\n        labels.append(word)\n        i+=1\n        if i >= 499:\n            break\n        \n    tsne_model = TSNE(n_components=2, init='pca', random_state=0, perplexity=perplexity, n_iter=n_iter)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n    \n    x_min, x_max = np.min(new_values, 0), np.max(new_values, 0)\n    X = (new_values - x_min) / (x_max - x_min)\n    shown_images = np.array([[1., 1.]])  # just something big\n    \n    plt.figure(figsize=(20, 20)) \n    for i in range(len(x)):\n        dist = np.sum((X[i] - shown_images) ** 2, 1)\n        '''if np.min(dist) < 1e-3:\n            # don't show points that are too close\n            continue'''\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(3, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use t-SNE to represent high-dimensional data \n# and the underlying relationships between vectors in a lower-dimensional space.\ntsne_plot(w2v_model,40,5000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First get the embeddings into a matrix\nembedding_size=200\nembeddings = np.zeros((len(w2v_model.wv.index2word), embedding_size))\nfor i in range(0, len(w2v_model.wv.index2word)):\n    w = w2v_model.wv.index2word[i]\n    embeddings[i] = w2v_model.wv[w]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=500, random_state=101)\nembeddings_2d_projection = svd.fit_transform(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a K-means cluster model with 6 clusters\nn_clusters = 6\nembedding_cluster_model = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroid_embedding_nearest_words = []\nfor centroid_embedding in embedding_cluster_model.cluster_centers_:\n    centroid_embedding_nearest_words.append(\n        np.argsort([i[0] for i in cdist(embeddings, np.array([centroid_embedding]), \"euclidean\")])[0:10]\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are people talking about?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncolors = itertools.cycle([\"b\",\"g\",\"r\",\"c\",\"m\",\"y\",\"k\",\"w\"])\nc = 0\nfor word_indices in centroid_embedding_nearest_words:\n    clr = next(colors)\n    plt.scatter(\n        embeddings_2d_projection[word_indices,0],\n        embeddings_2d_projection[word_indices,1],\n        color=clr,\n        label=\"Cluster \" + str(c)\n    )\n    for ix in word_indices:\n        x, y = embeddings_2d_projection[ix,:]\n        plt.annotate(w2v_model.wv.index2word[ix], (x, y))\n    c+=1\nplt.legend(loc='lower left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Relations: *dengue*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 200), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 200 to 12 dimensions with PCA\n    reduc = PCA(n_components=12).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**10 Most similar words vs. Top 10 Frequent words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'dengue', sorted(word_freq, key=word_freq.get, reverse=True)[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check interesting relations..."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'dengue', ['mburicao','cateura'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**10 Most similar words vs. 10 Most dissimilar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'dengue', [i[0] for i in w2v_model.wv.most_similar(negative=['dengue'])])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**10 Most similar words vs. 11th to 20th Most similar words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, \"dengue\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"dengue\"], topn=20)][10:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Because it's fun: language detection"},{"metadata":{},"cell_type":"markdown","source":"Paraguay are an official bilingual (Spanish-Guaraní) country with some use of *spanglish* and many *anglicism*, I inspect the languages presented in this dataset by two popular language detectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"import fasttext\n!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\nmodel = fasttext.load_model('lid.176.bin')\nimport langid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_language(text):\n    lang = 'unknown'\n    try:\n        lang1 = model.predict(text, k=2) # against two languages\n        lang1 = lang1[0][0].replace('__label__','') if lang1[0][0].replace('__label__','') in ['en','es','gn'] else lang1[0][0].replace('__label__','') if lang1[1][0]>=0.7 else 'undefined' #fasttext            \n    except:\n        lang1 = lang \n        \n    # priority fasttext and es en gn\n    if text: #example -> lang1:es, lang2:pt\n        if (lang1=='gn' or lang1=='es' or lang1=='en'):\n            return lang1\n        else:\n            try:\n                lang2 = langid.classify(text)[0] \n            except:\n                lang2 = lang\n            if (lang2=='gn' or lang2=='es' or lang2=='en'):\n                return lang2\n            elif (lang1==lang2):\n                return lang1\n            else:\n                return lang1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test language identification..."},{"metadata":{"trusted":true},"cell_type":"code","source":"set_language('Tapeuahẽporãite Vikipetãme') # -> https://gn.wikipedia.org/wiki/Ape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.loc[4,'language'] = \"\"\n\ndf_clean[\"language\"]=df_clean[\"tweet\"].apply(lambda text: set_language(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Languages in the corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 16), dpi=300, facecolor='w', edgecolor='k')\nax = sns.countplot(y=\"language\", data=df_clean)\nax = ax.set_title('Languages count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in df_clean.iterrows():\n    if(row['language']=='gn'):\n        print(row['tweet'], row['language'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Undefined are some spanish tweets and, of course, [**jopará**](https://en.wikipedia.org/wiki/Jopara_language) (Guaraní-Spanish code-switching): \n> Jopará is a colloquial form of Guarani spoken in Paraguay which uses a number of Spanish loan words ([Lustig, Wolf](https://www.staff.uni-mainz.de/lustig/guarani/art/jopara.pdf))."},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in df_clean.iterrows():\n    if(row['language'] in ['unknown','undefined']):\n        if('__label__gn' in model.predict(row['tweet'], k=10)[0]):\n            print(row['tweet'], row['language'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### References\n\n1. https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953\n1. https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n1. https://www.kaggle.com/niclasko/mercari-item-description-word2vec-embeddings-eda\n1. https://www.samyzaf.com/ML/nlp/nlp.html\n1. https://radimrehurek.com/gensim/apiref.html\n1. https://fasttext.cc/docs/en/language-identification.html"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}