{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents:\n1. [Introduction](#section-one)\n\n2. [Reviews Preprocessing and Cleaning](#section-two)\n\n3. [Story Generation and Visualization from reviews](#section-three)\n\n4. [Extracting Features from Cleaned reviews](#section-four)\n\n5. [Model Building: Sentiment Analysis](#section-five)\n\n<a id=\"section-one\"></a>\n# Introduction\n**Everyday we come across various products in our lives, on the digital medium we swipe across hundreds of product choices under one category. It will be tedious for the customer to make selection. Here comes 'reviews' where customers who have already got that product leave a rating after using them and brief their experience by giving reviews. As we know ratings can be easily sorted and judged whether a product is good or bad. But when it comes to sentence reviews we need to read through every line to make sure the review conveys a positive or negative sense. In the era of artificial intelligence, things like that have got easy with the Natural Langauge Processing(NLP) technology.**\n\n## Acknowledgements:\n1. [Ngram visualization analysis](https://www.kaggle.com/ratan123/sentiment-extraction-understanding-metric-eda) - Ratan Rohith\n2. [ROC AUC curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html) - Scikit learn documentation\n3. [Polarity and orange plots](https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a)-Susan Li\n\n\n\n## What is sentiment analysis?\n**Sentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative our neutral.Understanding people’s emotions is essential for businesses since customers are able to express their thoughts and feelings more openly than ever before.It is quite hard for a human to go through each single line and identify the emotion being the user experience.Now with technology, we can automatically analyzing customer feedback, from survey responses to social media conversations, brands are able to listen attentively to their customers, and tailor products and services to meet their needs.**\n\n## Problem statement\n**This is the Problem Statement given by ISRO to classify the customer comments. This would be helpful for the organization to understand Customer feedbacks.**\n\n**Webportals like Bhuvan get vast amount of feedback from the users. To go through all the feedback's can be a tedious job. You have to categorize opinions expressed in feedback forums. This can be utilized for feedback management system. We Classification of individual comments/reviews.and we also determining overall rating based on individual comments/reviews. So that company can get a complete idea on feedback's provided by customers and can take care on those particular fields. This makes more loyal Customers to the company, increase in business , fame ,brand value ,profits.**\n\n## Objectives of Project\n\n1. Reviews Preprocessing and Cleaning\n2. Story Generation and Visualization from reviews\n3. Extracting Features from Cleaned reviews\n4. Model Building: Sentiment Analysis\n\n\n## Import Libraries\n**Let's import all necessary libraries for the analysis and along with it let's bring down our dataset**\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n#Basic libraries\nimport pandas as pd \nimport numpy as np \n\n\n#NLTK libraries\nimport nltk\nimport re\nimport string\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Machine Learning libraries\nimport sklearn \nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn import svm, datasets\nfrom sklearn import preprocessing \n\n\n#Metrics libraries\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\n\n#Visualization libraries\nimport matplotlib.pyplot as plt \nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom textblob import TextBlob\nfrom plotly import tools\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n%matplotlib inline\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Other miscellaneous libraries\nfrom scipy import interp\nfrom itertools import cycle\nimport cufflinks as cf\nfrom collections import defaultdict\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing the dataset\n**Let's welcome our dataset and see what's inside the box**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"raw_reviews = pd.read_csv('../input/amazon-music-reviews/Musical_instruments_reviews.csv')\n## print shape of dataset with rows and columns and information \nprint (\"The shape of the  data is (row, column):\"+ str(raw_reviews.shape))\nprint (raw_reviews.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Details\n**This file has reviewer ID , User ID, Reviewer Name, Reviewer text, helpful, Summary(obtained from Reviewer text),Overall Rating on a scale 5, Review time**\n\n**Description of columns in the file:**\n\n* reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n* asin - ID of the product, e.g. 0000013714\n* reviewerName - name of the reviewer\n* helpful - helpfulness rating of the review, e.g. 2/3\n* reviewText - text of the review\n* overall - rating of the product\n* summary - summary of the review\n* unixReviewTime - time of the review (unix time)\n* reviewTime - time of the review (raw)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Preprocessing and cleaning\n\n**We got to do lot of preprocessing before sending the reviews to the model. Let's go step by step.**\n\n## Handling NaN values\n\n**Let's check for null values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a copy\nprocess_reviews=raw_reviews.copy()\n\n#Checking for null values\nprocess_reviews.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We got null values in reviewer names and review text. Reviewer names doesn't add any value(we got id's instead) to our objective of the project. So let's focus on review text. I don't think dropping wouldn't be a problem as there are only 7 null values, but instead I'm thinking to impute that as missing and explore why they didn't leave any review . Could it be due to ratings?**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['reviewText']=process_reviews['reviewText'].fillna('Missing')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenating review text and summary\nLet's combine review text and summary column. The sentiments won't be contradicting in nature. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['reviews']=process_reviews['reviewText']+process_reviews['summary']\nprocess_reviews=process_reviews.drop(['reviewText', 'summary'], axis=1)\nprocess_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating 'sentiment' column\n**This is an important preprocessing phase, we are deciding the outcome column (sentiment of review) based on the overall score. If the score is greater than 3, we take that as positive and if the value is less than 3 it is negative If it is equal to 3, we take that as neutral sentiment**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Figuring out the distribution of categories\nprocess_reviews['overall'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(row):\n    \n    '''This function returns sentiment value based on the overall ratings from the user'''\n    \n    if row['overall'] == 3.0:\n        val = 'Neutral'\n    elif row['overall'] == 1.0 or row['overall'] == 2.0:\n        val = 'Negative'\n    elif row['overall'] == 4.0 or row['overall'] == 5.0:\n        val = 'Positive'\n    else:\n        val = -1\n    return val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying the function in our new column\nprocess_reviews['sentiment'] = process_reviews.apply(f, axis=1)\nprocess_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling time column\n\n**Here we have an unusual review time column which has date and year, once we split both we will split the date further into month and date.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# new data frame which has date and year\nnew = process_reviews[\"reviewTime\"].str.split(\",\", n = 1, expand = True) \n  \n# making separate date column from new data frame \nprocess_reviews[\"date\"]= new[0] \n  \n# making separate year column from new data frame \nprocess_reviews[\"year\"]= new[1] \n\nprocess_reviews=process_reviews.drop(['reviewTime'], axis=1)\nprocess_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the date \nnew1 = process_reviews[\"date\"].str.split(\" \", n = 1, expand = True) \n  \n# adding month to the main dataset \nprocess_reviews[\"month\"]= new1[0] \n  \n# adding day to the main dataset \nprocess_reviews[\"day\"]= new1[1] \n\nprocess_reviews=process_reviews.drop(['date'], axis=1)\nprocess_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the helpfulness of the review\n**From the main dataframe we can see the helpful feature with values in list [a,b] format. It says that a out of b people found that review helpful. But with that format, it could not add value to the machine learning model and it will be difficult to decrypt the meaning for the machine. So I have planned to create helpful_rate feature which returns a/b value from [a,b]. The following codeblock contains the complete processing step. I have added comments on what's happenening in each code. Unhide to see the code**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Splitting the dataset based on comma and square bracket \nnew1 = process_reviews[\"helpful\"].str.split(\",\", n = 1, expand = True)\nnew2 = new1[0].str.split(\"[\", n = 1, expand = True)\nnew3 = new1[1].str.split(\"]\", n = 1, expand = True)\n\n#Resetting the index\nnew2.reset_index(drop=True, inplace=True)\nnew3.reset_index(drop=True, inplace=True)\n\n#Dropping empty columns due to splitting \nnew2=new2.drop([0], axis=1)\nnew3=new3.drop([1], axis=1)\n\n#Concatenating the splitted columns\nhelpful=pd.concat([new2, new3], axis=1)\n\n\n# I found few spaces in new3, so it is better to strip all the values to find the rate\ndef trim_all_columns(df):\n    \"\"\"\n    Trim whitespace from ends of each value across all series in dataframe\n    \"\"\"\n    trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n    return df.applymap(trim_strings)\n\n#Applying the function\nhelpful= trim_all_columns(helpful)\n\n#Converting into integer types\nhelpful[0]=helpful[0].astype(str).astype(int)\nhelpful[1]=helpful[1].astype(str).astype(int)\n\n#Dividing the two columns, we have 0 in the second columns when dvided gives error, so I'm ignoring those errors\ntry:\n  helpful['result'] = helpful[1]/helpful[0]\nexcept ZeroDivisionError:\n  helpful['result']=0\n\n#Filling the NaN values(created due to dividing) with 0\nhelpful['result'] = helpful['result'].fillna(0)\n\n#Rounding of the results to two decimal places\nhelpful['result']=helpful['result'].round(2) \n\n#Attaching the results to a new column of the main dataframe\nprocess_reviews['helpful_rate']=helpful['result']\n\n#dropping the helpful column from main dataframe\nprocess_reviews=process_reviews.drop(['helpful'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have successfully created the helpful_rate column through processing steps. Let's look at the values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['helpful_rate'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**0.00 indicates that the review hasn't been much helpful and 1.00 indicates that the review has been very helpful**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Review text-Punctuation Cleaning\n**Let's begin our text processing by removing the punctuations**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing unnecessary columns\nprocess_reviews=process_reviews.drop(['reviewerName','unixReviewTime'], axis=1)\n#Creating a copy \nclean_reviews=process_reviews.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_cleaning(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['reviews']=process_reviews['reviews'].apply(lambda x:review_cleaning(x))\nprocess_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have removed all punctuation in our review column**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Review text-Stop words\n**Coming to stop words, general nltk stop words contains words like not,hasn't,would'nt which actually conveys a negative sentiment. If we remove that it will end up contradicting the target variable(sentiment). So I have curated the stop words which doesn't have any negative sentiment or any negative alternatives.**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"stop_words= ['yourselves', 'between', 'whom', 'itself', 'is', \"she's\", 'up', 'herself', 'here', 'your', 'each', \n             'we', 'he', 'my', \"you've\", 'having', 'in', 'both', 'for', 'themselves', 'are', 'them', 'other',\n             'and', 'an', 'during', 'their', 'can', 'yourself', 'she', 'until', 'so', 'these', 'ours', 'above', \n             'what', 'while', 'have', 're', 'more', 'only', \"needn't\", 'when', 'just', 'that', 'were', \"don't\", \n             'very', 'should', 'any', 'y', 'isn', 'who',  'a', 'they', 'to', 'too', \"should've\", 'has', 'before',\n             'into', 'yours', \"it's\", 'do', 'against', 'on',  'now', 'her', 've', 'd', 'by', 'am', 'from', \n             'about', 'further', \"that'll\", \"you'd\", 'you', 'as', 'how', 'been', 'the', 'or', 'doing', 'such',\n             'his', 'himself', 'ourselves',  'was', 'through', 'out', 'below', 'own', 'myself', 'theirs', \n             'me', 'why', 'once',  'him', 'than', 'be', 'most', \"you'll\", 'same', 'some', 'with', 'few', 'it',\n             'at', 'after', 'its', 'which', 'there','our', 'this', 'hers', 'being', 'did', 'of', 'had', 'under',\n             'over','again', 'where', 'those', 'then', \"you're\", 'i', 'because', 'does', 'all']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['reviews'] = process_reviews['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nprocess_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have removed all the stop words in the review column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Story Generation and Visualization from reviews\n**In this section we will complete do exploratory data analysis on texts as well as other factors to understand what are all features which contributes to the sentiment.**\n\n**Prior analysis assumptions:**\n* Higher the helpful rate the sentiment becomes positive\n* There will be many negative sentiment reviews in the 2013 and 2014 year\n* There will be more reviews at the starting of a month\n\n**These assumptions will be verified with our plots also we will do text analysis alot.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sentiments vs Helpful rate\n**First lets look whether there any relationship between sentiment of review and helpfulness of it**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(process_reviews.groupby('sentiment')['helpful_rate'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the table we can see that the mean of of helpful rate is higher for any negative reviews than neutral and positive reviews. These mean value might have been influenced by the 0 values in helpful rates. Lets check how it is distributed through violin plot**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#plot layout\nplt.rcParams.update({'font.size': 18})\nrcParams['figure.figsize'] = 16,9\n\n# Creating dataframe and removing 0 helpfulrate records\nsenti_help= pd.DataFrame(process_reviews, columns = ['sentiment', 'helpful_rate'])\nsenti_help = senti_help[senti_help['helpful_rate'] != 0.00] \n\n#Plotting phase\nsns.violinplot( x=senti_help[\"sentiment\"], y=senti_help[\"helpful_rate\"])\nplt.title('Sentiment vs Helpfulness')\nplt.xlabel('Sentiment categories')\nplt.ylabel('helpful rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:** <br>\n\n**From the plot we can declare that more number of positive reviews are having high helpful rate. We got deceived by the mean value, it's better to look at a plot rather than taking some measures of central tendency under such situation. Our first assumption is correct !**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Year vs Sentiment count\n**In this block we will see how many reviews were posted based on sentiments in each year from 2004 to 2014**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"process_reviews.groupby(['year','sentiment'])['sentiment'].count().unstack().plot(legend=True)\nplt.title('Year and Sentiment count')\nplt.xlabel('Year')\nplt.ylabel('Sentiment count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:** <br>\n**From the plot we can clearly see the rise in positive reviews from 2010. Reaching its peak around 2013 and there is a dip in 2014, All the review rates were dropped at this time. Negative and neutral reviews are very low as compared to the positive reviews. Our second assumption is wrong !**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Day of month vs Reviews count\n**Let's check if there are any relationship between reviews and day of month**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Creating a dataframe\nday=pd.DataFrame(process_reviews.groupby('day')['reviews'].count()).reset_index()\nday['day']=day['day'].astype('int64')\nday.sort_values(by=['day'])\n\n#Plotting the graph\nsns.barplot(x=\"day\", y=\"reviews\", data=day)\nplt.title('Day vs Reviews count')\nplt.xlabel('Day')\nplt.ylabel('Reviews count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:** <br>\n**The review counts are more or less uniformly distributed.There isn't much variance between the days. But there is a huge drop at the end of month. Our third assumption is wrong ! Never trust your instincts unles you do EDA.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Creating few more features for text analysis\n**Now, let's create polarity, review length and word count**\n\n**Polarity:** **We use Textblob for for figuring out the rate of sentiment . It is between [-1,1] where -1 is negative and 1 is positive polarity**\n\n**Review length:** **length of the review which includes each letters and spaces**\n\n**Word length:** **This measures how many words are there in review**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['polarity'] = process_reviews['reviews'].map(lambda text: TextBlob(text).sentiment.polarity)\nprocess_reviews['review_len'] = process_reviews['reviews'].astype(str).apply(len)\nprocess_reviews['word_count'] = process_reviews['reviews'].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentiment polarity distribution\n**Let's look at our polarity distribution**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"process_reviews['polarity'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='Sentiment Polarity Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n* **We have a lot of positive polarities compared to the negative polarities**\n* **This polarity distributions assures the number of positive reviews we had**\n* **We can say that this polarity is a normally distributed but not standard normal**\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Review Rating Distribution\n**Let's check out how overall ratings are distributed**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"process_reviews['overall'].iplot(\n    kind='hist',\n    xTitle='rating',\n    linecolor='black',\n    yTitle='count',\n    title='Review Rating Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have a large number of 5 ratings(nearly 7k) followed by 4,3,2,1. It's linear in nature **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Review Text Length Distribution\n**Let's check out the length of review text**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"process_reviews['review_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='review length',\n    linecolor='black',\n    yTitle='count',\n    title='Review Text Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have a right skewed distribution where most of the lengths falls between 0-1000**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Review Text Word Count Distribution\n**Let's check out the word count of review text**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"process_reviews['word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='word count',\n    linecolor='black',\n    yTitle='count',\n    title='Review Text Word Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have a right skewed distribution with most of the words falling between 0-200 in a a review** ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## N-gram analysis\n**Welcome to the deep text analysis. Here we will be using ngrams to analyse the text, based on it's sentiment **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Monogram analysis\n**Here we will plot most frequent **one word in reviews** based on sentiments**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Filtering data\nreview_pos = process_reviews[process_reviews[\"sentiment\"]=='Positive'].dropna()\nreview_neu = process_reviews[process_reviews[\"sentiment\"]=='Neutral'].dropna()\nreview_neg = process_reviews[process_reviews[\"sentiment\"]=='Negative'].dropna()\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from positive reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_pos[\"reviews\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from neutral reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_neu[\"reviews\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'grey')\n\n## Get the bar chart from negative reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_neg[\"reviews\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of positive reviews\", \"Frequent words of neutral reviews\",\n                                          \"Frequent words of negative reviews\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we see, the words doen't match with the sentiment except few. Through monogram we can't judge a sendiment based on one word. So let's try with frequent two words**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Bigram analysis\n**Here we will plot most frequent two words in reviews based on sentiments**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## Get the bar chart from positive reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_pos[\"reviews\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from neutral reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_neu[\"reviews\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'grey')\n\n## Get the bar chart from negative reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_neg[\"reviews\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'brown')\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,horizontal_spacing=0.25,\n                          subplot_titles=[\"Bigram plots of Positive reviews\", \n                                          \"Bigram plots of Neutral reviews\",\n                                          \"Bigram plots of Negative reviews\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots\")\niplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we can get a clear idea about the sentiments from the bi-words**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Trigram analysis\n**Here we will plot most frequent three words in reviews based on sentiments**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## Get the bar chart from positive reviews ##\nfor sent in review_pos[\"reviews\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from neutral reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_neu[\"reviews\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'grey')\n\n## Get the bar chart from negative reviews ##\nfreq_dict = defaultdict(int)\nfor sent in review_neg[\"reviews\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04, horizontal_spacing=0.05,\n                          subplot_titles=[\"Tri-gram plots of Positive reviews\", \n                                          \"Tri-gram plots of Neutral reviews\",\n                                          \"Tri-gram plots of Negative reviews\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\niplot(fig, filename='word-plots')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have completed our text ngram analysis. Let's look at wordcloud **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud-Positive reviews\n\n**Let's look at the word cloud of positive reviews**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"text = review_pos[\"reviews\"]\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see positive words like great,affordable,expected,exactly etc.,**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud-Neutral reviews\n\n**Let's look at the word cloud of neutral reviews**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"text = review_neu[\"reviews\"]\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most of the neutral review words are focussed on the products and how can they be improved.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud-Negative reviews\n\n**Let's look at the word cloud of negative reviews**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"text = review_neg[\"reviews\"]\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = stop_words).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see negative review words such as noisy,didnt,frickin,wasnt,snap,problems,tension etc.,**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Extracting Features from Cleaned reviews\n**Before we build the model for our sentiment analysis, it is required to convert the review texts into vector formation as computer cannot understand words and their sentiment. In this project, we are going to use TF-TDF method to convert the texts**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Encoding target variable-sentiment\n**Let's encode our target variable with Label encoder.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calling the label encoder function\nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'sentiment'. \nprocess_reviews['sentiment']= label_encoder.fit_transform(process_reviews['sentiment']) \n  \nprocess_reviews['sentiment'].unique() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"process_reviews['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming the reviews\n**Stemming is a method of deriving root word from the inflected word. Here we extract the reviews and convert the words in reviews to its root word. for example,**\n* **Going->go**\n* **Finally->fina**\n\n**If you notice, the root words doesn't need to carry a semantic meaning. There is another technique knows as Lemmatization where it converts the words into root words which has a semantic meaning. Simce it takes time. I'm using stemming**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting 'reviews' for processing\nreview_features=process_reviews.copy()\nreview_features=review_features[['reviews']].reset_index(drop=True)\nreview_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Performing stemming on the review dataframe\nps = PorterStemmer()\n\n#splitting and adding the stemmed words except stopwords\ncorpus = []\nfor i in range(0, len(review_features)):\n    review = re.sub('[^a-zA-Z]', ' ', review_features['reviews'][i])\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stop_words]\n    review = ' '.join(review)\n    corpus.append(review)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus[3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is how a line looks like now, as computer cannot understand words and their sentiment we need to convert these words into 1's and 0's. To encode it we use TFIDF**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## TFIDF(Term Frequency — Inverse Document Frequency)\n**TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.**\n\n**Here we are splitting as bigram (two words) and consider their combined weight.Also we are taking only the top 5000 words from the reviews.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(2,2))\n# TF-IDF feature matrix\nX= tfidf_vectorizer.fit_transform(review_features['reviews'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we have considered 5000 words, we can confirm that we have 5000 columns from the shape.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the target variable(encoded)\ny=process_reviews['sentiment']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Imbalance target feature-SMOTE\n**In our target feature, we noticed that we got a lot of positive sentiments compared to negative and neutral. So it is crucial to balanced the classes in such situatio. Here I use SMOTE(Synthetic Minority Oversampling Technique) to balance out the imbalanced dataset problem.It aims to balance class distribution by randomly increasing minority class examples by replicating them.** <br>\n\n**SMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Original dataset shape : {Counter(y)}')\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X, y)\n\nprint(f'Resampled dataset shape {Counter(y_res)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, as you can see the resampled data has equally distributed classes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Train-test split(75:25)\n**Using train test split function we are splitting the dataset into 75:25 ratio for train and test set respectively.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Divide the dataset into Train and Test\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Model Building: Sentiment Analysis\n**As we have successfully processed the text data, not it is just a normal machine learning problem. Where from the sparse matrix we predict the classes in target feature.**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() / 2.\n    for i in range (cm.shape[0]):\n        for j in range (cm.shape[1]):\n            plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model selection\n\n**First select the best peforming model by using cross validaton. Let's consider all the classification algorithm and perform the model selection process**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the objects\nlogreg_cv = LogisticRegression(random_state=0)\ndt_cv=DecisionTreeClassifier()\nknn_cv=KNeighborsClassifier()\nsvc_cv=SVC()\nnb_cv=BernoulliNB()\ncv_dict = {0: 'Logistic Regression', 1: 'Decision Tree',2:'KNN',3:'SVC',4:'Naive Bayes'}\ncv_models=[logreg_cv,dt_cv,knn_cv,svc_cv,nb_cv]\n\n\nfor i,model in enumerate(cv_models):\n    print(\"{} Test Accuracy: {}\".format(cv_dict[i],cross_val_score(model, X, y, cv=10, scoring ='accuracy').mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the results, we can see logistic regression outdone the rest of the algorithms and all the accuracies from the results are more than 80%. That's great. So let's go with logistic regression with hyperparameter tuning.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression with Hyperparameter tuning\n**We use regularization parameter and penality for parameter tuning. let's see which one to plug.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': np.logspace(-4, 4, 50),\n             'penalty':['l1', 'l2']}\nclf = GridSearchCV(LogisticRegression(random_state=0), param_grid,cv=5, verbose=0,n_jobs=-1)\nbest_model = clf.fit(X_train,y_train)\nprint(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the selected params, we get accuracy. Let's plug and chug**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(C=10000.0, random_state=0)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have got 94% accuracy. That ain't bad. But for classification problems we need to get confusion matrix and check f1 score rather than accuracy**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Classification metrics\n**Here we plot the confusion matrix with ROC and check our f1 score**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = metrics.confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cm, classes=['Negative','Neutral','Positive'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check out the diagonal elements(2326+2195+1854), they are correctly predicted records and rest are incorrectly classified by the algorithm**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Classification Report:\\n\",classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since predicting both positive,negative and neutral reviews are important we are considering.We got a pretty good f1 score. As we see it got a good score across all classes classified **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ROC-AUC curve\n**This is a very important curve where we decide on which threshold to setup based upon the objective criteria. \nHere we plotted ROC for different classes which can help us understand which class was classified better. \nAlso we plot micro and macro averages on the roc curve.**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Binarizing the target feature\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n#Train-Test split(80:20)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,\n                                                    random_state=0)\n\n#OneVsRestClassifier\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=10))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\n#Computing TPR and FPR\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n# aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=4,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=4)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n* **Considering the ROC curve for classes, class 2 and 0 have been classified pretty well a their area under the curve is high. We can chose any threshold between 0.6-0.8 to get the optimal number of TPR and FPR**\n* **Coming to micro and macro average, micro average preforms really well and macro average shows a not very good score**\n* **If you don't understand what micro and macro average is, just remember the following 'A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance'**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n**We have done a pretty neat job on classifying all the classes starting from splitting the sentiments based on overall score,text cleaning, customize the stopwords list based on requirement and finally handling imbalance with smote. Here are few insights from the notebook.**\n\n* **Consider welcoming ngram in sentiment analysis as one word can't give is proper results and stop words got to be manually checked as they have negative words. It is advised to avoid using stop words in sentiment analysis**\n* **Most of our neutral reviews were actual critic of product from the buyers, so amazon can consider these as feedback and give them to the seller to help them improve their products**\n* **Most of the reviews in this dataset were about string instruments such as guitar.**\n* **Balancing the dataset got me a very fruitful accuracy score. Without balancing, I got good precision but very bad recall and inturn affected my f1 score. So balancing the target feature is important**\n* **In sentiment analysis, we should concentrate on our f1 score where we got an average of 94% so we did a pretty good job.**\n\n\n**Thank you for reading my notebook. You can find my other notebooks** [here](https://www.kaggle.com/benroshan/notebooks).\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}