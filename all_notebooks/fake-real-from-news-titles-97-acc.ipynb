{"cells":[{"metadata":{},"cell_type":"markdown","source":"As a first baseline model, I will categorize real and fake news with a layers of Embeddings Biderectional LSTMs. For this approach I will use the news titles only, and disregards the rest of the information in the data set (i.e. text, date, category). "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport json\nimport tensorflow as tf\nimport csv\nimport random\nimport numpy as np\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I load the data in the notebook and add labels (1 for real and 0 for fake news). These are the property I will predict). Once the label column is added, the two data sets are joined."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"real_news = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake_news = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\nreal_news['label'] = 1\nfake_news['label'] = 0\nwhole_dataset = pd.concat([real_news, fake_news])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I split the data into training (80% of the data) and validation (20% of the data), I stratify the label column, to have the same amount of fake news in the training and validation set (and the same for the real news). "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(whole_dataset['title'], whole_dataset['label'], test_size=0.2, random_state=10, stratify = whole_dataset['label'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.head())\nprint(y_test.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will tokenize the titles and pad them to have all the same length, which is set to the length of the longest title"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlengths = [len(x) for x in whole_dataset['title']]\nmax_length = max(lengths)\ntrunc_type = 'post'\npadding_type = 'post'\n\nembedding_dim = 100\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\nvocab_size=len(word_index)\n\nsequences = tokenizer.texts_to_sequences(X_train)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n\nprint(X_train.iloc[0], y_train[0])\nprint(X_train.iloc[1], y_train[1])\n#print(padded[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is now reday for the modeling. I define an early stopping callback to prevent too much overfitting. The model is a sequential stacking of 6 layers.\nThe model is reaching a 97% validation accuracy and has an early stoppinf after 6 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\noverfitCallback = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=5,\n                              verbose=0, mode='auto')\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, 15, input_length=max_length),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 50\nhistory = model.fit(padded, y_train, epochs=num_epochs, validation_data=(test_sequences, y_test), verbose=2, callbacks=[overfitCallback])\n\nprint(\"Training Complete\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next steps:\n- clean up of the titles\n- inclusion of news text in the prediction\n- plotting of the results and word clouds"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}