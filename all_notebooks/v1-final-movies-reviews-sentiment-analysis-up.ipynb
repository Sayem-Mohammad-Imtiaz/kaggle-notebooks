{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#  Importing Important Packages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk import ngrams\n# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Modeling packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Data","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Getting the number of words by splitting them by a space\nwords_per_review = data.Review.apply(lambda x: len(x.split(\" \")))\nwords_per_review.hist(bins = 100)\nplt.xlabel('Review Length (words)')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percent_val = 100 * data['Rating'].value_counts()/len(data)\npercent_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percent_val.plot.bar()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['sentiment'] = np.where(data.Rating >= 3,1,0)    \n# Mapping the ratings\ndata['sentiment'] = np.where(data.Rating > 3,1,0)\n\n## Removing neutral reviews \ndata = data[data.Rating != 3]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making two copies of Reviews to edit","metadata":{}},{"cell_type":"code","source":"#Edits After Removing Stopwords\nEdited_Review = data['Review'].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Having a look at 1st ten reviews in the data","metadata":{}},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Function","metadata":{}},{"cell_type":"markdown","source":"1. Converting words to lower/upper case\n2. Removing special characters\n3. Removing stopwords and high/low-frequency words\n4. lemmatization","metadata":{}},{"cell_type":"code","source":"data['reviews_text_new'] = data['Review'].str.lower()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For reviews converted to lower case\ntoken_lists_lower = [word_tokenize(each) for each in data['reviews_text_new']]\ntokens_lower = [item for sublist in token_lists_lower for item in sublist]\nprint(\"Number of unique tokens now: \",len(set(tokens_lower)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Selecting non alpha numeric charactes that are not spaces\nspl_chars = data['reviews_text_new'].apply(lambda review: \n                                                     [char for char in list(review) if not char.isalnum() and char != ' '])\n\n## Getting list of list into a single list\nflat_list = [item for sublist in spl_chars for item in sublist]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review_backup = data['reviews_text_new'].copy()\ndata['reviews_text_new'] = data['reviews_text_new'].str.replace(r'[^A-Za-z0-9]+', ' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise_words = []\neng_stop_words = stopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(eng_stop_words)\nwithout_stop_words = []\nstopword = []\nsentence = data['reviews_text_new'][3]\nwords = nltk.word_tokenize(sentence)\n\nfor word in words:\n    if word in stop_words:\n        stopword.append(word)\n    else:\n        without_stop_words.append(word)\n\nprint('-- Original Sentence --\\n', sentence)\nprint('\\n-- Stopwords in the sentence --\\n', stopword)\nprint('\\n-- Non-stopwords in the sentence --\\n', without_stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stopwords_removal(stop_words, sentence):\n    return [word for word in nltk.word_tokenize(sentence) if word not in stop_words]\n\ndata['reviews_text_nonstop'] = data['reviews_text_new'].apply(lambda row: stopwords_removal(stop_words, row))\ndata[['reviews_text_new','reviews_text_nonstop']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_sentences(data,name):\n    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n    # Removing double spaces if created\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting all the texts back to sentences\nmake_sentences(data,'reviews_text_nonstop')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data .head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization Function","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndata[\"After_lemmatization\"] = data['reviews_text_nonstop'].apply(lambda text: lemmatize_words(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results of Preprocessing data (Removing stopwords & Lemmatization)","metadata":{}},{"cell_type":"code","source":"data.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"- Old Review -\")\nprint(data['Review'][3])\nprint(\"\\n- New Review -\")\nprint(data['reviews_text_nonstop'][3])\nprint(\"\\n- Last Edit Review -\")\nprint(data['After_lemmatization'][3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['reviews_text_final'] = data['After_lemmatization'].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['reviews_text_final','sentiment']].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a machine learning model","metadata":{}},{"cell_type":"markdown","source":"# Bag-of-words and n-grams","metadata":{}},{"cell_type":"markdown","source":"# Divide into training and test sets:","metadata":{}},{"cell_type":"markdown","source":"# Applying logistic regression","metadata":{}},{"cell_type":"code","source":"### Changes with respect to the previous code\n### 1. Increasing the n-grams from just having 1-gram to (1-gram, 2-gram, 3-gram, and 4-gram)\n### 2. Including the stopwords in the bag of words features\n\nbow_counts = CountVectorizer(tokenizer= word_tokenize,\n                             lowercase=True,\n                             ngram_range=(1,1))\n\nbow_data = bow_counts.fit_transform(data.reviews_text_new)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,\n                                                                    data['sentiment'],\n                                                                    test_size = 0.2,\n                                                                    random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining and training the model\nlr_model_all_new = LogisticRegression(max_iter = 200)\nlr_model_all_new.fit(X_train_bow, y_train_bow)\n\n# Predicting the results\ntest_pred_lr_all = lr_model_all_new.predict(X_test_bow)\n\n\n## Calculate key performance metrics\n\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model_all_new.feature_names=bow_counts.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump, load \n\n# save model to file \ndump(lr_model_all_new, filename=\"Sentiment_Analysis_unigram2.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import a saved joblib model \nloaded_joblib_model = load(filename=\"Sentiment_Analysis_unigram2.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(loaded_joblib_model.feature_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfeats = bow_counts.get_feature_names()\nfeats_len = len(feats)\nsent ='My stay was extremely comfortable. A beautiful hotel surrounded by wonderful staff in a great location.'\nsent =sent.lower()\nsent = sent.translate(str.maketrans('', '', string.punctuation))\nfiltered_sentence = [] \nstop_words = set(stopwords.words('english')) \nword_tokens =word_tokenize(sent)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words ]\nlistToStr = ' '.join(map(str, filtered_sentence))\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(word_tokenize(text))\n    return ([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\nlemmatized_output =[]\nlemmatized_output = lemmatize_words(listToStr)\n\nsent_features=[]\nsent_dict = {}\nfor word in lemmatized_output:\n    if not word in sent_dict:\n        sent_dict[word] = 0\n    sent_dict[word] = sent_dict[word] + 1\nfor i in range(feats_len):\n    if not feats[i] in sent_dict:\n        sent_features.append(0)\n    else:\n        sent_features.append(sent_dict[feats[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent ='The condition of the rooms were very bad. Bed sheets, linens were dirty.'\nsent =sent.lower()\nsent = sent.translate(str.maketrans('', '', string.punctuation))\nfiltered_sentence = [] \nstop_words = set(stopwords.words('english')) \nword_tokens =word_tokenize(sent)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words ]\nlistToStr = ' '.join(map(str, filtered_sentence))\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(word_tokenize(text))\n    return ([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\nlemmatized_output =[]\nlemmatized_output = lemmatize_words(listToStr)\nprint(lemmatized_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(feats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf = pd.DataFrame(feats, columns=[\"features\"])\ndf.to_csv('unigram.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sent_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib_y_preds = loaded_joblib_model.predict([sent_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(joblib_y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}