{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making data frame from csv file \ndata = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\") \ndata = data[data['abstract'].notnull()]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom collections import Counter\nfrom string import punctuation\nnlp = spacy.load(\"en_core_web_lg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_sentence(text, limit):\n    keyword = []\n    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n    doc = nlp(text.lower())\n    for token in doc:\n        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n            continue\n        if(token.pos_ in pos_tag):\n            keyword.append(token.text)\n    \n    freq_word = Counter(keyword)\n    max_freq = Counter(keyword).most_common(1)[0][1]\n    for w in freq_word:\n        freq_word[w] = (freq_word[w]/max_freq)\n        \n    sent_strength={}\n    for sent in doc.sents:\n        for word in sent:\n            if word.text in freq_word.keys():\n                if sent in sent_strength.keys():\n                    sent_strength[sent]+=freq_word[word.text]\n                else:\n                    sent_strength[sent]=freq_word[word.text]\n    \n    summary = []\n    \n    sorted_x = sorted(sent_strength.items(), key=lambda kv: kv[1], reverse=True)\n    \n    counter = 0\n    for i in range(len(sorted_x)):\n        summary.append(str(sorted_x[i][0]).capitalize())\n\n        counter += 1\n        if(counter >= limit):\n            break\n            \n    return ' '.join(summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text=\"Human beings are social creatures and have always valued the importance of friends in their lives. To celebrate this noble feeling it was deemed fit to have a day dedicated to friends and friendship. Accordingly,first Sunday of August was declared as a holiday in USin honor of friends by a Proclamation made by US Congress in 1935. Since then, World Friendship Day is being celebrated every year on the first Sunday in the month of August.This beautiful idea of celebrating Friendship Day was joyfully accepted by several other countries across the world. And today, many countries including India, celebrate the first Sunday of August as Friendship Day every year. Celebrating Friendship Day in a traditional manner, people meet their friends and exchange cards and flowers to honor their friends. Lot many social and cultural organization too celebrate the occasion and mark Friendship Day by hosting programs and get together.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(top_sentence(text,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\n \n# Function to generate n-grams from sentences.\ndef extract_ngrams(data, num):\n    n_grams = ngrams(nltk.word_tokenize(data), num)\n    return [ ' '.join(grams) for grams in n_grams]\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install rouge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport mglearn\nimport re,math\nimport spacy\nimport gensim\nfrom gensim.models import CoherenceModel\nimport gensim.corpora as corpora\nimport rouge\n#from pyrouge import Rouge155\n\n#r = Rouge155()\n\nWORD=re.compile(r'\\w+')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cosine(actual,created):\n    intersection=set(actual.keys()) & set(created.keys())\n    numerator=sum([actual[x]*created[x] for x in intersection])\n    sum1=sum([actual[x]**2 for x in actual.keys()])\n    sum2=sum([created[x]**2 for x in created.keys()])\n    denominator=math.sqrt(sum1)* math.sqrt(sum2)\n    \n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator)/denominator\n\ndef text_vector(text):\n    words=WORD.findall(text)\n    return Counter(words)\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n    \n    \ndef make_bigrams(texts,bigram_mod):\n   ## bigram_mod = gensim.models.phrases.Phraser(bigram)\n            \n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n   # trigram_mod = gensim.models.phrases.Phraser(trigram)\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\n\n\ndef prepare_results(p, r, f):\n    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect=CountVectorizer(ngram_range=(1,1),stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lda_impsentence(created_summary):\n    created_summary=[created_summary]\n    \n    dtm=vect.fit_transform(created_summary)\n    pd.DataFrame(dtm.toarray(),columns=vect.get_feature_names())\n    lda=LatentDirichletAllocation(n_components=5)\n    lda.fit_transform(dtm)\n    sorting=np.argsort(lda.components_)[:,::-1]\n    features=np.array(vect.get_feature_names())\n    mglearn.tools.print_topics(topics=range(3), feature_names=features,\n    sorting=sorting, topics_per_chunk=5, n_words=4)\n            \n            \n            \n            \n    data_words=created_summary\n    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n            \n            \n            \n    data_words_bigrams = make_bigrams(created_summary,bigram_mod)\n    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n    id2word = corpora.Dictionary(data_lemmatized)\n    corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=10, \n                                       random_state=100,\n                                       chunksize=100,\n                                       passes=10,\n                                       per_word_topics=True)\n            \n            \n           \n    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n    print('\\nCoherence Score: ', coherence_lda)\n    return lda_model\n            \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\n\nabstract=data['abstract']\nwith open('topics.csv', 'a', newline='') as file:\n        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        \n        \n       \n        writer.writerow([\"Abstract\",\"Important Sentence\",\"Topic Modeling\"])\nfor i in abstract:\n     with open('topics.csv', 'a', newline='') as file:\n        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        \n        \n        res=top_sentence(str(i),1)\n        lda=lda_impsentence(str(res))\n        ngram=extract_ngrams(str(lda),4)\n        print(\"Ngram is\")\n        print(lda)\n       \n        writer.writerow([i,res,lda])\n        #print(res)\n ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}