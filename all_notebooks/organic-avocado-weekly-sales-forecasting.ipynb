{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Organic Avocado Weekly Sales - Forecasting\n## Triple Exponential Smoothing, SARIMA, and Facebook Prophet\n## Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport itertools\nimport datetime as dt\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# TSA from Statsmodels\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# Facebook Prophet\nfrom fbprophet import Prophet\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleansing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/avocado-prices-2020/avocado-updated-2020.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['date'] = pd.to_datetime(data['date'])\ndata['type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['geography'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select US demand for organic avocados\nmy_data = data[(data['geography'] == 'Total U.S.') &\n               (data['type'] == 'organic')][['date', 'total_volume']]\nmy_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data['date_diff'] = my_data['date'].diff()\nmy_data['date_diff'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# December 2018 is missing\n# Drop off 2019 and 2020 observations\nmy_data = my_data[(my_data['date'].dt.year != 2019) &\n                  (my_data['date'].dt.year != 2020)].set_index('date')\nmy_data.drop('date_diff', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we have one date difference between Jan 1st 2018 and the last date of 2017\n# Examine last date of 2017 and first date of 2018\nprint(my_data[my_data.index.year == 2017].tail(1))\nprint(my_data[my_data.index.year == 2018].head(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This value is duplicated\n# There may be a discrepancy among first days of a week  \nmy_data['weekday'] = my_data.index.strftime('%a')\nmy_data['weekday'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the first date of 2018, weekday\n# Rename our target\nmy_data.drop(pd.Timestamp('2018-01-01'), inplace=True)\nmy_data.drop('weekday', axis=1, inplace=True)\nmy_data.rename(columns={'date':'ds', 'total_volume':'y'}, inplace=True)\nmy_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data.index.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of observations each year\nmy_data.index.year.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data\ntrain = my_data[:181]\ntest = my_data[181:]\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the data (train series)\ntrain.plot(figsize=(12,5), legend=None)\nplt.title('Organic Avocado Weekly Sales', fontsize=16)\nplt.xlabel(None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Overview:**\n- There appears to be an overall increasing trend.\n- There appears to be differences in the variance over time.\n- There may be some seasonality.\n- There may be some outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot data by monthly basis\n# Use line chart and box plot\ntrain['Month'] = train.index.strftime('%b')\ntrain['Year'] = train.index.year\n\nfig, ax = plt.subplots(1,2, figsize=(15,5), sharey=True)\nsns.lineplot(data=train,\n             hue='Year',\n             x='Month',\n             y='y',\n             palette='Set2',\n             ax=ax[0])\nax[0].set_title('Organic Avocado Sales By Year', fontsize=16)\nax[0].set_ylabel(None)\n\nsns.boxplot(data=train,\n            x='Month',\n            y='y',\n            color='lightskyblue',\n            ax=ax[1])\nax[1].set_title('Organic Avocado Sales Distribution by Month', fontsize=16)\nax[1].set_ylabel(None)\n\nsns.despine()\ntrain.drop(['Year', 'Month'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be a peak every year around March, April, and May. Outliers seems not to be significant."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way to decompose the series characteristics\ndecomposition = seasonal_decompose(train['y'])\nfig = decomposition.plot()\nfig.set_size_inches(10,8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Forecasting\n### Exponential Smoothing"},{"metadata":{},"cell_type":"markdown","source":"Since the series has seasonality, let's go ahead and apply triple smoothing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try multiple combination to find a model that has the lowest RSME\n# Try different number of weeks for annual seasonality\n\ntrend = ['additive', 'multiplicative']\nseasonality = ['additive', 'multiplicative']\nperiods = range(52, 56)\n\nlowest_rmse = None\nlowest_rmse_model = None\n\nfor model in list(itertools.product(trend, seasonality, periods)):\n    # Modeling\n    fcast_model = ExponentialSmoothing(train['y'],\n                                       trend=model[0],\n                                       seasonal=model[1],\n                                       seasonal_periods=model[2]).fit()\n    y_fcast = fcast_model.forecast(len(test)).rename('y_fcast')\n    \n    # RSME\n    rmse = np.sqrt(np.mean((test['y'] - y_fcast)**2))\n    \n    # Store results\n    current_rmse = rmse\n        \n    # Set baseline for rmse\n    if lowest_rmse == None:\n        lowest_rmse = rmse\n        \n    # Compare results\n    if current_rmse <= lowest_rmse:\n        lowest_rmse = current_rmse\n        lowest_rmse_model = model      \n    print('{} trend, {} seasonality, {} week frequency - RSME: {}'.format(model[0], model[1], model[2], rmse))\n    \nprint('--------------------------------------------------------------------------------------')\nprint('Model that has the lowest RSME:')\nprint('{} trend, {} seasonality, {} week frequency - RSME: {}'.format(lowest_rmse_model[0], lowest_rmse_model[1],\n                                                                      lowest_rmse_model[2], lowest_rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will try 53-week and 54-week periods because their RSMEs are not much different."},{"metadata":{"trusted":true},"cell_type":"code","source":"def error_metrics(y_fcast, y_test):\n    \"\"\"\n    Return mean absolute percentage error (MAPE)\n           mean percentage error (MPE)\n           mean absolute error (MAE)\n           root mean square error (RMSE)\n           \n    \"\"\"\n    print(f'MAPE: {np.mean(np.abs((y_test - y_fcast)/y_test))*100}')\n    print(f'MPE:  {np.mean((y_test - y_fcast)/y_test)*100}')\n    print(f'MAE:  {np.mean(np.abs(y_test - y_fcast))*100}')\n    print(f'RMSE: {np.sqrt(np.mean((y_test - y_fcast)**2))}')\n    \n\ndef exp_smoothing(y_train,\n                  y_test,\n                  trend=None,\n                  seasonal=None,\n                  period=None,\n                  freq=None,\n                  plot=False,\n                  figsize=None):\n    \"\"\"\n    Forecast using Holt-Winters exponential smoothing.\n    Return a graph and error metrics.\n    \"\"\"\n    # Modeling\n    fcast_model = ExponentialSmoothing(y_train,\n                                       trend=trend,\n                                       seasonal=seasonal,\n                                       seasonal_periods=period).fit()\n    y_est = pd.DataFrame(fcast_model.fittedvalues).rename(columns={0:'y_fitted'}) # In-sample fit\n    y_fcast = fcast_model.forecast(len(y_test)).rename('y_fcast') # Out-of-sample fit\n    \n    # Plot Series\n    if plot:\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n        ax.set_title('Observed, Fitted, and Forecasted Series\\nTriple Exponential Smoothing',\n                     fontsize=16)\n        ax.set_ylabel('Organic Avocado Weekly Sales')\n        ax.plot(y_train,\n                label='In-sample data',\n                linestyle='-')\n        ax.plot(y_test,\n                label='Held-out data',\n                linestyle='-')\n        ax.plot(y_est,\n                label='Fitted values',\n                linestyle='--',\n                color='g')\n        ax.plot(y_fcast,\n                label='Forecasts',\n                linestyle='--',\n                color='k')\n        ax.legend(loc='best')\n        plt.xticks(rotation = 45)\n        plt.show(block = False)\n        plt.close()\n    \n    # Print error metrics\n    print('-----------------------------')\n    if seasonal != None:\n        print('{} trend, {} seasonality, {} {} frequency'.format(trend, seasonal, period, freq))\n    error_metrics(y_fcast=y_fcast, y_test=y_test)\n    print(f'AIC:  {fcast_model.aic}')\n    print(f'BIC:  {fcast_model.bic}')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"exp_smoothing(train['y'],\n              test['y'],\n              trend='additive',\n              seasonal='multiplicative',\n              period=53,\n              freq='week',\n              plot=True,\n              figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_smoothing(train['y'],\n              test['y'],\n              trend='additive',\n              seasonal='multiplicative',\n              period=54,\n              freq='week',\n              plot=True,\n              figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SARIMA\n#### Stationarization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(y, title, window , figsize=(12,5)):\n    \"\"\"\n    Test stationarity using moving average statistics and Dickey-Fuller test\n    Source: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n    \"\"\"\n    # Determing rolling statistics\n    rolmean = y.rolling(window=window, center=False).mean()\n    rolstd = y.rolling(window=window, center=False).std()\n    \n    # Plot rolling statistics:\n    fig = plt.figure(figsize=figsize)\n    orig = plt.plot(y,\n                    label='Original')\n    mean = plt.plot(rolmean,\n                    label='Rolling Mean',\n                    color='r')\n    std = plt.plot(rolstd,\n                   label='Rolling Std',\n                   color='orange')\n    plt.legend(loc = 'best')\n    plt.title('Rolling Mean & Standard Deviation for ' + title, fontsize=16)\n    plt.xticks(rotation = 45)\n    plt.show(block = False)\n    plt.close()\n\n    # Perform Dickey-Fuller test:\n    # Null Hypothesis (H_0): time series is not stationary\n    # Alternate Hypothesis (H_1): time series is stationary\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(y, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], \n                         index=['Test Statistic',\n                                'p-value',\n                                '# Lags Used',\n                                'Number of Observations Used'])\n    for k, v in dftest[4].items():\n        dfoutput['Critical Value (%s)'%k]=v\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(train['y'],\n                  'Total Volume',\n                  window=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously this is not a stationary series."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_general(y,\n                 title='title',\n                 lags=None,\n                 figsize=(12,8)):\n    \"\"\"\n    Examine the patterns of ACF and PACF, along with the time series plot and histogram.\n    Source: https://github.com/jeffrey-yau/Pearson-TSA-Training-Beginner/blob/master/1_Intro_and_Overview.ipynb\n    \"\"\"\n    fig = plt.figure(figsize=figsize)\n    layout = (2,2)\n    ts_ax = plt.subplot2grid(layout, (0,0))\n    hist_ax = plt.subplot2grid(layout, (0,1))\n    acf_ax = plt.subplot2grid(layout, (1,0))\n    pacf_ax = plt.subplot2grid(layout, (1,1))\n    \n    y.plot(ax=ts_ax)\n    ts_ax.set_xlabel(None)\n    ts_ax.set_title(title)\n    \n    y.plot(ax=hist_ax, kind='hist', bins=25)\n    hist_ax.set_title('Histogram')\n    \n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    sns.despine()\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_general(train['y'],\n             title='Organic Avocado Weekly Sales',\n             lags=60,\n             figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ACF does not help much with identifying the seasonality."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's go ahead and apply a log transformation\n# Then take first difference\ndef plot_diff(y,\n              title='title',\n              diff=1,\n              log=True,\n              test=True,\n              window=None,\n              lags=None):\n    if log:\n        y = np.log(y)\n        y_diff = y.diff(diff)\n        y_diff.dropna(inplace=True)\n    else:\n        y_diff = y.diff(diff)\n        y_diff.dropna(inplace=True)\n    plot_general(y_diff, title, lags)\n    if test:\n        test_stationarity(y_diff, title , window)\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_diff(train['y'],\n          window=4,\n          lags=60,\n          title='Log Total Sales\\n(First Difference)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The series is stationary.\n- The series appears slightly overdifferenced (the ACF displays a sharp cutoff and the lag-1 is negative).\n- The ACF cuts off at lag 1\n\nThis suggests: (p, d, q) = (0, 1, 1)\n\nBased on the triple smoothing result, let's try 53-week and 54-week periods for the seasonal difference."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_diff(train['y'],\n          window=4,\n          lags=60,\n          diff=53,\n          title='Log Total Sales\\n(53-week Seasonal Difference)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The series is stationary.\n- The autocorrelation is positive.\n- The ACF has spikes while the PACF cuts off right after lag 1.\n\nThis suggests: (P, D, Q) = (1, 1, 0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_diff(train['y'],\n          window=4,\n          lags=60,\n          diff=54,\n          title='Log Total Sales\\n(54-week Seasonal Difference)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The series is stationary.\n- The autocorrelation is positive.\n- The ACF has spikes while the PACF cuts off right after lag 1.\n\nThis suggests: (P, D, Q) = (1, 1, 0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log transformation\nlog_train = np.log(train['y'])\nlog_test = np.log(test['y'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Selection"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\n# Search over few models to find a model that has the lowest AIC/BIC\nmdl_index = []\nmdl_aic = []\nmdl_bic = []\n\np = range(0,2)\nd = range(0,2)\nq = range(0,2)\nP = range(0,2)\nD = range(1,2)\nQ = range(0,2)\nS = range(53,55)\n\n# Set variables to populate\n#lowest_aic = None\n#lowest_parm_aic = None\n#lowest_param_seasonal_aic = None\n\n#lowest_bic = None\n#lowest_parm_bic = None\n#lowest_param_seasonal_bic = None\n\n# GridSearch the hyperparameters of p, d, q and P, D, Q, S\nfor param in list(itertools.product(p, d, q)):\n    for param_seasonal in list(itertools.product(P, D, Q, S)):\n        mdl = sm.tsa.statespace.SARIMAX(log_train,\n                                        order=param,\n                                        seasonal_order=param_seasonal)\n        results = mdl.fit()      \n        # Store results\n        current_aic = results.aic\n        current_bic = results.bic\n        mdl_index.append('SARIMA{}x{}'.format(param, param_seasonal))\n        mdl_aic.append(current_aic)\n        mdl_bic.append(current_bic)\n            \n        # Set baseline for aic\n        #if lowest_aic == None:\n            #lowest_aic = results.aic\n        # Set baseline for bic\n        #if lowest_bic == None:\n            #lowest_bic = results.bic\n        # Compare results\n        #if current_aic <= lowest_aic:\n            #lowest_aic = current_aic\n            #lowest_parm_aic = param\n            #lowest_param_seasonal_aic = param_seasonal\n        #if current_bic <= lowest_bic:\n            #lowest_bic = current_bic\n            #lowest_parm_bic = param\n            #lowest_param_seasonal_bic = param_seasonal            \n        #print('SARIMA{}x{} - AIC:{} - BIC:{}'.format(param, param_seasonal, results.aic, results.bic))\n        \n#print('--------------------------------------------------------------------------------------')\n#print('Model that has the lowest AIC: SARIMA{}x{} - AIC:{}'.format(lowest_parm_aic, lowest_param_seasonal_aic, lowest_aic))\n#print('Model that has the lowest BIC: SARIMA{}x{} - BIC:{}'.format(lowest_parm_bic, lowest_param_seasonal_bic, lowest_bic))\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\nprint(pd.DataFrame(index=mdl_index, data=mdl_aic).rename(columns={0:'AIC'}).sort_values(by='AIC').head(5))\nprint('-----------------------------------------')\nprint(pd.DataFrame(index=mdl_index, data=mdl_bic).rename(columns={0:'BIC'}).sort_values(by='BIC').head(5))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- SARIMA(1, 1, 1)x(0, 1, 1, 54) appears to be the best model.\n- SARIMA(0, 1, 1)x(0, 1, 1, 54) has the common form of a seasonal model and it has the second highest BIC.\n\nWe will try both of these models."},{"metadata":{},"cell_type":"markdown","source":"#### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Sarima:\n    def __init__(self,\n                 y_train,\n                 y_test,\n                 order,\n                 seasonal_order):\n        self.y_train = y_train\n        self.y_test = y_test\n        self.order = order\n        self.seasonal_order = seasonal_order\n        \n        # Modeling\n        self._model = sm.tsa.statespace.SARIMAX(self.y_train,\n                                                order=self.order,\n                                                seasonal_order=self.seasonal_order)\n        self._results = self._model.fit()\n        \n        # Construct in-sample fit\n        self.y_est = self._results.get_prediction()\n        self.y_est_mean = self.y_est.predicted_mean\n        self.y_est_ci = self.y_est.conf_int(alpha=0.05)\n    \n        # Construct out-of-sample forecasts\n        self.y_fcast = self._results.get_forecast(steps=len(y_test)).summary_frame()\n        self.y_fcast.set_index(y_test.index, inplace=True)\n        \n    def results(self):\n        print(self._results.summary())\n    \n    def diagnostics(self):\n        print(self._results.plot_diagnostics(figsize=(15,8)))\n        \n    def plot(self):\n        # Transform forecast to original scale\n        inv_y_fcast = np.exp(self.y_fcast)\n        inv_y_est_mean = np.exp(self.y_est_mean)\n        inv_y_est_ci = np.exp(self.y_est_ci)\n        inv_y_train = np.exp(self.y_train)\n        inv_y_test = np.exp(self.y_test)\n        \n        # Plot the series\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n        start_index = self.order[1] + self.seasonal_order[3]\n        ax.set_title('Observed, Fitted, and Forecasted Series\\nSARIMA{}x{}'.format(self.order, self.seasonal_order),\n                     fontsize=16)\n        ax.set_ylabel('Organic Avocado Weekly Sales')\n        ax.plot(inv_y_train,\n                label='In-sample data',\n                linestyle='-')\n        ax.plot(inv_y_test,\n                label='Held-out data',\n                linestyle='-')\n        ax.plot(inv_y_est_mean[start_index :],\n                label='Fitted values',\n                linestyle='--',\n                color='g')\n        ax.plot(inv_y_fcast['mean'],\n                label='Forecasts',\n                linestyle='--',\n                color='k')\n        \n        # Plot confidence intervals\n        ax.fill_between(inv_y_est_mean[start_index :].index,\n                        inv_y_est_ci.iloc[start_index :, 0],\n                        inv_y_est_ci.iloc[start_index :, 1],\n                        color='g', alpha=0.05)\n        ax.fill_between(inv_y_fcast.index,\n                       inv_y_fcast['mean_ci_lower'],\n                       inv_y_fcast['mean_ci_upper'], \n                       color='k',\n                        alpha=0.05)\n        \n        ax.legend(loc='upper left')\n        plt.xticks(rotation = 45)\n        plt.show(block = False)\n        plt.close()\n        \n        # Return error metrics\n        error_metrics(inv_y_fcast['mean'], inv_y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SARIMA(1, 1, 1)x(0, 1, 1, 54)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Sarima(y_train=log_train,\n       y_test=log_test,\n       order=(1, 1, 1),\n       seasonal_order=(0, 1, 1, 54)).results()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Sarima(y_train=log_train,\n       y_test=log_test,\n       order=(1, 1, 1),\n       seasonal_order=(0, 1, 1, 54)).diagnostics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sarima(y_train=log_train,\n       y_test=log_test,\n       order=(1, 1, 1),\n       seasonal_order=(0, 1, 1, 54)).plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SARIMA(0, 1, 1)x(0, 1, 1, 54)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Sarima(y_train=log_train,\n       y_test=log_test,\n       order=(0, 1, 1),\n       seasonal_order=(0, 1, 1, 54)).results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sarima(y_train=log_train,\n       y_test=log_test,\n       order=(0, 1, 1),\n       seasonal_order=(0, 1, 1, 54)).diagnostics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sarima(y_train=log_train,\n       y_test=log_test,\n       order=(0, 1, 1),\n       seasonal_order=(0, 1, 1, 54)).plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Facebook Prophet"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.reset_index()\ndf.rename(columns={'date':'ds'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = Prophet(seasonality_mode='multiplicative')\nm.fit(df)\nfuture = m.make_future_dataframe(len(test), freq='W')\nforecast = m.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = m.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_metrics(y_fcast = forecast[-len(test):]['yhat'].values,\n              y_test = test['y'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"| Model | MAPE | MPE | MAE | RMSE |\n| :-----|------|-----|-----|------|\n|Triple Smoothing (53 weeks) | 7.62617 | -2.12897 | 10944425.60652 | 131606.55613 |\n|Triple Smoothing (54 weeks) | 8.57958 | -5.70595 | 12804559.72379 | 155153.19403 |\n|SARIMA(1, 1, 1)x(0, 1, 1, 54) | 6.82706 | -3.00048 | 10171574.44823 | 127108.04375 |\n|SARIMA(0, 1, 1)x(0, 1, 1, 54) | 7.88722 | -4.85496 | 11684456.79612 | 138819.46533 |\n|Facebook Prophet | 8.87980 | 7.89045 | 13514397.69157 | 169932.06145 |\n\nBased on this chart, SARIMA(1, 1, 1)x(0, 1, 1, 54) is the best model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}