{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <center> Explanatory Data Analysis & Models Comparison on Chronic Kidney Disease<center>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports \n\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom functools import partial\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel, RBF, RationalQuadratic, ExpSineSquared\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom functools import partial\nfrom sklearn.model_selection import GridSearchCV\n\nfrom IPython.display import display\nimport time\n\nimport warnings\npd.options.display.max_colwidth = 200\nwarnings.filterwarnings(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1). Read Data: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data \ndata = pd.read_csv(r'../input/ckdisease/kidney_disease.csv')\n# Split it to train and test\ntrain_data, test_data = train_test_split(data) \ntrain_data = train_data.set_index('id')\ntest_data = test_data.set_index('id')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data information\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the shape of the data (the number of rows & columns)\nprint(\"Shape :\",train_data.shape)\n# Statistical description of training data set\nprint(train_data.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe columns not taken into consideration in cell above, we will deal with categorical data after cleaning the data\ntrain_data[[c for c in train_data.columns if c not in train_data.describe().columns]].astype('category').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['cad'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is noise in data. As we can see, 'no' and '\\tno' are considered as two modalities. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeNonAlphanumeric(df) :\n    \"\"\" \n    Remove non-alphanumeric characters from data values\n    Input :\n        df -- dataframe \n    Output :\n        df -- cleaned dataframe\n    \"\"\"\n    for c in df.columns :\n        if df[c].dtype == \"O\" :\n            df[c] = df[c].str.replace('\\t', '')\n            df[c] = df[c].str.replace('[^a-zA-Z0-9]', '')\n    df = df.replace('',np.nan)\n    return df\n\ndef toNumeric(df):\n    \"\"\"\" \n    Convert string column corresponding to numerical values to numerical columns\n    Input : \n        df -- dataframe \n    Output :\n        df -- dataframe with converted columns\n    \"\"\"\n    for c in df.columns :\n        if df[c].dtype == \"O\" and all(df[c].str.isnumeric()):\n            df[c] = pd.to_numeric(df[c])\n    return df\n            \n\n\nclass HandleMissingTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Customized transformer to handles missing data\"\"\"\n    \n    def __init__(self, method,constant = ''):\n        '''' \n        Initialise The transformer\n        Inputs :\n            method -- method used to replace or impute missing data (drop/constant/most_frequent/median/mean)\n            constant -- if constant method is selected, the value of the constant must be specified\n        '''\n        self.method = method\n        self.constant = constant\n        self.imputerDict = {}\n        \n\n    def fit(self, df ):\n        '''\n        If impute method is selected i.e self.method not in ['drop', 'constant'], we must fit an imputer\n        Input : \n            df -- data with missing\n        '''\n        if self.method not in ['drop', 'constant'] :\n            if self.method != \"most_frequent\":\n                print(\"For non numerical columns, most frequent strategy is used\")\n            for c in df.columns :\n                imp = SimpleImputer(missing_values=np.nan, strategy=self.method if df[c].dtype!=\"O\" else \"most_frequent\")\n                imp = imp.fit(df[[c]])\n                self.imputerDict[c] = imp \n        return self\n            \n                \n        \n    def transform(self, df):\n        \"\"\"\n        If impute method is selected, impute missing values using imput_dict created in fit function\n        Input : \n            df -- data with missing values\n        \"\"\"\n        if self.method == \"drop\" :\n            df = df.dropna(inplace= True)\n        elif self.method == 'constant' :\n            df.fillna(self.constant, inplace= True)\n        else :\n            for c in df.columns : \n                df[c] = self.imputerDict[c].transform(df[[c]])\n        return df  \n    \ndef getCategFeat(df, n, target):\n    \"\"\"\n    get dataframe's categorical features \n    Inputs :\n        df     -- dataframe  \n        n      -- min modalities for numerical features\n        target -- target column name\n    \"\"\"\n    return [c for c in df.columns if (df[c].dtype == 'O' or df[c].nunique()<n) and c!=target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove non alphanumeric \ntransf_alphaN = FunctionTransformer(removeNonAlphanumeric, validate= False) \n# Transform function to Transformer object so that we can use in pipeline at test time\ntransf_num = FunctionTransformer(toNumeric, validate= False)\ntrain_data = transf_alphaN.transform(train_data)\ntrain_data = transf_num.transform(train_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['cad'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get columns with null values\nprint(\"Columns with null values before imputing\")\nprint(train_data.columns[train_data.isna().any()].tolist())\n\n# Handle missing values\ntransf_Missing = HandleMissingTransformer(method=\"most_frequent\")\ntrain_data = transf_Missing.fit(train_data).transform(train_data)\n\nprint(\"Columns with null values after imputing\")\nprint(train_data.columns[train_data.isna().any()].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minModalities = 7 ## To match dataset desciprtion on UCI website, numerical variables with less than 7 modalities are considered as nominal\ncategorical_features = getCategFeat(train_data,minModalities,'classification')\nprint(\"Categorical Features : \",categorical_features)\nnumerical_features = [c for c in train_data.columns if c not in categorical_features and c!=\"classification\"]\nprint(\"Numerical Features : \", numerical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_variable_exploration(df, target, xlabel, ylabel, title, positive=1) :\n    \"\"\" \n    plots the distribution of the classes\n    Input :\n        df -- dataframe containing classes\n        target -- class column\n        xlabel\n        ylabel \n        title\n        positive -- modality corresponding to positive class\n    \"\"\"\n    negative =  [c for c in df[target].unique() if c !=positive][0]\n    positive_class = df[target].value_counts()[positive]\n    negative_class = df[target].shape[0] - positive_class\n    positive_per = positive_class / df.shape[0] * 100\n    negative_per = negative_class / df.shape[0] * 100\n    plt.figure(figsize=(8, 8))\n    sns.countplot(df[target], order=[positive, negative]);\n    plt.xlabel(xlabel, size=15, labelpad=15)\n    plt.ylabel(ylabel, size=15, labelpad=15)\n    plt.xticks((0, 1), [ 'Positive class ({0:.2f}%)'.format(positive_per), 'Negative class ({0:.2f}%)'.format(negative_per)])\n    plt.tick_params(axis='x', labelsize=13)\n    plt.tick_params(axis='y', labelsize=13)\n    plt.title(title, size=15, y=1.05)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Target variable exploration\n\ntarget_variable_exploration(train_data, \"classification\", 'Class', ' Count', 'Training Set ckd Distribution', positive = 'ckd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classes are slithly umbalanced.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Pairplots\nsns.pairplot(train_data, diag_kind ='hist' , hue=\"classification\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse :\n    - For variables \"RC\", \"pvc\", \"hemo\", \"sod\", bigger values corresponds to higher probability of \"nonckd\" ;\n    - The histograms of the numerical features show that there are numerical variables present in the dataset that consists of few discrete values, e.g. \"sg\". Those features are considered as nominal (As stated in data description on the website UCI); \n    - Some modalities of categorical features (eg. \"ba\") exist only among individuals with ckd ;\n    - There are some categorical features, where the different categories occur with similar frequency, e.g. for \"sg\"; and there are feature, where one category is very dominant, e.g. for \"su\" ;\n    - Some features show a linear relationship with each other, for example \"hema\" and \"pcv\". We will study later correlation relationships between variables.\n    \n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_numeric(data, numeric_features, target) :\n    \"\"\" \n    plots analysing numerical features\n    Inputs : \n        data -- dataframe containing features to plot\n        numeric_features -- list of numerical features\n        target -- target column name\n     \"\"\"\n    # Looping through and Plotting Numeric features\n    for column in numeric_features:    \n        # Figure initiation\n        fig = plt.figure(figsize=(18,12))\n\n        ### Distribution plot\n        sns.distplot(data[column], ax=plt.subplot(221));\n        # X-axis Label\n        plt.xlabel(column, fontsize=14);\n        # Y-axis Label\n        plt.ylabel('Density', fontsize=14);\n        # Adding Super Title (One for a whole figure)\n        plt.suptitle('Plots for '+column, fontsize=18);\n\n        ### Distribution per Positive / Negative class Value\n        # Not Survived hist\n        classes = data[target].unique()\n        sns.distplot(data.loc[data[target]==classes[0], column].dropna(),\n                     color='red', label=str(classes[0]), ax=plt.subplot(222));\n        # Survived hist\n        sns.distplot(data.loc[data[target]==classes[1], column].dropna(),\n                     color='blue', label=str(classes[1]), ax=plt.subplot(222));\n        # Adding Legend\n        plt.legend(loc='best')\n        # X-axis Label\n        plt.xlabel(column, fontsize=14);\n        # Y-axis Label\n        plt.ylabel('Density per '+ str(classes[0])+' / '+str(classes[1]), fontsize=14);\n\n        ### Average Column value per positive / Negative Value\n        sns.barplot(x=target, y=column, data=data, ax=plt.subplot(223));\n        # X-axis Label\n        plt.xlabel('Positive or Negative?', fontsize=14);\n        # Y-axis Label\n        plt.ylabel('Average ' + column, fontsize=14);\n\n        ### Boxplot of Column per Positive / Negative class Value\n        sns.boxplot(x=target, y=column, data=data, ax=plt.subplot(224));\n        # X-axis Label\n        plt.xlabel('Positive or Negative ?', fontsize=14);\n        # Y-axis Label\n        plt.ylabel(column, fontsize=14);\n        # Printing Chart\n        plt.show()\n        \ndef plot_categ(train_data, target, nominal_features,positive =1) :\n    \"\"\" \n    plots analysing nominal categorical features\n    Inputs : \n        data -- dataframe containing features to plot\n        nominal_features -- list of nominal features\n        target -- target column name\n     \"\"\"\n    # Looping through and Plotting Categorical features\n    for column in nominal_features:\n    # Figure initiation\n        fig = plt.figure(figsize=(18,12))\n        \n        ### Number of occurrences per categoty - target pair\n        ax = sns.countplot(x=column, hue=target, data=train_data, ax = plt.subplot(211));\n        # X-axis Label\n        plt.xlabel(column, fontsize=14);\n        # Y-axis Label\n        plt.ylabel('Number of occurrences', fontsize=14);\n        # Adding Super Title (One for a whole figure)\n        plt.suptitle('Plots for '+column, fontsize=18);\n        # Setting Legend location \n        plt.legend(loc=1);\n\n        ### Adding percents over bars\n        # Getting heights of our bars\n        height = [p.get_height() if np.isnan(p.get_height()) == 0 else 0 for p in ax.patches] #  get nan if\n        # Counting number of bar groups \n        ncol = int(len(height)/2)\n        # Counting total height of groups\n        total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n        # Looping through bars\n        for i, p in enumerate(ax.patches):    \n            # Adding percentages\n            ax.text(p.get_x()+p.get_width()/2, height[i]*1.01 + 10,\n                    '{:1.0%}'.format(height[i]/total[i]), ha=\"center\", size=14) \n\n        negative = train_data[target].unique()[0] if train_data[target].unique()[0] != positive else train_data[target].unique()[1]\n        ### Positive class percentage for every value of feature\n        \n        sns.pointplot(x=train_data[column], y=train_data[target].map({negative:0 , positive: 1}), ax = plt.subplot(212));\n        # X-axis Label\n        plt.xlabel(column, fontsize=14);\n        # Y-axis Label\n        plt.ylabel(' Positive class percentage', fontsize=14);\n        # Printing Chart\n        plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Plotting Numeric Features\nplot_numeric(train_data, numerical_features, 'classification')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features contain outliers.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"##### Plotting nominal Categorical Features    \n\nplot_categ(train_data,\"classification\", categorical_features[:-2],positive='ckd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    - Induviduals having some features (eg \"su\" > 0, \"ane\"=yes, etc) are necessarly healthy (notckd) .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlationMap(df, target) :\n    \"\"\" \n    Correlation Heatmap\n    Inputs : \n        df -- dataframe containing features to plot\n        target -- target column name\n     \"\"\"\n    classes = df[target].unique()\n    if data[target].dtype == 'O' :\n        df[target+'_id'] = (df[target]== classes[0]).astype(int) #encode string target \n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(12, 9))\n    sns.heatmap(corr, vmax=.8,annot=True, square=True)\n    if data[target].dtype == 'O' :\n        df.drop([target+'_id'], axis=1, inplace=True)\n    # fix for matplotlib bug that cuts off top/bottom of seaborn viz\n    b, t = plt.ylim() # Gets the values for bottom and top\n    b += 0.5 # Add 0.5 to the bottom\n    t -= 0.5 # Subtract 0.5 from the top\n    plt.ylim(b, t) # update the ylim(bottom, top) values\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Correlation Analyses\n\ncorrelationMap(train_data,'classification')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen above in scatter plots, \"hemo\" and \"pvc\" are highly correlated","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can delete highly correlated features because they can negatively impact some models (eg. trees)\ntrain_data, test_data = train_data.drop(\"pcv\", axis=1), test_data.drop(\"pcv\", axis=1)\nnumerical_features = [f for f in numerical_features if f!=\"pcv\"]\nnumerical_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling and Encoding :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureEng(numerical_features, categorical_features):\n    \"\"\" \n    create pipeline for feature preprocessing \n    Inputs : \n        numerical_features -- list of numerical features\n        categorical_features -- list of categorical features\n    Outputs :\n        preproc -- pipeline with feature preprocessing steps\n     \"\"\"\n    numeric_transformer = StandardScaler()\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n    t =  ColumnTransformer([('Scaler', numeric_transformer, numerical_features),('OneHotEncod', categorical_transformer, categorical_features)])\n    preproc = Pipeline(steps=[('preprocessor', t)])\n    return preproc\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical features\n\ntransf_train = featureEng(numerical_features, categorical_features).fit(train_data)\nX_train = transf_train.transform(train_data)\ny_train = train_data['classification'].replace({'ckd':1,'notckd':0}).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  get columns names after transformations\ncolumns_eng = numerical_features + transf_train.named_steps['preprocessor'].transformers_[1][1].get_feature_names(categorical_features).tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA on numerical features\n\npca = PCA(n_components=10)\nprincipalComponents = pca.fit_transform(X_train[:,:len(numerical_features)])\n\n# Keep two PC\nprincipalDf = pd.DataFrame(data = principalComponents[:,:2]\n             , columns = ['principal component 1', 'principal component 2'])\nfinalDf = pd.concat([principalDf, pd.DataFrame(y_train)], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5 variables respect Kaiser rule ie lambda>=1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0, 1]\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf[0] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Projection on first PC\nn_axes = len(numerical_features)\n_, axes = plt.subplots(ncols=5,nrows=2, figsize=(20,10))\ncol_id = 0\nfor i in range(axes.shape[0]):\n    for j in range(axes.shape[1]):\n        try :\n            axes[i][j].scatter(principalComponents[:,0], X_train[:,col_id])\n            axes[i][j].set_title(f'1st component vs {numerical_features[col_id]}')\n            col_id = col_id+1\n        except exception as e:\n            print(e)\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Projection on second PC\nn_axes = len(numerical_features)\n_, axes = plt.subplots(ncols=5,nrows=2, figsize=(20,10))\ncol_id = 0\nfor i in range(axes.shape[0]):\n    for j in range(axes.shape[1]):\n        try :\n            axes[i][j].scatter(principalComponents[:,1], X_train[:,col_id])\n            axes[i][j].set_title(f'2st component vs {columns_eng[col_id]}')\n            col_id = col_id+1\n        except :\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"components = pca.components_\nplt.figure(figsize=(10,10))\nfor i, (x, y) in enumerate(zip(components[0,:], components[1,:])):\n    plt.plot([0, x], [0, y], color='k')\n    plt.text(x, y, numerical_features[i])\n\nplt.plot([-0.7, 0.7], [0, 0], color='grey', ls='--')\nplt.plot([0, 0], [-0.7, 0.7], color='grey', ls='--')\n\nplt.xlim(-0.7, 0.7)\nplt.ylim(-0.7, 0.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two PCS do not give a good presentation of all features (features are not close to the extremities).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data has now 47 features. To reduce execution time and complexity we will select the most important ones to feed them to models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nndiscrete = len(transf_train.named_steps['preprocessor'].transformers_[1][1].get_feature_names(categorical_features))\ndiscreteCol = np.arange(len(X_train[0])-ndiscrete,len(X_train[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class selectFeaturesTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Custom scaling transformer\"\"\"\n    def __init__(self, k=10,method='RF',discreteCol=[]):\n        \"\"\" \n        initialize transformer\n        Inputs : \n            k -- number of features to keep\n            method -- method to use, either 'Mutual Information or RF\n            discreteCol -- if Mutual Information is used, specify indexes of discrete columns\n        \"\"\"\n        self.k = k\n        self.method = method\n        self.order = []\n        self.discreteCol = discreteCol\n        \n        \n        \n\n    def fit(self, X_train,y_train):\n        \"\"\"\n        Fit the transformer on data\n        Input :\n            X_train -- features array\n            Y_train -- labels array\n        Output :\n            fitted transformer\n        \"\"\"\n        if self.method == \"Mutual Information\" :\n            discrete_mutual_info_classif = partial(mutual_info_classif, \n                                                   discrete_features=self.discreteCol)\n            featS = SelectKBest(k=self.k, score_func=discrete_mutual_info_classif).fit(X_train,y_train )\n            self.order = np.flip(featS.scores_.argsort())\n            #self.selectedColumns = [columns_eng[i]  for i in self.order[:self.k]]\n            #return X_train[:,order_mi[:self.k]]\n        \n        else :\n            rfModel = RandomForestClassifier(random_state =0).fit(X_train, y_train)\n            order = np.flip(rfModel.feature_importances_.argsort())\n            self.order = np.flip(rfModel.feature_importances_.argsort())\n            #self.selectedColumns = [columns_eng[i]  for i in order_rf[:self.k]]\n            #return X_train[:,order_[:self.k]]\n        return self\n            \n                \n        \n    def transform(self, X_train):\n        \"\"\"\n        apply fitted transformer to select features\n        Input :\n            X_train -- features array\n        Output :\n            array containing only selected features\n        \"\"\"\n        return X_train[:,self.order[:self.k]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FSelector_mi = selectFeaturesTransformer(k=10,method=\"Mutual Information\",discreteCol=discreteCol)\nFSelector_rf = selectFeaturesTransformer(k=10,method=\"Random Forest\")\nFSelector_mi.fit(X_train,y_train)\nFSelector_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Top 10 selected by Mutual information\")\nprint([columns_eng[i]  for i in FSelector_mi.order[:10]])\nprint(\"Top 10 selected by Random Forest\")\nprint([columns_eng[i]  for i in FSelector_rf.order[:10]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a big similarity between features selected by the two methods.\nUnlike correlations study, Mutual information takes into consideration non linear relationship between variables. However, It does not consider the interaction between variables like RF does. Therefore, we will use feature selected by this latter.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(44)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclassifiers = [\n    SGDClassifier(loss='log'), # for logistic regression\n   KNeighborsClassifier(),\n    SVC(),\n    GaussianProcessClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    MLPClassifier(),\n    GaussianNB()]\n\nker_rbf = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n\nker_rq = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RationalQuadratic(alpha=0.1, length_scale=1)\n\n#ker_expsine = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1))\n\nkernel_list = [ker_rbf, ker_rq]\n\nnames = [\"Logistic Regression with SGD\", \"Nearest Neighbors\", \"SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\",\"Gradient Boosting\", \"Neural Net\",\n         \"Naive Bayes\"]\n\nparameters = {\"Logistic Regression with SGD\" : {'Classifier__penalty':['l1','l2',None],\n                                               'Classifier__learning_rate' : ['constant','optimal','adaptive'],\n                                               'Classifier__eta0' : [0.1]},\n    \"Nearest Neighbors\" : {'Classifier__n_neighbors':[5,8,10]},\n        'SVM':{'Classifier__kernel':['linear','rbf'],'Classifier__C':[0.1,0.5,1.,1.5]},\n        \"Gaussian Process\":{\"Classifier__kernel\": kernel_list,\n                            \"Classifier__n_restarts_optimizer\": [1, 2, 3]},\n        \"Decision Tree\" : {\"Classifier__max_features\" : ['sqrt','log2',None],\n                        \"Classifier__max_depth\":[10,30,50,None]}\n        ,\"Random Forest\":{\"Classifier__n_estimators\":[8,10,20,50],\"Classifier__max_features\" : ['sqrt','log2',None],\n                        \"Classifier__max_depth\":[10,30,50,None]},\n       'Gradient Boosting':{\"Classifier__max_features\" : ['sqrt','log2',None],\n                        \"Classifier__max_depth\":[2,3,10],\n                        \"Classifier__learning_rate\":[1e-1,1e-2,1e-3]},\n         \"Neural Net\" : {'Classifier__hidden_layer_sizes': [(20,20,20), (25,50,25), (50,50)],\n                      'Classifier__activation': ['tanh', 'relu'],\"Classifier__learning_rate_init\":[1e-1,1e-2,1e-3]},\n        \"Naive Bayes\" : {\"Classifier__var_smoothing\" : [1e-8, 1e-9]}\n         }\nparameters_featuresSelection = {'FeatureSelection__method':['RF'],'FeatureSelection__k':[10,20,30,47]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(X_train, y_train, classifiers, names,parameters, parameters_featuresSelection, crossVal = True):\n    \"\"\" \n    training process\n    Inputs : \n        X_train -- features array\n        Y_train -- labels array\n        classifiers -- list of classifiers to test\n        names -- list of classifiers names\n        parameters -- tuning parameters corresponding for classifiers\n        parameters_featuresSelection -- parameters for fearures selection\n        crossVal -- whether to use cross validation or not\n     \"\"\"\n    results = pd.DataFrame()\n    for name, clf in zip(names, classifiers):\n        print('############# ', name, ' #############')\n        start = time.time()\n        #print(params[name])\n        FSelector = selectFeaturesTransformer()\n        pipeline = Pipeline([('FeatureSelection',FSelector),('Classifier',clf)])\n        parameters[name]['FeatureSelection__method'] = parameters_featuresSelection['FeatureSelection__method']\n        parameters[name]['FeatureSelection__k']= parameters_featuresSelection['FeatureSelection__k']\n        if crossVal:\n            classifier = GridSearchCV(pipeline, parameters[name], cv=3)\n        else:\n            classifier = pipeline\n        #print(classifier)\n        classifier.fit(X_train, y_train)\n        # All results\n        means = classifier.cv_results_['mean_test_score']\n        stds = classifier.cv_results_['std_test_score']\n        r = pd.DataFrame(means,columns = ['mean_test_score'])\n        r['std_test_score'] = stds\n        r['params'] = classifier.cv_results_['params']\n        r['classifier'] = name\n        \n        print('Training time (Cross Validation = ',crossVal,') :',(time.time()-start)/len(means))\n        display(r.sort_values(by=['mean_test_score','std_test_score'],ascending =False))\n        results = pd.concat([results, r], ignore_index=True)\n        #for mean, std, params in zip(means, stds, classifier.cv_results_['params']):\n        #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n    results_sorted = results.sort_values(by=['mean_test_score','std_test_score'],ascending =False)\n    return results_sorted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = train(X_train, y_train, classifiers, names,parameters, parameters_featuresSelection)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.groupby('classifier').head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the same performance, the first combination of parameters for Random Forset is the simplest. It is the one we will use for test (Occam’s Razor).\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply transformations on test data\ntest_data = transf_alphaN.transform(test_data)\ntest_data = transf_num.transform(test_data)\ntest_data = transf_Missing.transform(test_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = test_data['classification'].replace({'ckd':1,'notckd':0}).values\nX_test = transf_train.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_selected = results.iloc[0]\nmodel = classifiers[names.index(model_selected['classifier'])]\nparam = {key.split('__')[1]:val for key,val in model_selected['params'].items() if 'FeatureSelection' not in key } \nmodel.set_params(**param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train)\nmodel.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, model.predict(X_test), target_names=['notckd','ckd']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}