{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ансамбли моделей в машинном обучении\n\nТехника **ансамблирования** (т. е. комбинирования разных моделей для создания одной \"оптимальной\") весьма популярна в машинном обучении. Это даёт возможность не полагаться на одну-единственную модель, которая может быть переобучена или иметь другие недостатки. С частным случаем ансамбля моделей мы уже сталкивались, когда рассматривали Random Forest. Теперь рассмотрим эту технику в более общей форме познакомимся с другими типами ансамблей.\n\nВ основном различают три вида ансамблей:\n* бэггинг;\n* бустинг;\n* стекинг (блендинг).\n\n## Бэггинг\n\nВспомним, что случайный лес (Random Forest) --- это ансамбль решающих деревьев вида **bagging (bootstrap aggregating)**. Суть в том, что мы обучаем решающие деревья на выборках, полученных из исходной обучающей выборки путём бутстрэпа. При этом каждое отдельное дерево может быть переобученным, но **после агрегирования** ансамбль даёт лучшие результаты, чем каждая его компонента в отдельности. Итак, бэггинг --- это \"микс\" из различных **однородных** моделей, обучаемых **параллельно и независимо** друг от друга (чем более независимо, тем лучше!). Конечный результат получается путём агрегирования (часто --- просто усреднения) результатов.\n\n## Бустинг\n\n**Бустинг (boosting)** моделей основан на другой идее. Предположим, у нас есть некоторая базовая (слабая) модель. Давайте **последовательно улучшать её качество** путём, например, анализа ошибок, которые модель допускала на предыдущей итерации. При этом каждая новая модель будет иметь лучший score, чем предыдущая. В бустинге также рассматриваются **однородные модели** (как и в бэггинге, популярным выбором являются деревья, но это вовсе не обязательно). Бустинг похож на итеративный процесс оптимизации параметров функции по градиентному спуску, только в данном случае мы \"оптимизируем\" саму модель в некотором \"пространстве моделей\".\n\n## Стекинг\n\nНаконец, **стекинг (stacked generalization, stacking)** --- это ансамбль, основанный на идее **добавления метапризнаков** к исходному набору признаков и **обучения метамодели** на наборе метапризнаков. Как правило, метапризнаки строятся как результаты предсказаний различных моделей ML. В очень грубой форме стекинг можно описать так: \n* Шаг 1. Строим кучу разных моделей на исходном наборе признаков (например, линейную регрессию, регрессию по knn, регрессию по деревьям, random forest и пару нейросетей впридачу) --- всё, на что способна бурная фантазия ML engineer :)\n* Шаг 2. Каждая из моделей на шаге 1 дала какие-то предсказания, давайте добавим эти предсказания к исходному набору признаков. Получим так называемые \"метапризнаки\".\n* Шаг 3. Обучим новую модель ML (например, опять линейную регрессию) на наборе метапризнаков. Такая модель называется \"метамоделью\". Результат метамодели будем считать окончательным предсказанием.\n\nОписанный вид стекинга самый простой, его часто называют **\"блендингом\" (смешиванием)**. При желании можно делать многоуровневый стекинг, создавая \"метамета...метапризнаки\" и \"метамета...метамодели\" описанным способом. Стоит отметить, что этот приём часто помогает выигрывать различные соревнования по анализу данных на Kaggle, однако не всегда применим в реальных приложениях.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Ансамбли в контексте проблемы bias-variance\n\nВспомним, что в машинном обучении ошибку модели можно разложить на три составляющие:\n* смещение (bias);\n* разброс (variance);\n* неконтролируемая ошибка.\n\nВ идеале мы бы хотели, чтобы наша модель имела малый разброс (попадала \"точно в цель\") и малое смещение (результаты были устойчивыми к изменению входных данных). Но в жизни всё не так просто. Модели с очень малым смещением имеют, как правило, большой разброс (слишком сложные и переобученные), а модели с низким разбросом, в свою очередь, являются слишком простыми (недообученными) и имеют большое смещение. Задача инженера по машинному обучению --- найти компромиссную по сложности модель, или \"золотую середину\" между разбросом и смещением (bias-variance tradeoff).\n\n![](https://neurohive.io/wp-content/uploads/2019/04/1_kISLC1Udq0m6g5kwHhMuJg-2x-770x487.png)\n\n\"Плохие модели\" (или, как их ещё называют, \"слабые ученики\") имеют либо слишком большое смещение, либо слишком большой разброс. Различные типы ансамблей призваны сгладить эти недостатки. Разберёмся подробнее.\n\nОдним из важных моментов является то, что наш выбор слабых учеников должен быть согласован с тем, как мы агрегируем эти модели. \n* Если мы выбираем слабых учеников с **низким смещением, но высоким разбросом**, это должно быть с помощью метода агрегирования, который имеет тенденцию **уменьшать разброс**, тогда как \n* Если мы выбираем слабых учеников с **низким разбросом, но с высоким смещением**, это должен быть метод агрегирования, который имеет тенденцию **уменьшать смещение**.\n\nВ контексте рассмотренных нами видов ансамблей:\n* В **бэггинге** рассматриваются базовые модели с **низким смещением, но высоким разбросом** (например, переобученные деревья). Усреднение в бустинге будет уменьшать разброс.\n* В **бустинге и стекинге** рассматриваются базовые модели с **низким разбросом** (т. е. простые, \"глупые\"), но **высоким смещением** (например, бустинг часто начинают с неглубоких деревьев). Последовательное улучшение будет давать модели со всё меньшим смещением, чем исходная.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Полезные ссылки\n\n1. [Ансамблевые методы: бэггинг, бустинг и стекинг](https://neurohive.io/ru/osnovy-data-science/ansamblevye-metody-begging-busting-i-steking/)\n\n2. [Градиентный бустинг](https://habr.com/ru/company/ods/blog/327250/)\n\n3. [Cтекинг и блендинг](https://dyakonov.org/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Бустинг в действии\n\nВ модуле [ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) библиотеки sklearn реализовано два основных типа бустинговых ансамблей:\n* AdaBoost (адаптивный бустинг);\n* GradientBoosting (градиентный бустинг).\n\nТакже существуют другие популярные библиотеки, такие как \n* [LightGBM](https://lightgbm.readthedocs.io/en/latest/);\n* [XGBoost](https://xgboost.readthedocs.io/en/latest/).","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import RandomOverSampler\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_results(y_true, y_pred):\n    print(confusion_matrix(y_true, y_pred))\n    print('F1-score:', f1_score(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_validation_curve(model_grid, param_name, params=None):\n    # Рисуем валидационную кривую\n    # По оси х --- значения гиперпараметров (param_***)\n    # По оси y --- значения метрики (mean_test_score)\n\n    results_df = pd.DataFrame(model_grid.cv_results_)\n    \n    if params == None:\n        plt.plot(results_df['param_'+param_name], results_df['mean_test_score'])\n    else:\n        plt.plot(params, results_df['mean_test_score'])\n\n    # Подписываем оси и график\n    plt.xlabel(param_name)\n    plt.ylabel('Test F1 score')\n    plt.title('Validation curve')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/depression/b_depressed.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Удалим пропуски\ndf_1 = df.dropna()\n\n# Дропнем ненужные столбцы\ndf_2 = df_1.drop(['Survey_id', 'depressed'], axis=1)\n\n# Переведём признаки \"Номер виллы\" и \"Уровень образования\" в бинарные \n# * мы не уверены на 100 %, что уровень образования ранговый, поэтому считаем его категориальным\ndf_3 = pd.get_dummies(df_2, columns=['Ville_id', 'education_level'])\ndf_3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Разделение на train и valid\nX = df_3\ny = df_1['depressed']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25,\n                                                      random_state=19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Масштабирование\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_valid = scaler.transform(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab = AdaBoostClassifier(random_state=19)\nab.fit(X_train, y_train)\ny_pred = ab.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Тюнинг параметров\nab_n_estimators = {'n_estimators': np.arange(10, 100, 10)}\nab_grid = GridSearchCV(ab, ab_n_estimators, cv=5, scoring='f1', n_jobs=-1)\nab_grid.fit(X_train, y_train)\n\nprint(ab_grid.best_score_)\nprint(ab_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_validation_curve(ab_grid, 'n_estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab_n_estimators = {'n_estimators': np.arange(100, 201, 20)}\nab_grid = GridSearchCV(ab, ab_n_estimators, cv=5, scoring='f1', n_jobs=-1)\nab_grid.fit(X_train, y_train)\n\nprint(ab_grid.best_score_)\nprint(ab_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_validation_curve(ab_grid, 'n_estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab_best = ab_grid.best_estimator_\ny_pred = ab_best.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab_l_rate = {'learning_rate': [0.001, 0.01, 0.1, 0.5, 0.75, 1, 2, 10]}\nab_grid = GridSearchCV(ab, ab_l_rate, cv=5, scoring='f1', n_jobs=-1)\nab_grid.fit(X_train, y_train)\n\nprint(ab_grid.best_score_)\nprint(ab_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_validation_curve(ab_grid, 'learning_rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab_best = ab_grid.best_estimator_\ny_pred = ab_best.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab_params = {'n_estimators': np.arange(20, 201, 20), \n             'learning_rate': [0.001, 0.01, 0.1, 0.5, 0.75, 1, 2, 5, 10]}\nab_grid = GridSearchCV(ab, ab_params, cv=5, scoring='f1', n_jobs=-1)\nab_grid.fit(X_train, y_train)\n\nprint(ab_grid.best_score_)\nprint(ab_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ab_best = ab_grid.best_estimator_\ny_pred = ab_best.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingClassifier(random_state=19)\ngb.fit(X_train, y_train)\ny_pred = gb.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_n_estimators = {'n_estimators': np.arange(20, 201, 20)}\ngb_grid = GridSearchCV(gb, gb_n_estimators, cv=5, scoring='f1', n_jobs=-1)\ngb_grid.fit(X_train, y_train)\n\nprint(gb_grid.best_score_)\nprint(gb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_best = gb_grid.best_estimator_\ny_pred = gb_best.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_l_rate = {'learning_rate': [0.05, 0.1, 0.5, 0.75, 1, 2, 5, 10]}\ngb_grid = GridSearchCV(gb, gb_l_rate, cv=5, scoring='f1', n_jobs=-1)\ngb_grid.fit(X_train, y_train)\n\nprint(gb_grid.best_score_)\nprint(gb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_best = gb_grid.best_estimator_\ny_pred = gb_best.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gb_params = {'n_estimators': np.arange(20, 201, 20), \n#              'learning_rate': [0.1, 0.5, 0.75, 1, 2, 5, 10]}\n# gb_grid = GridSearchCV(gb, gb_params, cv=5, scoring='f1', n_jobs=-1)\n# gb_grid.fit(X_train, y_train)\n\n# print(gb_grid.best_score_)\n# print(gb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gb_best = gb_grid.best_estimator_\n# y_pred = gb_best.predict(X_valid)\n# print_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_max_depth = {'max_depth': np.arange(1, 11)}\ngb_grid = GridSearchCV(gb, gb_max_depth, cv=5, scoring='f1', n_jobs=-1)\ngb_grid.fit(X_train, y_train)\n\nprint(gb_grid.best_score_)\nprint(gb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_validation_curve(gb_grid, 'max_depth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_best = gb_grid.best_estimator_\ny_pred = gb_best.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gb_params = {'n_estimators': np.arange(20, 221, 50), \n#              'learning_rate': [0.1, 0.5, 1, 5, 10],\n#              'max_depth': np.arange(1, 11, 2)}\n# gb_grid = GridSearchCV(gb, gb_params, cv=5, scoring='roc_auc', n_jobs=-1)\n# gb_grid.fit(X_train, y_train)\n\n# print(gb_grid.best_score_)\n# print(gb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gb_best = gb_grid.best_estimator_\n# y_pred = gb_best.predict(X_valid)\n# print_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgbc = xgb.XGBClassifier()\nxgbc.fit(X_train, y_train)\ny_pred = xgbc.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {'n_estimators': [20, 50, 100, 200],\n             'max_depth': [2, 4, 6]}\nxgb_grid = GridSearchCV(xgbc, xgb_params, cv=5, scoring='roc_auc', n_jobs=-1)\nxgb_grid.fit(X_train, y_train)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best = xgb_grid.best_estimator_\ny_pred = xgb_best.predict(X_valid)\nprint_results(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}