{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n   # for filename in filenames:\n     #   print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottle_df = pd.read_csv(\"../input/calcofi/bottle.csv\")\nbottle_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rename Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"Cast Count\", \"Bottle Count\", \"Station ID\", \"Depth ID\", \"Depth\", \"Temperature\", \"Salinity\",\n           \"O2_mL/L\", \"H2O Density\", \"O2 Sat\", \"O2_Âµmol/Kg\", \"Bottle No\", \"Record Indicator\",\n           \"Temperature Precision\", \"Temperature Quality\", \"Salinity Precision\", \"Salinity Quality\",\n           \"Pressure Quality\", \"O2 Quality\", \"H20_Density Quality\", \"O2_Saturation Quality\",\n           \"Chlorophyll-a\", \"Chlorophyll-a Quality\", \"Phaeophytin_Concentration\", \"Phaeophytin Quality\", \"Phosphate Concentration\",\n           \"Phosphate Quality\", \"Silicate Concentration\", \"Silicate Quality\", \"Nitrite Concentration\",\n           \"Nitrite Quality\", \"Nitrate Concentration\", \"Nitrate Quality\", \"NH4 Concentration\", \"NH4 Quality\",\n           \"C14_As1\", \"C14_As1 Precision\", \"C14_As1 Quality\", \"C14_As2\", \"C14_As2 Precision\", \"C14_As2 Quality\",\n           \"C14_As_Dark\", \"C14_As_Dark Precision\", \"C14_As_Dark Quality\", \"Mean_C14_As\", \"Mean_C14_As Precision\",\n           \"Mean_C14_As Quality\", \"Incubation Time\", \"Light Intensity\", \"Reported Depth\", \"Reported Temperature\",\n           \"Reported Potential Temperature\", \"Reported Salinity\", \"Reported Potential Density\",\n           \"Reported Specific Volume Anomaly\", \"Reported Dynamic Height\", \"Reported O2_mL/L\", \"Reported O2 Sat\",\n           \"Reported Silicate Concentration\", \"Reported Phosphate Concentration\", \"Reported Nitrate Concentration\",\n           \"Reported Nitrite Concentration\", \"Reported NH4 Concentration\", \"Reported Chlorophyll-a\",\n           \"Reported Phaeophytin\", \"Pressure (decibars)\", \"Sample No\", \"Dissolved_Inorganic_Carbon1\",\n           \"Dissolved_Inorganic_Carbon2\", \"Total Alkalinity1\", \"Total Alkalinity2\", \"pH2\", \"pH1\",\n           \"DIC Quality Comment\"\n          ]\n\nbottle_df.columns = columns\n\nbottle_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check shape of dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"bottle_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottle_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see we have quite a number of missing values which will be dealt with going forward\n\nThe first four columns have no missing values (they are more of identifiers) so they won't be involved in preprocessing steps like imputation and scaling"},{"metadata":{},"cell_type":"markdown","source":"**Define a function that will handle the full preprocessing of the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(drop_threshold=70, num_strategy=\"constant\",\n                    cat_strategy=\"most_frequent\", fill_value=-999,\n                    scaling=\"normal\", file_name=\"prepared_data.csv\"):\n    \"\"\"\n    drop_threshold can accept any value between 0 and 100;\n    num_strategy can accept \"mean\", \"median\" or \"constant\"\n    fill_value: to be specified when num_strategy = \"constant\"...can take any value\n    scaling can accept \"standard\" or \"normal\"\n    file_name should be specified\n    \"\"\"\n    \n    data = bottle_df.copy() # make a copy of the original dataframe\n    \n    \"\"\"\"Drop columns with percent of missing values greater than the threshold\"\"\"\n    # Get the percentage of missing values for each column\n    percent_missing = round(data.isna().sum() / data.shape[0] * 100, 2)\n    \n    # create a dictionary of the missing values and percent per column\n    values = {\"Total number of missing values\": data.isna().sum(), \"Percent of Missing Values\": percent_missing}\n    \n    # convert the dictionary to a dataframe\n    missing = pd.DataFrame(values)\n    \n    # get the columns that fall above the drop_threshold \n    columns_to_drop = missing[missing[\"Percent of Missing Values\"] > drop_threshold].index\n    \n    # drop the columns\n    data.drop(columns_to_drop, axis=1, inplace=True)\n    \n    # since salinity is the target feature, it should no have missing values\n    data.dropna(subset=[\"Salinity\"], inplace=True)\n    \n    # As earlier stated, exclude the first four columns from the following steps\n    new_data = data.iloc[:, 4:]\n    \n    \"\"\"split the dataset into continuous and categorical columns\"\"\" \n    # create an empty dictionary to hold the number of unique values per column\n    uniques = {}\n    \n    # iterate through the data columns and append the number of unique values in each column to the\n    # unique dictionary\n    for column in new_data.columns:\n        uniques[column] = new_data[column].nunique()\n    \n    # from careful examination, a threshold of 6 unique values seems to be appropriate for the split\n    \n    # get the categorical and continuous columns based on the threshold \n    cat_attributes = [column for column in uniques if uniques[column] <= 6]\n    num_attributes = [column for column in uniques if uniques[column] > 6]\n    \n    # Create a new dataframe with the created attributes in a specific order\n    new_data = pd.concat([new_data[num_attributes], new_data[cat_attributes]], axis=1)\n    \n    \"\"\"Create a pipeline for imputation and scaling\"\"\"\n    # create a SimpleImputer object for the numerical columns based on the specified strategy\n    if num_strategy == \"constant\":\n        numerical_imputer = SimpleImputer( fill_value=fill_value)\n    else:\n        numerical_imputer = SimpleImputer(strategy=num_strategy)\n    \n    # create a SimpleImputer object for the categorical columns based on the specified strategy \n    categorical_imputer = SimpleImputer(strategy=cat_strategy)\n    \n    # create a scaling object for standardization or normalization\n    if scaling == \"standard\":\n        scaler = StandardScaler()\n    elif scaling == \"normal\":\n        scaler = MinMaxScaler()\n    \n    # Create a pipeline to perform imputation and scaling on the numerical attributes\n    numerical_pipeline = Pipeline([(\"imputer\", numerical_imputer), (\"scaler\", scaler)])\n    \n    # create a full pipeline for both numerical and categorical attributes\n    full_pipeline = ColumnTransformer([(\"num\", numerical_pipeline, num_attributes),\n                                       (\"cat\", categorical_imputer, cat_attributes)])\n    \n    # Preprocess the data using the full pipeline\n    prepared_bottle_df = pd.DataFrame(full_pipeline.fit_transform(new_data),\n                                     columns=num_attributes + cat_attributes)\n    \n    # restore the excluded columns\n    excluded_columns = data.iloc[:, 0:4]\n    excluded_columns.reset_index(drop=True, inplace=True)\n    prepared_bottle_df.reset_index(drop=True, inplace=True)\n    \n    prepared_df = pd.concat([excluded_columns, prepared_bottle_df], axis=1)\n    \n    # assert that there are no missing values in the prepared dataframe\n    assert not all(prepared_df.isna().sum()) \n    \n    # save the dataframe to a csv file\n    prepared_bottle_df.to_csv(file_name)\n    \n    return prepared_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_bottle = preprocess_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_bottle.to_csv('bottle1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_bottle.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}