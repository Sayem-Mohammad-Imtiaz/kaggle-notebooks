{"cells":[{"metadata":{},"cell_type":"markdown","source":"# STAT 201 Midterm #"},{"metadata":{},"cell_type":"markdown","source":"*************************************************************\n* **Author**:\t\tAlexis Harris\n* **Filename**:\t\tSTAT201_Midterm_AHarris.ipynb\n* **Date Created**:\t11/4/2020\n* **Modifications**:\tNone\n\t\t\t\n**************************************************************"},{"metadata":{},"cell_type":"markdown","source":"*************************************************************\n\n* Lab/Assignment: Stat 201 Midterm\n\n\n \n* Overview: This file contains the answers to the Stat 201 Midterm due Nov. 4, 2020.\n\n\n* Input: Input datasets from google.cloud's bigquery for problem 4. Also used Melbourne_housing_FULL.csv which should be uploaded to this notebook.\n\n\n* Output: Queries from google.cloud's data and Dataframes from csv file\n\n\n************************************************************"},{"metadata":{},"cell_type":"markdown","source":"## **1.)  List and explain the steps in a typical data science project workflow.**\n\n1. First, you should discuss with your boss or company what problem they want you to solve and what data you will need access to.\n2. List the data issues to fix and make the data usable.\n3. Produce a preliminary report on the data and figure out the questions that can be answered from the data description. May need to repeat the process again.\n4. List questions of interest.\n5. Attempt to thouroughly clean the data.\n6. Load Dataset. If not loading correctly, you need to clean again.\n7. Explore the data! Produce a descriptive analysis, visual data, communicate. See if the data creates more questions.\n8. Move to machine learning (perform/learn some task) - test and train data. This needs to answer original questions.\n9. Report findings.\n10. Celebrate!"},{"metadata":{},"cell_type":"markdown","source":"## **2.)  List at least three common problems that a data scientist might encounter when working with an employer and potential work-arounds for these problems.**\n1. Miscommunication - sometimes you and your employer may not be on the same page for what problem you should be working on. Make sure to communicate effectively so you can answer your employers question.\n2. Expecting a specific answer - I think sometimes the questions an employer asks a data scientist will not always have an answer that he or she wants to hear. The company might not have enough data for the data scientist to use to conclude a statistically sound answer. A workaround to this would be to try to find out near the beginning of the workflow whether the data the employer provides is usable or not.\n3. The employer not using the results found from the data science project - Sometimes the employer or company might not utilize the results found from a data scientist in their decision making process. Data scientists might have to deal with defending their work to the company if that company doesn't prioitize data science."},{"metadata":{},"cell_type":"markdown","source":"## **3.)  What is the major difference between a database and a flat file and why are databases more useful in many situations?**\n\nThe major difference between a database and a flat file is that a database has multiple data structures. This means there are multiples tables of information in a database. On the other hand, a flat file will only have one file with all the data information stored in one place. Databases are more useful because they are easier to navigate compared to a flat file and there is less chance of data redunancies/errors to occur."},{"metadata":{},"cell_type":"markdown","source":"## **4.)  For this question, we will be using SQL and the Bigquery environment.  Please provide code that is structured and commented according to the CST style guide posted on the course webpage.**"},{"metadata":{},"cell_type":"markdown","source":"**a.) Within Kaggle is a database of SanFransisco public data called “san_francisco”  Start by reading this data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom google.cloud import bigquery","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"client = bigquery.Client()  # import Kaggle's bigquery data\ndataset_ref = client.dataset(\"san_francisco\",project = \"bigquery-public-data\") # Call san_francisco dataset from bigquery data\nsfDataset = client.get_dataset(dataset_ref)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**b.)  Produce a list of the tables within this dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tables = list(client.list_tables(sfDataset)) # create a list to view the tables within the san_francisco dataset\nfor table in tables:\n    print(table.table_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**c.)  Determine the size of data required to import the film_locations table without actually loading it.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a query of the film_locations dataset. Will select everything from this table and see how many bytes this query will process\nquery1 = \"\"\"\n    SELECT *\n    FROM bigquery-public-data.san_francisco.film_locations\n\"\"\"\ndry_run_config = bigquery.QueryJobConfig(dry_run = True) #dry run\ndry_run_query_job = client.query(query1, job_config = dry_run_config)\n\nprint(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed)) #line of code that views total bytes processed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**d.)  Read this table and produce a data frame that counts the number of movies produced by each production company from before 1950 and listed from most movies to least.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"table_ref_film = dataset_ref.table('film_locations') # look at film_locations table\nfilmLocations = client.get_table(table_ref_film)\nclient.list_rows(filmLocations, max_results = 6).to_dataframe() # look at 6 rows of the film_locations data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OneHundMB = 1000*1000*100 # create a bytes size number that will process a query less than or equal to 100MB\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed = OneHundMB) \n#set a safe congifuration number so that we don't go over 100MB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a query with a list of movies from each production company before 1950, from highest number to lowest amount of movies.\nquery2 = \"\"\"\n    SELECT \n        COUNT(title) AS Movie_Count,  \n        production_company,\n    FROM `bigquery-public-data.san_francisco.film_locations`\n    WHERE \n        release_year < 1950\n    GROUP BY \n        production_company\n    ORDER BY\n        Movie_Count DESC\n\n\"\"\"\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed = OneHundMB) #need to re-run safe_config line everytime you creat a new query\nfilmLocations = client.query(query2, job_config=safe_config) #set safe config\nfilmLocations.to_dataframe() #output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**e.)  From the table sfpd_incidents, produce a table of the most common police incidents in the category of LARCENY/THEFT in the year 2016 ordered from most to least common.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"table_ref_sfpd_incidents = dataset_ref.table('sfpd_incidents') # look at sfpd_incidents table\nsfpdIncidents = client.get_table(table_ref_sfpd_incidents)\nclient.list_rows(sfpdIncidents, max_results = 6).to_dataframe() # look at 6 rows of the sfpd_incidents data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This query will produce a table of the most common police incidents in the category of LARCENY/THEFT \n# in the year 2016 ordered from most to least common.\nquery3 = \"\"\"\n    SELECT\n        descript,\n        COUNT(descript) AS Num_Incidences\n    FROM `bigquery-public-data.san_francisco.sfpd_incidents`\n    WHERE \n        EXTRACT(YEAR FROM timestamp) = 2016\n        AND\n        category = 'LARCENY/THEFT'\n    GROUP BY \n        descript\n    ORDER BY \n        COUNT(descript) DESC\n\n\"\"\"\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed = OneHundMB*10)\nsfpdIncidents = client.query(query3, job_config=safe_config)\nsfpdIncidents.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5.)  For this question, I would like you to use Pandas and Python to retrieve and clean a dataset according the following instructions.  As usual, please provide code that is structured and commented according to the CST style guide posted on the course webpage.**"},{"metadata":{},"cell_type":"markdown","source":"**a.) Using the Kaggle dataset on Melborne_housing_FULL.csv, start by reading and describing this dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/melbourne-houses/Melbourne_housing_FULL.csv\")#read in dataset as df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape #look at shape of df\ndf.describe() #overview of content in df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**b.)  Using the iloc method, produce the last 10 rows of the dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[-10:] #select last ten rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**c.)  There are columns with missing data in this dataset.  Clean the dataset by removing all rows with missing values.  (This is a BAD idea, but it is where we start).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna() #drops rows with missng values\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**d.)  Using python code, produce rows where the Suburb is in Albert Park and the house has 3 or more bedrooms and sold in 2017.  Print the 5 observations with lowest price where the price is not null.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#need to split up the 'Date' column since it is not timestamped\ndf.Date.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add three new columns Month, Day, and Year to the end of the dataset\ndf[['Month','Day','Year']] = df['Date'].str.split('/',expand=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a new df with the problem's conditions\nsub_Albert = df.loc[(df.Suburb == 'Albert Park') & (df.Rooms >= 3) & (df.Year == '2017')] \nsub_Albert.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**e.)  Produce a list of all the unique Sellers.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SellerG.unique() #show unique sellers","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}