{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom bs4 import BeautifulSoup  \nimport re\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.stem import SnowballStemmer\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport logging\nfrom gensim.models import word2vec\nfrom gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import KeyedVectors\n\ndf = pd.read_csv('../input/Amazon_Unlocked_Mobile.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(\"Summary statistics of numerical features : \\n\", df.describe())\n\nprint(\"\\nTotal number of reviews: \",len(df))\nprint(\"\\nTotal number of brands: \", len(list(set(df['Brand Name']))))\nprint(\"\\nTotal number of unique products: \", len(list(set(df['Product Name']))))\n\nprint(\"\\nPercentage of reviews with neutral sentiment : {:.2f}%\"\\\n      .format(df[df['Rating']==3][\"Reviews\"].count()/len(df)*100))\nprint(\"\\nPercentage of reviews with positive sentiment : {:.2f}%\"\\\n      .format(df[df['Rating']>3][\"Reviews\"].count()/len(df)*100))\nprint(\"\\nPercentage of reviews with negative sentiment : {:.2f}%\"\\\n      .format(df[df['Rating']<3][\"Reviews\"].count()/len(df)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ddc178a1534970aabc0ffdb312997517b18379c"},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf['Rating'].value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of Rating')\nplt.xlabel('Rating')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"024ed12163957cc913a9a7b30afd10a2bd29bad7"},"cell_type":"code","source":"brands = df[\"Brand Name\"].value_counts()\nplt.figure(figsize=(12,8))\nbrands[:20].plot(kind='bar')\nplt.title(\"Number of Reviews for Top 20 Brands\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e35f37e398ca55714a6f17cae5a782784594913"},"cell_type":"code","source":"df = df.sample(frac=0.1, random_state=0) #uncomment if needed\n\ndf.dropna(inplace=True)\n\ndf = df[df['Rating'] != 3]\n\ndf['Sentiment'] = np.where(df['Rating'] > 3, 1, 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9670b521ad2f2c9a5641cea09f663c94b7f622a"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], df['Sentiment'], \\\n                                                    test_size=0.1, random_state=0)\n\nprint('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adceca469fff69fe635998366d44b70d8c264982"},"cell_type":"code","source":"def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n             ):\n    '''\n    Convert a raw review to a cleaned review\n    '''\n    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n    words = letters_only.lower().split() # convert to lower case \n    \n    if remove_stopwords: # remove stopword\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n        \n    if stemming==True:\n        stemmer = SnowballStemmer('english') \n        words = [stemmer.stem(w) for w in words]\n        \n    if split_text==True: \n        return (words)\n    \n    return( \" \".join(words))\n\n# Preprocess text data in training set and validation set\nX_train_cleaned = []\nX_test_cleaned = []\n\nfor d in X_train:\n    X_train_cleaned.append(cleanText(d))\nprint('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n    \nfor d in X_test:\n    X_test_cleaned.append(cleanText(d))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afff2b19e7f3690484c989e1850d605ae299eff7"},"cell_type":"code","source":"# Fit and transform the training data to a document-term matrix using CountVectorizer\ncountVect = CountVectorizer() \nX_train_countVect = countVect.fit_transform(X_train_cleaned)\nprint(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #6378 \nprint(\"Show some feature names : \\n\", countVect.get_feature_names()[::100])\n\n# Train MultinomialNB classifier\nmnb = MultinomialNB()\nmnb.fit(X_train_countVect, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d87879c30c9df00aa16d44606aab606974d1e1eb"},"cell_type":"code","source":"def modelEvaluation(predictions):\n    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n    #print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))\n    \npredictions = mnb.predict(countVect.transform(X_test_cleaned))\nmodelEvaluation(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44d9f4a05183f8116d3256ca8866e8627663ead4"},"cell_type":"code","source":"# Fit and transform the training data to a document-term matrix using TfidfVectorizer \ntfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\nX_train_tfidf = tfidf.fit_transform(X_train)\nprint(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\nprint(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n\n# Logistic Regression\nlr = LogisticRegression()\nlr.fit(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67033843e807614048d66a8c7ac9a8b1c54dbd8f"},"cell_type":"code","source":"# Look at the top 10 features with smallest and the largest coefficients\nfeature_names = np.array(tfidf.get_feature_names())\nsorted_coef_index = lr.coef_[0].argsort()\nprint('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a20bbe3deae09026be0a91e088160ba7737ac468"},"cell_type":"code","source":"# Evluate on the validaton set\npredictions = lr.predict(tfidf.transform(X_test_cleaned))\nmodelEvaluation(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8af0ea309a2fde07a18956f17d3b8e9006f11f8"},"cell_type":"code","source":"# Split review text into parsed sentences uisng NLTK's punkt tokenizer\n# nltk.download()\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\ndef parseSent(review, tokenizer, remove_stopwords=False):\n    '''\n    Parse text into sentences\n    '''\n    raw_sentences = tokenizer.tokenize(review.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n    return sentences\n\n\n# Parse each review in the training set into sentences\nsentences = []\nfor review in X_train_cleaned:\n    sentences += parseSent(review, tokenizer)\n    \nprint('%d parsed sentence in the training set\\n'  %len(sentences))\nprint('Show a parsed sentence in the training set : \\n',  sentences[10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9e9da16a0a1b831481e9b0aa9f87af6f71f2ccb"},"cell_type":"code","source":"# Fit parsed sentences to Word2Vec model \n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n\nnum_features = 300  #embedding dimension                     \nmin_word_count = 10                \nnum_workers = 4       \ncontext = 10                                                                                          \ndownsampling = 1e-3 \n\nprint(\"Training Word2Vec model ...\\n\")\nw2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n                 window = context, sample = downsampling)\nw2v.init_sims(replace=True)\nw2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n\nprint(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \nprint(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"701e7973d43c57c9414525002c0dd48f454252c8"},"cell_type":"code","source":"# Transfrom the training data into feature vectors\n\ndef makeFeatureVec(review, model, num_features):\n    '''\n    Transform a review to a feature vector by averaging feature vectors of words \n    appeared in that review and in the volcabulary list created\n    '''\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    nwords = 0.\n    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n    isZeroVec = True\n    for word in review:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            featureVec = np.add(featureVec, model[word])\n            isZeroVec = False\n    if isZeroVec == False:\n        featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    '''\n    Transform all reviews to feature vectors using makeFeatureVec()\n    '''\n    counter = 0\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    for review in reviews:\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n        counter = counter + 1\n    return reviewFeatureVecs\n\n# Get feature vectors for training set\nX_train_cleaned = []\nfor review in X_train:\n    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\ntrainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\nprint(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n\n\n# Get feature vectors for validation set\nX_test_cleaned = []\nfor review in X_test:\n    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\ntestVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\nprint(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10b241c881119ddc939729f2bd3776865f02cd64"},"cell_type":"code","source":"# Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(trainVector, y_train)\npredictions = rf.predict(testVector)\nmodelEvaluation(predictions)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}