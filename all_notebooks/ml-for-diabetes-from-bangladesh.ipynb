{"cells":[{"metadata":{"_uuid":"d3f30f1b586d36f01341f75870eb404df361d189"},"cell_type":"markdown","source":"# ML for Diabetes from Bangladesh \n![Imgur](https://i.imgur.com/tRErGUn.gif)"},{"metadata":{"_uuid":"28f60eff26e4db4e82f5df0c98eb41cf4c554dae"},"cell_type":"markdown","source":"## &#x1F310; &nbsp; Code Library, Style, and Links"},{"metadata":{"_uuid":"918bd109e53e4f8ac9448f2260afbcc1df511fea"},"cell_type":"markdown","source":"Useful `LINKS`:\n\n&#x1F4E1; &nbsp; [Pima Indians and Diabetes video](https://www.youtube.com/watch?v=pN4HqWRybwk)\n\n&#x1F4E1; &nbsp; [Pima Indians and Diabetes research paper](https://pdfs.semanticscholar.org/ef31/2e378325707b371c4727f6b1f9225fc03a9f.pdf)\n\n&#x1F4E1; &nbsp; [Pandas Visualization](https://pandas.pydata.org/pandas-docs/stable/visualization.html)\n\n&#x1F4E1; &nbsp; [Pandas Styling](https://pandas.pydata.org/pandas-docs/stable/style.html)"},{"metadata":{"_uuid":"caa8f4b4c16f3f8173b4bf07137865905bf35543"},"cell_type":"markdown","source":"## &#x1F4D8; &nbsp;  Introduction of Pima Indians and Diabetes \n"},{"metadata":{"_uuid":"18956a28c9af50a9fe4bbf6f0f0482ff08c023c8"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/x3rrQyy.png)"},{"metadata":{"_uuid":"81d08143cef47e2d582c73e13f99d83343102641"},"cell_type":"markdown","source":"*Diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.*\n\n*This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.*"},{"metadata":{"_uuid":"86be4a5f2e02c6eab89337a233aaefee06a53fc0"},"cell_type":"markdown","source":"## &#x1F4D8; &nbsp; Objective\n\nWe will try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?"},{"metadata":{"_uuid":"06cc04b1f7dd01e9622d5ab99eb5e54c8de833a8"},"cell_type":"markdown","source":"## &#x1F310; Data\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n* Pregnancies: Number of times pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skin fold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U/ml)\n* BMI: Body mass index (weight in kg/(height in m)^2)\n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: Class variable (0 or 1)"},{"metadata":{"trusted":true,"_uuid":"da980d6c9f0885c12fa1960247607312740a4aa2","collapsed":true},"cell_type":"code","source":"%%html\n<style> \n@import url('https://fonts.googleapis.com/css?family=Orbitron|Roboto');\nbody {background-color: gainsboro;} \na {color: #37c9e1; font-family: 'Roboto';} \nh1 {color: #37c9e1; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;} \nh2, h3 {color: slategray; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;}\nh4 {color: #818286; font-family: 'Roboto';}\nspan {text-shadow: 4px 4px 4px #aaa;}\ndiv.output_prompt, div.output_area pre {color: slategray;}\ndiv.input_prompt, div.output_subarea {color: #37c9e1;}      \ndiv.output_stderr pre {background-color: gainsboro;}  \ndiv.output_stderr {background-color: slategrey;}       \n</style>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8743646cea77e5f23914936aa037ffe269480dc0"},"cell_type":"markdown","source":"## &#x1F310; &nbsp; Load libarary "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport itertools\nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"style_dict = {'background-color':'slategray',\n              'color':'#37c9e1',\n              'border-color': 'white',\n              'font-family':'Roboto'}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85c545e729d399f898ae306994264d37bee6824e"},"cell_type":"markdown","source":"## &#x1F310; &nbsp; Load the Data"},{"metadata":{"trusted":true,"_uuid":"f201a0648c758d1cf0805b633831842e6e196d04","collapsed":true},"cell_type":"code","source":"diabetes = pd.read_csv('../input/diabetes.csv')\nprint(diabetes.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7055c5fae3c14a3283b00dd00c7cb4d5bc74ed2","collapsed":true},"cell_type":"code","source":"diabetes.head().style.set_properties(**style_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a188c8d03c68cdd68d6683539ab6767ece87573a","collapsed":true},"cell_type":"markdown","source":"### &#x1F310; &nbsp; The diabetes dataset consists of 768 data points, with 9 features each"},{"metadata":{"trusted":true,"_uuid":"334023862684d4e40417f053dddde96fd1cc5b2a","collapsed":true},"cell_type":"code","source":"print(\"dimension of diabetes data: {}\".format(diabetes.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e55a17a2cd4cc1a0bfe275cd3d274d6be153b0b"},"cell_type":"markdown","source":"### &#x1F310; Outcome 0 means No diabetes, outcome 1 means diabetes\nOf these 768 data points, 500 are labeled as 0 and 268 as 1:"},{"metadata":{"trusted":true,"_uuid":"f69c9d4872b3729c56796bb8550599cc3742f44a","collapsed":true},"cell_type":"code","source":"print(diabetes.groupby('Outcome').size())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da863bff3b40118e6631399ff439d18ae1a91e28"},"cell_type":"markdown","source":"###  &#x1F310; Basic EDA"},{"metadata":{"trusted":true,"_uuid":"89a6292412fd53ff53118d441eda310311dcc826","collapsed":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(diabetes['Outcome'],label=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af7ceddd72f6ef329d42fe47688f3c4295ce54ec"},"cell_type":"markdown","source":"### &#x1F310; Brief Analysis of the data "},{"metadata":{"trusted":true,"_uuid":"aebd1ded34e0e1dc65da18296f27b44b3522f092","collapsed":true},"cell_type":"code","source":"diabetes.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"815b43ebfc1f15e64ead839efcab75cb377885ff","collapsed":true},"cell_type":"code","source":"columns=diabetes.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    diabetes[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9768ac4b88f1a6da5550d9d655b23dec7bc454e7"},"cell_type":"markdown","source":"### &#x1F310; PairPlots: "},{"metadata":{"trusted":true,"_uuid":"6441646fe4f15b644deedbe177636e9148ffe2f0","collapsed":true},"cell_type":"code","source":"sns.pairplot(data=diabetes,hue='Outcome',diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7892dea57e86a5503278c2a21f5f52eaef81c6f7"},"cell_type":"markdown","source":"### &#x1F310; Observations:\n1)The diagonal shows the distribution of the the dataset with the kernel density plots.\n\n2)The scatter-plots shows the relation between each and every attribute or features taken pairwise. Looking at the scatter-plots, we can say that no two attributes are able to clearly seperate the two outcome"},{"metadata":{"_uuid":"91b019ef042d4a3fbf5eed4ab05beeb10befe1c3"},"cell_type":"markdown","source":"##  &#x1F310; Predictive Modeling with Visualization"},{"metadata":{"_uuid":"7c60d3f7816d0097347ae7ae00b856bf031a054d"},"cell_type":"markdown","source":"##  &#x1F310;  k-Nearest Neighbors \n"},{"metadata":{"_uuid":"d556ce1c53bf2cf3f0a04f50c1feed56a9f42a8f"},"cell_type":"markdown","source":"The k-NN algorithm is arguably the simplest machine learning algorithm. Building the model     consists only of storing the training dataset. To make a prediction for a new data point, the     algorithm finds the closest data points in the training dataset—its “nearest neighbors.”"},{"metadata":{"_uuid":"dcb64a102804adf8e8e282e96efe6cb8c8c2b59b"},"cell_type":"markdown","source":"### &#x1F310; Let’s investigate whether we can confirm the connection between model complexity and accuracy"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c3aef3e158b034c8aa9e10b219eea4c3063ff2e3"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(diabetes.loc[:, diabetes.columns != 'Outcome'], \n                                                    diabetes['Outcome'], stratify=diabetes['Outcome'], random_state=66)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ba5a9c27e8b5bd0d0f9977a68dd0ca4eefe50cf"},"cell_type":"markdown","source":"###  &#x1F310; Visualization of model and accuracy "},{"metadata":{"trusted":true,"_uuid":"84b876c1cce5249e7d8d01da0b1829a3e3c8d8c6","collapsed":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n    # build the model\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(knn.score(X_train, y_train))\n    # record test set accuracy\n    test_accuracy.append(knn.score(X_test, y_test))\n\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nplt.savefig('knn_compare_model')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ffbcb570b1b5b45791b140c30066dd5a1bfdefd"},"cell_type":"markdown","source":"The plot shows the training and test set accuracy on the y-axis against the setting of n_neighbors on the x-axis. "},{"metadata":{"_uuid":"eaca1243489b83accf8de339ba2336303bcbce68"},"cell_type":"markdown","source":"The above plot suggests that we should choose n_neighbors=9. Here we are:"},{"metadata":{"trusted":true,"_uuid":"c1c50c0a0d86e74c07cf56d6a84514deb99f32c0","collapsed":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train, y_train)\n\nprint('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62074ca82b4987a8bc6468e1c323e17fdcb14bcc"},"cell_type":"markdown","source":"##  &#x1F310; Logistic Regression"},{"metadata":{"_uuid":"245a45130bff50cd966a51354cc4dbe3dc66ad28"},"cell_type":"markdown","source":"&#x1F310;  **Logistic regression is one of the most common classification algorithms.**"},{"metadata":{"trusted":true,"_uuid":"91307a76c663a1de6691b308cd7bc9945596e152","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8a9ad86a5087a6f371f4b6b2cdb066d60de5af7"},"cell_type":"markdown","source":"&#x1F310;  <b>The default value of C=1 provides with 78% accuracy on training and 77% accuracy on test set. "},{"metadata":{"trusted":true,"_uuid":"298674053cb2afa0a9d89b37407545d1372f302c","collapsed":true},"cell_type":"code","source":"logreg1 = LogisticRegression(C=0.01).fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg1.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg1.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840e0c3fc64a64afea59d718cd23f224200aad7d"},"cell_type":"markdown","source":"&#x1F310;  <b>Using C=0.01 results in lower accuracy on both the training and the test sets."},{"metadata":{"trusted":true,"_uuid":"c82f3326d3118251db116061d2fd6894cc073324","collapsed":true},"cell_type":"code","source":"logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg100.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e8f221217c5e8326b060e089af56506ba49203b"},"cell_type":"markdown","source":"###  &#x1F310; Vizualization of  Logistic Regression "},{"metadata":{"trusted":true,"_uuid":"e8890e4666e4c16b701ce563b8d9f81c60985a6f","collapsed":true},"cell_type":"code","source":"diabetes_features = [x for i,x in enumerate(diabetes.columns) if i!=8]\nplt.figure(figsize=(8,6))\nplt.plot(logreg.coef_.T, 'o', label=\"C=1\")\nplt.plot(logreg100.coef_.T, '^', label=\"C=100\")\nplt.plot(logreg1.coef_.T, 'v', label=\"C=0.001\")\nplt.xticks(range(diabetes.shape[1]), diabetes_features, rotation=90)\nplt.hlines(0, 0, diabetes.shape[1])\nplt.ylim(-5, 5)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.legend()\nplt.savefig('log_coef')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a06d78798cb6842ad79e06cddbac58a4e146d7fe"},"cell_type":"markdown","source":"##  &#x1F310; Decision Tree"},{"metadata":{"trusted":true,"_uuid":"3001a547b107bccf92840b197f5eb49e77aa2969","collapsed":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2af2ef5bcd9f4d8f2d215a9a1399ee9f421937f7"},"cell_type":"markdown","source":"<b> Observations:</b>\n\nThe accuracy on the training set is 100%, while the test set accuracy is much worse. This is an indicative that the tree is overfitting and not generalizing well to new data. Therefore, we need to apply pre-pruning to the tree.\n\nWe set max_depth=3, limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set."},{"metadata":{"trusted":true,"_uuid":"ac9d2f3efd9430647fb6d5fe4665ea26b777ad32","collapsed":true},"cell_type":"code","source":"tree = DecisionTreeClassifier(max_depth=3, random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"463991330a29d4eeace816b25b3aa03021646d92"},"cell_type":"markdown","source":"##  &#x1F310;  Feature importance in Decision trees"},{"metadata":{"_uuid":"7d2e23e8d8fc282dc3d0044404623b59e49fd475"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/ziXuIjp.jpg)"},{"metadata":{"_uuid":"fa5de6059a30ecbd2a8b8407ecc761c9726de659"},"cell_type":"markdown","source":"Feature importance rates how important each feature is for the decision a tree makes. It is a number between 0 and 1 for each feature, where 0 means “not used at all” and 1 means “perfectly predicts the target.” The feature importances always sum to 1:"},{"metadata":{"trusted":true,"_uuid":"bacb06566547e582a0f2249b5befa66dd6803edb","collapsed":true},"cell_type":"code","source":"print(\"Feature importances:\\n{}\".format(tree.feature_importances_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cce3d8ebe16a833194ceee204c10cb340f0513ca"},"cell_type":"markdown","source":"###  &#x1F310; Vizualization of Feature importance "},{"metadata":{"trusted":true,"_uuid":"0f58cc03d2c1b152b5cb9c0633e442e4fdcaaaee","collapsed":true},"cell_type":"code","source":"def plot_feature_importances_diabetes(model):\n    plt.figure(figsize=(8,6))\n    n_features = 8\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), diabetes_features)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)\n\nplot_feature_importances_diabetes(tree)\nplt.savefig('feature_importance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"477065c4c7ba39aa23321e687dc44692acd4e419"},"cell_type":"markdown","source":"<b> Observations:</b>\nFeature \"Glucose\" is by far the most important feature."},{"metadata":{"_uuid":"3e5cbe1b9c560776f862cf210a91d5e650be873e"},"cell_type":"markdown","source":"##  &#x1F310;  Random Forest "},{"metadata":{"_uuid":"a836184d1968cbac06d0e632c134094fe302ed0a"},"cell_type":"markdown","source":"let’s apply a random forest consisting of 100 trees on the diabetes dataset:"},{"metadata":{"trusted":true,"_uuid":"2e2ab2e324b1e81eca8e4c0df388d2dea657c969","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\nrf.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae98c0033436d602cc7a5511fb529d94c39fbc88"},"cell_type":"markdown","source":"<b> Observations:</b>\nThe random forest gives us an accuracy of 78.6%, better than the logistic regression model or a single decision tree, without tuning any parameters. "},{"metadata":{"trusted":true,"_uuid":"bb99a87445c419805d4844c8f46816ec4c392eb8","collapsed":true},"cell_type":"code","source":"rf1 = RandomForestClassifier(max_depth=3, n_estimators=100, random_state=0)\nrf1.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf1.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf1.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dccfaa2183b901cd3ef092de840fe019d5fc4fcd"},"cell_type":"markdown","source":"## &#x1F310; Vizualization of Feature importance in Random Forest"},{"metadata":{"trusted":true,"_uuid":"ac17af4d1bfeb2e833f4c9c9f220bab1601d6b68","collapsed":true},"cell_type":"code","source":"plot_feature_importances_diabetes(rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53d5dc1da1db6f86f4cfe8dbe6654f8f2858c899"},"cell_type":"markdown","source":"<b> Observations:</b>\nSimilarly to the single decision tree, the random forest also gives a lot of importance to the “Glucose” feature, but it also chooses “BMI” to be the 2nd most informative feature overall. The randomness in building the random forest forces the algorithm to consider many possible explanations, the result being that the random forest captures a much broader picture of the data than a single tree."},{"metadata":{"_uuid":"d9b7f97e2b96115081c5a51d63e5cbe1c0915004"},"cell_type":"markdown","source":"## &#x1F310; Gradient Boosting"},{"metadata":{"trusted":true,"_uuid":"da7927388e26c8adde874660d58960af5694e2fc","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(random_state=0)\ngb.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gb.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gb.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7852f353159b500691f470db2cff757d13767f8","collapsed":true},"cell_type":"code","source":"gb1 = GradientBoostingClassifier(random_state=0, max_depth=1)\ngb1.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gb1.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gb1.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01a7e4787a91a529e97c2efc15938815b71c3f9e","collapsed":true},"cell_type":"code","source":"gb2 = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\ngb2.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gb2.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gb2.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b5f5328cae841717726e1722d37d4e3af11a187"},"cell_type":"markdown","source":"<b> Observations:</b>\nBoth methods of decreasing the model complexity reduced the training set accuracy, as expected. In this case, none of these methods increased the generalization performance of the test set."},{"metadata":{"_uuid":"b980bd46f7c8c7c75e20b31a449c1e2f4dcf6660"},"cell_type":"markdown","source":"## &#x1F310; Vizualization of Feature importance in Gradient Boosting "},{"metadata":{"_uuid":"c8aac708de313d7f568df3a507b5c7e9c9296aa2"},"cell_type":"markdown","source":"<b>Short note:</b>\nWe can visualize the feature importances to get more insight into our model even though we are not really happy with the model."},{"metadata":{"trusted":true,"_uuid":"7e82a20530554cb8c61862f572af02325698e023","collapsed":true},"cell_type":"code","source":"plot_feature_importances_diabetes(gb1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40fe2ac913ae028aae39f9375067157fbd1aa892"},"cell_type":"markdown","source":"## &#x1F310; Neural Networks"},{"metadata":{"trusted":true,"_uuid":"bc22a286fd54f704c195e38efc315a0651fc615b","collapsed":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(random_state=42)\nmlp.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8812122ee0ca8e594b220c6fd4eb16bf0763a77"},"cell_type":"markdown","source":"<b> Observations:</b>\n\nThe accuracy of the MLP is not as good as the other models at all, this is likely due to scaling of the data. Neural networks also expect all input features to vary in a similar way, and ideally to have a mean of 0, and a variance of 1. "},{"metadata":{"trusted":true,"_uuid":"967e55325f98f4a44e2382cda66bb66feaeba6a6","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)\nmlp = MLPClassifier(random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7332b53bde75161ffdb412819e740d7e054f5b84","collapsed":true},"cell_type":"code","source":"mlp = MLPClassifier(max_iter=1000, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e5d7d9a1d6abf50c9b6d6ef8ecd875509d61768","collapsed":true},"cell_type":"code","source":"mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"100d7f1e6af541e025c3935e6cbdd2140ae09c7f"},"cell_type":"markdown","source":"## &#x1F310; Vizualization of Neural Networks "},{"metadata":{"trusted":true,"_uuid":"0c6b1771d232190faace9a4661343cacc856fb1f","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\nplt.yticks(range(8), diabetes_features)\nplt.xlabel(\"Columns in weight matrix\")\nplt.ylabel(\"Input feature\")\nplt.colorbar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31ad452eb897ba7bf93b54eeccb9d08d541c66c6"},"cell_type":"markdown","source":"<b> Observations:</b> \n\nFrom the heat map, it is not easy to point out quickly that which feature (features) have relatively low weights compared to the other features."},{"metadata":{"_uuid":"a565462ad87478d38d140cb1c81fd5c14d91f572"},"cell_type":"markdown","source":"## &#x1F4D8; Summary\n![Imgur](https://i.imgur.com/RISfOXB.jpg)\n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}