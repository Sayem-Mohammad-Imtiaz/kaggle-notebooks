{"cells":[{"metadata":{"_uuid":"5df31c603dd75d38793692236eeeba4e057cd189","_cell_guid":"407a3559-ba70-406f-878c-9b3bcccd84d0"},"cell_type":"markdown","source":"## 用决策树来分类贷款是否优良"},{"metadata":{"_uuid":"17706883ba03560cb2e39bd2ad62cba4675e3ee8","_cell_guid":"94bd01f9-2a23-44d1-b6e4-063a00cedb6c"},"cell_type":"markdown","source":"[LendingClub](https://www.lendingclub.com/) 是一家贷款公司. 在本次作业中,我们需要手动实现决策树来预测一份贷款是否安全，并对比不同复杂度下决策树的表现"},{"metadata":{"_uuid":"b52f0ce13e088aa9488baaf9b09a539988824774","_cell_guid":"4302e923-05aa-4ea1-9fa6-1ff39f5a253b","collapsed":true,"trusted":false},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe1191e114bbf7353f13bb9792266a2b0713a1e2","_cell_guid":"c498431b-8124-40c5-9be4-da8cddabf724"},"cell_type":"markdown","source":"## 读取数据"},{"metadata":{"_uuid":"7044fc9ec1ac2a3aeefe9d553e398caebe00d839","_cell_guid":"256160ff-42d3-420c-9fef-bb61eb69976c","collapsed":true,"trusted":false},"cell_type":"code","source":"#data = pd.read_csv(os.path.join(\"data\", \"loan_sub.csv\"), sep=',')\ndata = pd.read_csv(os.path.join(\"../input\", \"loan_sub.csv\"), sep=',')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17d2e96acc79be8f51051ff71cc804caadcae9b2","_cell_guid":"68c4cbf0-18b4-4594-bce1-735bda545ba3"},"cell_type":"markdown","source":"## 打印可用特征"},{"metadata":{"_uuid":"a9c1f4cc4249ee4ce672f6dc0e081d1018f3b20f","_cell_guid":"87d43b64-cf0a-49bb-968d-7c93e21cd04b","collapsed":true,"trusted":false},"cell_type":"code","source":"data.bad_loans.dtype","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23a50a996d5cc564b0a248508701d7c38c923019","_cell_guid":"37af24da-fae4-4379-9c1b-0feb4ae43322"},"cell_type":"markdown","source":"## 预处理预测目标\n\n预测目标是一列'bad_loans'的数据。其中**1**表示的是不良贷款，**0**表示的是优质贷款。\n\n将预测目标处理成更符合直觉的标签，创建一列 `safe_loans`. 并且: \n\n* **+1** 表示优质贷款, \n* **-1** 表示不良贷款. "},{"metadata":{"_uuid":"c97e524bdc835ef5cc9d9567a53bce69c2462d88","_cell_guid":"0e643ff3-3ffb-4744-afb5-131ec14a7ed0","collapsed":true,"trusted":false},"cell_type":"code","source":"# safe_loans =  1 => safe\n# safe_loans = -1 => risky\n#TODO\ndf1 = pd.DataFrame({'safe_loans' : data.bad_loans.apply(lambda x : +1 if x==0 else -1)})\ndata = pd.concat([data, df1], axis = 1)\ndata = data.drop('bad_loans', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"956af464c8d6f8340a0d14b8a1adb6647a28fca4","_cell_guid":"78e4445a-931e-4905-b772-e46cae985a32"},"cell_type":"markdown","source":"## 打印优质贷款与不良贷款的比例"},{"metadata":{"_uuid":"58781c2a2022c5ac519705fb464e36bd6d33162f","_cell_guid":"1db03d84-2c54-4460-ae03-62d99639c4ee","collapsed":true,"trusted":false},"cell_type":"code","source":"print(data.head())\ndata.drop_duplicates()\ndata['safe_loans'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4db5078f21aa0b68287cc6687cfff62eee254cd","_cell_guid":"5d9bff3e-1e76-418c-ae5b-62594b9422f7"},"cell_type":"markdown","source":"#### 这是一个不平衡数据， 好的贷款远比坏的贷款要多. "},{"metadata":{"_uuid":"4bead2914b6d9200e7f54c95f1dc4cfef247ec4e","_cell_guid":"bb3019ac-82ac-4464-9f94-3ac2d4291f30"},"cell_type":"markdown","source":"## 选取用于预测的特征"},{"metadata":{"_uuid":"f16f2ce0e1278023c11814f30c2ef01cd02c0e16","_cell_guid":"f1805231-5883-49c7-9e3e-5bee2f7fc956","scrolled":true,"collapsed":true,"trusted":false},"cell_type":"code","source":"cols = ['grade', 'term','home_ownership', 'emp_length']\ntarget = 'safe_loans'\n\ndata = data[cols + [target]]#TODO\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e19dfcb51e380d0585e372029a99c77e6bba59b3","_cell_guid":"98338989-bab9-429c-b558-b04ebd1401f3"},"cell_type":"markdown","source":"## 创建更为平衡的数据集  \n\n* 对占多数的标签进行下采样  \n* 注意有很多方法处理不平衡数据，下采样只是其中之一"},{"metadata":{"_uuid":"51dca67fec51e2cc260c0ede9c8032544fda3195","_cell_guid":"50c6b4e7-6ef4-4831-bd5c-11aa9ddb036c","collapsed":true,"trusted":false},"cell_type":"code","source":"data['safe_loans'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e9e4076f543aefe1b7e86a93470d99205c3a71f","_cell_guid":"04a4f10b-385f-4328-84f4-48f2dbc5f87d","collapsed":true,"trusted":false},"cell_type":"code","source":"\n# use the percentage of bad and good loans to undersample the safe loans.\nbad_ones = data[data[target] == -1]\nsafe_ones = data[data[target] == 1]\npercentage = len(bad_ones)/float(len(safe_ones))#TODO\n\nrisky_loans = bad_ones\n# Important, this step is to balance out the sample\nsafe_loans = safe_ones.sample(frac = percentage, random_state = 33)#TODO\nprint(len(risky_loans))\nprint(len(safe_loans))\n# # combine two kinds of loans\ndata_set = pd.concat([risky_loans, safe_loans], axis = 0)#TODO","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a62a030d4225aedddc2184edcbd52db832391ce","_cell_guid":"8a2e97a1-9925-4897-b5a0-dd9663b8e2a3"},"cell_type":"markdown","source":"Now, let's verify that the resulting percentage of safe and risky loans are each nearly 50%."},{"metadata":{"_uuid":"4f4699a58c1a6105a68245db67c1f96c5e0245f0","_cell_guid":"4d98266e-603d-4425-a588-e15df693c1f2","collapsed":true,"trusted":false},"cell_type":"code","source":"data_set[target].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39dffd3c49dea40545d1eccdf78060e4b47d3fbf","_cell_guid":"63ef6f6a-9cdd-49cb-96aa-dcbcdd709461"},"cell_type":"markdown","source":"## Preprocessing your features"},{"metadata":{"_uuid":"9d3fd72e15633094f06183aaa6bda157ca3d1303","_cell_guid":"9b663c5c-a82e-42a7-8f41-374c953b32fe","collapsed":true,"trusted":false},"cell_type":"code","source":"def dummies(data, columns=['pclass','name_title','embarked', 'sex']):\n    for col in columns:\n        data[col] = data[col].apply(lambda x: str(x))\n        new_cols = [col + '_' + i for i in data[col].unique()]\n        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)[new_cols]], axis=1)\n        del data[col]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07fc7fcfc42fd2332d846e323862823c29602047","_cell_guid":"0396371d-93da-4827-bf0b-8c6c4e24ada9","collapsed":true,"trusted":false},"cell_type":"code","source":"#grade, home_ownership, target\ncols = ['grade', 'term','home_ownership', 'emp_length']\ndata_set = dummies(data_set, columns=cols)\ndata_set.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"758594d8eb40c978ff84262708741ebd40fb56b6","_cell_guid":"47788626-7337-4f28-a6f1-b5e975c1d6fc"},"cell_type":"markdown","source":"## 将数据分成训练集和测试集"},{"metadata":{"_uuid":"17675e6251b913d2d2ee0f3aebb51b4904619ad7","_cell_guid":"b86babc2-61d5-4af9-8e89-0f55e10f8782"},"cell_type":"markdown","source":"重要的事情说三遍!!  \n\n**把你的爪子从TEST DATA上拿开!!**   \n**把你的爪子从TEST DATA上拿开!!**  \n**把你的爪子从TEST DATA上拿开!!**  \n"},{"metadata":{"_uuid":"6bd672d511e95d9f6d153f89710dfc944ecce641","_cell_guid":"284aca19-721e-43d8-933f-349b1bf2263a","collapsed":true,"trusted":false},"cell_type":"code","source":"train_data, test_data = train_test_split(data_set, test_size = 0.2, random_state = 0)#TODO\ntrainX, trainY = train_data[train_data.columns[1:]], pd.DataFrame(train_data[target])#TODO\ntestX, testY = test_data[test_data.columns[1:]], pd.DataFrame(test_data[target])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"736020935f2584402dcf6e632ec2a35837bed581","_cell_guid":"e91d0a90-602f-4a60-8776-ea0c456d3d66","collapsed":true,"trusted":false},"cell_type":"code","source":"train_data.safe_loans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e90801bb1e53a778749e2f53b50a63682c51827","_cell_guid":"a2baf96c-f790-4f8f-b3c4-b19767a52be7"},"cell_type":"markdown","source":"## 建自己的决策树!  \n\n任务：  \n1 实现根据error来选择最佳划分特征的函数best_split()  \n2 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n3 实现树节点的类TreeNode  \n4 实现模型的类MyDecisionTree  "},{"metadata":{"_uuid":"2d4997336da3d66e289652f9bb6a87544e77912c","_cell_guid":"da51dc06-637f-4f50-a8dc-39daaff97447"},"cell_type":"markdown","source":"#### 任务1， 实现根据error来选择最佳划分特征的函数best_split()  \n约定树的左边对应target == 0， 树的右边对应target == 1"},{"metadata":{"_uuid":"d9b1496e0577033d6375cb0efff6caa72cc1e7c7","_cell_guid":"8e4193a2-cf2b-4d57-baf9-e2743281ebba","collapsed":true,"trusted":false},"cell_type":"code","source":"def count_errors(labels_in_node):\n    if len(labels_in_node) == 0:\n        return 0\n    \n    positive_ones = labels_in_node.apply(lambda x: x== 1).sum()#TODO\n    negative_ones = labels_in_node.apply(lambda x: x== -1).sum() #TODO\n    \n    return min(positive_ones, negative_ones) # TODO\n\n\ndef best_split(data, features, target):\n    # return the best feature\n    best_feature = None\n    best_error = 2.0 \n    num_data_points = float(len(data))  \n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split = data[data[feature] == 0]# TODO\n        \n        # 右分支对应当前特征为1的数据点\n        right_split = data[data[feature] == 1]#TODO\n        \n        # 计算左边分支里犯了多少错\n        left_misses = count_errors(left_split[target])#TODO            \n\n        # 计算右边分支里犯了多少错\n        right_misses = count_errors(right_split[target]) #TODO\n            \n        # 计算当前划分之后的分类犯错率\n        error = (right_misses + left_misses) *1.0 /len(num_data_points)#TODO\n\n        # 更新应选特征和错误率，注意错误越低说明该特征越好\n        if error < best_error:\n            best_error = error#TODO\n            best_feature = feature#TODO\n    return best_feature","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12daddfd025b340eedd431a0009d02cd2f4f9835","_cell_guid":"fc268c1f-7070-4f4f-ab0d-c7d3b232a9a9"},"cell_type":"markdown","source":"#### 任务2， 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n"},{"metadata":{"_uuid":"da192acb46cf54f16c2ec96929f1ce7e586831d2","_cell_guid":"4db7c136-8f69-4d20-a07a-f886868550bb","collapsed":true,"trusted":false},"cell_type":"code","source":"def entropy(labels_in_node):\n    n = len(labels_in_node)\n    s1 = (labels_in_node==1).sum()\n    if s1 == 0 or s1 == n:\n        return 0\n    p1 = float(s1)/n\n    \n    p0 = 1- p1\n    return -p0*np.log2(p0)- p1*np.log2(p1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6ad2f8e15bd3446781f4134d37868ed79a4771e","_cell_guid":"94dc545b-f8d7-4a81-af97-6c6fc9a7a944","collapsed":true,"trusted":false},"cell_type":"code","source":"def best_split_entropy(data, features, target):\n    \n    best_feature = None\n    best_info_gain = float('-inf') \n    num_data_points = float(len(data))\n    # 计算划分之前数据集的整体熵值\n    entropy_original = entropy(data[target])#TODO\n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split = data[data[feature] == 0]#TODO\n        \n        # 右分支对应当前特征为1的数据点\n        right_split = data[data[feature] == 1]#TODO \n        \n        # 计算左边分支的熵值\n        left_entropy = entropy(left_split[target])#TODO           \n\n        # 计算右边分支的熵值\n        right_entropy = entropy(right_split[target])#TODO\n            \n        # 计算左边分支与右分支熵值的加权和（数据集划分后的熵值）\n        entropy_split = len(left_split)/ num_data_points * left_entropy + len(right_split)/num_data_points*right_entropy#TODO\n        \n        # 计算划分前与划分后的熵值差得到信息增益\n        info_gain = entropy_original - entropy_split #TODO\n\n        # 更新最佳特征和对应的信息增益的值\n        if info_gain > best_info_gain:\n            best_info_gain = info_gain\n            best_feature = feature\n    return best_feature\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5ff55a069b38e6adc51cd9ab1769f2dfd9bc866","_cell_guid":"675a98e0-594e-4b74-84a8-0adf54719a67","collapsed":true,"trusted":false},"cell_type":"code","source":"best_split_entropy(train_data, trainX.columns, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1054a895ce42c3a5b067a8eebfaa879934d72eff","_cell_guid":"9ea4dc92-964b-445a-bb69-c5e9416144b9"},"cell_type":"markdown","source":"#### 任务3，实现树节点的类TreeNode，每个树节点应该包含如下信息:  \n\n   3.1 is_leaf: True/False  表示当前节点是否为叶子节点  \n   \n   3.2 prediction: 当前节点做全民公投的预测结果\n   \n   3.3 left: 左子树  \n   \n   3.4 right: 右子树 \n   \n   3.5 split_feature: 当前节点用来划分数据时所采用的特征"},{"metadata":{"_uuid":"db3fd70be8a7e5e5faea5f931d0d7d595e2c58d0","_cell_guid":"226b379b-83c0-42aa-95d2-2b655e26f457","collapsed":true,"trusted":false},"cell_type":"code","source":"class TreeNode:\n    def __init__(self, is_leaf, prediction, split_feature):\n    # TODO\n        self.is_leaf = is_leaf\n        self.prediciton = prediction\n        self.split_feature = split_feature\n        self.left = None\n        self.right = None       ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34826c20fad54475201c106fa83367e8c752eafc","_cell_guid":"3c1af8b0-1c98-4ad0-90e3-b5f64e283ef6"},"cell_type":"markdown","source":"#### 任务4，实现模型的类MyDecisionTree， 实现如下主要函数  \n  \n  \n   4.1 fit(): 模型在训练集上的学习  \n   \n   4.2 predict(): 模型在数据集上的预测\n   \n   4.3 score(): 模型在测试集上的得分   \n   \n   \n   \n   \n   为了实现4.1 - 4.3的方法， 需要实现如下辅助函数  \n   4.4 create_tree(): 创建一棵树  \n   \n   4.5 create_leaf(): 创建叶子节点  \n   \n   4.6 predict_single_data(): 模型预测单个数据  \n   \n   4.7 count_leaves(): 统计模型的叶子数"},{"metadata":{"_uuid":"5dfd561a64d43ca8972a33919871a0565ac6fb4f","_cell_guid":"58c5e4f0-29cc-4b58-9ddf-38f243adf59a","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7a852a0ccbc6f6d88cc6e3d1c9c9e038062a94f","_cell_guid":"f217094a-a608-43c6-a3b6-a8b5de2b6ce3","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.metrics import accuracy_score\nclass MyDecisionTree(BaseEstimator):\n    \n    def __init__(self, max_depth, min_error):\n        self.max_depth = max_depth\n        self.min_error = min_error\n    \n    def fit(self, X, Y, data_weights = None):\n        \n        data_set = pd.concat([X, Y], axis=1)\n        features = X.columns#TODO\n        target = Y.columns[0]\n        # need to get teh depth\n        self.root_node = self.create_tree(data_set, features, target, current_depth=0, max_depth = self.max_depth, min_error= self.min_error)# TODO\n        \n        \n    def predict(self, X):\n        prediction = X.apply(lambda row: self.predict_single_data(self.root_node, row), axis=1)\n        return prediction\n        \n        \n    def score(self, testX, testY):\n        target = testY.columns[0]# TODO\n        result = self.predict(testX)\n        return accuracy_score(testY[target], result)\n    \n    \n    def create_tree(self, data, features, target, current_depth = 0, max_depth = 10, min_error=0):\n        \"\"\"\n        探索三种不同的终止划分数据集的条件  \n  \n        termination 1, 当错误率降到min_error以下, 终止划分并返回叶子节点  \n        termination 2, 当特征都用完了, 终止划分并返回叶子节点  \n        termination 3, 当树的深度等于最大max_depth时, 终止划分并返回叶子节点\n        \"\"\"        \n    \n        # 拷贝以下可用特征\n        remaining_features = features[:]#TODO\n        target_values = data[target]# TODO\n\n        # termination 1\n        if count_errors(target_values) <= min_error:\n            print(\"Termination 1 reached.\")     \n            return self.create_leaf(target_values)# TODO\n\n        # termination 2\n        if len(remaining_features) == 0:\n            print(\"Termination 2 reached.\")    \n            return self.create_leaf(target_values)# TODO    \n\n        # termination 3\n        if current_depth >= max_depth: \n            print(\"Termination 3 reached.\")\n            return self.create_leaf(target_values)# TODO\n        \n        # 选出最佳当前划分特征\n        #split_feature = # TODO   #根据正确率划分\n        split_feature = best_split_entropy(data, features, target)# TODO  # 根据信息增益来划分\n\n        # 选出最佳特征后，该特征为0的数据分到左边，该特征为1的数据分到右边\n        left_split = data[data[split_feature] == 0]# TODO\n        right_split = data[data[split_feature] == 1]# TODO\n\n        # 剔除已经用过的特征\n        remaining_features = remaining_features.drop(split_feature)# TODO\n        print(\"Split on feature %s. (%s, %s)\" % (split_feature, str(len(left_split)), str(len(right_split))))\n\n        # 如果当前数据全部划分到了一边，直接创建叶子节点返回即可\n        if len(left_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(left_split[target])\n        if len(right_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(right_split[target])\n\n        # 递归上面的步骤\n        #create_tree(data, features, target, current_depth = 0, max_depth = 10, min_error=0)\n        left_tree = self.create_tree(left_split, remaining_features, target, current_depth+1, max_depth, min_error)# TODO     \n        right_tree = self.create_tree(right_split, remaining_features, target, current_depth+1, max_depth, min_error)# TODO\n\n        #生成当前的树节点\n        result_node = TreeNode(False, None, split_feature)\n        result_node.left = left_tree\n        result_node.right = right_tree\n        return result_node\n    \n    def create_leaf(self, target_values):\n        # 用于创建叶子的函数\n        \n        # 初始化一个树节点\n        leaf = TreeNode(True, None, None)# TODO\n\n        # 统计当前数据集里标签为+1和-1的个数，较大的那个即为当前节点的预测结果\n                ### ??? what is the datatype of target_values\n        num_positive_ones = len(target_values[target_values == +1])\n        num_negative_ones = len(target_values[target_values == -1])\n        '''\n        ### important, don't make such mistake again, apply lambda doesnt drop the unmatched elements from the list\n        # num_positive_ones = len(target_values.apply(lambda x: x== +1))#TODO\n        # num_negative_ones = len(target_values.apply(lambda x: x== -1))# TODO\n        '''\n\n        if num_positive_ones > num_negative_ones:\n            leaf.prediction = 1\n        else:\n            leaf.prediction = -1\n\n        # 返回叶子        \n        return leaf \n    \n    \n    \n    def predict_single_data(self, tree, x, annotate = False):   \n        #Q1: what is x, what is the datatype\n        # If tree is already a leaf, return the prediction\n        if tree.is_leaf:\n            if annotate: \n                print(\"leaf node, predicting %s\" % tree.prediction)\n            return tree.prediction# TODO \n        else:\n            # check what the featuer sets, assumming x is the feature set\n            split_feature_value = x[tree.split_feature]# TODO\n\n            if annotate: \n                print(\"Split on %s = %s\" % (tree.split_feature, split_feature_value))\n            if split_feature_value == 0:\n                #如果数据在该特征上的值为0，交给左子树来预测\n                return self.predict_single_data(tree.left, x, None)# TODO\n            else:\n                #如果数据在该特征上的值为0，交给右子树来预测\n                return self.predict_single_data(tree.right, x, None)# TODO    \n    \n    def count_leaves(self):\n        return self.count_leaves_helper(self.root_node)\n    \n    def count_leaves_helper(self, tree):\n        # TODO\n        if tree.is_leaf: \n            return 1\n        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5418df4a9ab2493442a89f7dc0e374328f39780","_cell_guid":"938f596c-b02a-40fb-abc7-22478131002d","collapsed":true,"trusted":false},"cell_type":"code","source":"m = MyDecisionTree(max_depth = 10, min_error = 1e-15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eba3f893824748c2e9dda841bf8586248d09d875","_cell_guid":"00f9c458-0e7b-4c99-864b-366b3a4c2fe8","collapsed":true,"trusted":false},"cell_type":"code","source":"m.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35519847a084d8cf97518a3dc174cd14f518053b","_cell_guid":"5a4544cd-8736-4b9c-9ba2-c4ee27d6543c","collapsed":true,"trusted":false},"cell_type":"code","source":"m.score(testX, testY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc3ee6840fd3e020e419251e2f509d211453f0b8","_cell_guid":"2670a5b3-5134-4155-b020-7a6cfd3d2428","collapsed":true},"cell_type":"markdown","source":"### 决策树复杂度的探讨"},{"metadata":{"_uuid":"3e6fb6cad69a098932956cfe578973619497348f","_cell_guid":"32743457-ec36-4580-93ce-86a0df649573","collapsed":true,"trusted":false},"cell_type":"code","source":"m.count_leaves()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38b8ae1cf325676403d32b93e575a5d6b2543413","_cell_guid":"6f89f28b-333b-4da8-92d3-85b0446229d6"},"cell_type":"markdown","source":"#### 探索不同树深度对决策树的影响  \n\n1 max_depth = 3  \n2 max_depth = 7  \n3 max_depth = 15\n"},{"metadata":{"_uuid":"08d5e83d15734c70ae8c4122dfe547baf324f1c1","_cell_guid":"8aa32b6b-aed7-4a08-a483-8a67b918cc4c","trusted":false,"collapsed":true},"cell_type":"code","source":"model_1 = MyDecisionTree(max_depth = 3, min_error = 1e-15)# TODO\nmodel_2 = MyDecisionTree(max_depth = 7, min_error = 1e-15)# TODO\nmodel_3 = MyDecisionTree(max_depth = 15, min_error = 1e-15)# TODO\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3f66cc76a7ecd6417031d29991522ade474a9af","_cell_guid":"fa0a8f75-6465-40ee-845e-6360031b7779","collapsed":true,"trusted":false},"cell_type":"code","source":"model_1.fit(trainX, trainY)\nmodel_2.fit(trainX, trainY)\nmodel_3.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb7d2f6c6b323ed7ad253b86e1249b75e39fec9d","_cell_guid":"c856b0ea-f972-4046-9d43-1d67d5d7d05d","collapsed":true,"trusted":false},"cell_type":"code","source":"print(\"model_1 training accuracy :\", model_1.score(trainX, trainY))\nprint(\"model_2 training accuracy :\", model_2.score(trainX, trainY))\nprint(\"model_3 training accuracy :\", model_3.score(trainX, trainY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"686b7a26500e842346fb84b1a43a929958203fc0","_cell_guid":"98be1e29-9b03-4e21-8241-58c328ddf664","collapsed":true,"trusted":false},"cell_type":"code","source":"print(\"model_1 testing accuracy :\", model_1.score(testX, testY))\nprint(\"model_2 testing accuracy :\", model_2.score(testX, testY))\nprint(\"model_3 testing accuracy :\", model_3.score(testX, testY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d99262b1aaf612f79dafc09f099540452dd93fa9","_cell_guid":"ba04dda3-7093-4331-9b1f-e043ce1bb276","collapsed":true,"trusted":false},"cell_type":"code","source":"print(\"model_1 complexity is: \", model_1.count_leaves())\nprint(\"model_2 complexity is: \", model_2.count_leaves())\nprint(\"model_3 complexity is: \", model_3.count_leaves())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7836cc1a458bf99741380ee769e2d64a581afad","_cell_guid":"c2ceae3d-bccf-439a-a8b2-a2cb011fa3e6","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":1}