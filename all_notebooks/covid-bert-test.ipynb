{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\njson_files = []\nIGN = ['metadata.csv', 'json_schema.txt', 'metadata.readme', 'COVID.DATA.LIC.AGMT.pdf']\nfor dirname, _, filenames in os.walk('/kaggle/input/CORD-19-research-challenge'):\n    for filename in filenames:\n        if not filename in IGN:\n            json_files.append(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install -U sentence-transformers\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nroot_path = '/kaggle/input/CORD-19-research-challenge'\nall_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class COVIDReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef breaking(txt, length):\n    data = \"\"\n    words = txt.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shm = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [],\n      'journal': [], 'abstract_summary': []}\n\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(\"processing index: {}\".format(idx))\n    content = COVIDReader(entry)\n    \n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    if len(meta_data) == 0:\n        continue\n        \n    shm['paper_id'].append(content.paper_id)\n    shm['abstract'].append(content.abstract)\n    shm['body_text'].append(content.body_text)\n    \n    if len(content.abstract) == 0:\n        shm['abstract_summary'].append(\"Not provided.\")\n        \n    elif len(content.abstract.split(\" \")) > 100:\n        info = content.abstract.split(' ')[:100]\n        summary = breaking(' '.join(info), 40)\n        shm['abstract_summary'].append(summary + \"...\")\n    else:\n        summary = breaking(content.abstract, 40)\n        shm['abstract_summary'].append(summary)\n        \n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            shm['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            shm['authors'].append(\". \".join(authors))\n    except Exception as e:\n        shm['authors'].append(meta_data['authors'].values[0])\n    \n    try:\n        title = breaking(meta_data['title'].values[0], 40)\n        shm['title'].append(title)\n    except Exception as e:\n        shm['title'].append(meta_data['title'].values[0])\n    \n    shm['journal'].append(meta_data['journal'].values[0])\n    \n    \ndf_covid = pd.DataFrame(shm, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['body_text'] = df_covid['body_text'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract'] = df_covid['abstract'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.to_csv(\"/kaggle/df_covid.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df_covid.drop([\"authors\", \"journal\"], axis=1)\ntext_dict = text.to_dict()\n\nlen_text = len(text_dict[\"paper_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paper_id_list  = []\nbody_text_list = []\n\ntitle_list = []\nabstract_list = []\nabstract_summary_list = []\nfor i in tqdm(range(0,len_text)):\n    paper_id = text_dict[\"paper_id\"][i]\n    body_text = text_dict[\"body_text\"][i].split(\"\\n\")\n    title = text_dict[\"title\"][i]\n    abstract = text_dict[\"abstract\"][i]\n    abstract_summary = text_dict[\"abstract_summary\"][i]\n    for b in body_text:\n        paper_id_list.append(paper_id)\n        body_text_list.append(b)\n        title_list.append(title)\n        abstract_list.append(abstract)\n        abstract_summary_list.append(abstract_summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sentences = pd.DataFrame({\"paper_id\":paper_id_list},index=body_text_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sentences = pd.DataFrame({\"paper_id\":paper_id_list,\"title\":title_list,\"abstract\":abstract_list,\"abstract_summary\":abstract_summary_list},index=body_text_list)\ndf_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sentences = df_sentences[\"paper_id\"].to_dict()\ndf_sentences_list = list(df_sentences.keys())\nlen(df_sentences_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sentences_list = [str(d) for d in tqdm(df_sentences_list)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport scipy.spatial\nimport pickle as pkl\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install flair","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.embeddings import BertEmbeddings\nfrom flair.data import Sentence\nimport torch\nimport pickle as pkl\n\n\n\nbm = BertEmbeddings()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = None\nif torch.cuda.is_available():\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = Sentence('Test to get embedding size')\nbm.embed(sentence)\nfor token in sentence:\n    print(token.embedding)\n                \n                    \n                \nz = token.embedding.size()[0]\n\ns = torch.zeros(0,z).to(device)\n\nfor paragraph in tqdm(df_sentences_list):\n    w = torch.zeros(0, z).to(device)\n    para = Sentence(paragraph)\n    bm.embed(sentence)\n    for token in para:\n        w = torch.cat((w, token.embedding.view(-1,z)), 0).to(device)\n    s = torch.cat((s, w.mean(dim=0).view(-1, z)))\n    \ntorch.save(s, \"embedded-docs.pt\")\n\nwith open(\"tensor-pickle.pkl\", \"wb\") as fout:\n    pkl.dump(s, fout, protocol=pkl.HIGHEST_PROTOCOL)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}