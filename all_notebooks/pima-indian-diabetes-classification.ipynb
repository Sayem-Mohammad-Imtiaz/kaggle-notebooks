{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PIMA INDIAN DIABETES CLASSIFICATION\n\n* Who are 'Pima Indians'?\n* What is 'Diabetes'?\n* What is 'Classification'?\n\n**Who are 'Pima Indians'?**\n\n> Pima, North American Indians who traditionally lived along the Gila and Salt rivers in Arizona, U.S., in what was the core area of the prehistoric Hohokam culture. The Pima, who speak a Uto-Aztecan language and call themselves the “River People,” are usually considered to be the descendants of the Hohokam.\n\n**What is 'Diabetes'?**\n\n> Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. The most common types of diabetes are type 1, type 2, and gestational diabetes. In type 1 diabetes, body does not make Insulin, immune system attacks and destroys the cells in pancreas that make insulin. People who has type 1 diabetes need to take Insulin externally to keep themselves alive. Type 2 is the most common type of diabetes where body does not make or use insulin well. Gestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. \n\n**What is 'Classification'?**\n\n> Classification is a type of supervised learning. It specifies the class to which data elements belong to and is best used when the output has finite and discrete values. A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease”. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***That's all about background study. Before prediction, we need to have a good picture ragrding the dataset. To that we need to familiar about dataset.***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we need to import libraries which need and import dataset to have a insight look.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd  # data processing\nimport numpy as np   # linear algebra\nimport matplotlib.pyplot as plt  #Plotting\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here by looking about results we can see that there are some misleading data points. For example here in the SkinThickness varibale contains 0 as a value which is not correct. The vaibales 'pregnancies', 'Insulin' & 'Outcome' can have 0 as its value, but in other vaibles it can't. Therefore we have to identify those values as missing or misleading value. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Glucose','BloodPressure','SkinThickness','BMI','DiabetesPedigreeFunction','Age']] = data[['Glucose','BloodPressure','SkinThickness','BMI','DiabetesPedigreeFunction','Age']].replace(0,np.NaN)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have indenfied misleading values, we need to have an idea about how many misleading values present and how they are going to effect the final prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_nan = data.isna().sum()\ndata_nan = pd.DataFrame(data_nan, columns=['NaN count'])\ndata_nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_nan = data_nan.reset_index()\nplt.figure(figsize = (12,8))\nplot = sns.barplot(x = 'index', y = 'NaN count', data = data_nan, palette = 'rocket')\nfor p in plot.patches:\n    plot.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points', fontsize = 12)\n\nplt.xticks(fontsize = 12, rotation=40)\nplt.xlabel(\"Variable\", fontsize=15)\nplt.yticks(fontsize = 12)\nplt.ylabel(\"NaN Count\", fontsize=15)\nplt.title('NaN Count of variables', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that SkinThickness has the highest number of misleading values. While building models for prediction, we need to take care about this issue. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Let's start Explonatory Data Analysis (EDA) to get good idea about the dataset.\n\n1. Univariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplot_outcome = sns.countplot(x = 'Outcome', data = data, palette=\"husl\")\nfor p in plot_outcome.patches:\n    plot_outcome.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points', fontsize = 12)\n\nplt.title('Count of Outcome', fontsize = 20)\nplt.xlabel('Outcome', fontsize = 15)\nplt.xticks(np.arange(2), ('No', 'Yes'), fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt_preg = sns.countplot(x = 'Pregnancies', data = data, palette=\"husl\")\nfor p in plt_preg.patches:\n    plt_preg.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points', fontsize = 12)\n\nplt.title('Count of Number of Pregnancies', fontsize = 20)\nplt.xlabel('Number of Pregnancies', fontsize = 15)\nplt.xticks(np.arange(18), fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(data['Glucose'], kde = True, color = 'Orange')\nplt.title('Histogram of Glucose', fontsize = 20)\nplt.xlabel('Glucose Level', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(data['BloodPressure'], kde = True, color = 'Purple')\nplt.title('Histogram of BloodPressure', fontsize = 20)\nplt.xlabel('BloodPressure Level', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(data['SkinThickness'], kde = True, color = 'Red')\nplt.title('Histogram of SkinThickness', fontsize = 20)\nplt.xlabel('SkinThickness Level', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(data['Insulin'], kde = True, color = 'Orange')\nplt.title('Histogram of Insulin', fontsize = 20)\nplt.xlabel('Insulin Level', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(data['BMI'], kde = True, color = 'Blue')\nplt.title('Histogram of BMI', fontsize = 20)\nplt.xlabel('BMI Value', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(data['DiabetesPedigreeFunction'], kde = True, color = 'Brown')\nplt.title('Histogram of Diabetes Pedigree Function', fontsize = 20)\nplt.xlabel('Diabetes Pedigree Function Value', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(data['Age'], kde = True, color = 'Black')\nplt.title('Histogram of Age', fontsize = 20)\nplt.xlabel('Age in years', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's have short discussion regarding the results we got from Univariate Analysis.**\n\n1. Outcome : There are 500 non-diabetic(0) paitents along with 268 diabetic(1) paitents, which makes the dataset imbalance. Imbalance dataset can lead to misleading predictions. Therefore we need to take care of this issue as well.\n2. Pregnancies : This indicates number of pregnancies paitent had, which ranges from 0 to 17.\n3. Glucose : This incicates the glucose level of the paitent. Data is normally distributed.\n4. BloodPressure : Level of blood pressure of the paitent. Data is normally distributed.\n5. SkinThickness : Thickness of the skin of the paitent. Data is normally distributed.\n6. Insulin : Amount of Insulin paitent has. Data is skewed as there are many 0 values, which can be result of presence of type 1 diabetic paitents. \n7. BMI : BMI value of the paitent. Data is normally distributed.\n8. DiabetesPedigreeFunction : A function which scores likelihood of diabetes based on family history. Data is skewed. \n9. Age : Age of the paitent. Variates from 21 to around 75. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"2. Bivariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf1 = data[data['Outcome'] == 0]\nsns.distplot(df1['Age'],  kde = True, label = 'No', color='red')\ndf2 = data[data['Outcome'] == 1]\nsns.distplot(df2['Age'],  kde = True, label = 'Yes', color='blue')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Age Vs. Outcome', fontsize = 20)\nplt.xlabel('Age', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf1 = data[data['Outcome'] == 0]\nsns.distplot(df1['DiabetesPedigreeFunction'],  kde = True, label = 'No', color='green')\ndf2 = data[data['Outcome'] == 1]\nsns.distplot(df2['DiabetesPedigreeFunction'],  kde = True, label = 'Yes', color='orange')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Diabetes Pedigree Function Vs. Outcome', fontsize = 20)\nplt.xlabel('Diabetes Pedigree Function Value', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf1 = data[data['Outcome'] == 0]\nsns.distplot(df1['BMI'],  kde = True, label = 'No', color='purple')\ndf2 = data[data['Outcome'] == 1]\nsns.distplot(df2['BMI'],  kde = True, label = 'Yes', color='red')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('BMI Vs. Outcome', fontsize = 20)\nplt.xlabel('BMI Value', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf1 = data[data['Outcome'] == 0]\nsns.distplot(df1['Insulin'],  kde = True, label = 'No', color='black')\ndf2 = data[data['Outcome'] == 1]\nsns.distplot(df2['Insulin'],  kde = True, label = 'Yes', color='yellow')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Insulin Vs. Outcome', fontsize = 20)\nplt.xlabel('Insulin Level', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf1 = data[data['Outcome'] == 0]\nsns.distplot(df1['SkinThickness'],  kde = True, label = 'No', color='red')\ndf2 = data[data['Outcome'] == 1]\nsns.distplot(df2['SkinThickness'],  kde = True, label = 'Yes', color='green')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('SkinThickness Vs. Outcome', fontsize = 20)\nplt.xlabel('SkinThickness', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf1 = data[data['Outcome'] == 0]\nsns.distplot(df1['BloodPressure'],  kde = True, label = 'No', color='crimson')\ndf2 = data[data['Outcome'] == 1]\nsns.distplot(df2['BloodPressure'],  kde = True, label = 'Yes', color='gold')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Blood Pressure level Vs. Outcome', fontsize = 20)\nplt.xlabel('Blood Pressure Level', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\ndf1 = data[data['Outcome'] == 0]\nsns.distplot(df1['Glucose'],  kde = True, label = 'No', color='teal')\ndf2 = data[data['Outcome'] == 1]\nsns.distplot(df2['Glucose'],  kde = True, label = 'Yes', color='peru')\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Glucose level Vs. Outcome', fontsize = 20)\nplt.xlabel('Glucose Level', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt_preg = sns.countplot(x = 'Pregnancies', data = data, palette=\"rocket\", hue = 'Outcome')\nfor p in plt_preg.patches:\n    plt_preg.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points', fontsize = 12)\n\nplt.title('Number of Pregnancies Vs. Outcome', fontsize = 20)\nplt.xlabel('Number of Pregnancies', fontsize = 15)\nplt.xticks(np.arange(18), fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.legend(['No','Yes'], loc = 'upper right', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's have short discussion regarding the results we got from Bivariate Analysis.**\n\n1. Age Vs. Outcome : By the histogram see that when paitent getting older there's high risk in getting diabeteics. \n2. DiabetesPedigreeFunction Vs. Outcome : By the histogram see that when Diabetes Pedigree Function value goes high, there's considerable risk of being diabetes paitent.\n3. BMI Vs. Outcome : By the plot we can see that who has high BMI value tend to have diabetes.\n4. Insulin Vs. Outcome : Both histograms are skewed due to 0 value and many of the paitents who has 0 insulin level is non-diabetic paitent surprisingly.  \n5. SkinThickness Vs. Outcome : According to the plot people who has high skin thickness tend to have diabetes. \n6. BloodPressure Vs. Outcome : Both histograms for diabetic and non-diabetic paitents overlays with each other indicating blood pressure is not much involved in diabetes.\n7. Glucose Vs. Outcome : Histgrams shows that when the Glucose level is high, Being diabetic paitent is more likely.\n8. Pregnancies Vs. Outcome : By the bar plot we can see that when number of pregnancies goes above 8, there's high risk of being a diabetic paitent. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"3. Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (12,8))\nsns.set(font_scale = 1.5)\nsns.heatmap(data.corr(), annot=True, fmt='.2f')\nplt.title('Correlation Plot', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the variables 'SkinThickness' and 'BMI' has considerable amount of correlation which means presence of multicolinearity. This issue also have to take care while building models for predictions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**That's all about Explonatory Data Analysis. Next is Data modeling. But before data modelling we need to solve the issues we identified in EDA.**\n\n* **Imputing misleading/missing data**\n* **Addressing imbalance data**\n* **Addressing multicolinearity**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. **Imputing misleading/missing data**\n\nThere are many ways of imputing misleading/missing data, I have used K-Nearest Neighbour method here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ndata_imputed = imputer.fit_transform(data)\ndata_imputed = pd.DataFrame(data_imputed, columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'])\ndata_imputed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Addressing imbalance data\n\nThere are many ways to address the issue of imbalance data. Here I have used the upsampling method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_imputed['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\ndf_majority = data_imputed[data_imputed.Outcome==0]\ndf_minority = data_imputed[data_imputed.Outcome==1]\n\ndata_minority_upsampled = resample(df_minority, replace=True, n_samples=500, random_state=123) \ndata_upsampled = pd.concat([df_majority, data_minority_upsampled])\n\ndata_upsampled['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Addressing multicolinearity\n\nTo address the issue of multicolinearity we can use regularization modeling methods instead of standard logistic classification such as Logistic Ridge classification, Logistic Lasso classification.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Since now we have resolved all the issues, let's start data modelling. Here I'm building 5 different prediction models and compare them using Accuracy and Area Under Curve (AUC) value.**\n\n* **Logistic Ridge Classification**\n* **Logistic Lasso Classification**\n* **Neural Network**\n* **K-Nearest Neighbor Classification**\n* **Random Forest**\n\nBefore start building models we need to Scale data and define the explonatory and response varibales. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_upsampled.drop(columns = ['Outcome'], axis=1)\ny = data_upsampled.Outcome","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Scale Data : Scale difference in explonatory variables can effects the prediction results.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx = StandardScaler().fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we have to split the dataset as traning and test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\naccuracy = pd.DataFrame(columns=['classifiers', 'accuracy','auc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **Logistic Ridge Classification**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression, LogisticRegressionCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = np.linspace(1,10,100)\nridgeClassifiercv = LogisticRegressionCV(penalty = 'l2', Cs = 1/alphas, solver = 'liblinear')\nridgeClassifiercv.fit(x_train, y_train)\nridgeClassifiercv.C_  #Inverse of best alpha value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression(penalty = 'l2', C = ridgeClassifiercv.C_[0], solver = 'liblinear')\nLR.fit(x_train, y_train)\ny_predLR = LR.predict(x_test)\naccLR = accuracy_score(y_test, y_predLR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_roc_auc = roc_auc_score(y_test, LR.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, LR.predict_proba(x_test)[:,1])\nresult_table = result_table.append({'classifiers':'Logistics Ridge', 'fpr':fpr, 'tpr':tpr, 'auc':clf_roc_auc}, ignore_index=True)\nplt.figure(figsize = (12,8))\nplt.plot(fpr, tpr, label='Logistic Ridge (area = %0.2f)' % clf_roc_auc, lw = 2)\nplt.plot([0, 1], [0, 1],'r--', lw = 2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize = 15)\nplt.ylabel('True Positive Rate', fontsize = 15)\nplt.title('Receiver operating characteristic curve for Logistic Ridge Classification', fontsize = 20)\nplt.legend(loc=\"lower right\", fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy.append({'classifiers':'Logistic Ridge', 'accuracy':accLR, 'auc':clf_roc_auc}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Logistic Lasso Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lassoClassifiercv = LogisticRegressionCV(penalty = 'l1', Cs = 1/alphas, solver = 'liblinear')\nlassoClassifiercv.fit(x_train, y_train)\nlassoClassifiercv.C_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LL = LogisticRegression(penalty = 'l1', C = lassoClassifiercv.C_[0], solver = 'liblinear')\nLL.fit(x_train, y_train)\ny_predLL = LL.predict(x_test)\naccLL = accuracy_score(y_test, y_predLL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_roc_auc = roc_auc_score(y_test, LL.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, LL.predict_proba(x_test)[:,1])\nresult_table = result_table.append({'classifiers':'Logistics Lasso', 'fpr':fpr, 'tpr':tpr, 'auc':clf_roc_auc}, ignore_index=True)\nplt.figure(figsize = (12,8))\nplt.plot(fpr, tpr, label='Logistic Lasso (area = %0.2f)' % clf_roc_auc, lw = 2)\nplt.plot([0, 1], [0, 1],'r--', lw = 2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize = 15)\nplt.ylabel('True Positive Rate', fontsize = 15)\nplt.title('Receiver operating characteristic curve for Logistic Lasso Classification', fontsize = 20)\nplt.legend(loc=\"lower right\", fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy.append({'classifiers':'Logistic Lasso', 'accuracy':accLL, 'auc':clf_roc_auc}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(123)\nmodel = Sequential()\nmodel.add(Dense(8, activation = 'tanh', input_dim = 8))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.fit(x_train, y_train, batch_size = 10, epochs = 10, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(x_test)\naccNN = accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_roc_auc = roc_auc_score(y_test, model.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test))\nresult_table = result_table.append({'classifiers':'Neural Network', 'fpr':fpr, 'tpr':tpr, 'auc':clf_roc_auc}, ignore_index=True)\nplt.figure(figsize = (12,8))\nplt.plot(fpr, tpr, label='Neural Network (area = %0.2f)' % clf_roc_auc, lw = 2)\nplt.plot([0, 1], [0, 1],'r--', lw = 2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize = 15)\nplt.ylabel('True Positive Rate', fontsize = 15)\nplt.title('Receiver operating characteristic curve for Neural Network', fontsize = 20)\nplt.legend(loc=\"lower right\", fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy.append({'classifiers':'Neural Network', 'accuracy':accNN, 'auc':clf_roc_auc}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. K-Nearest Neighbor Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\nk = range(1,30)\nfor i in k:\n  knn = KNeighborsClassifier(n_neighbors=i)\n  knn.fit(x_train, y_train)\n  pred_i = knn.predict(x_test)\n  error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(12,8))\nplt.plot(k,error_rate, color='black', linestyle='dashed', marker='o', markerfacecolor='pink')\nplt.title('Error Rate vs. K Value', fontsize = 20)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.xlabel('K', fontsize = 15)\nplt.ylabel('Error Rate', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above graph can be used to decide best K value. Here we can see that k=1 is best for this dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN = KNeighborsClassifier(n_neighbors=1, p=2, metric='euclidean')\nKNN.fit(x_train,y_train)\ny_predKNN = KNN.predict(x_test)\naccKNN = accuracy_score(y_test, y_predKNN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_roc_auc = roc_auc_score(y_test, KNN.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, KNN.predict_proba(x_test)[:,1])\nresult_table = result_table.append({'classifiers':'KNN', 'fpr':fpr, 'tpr':tpr, 'auc':clf_roc_auc}, ignore_index=True)\nplt.figure(figsize = (12,8))\nplt.plot(fpr, tpr, label='K Nearest Neighbour (area = %0.2f)' % clf_roc_auc, lw = 2)\nplt.plot([0, 1], [0, 1],'r--', lw = 2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize = 15)\nplt.ylabel('True Positive Rate', fontsize = 15)\nplt.title('Receiver operating characteristic curve for K Nearest Neighbour Classification', fontsize = 20)\nplt.legend(loc=\"lower right\", fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy.append({'classifiers':'KNN', 'accuracy':accKNN, 'auc':clf_roc_auc}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate_RF = []\nn = range(1,20)\nfor i in n:\n  RFC = RandomForestClassifier(n_estimators=i)\n  RFC.fit(x_train, y_train)\n  pred_i = RFC.predict(x_test)\n  error_rate_RF.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(12,8))\nplt.plot(n,error_rate_RF, color='black', linestyle='dashed', marker='o', markerfacecolor='maroon')\nplt.title('Error Rate vs. Number of estimators(Trees)', fontsize = 20)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.xlabel('Number of estimators(Trees)', fontsize = 15)\nplt.ylabel('Error Rate', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to above graph optimal number of trees is 18.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC = RandomForestClassifier(n_estimators=18)\nRFC.fit(x_train, y_train)\ny_predRFC = RFC.predict(x_test)\naccRF = accuracy_score(y_test, y_predRFC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_roc_auc = roc_auc_score(y_test, RFC.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, RFC.predict_proba(x_test)[:,1])\nresult_table = result_table.append({'classifiers':'Random Forest', 'fpr':fpr, 'tpr':tpr, 'auc':clf_roc_auc}, ignore_index=True)\nplt.figure(figsize = (12,8))\nplt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % clf_roc_auc, lw = 2)\nplt.plot([0, 1], [0, 1],'r--', lw = 2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize = 15)\nplt.ylabel('True Positive Rate', fontsize = 15)\nplt.title('Receiver operating characteristic curve for Random Forest Classification', fontsize = 20)\nplt.legend(loc=\"lower right\", fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy.append({'classifiers':'Random Forest', 'accuracy':accRF, 'auc':clf_roc_auc}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_table.set_index('classifiers', inplace=True)\naccuracy.set_index('classifiers', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='black', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1), fontsize = 12)\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1), fontsize = 12)\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontsize=20)\nplt.legend(prop={'size':13}, loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above table summaries the final results. Accuracy and AUC value both can be used to identify best model. If we consider accuracy **Random Forest** is the best model. In AUC method **Neural Network** is the best. We can use any of these two models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***That'a all.***\n\n# Thank You. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}