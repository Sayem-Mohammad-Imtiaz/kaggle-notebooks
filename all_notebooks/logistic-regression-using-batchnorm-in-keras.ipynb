{"cells":[{"metadata":{"_uuid":"ecd3ae11b5c92d7cf923c6ed2fd9e49d6020bb7d"},"cell_type":"markdown","source":"<h2>Script-level imports</h2>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b547aeb251a62371a377baf7fc04e630c7bc2a2"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":112,"outputs":[]},{"metadata":{"_uuid":"99cbf155b8be17c8ba123d5ff5cb1950bb60ce7b"},"cell_type":"markdown","source":"<h2>Reading the input</h2>"},{"metadata":{"trusted":true,"_uuid":"ef035cff0d9954df7028b164433c8914798ee2a3"},"cell_type":"code","source":"df = pd.read_csv('../input/diabetes.csv')\nprint(df.info())","execution_count":199,"outputs":[]},{"metadata":{"_uuid":"e732b37b39bd93457b1e04e4ead20c5391967128"},"cell_type":"markdown","source":"<h2>Data exploration</h2>\n\nSteps:\n<ul>\n    <li> Check class frequency.</li>\n    <li> Check feature dependency.</li>\n</ul>"},{"metadata":{"_uuid":"35f03e55f50b8983ac7288bf972993ff4c8f8936"},"cell_type":"markdown","source":"<h2>Analyzing class label frequencies</h2>"},{"metadata":{"trusted":true,"_uuid":"a3b14f215179196452864bf510d06297920836bf"},"cell_type":"code","source":"# Plotting the counts for the 'Outcome' column (class labels)\nsb.countplot(x='Outcome', data=df)","execution_count":200,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3bf2fff3a23397377644c0a0f0537a099fdf4bed"},"cell_type":"code","source":"# We replace the 0s in each column by the columnar mean.\nfor column in set(df.columns).difference({'Pregnancies', 'Outcome'}):\n    df[column] = df[column].replace(0, df[column].mean())","execution_count":201,"outputs":[]},{"metadata":{"_uuid":"1623300ae47428344d85893a8918a5c2af4b0a4f"},"cell_type":"markdown","source":"<h2>Heatmaps for feature correlation</h2>\n\nInter-feature correlation analysis is an extremely useful tool that allows us to analyze which features are 'bonded' with each other."},{"metadata":{"trusted":true,"_uuid":"30cb0b54371508d5e0b58920c987a949922995f6"},"cell_type":"code","source":"# Displaying the heatmap.\nsb.heatmap(df.corr())","execution_count":202,"outputs":[]},{"metadata":{"_uuid":"b2d760bc2bd8b9dd91cc47ab52e0d75567f48798"},"cell_type":"markdown","source":"<h3>Analysis result</h3>\n\nFrom the heatmap above, we can see that the following features have a high correlational strength:\n1. **BMI** & **Skin Thickness**\n2. ** Pregnancies** & **Age**"},{"metadata":{"trusted":true,"_uuid":"9dddf25d0ea2ef42e5a4d1077686f06643dd93fc"},"cell_type":"code","source":"print(df.head())","execution_count":203,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c04ae6ca4ff42bce36e7883ea11536f9a2065d1d"},"cell_type":"code","source":"# Converting the dataframe into a numpy matrix.\ndf_values = df.values\n\n# Shuffling rows of the matrix.\nnp.random.shuffle(df_values)","execution_count":204,"outputs":[]},{"metadata":{"_uuid":"f0199db312dbd99633d9af3f2a520837abc197e9"},"cell_type":"markdown","source":"<h2>Splitting the data into Input and Output and computing class weights</h2>\n\nHere, We split the numpy matrix into input and output.\n\nWe also compute class weights, i.e., the ratio of instances in each class. Performing this step is **extremely** crucial in imbalanced problems."},{"metadata":{"trusted":true,"_uuid":"19420b2469210556ab296d2a628128c93c37c4b1"},"cell_type":"code","source":"# Splitting the first N-1 columns as X.\nx = df_values[:,:-1]\n\n# Splitting the last column as Y.\ny = df_values[:, -1].reshape(x.shape[0], 1)\n\nprint(x.shape)\nprint(y.shape)\n\nfrom sklearn.utils import class_weight\n\n# Computing the class weights.\n# Note: This returns an ndarray.\nweights = class_weight.compute_class_weight('balanced', np.unique(y), y.ravel()).tolist()\n\n# Converting the ndarray to a dict.\nweights_dict = {\n    i: weights[i] for i in range(len(weights))\n}\n\nprint(\"Class weights: \", weights_dict)","execution_count":205,"outputs":[]},{"metadata":{"_uuid":"f229e4a3a72ddab96aa885d236d9715b18c33634"},"cell_type":"markdown","source":"<h2>Keras-specific imports</h2>"},{"metadata":{"trusted":true,"_uuid":"5ea24bd671bf7a63b0516ca8c1676effeedf2418"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Activation, Input\nimport keras.regularizers","execution_count":206,"outputs":[]},{"metadata":{"_uuid":"4f514eb4263b91a8e61e10553a1a023c20f9adf9"},"cell_type":"markdown","source":"<h2>Building the model</h2>\n\nWe use a simple logistic regression model with a few tweaks added in:\n* Batch Normalization: This causes covariate shift, resulting in accelerated learning.\n* L2 regularization: By imposing penalties on the weights, we ensure that the model doesn't overfit."},{"metadata":{"trusted":true,"_uuid":"49bd338cc328e23b268e969f5eec9892d418bb07"},"cell_type":"code","source":"# Instantiate the model.\nmodel = Sequential()\n\n# Add the input layer and the output layer.\n# The '1' indicates the number of output units.\n# The 'input_shape' is where we specify the dimensionality of our input instances.\n# The 'kernel_regularizer' specifies the strength of the L2 regularization.\nmodel.add(Dense(1, input_shape=(x.shape[1], ), kernel_regularizer=keras.regularizers.l2(0.017)))\n\n# Adding the BatchNorm layer.\nmodel.add(BatchNormalization())\n\n# Adding the final activation, i.e., sigmoid.\nmodel.add(Activation('sigmoid'))\n\n# Printing the model summary.\nprint(model.summary())","execution_count":207,"outputs":[]},{"metadata":{"_uuid":"e8d1551427c22df193c6be10d702b5efba234a23"},"cell_type":"markdown","source":"<h2>Normalizing the inputs</h2>\n\nAny ML model prefers its inputs to be scaled, i.e, with 0 mean and unit standard deviation. This makes learning **much faster**.\n\nTo achieve this, we:\n1. Obtain the mean of the input.\n2. Obtain the std. deviation of the input.\n3. Subtract the mean from the input and divide the result by the standard deviation"},{"metadata":{"trusted":true,"_uuid":"63bf951303a1ab977797a1fe162b54e950833853"},"cell_type":"code","source":"# Mean, columnar axis.\nx_mean = np.mean(x, axis=0, keepdims=True)\n\n# Std. Deviation, columnar axis.\nx_std = np.std(x, axis=0, keepdims=True)\n\n# Normalizing.\nx = (x - x_mean)/x_std\n\nprint(x[:5, :])","execution_count":208,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4afa8f8ed7c64430c9ede5612081fc45b74c91bf"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the model into a 0.9-0.1 train-test split.\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=5)\n\nprint(\"Shape of x_train: \", x_train.shape)\nprint(\"Shape of y_train: \", y_train.shape)\nprint(\"Shape of x_test: \", x_test.shape)\nprint(\"Shape of y_test: \", y_test.shape)","execution_count":209,"outputs":[]},{"metadata":{"_uuid":"6aaf9f87d96ec86cfe79cbb124fc87eee22305e6"},"cell_type":"markdown","source":"<h2>Compiling the model</h2>\n\nWe now compile our model with our preferred metrics and optimizer before fitting the data to it (the model).\n\nSince we're performing logistic regression, the obvious choice for the loss function would be *binary crossentropy*.\nFor the optimizer, I've chosen AdaMax. You may use any one you wish."},{"metadata":{"trusted":true,"_uuid":"134457fa37659cb53637e1a77a9d742f381fa8dc"},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam')\nprint('Model compiled!')","execution_count":210,"outputs":[]},{"metadata":{"_uuid":"9fafa05e09aa408e86fee8540ec6c964a953c2f4"},"cell_type":"markdown","source":"<h2>Fitting data to the model</h2>\n\n\nWe harness a technique called Early Stopping that stops training immediately as soon as it notices that the validation loss is increasing, which indicates overfitting.\n\n\nWe fit the data to the model with the following parameters:\n* Input: x_train.\n* Output: y_train.\n* Batch size: 128.\n* Callbacks: The Early Stopper.\n* Class weights: The dictionary we had created before. This ensures that each label instance is weighted according to its frequency in the dataset.\n* Epochs: Any high number (The Early Stopping technique will abort training as soon as the model starts showing signs of overfitting)."},{"metadata":{"trusted":true,"_uuid":"964e1f46e8fbbd38d8bb62ebb0a565c36d8d4d21"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\n# Initialize the Early Stopper.\nstopper = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n\n# Fit the data to the model and get the per-batch metric history.\nhistory = model.fit(x_train, y_train, validation_split=0.1, \n                    batch_size=128, epochs=700, \n                    callbacks=[stopper], class_weight=weights_dict, verbose=1)","execution_count":211,"outputs":[]},{"metadata":{"_uuid":"293011c3a9163fdf944bda2c43d168fdee851e9b"},"cell_type":"markdown","source":"<h2>Plotting the losses</h2>\n\nPlotting the losses is a great way to see how your model is performing. For every batch, we:\n1. Compute the loss for the batch.\n2. Back-propagate the error derivative.\n3. Update each weight,\n4. Get the next batch and repeat step 1."},{"metadata":{"trusted":true,"_uuid":"3d5f8d9e161afba5b0d5b38599ac8375904c2021"},"cell_type":"code","source":"# Plot the training loss.\nplt.plot(history.history['loss'], 'r-')\n\n# Plot the validation loss.\nplt.plot(history.history['val_loss'], 'b-')\n\n# X-axis label.\nplt.xlabel('Epochs')\n\n# Y-axis label.\nplt.ylabel('Cost')\n\n# Graph legend.\nplt.legend([\"Training loss\", \"Validation loss\"])\n\n# Graph title.\nplt.title('Loss Graph')\n\nplt.show()","execution_count":212,"outputs":[]},{"metadata":{"_uuid":"dc831b4cbb480a3bc409ec59f29f778bdd897d72"},"cell_type":"markdown","source":"<h2>Are we done?</h2>\n\n\n<h3> Absolutely NOT. </h3> <br />\n\n\nA common problem with imbalanced binary classification problems is that the classifier opts for a simple way out and starts predicting each input  as an instance belonging to the output class with a large volume. This way, not only does the classifier satisfy the optimization objective, but also gets a high *accuracy*. Therefore, it is **strongly** recommended to use an alternative scoring metric, for instance, the F1-score."},{"metadata":{"trusted":true,"_uuid":"dc127087c7acfe68c74163a9c0eb269cf8965786"},"cell_type":"code","source":"# Initialize variables.\ntp = 0\nfp = 0\nfn = 0\ntn = 0\n\n# Get the predictions for the test inputs.\n# One critical thing to note here is that, unlike scikit-learn,\n# Keras will return the non-rounded prediction confidence\n# probabilities.Therefore, rounding-off is critical.\npredictions = model.predict(x_test)\n\n# The hyperparameter that controls the tradeoff between how\n# 'precise' the model is v/s how 'safe' the model is.\npr_hyperparameter = 0.5\n\n# Rounding-off the predictions.\npredictions[predictions > pr_hyperparameter] = 1\npredictions[predictions <= pr_hyperparameter] = 0\n\n# Computing the precision and recall.\nfor i in range(predictions.shape[0]):\n    if y_test[i][0] == 1 and predictions[i][0] == 1:\n        tp += 1\n    elif y_test[i][0] == 1 and predictions[i][0] == 0:\n        fn += 1\n    elif y_test[i][0] == 0 and predictions[i][0] == 1:\n        fp += 1\n    else:\n        tn += 1\n\npr_positive = tp/(tp + fp + 1e-8)\nre_postive = tp/(tp + fn + 1e-8)\npr_negative = tn/(tn + fn + 1e-8)\nre_negative = tn/(tn + fp + 1e-8)\n\n# Computing the F1 scores.\nf1 = (2*pr_positive*re_postive)/(pr_positive + re_postive + 1e-8)\nf1_neg = (2*pr_negative*re_negative)/(pr_negative + re_negative + 1e-8)\n\nprint(\"F1 score (y=1): {}\".format(f1))\nprint(\"F1 score (y=0): {}\".format(f1_neg))","execution_count":213,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"27e191f1f834b9e105b1bfd02ee3745336594493"},"cell_type":"code","source":"from sklearn import metrics\n\n# Print the detailed classification report.\nprint(metrics.classification_report(y_true=y_test, y_pred=predictions))\n\n# Compute the confusion matrix.\nconf_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=predictions)\n\n# Print the confusion matrix.\nprint(conf_matrix)","execution_count":214,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac6ed3853d0cc87cb56f2b6d1e526a8cbbbc4d17"},"cell_type":"code","source":"# Display the heatmap for the confusion matrix.\nsb.heatmap(conf_matrix)","execution_count":215,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"260f2bf49c510b5be20b5516e6181d51735d7f09"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}