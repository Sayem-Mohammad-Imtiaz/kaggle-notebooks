{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Insurance Claims Prediction\nThis notebook contains the analysis of insurance claim details and predicting the insurance expenses using multiple Machine Learning models\nFind the more about the data and the analysis from different data scientists here - \n[https://www.kaggle.com/mirichoi0218/insurance](http://url)\n"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, Ihave tried multiple options with the data to find the better R2 Score and RMSE.\n1. Multi Linear Regressionw with all independent variable\n2. Multi Linear Regression with Outliers removed\n3. Multi Linear Regression by removing the independent variables in less coefficients\n4. Polynomial Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the basic libraries that are mostly used to build any machine learning algorithm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the input excel file\nins=pd.read_csv('../input/insurance/insurance.csv')\nins.head()\n#check for any NaN\nprint(ins.isna().sum())\nprint(ins.info())\nins.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average claim amount is $ 13270. Assume the data is from USA. Now lets see how the dependent variable increase/decrease with BMI. BMI is the only linear independent variable in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets get dummies for the categorical variables.\ncatcol=['sex','smoker','region']\nins1=pd.get_dummies(ins,columns=catcol)\nins1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see now if there is any outlier on the independent variable\n#Lets import the seaborn library to plot the outlier. \nimport seaborn as sns\nsns.boxplot(x=ins1['bmi'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are outliers in the dependent variable BMI. If we run the liner regression, these outliers will make an impact on the prediction. Lets see what are thes eoutliers and decide whether we should remove them or normalize them.\nWe had to see the outlier only for  BMI variable as thats the only independent variable with the possibility to have the outliers. Others are categorical variable or we do not expect a outlier. Lets check for age."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=ins1['age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identify the outlier"},{"metadata":{},"cell_type":"markdown","source":"Lets follow the zero score method to identify the records with outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the stats fucntion from the scipy library\nfrom scipy import stats\nz = np.abs(stats.zscore(ins1))\n#x=stats.zscore(ins1)\nprint(z)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lest define a threshold, usually 3, for the outlier score and print the locations of those outliers.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.where(z>3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove the outlier"},{"metadata":{},"cell_type":"markdown","source":"Lets take a copy of the dataframe before to remove the outlier. We shall also see how the machine learning models behave when train the models with and without outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"ins1_o=ins1[(z<3).all(axis=1)]\nins1_o","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now its the time for copy paste.... :)"},{"metadata":{},"cell_type":"markdown","source":"Lets do the Linear regresssion to train our model and see how is our model against the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the libraries\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Lets define the dependent and independent variables.\nX=ins1[['age','bmi','sex_female','sex_male','smoker_no','smoker_yes','region_northeast','region_northwest','region_southeast','region_southwest']]\ny=ins1['charges']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(15,10))\n#plt.tight_layout()\nsns.distplot(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see most of the dependent variable is distributed between 0 to 15000"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nLRM = LinearRegression()  \nLRM.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check the best intercept and the coefficients\nprint('Intercept:',LRM.intercept_)\nprint('Coefficients for different independent variables:',LRM.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets put the coefficients in a dataframe format to understand which variable contribute more positively and negatively to decide the dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_df = pd.DataFrame(LRM.coef_, X.columns, columns=['Coefficient'])  \ncoeff_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Smoking_yes and region_northeast contibutes more positively to decide the charges. And the other fctors that contirubuters are age, bmi which is reasonable.\nLets predict the charges with the above intercept and coefficient values and compare it against the actuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets predict using the trained model and compare it against the actuals\ny_pred = LRM.predict(X_test)\ncompare = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ncompare.head(10) #since it is 250+ rows, lets see only the first 10 rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets visualize"},{"metadata":{"trusted":true},"cell_type":"code","source":"compare.head(25).plot(kind='bar',figsize=(10,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not Bad! Lets see the statistics of this regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error without removing the outliers:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error without removing the outliers:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error without removing the outliers:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R2 value without removing the outliers:', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ITS BAD! The Root Mean Square error is huge beyond the acceptable limit. Lets rerun the model without the region columns as we believe the region is very little to do (eSpecially in USA) with the insurance claims."},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=ins1[['age','bmi','sex_female','sex_male','smoker_no','smoker_yes']]\ny1=ins1['charges']\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\nLRM1 = LinearRegression()  \nLRM1.fit(X1_train, y1_train)\n#Lets check the best intercept and the coefficients\nprint('Intercept:',LRM1.intercept_)\nprint('Coefficients for different independent variables:',LRM1.coef_)\ncoeff_df1 = pd.DataFrame(LRM1.coef_, X1.columns, columns=['Coefficient'])  \ncoeff_df1\n\n#Lets predict using the trained model and compare it against the actuals\ny1_pred = LRM1.predict(X1_test)\ncompare1 = pd.DataFrame({'Actual': y1_test, 'Predicted': y1_pred})\ncompare1.head(10) #since it is 250+ rows, lets see only the first 10 rows\ncompare1.head(25).plot(kind='bar',figsize=(10,8))\nprint('Mean Absolute Error without removing the outliers and without regions:', metrics.mean_absolute_error(y1_test, y1_pred))  \nprint('Mean Squared Error without removing the outliers and without regions:', metrics.mean_squared_error(y1_test, y1_pred))  \nprint('Root Mean Squared Error without removing the outliers and without regions:', np.sqrt(metrics.mean_squared_error(y1_test, y1_pred)))\n\nprint('R2 value without removing the outliers and without regions:', r2_score(y1_test, y1_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, not much change in the metrics! Lets train the model against the data without outliers. will see how the model behave."},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=ins1_o[['age','bmi','sex_female','sex_male','smoker_no','smoker_yes']]\ny2=ins1_o['charges']\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=0)\nLRM2 = LinearRegression()  \nLRM2.fit(X2_train, y2_train)\n#Lets check the best intercept and the coefficients\nprint('Intercept:',LRM2.intercept_)\nprint('Coefficients for different independent variables:',LRM2.coef_)\ncoeff_df1 = pd.DataFrame(LRM1.coef_, X1.columns, columns=['Coefficient'])  \ncoeff_df1\n\n#Lets predict using the trained model and compare it against the actuals\ny2_pred = LRM2.predict(X2_test)\ncompare2 = pd.DataFrame({'Actual': y2_test, 'Predicted': y2_pred})\ncompare2.head(10) #since it is 250+ rows, lets see only the first 10 rows\ncompare2.head(25).plot(kind='bar',figsize=(10,8))\nprint('Mean Absolute Error with outliers removed:', metrics.mean_absolute_error(y2_test, y2_pred))  \nprint('Mean Squared Error with outliers removed::', metrics.mean_squared_error(y2_test, y2_pred))  \nprint('Root Mean Squared Error with outliers removed:', np.sqrt(metrics.mean_squared_error(y2_test, y2_pred)))\nprint('R2 value with outliers removed:', r2_score(y2_test, y2_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Machine Learning is all about reviewing the result and tunign the input. The output whatever we brough through the learning so far is not upto the acceptable mark.Lets see if we can use some other algorithm."},{"metadata":{},"cell_type":"markdown","source":"### Ploynomial Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nXP=ins1_o[['age','bmi','sex_female','sex_male','smoker_no','smoker_yes']]\nyP=ins1_o['charges']\nXP_train, XP_test, yP_train, yP_test = train_test_split(XP, yP, test_size=0.2, random_state=0)\npolynomial_features= PolynomialFeatures(degree=4)\nX_poly = polynomial_features.fit_transform(XP_train)\nXP_poly_test = polynomial_features.fit_transform(XP_test)\nmodel = LinearRegression()\nmodel.fit(X_poly, yP_train)\ny_poly_pred = model.predict(XP_poly_test)\nrmse = np.sqrt(mean_squared_error(yP_test,y_poly_pred))\nr2 = r2_score(yP_test,y_poly_pred)\nprint('Root Mean Square Value through Polynominal Regression',rmse)\nprint('R2 Score through Polynominal Regression:', r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_poly = pd.DataFrame({'Actual': yP_test, 'Predicted': y_poly_pred})\ncompare_poly.head(10) #since it is 250+ rows, lets see only the first 10 rows\ncompare_poly.head(25).plot(kind='bar',figsize=(10,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the all given analysis, ploynominal regression model results better R2 value though the MSME value is not better."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}