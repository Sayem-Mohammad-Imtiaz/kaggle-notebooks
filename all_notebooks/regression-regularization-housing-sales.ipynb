{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Hi, welcome to my project! Today we will build regression models, linear, ridge, lasso and elastic net, compute their error metrics for our case of study and choose the best one.  ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's read our csv file Ames_Housing_Sales: ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/regression/Ames_Housing_Sales.csv', sep=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find out how many unique values are in each object type column:","metadata":{}},{"cell_type":"code","source":"categorical=data.dtypes[data.dtypes == np.object]\ncategorical.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n=0\nfor x in categorical.index:\n    print (x, len(data[x].unique()))\n    n=n+len(data[x].unique())\nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we see 258 corresponds to the sum of all unique values for each column, so after one-hot encoding them we should get 258 features.","metadata":{}},{"cell_type":"markdown","source":"# Encoding of categorical variables:","metadata":{}},{"cell_type":"markdown","source":"Let's create a list of categorial data and one-hot encode them. Pandas one-hot encoder (get_dummies) works well with data that is defined as a categorical.","metadata":{}},{"cell_type":"code","source":"categorical_var = data.dtypes[data.dtypes == np.object]  \ncategorical_var = categorical_var.index.tolist()  # list of categorical fields","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_var","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding using get_dummies function:","metadata":{}},{"cell_type":"code","source":"data=pd.get_dummies(data, columns=categorical_var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see above, the number of features has increased considerably because of the encoding.","metadata":{}},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We confirm that 258 features were obtained by the encoding of categorical variables as we said before.\nNext, split the data in train and test data sets:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a number of columns which correspond to skewed features, as we know a log transformation can be applied to them. Take into account that our label \"SalePrice\" is in this group too, we will leave it in such group only to see its corresponding skew, but then we will omit it from the transformation. ","metadata":{}},{"cell_type":"code","source":"# Create a list of float colums to check for skewing\nmask = data.dtypes == np.float\nfloat_cols = data.columns[mask]\nfloat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(float_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's print the skew of each feature in the training dataset:","metadata":{}},{"cell_type":"code","source":"train[float_cols].skew().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_limit = 0.75\nskew_vals = train[float_cols].skew()\n\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {0}'.format(skew_limit)))\n\nskew_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Apply the log transformation to the features with skew greater than 0.75 excluding the label:","metadata":{}},{"cell_type":"code","source":"for col in skew_cols.index.tolist():\n    if col == \"SalePrice\":\n        continue\n    train[col] = np.log1p(train[col])\n    test[col]  = np.log1p(test[col])  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define X_train, X_test, y_train and y_test:","metadata":{}},{"cell_type":"code","source":"feature_cols = [x for x in train.columns if x != 'SalePrice']\nX_train = train[feature_cols]\ny_train = train['SalePrice']\n\nX_test = test[feature_cols]\ny_test = test['SalePrice']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to obtain error metric for this model we will declare a function \"rmse\" which takes the actual and predicted values and returns the root-mean-squared error. Use sklearn's mean_squared_error.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y_true,y_predicted):\n    return np.sqrt(mean_squared_error(y_true,y_predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building our regression models: \n### We will start with a basic linear regression model and compute its root-mean-squared error.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinearr=LinearRegression().fit(X_train,y_train)\nLinearRegression_rmse=rmse(y_test,linearr.predict(X_test))\nprint(LinearRegression_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take into account this rmse as we will build models with regularization and best hyperparameters to reduce error metrics.","metadata":{}},{"cell_type":"markdown","source":"Plotting the predicted vs actual sale price based on the model.","metadata":{}},{"cell_type":"code","source":"f = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nax.plot(y_test, linearr.predict(X_test), marker='o', ls='', ms=3.0)\n\nlim = (0, y_test.max())\n\nax.set(xlabel='Actual Price', \n       ylabel='Predicted Price', \n       xlim=lim,\n       ylim=lim,\n       title='Linear Regression Results');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's fit a linear regression with L2 regularization \"Ridge\" and Cross Validation to find the best alpha value:\n","metadata":{}},{"cell_type":"code","source":"# Mute the sklearn warning about regularization\nimport warnings\nwarnings.filterwarnings('ignore', module='sklearn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\nalphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n\nRidgeCV = RidgeCV(alphas=alphas, cv=4).fit(X_train,y_train)\nRidgeCV_rmse = rmse(y_test,RidgeCV.predict(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(RidgeCV.alpha_,RidgeCV_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the same linear regression but now with L1 regularization \"Lasso\" and Cross Validation:","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\nalphas2 = np.array([1e-5, 5e-5, 0.0001, 0.0005])\n\nlassoCV = LassoCV(alphas=alphas2, max_iter=5e4, cv=3).fit(X_train, y_train)\n\nlassoCV_rmse = rmse(y_test, lassoCV.predict(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lassoCV.alpha_, lassoCV_rmse)  # Lasso is slower","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can determine how many of these coefficients remain non-zero.","metadata":{}},{"cell_type":"code","source":"len(lassoCV.coef_) # Remember that number of coefficients is the same as number of features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"L1 regularization \"Lasso\" will selectively shrink coefficients, some of them until zero, thus performing feature elimination, following we can see how many of our coefficients are not zero after lasso:","metadata":{}},{"cell_type":"code","source":"len(lassoCV.coef_.nonzero()[0])  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see above that our model has 294 coefficients, where 22 of them are zero or we could say the features were eliminated.","metadata":{}},{"cell_type":"markdown","source":"### Now try Elastic Net, with the same alphas as in Lasso, and l1_ratios between 0.1 and 0.9:","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\n\nl1_ratios = np.linspace(0.1, 0.9, 9)\n\nelasticNetCV = ElasticNetCV(alphas=alphas2, \n                            l1_ratio=l1_ratios,\n                            max_iter=1e4).fit(X_train, y_train)\nelasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing the RMSE calculation for every model in a table:","metadata":{}},{"cell_type":"code","source":"new_df = [['Linear', LinearRegression_rmse], ['Ridge',RidgeCV_rmse], ['Lasso', lassoCV_rmse], ['ElasticNet',elasticNetCV_rmse]]\ntable_rmse=pd.DataFrame(new_df,columns=['Model','RMSE']).set_index('Model')\ntable_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From the table above we can conclude that our Ridge regression is the best model for our case of study, due to the fact that has the lowest RMSE.**","metadata":{}},{"cell_type":"markdown","source":"# Plotting actual vs predicted house prices for the three models:","metadata":{}},{"cell_type":"code","source":"lim = (0, y_test.max())\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nfig.suptitle('Regression Results:')\nax1.plot(y_test, RidgeCV.predict(X_test), marker='o', ls='', ms=3.0, color='green')\nax1.set(xlabel='Actual Price', ylabel='Predicted Price', xlim=lim, ylim=lim, title='RidgeCV Results')\nax2.plot(y_test, lassoCV.predict(X_test), marker='o', ls='', ms=3.0, color='red')\nax2.set(xlabel='Actual Price', ylabel='Predicted Price', xlim=lim, ylim=lim, title='LassoCV Results')\nax3.plot(y_test, elasticNetCV.predict(X_test), marker='o', ls='', ms=3.0, color='blue')\nax3.set(xlabel='Actual Price', ylabel='Predicted Price', xlim=lim, ylim=lim, title='ElasticNetCV Results')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}