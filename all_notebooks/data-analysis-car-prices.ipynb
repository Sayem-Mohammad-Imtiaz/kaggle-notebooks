{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Hello!, Welcome to my first project!, today I will show you how to explore your data and build some basic models using sklearn library in order to predict car price based on using one or several features.\n\nFor this project we will use a dataset created by an automobile importer, which stores several characteristics of cars and its corresponding prices. \n### Let's get started!","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load data and store in dataframe df:","metadata":{}},{"cell_type":"code","source":"# path of data \nfilename = '../input/auto-eda/automobileEDA.csv'\ndf = pd.read_csv(filename)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking for nan or null values:","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=df[df.isna().any(axis=1)]\ndf1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing nan values:","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see distribution of column types in our dataframe:","metadata":{}},{"cell_type":"code","source":"df.dtypes.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df.select_dtypes(include=['object'])).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='body-style',y='price',data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featurecols=df.drop(['price'],axis=1)\nlabel=df['price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featurecols.corrwith(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abs(featurecols.corrwith(label)).sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Linear Regression and Polynomial Regression","metadata":{}},{"cell_type":"markdown","source":"#### Let's load the modules for linear regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create the linear regression object","metadata":{}},{"cell_type":"code","source":"lm = LinearRegression()\nlm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### How could Highway-mpg help us predict car price?","metadata":{}},{"cell_type":"code","source":"X = df[['highway-mpg']]\nY = df['price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now fit the linear model using highway-mpg as feature and price as label.","metadata":{}},{"cell_type":"code","source":"lm.fit(X,Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make a prediction of the model we have to use the '.predict( )' and using X as its argument.","metadata":{}},{"cell_type":"code","source":"Yhat=lm.predict(X)\nYhat[0:5]  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using seaborn library we can easily make a regression plot of these:","metadata":{}},{"cell_type":"code","source":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\nsns.regplot(x=\"highway-mpg\", y=\"price\", data=df)\nplt.ylim(0,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The regression plot does not seem too accurate for this feature, we can see several points far from the line, which is indicative of underfitting, for this reason we will use a residual plot from seaborn which measures and plots the difference between the predicted and the actual point: ","metadata":{}},{"cell_type":"code","source":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\nsns.residplot(df['highway-mpg'], df['price'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In models with more accuracy we can expect the residual plot to concentrate much more points near zero in the y-axis. ","metadata":{}},{"cell_type":"markdown","source":"As this is a linear regression function, we expect to obtain its intercept and slope:","metadata":{}},{"cell_type":"markdown","source":"#### What is the value of the intercept (a)?","metadata":{}},{"cell_type":"code","source":"lm.intercept_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What is the value of the Slope (b)?","metadata":{}},{"cell_type":"code","source":"lm.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Error metrics for regression model:\nAs we are dealing with predicting a continuous value, we calculate the errors using mean squared error and coefficient of determination (R2 score):","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(Y, Yhat))\nmse = mean_squared_error(Y, Yhat)\nprint('The mean square error of price and predicted value is: ', mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We saw earlier in the two plots and now in error metrics that a linear model did not provide the best fit while using highway-mpg as the predictor variable, but we could improve this accuracy by transforming this feature to a polynomial type. ","metadata":{}},{"cell_type":"markdown","source":"# Polynomial features \nConsidered as a particular case of the general linear regression model or multiple linear regression models, we get non-linear relationships by squaring or setting higher-order terms of the predictor variables.","metadata":{}},{"cell_type":"markdown","source":"### Let's transform our feature to polynomial and fit a new model:","metadata":{}},{"cell_type":"code","source":"X = df[['highway-mpg']]\nY = df['price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npr=PolynomialFeatures(degree=6)  #Defining our function to convert our feature to a 6th degree polynomial\npoly_feat=pr.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we see our actual feature, an then the 6th degree polynomial created from this: ","metadata":{}},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(poly_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have to consider this last one as our new \"predictor variable\" despite the fact that it contains 7 columns, we we fit a new linear regression and predict as before:","metadata":{}},{"cell_type":"code","source":"lm_poly=LinearRegression()\nlm_poly.fit(poly_feat,Y)\npoly_pred=lm_poly.predict(poly_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(label, poly_pred))\nmse = mean_squared_error(label, poly_pred)\nprint('The mean square error of price and predicted value is: ', mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we see the error metrics has improved considerably, making our model much more accurate.","metadata":{}},{"cell_type":"markdown","source":"We will use the following code to plot the polynomial function: ","metadata":{}},{"cell_type":"code","source":"def PlotPolly(model, independent_variable, dependent_variabble, Name):\n    x_new = np.linspace(15, 55, 100)\n    y_new = model(x_new)\n\n    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n    ax = plt.gca()\n    ax.set_facecolor((0.898, 0.898, 0.898))\n    fig = plt.gcf()\n    plt.xlabel(Name)\n    plt.ylabel('Price of Cars')\n\n    plt.show()\n    plt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df['highway-mpg']\ny = df['price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = np.polyfit(x, y, 6)\np = np.poly1d(f)\nprint(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PlotPolly(p, x, y, 'highway-mpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we confirm the increase in accuracy was because the function fits more the data points, improving the prediction.","metadata":{}},{"cell_type":"markdown","source":"# 2. Multiple linear Regression\nAs we want to get the highest possible accurary from our models we will achieve this when we make use of all posible variables incluiding the categorical (nominal and ordinal), to achieve this we must transform these features using LabelBinarizer, LabelEncoder and OneHotEncoder, but for the current project we will only use numerical features so as to keep our focus on developing our models. We will deal with categorical variables the next project in which we will build more complex models.","metadata":{}},{"cell_type":"markdown","source":"### Now let's fit a model with all numerical features:","metadata":{}},{"cell_type":"code","source":"numerical_cols=featurecols.select_dtypes(exclude=['object']) #Select all columns which are not object type.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm2=LinearRegression()    #We will use the same object because the only thing different than before is the multiple predictors. \nlm2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit the linear model using all of our numeric features above.","metadata":{}},{"cell_type":"code","source":"lm2.fit(numerical_cols, label) #Fitting our numerical columns as predictors and price as label.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we know the function of our model will have one coefficient by each feature\nWe should get a final linear function with the following structure:\n$$\nYhat = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4 + ... + b_n X_n\n$$","metadata":{}},{"cell_type":"markdown","source":"#### What is the value of the intercept(a)?","metadata":{}},{"cell_type":"code","source":"lm2.intercept_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What are the values of the coefficients (b1, b2, b3, b4, ... , bn)?","metadata":{}},{"cell_type":"code","source":"lm2.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_predicted = lm2.predict(numerical_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How do we visualize a model for Multiple Linear Regression? This gets a bit more complicated because we can't visualize it with regression or residual plot as before.\n\nOne way to look at the fit of the model is by looking at the distribution plot: We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(Y_predicted, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of Cars')\n\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(label, Y_predicted))\nmse = mean_squared_error(label, Y_predicted)\nprint('The mean square error of price and predicted value is: ', mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement.","metadata":{}},{"cell_type":"markdown","source":"### Now, we are going to create polynomial features, then standardize every column and finally feed our linear model with these features:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr=PolynomialFeatures(degree=2)\npr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly_feat=pr.fit_transform(numerical_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initially we had 18 features to use as predictors and after converting them to polynomial 2nd degree we see below the total number of features now has increased to 190.","metadata":{}},{"cell_type":"code","source":"poly_feat.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\npoly_feat=scaler.fit_transform(poly_feat)  #Apply standardization to our polynomial features ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we will use this processed features to feed our model and expect a much better performance: ","metadata":{}},{"cell_type":"code","source":"lm4=LinearRegression()\nlm4.fit(poly_feat,label)   \nYpoly_predicted = lm4.predict(poly_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(Ypoly_predicted, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of Cars')\n\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see the performance of this last model is almost perfect!, both curves are almost the same and we can see the difference between them quantified by computing the error metrics.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(label, Ypoly_predicted))\nmse = mean_squared_error(label, Ypoly_predicted)\nprint('The mean square error of price and predicted value is: ', mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we could predict the price of a car with a relatively high accuracy by only giving the features of the new one and use these in the argument of the function lm4.predict( )**","metadata":{}}]}