{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom math import sqrt\nimport seaborn as sns\nfrom scipy import stats\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Preparation**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read the data\ndf=pd.read_csv(\"../input/19000-spotify-songs/song_data.csv\")\ndf_info=pd.read_csv(\"../input/19000-spotify-songs/song_info.csv\")\n\n#remove the duplicates\ndf = df.drop_duplicates(subset=['song_name'])\n\n# Join the categorical variables\nnames=df_info['artist_name']\nplaylist = df_info['playlist']\nalbum = df_info['album_names']\ndf = df.join(names)\ndf = df.join(playlist)\ndf = df.join(album)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete records with popularity equal to zero (wrong values)\nindex_delete = df.index[df['song_popularity']==0]\ndf = df.drop(index_delete)\n\n#check that there is no null value\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I want to use also the categorical variables, in order to predict the popularity. Each unique value is assigned to a different integer."},{"metadata":{"trusted":true},"cell_type":"code","source":"leble_en=preprocessing.LabelEncoder()\ndf['artist_name']=leble_en.fit_transform(df['artist_name'])\ndf['playlist']=leble_en.fit_transform(df['playlist'])\ndf['album_names']=leble_en.fit_transform(df['album_names'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the variable \"time_signature\", which can assume only 5 values, I use the One-Hot Encoding. A column for each value is created"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.get_dummies(data=df,columns=['time_signature'])\n\ndf.head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features Selection**"},{"metadata":{},"cell_type":"markdown","source":"Now I want to remove redundant data, which does not affect accuracy, or can even reduce the accuracy. So I want to select a subset of features among those I have, in order to remove the noise and reduce the training time. To do that I look to the linear correlation between the features. I observe that these attributes, 'audio_mode','key','speechiness','album_names','song_duration_ms', have a very low correlation with the target: I try to remove them and the result is an improvement of the accuracy. Removing these features I avoid overfitting. Furthermore I notice that there is an high correlation between 'energy' and 'loudness': this means that both represent the same information. The accuracy improves removing the attribute 'energy'."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(24, 10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the features \ndf = df.drop(['song_name','audio_mode','key','speechiness','album_names','song_duration_ms','energy'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Outliers Detection**"},{"metadata":{},"cell_type":"markdown","source":"I want to find out if are there any outliers, wrong values that can bring noise to the model. \nWith the use of the boxplot many outliers seem to be spotted in these attributes, but their removal leads to a worsening of the efficiency. These are real data!","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['liveness'])\nplt.show()\nsns.boxplot(x=df['loudness'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only outlier, whose removal gives a better performance is \"tempo\" == 0","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['tempo'])\nplt.show()\n\nindex_delete = df.index[df['tempo']==0]\ndf = df.drop(index_delete)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Scaling**"},{"metadata":{},"cell_type":"markdown","source":"All the data have different numerical scales. I want to scale the data so that the algorithm can perform the calculations better. I use the Min-Max Scaler, that preserves the original distribution of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.MinMaxScaler()\ndf_scal = scaler.fit_transform(df)\ndf_scal = pd.DataFrame(df_scal, columns = df.columns)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(df['loudness'], ax=ax1)\nsns.kdeplot(df['danceability'], ax=ax1)\nsns.kdeplot(df['tempo'], ax=ax1)\nax2.set_title('After Min-Max Scaling')\nsns.kdeplot(df_scal['loudness'], ax=ax2)\nsns.kdeplot(df_scal['danceability'], ax=ax2)\nsns.kdeplot(df_scal['tempo'], ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train, Validation and Test**"},{"metadata":{},"cell_type":"markdown","source":"The dataset is split in three parts: train (60%), validation (20%) and test (20%).\nThe model is fitted with the train set and subsequently, varying the parameters, it is evaluated on the validation set, in order to get the best combination. The algorithm chosen is ****Random Forest****.","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate the target\nx=df_scal.drop(['song_popularity'],axis=1)\ny=df_scal['song_popularity']\n\n# train, validation and test\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=20)\nx_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.20,random_state=20)\n\n# range of the Hyperparameters\nmax_depth=[5,20, 50, 90]\nmin_samples_leaf = [1, 3, 5]\nmin_samples_split = [2, 6, 12]\n# range of the parameter for the number of trees\nn_estimators = [100,300,500]\n\nbest_err = 1\nfor num in n_estimators:\n    for min_split in min_samples_split:\n        for min_leaf in min_samples_leaf:\n            for depth in max_depth:\n                rf=RandomForestRegressor(n_estimators=num,max_depth=depth,max_features='sqrt',min_samples_leaf=min_leaf,min_samples_split=min_split, random_state=42)\n                rf.fit(x_train,y_train)\n                prediction=rf.predict(x_val)\n                err = sqrt(metrics.mean_squared_error(y_val, prediction))\n                if(err < best_err):\n                    best_err = err\n                    best_num = num\n                    best_split = min_split\n                    best_leaf = min_leaf\n                    best_depth = depth\nrf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I initialize a new random forest with the parameters founded previously. It is trained on the train+validation set and is tested on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_x_train = x_train.append(x_val)\nnew_y_train = y_train.append(y_val)\n\nrandomForest = RandomForestRegressor(n_estimators=best_num,max_depth=best_depth,max_features='sqrt',min_samples_leaf=best_leaf,min_samples_split=best_split, random_state=42)\nrandomForest.fit(new_x_train, new_y_train)\ntest_prediction = randomForest.predict(x_test)\n\nprint('Root Mean Squared Error:', sqrt(metrics.mean_squared_error(y_test, test_prediction)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}