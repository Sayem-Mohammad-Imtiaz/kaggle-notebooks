{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport plotly\nfrom plotly.offline import iplot\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport cufflinks as cf\n\ncf.go_offline()\nplotly.offline.init_notebook_mode()\ncf.set_config_file(world_readable=True, theme='space', offline=True)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier\nfrom sklearn.svm import OneClassSVM\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom xgboost import XGBClassifier\n\nimport optuna","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-26T23:38:22.432936Z","iopub.execute_input":"2021-08-26T23:38:22.433445Z","iopub.status.idle":"2021-08-26T23:38:27.65767Z","shell.execute_reply.started":"2021-08-26T23:38:22.433337Z","shell.execute_reply":"2021-08-26T23:38:27.656832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/loan-prediction-based-on-customer-behavior/Training Data.csv\", index_col='Id')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:27.659042Z","iopub.execute_input":"2021-08-26T23:38:27.659543Z","iopub.status.idle":"2021-08-26T23:38:28.767854Z","shell.execute_reply.started":"2021-08-26T23:38:27.659476Z","shell.execute_reply":"2021-08-26T23:38:28.766884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"feats = df.columns[:-1]\nis_cat = np.array([df[f].dtype == 'object' for f in feats])\ncat_feats, num_feats = feats[is_cat].tolist(), feats[~is_cat].tolist()\nprint(cat_feats, num_feats, sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:28.769713Z","iopub.execute_input":"2021-08-26T23:38:28.770011Z","iopub.status.idle":"2021-08-26T23:38:28.796985Z","shell.execute_reply.started":"2021-08-26T23:38:28.769982Z","shell.execute_reply":"2021-08-26T23:38:28.795947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_hists(df: pd.DataFrame) -> None:\n    fig = plt.figure(figsize=(12, 8))\n    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n    for i, f in enumerate(df.columns):\n        axis = fig.add_subplot(3, 2, i + 1)\n        axis.hist(df[f])\n        if df[f].dtype == 'object' and df[f].nunique() > 3:\n            axis.set(xlabel=None, title=f)\n        else:\n            axis.set(title=f)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:28.798477Z","iopub.execute_input":"2021-08-26T23:38:28.798772Z","iopub.status.idle":"2021-08-26T23:38:28.807043Z","shell.execute_reply.started":"2021-08-26T23:38:28.798744Z","shell.execute_reply":"2021-08-26T23:38:28.806326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_hists(df[num_feats])\nshow_hists(df[cat_feats[:3]])","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:28.8083Z","iopub.execute_input":"2021-08-26T23:38:28.808904Z","iopub.status.idle":"2021-08-26T23:38:30.49668Z","shell.execute_reply.started":"2021-08-26T23:38:28.80886Z","shell.execute_reply":"2021-08-26T23:38:30.495764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feat_likelihoods(\n    df: pd.DataFrame,\n    feat: str,\n    target: str,\n    is_num=False\n) -> pd.Series:\n    lh = (df[[feat, target]].groupby(feat).mean() - df[target].mean())[target].copy()\n    if lh.size > 10 and not is_num:\n        lh.sort_values(ascending=False, inplace=True)\n    return lh","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:30.498024Z","iopub.execute_input":"2021-08-26T23:38:30.498311Z","iopub.status.idle":"2021-08-26T23:38:30.504562Z","shell.execute_reply.started":"2021-08-26T23:38:30.498284Z","shell.execute_reply":"2021-08-26T23:38:30.503439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lh_by_cat_feats = pd.Series({f: feat_likelihoods(df, f, 'Risk_Flag') for f in cat_feats})\nlh_by_num_feats = pd.Series({f: feat_likelihoods(df, f, 'Risk_Flag', is_num=True) for f in num_feats[1:]})\nlikelihoods = pd.concat([lh_by_cat_feats, lh_by_num_feats])\n\nfor f, lh in likelihoods.items():\n    if lh.size > 10:\n        lh.iplot(orientation='h', title=f)\n    else:\n        lh.iplot(kind='bar', orientation='h', title=f)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:30.506122Z","iopub.execute_input":"2021-08-26T23:38:30.506413Z","iopub.status.idle":"2021-08-26T23:38:32.129122Z","shell.execute_reply.started":"2021-08-26T23:38:30.506386Z","shell.execute_reply":"2021-08-26T23:38:32.126744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical feats encoding","metadata":{}},{"cell_type":"markdown","source":"**Married/Single** and **Car_Ownership** are easy to encode.    \nFor **CITY** and **STATE** we will try target encoding.  \nFor **Profession** we can use one-hot encoding after replacing each profession with its category (software, medicine, ...).","metadata":{}},{"cell_type":"code","source":"prof_categories = {\n    'Mechanical_engineer'       : 'Tech',\n    'Software_Developer'        : 'Soft',\n    'Technical_writer'          : 'Tech',\n    'Civil_servant'             : 'Gov',\n    'Economist'                 : 'Math|Sc',\n    'Flight_attendant'          : 'Avia',\n    'Architect'                 : 'Design|Tech|Draw',\n    'Designer'                  : 'Design',\n    'Physician'                 : 'Sc|Tech',\n    'Financial_Analyst'         : 'Fin|Math|Sc',\n    'Air_traffic_controller'    : 'Avia',\n    'Politician'                : 'Gov',\n    'Police_officer'            : 'Law|Force',\n    'Artist'                    : 'Art|Draw',\n    'Surveyor'                  : 'Geo',\n    'Design_Engineer'           : 'Design|Tech',\n    'Chemical_engineer'         : 'Chem|Tech',\n    'Hotel_Manager'             : 'Fin|Manage',\n    'Dentist'                   : 'Med',\n    'Comedian'                  : 'Art',\n    'Biomedical_Engineer'       : 'Bio|Tech',\n    'Graphic_Designer'          : 'Design',\n    'Computer_hardware_engineer': 'Soft|Tech',\n    'Petroleum_Engineer'        : 'Tech|Chem|Geo',\n    'Computer_operator'         : 'Soft|Tech',\n    'Chartered_Accountant'      : 'Fin|Staff|Office',\n    'Microbiologist'            : 'Bio|Sc',\n    'Fashion_Designer'          : 'Design',\n    'Technician'                : 'Tech',\n    'Aviator'                   : 'Avia',\n    'Psychologist'              : 'Med|Sc',\n    'Magistrate'                : 'Law|Gov',\n    'Lawyer'                    : 'Law',\n    'Engineer'                  : 'Tech',\n    'Official'                  : 'Gov',\n    'Analyst'                   : 'Math|Sc|Office',\n    'Geologist'                 : 'Geo|Sc',\n    'Drafter'                   : 'Tech|Soft|Draw',\n    'Statistician'              : 'Math|Sc',\n    'Web_designer'              : 'Design',\n    'Army_officer'              : 'Gov|Force',\n    'Surgeon'                   : 'Med',\n    'Scientist'                 : 'Sc',\n    'Civil_engineer'            : 'Tech',\n    'Industrial_Engineer'       : 'Tech',\n    'Technology_specialist'     : 'Tech',\n    'Firefighter'               : 'Rescue',\n    'Consultant'                : 'Staff',\n    'Chef'                      : 'Food',\n    'Secretary'                 : 'Fin|Staff|Office',\n    'Librarian'                 : 'Staff'\n}\n\ndf['Profession'].replace(prof_categories, inplace=True)\ndf = pd.concat([df, df['Profession'].str.get_dummies('|')], axis=1)\ndf.drop(columns=['Profession'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:32.13083Z","iopub.execute_input":"2021-08-26T23:38:32.131245Z","iopub.status.idle":"2021-08-26T23:38:39.917452Z","shell.execute_reply.started":"2021-08-26T23:38:32.131197Z","shell.execute_reply":"2021-08-26T23:38:39.916378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Own_House']  = (df['House_Ownership'] == 'owned')\ndf['Rent_House'] = (df['House_Ownership'] == 'rented')\ndf.drop(columns=['House_Ownership'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:39.921123Z","iopub.execute_input":"2021-08-26T23:38:39.921586Z","iopub.status.idle":"2021-08-26T23:38:40.034668Z","shell.execute_reply.started":"2021-08-26T23:38:39.92154Z","shell.execute_reply":"2021-08-26T23:38:40.033681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_binary = ['Married/Single', 'Car_Ownership']\nto_target_encoding = ['STATE', 'CITY']\n\ndef encode_categories(data: pd.DataFrame) -> None:\n    for bf in to_binary:\n        encoding = {name: code for code, name in enumerate(data[bf].unique())}\n        data[bf].replace(encoding, inplace=True)\n    for tef in to_target_encoding:\n        data[tef].replace(likelihoods[tef], inplace=True)\n    data.rename(columns={f: f+\"_lh\" for f in to_target_encoding}, inplace=True)\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:40.036224Z","iopub.execute_input":"2021-08-26T23:38:40.036498Z","iopub.status.idle":"2021-08-26T23:38:40.042949Z","shell.execute_reply.started":"2021-08-26T23:38:40.036471Z","shell.execute_reply":"2021-08-26T23:38:40.042004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = encode_categories(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:40.044326Z","iopub.execute_input":"2021-08-26T23:38:40.044631Z","iopub.status.idle":"2021-08-26T23:38:44.60359Z","shell.execute_reply.started":"2021-08-26T23:38:40.044602Z","shell.execute_reply":"2021-08-26T23:38:44.602698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model selection","metadata":{}},{"cell_type":"markdown","source":"As you can see, dataset is imbalanced. Because of this we will try to solve the task as anomaly detection and as imbalanced classification using SMOTE oversampling.","metadata":{}},{"cell_type":"code","source":"def scale(train_X: pd.DataFrame, test_X: pd.DataFrame) -> None:\n    scaler = StandardScaler()\n    train_X = scaler.fit_transform(train_X)\n    test_X = scaler.transform(test_X)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:44.604831Z","iopub.execute_input":"2021-08-26T23:38:44.605126Z","iopub.status.idle":"2021-08-26T23:38:44.609449Z","shell.execute_reply.started":"2021-08-26T23:38:44.605096Z","shell.execute_reply":"2021-08-26T23:38:44.608706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_predict(model, train_X, train_y, test_X):\n    model.fit(train_X, train_y)\n    return pd.Series(model.predict(test_X))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:44.610471Z","iopub.execute_input":"2021-08-26T23:38:44.610872Z","iopub.status.idle":"2021-08-26T23:38:44.623574Z","shell.execute_reply.started":"2021-08-26T23:38:44.610844Z","shell.execute_reply":"2021-08-26T23:38:44.622762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_confusion_matrix(actual, predict):\n    cfm = confusion_matrix(actual, predict)\n    group_names = ['TN', 'FP', 'FN', 'TP']\n    group_percentages = [\n        '{0:.2%}'.format(value)\n        for value in cfm.flatten() / np.sum(cfm)\n    ]\n    labels = [\n        f\"{v2}\\n{v3}\"\n        for  v2, v3 in zip(group_names, group_percentages)\n    ]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cfm, annot=labels, fmt='', cmap='inferno')\n    plt.show()\n\ndef try_models(clfs, X_train, y_train, X_test, fit_predict=fit_predict):\n    for clf_name, clf_model in clfs.items():\n        real_y, pred_y = y_test, fit_predict(clf_model, X_train, y_train, X_test)\n        print(f\"{clf_name}:\")\n        print(f\"Accuracy: {round(accuracy_score(real_y, pred_y) * 100, 1)}%\")\n        print(f\"ROC-AUC: {round(roc_auc_score(real_y, pred_y), 3)}\\n\")\n        show_confusion_matrix(real_y, pred_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:44.624994Z","iopub.execute_input":"2021-08-26T23:38:44.625585Z","iopub.status.idle":"2021-08-26T23:38:44.636507Z","shell.execute_reply.started":"2021-08-26T23:38:44.625539Z","shell.execute_reply":"2021-08-26T23:38:44.63575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task - anomalies detection.","metadata":{}},{"cell_type":"code","source":"X, y = df.drop(columns=['Risk_Flag']), df['Risk_Flag']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nscale(X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:44.637885Z","iopub.execute_input":"2021-08-26T23:38:44.638453Z","iopub.status.idle":"2021-08-26T23:38:46.830044Z","shell.execute_reply.started":"2021-08-26T23:38:44.638408Z","shell.execute_reply":"2021-08-26T23:38:46.82897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anomalies_fp(model, train_X, train_y, test_X):\n    return fit_predict(model, train_X, train_y, test_X).replace({-1: 1, 1: 0})\n\nclfs = {\n    'OneClassSVM': OneClassSVM(),\n    'IsolationForest': IsolationForest(random_state=0)\n}\ntry_models(clfs, X_train, y_train, X_test, anomalies_fp)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T23:38:46.83137Z","iopub.execute_input":"2021-08-26T23:38:46.831783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Conclusion.\n* Both models make too many type I errors, so they can't be used.","metadata":{}},{"cell_type":"markdown","source":"# Task - imbalanced classification.","metadata":{}},{"cell_type":"code","source":"def smote_oversampling(train_X: pd.DataFrame, train_y: pd.Series) -> None:\n    smote = SMOTE(random_state=0)\n    train_X, train_y = smote.fit_resample(train_X, train_y)\n    \nX, y = df.drop(columns=['Risk_Flag']), df['Risk_Flag']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nsmote_oversampling(X_train, y_train)\nscale(X_train, X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clfs = {\n    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n    \"Logistic Regression\": LogisticRegression(solver='liblinear'),\n    \"Random Forest\": RandomForestClassifier(random_state=0),\n    \"XGBoost Classifier\": XGBClassifier(\n        n_estimators=5000, eval_metric='auc',\n        use_label_encoder=False)\n}\n\ntry_models(clfs, X_train, y_train, X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Conclusion.\n* Logistic Regression never detect loan defaults, so it can't be used\n* Precisions:\n    - Decision Tree ~ 0.52\n    - Random Forest ~ 0.61\n    - XGBoost       ~ 0.59\n* Random Forest is model with the best precision\n* Decision Tree is the model with the best roc-auc score","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameters tuning","metadata":{}},{"cell_type":"code","source":"def rf_objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int('n_estimators', 10, 100),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\", [7, 8, 9, 10, 11, 12, None]),\n        \"criterion\": trial.suggest_categorical('criterion', [\"gini\", \"entropy\"]),\n        \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 5),\n        \"min_samples_leaf\": trial.suggest_categorical('min_samples_leaf', [1, 2]),\n        \"max_features\": trial.suggest_categorical('max_features', [\"auto\", \"sqrt\", \"log2\"]),\n        \"class_weight\": trial.suggest_categorical('class_weight', [\"balanced\"]),\n        \"random_state\": trial.suggest_categorical('random_state', [0]),\n        \"n_jobs\": trial.suggest_categorical('n_jobs', [-1]),\n    }\n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    return -roc_auc_score(y_test, model.predict(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dt_objective(trial):\n    params = {\n        \"criterion\": trial.suggest_categorical('criterion', [\"gini\", \"entropy\"]),\n        \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 5),\n        \"min_samples_leaf\": trial.suggest_categorical('min_samples_leaf', [1, 2]),\n        \"max_features\": trial.suggest_categorical('max_features', [\"auto\", \"sqrt\", \"log2\"]),\n        \"class_weight\": trial.suggest_categorical('class_weight', [\"balanced\"]),\n        \"random_state\": trial.suggest_categorical('random_state', [0])\n    }\n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    return -roc_auc_score(y_test, model.predict(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best score is 0.8412\n# study = optuna.create_study()\n# study.optimize(rf_objective, n_trials=200, timeout=3600 * 2)\n# print(f\"Best RandomForest ROC-AUC: {-round(study.best_value, 4)} with parameters {study.best_params}\\n\\n\")\n\nrf_best_score = 0.8438\nrf_best_params = {\n    'n_estimators': 10, 'criterion': 'entropy',\n    'min_samples_split': 2, 'min_samples_leaf': 2,\n    'max_features': 'log2', 'class_weight': 'balanced',\n    'random_state': 0, 'n_jobs': -1}\nrf = RandomForestClassifier(**rf_best_params)\nrf.fit(X_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best score is 0.8409\n# study = optuna.create_study()\n# study.optimize(dt_objective, n_trials=200, timeout=3600 * 2)\n# print(f\"Best DecisionTree ROC-AUC: {-round(study.best_value, 4)} with parameters {study.best_params}\\n\\n\")\n\ndt_best_score = 0.8409\ndt_best_params = {\n    'criterion': 'gini', 'min_samples_split': 2,\n    'min_samples_leaf': 2, 'max_features': 'sqrt',\n    'class_weight': 'balanced', 'random_state': 0}\ndt = DecisionTreeClassifier(**dt_best_params)\ndt.fit(X_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Random Forest Best: {rf_best_score}\")\nshow_confusion_matrix(y_test, rf.predict(X_test))\n\nprint(f\"Decision Tree Best: {dt_best_score}\")\nshow_confusion_matrix(y_test, dt.predict(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"markdown","source":"***Random Forest gives more accurate predictions, and its precision is greater. But Decision Tree can also be used, it have greater recall.***","metadata":{}}]}