{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn import linear_model\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/startup-logistic-regression/50_Startups.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Missing Data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, kind='reg', diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spilitting The Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.iloc[:,:4].values\ny = df.loc[:, 'Profit'].values\nprint('features = ', x)\nprint('\\ntarget = ', y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Appliying OneHotEncoding\n#### Using ColumnTransformer on state column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntransformer = ColumnTransformer(transformers = [(\"asda\",OneHotEncoder(),[3])],remainder = 'passthrough')\n\nx = transformer.fit_transform(x)\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Avoiding Dummy variable trap\n#### For more info :- https://www.geeksforgeeks.org/ml-dummy-variable-trap-in-regression-models/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x[:,1:]\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into test and training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalization of Data\n#### using StandardScaler","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scalar = StandardScaler()\nx_train[:,3:] = scalar.fit_transform(x_train[:,3:]) \nx_test[:,3:] =scalar.fit_transform(x_test[:,3:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying simple Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LinearRegression()\nmodel.fit(x_train,y_train)\nbscr = model.score(x_test,y_test)\nprint(bscr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding the Dummy Feature\n\nThe equation of our line (or rather, plane) is y=b+m1.x1+m2.x2+m3.x3+m4.x4.\n\nWhen we make a linear model with sklearn, the bias term ‘b’ is calculated separately. However, for performing Backward elimination, we are required to use the linear model provided by statsmodels library — which does not consider the bias term. Hence, by adding a dummy feature with value as ‘1’, \n\nour equation becomes y=b.x0+m1.x1+m2.x2+m3.x3+m4.x4 where x0 = 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.append(np.ones((x.shape[0],1),dtype=np.int), values = x, axis=1)\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using OLS for Backward Elimination ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nx_opt = np.array(x[:, [0, 1, 2, 3, 4, 5]], dtype=float)\nregressor = sm.OLS(y, x_opt).fit()\nregressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_opt = np.array(x[:, [0, 1, 3, 4, 5]], dtype=float)\nregressor = sm.OLS(y, x_opt).fit()\nregressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_opt = np.array(x[:, [0, 3, 4, 5]], dtype=float)\nregressor = sm.OLS(y, x_opt).fit()\nregressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_opt = np.array(x[:, [0, 3, 5]], dtype=float)\nregressor = sm.OLS(y, x_opt).fit()\nregressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_opt = np.array(x[:, [0, 3]], dtype=float)\nregressor = sm.OLS(y, x_opt).fit()\nregressor.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Features \n\nNow we see that all features are below our significance level, which means we can no longer eliminate features.\n\nThe first 3 features x1,x2 and x3 are binary variables for the feature ‘State’ after One-Hot encoding. \n\nHence, we’re essentially left with 1 features — R&D Spend. \n\nWith these features, we’ll now create a Linear Model with Sklearn and test its score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#spliting the data into training and test \nx_opt = np.array(x[:, [3]], dtype=float)\nx_train, x_test, y_train, y_test = train_test_split(x_opt, y, test_size = 0.2, random_state = 0)\n\n# train the linear reggression model\nmodel = linear_model.LinearRegression()\nmodel.fit(x_train,y_train)\nscr = model.score(x_test,y_test)\n\nprint('Score before Backward Elimination :', bscr)\nprint('\\nFinal Score after Backward Elimination :', scr)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ----------------------------------------End----------------------------------------","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}