{"cells":[{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')\nTest = pd.read_csv(\"../input/bigmart-sales-data/Test.csv\")\nTrain = pd.read_csv(\"../input/bigmart-sales-data/Train.csv\")\nTrain.drop(['Item_Identifier','Outlet_Identifier'],axis=1, inplace = True)\n\ncate_col = [col for col in Train.columns if Train[col].dtypes == 'O']\nnum_col = [col for col in Train.columns if Train[col].dtype in ['int64','float64']]\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef simple_encoder (df,column_names=[]):\n    label_encoder= LabelEncoder()\n    if len(column_names) > 0:\n        for i in column_names:\n            df[i] = label_encoder.fit_transform(df[i])\n    elif len(column_names) == 0:\n        for i in cate_col:\n            df[i] = label_encoder.fit_transform(df[i])\n    return df\n\ndef cross_df_encoder(df1,df2, cl = []):\n    #a must bigger than b\n    a=df1.columns.tolist()\n    b=df2.columns.tolist()\n    label=LabelEncoder()\n    for i in cl:\n        #print(i)\n        try:\n            b.index(i)\n            #print('try_success')\n            df1[i] = label.fit_transform(df1[i])\n            df2[i] = label.transform(df2[i])\n        except:\n            #print('try_fail')\n            df1[i] = label.fit_transform(df1[i])\n    return df1, df2\n    \n    \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train[num_col].head()\n#we need to change Outlet_Establishment_Year into Object data type","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EDA**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.violinplot(data = Train, x='Outlet_Type', y='Item_Outlet_Sales',ax = ax[0])\nsns.violinplot(data = Train, x='Outlet_Location_Type', y='Item_Outlet_Sales', ax = ax[1])\n\n#Grocery store only sell low-cost products\n#Supermarket has many more products varying in size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.violinplot(data = Train, x='Outlet_Size', y='Item_Outlet_Sales',ax = ax[0])\nsns.violinplot(data = Train, x='Item_Fat_Content', y='Item_Outlet_Sales', ax = ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig, ax = plt.subplots(1,2, figsize= (20,5))\nz=Train['Item_Fat_Content'].value_counts().reset_index()\n#x=Train['Outlet_Identifier'].value_counts().reset_index()\nplt.bar(z.index,z.Item_Fat_Content, tick_label = z['index'])\nplt.xlabel('Item_Fat_Content')\n#ax[1].bar(x.index,x.Outlet_Identifier, tick_label = x['index'])\n#ax[1].set_xlabel('Outlet_Identifier')\n\n#Item fat content got mis label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z=Train['Item_Type'].value_counts().reset_index()\nplt.figure(figsize=(20,5))\nplt.bar(z.index,z.Item_Type, tick_label = z['index'])\nplt.xlabel('Item_Type')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CLEANING DATA**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mis_label={'low fat':'Low Fat','LF': 'Low Fat', 'reg':'Regular', 'Low Fat':'Low Fat', 'Regular': 'Regular'}\nTrain['Item_Fat_Content']=Train['Item_Fat_Content'].map(mis_label)\nTrain['Item_Fat_Content'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of null value in Outlet_Size column %s'%(round(Train['Outlet_Size'].isnull().sum()/len(Train.Outlet_Size),2)))\nprint('Percentage of null value in Item_Weight column %s'%(round(Train['Item_Weight'].isnull().sum()/len(Train.Item_Weight),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is strongly recommended that when null values contribute more than 20% of total data, we should not deal with it by DROP NA, FFILL or MOST POPULAR DATA\nbecause it will strongly affect our model accuracy\nI will illustrate how it effect the result latter\nNow we will find away to deal with null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sample= Train.copy()\ntrain_sample.dropna(axis=0, inplace = True)\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor col in cate_col:\n    train_sample[col] = label_encoder.fit_transform(train_sample[col])\n\nplt.figure(figsize=(15,5))\ntrain_sample=train_sample.corr()\nsns.heatmap(train_sample,annot=True)\n\n#At the beginning, I think about fill na values of Outlet_size column with the most popular item and Item weight with the mean\n#However, from the Heatmap below, Outlet_size has a strong correlation with Outlet_Location_Type, Outlet_Identifier, Outlet_Type and Outlet_Establishment_Year\n#we can make a prediction on that\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare some model\n\ndef loss (y_true, y_pred, retu = False):\n    pre = precision_score(y_true, y_pred, average='micro')\n    rec = recall_score(y_true, y_pred, average='micro')\n    f1 = f1_score(y_true, y_pred, average='micro')\n    #log = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    if retu:\n        return pre, rec, f1, acc\n    else:\n        print('   pre : %.3f\\n    rec : %.3f\\n    f1 : %.3f\\n    acc : %.3f\\n ' %(pre,rec,f1,acc))\n        \ndef train(X,y,models):\n    for name, model in models.items():\n        print(name + ' : ')\n        result_list=[]\n        name_loss = ['pre','rec','f1','acc']\n        for train, test in skf.split(X,y):\n            model.fit(X.iloc[train], y.iloc[train])\n            \n            y_predict = model.predict(X.iloc[test])\n\n            result_list.append(loss(y.iloc[test],y_predict, retu = True))\n        print(pd.DataFrame(np.array(result_list).mean(axis=0), index= name_loss)[0])\n        print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# because the high correlation between {Outlet_location_type, Outlet_establishment_year} and {Outlet_size}\n# I wont preproessing to increase the correlation\n\n\ntrain_sample=Train.copy()\ntrain_sample.dropna(axis=0,inplace=True)\nfor col in cate_col:\n    train_sample[col] = label_encoder.fit_transform(train_sample[col])\nX=train_sample[['Outlet_Establishment_Year','Outlet_Type','Outlet_Location_Type']]\ny=train_sample[['Outlet_Size']]\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\ntrain_models = {\n'Decision Tree Classifer ' :DecisionTreeClassifier(random_state = 42),\n'SVC ' : SVC(random_state = 42),\n'Random Forest Classifier' : RandomForestClassifier(random_state=42),\n'K Neighbors Classifier' : KNeighborsClassifier()\n}\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold (n_splits = 5, random_state =42, shuffle =True)\n\ndef loss (y_true, y_pred, retu = False):\n    pre = precision_score(y_true, y_pred, average='micro')\n    rec = recall_score(y_true, y_pred, average='micro')\n    f1 = f1_score(y_true, y_pred, average='micro')\n    #log = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    if retu:\n        return pre, rec, f1, acc\n    else:\n        print('   pre : %.3f\\n    rec : %.3f\\n    f1 : %.3f\\n    acc : %.3f\\n ' %(pre,rec,f1,acc))\n        \ndef train(X,y,models):\n    for name, model in models.items():\n        print(name + ' : ')\n        result_list=[]\n        name_loss = ['pre','rec','f1','acc']\n        for train, test in skf.split(X,y):\n            model.fit(X.iloc[train], y.iloc[train])\n            \n            y_predict = model.predict(X.iloc[test])\n\n            result_list.append(loss(y.iloc[test],y_predict, retu = True))\n        print(pd.DataFrame(np.array(result_list).mean(axis=0), index= name_loss)[0])\n        print('\\n')\n\n        \n        \nfrom sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\ntrain(X,y, train_models)\n#At this point, I am not really sure whether I overfit the model or not as the result was so high lol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_sample = Train.copy()\n\n#due to data leakage, I need to map 'Outlet_Establishment_Year' by this way\nyear_dict = {1985:1 , 1987:2 , 1997:3 , 1998:4 , 1999:5 , 2002:6 , 2004:7 , 2007:8 , 2009:9}\nTrain_sample['Outlet_Establishment_Year'] = Train_sample['Outlet_Establishment_Year'].map(year_dict)\n\nnon_null_values = Train_sample[~Train_sample.Outlet_Size.isnull()][['Outlet_Establishment_Year', 'Outlet_Location_Type','Outlet_Type', 'Outlet_Size']]\nnull_values = Train_sample[Train_sample.Outlet_Size.isnull()][['Outlet_Establishment_Year', 'Outlet_Location_Type','Outlet_Type']]\n\nnon_null_values, null_values = cross_df_encoder(df1=non_null_values,df2=null_values, cl = ['Outlet_Location_Type','Outlet_Type', 'Outlet_Size'])\n\nmodel = DecisionTreeClassifier(random_state = 42)\n\nmodel.fit(non_null_values[['Outlet_Establishment_Year', 'Outlet_Location_Type','Outlet_Type']],\n          non_null_values[['Outlet_Size']])\n\nz=model.predict(null_values[['Outlet_Establishment_Year', 'Outlet_Location_Type','Outlet_Type']])\nafter_train=pd.concat([null_values.reset_index(),pd.Series(z,name= 'Outlet_Size')],axis=1)\n\noutlet_df = pd.concat([non_null_values,after_train.set_index('index')],axis=0).sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = ['Outlet_Establishment_Year', 'Outlet_Location_Type', 'Outlet_Type', 'Outlet_Size']\nTrain[label] = outlet_df[label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dealing with Null values on Item_Weight column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df = Train.dropna(axis=0)[['Item_Fat_Content','Item_Type','Item_Weight','Item_Visibility','Item_MRP']]\ncorr_df = simple_encoder(corr_df, column_names = ['Item_Fat_Content','Item_Type'])\ncorr = corr_df.corr()\nsns.heatmap(corr, annot = True)\n\n#the correlation is quite low, let have a deeper look at data for a way to improve the corelation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3, figsize = (20,5))\nsns.distplot(Train.Item_Visibility, ax = ax[0])\nsns.distplot(Train.Item_MRP, ax = ax[1])\nsns.distplot(Train.Item_Weight, ax = ax[2])\n#sns.distplot(Train.Item_Fat_Content, ax = ax[3])\n\n\nax[0].set_xlabel('Item Visibility')\nax[1].set_xlabel('Item MRP')\nax[2].set_xlabel('Item Weight')\n#ax[3].set_xlabel('Item Fat Content')\n\n#Item Visibility got skewness\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,5, figsize = (20,5))\nsns.distplot(Train.Item_Visibility, ax = ax[0])\nsns.distplot(np.sqrt(Train.Item_Visibility), ax=ax[1])\nax[2].hist(Train.Item_Visibility, bins = 4)\nsns.distplot(Train.Item_Visibility[Train.Item_Visibility < 0.2], ax=ax[3])\nz=(Train.Item_Visibility == 0).value_counts().reset_index()\n#z['Item_Visibility'].tolist()\nax[4].bar(['Visible', 'UnVisible'],z['Item_Visibility'].tolist())\n\nax[0].set_xlabel('Original Item Visibility')\nax[1].set_xlabel('Sqrt')\nax[2].set_xlabel('Binning (4) ')\nax[3].set_xlabel('Drop outliner')\nax[4].set_xlabel('Boolean')\n#sns.distplot((Train.Item_Visibility == 0).astype('int'), ax=ax[2])\n\n#Binning method still gets skewness in Data Distribution\n#Sqrt method reduce the skeness, however, the 0 value still makes the data distribution abnormal\n#we will check how the correlation values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df['sqrt']= np.sqrt(corr_df['Item_Visibility'])\ncorr_df['binning4'] = pd.qcut(corr_df['Item_Visibility'], q=4)\ncorr_df['bool'] = (Train.Item_Visibility == 0)\naa = corr_df[['Item_Weight','Item_Visibility','sqrt', 'binning4', 'bool']]\naa = simple_encoder (aa,column_names=['binning4']).corr()\nsns.heatmap(aa, annot=True)\n\n#binning method has the highest correlevance to Item Weight so we gonna implement this method","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check correlation between Item Weight and Item Visibility after droping outliner\ncorr_df[corr_df['Item_Visibility'] < 0.2][['Item_Weight','Item_Visibility']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_test = Train.copy()[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Fat_Content','Item_Type']]\nnull_values = Train_test[Train_test.Item_Weight.isnull()][[ 'Item_Visibility', 'Item_MRP', 'Item_Fat_Content', 'Item_Type']]\nnon_null_values = Train_test[~Train_test.Item_Weight.isnull()][['Item_Visibility', 'Item_MRP', 'Item_Fat_Content','Item_Weight', 'Item_Type']]\nnon_null_values , null_values = cross_df_encoder(non_null_values, null_values, cl = ['Item_Fat_Content','Item_Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = non_null_values[['Item_Visibility', 'Item_MRP', 'Item_Fat_Content','Item_Type']]\ny = non_null_values['Item_Weight']\ntrain_X, val_X, train_y, val_y = train_test_split(X,y ,test_size = 0.3, random_state = 42)\n\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import Lasso, LinearRegression, BayesianRidge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmodels_dict = { 'LinearSVR' : LinearSVR(),'Lasso' : Lasso(), 'Linear Regressionn' : LinearRegression() , 'BayesianRidge' : BayesianRidge()}\nfor name, model in models_dict.items():\n    print(name + ' : ')\n    model.fit(train_X,train_y)\n    predict_result = model.predict(val_X)\n    print('MSE : %s' %(mean_squared_error(val_y,predict_result)))\n    print('MAE : %s' %(mean_absolute_error(val_y,predict_result)))\n    print('\\n')\n    \n#I gonna choos Bayesian Ridge for making prediction.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BayesianRidge()\nmodel.fit(X,y)\nnull_values['Item_Weight'] = model.predict(null_values)\nitem_col = pd.concat([non_null_values, null_values] ,axis = 0)\nlabel = ['Item_Visibility', 'Item_MRP', 'Item_Fat_Content', 'Item_Weight','Item_Type']\nTrain[label] = item_col[label]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check corr\nz= Train.corr()\nplt.figure(figsize= (15,8))\nsns.heatmap(z, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= Train.drop('Item_Outlet_Sales',axis =1)\ny= Train['Item_Outlet_Sales']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_dict = { 'LinearSVR' : LinearSVR(),'Lasso' : Lasso(), 'Linear Regressionn' : LinearRegression() , 'BayesianRidge' : BayesianRidge()}\n\nfrom sklearn.model_selection import cross_validate\n\nfor name, model in models_dict.items():\n    print(name)\n    print(pd.DataFrame(cross_validate(model,X,y, cv=5,scoring = ['neg_mean_absolute_error','neg_mean_squared_error'])).mean())\n    print('\\n')\n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}