{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport warnings\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageFile\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nimport copy \nfrom pathlib import Path\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# Any results you write to the current directory are saved as output.\nuse_cuda = torch.cuda.is_available()\nsns.set(style=\"darkgrid\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif not use_cuda:\n    print('No GPU found. Please use a GPU to train your neural network.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n# open an image file (.bmp,.jpg,.png,.gif) you have in the working folder\nimageFile = \"/kaggle/input/mydatasetforaerialcactus/preprocess_v1/preprocess/train/Large/clean1922.png\"\nim1 = Image.open(imageFile)\n# adjust width and height to your needs\nwidth = 224\nheight = 224\n# use one of these filter options to resize the image\nim2 = im1.resize((width, height), Image.NEAREST)      # use nearest neighbour\nim3 = im1.resize((width, height), Image.BILINEAR)     # linear interpolation in a 2x2 environment\nim4 = im1.resize((width, height), Image.BICUBIC)      # cubic spline interpolation in a 4x4 environment\nim5 = im1.resize((width, height), Image.ANTIALIAS) \n\ndisplay(im5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convertDate(x,y):\n    print(x)\n    if x != \"\":\n        return path/x\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = Path('/kaggle/input/mydatasetforaerialcactus/preprocess_v1/preprocess')\n\ndf_train = pd.read_csv(path/'train.csv', low_memory=False)\ndf_test = pd.read_csv(path/'test.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train = df_train[:18528]\n#_File\"] = df_train[[\"Image_File\", \"Class\"]].applymap(convertDate)\n#.apply(lambda se: se.str.zfill(4))\nprint(df_train.shape)\nprint(df_test.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 26))\nsns.countplot(x=\"Class\", data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: class is inherited from Dataset\nclass LumarImageLabelDataset(Dataset):\n    def __init__(self, df_data, prediction, folder=\"train\", transformtype=\"train\"):\n        super().__init__()\n        self.df = df_data\n        self.labels_dict = {\"Large\":0, \"Small\":1}\n        self.prediction = prediction\n        self.folder = folder\n        self.transformtype = transformtype\n\n    def __len__(self):\n        return len(self.df)\n    \n    @property\n    def train_labels(self):\n        #warnings.warn(\"train_labels has been renamed targets\")\n        return self.prediction\n    \n    def __getitem__(self, index):\n        #print(index)\n        label = self.prediction[index]\n        #print(label)\n        tensorimage = self.preprocess_image(self.df[index], label)\n        #label_tensor = self.get_dummies(label)\n        #print(tensorimage)\n        label_tensor = int(self.labels_dict[label])\n        return tensorimage, label_tensor    \n   \n    def preprocess_image(self, img_path, label):\n        data_transform = transforms.Compose([transforms.ToPILImage(),\n                                             #transforms.Resize(256),\n                                             #transforms.RandomResizedCrop(224),\n                                             transforms.RandomHorizontalFlip(),\n                                             transforms.RandomVerticalFlip(),\n                                             transforms.RandomRotation(degrees=40), \n                                             transforms.ToTensor(),\n                                             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                            ])\n        \n        test_transform = transforms.Compose([transforms.ToPILImage(),\n                                             #transforms.Resize(256),\n                                             #transforms.CenterCrop(224),\n                                             transforms.ToTensor(),\n                                             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                            ])\n        image = None\n        path = Path('/kaggle/input/mydatasetforaerialcactus/preprocess_v1/preprocess')\n        if self.transformtype == 'train':\n            #print(path/\"{}/{}/{}\".format(self.folder, label, img_path))\n            image = Image.open(path/\"{}/{}/{}\".format(self.folder, label, img_path)).convert('RGB') \n            #image = cv2.imread(path/\"{}/{}/{}\".format(self.folder, label, img_path), 0)\n            image = data_transform(np.array(image))\n        else:\n            image = Image.open(path/\"{}/{}\".format(self.folder, img_path)).convert('RGB') \n            #image = cv2.imread(path/\"{}/{}\".format(self.folder, img_path))\n            image = data_transform(np.array(image))\n            \n        return image\n    \n    def get_dummies(self, attribute_id):\n        label_tensor = torch.zeros((1, 102))\n        #label_tensor = torch.zeros((1, 1))\n        #for label in attribute_id.split():\n        label_tensor[0, int(attribute_id)-1] = 1\n        #label_tensor[0, 0] = attribute_id\n        return label_tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n    Arguments:\n        indices (list, optional): a list of indices\n        num_samples (int, optional): number of samples to draw\n    \"\"\"\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n                \n        # if indices is not provided, \n        # all elements in the dataset will be considered\n        self.indices = list(range(len(dataset))) \\\n            if indices is None else indices\n            \n        # if num_samples is not provided, \n        # draw `len(indices)` samples in each iteration\n        self.num_samples = len(self.indices) \\\n            if num_samples is None else num_samples\n            \n        # distribution of classes in the dataset \n        label_to_count = {}\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in label_to_count:\n                label_to_count[label] += 1\n            else:\n                label_to_count[label] = 1\n                \n        # weight for each sample\n        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n                   for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n\n    def _get_label(self, dataset, idx):\n        dataset_type = type(dataset)\n        if dataset_type is torchvision.datasets.MNIST:\n            return dataset.train_labels[idx].item()\n        elif dataset_type is ImageLabelDataset:\n            return dataset.train_labels[idx].item()\n        else:\n            raise NotImplementedError\n                \n    def __iter__(self):\n        return (self.indices[i] for i in torch.multinomial(\n            self.weights, self.num_samples, replacement=True))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.head()\nbatch_size = 64\ntrain_image = df_train[\"Image_File\"]\n#target = df_train.drop(['id', 'attribute_ids'],axis=1)\ntarget = df_train[\"Class\"]\n\ndf_test[\"Class\"] = \"Large\"\n\nX_train, X_val, y_train, y_val = train_test_split(train_image, target, test_size=0.33)\n\n#smt = SMOTE()\n#X_train, y_train = smt.fit_sample(X_train.reshape(-1, 1), y_train)\n#print(np.bincount(y_train))\n\ntest_image = df_test[\"Image_File\"]\ntest_target = df_test[\"Class\"]\n#test_target = df_test.drop(['id', 'attribute_ids'],axis=1)\n\ntrain_set = LumarImageLabelDataset(df_data=X_train.values, prediction=y_train.values, \n                              folder=\"train\", transformtype=\"train\")\nval_set = LumarImageLabelDataset(df_data=X_val.values, prediction=y_val.values, \n                            folder=\"train\", transformtype=\"train\")\npredict_set = LumarImageLabelDataset(df_data=test_image, prediction=test_target, \n                                folder=\"test\", transformtype=\"test\")\n\ntrain_loader = torch.utils.data.DataLoader(train_set, \n                                           batch_size=batch_size,\n                                           num_workers=4)\nval_loader = torch.utils.data.DataLoader(val_set, \n                                         batch_size=batch_size,\n                                         num_workers=4)\ntest_loader = torch.utils.data.DataLoader(predict_set, batch_size=1, shuffle=False, num_workers=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nn_output=1\n# Number of Epochs\nnum_epochs = 20\n# Learning Rate\nlearning_rate = 0.0001\n# Model parameters\n\n# Show stats for every n number of batches\nshow_every_n_batches = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_transfer = models.densenet121(pretrained=True)\nprint(model_transfer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_transfer.features.conv0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_rnn(model, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n    train_loss = 0\n    valid_loss = 0\n    valid_corrects = 0\n    batch_losses = []\n    val_batch_losses = []\n    valid_loss_min = np.Inf\n    \n    model.train()\n    \n    previousLoss = np.Inf\n    minLoss = np.Inf\n\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            #print(batch_idx + len(train_loader.dataset))\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            # clear the gradients of all optimized variables\n            target = np.squeeze(target)\n            #target = target.view(1,-1)\n            target = target.long()\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # forward pass: Train compute predicted outputs by passing inputs to the model\n            \n            output = model(data)\n            \n            #with torch.no_grad():\n                #output = model(data)\n        \n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item() * data.size(0)\n            \n        model.eval()\n        print(\"Model eval\")\n        for batch_idx, (data, target) in enumerate(val_loader):\n            # move to GPU\n            #print(batch_idx)\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            target = np.squeeze(target)\n            target= target.long()\n            # zero the parameter gradients\n            #optimizer.zero_grad()\n            with torch.no_grad():\n                output = model(data)\n                _, preds = torch.max(output, 1)\n                loss = criterion(output, target)\n            #print(output)\n            # update average validation loss \n            valid_loss += loss.item() * data.size(0)\n            #print(loss.item())\n            valid_corrects += torch.sum(preds == target.data)\n        \n        valid_loss = valid_loss/len(val_loader.dataset)\n        train_loss = train_loss/len(train_loader.dataset)\n        valid_corrects = valid_corrects.double()/len(val_loader.dataset)\n        \n        # print training/validation statistics \n        if epoch_i%show_every_n_batches == 0:\n            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch_i, train_loss, valid_loss))\n        \n            ## TODO: save the model if validation loss has decreased\n            # save model if validation loss has decreased\n            if valid_loss < valid_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Accuracy ({:.3}) Saving model ...'.format(\n                valid_loss_min,\n                valid_loss, valid_corrects))\n                with open('trained_rnn_new', 'wb') as pickle_file:\n                    torch.save(model.state_dict(), 'trained_rnn_new')\n                valid_loss_min = valid_loss\n                train_loss = 0\n                valid_loss = 0\n                valid_corrects = 0\n                #batch_losses = []\n                #val_batch_losses = []\n  \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freeze training for all \"features\" layers\n# 362 features\ncount = 0\nfor param in model_transfer.features.parameters():\n    count += 1\n    if count < 100:\n        param.requires_grad = False\n\n## Freezing all layers\n#for params in model_transfer.parameters():\n#    params.requires_grad = False\n\n#custom_model = nn.Linear(1024, 1)\ncustom_model = nn.Sequential(nn.Linear(1024, 1),nn.Softmax(dim=1))\n\n\"\"\"\ncustom_model = nn.Sequential(nn.Linear(1024, 256), \n                  nn.ReLU(),\n                  nn.Dropout(p=0.25), \n                  nn.Linear(256, 102)\n                 )\n\"\"\"\n#if use_cuda:\n#    custom_model = custom_model.to(device)\n    \nmodel_transfer.fc = custom_model\n\n#model_transfer.load_state_dict(torch.load('save/trained_rnn_new'))\n\nif use_cuda:\n    model_transfer = model_transfer.cuda()\n    model_transfer = torch.nn.DataParallel(model_transfer)\n\n# print(model_transfer)\n\n# specify loss function\ncriterion_scratch = nn.CrossEntropyLoss()\n#criterion_scratch = nn.BCELoss()\n#criterion_scratch = nn.BCELoss(reduction=\"mean\").to('cuda:0')\n\n# specify optimizer\n\n#optimizer_scratch = optim.Adadelta(model_transfer.parameters())\noptimizer_scratch = optim.SGD(model_transfer.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.000001)\n#optimizer_scratch = optim.Adam(model_transfer.parameters(), lr=learning_rate)\n#optimizer_scratch = optim.SGD(list(filter(lambda p: p.requires_grad, model_transfer.parameters())), lr=learning_rate, momentum=0.9)\n\n\n#trained_rnn = train_rnn(model_transfer, batch_size, optimizer_scratch, criterion_scratch, num_epochs, show_every_n_batches)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_transfer.load_state_dict(torch.load('trained_rnn_new'))\nmodel_transfer.eval()\nprint(\"Model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = 0\nmodel_transfer.eval()\ncount = 0\nequality = 0\nfor batch_idx, (data, target) in enumerate(val_loader):\n    # move to GPU\n    #print(batch_idx)\n    if use_cuda:\n        data, target = data.cuda(), target.cuda()\n    # forward pass: compute predicted outputs by passing inputs to the model\n    target = np.squeeze(target)\n    target= target.long()\n    # zero the parameter gradients\n    #optimizer.zero_grad()\n    with torch.no_grad():\n        output = model_transfer(data)\n        _, preds = torch.max(output, 1)\n    #print(output)\n    #print(target)\n    equality = r2_score(preds.cpu(), target.cpu())\n    if equality < 0:\n        equality = 0\n    \n    #print(equality)\n    count += 1\n    # Accuracy is number of correct predictions divided by all predictions\n    accuracy += equality\n    \n#print(count)\nprint(\"Test accuracy: {:.3f}\".format(accuracy/len(val_loader)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = []\nsubmission = df_test\nmodel_transfer.eval()\nfor batch_idx, (data, target) in enumerate(test_loader):\n    # move to GPU\n    if use_cuda:\n        data = data.cuda()\n        \n    output = model_transfer(data)\n    output = output.detach().cpu().max(1)[1]\n    output = output.numpy()\n    #print(output)\n    test_preds = np.concatenate((test_preds, output), axis=0).astype(int)\n\nprint(test_preds)    \nprint(len(test_preds))\nprint(submission.shape)\nsubmission[\"Class\"] = test_preds\ndisplay(submission.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_dict_out = {0:\"Large\", 1:\"Small\"}\nsubmission[\"Class\"] = submission[\"Class\"].apply(lambda x: labels_dict_out[x])\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename=\"submission_1.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%reset","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":1}