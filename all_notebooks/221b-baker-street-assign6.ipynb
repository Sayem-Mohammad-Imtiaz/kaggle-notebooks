{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Team Name: 221B Baker Street\n# Team Members: Jai Agarwal, Shreyas Nitin Pujari, Guruprasad M\n# Members' USNs: 01FB16ECS144, 01FB16ECS371, 01FB16ECS126\n# Data Anamytics - Assignment 6\n\n#Technique 1: Clustering (comparison of Agglomerative and K-Means Clustering)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Importing the dataset\nData = pd.read_csv('../input/Absenteeism_at_work.csv')\n#Printing the first 5 lines of the data set\nprint(Data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19103cfed093f7ebcfad808cabb1fab02fe66e96"},"cell_type":"code","source":"#Prints the total number of null values in each column of the dataset\nData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e059f95c498ac3551461f06d3c745455a038192"},"cell_type":"code","source":"#Importing Counter\nfrom collections import Counter\nAbs = Data['Absenteeism time in hours']\n#print(type(Data['Absenteeism time in hours']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"991a79e5704b8a809e6b2cbff6a6e9a34fcca94c"},"cell_type":"code","source":"Seq = Counter(Abs)\n#print (Seq)\nKeys = list(Seq.keys())\nFreq = list(Seq.values())\nprint (\"Classes = \",Keys)  #Each class value corresponds to number of absent hours\nprint ('Number of classes: ',len(Keys))  #Displays number of distinct classes of hours of absenteeism.\nprint (\"Freq = \",Freq)  #Frequency of each class (Indices correspond, that is, first index of Freq is for First index for Classes and so on)\n#print(sorted (Freq))\n#Vectors are of 14-Dimension (14-attributes present)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a42bcb86dd92ae342224dec100ab75631ecb8180"},"cell_type":"code","source":"nrows = Data.shape[0]\nncols = Data.shape[1]\n#print (nrows, ncols)\n#Data_train, Data_test = train_test_split(Data, test_size = 0.3,random_state=42) #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e50529ef7867c1659803b3b03f2c211f407bc06"},"cell_type":"code","source":"#The below code converts the values of data frame (except absenteeism column) into a 2-D list (List within a list)\n\nDF_List = []\nfor i in range(nrows):\n    Temp = []\n    for j in range(ncols-1):\n        k = Data.iat[i,j]\n        Temp.append(k)\n    DF_List.append (Temp)\n#Printing the data frame in the form of a list\n#print (DF_List)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4cbb2feab47cd35429bc69fae7f370c01d9d0b3"},"cell_type":"code","source":"import sklearn.cluster  #For agglomerative clustering\nimport sklearn.metrics  #For RMSE\nfrom math import *  #For Square root\nfrom scipy import spatial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26562271fe61a43e40790f5cbdbce649a6443444"},"cell_type":"code","source":"#Parameters for agglomerative clustering:\n#n_clusters = Number of clusters. Here, considered as Keys (=19) because number of classes = 19 (shown above)\n#affinity represents the distance metric used. here, Three different metrics have been used to compare them\n#Linkage represents the distance to use when a pair of clusters are merged to a single cluster. Here, the average distance is used\n#That is, the centroid of the merged cluster is the centroid of the two clusters which got merged into one.\nagglomerative1 = sklearn.cluster.AgglomerativeClustering(n_clusters=len(Keys), affinity = 'cosine', linkage = 'average')\nagglomerative2 = sklearn.cluster.AgglomerativeClustering(n_clusters=len(Keys), affinity = 'manhattan', linkage = 'average')\nagglomerative3 = sklearn.cluster.AgglomerativeClustering(n_clusters=len(Keys), affinity = 'euclidean', linkage = 'average')\nAgg1 = agglomerative1.fit_predict(DF_List)\nAgg2 = agglomerative2.fit_predict(DF_List)\nAgg3 = agglomerative3.fit_predict(DF_List)\n#print(Y)\nAgg_Res1 = Counter(Agg1)\nAgg_Freq1 = list(Agg_Res1.values())\nAgg_Freq1 = sorted (Agg_Freq1)\nAgg_Res2 = Counter(Agg2)\nAgg_Freq2 = list(Agg_Res2.values())\nAgg_Freq2 = sorted (Agg_Freq2)\nAgg_Res3 = Counter(Agg3)\nAgg_Freq3 = list(Agg_Res3.values())\nAgg_Freq3 = sorted (Agg_Freq3)\n#print(Counter(Agg))\nprint ('Actual class frequencies\\n',sorted(Freq))  #Prints the actual counts of each classes (given)\nprint ('Frequencies using Cosine distance\\n',sorted(Agg_Freq1)) #Prints the counts of each cluster, where each cluster corresponds to a class\nprint ('Frequencies using Manhattan distance\\n',sorted(Agg_Freq2))\nprint ('Frequencies using Euclidean distance\\n',sorted(Agg_Freq3))\n#Above 3 lines, sorting has been performed because, the classes generated by agglomerative clustering do not correspond to the actual classes (with respect to list indices)\n#Hence, by sorting, we get them to correspond (with reference to list indices)\n#print ('RMSE for Cosine distance',sqrt(sklearn.metrics.mean_squared_error(Freq, Agg_Freq1)))\n#print ('RMSE for Manhattan distance',sqrt(sklearn.metrics.mean_squared_error(Freq, Agg_Freq2)))\n#print ('RMSE for Euclidean distance',sqrt(sklearn.metrics.mean_squared_error(Freq, Agg_Freq3)))\n\n#Cosine similarity  (higher the value, more similar are the two lists)\nprint ('Cosine similarity for cosine distance',spatial.distance.cosine(Freq, Agg_Freq1))\nprint ('Cosine similarity for manhattan distance',spatial.distance.cosine(Freq, Agg_Freq2))\nprint ('Cosine similarity for euclidean distance',spatial.distance.cosine(Freq, Agg_Freq3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a30e6654eef98a8c00fbdce95ccc80eba93d04fb"},"cell_type":"code","source":"#Comparing with K-Means\n#Here, K is considered as 19 as there are 19 classes\nkmeans = sklearn.cluster.KMeans(n_clusters=19, random_state=0, n_init = 200, max_iter = 500)\nkmeans.fit(DF_List)\nclust_labels = kmeans.predict(DF_List)\nKMRes = Counter(clust_labels)\nKM_freq = list(KMRes.values())\nKM_freq = sorted (KM_freq)\nprint ('Class Frequencies using K-Means\\n',KM_freq)\nprint ('Cosine similarity using K-Means algorithm',spatial.distance.cosine(Freq, KM_freq))\n\n#Results and Analysis\n\n#Therefore, from the above cosine similarities, we can conclude that Agglomerative clustering is better for the given data set\n#compared to K-Means Algorithm\n#This may be because, the data might not be in a spherical pattern, hence Agglomerative has given better similarity than K-Means\n\n#Also , in Agglomerative clustering, Manhattan distance seems to be the best distance metric among Manhattan, Euclidean and Cosine distances\n#(as observed from the Cosine similarity values)\n\n#Using these results, if a new entry is considered, we record the initial frequency value list\n#We then add the new entry to the list. And re-run the Agglomerative Clustering Algorithm using Manhattan distance\n#We now obtain the new frequency values\n#By comparing with old class value frequency list, we can predict the class to which the new value belongs\n#Since each class is nothing but a certain number of hours of absenteeism, the value of Absenteeism time in hours\n#can be predicted.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1dd2ff481429e3bc1f58d55503fad1f0b261489"},"cell_type":"code","source":"#Technique2: Classfication\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5265a5c2e4b41d70f0602d5dd2a3436eb7204c5a"},"cell_type":"code","source":"#Importing dataset\nData = pd.read_csv(\"../input/Absenteeism_at_work.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef143d3815795525ca8fda416c0e99e8bfbd79cc"},"cell_type":"code","source":"#Print the beginning few rows of the dataset\nData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a14fa605efce932f3f5e17d49565f5c26207ec3"},"cell_type":"code","source":"#Describe and summarise basic features of the dataset\nData.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c94a434889186f5e8c0da792834ae4297eca2b3e"},"cell_type":"code","source":"#Data PreProcessing\n#Check if any null or missing values\nData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8424fdfa943e40872c203deb78676a3cd7cca6c9"},"cell_type":"code","source":"#Preparing the training and testing datasets\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(Data, test_size = 0.3,random_state=42)\nx_train = train.drop('Absenteeism time in hours',axis=1)\ny_train = train['Absenteeism time in hours']\n\nx_test = test.drop('Absenteeism time in hours', axis=1)\ny_test = test['Absenteeism time in hours']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0384dfecbc6eed67ef06feefa96e6ff9b6b75ef1"},"cell_type":"code","source":"#Scaling all the features in the training dataset to have uniform values..\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\nx_train_scaled = scaler.fit_transform(x_train)\nx_train = pd.DataFrame(x_train_scaled)\nx_test_scaled = scaler.fit_transform(x_test)\nx_test = pd.DataFrame(x_test_scaled)\n#from sklearn import preprocessing\n#x_scaled_train = preprocessing.scale(x_train)\n#x_scaled_test = preprocessing.scale(x_test)\n#x_scaled_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49670e6c00c8c0f4e202adb46c405f7c0759a6ce"},"cell_type":"code","source":"#import required packages\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#Running knn for k values from 1 to 100, plotting errors and finding best value for K to fit model\nrmse_val = [] #to store rmse values for different k\nfor K in range(30):\n    K = K+1\n    model = KNeighborsClassifier(n_neighbors=K)\n    model.fit(x_train, y_train)  #fit the model\n    y_pred=model.predict(x_test) #make prediction on test set\n    error = sqrt(mean_squared_error(y_test,y_pred)) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42cf8a20fd4c2380e52f44aa4703f7f7ef9fe456"},"cell_type":"code","source":"#plotting the rmse values against k values\ncurve = pd.DataFrame(rmse_val) #elbow curve \ncurve.plot()\n#From curve we can conclude that for k=8 is the best kNN classifier model\n#k = 34 is min RMSE value with 14.424\n\n#For the plot, Y axis corresponds to RMSE and X axis corresponds to value of K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3131c3c345c2a3b23a845f322027fc3a58ac4037"},"cell_type":"code","source":"#Choosing K=8 as optimal nearest neighbors value\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"======================================================================\\n\")\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dfb124779596dc87a70b8f258c8599605739f4d"},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55db47bc9e6a53ca1f2ff87086bed44b3952070e"},"cell_type":"code","source":"#Results and Analysis\n\n#For model 2 we used KNN classifier which chooses K nearest neighbors(in this case 8).\n#The model works by predicting value of a new datapoint by taking average with surrounding 8 neighbors and hence the time in \n#hours that a new datapoint would be absent is the average of these neighbor values\n#With this model we achieved a low accuracy of ~40% , which could be further improved by using a better classification technique\n#Accuracy in this case could also be further improved by study correlation between attributes and dropping columns to prevent multi-collinearity\n\n#The chosen value of K in this case is 8 because it had the lowest RMSE for all possible neighbor values (ranging from 1 to 30)\n#The model was fitted for each such value of K and the resulting model RMSE was plotted versus the K value.\n#We get a 'Elbow Curve' which has a local minima at 8, hence we chose the value as 8 for K in the KNN model, as it has the least RMSE.\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}