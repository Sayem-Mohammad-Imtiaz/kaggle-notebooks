{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional libraries\nimport matplotlib.pyplot as plt # for plots\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold # for splits\n\nfrom sklearn import metrics # for post-analysis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Reading the input data files "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# reading in data files\ndata_train = pd.read_csv('../input/mnist_train.csv')\ndata_test = pd.read_csv('../input/mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Basic Diagnostics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The shape of TRAINING Data is: \", data_train.shape)\nprint(\"The shape of TEST Data is: \", data_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top rows\ndata_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the column names\nprint(\"Column Names\")\ndata_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what are the data types of the columns?\nprint(\"Data Types of Column\")\nprint(data_train.dtypes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.describe() #seems useless","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for null values\ndata_train.isnull().values.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.isnull().values.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ready for Take-Off (Pre-Analysis does not hint at any further action. NULL Values, Character Encoding, etc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data set\nX_train = np.array(data_train.iloc[:,1:])\ny_train = np.array(data_train.iloc[:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.array(data_test.iloc[:,1:])\ny_test = np.array(data_test.iloc[:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample rows\nX_test[0:2,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample rows\ny_test[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# takes way too long ...\n# commenting out ...\n\n#clf = SVC() \n#clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_predict = clf.predict(X_test)\n#accuracy_score(y_predict,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA to reduce dimensionality \n\n[1] states:\nOne of the things that you can do to speed up the fitting of a machine learning algorithm is changing the optimization algorithm. \nA more common way of speeding up a machine learning algorithm is by using Principal Component Analysis (PCA). If your learning algorithm is too slow because the input dimension is too high, then using PCA to speed it up can be a reasonable choice\n\n[1]: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"},{"metadata":{},"cell_type":"markdown","source":"[1] PCA is effected by scale so you need to scale the features in your data before applying PCA. Use StandardScaler to help you standardize the datasetâ€™s features onto unit scale (mean = 0 and variance = 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Standardizing the features\nscaler = StandardScaler()\n\n# Fit on training set only.\nscaler.fit(X_train)\n\n# Apply transform to both the training set and the test set.\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled[0:2,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice the code below has .95 for the number of components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.\n\nChanging it to optional parameter with default value. In the calling code, will reduce it to .5 or 50%"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ndef get_pca(x_train, x_test,pc_comp=0.95):\n    pca = PCA(n_components=pc_comp)\n    pca.fit(x_train)\n    print(\"Explained Variance Ratios:\", pca.explained_variance_ratio_)\n    print(\"N Components are: \", pca.n_components_)\n    x_train = pca.transform(x_train)\n    x_test = pca.transform(x_test)\n    print(x_train.shape, x_test.shape)\n    return x_train, x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sd_pca, X_test_sd_pca = get_pca(X_train_scaled,X_test_scaled,0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"50% variance is explained by 39 (transformed) features compared to 784 (original) features"},{"metadata":{},"cell_type":"markdown","source":"Lets start small and use Log Reg first"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all parameters not specified are set to their defaults\n# default solver is incredibly slow which is why it was changed to 'lbfgs'\n\nclf_logReg = LogisticRegression(solver = 'lbfgs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_logReg.fit(X_train_sd_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict for One Observation (image)\nclf_logReg.predict(X_test_sd_pca[0].reshape(1,-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_logReg.score(X_test_sd_pca,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to go back to SVC"},{"metadata":{},"cell_type":"markdown","source":"1. Use GridSearchCV() to tune the parameter 'C' of linear SVM.\n1. Then use Non linear SVM ('RBF') and tune 'C' using GridSearchCV.\n1. Tune the parameters using training data set and finally test using test data set.\n1. You can use PCA for dimension reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# picking these parameters at random to give SVM a spin first\nclf_svm = SVC(kernel='linear', C=0.1)\nclf_svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svm.fit(X_train_sd_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm = clf_svm.predict(X_test_sd_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SVM accuracy for TRAIN: \", clf_svm.score(X_train_sd_pca,y_train))\nprint(\"SVM accuracy for TEST: \", clf_svm.score(X_test_sd_pca,y_test))\n\nprint(\"LOG-REG accuracy for TRAIN: \", clf_logReg.score(X_train_sd_pca,y_train))\nprint(\"LOG-REG accuracy for TEST: \", clf_logReg.score(X_test_sd_pca,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Use GridSearchCV() to tune the parameter 'C' of linear SVM.\n\n# create a parameter grid: map the parameter names to the values that should be searched\n# simply a python dictionary\n# key: parameter name\n# value: list of values that should be searched for that parameter\n# single key-value pair for param_grid\n\nCs = [0.001, 0.01, 0.1, 1, 10,100,500]\nparamGrid = {'C':Cs}\n#paramGrid = {'C':np.arange(0.01,500,10)}\n#print(paramGrid)\n\n# the cv with 5 does not finish, reducing it down to 2\n#grid_search = GridSearchCV(clf_svm, paramGrid, cv=5)\ngrid_search = GridSearchCV(clf_svm, paramGrid, cv=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In previous iteration, even 0.5 PCAed data could not finish execution of Grid Search on Kaggle\n# let us reduce it down to .25\n# X_train_sd_pca2, X_test_sd_pca2 = get_pca(X_train_scaled,X_test_scaled,0.25)\n\n# let us reduce it down to .15\n# X_train_sd_pca2, X_test_sd_pca2 = get_pca(X_train_scaled,X_test_scaled,0.15)\n\n# With Incremental Learning, trying again .25\nX_train_sd_pca2, X_test_sd_pca2 = get_pca(X_train_scaled,X_test_scaled,0.25)\n\n\n# need to combine scaled version of X_test and X_train\n# X = np.vstack((X_test_sd_pca2,X_train_sd_pca2))\nX = np.concatenate((X_test_sd_pca2,X_train_sd_pca2))\n\n# need to combine original version of y_test and y_train\n#y = np.vstack((y_test,y_train))\n\n# diagnostic\nprint(\"the shapes of y are:\", y_train.shape, y_test.shape)\nprint(\"the type of y are:\", y_train.dtype, y_test.dtype)\nprint(\"the dim of y are:\", y_train.ndim, y_test.ndim)\n\n# np.vstack needs the length of two 1D arrays to be same, so switching to concatenate\ny = np.concatenate((y_test,y_train))\n\n","execution_count":81,"outputs":[{"output_type":"stream","text":"Explained Variance Ratios: [0.05646717 0.04078272 0.0373938  0.02885115 0.02521109 0.0219427\n 0.01923344 0.01745799 0.01535092]\nN Components are:  9\n(60000, 9) (10000, 9)\nthe shapes of y are: (60000,) (10000,)\nthe type of y are: int64 int64\nthe dim of y are: 1 1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search \n\n#grid_search.fit(X, y)\n#print(\"the best C value we get is: \",grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search - despite lowering variance retained in PCA and few value of C is too slow\n\n# Alternate is to try \"Incremental Learning\"\n# Details here :https://scikit-learn.org/0.15/modules/scaling_strategies.html\n\n# new classifier - Stochastic Gradient with loss function hinge for SVC\nfrom sklearn.linear_model import SGDClassifier\nclf_sgd_svc = SGDClassifier()\n\nclf_sgd_svc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# break train and test set into chunks\n\n# diagnostic\nprint(\"the shape of X,y are:\", X.shape, y.shape)\nprint(\"the type of X,y are:\", X.dtype, y.dtype)\nprint(\"the dim of X,y are:\", X.ndim, y.ndim)","execution_count":82,"outputs":[{"output_type":"stream","text":"the shape of X,y are: (70000, 9) (70000,)\nthe type of X,y are: float64 int64\nthe dim of X,y are: 2 1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels of all classes\nclass_labels = np.unique(y)\nprint(class_labels)","execution_count":83,"outputs":[{"output_type":"stream","text":"[0 1 2 3 4 5 6 7 8 9]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"chunk_size = 50\nX_size = X.shape[0]\niterations = (X_size / chunk_size)\n#print(iterations)\n\n\n\nfor i in range(1,X_size,chunk_size):\n    # outer loop \n    for j in range(i,i+chunk_size):\n        # inner loop\n        # add chunk row to the partial fit\n        #print(\"second-loop\")\n        clf_sgd_svc.partial_fit(X[i:i+chunk_size,],y[i:i+chunk_size,], class_labels)\n        #print(j)\n\n\n\n","execution_count":84,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Number of features 9 does not match previous data 4.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-4a1cc87987b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# add chunk row to the partial fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(\"second-loop\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mclf_sgd_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print(j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                  \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                                  coef_init=None, intercept_init=None)\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     def fit(self, X, y, coef_init=None, intercept_init=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             raise ValueError(\"Number of features %d does not match previous \"\n\u001b[0;32m--> 539\u001b[0;31m                              \"data %d.\" % (n_features, self.coef_.shape[-1]))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of features 9 does not match previous data 4."]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf_sgd_svc.score(X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try bagging and boosting\n# https://www.kaggle.com/mayankkestwal10/ensemble-learning-bagging-and-boosting","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}