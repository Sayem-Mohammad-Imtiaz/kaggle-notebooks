{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Unsupervised Machine Learning on Wholesale Customers Data**","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T21:30:23.10035Z","iopub.execute_input":"2021-08-09T21:30:23.101061Z","iopub.status.idle":"2021-08-09T21:30:23.120717Z","shell.execute_reply.started":"2021-08-09T21:30:23.101014Z","shell.execute_reply":"2021-08-09T21:30:23.1196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:23.122562Z","iopub.execute_input":"2021-08-09T21:30:23.123191Z","iopub.status.idle":"2021-08-09T21:30:23.309781Z","shell.execute_reply.started":"2021-08-09T21:30:23.123144Z","shell.execute_reply":"2021-08-09T21:30:23.308382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading the CSV data file and creating the data frame**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nwholesale_data = pd.read_csv('../input/wholesale-customers-data-set/Wholesale customers data.csv')\n\n# just printing random observations and attributes.\nprint(wholesale_data.sample(5))\nprint(\"\\n\\n\")\n\n# dropping of the attributes 'Channel' and 'Region'\n# 'Channel' represents the hotel, cafe or retail store\n# 'Region' represents the customer region \n# dropping of the attributes 'Channel' & ' Region' won't affect the clustering, as we are trying to relate the customers to the products\n# they buy in order to maximize the business\nwholesale_data.drop(labels=['Channel', 'Region'], axis=1, inplace=True)\nprint(wholesale_data.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:23.311686Z","iopub.execute_input":"2021-08-09T21:30:23.312039Z","iopub.status.idle":"2021-08-09T21:30:23.357292Z","shell.execute_reply.started":"2021-08-09T21:30:23.312007Z","shell.execute_reply":"2021-08-09T21:30:23.356055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now to check for null values in the dataset**","metadata":{}},{"cell_type":"code","source":"# Gives us the basic analysis information of the wholesale customer dataset \nwholesale_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:23.358882Z","iopub.execute_input":"2021-08-09T21:30:23.359304Z","iopub.status.idle":"2021-08-09T21:30:23.379007Z","shell.execute_reply.started":"2021-08-09T21:30:23.359241Z","shell.execute_reply":"2021-08-09T21:30:23.376925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since the above attributes have non-null values we do not need to do null checks for the attributes**","metadata":{}},{"cell_type":"markdown","source":"**Let's take a look at the basic statisical data**","metadata":{}},{"cell_type":"code","source":"wholesale_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:23.380836Z","iopub.execute_input":"2021-08-09T21:30:23.381359Z","iopub.status.idle":"2021-08-09T21:30:23.436165Z","shell.execute_reply.started":"2021-08-09T21:30:23.381315Z","shell.execute_reply":"2021-08-09T21:30:23.434483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting the results of describe, can observe that mean >> median in almost all the cases: distribution are scre","metadata":{}},{"cell_type":"code","source":"wholesale_data.describe().transpose()[['mean','50%']].plot.barh(figsize=(10,6))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:23.437594Z","iopub.execute_input":"2021-08-09T21:30:23.437932Z","iopub.status.idle":"2021-08-09T21:30:23.746543Z","shell.execute_reply.started":"2021-08-09T21:30:23.437898Z","shell.execute_reply":"2021-08-09T21:30:23.745476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now let us perform 'Standardization' and 'Decomposition'** **(Pre processing)**\n\nPreviously we saw the features 'Channel' & 'Region' removed. These values have a low magnitude, whereas the other features like fresh, milk, grocery, frozen, detergents_paper, delicassen seem to have a higher magnitude and it is very necessary to bring these data to the same magnitude, because K- means algorithm is distance based and can have adverse effect with magnitude. Hence similar magnitude is preferred.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nstandard_scaler = StandardScaler()\n# making sample mean= 0 and std=1\nscaled_data = standard_scaler.fit_transform(wholesale_data)\n\n_temp_PCA=PCA(6)\n_temp_PCA.fit_transform(scaled_data)\n\nplt.bar(range(1,7),_temp_PCA.explained_variance_ratio_,color='black')\nplt.xlabel('PCA dims')\nplt.title('Variance ratio by 6 features')\nplt.ylabel('variance preserve')\nplt.xticks(range(1,7))\ntraining_PCA_data = PCA(2).fit_transform(scaled_data)\n\n# print(scaled_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:23.748368Z","iopub.execute_input":"2021-08-09T21:30:23.748812Z","iopub.status.idle":"2021-08-09T21:30:24.080945Z","shell.execute_reply.started":"2021-08-09T21:30:23.748766Z","shell.execute_reply":"2021-08-09T21:30:24.079797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(scaled_data,columns=['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']).describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:24.083307Z","iopub.execute_input":"2021-08-09T21:30:24.083651Z","iopub.status.idle":"2021-08-09T21:30:24.123821Z","shell.execute_reply.started":"2021-08-09T21:30:24.083615Z","shell.execute_reply":"2021-08-09T21:30:24.122797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_PCA_DF = pd.DataFrame(training_PCA_data,columns=['d1','d2'])\ntraining_PCA_DF.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:24.125772Z","iopub.execute_input":"2021-08-09T21:30:24.126155Z","iopub.status.idle":"2021-08-09T21:30:24.138988Z","shell.execute_reply.started":"2021-08-09T21:30:24.126119Z","shell.execute_reply":"2021-08-09T21:30:24.137455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_PCA_DF.plot.scatter('d1','d2',alpha=.07,s=100,color='BLACK',figsize=(8,5))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:24.140845Z","iopub.execute_input":"2021-08-09T21:30:24.141306Z","iopub.status.idle":"2021-08-09T21:30:24.374647Z","shell.execute_reply.started":"2021-08-09T21:30:24.141243Z","shell.execute_reply":"2021-08-09T21:30:24.373615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now to determine the number of clusters for the K-means algorithm, this can be calculated in 2 ways:**\n\n1. Elbow Method\n2. Silhouette Method","metadata":{}},{"cell_type":"markdown","source":"**Using Elbow Method**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Using elbow methods to determine the number of clusters for the K-Means algorithm\n# WCSS = Within Cluster Sum of Squares\nwcss = [] \nfor i in range(1, 25):\n    km = KMeans(n_clusters = i, init = 'k-means++', \n                max_iter = 300, n_init = 10, random_state = 0)\n    km.fit(training_PCA_DF)\n    wcss.append(km.inertia_)\nplt.plot(range(1, 25), wcss)\nplt.legend(['wcss'])\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('Number of Clusters')\nplt.ylabel('wcss')\nplt.figure(figsize = (10,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:24.375865Z","iopub.execute_input":"2021-08-09T21:30:24.376497Z","iopub.status.idle":"2021-08-09T21:30:27.598856Z","shell.execute_reply.started":"2021-08-09T21:30:24.376449Z","shell.execute_reply":"2021-08-09T21:30:27.597534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using Silhouette Method**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\n# Averaging the clusters with silhouette methods \nsil_avg=[]\ncluster_numbers = [4, 5, 6,7] \nprint(\"Average Silhouette Method\\n\")\nfor one_cluster in cluster_numbers: \n    cluster = KMeans(n_clusters = one_cluster) \n    cluster_labels = cluster.fit_predict(training_PCA_DF) \n    silhouette_avg = silhouette_score(training_PCA_DF, cluster_labels)\n    sil_avg.append([one_cluster,silhouette_avg])\n    print(f\"For clusters = {one_cluster}\")\n    print(f\"The average silhouette score for {one_cluster} is = {silhouette_avg}\")\nsil_avg=np.array(sil_avg)\nplt.plot(sil_avg[:,0],sil_avg[:,1],linestyle='dashed')\nplt.xlabel('Number of Clusters')\nplt.ylabel('avg_sil')\nplt.legend(['avg_sil'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:27.600431Z","iopub.execute_input":"2021-08-09T21:30:27.600778Z","iopub.status.idle":"2021-08-09T21:30:28.308001Z","shell.execute_reply.started":"2021-08-09T21:30:27.600745Z","shell.execute_reply":"2021-08-09T21:30:28.307222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can from the above averages that most values are closer to 5, meaning that number of cluster are fixed to 5**","metadata":{}},{"cell_type":"markdown","source":"Now we perform the K-Means clustering and plot the results in a scatterplot","metadata":{}},{"cell_type":"code","source":"print(training_PCA_DF.sample(10))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:28.309299Z","iopub.execute_input":"2021-08-09T21:30:28.30961Z","iopub.status.idle":"2021-08-09T21:30:28.317939Z","shell.execute_reply.started":"2021-08-09T21:30:28.309578Z","shell.execute_reply":"2021-08-09T21:30:28.316871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclusters_K_Means = 5\nrandom_state_K_Means = 0\n\nkmean = KMeans(n_clusters=clusters_K_Means, random_state=\n               random_state_K_Means).fit(training_PCA_DF)\nkmean_Y = kmean.predict(training_PCA_DF)\nlab = kmean.labels_\n# print(np.unique(lab))\nplt.figure(figsize=(10,5))\nplt.title(f\"K- Means with cluster value = {clusters_K_Means}\",fontsize=15)\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'],c = kmean_Y, s=105, \n        alpha=0.6,marker='o')\nplt.xlabel(\"X Axis\")\nplt.ylabel(\"Y Axis\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:28.319741Z","iopub.execute_input":"2021-08-09T21:30:28.320183Z","iopub.status.idle":"2021-08-09T21:30:28.554372Z","shell.execute_reply.started":"2021-08-09T21:30:28.320135Z","shell.execute_reply":"2021-08-09T21:30:28.553567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.cluster.hierarchy as shc\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\n\n#Agglomerative Clustering\n\nagglometric_clustering= AgglomerativeClustering(n_clusters=clusters_K_Means,affinity = 'euclidean',linkage = 'ward')\nagglometric_clustering_y = agglometric_clustering.fit_predict(training_PCA_DF)\nplt.figure(figsize =(10,5))\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'],c = agglometric_clustering_y, s=80, alpha=0.6,marker='o')\nplt.title('Agglomerative Clustering',fontsize = 20)\nplt.show()\n\nplt.figure(figsize=(10,5))\nplt.title('Agglomerative Clustering : Dendrogram',fontsize = 20)\ndend=shc.dendrogram(shc.linkage(training_PCA_DF,method='ward') ,truncate_mode='level', p=5) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:28.555569Z","iopub.execute_input":"2021-08-09T21:30:28.556026Z","iopub.status.idle":"2021-08-09T21:30:29.193567Z","shell.execute_reply.started":"2021-08-09T21:30:28.555978Z","shell.execute_reply":"2021-08-09T21:30:29.1925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster=AgglomerativeClustering(n_clusters=clusters_K_Means,affinity='euclidean',linkage='ward')\ncluster.fit_predict(training_PCA_DF)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:29.195419Z","iopub.execute_input":"2021-08-09T21:30:29.195851Z","iopub.status.idle":"2021-08-09T21:30:29.212794Z","shell.execute_reply.started":"2021-08-09T21:30:29.195802Z","shell.execute_reply":"2021-08-09T21:30:29.211526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import Birch\n\n\n#birch clustering\nbirch_clustering = Birch(branching_factor=500, n_clusters=clusters_K_Means, threshold=1.5)\nbirch_clustering.fit(training_PCA_DF)\nlabels = birch_clustering.predict(training_PCA_DF)\n\nplt.title('Birch Clustering',fontsize = 20)\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'], c=labels,alpha=0.6,marker='o',s=150)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:29.216411Z","iopub.execute_input":"2021-08-09T21:30:29.216809Z","iopub.status.idle":"2021-08-09T21:30:29.443518Z","shell.execute_reply.started":"2021-08-09T21:30:29.216759Z","shell.execute_reply":"2021-08-09T21:30:29.442228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\n# mini batch clustering\n\nminibatch_clustering = MiniBatchKMeans(n_clusters=clusters_K_Means, random_state=random_state_K_Means)\nminibatch_clustering.fit(training_PCA_DF)\n\nlabels = minibatch_clustering.predict(training_PCA_DF)\nplt.title('MiniBatchKMeans clustering',fontsize = 20)\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'], c=labels,alpha=0.6,marker='o',s=150)\nplt.figure(figsize=(20,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:30:29.445321Z","iopub.execute_input":"2021-08-09T21:30:29.445681Z","iopub.status.idle":"2021-08-09T21:30:29.644979Z","shell.execute_reply.started":"2021-08-09T21:30:29.445644Z","shell.execute_reply":"2021-08-09T21:30:29.643923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}