{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.25684Z","iopub.execute_input":"2021-09-01T11:15:19.257559Z","iopub.status.idle":"2021-09-01T11:15:19.262666Z","shell.execute_reply.started":"2021-09-01T11:15:19.257511Z","shell.execute_reply":"2021-09-01T11:15:19.261413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import all required packages\n\n# importing pandas and numpy\nimport pandas as pd\nimport numpy as np\n\n# importing sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n# r2_score for model evaluation\nfrom sklearn.metrics import r2_score\n\n# importing stastsmodels\nimport statsmodels.api as sm\n# importing VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.271157Z","iopub.execute_input":"2021-09-01T11:15:19.271827Z","iopub.status.idle":"2021-09-01T11:15:19.284238Z","shell.execute_reply.started":"2021-09-01T11:15:19.271781Z","shell.execute_reply":"2021-09-01T11:15:19.28341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading the data and understanding the dataset","metadata":{}},{"cell_type":"code","source":"# Importing day.csv\nday = pd.read_csv('../input/bike-sharing/day.csv')\nday.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.305062Z","iopub.execute_input":"2021-09-01T11:15:19.305632Z","iopub.status.idle":"2021-09-01T11:15:19.331505Z","shell.execute_reply.started":"2021-09-01T11:15:19.305598Z","shell.execute_reply":"2021-09-01T11:15:19.330749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets see the columns information in the dataset\nday.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.332681Z","iopub.execute_input":"2021-09-01T11:15:19.333095Z","iopub.status.idle":"2021-09-01T11:15:19.35596Z","shell.execute_reply.started":"2021-09-01T11:15:19.333066Z","shell.execute_reply":"2021-09-01T11:15:19.354556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets see the overall description of numeric variables\nday.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.357723Z","iopub.execute_input":"2021-09-01T11:15:19.358011Z","iopub.status.idle":"2021-09-01T11:15:19.418276Z","shell.execute_reply.started":"2021-09-01T11:15:19.357984Z","shell.execute_reply":"2021-09-01T11:15:19.417232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see total records and total columns\nday.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.419784Z","iopub.execute_input":"2021-09-01T11:15:19.420127Z","iopub.status.idle":"2021-09-01T11:15:19.425834Z","shell.execute_reply.started":"2021-09-01T11:15:19.4201Z","shell.execute_reply":"2021-09-01T11:15:19.424859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initialTotalRecords = (day.shape)[0]\ninitialTotalColumns = (day.shape)[1]\n\nprint(\"Total Rows : \", initialTotalRecords);\nprint(\"Total Columns : \", initialTotalColumns);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.427484Z","iopub.execute_input":"2021-09-01T11:15:19.427818Z","iopub.status.idle":"2021-09-01T11:15:19.442328Z","shell.execute_reply.started":"2021-09-01T11:15:19.427788Z","shell.execute_reply":"2021-09-01T11:15:19.441272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see if there are any null data in our dataset or not\nround(((day.isnull().sum()/initialTotalRecords)*100), 2)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.444017Z","iopub.execute_input":"2021-09-01T11:15:19.444464Z","iopub.status.idle":"2021-09-01T11:15:19.47823Z","shell.execute_reply.started":"2021-09-01T11:15:19.444419Z","shell.execute_reply":"2021-09-01T11:15:19.476977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Looks like there are no null data in our dataset so we are good to proceed further","metadata":{}},{"cell_type":"code","source":"# So lets remove some columns which are not useful for our predictions\n# instant - Its an index no much significance of it to keep in dataset\n# dteday - Its a date column, which is a kind of redundant column because we already have yr and month columns seperately\n# casual & registered - It is also kind of redundant columns because its combined count is already mentioned in cnt column. \n#                  And we also need to work on the count of the bikes instead of category count so we can remove this colum\n# Lets make a new dataframe without these columns with name 'bike'\n\nbike = day.copy(deep=True) # deep=True, since when we change one data frame other should not get updated/effected by other.\ncolumns_to_remove = ['instant', 'dteday', 'casual', 'registered'];\nbike.drop(columns_to_remove, axis = 1, inplace = True)\nbike.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.479431Z","iopub.execute_input":"2021-09-01T11:15:19.479708Z","iopub.status.idle":"2021-09-01T11:15:19.497717Z","shell.execute_reply.started":"2021-09-01T11:15:19.479682Z","shell.execute_reply":"2021-09-01T11:15:19.496999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_total_records = (bike.shape)[0]\nupdated_total_columns = (bike.shape)[1]\n\nprint(\"Updated total rows : \", updated_total_records);\nprint(\"Updated total columns : \", updated_total_columns);\n\n# Columns were reduced from 16 to 12","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.499715Z","iopub.execute_input":"2021-09-01T11:15:19.500123Z","iopub.status.idle":"2021-09-01T11:15:19.510642Z","shell.execute_reply.started":"2021-09-01T11:15:19.500094Z","shell.execute_reply":"2021-09-01T11:15:19.509551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check the unique values count for the categorical variables\ncategorical_columns = ['season', 'mnth', 'weathersit', 'weekday'];\nfor categorical_col in categorical_columns:\n    print(bike[[categorical_col]].value_counts(), \"\\n\");","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.512359Z","iopub.execute_input":"2021-09-01T11:15:19.512641Z","iopub.status.idle":"2021-09-01T11:15:19.539656Z","shell.execute_reply.started":"2021-09-01T11:15:19.512613Z","shell.execute_reply":"2021-09-01T11:15:19.538653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the dataset on original data","metadata":{}},{"cell_type":"code","source":"# If we look our data set, it looks like all are numerical data, but actually columns\n# 'cnt', 'temp', 'atemp', 'hum', 'windspeed' are actual numeric variable, remaining are numerical categorical variables\n# But later on any way we will convert the, to dummy variables so, lets visualize only the actual numeric variables\n# For any kind of corelation among them and with target variable 'cnt'\n# If no linear relation observed among the cnt and any of the variable then regression model is not possible\n\nnumeric_columns = ['cnt', 'temp', 'atemp', 'hum', 'windspeed'];\nsns.pairplot(bike[numeric_columns])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:19.541046Z","iopub.execute_input":"2021-09-01T11:15:19.54165Z","iopub.status.idle":"2021-09-01T11:15:24.931871Z","shell.execute_reply.started":"2021-09-01T11:15:19.541606Z","shell.execute_reply":"2021-09-01T11:15:24.927406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above plots we can observe there is a kind of correlation among\n    1. cnt with temp, atemp variables\n    2. temp and atemp","metadata":{}},{"cell_type":"code","source":"# Visualize the categorical data wrt to target variable cnt before making the dummy data\nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bike)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bike)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bike)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bike)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bike)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bike)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:24.933354Z","iopub.execute_input":"2021-09-01T11:15:24.933912Z","iopub.status.idle":"2021-09-01T11:15:25.973368Z","shell.execute_reply.started":"2021-09-01T11:15:24.933878Z","shell.execute_reply":"2021-09-01T11:15:25.972351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can able to make the following insights from the above box plots wrt to target variable 'cnt'\n\n    1. Working day:\n    - Almost 69% of users books bike in working day which is closes to 5000\n    - This indicates, workingday can be a good predictor for the dependent variable\n    \n    2. Weekday:\n        - Almost all weekdays, the no.of bike users count was similar and it is around in between 3000-6000\n        - Medians of all the weekdays are around in between the 4000-6000 that means, more than 50% of people using bikes in all days of a week irrespective of the day of the week.\n        - Difference/distance between the 25% and 75% of box is more for weekdays 3(Wednesday) & 6(Saturday) but not a  significant difference when compared with others [Considering start of week as Sunday]\n        \n    3. Holiday:\n        - Almost 97.6% of the bike booking were happening when it is not a holiday which means this data is clearly biased. This indicates, holiday CANNOT be a good predictor for the dependent variable.\n         \n    4. Weathersit:\n        - Most of the bike users are in weathersit 1(Clear, Few clouds, Partly cloudy, Partly cloudy), followed by 2(Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist) which is almost 67% of users.\n        - This was followed by weathersit2 with 30% of total booking\n        - Very less no.of bike users are available in weathersit 3(Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds).\n        - This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable.\n   \n    5. Month:\n        - Most of the bookings are happening around the months 6, 7, 8, 9 (more than 5000 bookings are happening. Almost 10%)\n        - Where as months 1 & 2 are having less bookings (Less than 3000)\n        - In almost all months the differenct between the 25% to 75% is similar but for months 3,4,9,10 is having significantly more difference\n        - This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable.\n        \n    6. Season:\n        - For season 3 having more no.of bike users (More than or equal to 5000 users. Almost 32%) followed by season2 & season4 with 27% & 25% (greater than 4000 and near to 5000)\n        - Season 1 is having less users which is less than 3000\n        - Almost all seasons are having the difference between 25% and 75% is significantly having no difference among them.\n        - This indicates, season can be a good predictor for the dependent variable.","metadata":{}},{"cell_type":"code","source":"# Correlatio matrix to visulaize which columns are having corelation with cnt\nplt.figure(figsize = (16, 10))\nsns.heatmap(bike.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:25.974609Z","iopub.execute_input":"2021-09-01T11:15:25.974902Z","iopub.status.idle":"2021-09-01T11:15:26.867452Z","shell.execute_reply.started":"2021-09-01T11:15:25.974873Z","shell.execute_reply":"2021-09-01T11:15:26.866311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above heatmap it is clearly observed that 'cnt' column is having \n    - More Positive Correlation with the predictor variables  'atemp', 'temp' followed by 'yr'.\n    - More Negative Correlation with 'weathersit', 'windspeed'","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing data for model","metadata":{}},{"cell_type":"markdown","source":"#### Now lets convert the numeric categorical to the dummy variables\n#### To do that we need to first convert the type of those columns and then convert them to dummy variables\n\n### Why to convert the data type from numeric to category?\n    Before answering that, lets look at the syntax of get_dummies.\n#### Syntax: \n####    `pandas.get_dummies(data, prefix=None, prefix_sep=’_’, dummy_na=False, columns=None, sparse=False, drop_first=False, \n#### dtype=None)`\n\n#### Lets see the description of some attributes which are rerquired for our question:\n#### columns: \n        This attribute specifies the columns that needs to get dummies. Default is None. If not specified the columns it by default takes object, category datatype columns to get dummies.\n        \n#### drop_first:\n        This is default false. It means that we get dummies for all categorical of all 'k' levels. If it is true, then we get the dummies for 'k-1' level which is expected for our model.","metadata":{}},{"cell_type":"code","source":"# Lets change the data types of some of the columns which needs to get dummy data.\ncolumns_to_get_dummy = ['season', 'mnth', 'weathersit', 'weekday'];\n# Convert datatype form numeric to category so that we can get dummies for these columns\nbike[columns_to_get_dummy] = bike[columns_to_get_dummy].astype('category')","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:26.869236Z","iopub.execute_input":"2021-09-01T11:15:26.869654Z","iopub.status.idle":"2021-09-01T11:15:26.881934Z","shell.execute_reply.started":"2021-09-01T11:15:26.869614Z","shell.execute_reply":"2021-09-01T11:15:26.880656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check info wether they have converted or not\nbike[columns_to_get_dummy].info()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:26.883713Z","iopub.execute_input":"2021-09-01T11:15:26.884146Z","iopub.status.idle":"2021-09-01T11:15:26.906541Z","shell.execute_reply.started":"2021-09-01T11:15:26.884099Z","shell.execute_reply":"2021-09-01T11:15:26.905306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, lets creqate dummy variables for the categorical varibles with drop_first=True \nbike = pd.get_dummies(bike, drop_first=True)\nbike","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:26.908226Z","iopub.execute_input":"2021-09-01T11:15:26.908676Z","iopub.status.idle":"2021-09-01T11:15:26.958903Z","shell.execute_reply.started":"2021-09-01T11:15:26.908631Z","shell.execute_reply":"2021-09-01T11:15:26.958141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now lets check the columns available in our data set after creating dummies\nbike.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:26.960129Z","iopub.execute_input":"2021-09-01T11:15:26.960435Z","iopub.status.idle":"2021-09-01T11:15:26.979119Z","shell.execute_reply.started":"2021-09-01T11:15:26.960407Z","shell.execute_reply":"2021-09-01T11:15:26.97808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Since, we used drop_first=True, categorical levels of season, month, weekday, weather were reduced by 1 from their actual levels","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the data set into training and test data set","metadata":{}},{"cell_type":"code","source":"# Lets split the data set into training and test data set of 70-30 percentages respectively.\n# Use the standard notation for them as df_train, df_test. Sicne, we are having more than one predictor features\n# train_test_split returns data frames instead of series data.\n\n# random_state: Controls the shuffling applied to the data before applying the split\n# shuffel: Deafult True, so no need to specify it explicitly\ndf_train, df_test = train_test_split(bike, train_size=0.7, random_state=333);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:26.980366Z","iopub.execute_input":"2021-09-01T11:15:26.980661Z","iopub.status.idle":"2021-09-01T11:15:26.988325Z","shell.execute_reply.started":"2021-09-01T11:15:26.980631Z","shell.execute_reply":"2021-09-01T11:15:26.987129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check the shape and data in df_train, df_test\nprint(\"Training data set : \", df_train.shape);\nprint(\"Testing data set : \", df_test.shape);\n\n### Data set was splitted as per our requirement\n### 730: Actual total data\n### 510: 70% of actual data (Training set)\n### 220: 30% of actual data (Testing set)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:26.989704Z","iopub.execute_input":"2021-09-01T11:15:26.990034Z","iopub.status.idle":"2021-09-01T11:15:27.002495Z","shell.execute_reply.started":"2021-09-01T11:15:26.989999Z","shell.execute_reply":"2021-09-01T11:15:27.001259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see how data looks like in training data set\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.0039Z","iopub.execute_input":"2021-09-01T11:15:27.004212Z","iopub.status.idle":"2021-09-01T11:15:27.034861Z","shell.execute_reply.started":"2021-09-01T11:15:27.004183Z","shell.execute_reply":"2021-09-01T11:15:27.033698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.036418Z","iopub.execute_input":"2021-09-01T11:15:27.036772Z","iopub.status.idle":"2021-09-01T11:15:27.131362Z","shell.execute_reply.started":"2021-09-01T11:15:27.036728Z","shell.execute_reply":"2021-09-01T11:15:27.130465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### If we look at the above data, all of them are numerical data.\n#####  But, most of the columns are having data either 0/1 except the columns temp, atemp, hum, windspeed, cnt.\n##### So, we need to rescale the data. So, that our predictions gets much more reliable and accurate.\n\n#### So, lets rescale the data using `MinMaxScaling/Normalization` method.\n\n### Why Normalization method is using?\n#### Answer:\n##### Beacuse, \n    1. Normalization methods makes the data to be present in between 0-1 which looks similar to the other columns data.\n    2. It doesnt create any effect or change in the categorical data or dummy data that we have created already.","metadata":{}},{"cell_type":"markdown","source":"### Rescaling the features","metadata":{}},{"cell_type":"code","source":"### Need to do scaling on the training data set. Sicne it was used for training the model\n\n# Create scaler object\nscaler = MinMaxScaler();\n\n# Create Columns list required for scaling\ncolumns_req_for_scaling = ['temp', 'atemp', 'hum', 'windspeed', 'cnt'];\n\n# Now, fit and transform the data for the above columns in our dataset\ndf_train[columns_req_for_scaling] = scaler.fit_transform(df_train[columns_req_for_scaling]);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.132487Z","iopub.execute_input":"2021-09-01T11:15:27.132765Z","iopub.status.idle":"2021-09-01T11:15:27.14676Z","shell.execute_reply.started":"2021-09-01T11:15:27.132739Z","shell.execute_reply":"2021-09-01T11:15:27.14446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets check wether the data is scaled or not\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.147964Z","iopub.execute_input":"2021-09-01T11:15:27.148239Z","iopub.status.idle":"2021-09-01T11:15:27.251864Z","shell.execute_reply.started":"2021-09-01T11:15:27.148213Z","shell.execute_reply":"2021-09-01T11:15:27.250666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Now, we can observe that mostly all the columns are sacled in between 0-1. It can be observed by checking the min value and max value of columns\n\n##### Now, our dataset is ready for the training.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the Model","metadata":{}},{"cell_type":"code","source":"# Lets create the X & Y variables from training data frame for training the model \ny_train = df_train.pop('cnt') # Our target varaiable is cnt, So lets assume it as y_train and rest columns as X_train\nX_train = df_train","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.256576Z","iopub.execute_input":"2021-09-01T11:15:27.256894Z","iopub.status.idle":"2021-09-01T11:15:27.261393Z","shell.execute_reply.started":"2021-09-01T11:15:27.256864Z","shell.execute_reply":"2021-09-01T11:15:27.260621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see what data is available in X_train\nX_train.head()\n# We can observe all the columns except cnt is available in X_train","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.263706Z","iopub.execute_input":"2021-09-01T11:15:27.264111Z","iopub.status.idle":"2021-09-01T11:15:27.291612Z","shell.execute_reply.started":"2021-09-01T11:15:27.264083Z","shell.execute_reply":"2021-09-01T11:15:27.290514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.292583Z","iopub.execute_input":"2021-09-01T11:15:27.292861Z","iopub.status.idle":"2021-09-01T11:15:27.387376Z","shell.execute_reply.started":"2021-09-01T11:15:27.292826Z","shell.execute_reply":"2021-09-01T11:15:27.386261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Selecting features using RFE","metadata":{}},{"cell_type":"code","source":"# Create Linear regression object\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Running RFE with the output number of the variable equal to 15 (50% of actual no.of columns available and it is suggestable)\n# By passing the fitted model with it\nrfe = RFE(lm, 15)\n\n# get the fitted rfe\nrfe = rfe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.388566Z","iopub.execute_input":"2021-09-01T11:15:27.388846Z","iopub.status.idle":"2021-09-01T11:15:27.614075Z","shell.execute_reply.started":"2021-09-01T11:15:27.388811Z","shell.execute_reply":"2021-09-01T11:15:27.61332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List down all the 15 outcomes given by the RFE with rankings and its significant boolean\n# support_ : provides wether that column is supported for the model or not\n# ranking_ : provides the ranking of the models suitable for the model to pick\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.615306Z","iopub.execute_input":"2021-09-01T11:15:27.615798Z","iopub.status.idle":"2021-09-01T11:15:27.624117Z","shell.execute_reply.started":"2021-09-01T11:15:27.615767Z","shell.execute_reply":"2021-09-01T11:15:27.623408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_supports_model = X_train.columns[rfe.support_]\ncols_supports_model","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.625241Z","iopub.execute_input":"2021-09-01T11:15:27.625772Z","iopub.status.idle":"2021-09-01T11:15:27.6369Z","shell.execute_reply.started":"2021-09-01T11:15:27.625741Z","shell.execute_reply":"2021-09-01T11:15:27.635683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_not_supported = X_train.columns[~rfe.support_]\ncols_not_supported","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.638196Z","iopub.execute_input":"2021-09-01T11:15:27.638516Z","iopub.status.idle":"2021-09-01T11:15:27.649115Z","shell.execute_reply.started":"2021-09-01T11:15:27.638486Z","shell.execute_reply":"2021-09-01T11:15:27.647985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove columns which are not supported for model and store them \n# so that it will be used while testing the model with test dataset\nX_train = X_train[cols_supports_model];","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.65052Z","iopub.execute_input":"2021-09-01T11:15:27.650825Z","iopub.status.idle":"2021-09-01T11:15:27.660201Z","shell.execute_reply.started":"2021-09-01T11:15:27.650797Z","shell.execute_reply":"2021-09-01T11:15:27.659439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Cols_deleted = []\nfor val in cols_not_supported.values:\n    Cols_deleted.append(val);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.661572Z","iopub.execute_input":"2021-09-01T11:15:27.662231Z","iopub.status.idle":"2021-09-01T11:15:27.671567Z","shell.execute_reply.started":"2021-09-01T11:15:27.662183Z","shell.execute_reply":"2021-09-01T11:15:27.670391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Cols_deleted","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.673223Z","iopub.execute_input":"2021-09-01T11:15:27.673706Z","iopub.status.idle":"2021-09-01T11:15:27.685645Z","shell.execute_reply.started":"2021-09-01T11:15:27.673662Z","shell.execute_reply":"2021-09-01T11:15:27.684559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.686804Z","iopub.execute_input":"2021-09-01T11:15:27.687242Z","iopub.status.idle":"2021-09-01T11:15:27.707211Z","shell.execute_reply.started":"2021-09-01T11:15:27.687189Z","shell.execute_reply":"2021-09-01T11:15:27.706256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Common method to print the VIF continously\ndef printVIF(trainingDataSet, constantVariable):\n    vif = pd.DataFrame();\n    actualDataSet = trainingDataSet;\n    dataSetWithoutConstant = actualDataSet.drop(constantVariable, axis=1);\n    X = dataSetWithoutConstant;\n    vif['Features'] = X.columns;\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])];\n    vif['VIF'] = round(vif['VIF'], 2);\n    vif = vif.sort_values(by = \"VIF\", ascending = False);\n    print(vif);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.708392Z","iopub.execute_input":"2021-09-01T11:15:27.708673Z","iopub.status.idle":"2021-09-01T11:15:27.718172Z","shell.execute_reply.started":"2021-09-01T11:15:27.708646Z","shell.execute_reply":"2021-09-01T11:15:27.71687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fixed target variable for our model\nconstant_variable = 'const';","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.719856Z","iopub.execute_input":"2021-09-01T11:15:27.720194Z","iopub.status.idle":"2021-09-01T11:15:27.729429Z","shell.execute_reply.started":"2021-09-01T11:15:27.72016Z","shell.execute_reply":"2021-09-01T11:15:27.728415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Building the Model with the variables supported for model","metadata":{}},{"cell_type":"code","source":"# To build the model lets add constant to the X_train so that our model soent pass through origin\n# store it in new variable say X_train_lm\nX_train_lm = sm.add_constant(X_train);\n\n# Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.730842Z","iopub.execute_input":"2021-09-01T11:15:27.731139Z","iopub.status.idle":"2021-09-01T11:15:27.755328Z","shell.execute_reply.started":"2021-09-01T11:15:27.731111Z","shell.execute_reply":"2021-09-01T11:15:27.754402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the parameters given by the model\nlm.params","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.756578Z","iopub.execute_input":"2021-09-01T11:15:27.756898Z","iopub.status.idle":"2021-09-01T11:15:27.76556Z","shell.execute_reply.started":"2021-09-01T11:15:27.75687Z","shell.execute_reply":"2021-09-01T11:15:27.764356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets look at the summary\nlm.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.767275Z","iopub.execute_input":"2021-09-01T11:15:27.767808Z","iopub.status.idle":"2021-09-01T11:15:27.803643Z","shell.execute_reply.started":"2021-09-01T11:15:27.767762Z","shell.execute_reply":"2021-09-01T11:15:27.802606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since we cannot take the decision based on the only pf values for feature removal. \n# Lets calculate the VIF for the X_train_lm columns\nprintVIF(X_train_lm, constant_variable);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.804885Z","iopub.execute_input":"2021-09-01T11:15:27.805156Z","iopub.status.idle":"2021-09-01T11:15:27.842367Z","shell.execute_reply.started":"2021-09-01T11:15:27.80513Z","shell.execute_reply":"2021-09-01T11:15:27.841085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### From above model summary and VIF data we can see that \n    High p-value features are `atemp`\n    High VIF value feature are `temp`\n    \n    R-Square value is 84.8%\n    \n##### Since, we follow one of the golden thumb rule is to remove the feature whic is having more p-value before removing the High VIF value.\n##### So, lets remove the colum `atemp` from the model and lets rebuild model again","metadata":{}},{"cell_type":"code","source":"# Droping feature 'atemp' from training data set\nX_train_lm = X_train_lm.drop(['atemp'], axis=1)\nX_train_lm.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.843856Z","iopub.execute_input":"2021-09-01T11:15:27.84425Z","iopub.status.idle":"2021-09-01T11:15:27.853614Z","shell.execute_reply.started":"2021-09-01T11:15:27.844205Z","shell.execute_reply":"2021-09-01T11:15:27.852662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add deleted column into our Cols_deleted array for future use\nCols_deleted.append('atemp');\nCols_deleted","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.855174Z","iopub.execute_input":"2021-09-01T11:15:27.855878Z","iopub.status.idle":"2021-09-01T11:15:27.864266Z","shell.execute_reply.started":"2021-09-01T11:15:27.855835Z","shell.execute_reply":"2021-09-01T11:15:27.863115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running the linear model again\nlm = sm.OLS(y_train,X_train_lm).fit();\n# Print summary of rebuilded model\nlm.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.865734Z","iopub.execute_input":"2021-09-01T11:15:27.86613Z","iopub.status.idle":"2021-09-01T11:15:27.898955Z","shell.execute_reply.started":"2021-09-01T11:15:27.866089Z","shell.execute_reply":"2021-09-01T11:15:27.897753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets print the VIF for the current columns available in the training data set\nprintVIF(X_train_lm, constant_variable);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.900578Z","iopub.execute_input":"2021-09-01T11:15:27.901185Z","iopub.status.idle":"2021-09-01T11:15:27.9339Z","shell.execute_reply.started":"2021-09-01T11:15:27.901134Z","shell.execute_reply":"2021-09-01T11:15:27.932907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### From the above summary and model we can derive following data\n    High VIF value is `temp`\n    No features with high p-values (>0.05)\n    \n    R-Square is still same 84.2%\n    \n##### So, now lets remove the columm `temp` which is having more VIF and rebuild our model again","metadata":{}},{"cell_type":"code","source":"# Droping feature 'temp' from training data set\nX_train_lm = X_train_lm.drop(['temp'], axis=1)\nX_train_lm.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.935138Z","iopub.execute_input":"2021-09-01T11:15:27.935499Z","iopub.status.idle":"2021-09-01T11:15:27.945329Z","shell.execute_reply.started":"2021-09-01T11:15:27.935456Z","shell.execute_reply":"2021-09-01T11:15:27.943992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add deleted column into our Cols_deleted array for future use\nCols_deleted.append('temp');\nCols_deleted","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.947102Z","iopub.execute_input":"2021-09-01T11:15:27.947576Z","iopub.status.idle":"2021-09-01T11:15:27.95798Z","shell.execute_reply.started":"2021-09-01T11:15:27.947531Z","shell.execute_reply":"2021-09-01T11:15:27.956929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running the linear model again\nlm = sm.OLS(y_train,X_train_lm).fit();\n# Print summary of rebuilded model\nlm.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.9594Z","iopub.execute_input":"2021-09-01T11:15:27.960136Z","iopub.status.idle":"2021-09-01T11:15:27.99261Z","shell.execute_reply.started":"2021-09-01T11:15:27.960089Z","shell.execute_reply":"2021-09-01T11:15:27.9915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets print the VIF for the current columns available in the training data set\nprintVIF(X_train_lm, constant_variable);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:27.993965Z","iopub.execute_input":"2021-09-01T11:15:27.99458Z","iopub.status.idle":"2021-09-01T11:15:28.023436Z","shell.execute_reply.started":"2021-09-01T11:15:27.994534Z","shell.execute_reply":"2021-09-01T11:15:28.022423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### From the above summary and VIF we can say that:\n    High P-value feature is `hum`\n    High VIF feature is `hum`\n    \n    R-Square value was changed from 84.2 to 77.3 percentage\n   \n##### S0, lets remove the colum `hum` from the model and lets rebuild model again","metadata":{}},{"cell_type":"code","source":"# Droping feature 'hum' from training data set\nX_train_lm = X_train_lm.drop(['hum'], axis=1)\nX_train_lm.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.02482Z","iopub.execute_input":"2021-09-01T11:15:28.02541Z","iopub.status.idle":"2021-09-01T11:15:28.034214Z","shell.execute_reply.started":"2021-09-01T11:15:28.025367Z","shell.execute_reply":"2021-09-01T11:15:28.033238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add deleted column into our Cols_deleted array for future use\nCols_deleted.append('hum');\nCols_deleted","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.036393Z","iopub.execute_input":"2021-09-01T11:15:28.037313Z","iopub.status.idle":"2021-09-01T11:15:28.05197Z","shell.execute_reply.started":"2021-09-01T11:15:28.037238Z","shell.execute_reply":"2021-09-01T11:15:28.05061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running the linear model again\nlm = sm.OLS(y_train,X_train_lm).fit();\n# Print summary of rebuilded model\nlm.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.053896Z","iopub.execute_input":"2021-09-01T11:15:28.054647Z","iopub.status.idle":"2021-09-01T11:15:28.08255Z","shell.execute_reply.started":"2021-09-01T11:15:28.054599Z","shell.execute_reply":"2021-09-01T11:15:28.081501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets print the VIF for the current columns available in the training data set\nprintVIF(X_train_lm, constant_variable);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.083876Z","iopub.execute_input":"2021-09-01T11:15:28.084465Z","iopub.status.idle":"2021-09-01T11:15:28.11105Z","shell.execute_reply.started":"2021-09-01T11:15:28.084421Z","shell.execute_reply":"2021-09-01T11:15:28.10989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### From the above summary and VIF we can say that:\n    NO Feature having p-value (> 0.05)\n    No High VIF feature\n    \n    R-Square value was changed slightly(negligable) from 77.3 to 77.2 percentage","metadata":{}},{"cell_type":"markdown","source":"#### Since we are not having any High VIF and High P-Values we can stop modeling here and can make this as our best fit model.\n#### With \n    1. R-Square : 77.2%\n    2. Total Coefficients : 12 + 1 Constant","metadata":{}},{"cell_type":"code","source":"# Coeffficents of model\nlm.params","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.112394Z","iopub.execute_input":"2021-09-01T11:15:28.112981Z","iopub.status.idle":"2021-09-01T11:15:28.121681Z","shell.execute_reply.started":"2021-09-01T11:15:28.112936Z","shell.execute_reply":"2021-09-01T11:15:28.120666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Residual Analysis","metadata":{}},{"cell_type":"code","source":"# Get the predicted values of y from model using training data set\ny_train_pred = lm.predict(X_train_lm)\n\n# Calculate residuals \nres = (y_train - y_train_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.123249Z","iopub.execute_input":"2021-09-01T11:15:28.123675Z","iopub.status.idle":"2021-09-01T11:15:28.135709Z","shell.execute_reply.started":"2021-09-01T11:15:28.12363Z","shell.execute_reply":"2021-09-01T11:15:28.133436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the residuals\nsns.distplot(res);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.137305Z","iopub.execute_input":"2021-09-01T11:15:28.138039Z","iopub.status.idle":"2021-09-01T11:15:28.389879Z","shell.execute_reply.started":"2021-09-01T11:15:28.137985Z","shell.execute_reply":"2021-09-01T11:15:28.388916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can observe that residuals are centered around the mean of 0 and it is normally distributed.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prediction and evaluation on Test data","metadata":{}},{"cell_type":"code","source":"# Lets do pre processing on the test data set as we did it on training data set\n# we do rescaling on test dataset for columns required for sacling and do transform on it instead of fit again\n# Now, fit and transform the data for the above columns in our dataset\ndf_test[columns_req_for_scaling] = scaler.fit_transform(df_test[columns_req_for_scaling]);\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.391309Z","iopub.execute_input":"2021-09-01T11:15:28.391729Z","iopub.status.idle":"2021-09-01T11:15:28.425843Z","shell.execute_reply.started":"2021-09-01T11:15:28.391688Z","shell.execute_reply":"2021-09-01T11:15:28.424917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.427206Z","iopub.execute_input":"2021-09-01T11:15:28.427537Z","iopub.status.idle":"2021-09-01T11:15:28.522548Z","shell.execute_reply.started":"2021-09-01T11:15:28.427508Z","shell.execute_reply":"2021-09-01T11:15:28.521407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now lets create y_test, X_test data sets for evalution\ny_test = df_test.pop('cnt')\nX_test = df_test","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.524092Z","iopub.execute_input":"2021-09-01T11:15:28.524504Z","iopub.status.idle":"2021-09-01T11:15:28.531671Z","shell.execute_reply.started":"2021-09-01T11:15:28.524463Z","shell.execute_reply":"2021-09-01T11:15:28.530539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets add constant for X_test for fitting model on test data set\nX_test_lm = sm.add_constant(X_test);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.533208Z","iopub.execute_input":"2021-09-01T11:15:28.53366Z","iopub.status.idle":"2021-09-01T11:15:28.548458Z","shell.execute_reply.started":"2021-09-01T11:15:28.533617Z","shell.execute_reply":"2021-09-01T11:15:28.54751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns before dropping deleted columns from final model\nX_test_lm.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.549966Z","iopub.execute_input":"2021-09-01T11:15:28.550398Z","iopub.status.idle":"2021-09-01T11:15:28.557789Z","shell.execute_reply.started":"2021-09-01T11:15:28.550354Z","shell.execute_reply":"2021-09-01T11:15:28.556457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove columns that are not available in the final model so that our predictions will be accurate and matches with final model\nX_test_lm = X_test_lm.drop(Cols_deleted, axis=1)\nX_test_lm.columns","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.559514Z","iopub.execute_input":"2021-09-01T11:15:28.559885Z","iopub.status.idle":"2021-09-01T11:15:28.570081Z","shell.execute_reply.started":"2021-09-01T11:15:28.559856Z","shell.execute_reply":"2021-09-01T11:15:28.569094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now predict the model based on the test data set using the final model obj\ny_test_pred = lm.predict(X_test_lm)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.571341Z","iopub.execute_input":"2021-09-01T11:15:28.571612Z","iopub.status.idle":"2021-09-01T11:15:28.579539Z","shell.execute_reply.started":"2021-09-01T11:15:28.571586Z","shell.execute_reply":"2021-09-01T11:15:28.57853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Assumptions","metadata":{}},{"cell_type":"code","source":"### Residuals are normally distributed\n# Get the predicted values of y from model using training data set\ny_train_pred = lm.predict(X_train_lm)\n\n# Calculate residuals \nres = (y_train - y_train_pred)\n# Plot the residuals\nsns.distplot(res);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.580652Z","iopub.execute_input":"2021-09-01T11:15:28.580936Z","iopub.status.idle":"2021-09-01T11:15:28.819175Z","shell.execute_reply.started":"2021-09-01T11:15:28.580903Z","shell.execute_reply":"2021-09-01T11:15:28.818439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can observe that residuals are centered around the mean of 0 and it is normally distributed.","metadata":{}},{"cell_type":"code","source":"### There is a linear relationship between X & Y\nbike_assump = bike[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\nsns.pairplot(bike_assump, diag_kind='kde')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:28.820435Z","iopub.execute_input":"2021-09-01T11:15:28.820707Z","iopub.status.idle":"2021-09-01T11:15:33.426049Z","shell.execute_reply.started":"2021-09-01T11:15:28.82068Z","shell.execute_reply":"2021-09-01T11:15:33.425331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We can see from above graph that there is a linear relationship between X('temp', 'atemp') & y('cnt')","metadata":{}},{"cell_type":"code","source":"#### There is no multicollinearity between the predictor variables in the final model\nprintVIF(X_train_lm, constant_variable);","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:33.427204Z","iopub.execute_input":"2021-09-01T11:15:33.427684Z","iopub.status.idle":"2021-09-01T11:15:33.46026Z","shell.execute_reply.started":"2021-09-01T11:15:33.427651Z","shell.execute_reply":"2021-09-01T11:15:33.45928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We can observe that all of the feature variables in the final model are having VIF less than 5. Hence we can say that there is no collinearity between the predictors","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\nfig = plt.figure()\nplt.scatter(y_test, y_test_pred, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:33.46169Z","iopub.execute_input":"2021-09-01T11:15:33.462075Z","iopub.status.idle":"2021-09-01T11:15:33.630462Z","shell.execute_reply.started":"2021-09-01T11:15:33.462032Z","shell.execute_reply":"2021-09-01T11:15:33.62946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above graph we can say that spread of the y_test and y_pred are linear and high nice spread among them.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyszing the R-Square for Test and Training data Set","metadata":{}},{"cell_type":"code","source":"# Calculate the R-Square for the training set\ntrain_r2_score = r2_score(y_true = y_train, y_pred = y_train_pred);\ntrain_r2_score","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:33.631739Z","iopub.execute_input":"2021-09-01T11:15:33.632023Z","iopub.status.idle":"2021-09-01T11:15:33.63908Z","shell.execute_reply.started":"2021-09-01T11:15:33.631994Z","shell.execute_reply":"2021-09-01T11:15:33.638037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the R-Square for the predicted set\ntest_r2_score = r2_score(y_true = y_test, y_pred = y_test_pred);\ntest_r2_score","metadata":{"execution":{"iopub.status.busy":"2021-09-01T11:15:33.640428Z","iopub.execute_input":"2021-09-01T11:15:33.640722Z","iopub.status.idle":"2021-09-01T11:15:33.652277Z","shell.execute_reply.started":"2021-09-01T11:15:33.640697Z","shell.execute_reply":"2021-09-01T11:15:33.651111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion","metadata":{}},{"cell_type":"markdown","source":"#### Hence, It is observed that our R-Squares values for training and predicted data sets are similar.\n#### Our model is best fit for our data prediction.","metadata":{}},{"cell_type":"markdown","source":"### Final MLR line is as follows\n\n### cnt = 0.235851 + (yr * 0.243876) + (workingday * 0.043512) - (windspeed * 0.175637) + (season_2 * 0.268363) + \n###            (season_3 * 0.316649) + (season_4 * 0.199574) + (mnth_3 * 0.056134) + (mnth_9 * 0.090393) +\n###            (mnth_10 * 0.105879) + (weekday_6 * 0.045960) - (weathersit_2 * 0.083298) - (weathersit_3 * 0.347539)","metadata":{}},{"cell_type":"markdown","source":"#### Above equation interpretations\n    cnt: Total no.of biker users count and it is the target variable in our model\n    yr: Year in which bike was rented/used. Unit change in year causes the 0.243876 units change in users count\n    workingday: Wether the bike rented day is holiday or workingday. Unit increase in workingday increases 0.043512 unit changes in users count\n    windspeed: Unit increase in windspeed decreases 0.175637 units in users count\n    season_2: Season_2 is summer. Unit increase in season_2 increases 0.268363 units in users count\n    season_3: season_3 is fall. Unit increase in season_3 increases 0.316649 units in users count\n    season_4: season_4 is winter. Unit increase in season_4 increases 0.199574 units in user count\n    mnth_3: mnth_3 is march. Unit increase in mnth_3 increases 0.056134 units in users count\n    mnth_9: mnth_9 is september. Unit increase in mnth_9 increases 0.090393 units in users count\n    mnth_10: mnth_10 is october. Unit increase in mnth_10 increases 0.105879 units in users count\n    weekday_6: weekday_6 is friday. Unit increase in weekday_6 increases 0.045960 units in users count\n    weathersit_2: unit increase in weathersit_2 decreases 0.083298 units in users count\n    weathersit_3: unit increase in weathersit_3 decreases 0.347539 units in users count","metadata":{}},{"cell_type":"markdown","source":"#### From the above equation for the obtained model top 3 predictor variables are \n####  `season_3` , `season_2`,  `yr`  with its coefficients 0.316649, 0.268363, 0.243876 respectively\n\n#### where as `weathersit_3`, `windspeed`, `weathersit_2` are negatively related with target variable with its coefficients 0.347539, 0.175637, 0.083298","metadata":{}}]}