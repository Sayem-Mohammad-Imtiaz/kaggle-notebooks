{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **How to use the Python programming Language for Time Series Analysis!**\n\nThis work was prepared together with [Gul Bulut](https://www.kaggle.com/gulyvz) and [Bulent Siyah](https://www.kaggle.com/bulentsiyah/). **The whole study consists of two parties**\n* [Time Series Forecasting and Analysis- Part 1](https://www.kaggle.com/gulyvz/time-series-forecasting-and-analysis-part-1)\n* [Time Series Forecasting and Analysis- Part 2](https://www.kaggle.com/bulentsiyah/time-series-forecasting-and-analysis-part-2)\n\nThis kernel will teach you everything you need to know to use Python for forecasting time series data to predict new future data points.\n\n![](https://iili.io/JaZxFS.png)\n\nwe'll learn about state of the art Deep Learning techniques with Recurrent Neural Networks that use deep learning to forecast future data points.\n\n![](https://iili.io/JaZCMl.png)\n\n\nThis kernel even covers Facebook's Prophet library, a simple to use, yet powerful Python library developed to forecast into the future with time series data.\n\n![](https://iili.io/JaZnP2.png)\n\n# **Content Part 1** \n\n1. [How to Work with Time Series Data with Pandas](https://www.kaggle.com/gulyvz/time-series-forecasting-and-analysis-part-1#1.)\n1. [Use Statsmodels to Analyze Time Series Data](https://www.kaggle.com/gulyvz/time-series-forecasting-and-analysis-part-1#2.)\n1. [General Forecasting Models - ARIMA(Autoregressive Integrated Moving Average)](https://www.kaggle.com/gulyvz/time-series-forecasting-and-analysis-part-1#3.)\n1. [General Forecasting Models - SARIMA(Seasonal Autoregressive Integrated Moving Average)](https://www.kaggle.com/gulyvz/time-series-forecasting-and-analysis-part-1#4.)\n1. [General Forecasting Models - SARIMAX](https://www.kaggle.com/gulyvz/time-series-forecasting-and-analysis-part-1#5.)\n\n# **Content Part 2**\n\n1. [Deep Learning for Time Series Forecasting - (RNN)](#1.)\n1. [Multivariate Time Series with RNN](#2.)\n1. [Use Facebook's Prophet Library for forecasting](#3.)\n"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"1.\"></a> \n# 1.Deep Learning for Time Series Forecasting - (RNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/for-simple-exercises-time-series-forecasting/Alcohol_Sales.csv',index_col='DATE',parse_dates=True)\ndf.index.freq = 'MS'\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['Sales']\ndf.plot(figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nresults = seasonal_decompose(df['Sales'])\nresults.observed.plot(figsize=(12,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.trend.plot(figsize=(12,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.seasonal.plot(figsize=(12,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.resid.plot(figsize=(12,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"len(df)\", len(df))\n\ntrain = df.iloc[:313]\ntest = df.iloc[313:]\n\n\nprint(\"len(train)\", len(train))\nprint(\"len(test)\", len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scale Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\n# IGNORE WARNING ITS JUST CONVERTING TO FLOATS\n# WE ONLY FIT TO TRAININ DATA, OTHERWISE WE ARE CHEATING ASSUMING INFO ABOUT TEST SET\nscaler.fit(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Series Generator\n\nThis class takes in a sequence of data-points gathered at\nequal intervals, along with time series parameters such as\nstride, length of history, etc., to produce batches for\ntraining/validation."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from keras.preprocessing.sequence import TimeseriesGenerator\nscaled_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define generator\nn_input = 2\nn_features = 1\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)\n\nprint('len(scaled_train)',len(scaled_train))\nprint('len(generator)',len(generator))  # n_input = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What does the first batch look like?\nX,y = generator[0]\n\nprint(f'Given the Array: \\n{X.flatten()}')\nprint(f'Predict this y: \\n {y}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's redefine to get 12 months back and then predict the next month out\nn_input = 12\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)\n\n# What does the first batch look like?\nX,y = generator[0]\n\nprint(f'Given the Array: \\n{X.flatten()}')\nprint(f'Predict this y: \\n {y}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nmodel.fit_generator(generator,epochs=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.history.history.keys()\nloss_per_epoch = model.history.history['loss']\nplt.plot(range(len(loss_per_epoch)),loss_per_epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_eval_batch = scaled_train[-12:]\nfirst_eval_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_eval_batch = first_eval_batch.reshape((1, n_input, n_features))\nmodel.predict(first_eval_batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_test[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-n_input:]\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n    \ntest_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inverse Transformations and Compare"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(test_predictions)\ntrue_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Predictions'] = true_predictions\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.plot(figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving and Loading Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('my_rnn_model.h5')\n'''from keras.models import load_model\nnew_model = load_model('my_rnn_model.h5')'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"2.\"></a> \n# 2.Multivariate Time Series with RNN"},{"metadata":{},"cell_type":"markdown","source":"Experimental data used to create regression models of appliances energy use in a low energy building. Data Set Information: The data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters). "},{"metadata":{},"cell_type":"markdown","source":"## Data\n\nLet's read in the data set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('../input/for-simple-exercises-time-series-forecasting/energydata_complete.csv',index_col='date', infer_datetime_format=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Windspeed'].plot(figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Appliances'].plot(figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc['2016-05-01':]\ndf = df.round(2)\n\nprint('len(df)',len(df))\ntest_days = 2\ntest_ind = test_days*144 # 24*60/10 = 144\ntest_ind","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.iloc[:-test_ind]\ntest = df.iloc[-test_ind:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scale Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# IGNORE WARNING ITS JUST CONVERTING TO FLOATS\n# WE ONLY FIT TO TRAININ DATA, OTHERWISE WE ARE CHEATING ASSUMING INFO ABOUT TEST SET\nscaler.fit(train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Series Generator\n\nThis class takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training/validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n\n# define generator\nlength = 144 # Length of the output sequences (in number of timesteps)\nbatch_size = 1 #Number of timeseries samples in each batch\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('len(scaled_train)',len(scaled_train))\nprint('len(generator) ',len(generator))\n\nX,y = generator[0]\n\nprint(f'Given the Array: \\n{X.flatten()}')\nprint(f'Predict this y: \\n {y}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\n\nscaled_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\n\n# Simple RNN layer\nmodel.add(LSTM(100,input_shape=(length,scaled_train.shape[1])))\n\n# Final Prediction (one neuron per feature)\nmodel.add(Dense(scaled_train.shape[1]))\n\nmodel.compile(optimizer='adam', loss='mse')\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EarlyStopping"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss',patience=1)\nvalidation_generator = TimeseriesGenerator(scaled_test,scaled_test, \n                                           length=length, batch_size=batch_size)\n\nmodel.fit_generator(generator,epochs=10,\n                    validation_data=validation_generator,\n                   callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.history.history.keys()\n\nlosses = pd.DataFrame(model.history.history)\nlosses.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_eval_batch = scaled_train[-length:]\nfirst_eval_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_eval_batch = first_eval_batch.reshape((1, length, scaled_train.shape[1]))\nmodel.predict(first_eval_batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_test[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features = scaled_train.shape[1]\ntest_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inverse Transformations and Compare"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(test_predictions)\n\ntrue_predictions = pd.DataFrame(data=true_predictions,columns=test.columns)\ntrue_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"3.\"></a> \n# 3.Use Facebook's Prophet Library for forecasting"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data\n\nThe input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/for-simple-exercises-time-series-forecasting/Miles_Traveled.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['ds','y']\ndf['ds'] = pd.to_datetime(df['ds'])\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.plotting.register_matplotlib_converters()\n\ntry:\n    df.plot(x='ds',y='y',figsize=(18,6))\nexcept TypeError as e:\n    figure_or_exception = str(\"TypeError: \" + str(e))\nelse:\n    figure_or_exception = df.set_index('ds').y.plot().get_figure()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('len(df)',len(df))\nprint('len(df) - 12 = ',len(df) - 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.iloc[:576]\ntest = df.iloc[576:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create and Fit Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is fitting on all the data (no train test split in this example)\nm = Prophet()\nm.fit(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Forecasting\n\n**NOTE: Prophet by default is for daily data. You need to pass a frequency for sub-daily or monthly data. Info: https://facebook.github.io/prophet/docs/non-daily_data.html**"},{"metadata":{"trusted":true},"cell_type":"code","source":"future = m.make_future_dataframe(periods=12,freq='MS')\nforecast = m.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Forecast\n\nWe can use Prophet's own built in plotting tools"},{"metadata":{"trusted":true},"cell_type":"code","source":"m.plot(forecast);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nm.plot(forecast)\nplt.xlim(pd.to_datetime('2003-01-01'),pd.to_datetime('2007-01-01'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.plot_components(forecast);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tools.eval_measures import rmse\npredictions = forecast.iloc[-12:]['yhat']\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(predictions,test['y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prophet Diagnostics\n\nProphet includes functionality for time series cross validation to measure forecast error using historical data. This is done by selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point. We can then compare the forecasted values to the actual values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.diagnostics import cross_validation,performance_metrics\nfrom fbprophet.plot import plot_cross_validation_metric\n\nlen(df)\nlen(df)/12\n\n# Initial 5 years training period\ninitial = 5 * 365\ninitial = str(initial) + ' days'\n# Fold every 5 years\nperiod = 5 * 365\nperiod = str(period) + ' days'\n# Forecast 1 year into the future\nhorizon = 365\nhorizon = str(horizon) + ' days'\n\ndf_cv = cross_validation(m, initial=initial, period=period, horizon = horizon)\n\ndf_cv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_metrics(df_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cross_validation_metric(df_cv, metric='rmse');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cross_validation_metric(df_cv, metric='mape');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}