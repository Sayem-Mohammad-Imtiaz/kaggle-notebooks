{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Diabetes Prediction Using Machine Learning\n# importing the necessary libraries\nfrom mlxtend.plotting import plot_decision_regions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading the dataset\ndiabetes_data = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n\n#Print the first 5 rows of the dataframe.\ndiabetes_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## gives information about the data types,columns, null value counts, memory usage etc\n## function reference : https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\ndiabetes_data.info(verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DataFrame.describe()** method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include=\"all\" is passed.\n\nNow, let's understand the statistics that are generated by the describe() method:\n* count tells us the number of NoN-empty rows in a feature.\n* mean tells us the mean value of that feature.\n* std tells us the Standard Deviation Value of that feature.\n* min tells us the minimum value of that feature.\n* 25%, 50%, and 75% are the percentile/quartile of each features. This quartile information helps us to detect Outliers.\n* max tells us the maximum value of that feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## basic statistic details about the data (note only numerical columns would be displayed here unless parameter include=\"all\")\n## for reference: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html#pandas.DataFrame.describe\ndiabetes_data.describe()\n\n## Also see :\n##to return columns of a specific dtype: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data.describe().T  # creating the trsnspose of the description of the Dataframe and then showing it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Question creeping out of this summary\n\n#### Can minimum value of below listed columns be zero (0)?\n\nOn these columns, a value of zero does not make sense and thus indicates missing value.\n\nFollowing columns or variables have an invalid zero value:\n1. Glucose\n2. BloodPressure\n3. SkinThickness\n4. Insulin\n5. BMI","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is better to replace zeros with nan since after that counting them would be easier and zeros need to be replaced with suitable values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data_copy = diabetes_data.copy(deep = True) # creating the copy of the dataset\n# replacing the 0 values with Nan\ndiabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\n## showing the count of Nans\nprint(diabetes_data_copy.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### To fill these Nan values the data distribution needs to be understood","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p = diabetes_data_copy.hist(figsize = (20,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aiming to impute nan values for the columns in accordance with their distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data_copy['Glucose'].fillna(diabetes_data_copy['Glucose'].mean(), inplace = True)\ndiabetes_data_copy.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data_copy['BloodPressure'].fillna(diabetes_data_copy['BloodPressure'].mean(), inplace = True)\ndiabetes_data_copy.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data_copy['SkinThickness'].fillna(diabetes_data_copy['SkinThickness'].median(), inplace = True)\ndiabetes_data_copy.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data_copy['Insulin'].fillna(diabetes_data_copy['Insulin'].median(), inplace = True)\ndiabetes_data_copy.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data_copy['BMI'].fillna(diabetes_data_copy['BMI'].median(), inplace = True)\ndiabetes_data_copy.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we have imputated all the missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Plotting after Nan removal ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p = diabetes_data_copy.hist(figsize = (20,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Skewness\n\nA ***left-skewed distribution*** has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA ***right-skewed distribution*** has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n\n![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2014/02/pearson-mode-skewness.jpg)\n\n\n#### to learn more about skewness\nhttps://www.statisticshowto.datasciencecentral.com/probability-and-statistics/skewed-distribution/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## observing the shape of the data\ndiabetes_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## null count analysis\nimport missingno as msno\np=msno.bar(diabetes_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking the balance of the data by plotting the count of outcomes by their value\ncolor_wheel = {1: \"#0392cf\", \n               2: \"#7bc043\"}\ncolors = diabetes_data[\"Outcome\"].map(lambda x: color_wheel.get(x + 1))\nprint(diabetes_data.Outcome.value_counts())\np=diabetes_data.Outcome.value_counts().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above graph shows that the data is biased towards datapoints having outcome value as 0 where it means that diabetes was not present actually. The number of non-diabetics is almost twice the number of diabetic patients","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Scatter matrix of uncleaned data\n\n1.   List item\n2.   List item","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\np=scatter_matrix(diabetes_data,figsize=(25, 25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The pairs plot builds on two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows us to see the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables.\n\nFor Reference: https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Pair plot for clean data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p=sns.pairplot(diabetes_data_copy, hue = 'Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Pearson's Correlation Coefficient***: helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Heatmap for unclean data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_data.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Heatmap for clean data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_data_copy.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling the data \ndata Z is rescaled such that μ = 0 and 𝛔 = 1, and is done through this formula:\n![](https://cdn-images-1.medium.com/max/800/0*PXGPVYIxyI_IEHP7.)\n\n\n#### to learn more about scaling techniques\nhttps://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc\nhttps://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataframe before transformation\ndiabetes_data_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(diabetes_data_copy.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()  # looking at the transformed data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = diabetes_data.drop(\"Outcome\",axis = 1)\ny = diabetes_data_copy.Outcome  # assigning the label column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Train Split and Cross Validation methods\n\n\n\n***Train Test Split*** : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n\n![](https://cdn-images-1.medium.com/max/1600/1*-8_kogvwmL1H6ooN1A1tsQ.png)\n\n***Cross Validation***: When model is split into training and testing it can be possible that specific type of data point may go entirely into either training or testing portion. This would lead the model to perform poorly. Hence over-fitting and underfitting problems can be well avoided with cross validation techniques\n\n![](https://cdn-images-1.medium.com/max/1600/1*4G__SV580CxFj78o9yUXuQ.png)\n\n\n***About Stratify*** : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.\n\nFor Reference : https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_scores)\nprint(test_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The best result is captured at k = 11 hence 11 is used for the final model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(11)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value = 20000\nwidth = 20000\nplot_decision_regions(X.values, y.values, clf=knn, legend=2, \n                      filler_feature_values={2: value, 3: value, 4: value, 5: value, 6: value, 7: value},\n                      filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width, 7: width},\n                      X_highlight=X_test.values)\n\n# Adding axes annotations\nplt.title('KNN with Diabetes Data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Performance Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Confusion Matrix\n\nThe confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n![](https://cdn-images-1.medium.com/max/1600/0*-GAP6jhtJvt7Bqiv.png)\n\n\n\n### ***In the famous cancer example***:\n\n\n###### Cases in which the doctor predicted YES (they have the disease), and they do have the disease will be termed as TRUE POSITIVES (TP). The doctor has correctly predicted that the patient has the disease.\n\n###### Cases in which the doctor predicted NO (they do not have the disease), and they don’t have the disease will be termed as TRUE NEGATIVES (TN). The doctor has correctly predicted that the patient does not have the disease.\n\n###### Cases in which the doctor predicted YES, and they do not have the disease will be termed as FALSE POSITIVES (FP). Also known as “Type I error”.\n\n###### Cases in which the doctor predicted NO, and they have the disease will be termed as FALSE NEGATIVES (FN). Also known as “Type II error”.\n\n![](https://cdn-images-1.medium.com/max/1600/0*9r99oJ2PTRi4gYF_.jpg)\n\nFor Reference: https://medium.com/@djocz/confusion-matrix-aint-that-confusing-d29e18403327","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above. Creating the confusion Matrix\ny_pred = knn.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Heatmap for the confusion matrix. \ny_pred = knn.predict(X_test)\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}