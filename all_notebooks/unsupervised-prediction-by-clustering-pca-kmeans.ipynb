{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Unsupervised prediction by clustering (PCA+KMeans)"},{"metadata":{},"cell_type":"markdown","source":"The main goal of given code to answer the question - how accurate we can make mushrooms` classification using Clustering algorithm. Here we will use PCA for dimention reduction and KMeans for reduced matrix clustering. \nAs well during data preparation we will create new features as a combination\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_palette('husl')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn import metrics\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read data and define proportion of classes\n\nadd = \"../input/mushroom-classification/mushrooms.csv\"\ndata = pd.read_csv(add)\ny=data['class'].copy()\nprint('Proportion of e/p','\\n',\n      data['class'].value_counts())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Useful_function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode (data):\n    label=LabelEncoder()\n    for c in  data.columns:\n        if(data[c].dtype=='object'):\n            data[c]=label.fit_transform(data[c])\n        else:\n            data[c]=data[c]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# New Features creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating combination from features\n\ndata1=data.drop('class',axis=1)\n\na=pd.DataFrame()\ncolor_col=data1.columns\ncomb = combinations(color_col, 2) \ncomb_feat=[]\n# Print the obtained combinations \nfor i in list(comb): \n    comb_feat.append(i)\ncomb_feat=pd.DataFrame(comb_feat)\n\n\n\nfor i in range(0,len(comb_feat),1):\n    col1=comb_feat[0][i]\n    col2=comb_feat[1][i]\n    a[col1+col2]=data[col1]+data[col2]\n    \ndf_cor=pd.concat([a,y],axis=1)\nencode(df_cor)\ncorrmat= abs(df_cor.corr())\n\ncorrmat['class'].sort_values(ascending=False)[1:31]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_feat=pd.DataFrame(corrmat['class'].sort_values(ascending=False)[1:31]).reset_index()\nnew_feat.columns=['feat','corr']\n\nfor i in new_feat['feat']:\n    data1[i]=a[i]\n\ndata1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing data for model"},{"metadata":{"trusted":true},"cell_type":"code","source":"data1= pd.get_dummies(data1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode(data1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=pd.DataFrame(data['class'].copy())\n#data_x=data1.drop(['class'],axis=1)\nencode(y)\ny=y['class'].copy()\nsc = StandardScaler()\ndata_x_sc=pd.DataFrame(sc.fit_transform(data1),columns=data1.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Reduction with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define n_component to cover 90% of feature variation-110\ndef Reduction_compon(red_model,X,n_comp):\n    red_ = red_model(n_components = n_comp)\n    principalComponents = red_.fit_transform(X)\n\n    # Plot the explained variances\n    features = range(red_.n_components_)\n    fig=plt.figure()\n    \n    plt.bar(features, red_.explained_variance_ratio_, color='black')\n    plt.xlabel('PCA features')\n    plt.ylabel('variance %')\n    #plt.xticks(features)\n    plt.show()\n    # Save components to a DataFrame\n    Red_components = pd.DataFrame(principalComponents)\n    print('Cumsum_expl_var',np.cumsum(red_.explained_variance_ratio_))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Should be 90%\nReduction_compon(PCA,data_x_sc,110)\n#Reduction_compon(PCA,X_train,110)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at the PCA features scatter plot\n#Define n_clusters by Elbow_point\ndef red_model_scatter(red_model,comp,X,y):\n    red_ = red_model(n_components = comp)\n    principalComponents = red_.fit_transform(X)\n\n    plt.scatter(principalComponents[:, 0], principalComponents[:, 1], c=y, \n                edgecolor='none', alpha=0.7, s=40,\n                #cmap=plt.cm.get_cmap('nipy_spectral', 10)\n               )\n    fig=plt.figure()\n    plt.xlabel('PCA 1')\n    plt.ylabel('PCA 2')\n    print('Model,components',str(red_model),comp)\n    \n    reduc_matrix=pd.DataFrame(principalComponents)\n    \n    #Find Elbow point and cluster number\n    ks = range(1, 30)\n    inertias = []\n    for k in ks:\n        # Create a KMeans instance with k clusters: model\n        model = KMeans(n_clusters=k)\n\n        # Fit model to samples\n        model.fit(reduc_matrix.iloc[:,:3])\n\n        # Append the inertia to the list of inertias\n        inertias.append(model.inertia_)\n\n    plt.plot(ks, inertias, '-o', color='black')\n    plt.xlabel('number of clusters, k')\n    plt.ylabel('inertia')\n    plt.xticks(ks)\n    #plt.show()\n    print('Choose n_cluster by Elbow point')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red_model_scatter(PCA,110,data_x_sc,y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for dataset clusterization \ndef clusteriz(n_cl,red_model,comp,X):\n    \n    red_ = red_model(n_components = comp)\n    principalComponents = red_.fit_transform(X)\n    reduc_matrix=pd.DataFrame(principalComponents)\n    \n    kmeans = KMeans(n_clusters=n_cl)\n    kmeans.fit(reduc_matrix)\n    \n    y_kmeans = kmeans.predict(reduc_matrix)\n    X_clust=pd.DataFrame(y_kmeans)\n    X_clust.columns=['cluster']\n    \n    cluster_df = pd.DataFrame()\n\n    cluster_df['cluster'] = X_clust['cluster']\n    cluster_df['class'] = y\n    sns.factorplot(col='cluster', y=None, x='class', data=cluster_df, kind='count', order=[1,0], palette=([\"#7d069b\",\"#069b15\"]))\n    return X_clust\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_clust=clusteriz(4,PCA,110,data_x_sc)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# We have 4 clusters, lets define which cluster means eatable/poison mushrooms\ncluster_df = pd.DataFrame()\n\ncluster_df['cluster'] = X_clust['cluster']\ncluster_df['class'] = y\n\n\na=(pd.DataFrame(cluster_df.groupby(['cluster','class'])['class'].count()).unstack()).fillna(0)\na.iloc[:, a.columns.get_level_values('class')==0][:1]\n\ncluster_pred_to_biclust=[]\nclusters=[]\n\nfor i in range(0,len(a)):\n    class0_val=int(a.iloc[:, a.columns.get_level_values('class')==0][i:i+1].values)\n    class1_val=int(a.iloc[:, a.columns.get_level_values('class')==1][i:i+1].values)\n    \n    if class0_val>=class1_val:\n        \n        cluster_value=0\n    else:\n        cluster_value=1\n        \n    cluster_pred_to_biclust.append(cluster_value)\n    \ncluster_pred_to_biclust=pd.DataFrame(cluster_pred_to_biclust)\na=pd.concat([a,cluster_pred_to_biclust],axis=1)\nlist(a[0])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Lets recode our 4 predicted clusters to 2 values and Calculate Accuracy\nX_clust['clus_to_bi_pred']=0\n\nX_clust['clus_to_bi_pred'] = (\n    np.select(\n        condlist=[X_clust['cluster']==0,\n                  X_clust['cluster']==1,\n                  X_clust['cluster']==2,\n                  X_clust['cluster']==3],\n        choicelist=list(a[0]), \n        default='-'))\nX_clust['clus_to_bi_pred']=X_clust['clus_to_bi_pred'].astype(int)\naccuracy=pd.Series(metrics.accuracy_score((X_clust['clus_to_bi_pred']).values,y))\n\nprint('Clustering Accuracy',accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Obtained accuracy of Kmeans with PCA is around 80%. Each time we recalculate clustering we are getting different values (70-88%).\nClustering with PCA good to use for big dataset.\n\nAccuracy obtained with supervised algorightm (DecisionTrees,Boosting,SVM,LogisticRegression,etc.) close or equal to 1."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}