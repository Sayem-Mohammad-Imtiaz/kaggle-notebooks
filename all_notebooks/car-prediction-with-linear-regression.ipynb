{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom scipy.stats import pointbiserialr\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gather and Clean Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho = pd.read_csv(\"../input/vehicle-dataset-from-cardekho/CAR DETAILS FROM CAR DEKHO.csv\")\ndata_dekho.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho.tail() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see abowe, we have 8 columns. \n* We are lucky, we have no null or NaN values in our dataset. \n* We can start to analyze our dataset with the feature that is called <b>name</b>. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho[\"name\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* You can see different names for cars in the dataset. \n* But I don't want to use this feature. Because what I wanna do is that just predicting prices, but this shouldn't include their name. If I did this, I would have to be divide them. But I don't want.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho.drop(['name'], axis=1, inplace=True)\ndata_dekho.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Analyze transmission \n* Manual = 0, Automatic = 1 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho['transmission'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transmission = data_dekho['transmission'] \ntransmission_clean = [0 if i == \"Manual\"  else 1  for i in data_dekho['transmission']]\n\ntransmission_clean = np.array(transmission_clean)\ntransmission_clean.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho.drop(['transmission'], axis=1, inplace=True)\ndata_dekho['transmission'] = transmission_clean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Analyze owner","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho['owner'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"owner_unique_names = data_dekho['owner'].unique()\nowner_unique_names = pd.Series(index=owner_unique_names, data=[0,1,2,3,4])\ndict(owner_unique_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dekho['owner'].replace(dict(owner_unique_names), inplace=True)\ndata_dekho.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuel_unique_names = data_dekho['fuel'].unique()\nfuel_unique_names = pd.Series(index=fuel_unique_names, data=[0,1,2,3,4])\ndict(fuel_unique_names)\ndata_dekho['fuel'].replace(dict(fuel_unique_names), inplace=True)\ndata_dekho.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seller_type_unique_names = data_dekho['seller_type'].unique()\nseller_type_unique_names = pd.Series(index=seller_type_unique_names, data=[0,1,2])\ndict(seller_type_unique_names)\ndata_dekho['seller_type'].replace(dict(seller_type_unique_names), inplace=True)\ndata_dekho.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data_dekho.drop('selling_price', axis=1) \ndata['price'] = data_dekho['selling_price'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.count() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.isnull(data).any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now, we have a clean dataset with 6 explanatory variables and target variables that is called price. \n\n# Visualising Data - Histograms, Distributions and Bar Charts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.hist(data['price'], bins=50,ec='black', color='#2196f3') \nplt.xlabel('prices', fontsize=14)\nplt.ylabel('Nr of Prices', fontsize=14)\nplt.title(\"The Distribution of The Target Variable\", fontsize=14)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have a big problem about outliers. \n* If we leave this as above, this situation manipulates our stats. \n* So, we should get rid of this. \n\n* Coming cells, we will be looking for BOX PLOT and IQR.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['price'].skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"price\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"price\"].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['price'].mean() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Box Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\n\nsns.boxplot(x=data['price'])\nplt.xlabel(\"price\",fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = data['price'].quantile(0.25)\nQ3 = data['price'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = (data['price'] < (Q1 - 1.5 * IQR)) | (data['price'] > (Q3 + 1.5 * IQR))\nprint(res[res.values == True].count(), \"outliers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In this case, we have two way we can try \n* a. Data Transformation \n* b. Removing Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## a. Data Transformation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"log_prices = np.log(data['price'])\ndata_log_prices = data.drop(['price'], axis=1)\ndata_log_prices['price'] = log_prices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.hist(data_log_prices['price'], bins=50,ec='black', color='#2196f3') \nplt.xlabel('prices', fontsize=14)\nplt.ylabel('Nr of Prices', fontsize=14)\nplt.title(f\"The Distribution of The Target Variable skew:{str(round(data_log_prices['price'].skew(),3))}\", fontsize=14)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## b. Removing Outliers ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[12,:] # example of an outlier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_idx = res[res.values == True].index\ndata_rem_out = data.drop(index=out_idx)\ndata_rem_out.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.hist(data_rem_out['price'], bins=50,ec='black', color='#2196f3') \nplt.xlabel('prices', fontsize=14)\nplt.ylabel('Nr of Prices', fontsize=14)\nplt.title(f\"The Distribution of The Target Variable skew:{str(round(data_rem_out['price'].skew(),3))}\", fontsize=14)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have a lot more normal disribution with log prices, \n* We are going to use like this. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data_log_prices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nfreq = data['year'].value_counts()\nplt.bar(x=freq.index, height=freq.values)\nplt.xlabel('years', fontsize=14)\nplt.ylabel('Nr of Years', fontsize=14)\nplt.title(f\"The Distribution of The Year Variable\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['year'].min() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['year'].max() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['year'] < 2000, 'price'] .mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['year'] > 2000, 'price'] .mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.hist(data['km_driven'], bins=50,ec='black', color='#2196f3') \nplt.xlabel('prices', fontsize=14)\nplt.ylabel('Nr of Prices', fontsize=14)\nplt.title(\"The Distribution of The Target Variable\", fontsize=14)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['km_driven'].mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['km_driven'].min() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['km_driven'].max() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(x=fuel_unique_names.index, height=data['fuel'].value_counts())\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(data.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation \n\n* We care about two things, \n    a. Strength \n    b. Direction \n* Actually, We want the correlation that is not close to zero with target variable. \n* Also, the correlation among features shouldn't be too high, if it is, we can suspect \"MULTICOLLINEARRITY\"\n\n* For now, let's begin with <b>Correlation</b>\n\n## $$ \\rho _{XY} = corr(X,Y)$$\n## $$ -1.0 \\leq \\rho _{XY} \\leq +1.0 $$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr() # Pearson Correlation Coefficients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.zeros_like(data.corr())\ntriangle_indices = np.triu_indices_from(mask)\nmask[triangle_indices] = True\nmask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.heatmap(data.corr(), mask=mask, annot=True, annot_kws={\"size\": 14})\nsns.set_style('white')\nplt.xticks(fontsize=11)\nplt.yticks(fontsize=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We should take the note, now we are using Pearson Correlation.\n* Pearson's correlation coefficient is a measure of the strength of the association between the two variables but it works with countinuos variables.\n\n* So, you can see above, we have six features but all of them are not countinuos. \n* There are just two countinuos features \"year\" and \"km_driven\". So that we can analyze them with Pearson correlation, actually default in pandas corr function. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = round(data['price'].corr(data['km_driven']),3) \nsns.lmplot(x=\"km_driven\", y=\"price\", data=data, height=6, \n           line_kws={'color': 'cyan'}, scatter_kws={'color': 'purple', 'alpha': 0.7})\nplt.title(f'price vs km corr:{cor}', fontsize=14)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* km_driven feature has a low corr with target. \n* We have to fix this. \n* We can miss a explanatory variable or something alse, \n* let's have a look at distribution of the km_driven","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(data['km_driven'])\nplt.title(f\"The Histogram of the km_driven skew:{round(data['km_driven'].skew(),3)}\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['price'].corr(np.log(data['km_driven']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km_log = np.log(data['km_driven'])\ndata['km_driven'] = km_log\ndata.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = round(data['price'].corr(data['km_driven']),3) \nsns.lmplot(x=\"km_driven\", y=\"price\", data=data, height=6, \n           line_kws={'color': 'cyan'}, scatter_kws={'color': 'purple', 'alpha': 0.7})\nplt.title(f'price vs km corr:{cor}', fontsize=14)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6), dpi=300)\nplt.scatter(data['year'], data['km_driven'], color='indigo', s=80, alpha=0.7)\nplt.title(f\"Year vs Km_Driven Corr: {round(data['year'].corr(data['km_driven']),3)}\")\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Km_Driven', fontsize=14)          \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nsns.pairplot(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training & Test Dataset Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"log_target = data['price']\nfeatures = data.drop(['price'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, log_target, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariable Regression ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model using log price and log km_driven \nregr = LinearRegression() \nmodel_log_price_km = regr.fit(X_train, y_train)\n\nlog_price_log_km = regr.score(X_train, y_train)\n\n\nprint('Intercept is', round(regr.intercept_,3))\nprint('R-squared for training set is', regr.score(X_train, y_train))\nprint('R-squared for testing set is', regr.score(X_test, y_test))\n\npd.DataFrame(regr.coef_, columns=['coef'], index=features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = np.e**data['price']\nfeatures = data.drop(['price'], axis=1)\nfeatures['km_driven'] = np.e**data['km_driven']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model using normal price and normal km_driven \nregr = LinearRegression() \nregr.fit(X_train, y_train)\n\nnorm_price_norm_km = regr.score(X_train, y_train)\n\nprint('Intercept is', round(regr.intercept_,3))\nprint('R-squared for training set is', regr.score(X_train, y_train))\nprint('R-squared for testing set is', regr.score(X_test, y_test))\n\npd.DataFrame(regr.coef_, columns=['coef'], index=features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model using log price and norm km_driven \n\ntarget = data['price']\nfeatures = data.drop(['price'], axis=1)\nfeatures['km_driven'] = np.e**data['km_driven']\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)\n\n\nregr = LinearRegression() \nregr.fit(X_train, y_train)\n\nlog_price_norm_km = regr.score(X_train, y_train)\n\nprint('Intercept is', round(regr.intercept_,3))\nprint('R-squared for training set is', regr.score(X_train, y_train))\nprint('R-squared for testing set is', regr.score(X_test, y_test))\n\npd.DataFrame(regr.coef_, columns=['coef'], index=features.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation \n\n  ### a. R-Squared","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = np.asanyarray([log_price_log_km, log_price_norm_km, norm_price_norm_km])\n\npd.DataFrame(arr, columns=['R-Squared'], index=['LOG PRICE AND LOG KM', 'LOG PRICE AND NORMAL KM', 'NORMAL PRICE AND NORMAL KM'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## b. P-Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_incl_const = sm.add_constant(X_train)\nmodel = sm.OLS(y_train, X_incl_const) \nresults = model.fit() \nround(results.pvalues, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_km_x_train = X_train\nlog_km_x_train['km_driven'] = np.log(X_train['km_driven'])\nX_incl_const_log_km = sm.add_constant(log_km_x_train)\nmodel = sm.OLS(y_train, X_incl_const_log_km) \nresults = model.fit() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(results.pvalues, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* You can see abowe, km_driven has a p_value that is equal to 0.049. Because of this, It doesn't look significance statistically. \n* We tried data transformation for it. We used log function, but in this case it increased.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# c. Multicollinearity\n\n* Actually, We didn't suspect multicollinearity because of our correlatin table. \n* If two or more variable were higly related to one another, they wouln't provide unique or independent information for our model. \n* But we're gonna look at it with VIF. \n* VIF(Variation Inflation Factor). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"variance_inflation_factor(exog=np.asanyarray(X_incl_const_log_km), exog_idx=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vifs = [variance_inflation_factor(exog=np.asanyarray(X_incl_const_log_km), exog_idx=i) \n        for i in range(len(X_incl_const.columns))]\npd.DataFrame(np.asanyarray(vifs).reshape(1,7),  columns=X_incl_const.columns, index=['VIF'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# d. Model Simplification & the BIC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model using log price and norm km_driven \n\nX_incl_const = sm.add_constant(X_train)\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit() \n\nprint(\"R-squared is\", results.rsquared)\nprint(\"BIC is\", results.bic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model using log price without km_driven \n\nX_incl_const = sm.add_constant(X_train.drop(['km_driven'], axis=1))\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit() \n\nprint(\"R-squared is\", results.rsquared)\nprint(\"BIC is\", results.bic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Residual and Residual Plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We'll analyze the residual vs predicted values and the dist of the residual. The residual vs predicted shouldn't have any pattern. If you catch any pattern in scatter plot, may be you are missig an explanotory variable, or something else is wrong for your model. \n\n* And also for a good linear regression model, you should have normally distributed residual. It's important. Because you can see how it is working your model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted log prices vs Actual Log prices \n\nregr = LinearRegression().fit(X_train, y_train) \n\npredicted_values = pd.Series(regr.predict(X_train))\ncorr = np.round(y_train.corr(predicted_values), 3)\n\nplt.figure(figsize=(10,6))\n\nplt.scatter(x=predicted_values, y=y_train)\nplt.plot(y_train, y_train, c='red')\nplt.title(f\"Predicted log prices vs Actual Log prices {corr}\", fontsize=14)\nplt.xlabel('Predicted Price',fontsize=14)\nplt.ylabel('Actual Price', fontsize=14) \n\n\n# residual vs predicted values \nplt.figure(figsize=(10,6))\ny = np.asanyarray(y_train)\ny_hat = np.asanyarray(predicted_values)\nresi = y - y_hat\n\nplt.scatter(x=predicted_values, y=resi, c=\"skyblue\",alpha=0.7)\nplt.xlabel('Residual', fontsize=14)\nplt.ylabel('Predicted Values', fontsize=14)\nplt.title(\"Residual vs Predicted Values\", fontsize=14)\n\nplt.figure(figsize=(10,6))\nsns.distplot(resi)\nplt.title(f'The Distribution of the Residuals Skew:{round(pd.Series(resi).skew(), 2)}', fontsize=14)\n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"R-squared is\", regr.score(X_train, y_train))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}