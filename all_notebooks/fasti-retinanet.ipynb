{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fastai v1 : Object detection Tutorial","metadata":{"papermill":{"duration":0.012908,"end_time":"2020-08-19T09:49:40.972005","exception":false,"start_time":"2020-08-19T09:49:40.959097","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install fastai -q --upgrade\nfrom fastai.vision.all import *","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:01:56.156039Z","iopub.execute_input":"2021-06-20T15:01:56.156523Z","iopub.status.idle":"2021-06-20T15:03:26.110311Z","shell.execute_reply.started":"2021-06-20T15:01:56.156424Z","shell.execute_reply":"2021-06-20T15:03:26.109148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import defaultdict\nimport os\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom sklearn.model_selection import StratifiedKFold,KFold\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":4.073244,"end_time":"2020-08-19T09:49:55.819361","exception":false,"start_time":"2020-08-19T09:49:51.746117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:26.111967Z","iopub.execute_input":"2021-06-20T15:03:26.112261Z","iopub.status.idle":"2021-06-20T15:03:26.183157Z","shell.execute_reply.started":"2021-06-20T15:03:26.112232Z","shell.execute_reply":"2021-06-20T15:03:26.182189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('darkgrid')","metadata":{"papermill":{"duration":0.031794,"end_time":"2020-08-19T09:49:55.87059","exception":false,"start_time":"2020-08-19T09:49:55.838796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:26.185128Z","iopub.execute_input":"2021-06-20T15:03:26.185459Z","iopub.status.idle":"2021-06-20T15:03:26.190072Z","shell.execute_reply.started":"2021-06-20T15:03:26.185428Z","shell.execute_reply":"2021-06-20T15:03:26.189043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('/kaggle/input/dsta-brainhack-2021/c1_release/c1_release');path.ls()\n\n# Other dataset\n# path_pascal = untar_data(URLs.PASCAL_2007)","metadata":{"papermill":{"duration":0.031732,"end_time":"2020-08-19T09:49:55.919701","exception":false,"start_time":"2020-08-19T09:49:55.887969","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:26.191797Z","iopub.execute_input":"2021-06-20T15:03:26.192191Z","iopub.status.idle":"2021-06-20T15:03:26.220532Z","shell.execute_reply.started":"2021-06-20T15:03:26.192163Z","shell.execute_reply":"2021-06-20T15:03:26.219479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs, lbl_bbox = get_annotations(path/'train.json')\n\n# testing out the model\n# from IPython.display import Image\n# Image(\"../input/dsta-brainhack-2021/c1_release/c1_release/images\"+\"/\"+imgs[0])\n\nlbl_bbox[0],imgs[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:26.22205Z","iopub.execute_input":"2021-06-20T15:03:26.222372Z","iopub.status.idle":"2021-06-20T15:03:26.30555Z","shell.execute_reply.started":"2021-06-20T15:03:26.222342Z","shell.execute_reply":"2021-06-20T15:03:26.304431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import Image\n# Image(\"./input/dsta-brainhack-2021/c1_release/c1_release/images/3e15222c5563afcb.jpg\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:26.306895Z","iopub.execute_input":"2021-06-20T15:03:26.307291Z","iopub.status.idle":"2021-06-20T15:03:26.311241Z","shell.execute_reply.started":"2021-06-20T15:03:26.30725Z","shell.execute_reply":"2021-06-20T15:03:26.310083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img2bbox = dict(zip(imgs, lbl_bbox))","metadata":{"papermill":{"duration":0.043879,"end_time":"2020-08-19T09:49:56.267135","exception":false,"start_time":"2020-08-19T09:49:56.223256","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:26.312685Z","iopub.execute_input":"2021-06-20T15:03:26.313117Z","iopub.status.idle":"2021-06-20T15:03:26.323094Z","shell.execute_reply.started":"2021-06-20T15:03:26.313077Z","shell.execute_reply":"2021-06-20T15:03:26.322414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the first item\nfirst = {k: img2bbox[k] for k in list(img2bbox)[:1]}; first","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:26.324Z","iopub.execute_input":"2021-06-20T15:03:26.324265Z","iopub.status.idle":"2021-06-20T15:03:26.338464Z","shell.execute_reply.started":"2021-06-20T15:03:26.324239Z","shell.execute_reply":"2021-06-20T15:03:26.337421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getters = [lambda o: path/'images'/o, lambda o: img2bbox[o][0], lambda o: img2bbox[o][1]]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:26.342616Z","iopub.execute_input":"2021-06-20T15:03:26.342928Z","iopub.status.idle":"2021-06-20T15:03:26.349335Z","shell.execute_reply.started":"2021-06-20T15:03:26.3429Z","shell.execute_reply":"2021-06-20T15:03:26.348399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformation","metadata":{}},{"cell_type":"code","source":"item_tfms = [Resize(128, method='pad'),]\nbatch_tfms = [Rotate(), Flip(), Dihedral(), Normalize.from_stats(*imagenet_stats)]\n# aug_tfms = [RandomFlip(tfm_y=TfmType.COORD),\n#         RandomRotate(30, tfm_y=TfmType.COORD),\n#         RandomLighting(0.1,0.1, tfm_y=TfmType.COORD)]\n\ndef get_train_imgs(noop):  return imgs\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:26.351761Z","iopub.execute_input":"2021-06-20T15:03:26.352284Z","iopub.status.idle":"2021-06-20T15:03:26.361469Z","shell.execute_reply.started":"2021-06-20T15:03:26.352242Z","shell.execute_reply":"2021-06-20T15:03:26.360767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building Datablock","metadata":{}},{"cell_type":"code","source":"pascal = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n                 splitter=RandomSplitter(),\n                 get_items=get_train_imgs,\n                 getters=getters,\n                 item_tfms=item_tfms,\n                 batch_tfms=batch_tfms,\n                 n_inp=1)\n\ndls = pascal.dataloaders(path/'images')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:26.362537Z","iopub.execute_input":"2021-06-20T15:03:26.362967Z","iopub.status.idle":"2021-06-20T15:03:27.01179Z","shell.execute_reply.started":"2021-06-20T15:03:26.362929Z","shell.execute_reply":"2021-06-20T15:03:27.010869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.c = 6\ndls.show_batch(max_n=4,nrows=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:27.013145Z","iopub.execute_input":"2021-06-20T15:03:27.013455Z","iopub.status.idle":"2021-06-20T15:03:31.438711Z","shell.execute_reply.started":"2021-06-20T15:03:27.013425Z","shell.execute_reply":"2021-06-20T15:03:31.437702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# previous code\n\n# train = pd.read_csv(path/'train.csv')\n# tr = train.image_id.value_counts()\n# tr = pd.DataFrame({'image_id':tr.index,'wheat_count':tr.values})\n# tr = tr.sample(frac=1.,random_state=2020).reset_index(drop=True)\n# ","metadata":{"papermill":{"duration":0.091639,"end_time":"2020-08-19T09:49:56.377934","exception":false,"start_time":"2020-08-19T09:49:56.286295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:31.44001Z","iopub.execute_input":"2021-06-20T15:03:31.440308Z","iopub.status.idle":"2021-06-20T15:03:31.443811Z","shell.execute_reply.started":"2021-06-20T15:03:31.440279Z","shell.execute_reply":"2021-06-20T15:03:31.442852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Model","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ExtremelySunnyYK/Practical-Deep-Learning-for-Coders-2.0.git\noutput_cv_path = \"Practical-Deep-Learning-for-Coders-2.0/Computer Vision\"\nos.chdir(f\"/kaggle/working/{output_cv_path}\")\nfrom imports import *\nfrom object_detection_metrics.BoundingBox import BoundingBox, BBType, BBFormat\nfrom object_detection_metrics.BoundingBoxes import BoundingBoxes\nfrom object_detection_metrics.Evaluator import Evaluator\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:31.445162Z","iopub.execute_input":"2021-06-20T15:03:31.445461Z","iopub.status.idle":"2021-06-20T15:03:36.572721Z","shell.execute_reply.started":"2021-06-20T15:03:31.445434Z","shell.execute_reply":"2021-06-20T15:03:36.569258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_metrics_path = Path(f\"/kaggle/working/{output_cv_path}\")\nobject_metrics_path.ls()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.573965Z","iopub.status.idle":"2021-06-20T15:03:36.574394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = create_body(resnet50, pretrained=True)\nNUM_CLASSES = get_c(dls)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.575164Z","iopub.status.idle":"2021-06-20T15:03:36.575548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Architecture","metadata":{}},{"cell_type":"code","source":"# arch = RetinaNet(encoder, get_c(dls), final_bias=-4)\n\n# from torchvision.models.detection import maskrcnn_resnet50_fpn\n# learn = mask_rcnn_learner(dls, maskrcnn_resnet50_fpn, 2)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.576437Z","iopub.status.idle":"2021-06-20T15:03:36.57682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratios = [1/2,1,2]\nscales = [1,2**(-1/3), 2**(-2/3)]\ncrit = RetinaNetFocalLoss(arch, scales=scales, ratios=ratios)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.577521Z","iopub.status.idle":"2021-06-20T15:03:36.577921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _retinanet_split(m): return L(m.encoder,nn.Sequential(m.c5top6, m.p6top7, m.merges, m.smoothers, m.classifier, m.box_regressor)).map(params)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.578601Z","iopub.status.idle":"2021-06-20T15:03:36.579022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pdb\nimport torchvision\n\nclass ThresholdingAndNMS(Callback):\n    def __init__(self, threshold=0.3):\n        self.threshold=threshold\n    def after_loss(self):\n        if self.training: return # only do this expensive computation during validation/show_results\n        box_pred, cls_pred = self.learn.pred\n        scores = torch.sigmoid(cls_pred)\n        anchors = self.learn.loss_func.anchors\n        recovered_boxes = torch.clamp(cthw2tlbr(activ_to_bbox(box_pred, anchors).view(-1,4)).view(*box_pred.shape), min=-1, max=1)\n        cls_clean, box_clean = [],[]\n        \n        one_batch_boxes = []\n        one_batch_scores = []\n        one_batch_cls_pred = [] \n        for i in range(cls_pred.shape[0]):\n            cur_box_pred = recovered_boxes[i]\n            cur_scores = scores[i]\n            max_scores, cls_idx = torch.max(cur_scores, dim=1)\n            thresh_mask = max_scores > self.threshold\n            \n            cur_keep_boxes = cur_box_pred[thresh_mask]\n            cur_keep_scores = cur_scores[thresh_mask]\n            cur_keep_cls_idx = cls_idx[thresh_mask]\n            \n            one_img_boxes = []\n            one_img_scores = []\n            one_img_cls_pred = []\n            for c in range(NUM_CLASSES):\n                cls_mask   = cur_keep_cls_idx==c\n                if cls_mask.sum()==0:\n                    continue\n                cls_boxes  = cur_keep_boxes[cls_mask]\n                cls_scores = cur_keep_scores[cls_mask].max(dim=1)[0]\n                nms_keep_idx = torchvision.ops.nms(cls_boxes,cls_scores, iou_threshold=0.5)\n                one_img_boxes += [*cls_boxes[nms_keep_idx]]\n                one_img_scores += [*cur_keep_scores[nms_keep_idx]]\n                one_img_cls_pred += [*tensor([c]*len(nms_keep_idx))]\n                \n            one_batch_boxes.append(one_img_boxes)\n            one_batch_scores.append(one_img_scores)\n            one_batch_cls_pred.append(one_img_cls_pred)\n        \n        \n        \n        #padded_boxes, padded_cls_pred = pad_and_merge(one_batch_boxes, one_batch_cls_pred)\n        #print(f\"padded_boxes: {padded_boxes.shape} - padded_cls_pred: {padded_cls_pred.shape}\")\n        #self.learn.pred = to_device((padded_boxes, padded_cls_pred), cls_pred.device)\n        padded_boxes, padded_scores = pad_and_merge_scores(one_batch_boxes, one_batch_scores)\n        #print(f\"padded_boxes: {padded_boxes.shape} - padded_scores: {padded_scores.shape}\")\n        self.learn.pred = to_device((padded_boxes, padded_scores), cls_pred.device)\n        \n\ndef pad_and_merge_scores(boxes_batch, scores_batch):\n    max_n_boxes = max([len(boxes_img) for boxes_img in boxes_batch])\n    \n    padded_boxes = torch.zeros(len(boxes_batch), max_n_boxes, 4).float()\n    padded_scores = torch.zeros(len(boxes_batch), max_n_boxes, NUM_CLASSES).float()\n    padded_scores[:,:] = 10 # set all to 10, if its a padded box, this is very ugly, the metric will remove \n    # these rows\n    \n    for i, (boxes_img, scores_img) in enumerate(zip(boxes_batch, scores_batch)):\n        for j, (box, score) in enumerate(zip(boxes_img, scores_img)):\n            padded_boxes[i,j] = box\n            padded_scores[i,j] = score\n    return (TensorBBox(padded_boxes), TensorMultiCategory(padded_scores))\n\ndef tlbr2xyxy(box, img_size=(224,224)):\n    h,w = img_size  # ????\n    # assume shape = (4)\n    # converting from pytorch -1 to 1 -> 0 to 1\n    #print(f\"box shape: {box.shape}\")\n    box = box.squeeze()\n    box = (box + 1) / 2\n    x1 = int(box[0]*w)\n    x2 = int(box[2]*w)\n    y1 = int(box[1]*h)\n    y2 = int(box[3]*h)\n    return [x1,y1,x2,y2]\n\n\n\nclass mAP(Metric):\n    def __init__(self):\n        self.boxes = BoundingBoxes()\n        self.count = 0\n        self.res = None\n    \n    def reset(self):\n        self.boxes.removeAllBoundingBoxes()\n        self.count = 0\n    \n    def accumulate(self, learn):\n        # add predictions and ground truths\n        #pdb.set_trace()\n        pred_boxes, pred_scores = learn.pred\n        # remove padded boxes in batch\n        pred_cls = pred_scores.argmax(dim=-1)\n        gt_boxes, gt_cls = learn.yb\n        #pdb.set_trace()\n        for img_box_pred, img_score_pred, img_box_gt, img_cls_gt in zip(pred_boxes, pred_scores, gt_boxes, gt_cls): \n            \n            pred_nonzero_idxs = (img_score_pred.sum(dim=-1) < 5).float().nonzero()\n            #pdb.set_trace()\n            if not pred_nonzero_idxs.numel() == 0:\n                img_cls_pred = img_score_pred[pred_nonzero_idxs].argmax(dim=-1)\n                #pdb.set_trace()\n                #add predictions for this img\n                for box_pred, cls_pred, score_pred in zip(img_box_pred[pred_nonzero_idxs], img_cls_pred, img_score_pred[pred_nonzero_idxs]):\n                    b = BoundingBox(self.count, learn.dls.vocab[cls_pred.item()+1], *tlbr2xyxy(box_pred), \n                                bbType=BBType.Detected, format=BBFormat.XYX2Y2, classConfidence=score_pred.squeeze()[cls_pred.item()])\n                    self.boxes.addBoundingBox(b)\n                    #print(f\"adding detection {learn.dls.vocab[cls_pred.item()]}\")\n             #       pdb.set_trace()\n            \n            gt_nonzero_idxs   = img_cls_gt.nonzero()#.squeeze()\n            for box_gt, cls_gt in zip(img_box_gt[gt_nonzero_idxs], img_cls_gt[gt_nonzero_idxs]):\n                b = BoundingBox(self.count, learn.dls.vocab[cls_gt.item()], *tlbr2xyxy(box_gt), \n                            bbType=BBType.GroundTruth, format=BBFormat.XYX2Y2)\n                self.boxes.addBoundingBox(b)\n                #print(f\"adding gt {learn.dls.vocab[cls_gt.item()]}\")\n          #      pdb.set_trace()\n            # increment counter\n            self.count += 1\n    \n    @property\n    def value(self):\n        if len(self.boxes.getBoundingBoxes()) == 0:\n            return 0\n        self.res = Evaluator().GetPascalVOCMetrics(self.boxes)\n        return np.mean([cat[\"AP\"] for cat in self.res])\n    \n    @property\n    def name(self):\n        return \"mAP\"","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.579779Z","iopub.status.idle":"2021-06-20T15:03:36.58022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LookUpMetric(Metric):\n    def __init__(self, reference_metric, metric_name, lookup_idx):\n        store_attr(self, \"reference_metric,metric_name,lookup_idx\")\n    \n    def reset(self):\n        pass\n    def accumulate(self, learn):\n        pass\n    \n    @property\n    def value(self):\n        if self.reference_metric.res is None:\n            _ = self.reference_metric.value\n        return self.reference_metric.res[self.lookup_idx][\"AP\"]\n    \n    @property\n    def name(self):\n        return self.metric_name + \"AP\"","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.581129Z","iopub.status.idle":"2021-06-20T15:03:36.581522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_metric = mAP()\nmetrics = [map_metric]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.582262Z","iopub.status.idle":"2021-06-20T15:03:36.582661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n!pip install fastai==2.0.15\n!pip install fastai2==0.0.30\n!pip install fastcore==1.0.16","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.58342Z","iopub.status.idle":"2021-06-20T15:03:36.583813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor lookup_idx in range(NUM_CLASSES):\n    metrics.append(LookUpMetric(map_metric, dls.vocab[lookup_idx+1], lookup_idx))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.584778Z","iopub.status.idle":"2021-06-20T15:03:36.585233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, arch, loss_func=crit, splitter=_retinanet_split, \n                cbs=[ThresholdingAndNMS()], metrics=metrics)\nlearn.to_fp16()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.585931Z","iopub.status.idle":"2021-06-20T15:03:36.586332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.freeze()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.587292Z","iopub.status.idle":"2021-06-20T15:03:36.587696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(1, lr_max=slice(5e-5,5e-4))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.588634Z","iopub.status.idle":"2021-06-20T15:03:36.589082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.save(\"freeze-1\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.589949Z","iopub.status.idle":"2021-06-20T15:03:36.590365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.unfreeze()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.591225Z","iopub.status.idle":"2021-06-20T15:03:36.591637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(15, lr_max=slice(1e-5, 1e-4))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.592418Z","iopub.status.idle":"2021-06-20T15:03:36.592813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create_head(124, 6)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.593723Z","iopub.status.idle":"2021-06-20T15:03:36.594142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have one with a smoother, a classifer, and a box_regressor (to get our points)\n\n","metadata":{}},{"cell_type":"markdown","source":"### Loss Function","metadata":{}},{"cell_type":"code","source":"ratios = [1/2,1,2]\nscales = [1,2**(-1/3), 2**(-2/3)]\ncrit = RetinaNetFocalLoss(arch, scales=scales, ratios=ratios)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.594945Z","iopub.status.idle":"2021-06-20T15:03:36.59535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learner","metadata":{}},{"cell_type":"code","source":"def _retinanet_split(m): return L(m.encoder,nn.Sequential(m.c5top6, m.p6top7, m.merges, m.smoothers, m.classifier, m.box_regressor)).map(params)\n\n\nlearn = Learner(dls, arch, loss_func=crit, splitter=_retinanet_split)\nlearn.freeze()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.596208Z","iopub.status.idle":"2021-06-20T15:03:36.596603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"os.chdir(\"/kaggle\")\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.597433Z","iopub.status.idle":"2021-06-20T15:03:36.597832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dls = pascal.dataloaders(path/'images')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.598746Z","iopub.status.idle":"2021-06-20T15:03:36.599206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper params\nn_epochs = 10\nlr_max =  slice(1e-5, 1e-4)\n\n# learn.fine_tune(4)\n\n# learn.fit_one_cycle(n_epochs, lr_max)\nlearn.fit(n_epochs,lr_max)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.600039Z","iopub.status.idle":"2021-06-20T15:03:36.600425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.601235Z","iopub.status.idle":"2021-06-20T15:03:36.601638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exporting Model","metadata":{}},{"cell_type":"code","source":"learn.export()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:36.602446Z","iopub.status.idle":"2021-06-20T15:03:36.602845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Is an Anchor Box?\n* **Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect** and are typically chosen based on object sizes in your training datasets. During detection, the predefined anchor boxes are tiled across the image. The network predicts the probability and other attributes, such as background, intersection over union (IoU) and offsets for every tiled anchor box. The predictions are used to refine each individual anchor box. You can define several anchor boxes, each for a different object size. Anchor boxes are fixed initial boundary box guesses.\n\n* **The network does not directly predict bounding boxes, but rather predicts the probabilities and refinements that correspond to the tiled anchor boxes.** The network returns a unique set of predictions for every anchor box defined. The final feature map represents object detections for each class. The use of anchor boxes enables a network to detect multiple objects, objects of different scales, and overlapping objects.","metadata":{"papermill":{"duration":0.019517,"end_time":"2020-08-19T09:50:07.573707","exception":false,"start_time":"2020-08-19T09:50:07.55419","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Advantage of Using Anchor Boxes\n* When using anchor boxes, you can evaluate all object predictions at once. Anchor boxes eliminate the need to scan an image with a sliding window that computes a separate prediction at every potential position. ","metadata":{"papermill":{"duration":0.01975,"end_time":"2020-08-19T09:50:07.613352","exception":false,"start_time":"2020-08-19T09:50:07.593602","status":"completed"},"tags":[]}},{"cell_type":"code","source":"anchors = create_anchors(sizes=[(32,32),(16,16),(8,8),(4,4)], ratios=[0.5, 1, 2], scales=[0.35, 0.55, 0.75, 1, 1.25, 1.45])","metadata":{"papermill":{"duration":0.031436,"end_time":"2020-08-19T09:50:07.664809","exception":false,"start_time":"2020-08-19T09:50:07.633373","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.603773Z","iopub.status.idle":"2021-06-20T15:03:36.604205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,10))\nax.imshow(image2np(data.valid_ds[0][0].data))\n\nfor i, bbox in enumerate(anchors[:18]):\n    bb = bbox.numpy()\n    x = (bb[0] + 1) * size / 2 \n    y = (bb[1] + 1) * size / 2 \n    w = bb[2] * size / 2\n    h = bb[3] * size / 2\n    \n    rect = [x,y,w,h]\n    draw_rect(ax,rect)","metadata":{"papermill":{"duration":0.760427,"end_time":"2020-08-19T09:50:08.445497","exception":false,"start_time":"2020-08-19T09:50:07.68507","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.60501Z","iopub.status.idle":"2021-06-20T15:03:36.605435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(anchors)","metadata":{"papermill":{"duration":0.060205,"end_time":"2020-08-19T09:50:08.550054","exception":false,"start_time":"2020-08-19T09:50:08.489849","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.606283Z","iopub.status.idle":"2021-06-20T15:03:36.60669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{"papermill":{"duration":0.043092,"end_time":"2020-08-19T09:50:08.634926","exception":false,"start_time":"2020-08-19T09:50:08.591834","status":"completed"},"tags":[]}},{"cell_type":"code","source":"n_classes = data.train_ds.c\n\ncrit = RetinaNetFocalLoss(anchors)\n\nencoder = create_body(models.resnet18, True, -2)\n\nmodel = RetinaNet(encoder, n_classes=data.train_ds.c, n_anchors=18, sizes=[32,16,8,4], chs=32, final_bias = -4., n_conv = 2)","metadata":{"papermill":{"duration":2.300352,"end_time":"2020-08-19T09:50:10.9795","exception":false,"start_time":"2020-08-19T09:50:08.679148","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.607669Z","iopub.status.idle":"2021-06-20T15:03:36.60811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **n_anchors = len(ratios) x len(scales)**","metadata":{"papermill":{"duration":0.026902,"end_time":"2020-08-19T09:50:11.033657","exception":false,"start_time":"2020-08-19T09:50:11.006755","status":"completed"},"tags":[]}},{"cell_type":"code","source":"voc = PascalVOCMetric(anchors, size, [i for i in data.train_ds.y.classes[1:]])\nlearn = Learner(data,\n                model, \n                loss_func=crit,\n                callback_fns=[BBMetrics],\n                metrics=[voc],\n                model_dir = '/kaggle/working/')","metadata":{"papermill":{"duration":0.097851,"end_time":"2020-08-19T09:50:11.158107","exception":false,"start_time":"2020-08-19T09:50:11.060256","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.608972Z","iopub.status.idle":"2021-06-20T15:03:36.609375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.split([model.encoder[6], model.c5top5]);\nlearn.freeze_to(-2)\n#learn = learn.to_fp16()","metadata":{"papermill":{"duration":0.044328,"end_time":"2020-08-19T09:50:11.228953","exception":false,"start_time":"2020-08-19T09:50:11.184625","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.610175Z","iopub.status.idle":"2021-06-20T15:03:36.610571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#learn.lr_find()\n#learn.recorder.plot()","metadata":{"papermill":{"duration":0.038473,"end_time":"2020-08-19T09:50:11.293738","exception":false,"start_time":"2020-08-19T09:50:11.255265","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.611324Z","iopub.status.idle":"2021-06-20T15:03:36.611719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.166754,"end_time":"2020-08-19T09:50:11.49164","exception":false,"start_time":"2020-08-19T09:50:11.324886","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.612809Z","iopub.status.idle":"2021-06-20T15:03:36.613246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#learn.unfreeze()\nlearn.fit_one_cycle(4, 1e-3 ,callbacks = [SaveModelCallback(learn, every ='improvement', monitor ='AP-wheat', name ='best_wheat')])","metadata":{"papermill":{"duration":991.371938,"end_time":"2020-08-19T10:06:42.892346","exception":false,"start_time":"2020-08-19T09:50:11.520408","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.614261Z","iopub.status.idle":"2021-06-20T15:03:36.614701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.load('best_wheat');\nlearn.export('/kaggle/working/gwheat.pkl')","metadata":{"papermill":{"duration":4.581113,"end_time":"2020-08-19T10:06:47.686124","exception":false,"start_time":"2020-08-19T10:06:43.105011","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.615582Z","iopub.status.idle":"2021-06-20T15:03:36.615999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.recorder.plot_losses()","metadata":{"papermill":{"duration":0.543491,"end_time":"2020-08-19T10:06:48.461056","exception":false,"start_time":"2020-08-19T10:06:47.917565","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.616845Z","iopub.status.idle":"2021-06-20T15:03:36.617261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_results_side_by_side(learn, anchors, detect_thresh=0.5, nms_thresh=0.1, image_count=5)","metadata":{"papermill":{"duration":2.612162,"end_time":"2020-08-19T10:06:51.421466","exception":false,"start_time":"2020-08-19T10:06:48.809304","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.61806Z","iopub.status.idle":"2021-06-20T15:03:36.618454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## If you reached till here please don't forget to upvote.","metadata":{"papermill":{"duration":0.225628,"end_time":"2020-08-19T10:06:51.873124","exception":false,"start_time":"2020-08-19T10:06:51.647496","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Prediction Test images helpful functions","metadata":{"papermill":{"duration":0.223093,"end_time":"2020-08-19T10:06:52.354858","exception":false,"start_time":"2020-08-19T10:06:52.131765","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def show_output(item,bboxs_tot,scores_tot):\n    fig,ax = plt.subplots(figsize=(10,10))\n    ax.imshow(image2np(item.data))\n    plt.axis('off')\n    area_max = 512**2/5 \n    for bbox, c in zip(bboxs_tot[0], scores_tot[0].numpy()):\n        txt = 'wheat, {0:.4f}'.format(c)\n        if bbox[2]*bbox[3] <= area_max:\n            draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=txt,text_size=12,color='red')","metadata":{"papermill":{"duration":0.241309,"end_time":"2020-08-19T10:06:52.818801","exception":false,"start_time":"2020-08-19T10:06:52.577492","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.61925Z","iopub.status.idle":"2021-06-20T15:03:36.619652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_preds_show(item,clas,bboxs,show_img,cnt,i):\n    detect_thresh=0.4   # set your own detection threshold\n    nms_thresh=0.1\n    pred_string = []\n    scores_tot = []\n    bboxs_tot = []\n    show_img = True if i<cnt else False\n    for clas_pred, bbox_pred in list(zip(clas, bboxs)):\n        bbox_pred, scores, preds = process_output(clas_pred, bbox_pred, anchors, detect_thresh)\n        if bbox_pred is not None:\n            to_keep = nms(bbox_pred, scores, nms_thresh)\n            bbox_pred, preds, scores = bbox_pred[to_keep].cpu(), preds[to_keep].cpu(), scores[to_keep].cpu()\n        t_sz = torch.Tensor([size])[None].cpu()\n        if bbox_pred is not None:\n            bbox_pred = to_np(rescale_boxes(bbox_pred, t_sz))\n                # change from center to top left\n            bbox_pred[:, :2] = bbox_pred[:, :2] - bbox_pred[:, 2:] / 2\n            bboxs_tot.append(bbox_pred)\n            scores_tot.append(scores)\n    if show_img:\n        show_output(item,bboxs_tot,scores_tot)\n    area_max = (1024**2)/5\n    for s,bbx in zip(scores_tot[0].numpy(),bboxs_tot[0]):\n        bbx = [int(round(x)) for x in bbx*2]\n        if bbx[2]*bbx[3] <= area_max :\n            res = \"{0:.4f} {1} {2} {3} {4}\".format(s,bbx[1],bbx[0],bbx[3],bbx[2])\n            pred_string.append(res)\n    return pred_string","metadata":{"papermill":{"duration":0.255738,"end_time":"2020-08-19T10:06:53.301324","exception":false,"start_time":"2020-08-19T10:06:53.045586","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.62049Z","iopub.status.idle":"2021-06-20T15:03:36.620908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(show_img=True,cnt=10): \n    # Set show img True to see img or else false for bboxs only, cnt for number of images to show\n    preds_str = {}\n    for i in range(len(data.test_ds)):\n        item = learn.data.test_ds[i][0]  #Pick one image\n        batch = learn.data.one_item(item)\n        clas,bboxs,xtr = learn.pred_batch(batch=batch)\n        prd = process_preds_show(item,clas,bboxs,show_img,cnt,i) \n        preds_str[image_id[i]] = \" \".join(prd)\n    return preds_str","metadata":{"papermill":{"duration":0.234411,"end_time":"2020-08-19T10:06:53.79904","exception":false,"start_time":"2020-08-19T10:06:53.564629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.621782Z","iopub.status.idle":"2021-06-20T15:03:36.622243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = get_prediction()\n# Set False to not show images\n# Regardless of that it will give prediction string ","metadata":{"papermill":{"duration":6.911761,"end_time":"2020-08-19T10:07:00.934029","exception":false,"start_time":"2020-08-19T10:06:54.022268","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.623087Z","iopub.status.idle":"2021-06-20T15:03:36.62349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame.from_dict(prediction,orient='index').reset_index()\nsubmit.columns = ['image_id','PredictionString']\nsubmit.head(10)","metadata":{"papermill":{"duration":0.318701,"end_time":"2020-08-19T10:07:01.559027","exception":false,"start_time":"2020-08-19T10:07:01.240326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-20T15:03:36.624277Z","iopub.status.idle":"2021-06-20T15:03:36.624671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.321646,"end_time":"2020-08-19T10:07:02.164779","exception":false,"start_time":"2020-08-19T10:07:01.843133","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}