{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Topic analysis using LDA\n\nThis notebook aims to discover which topics were the most discussed in the Cricket Subreddit. For this, we'll use a popular technique called LDA (Latent Dirichlet Allocation)","metadata":{}},{"cell_type":"markdown","source":"First we'll take a look on the structure of the dataset","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:23.460906Z","iopub.execute_input":"2021-08-29T00:06:23.461414Z","iopub.status.idle":"2021-08-29T00:06:23.46618Z","shell.execute_reply.started":"2021-08-29T00:06:23.461371Z","shell.execute_reply":"2021-08-29T00:06:23.465128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport gensim\nimport nltk\nimport pandas as pd\nimport pyLDAvis.gensim_models\nimport re\nimport seaborn\n\nfrom gensim.utils import simple_preprocess\nimport gensim.corpora as corpora\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom pprint import pprint\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:23.49066Z","iopub.execute_input":"2021-08-29T00:06:23.491209Z","iopub.status.idle":"2021-08-29T00:06:23.500085Z","shell.execute_reply.started":"2021-08-29T00:06:23.491165Z","shell.execute_reply":"2021-08-29T00:06:23.498961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cricket-on-reddit/reddit_cricket.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:23.517786Z","iopub.execute_input":"2021-08-29T00:06:23.51814Z","iopub.status.idle":"2021-08-29T00:06:23.565977Z","shell.execute_reply.started":"2021-08-29T00:06:23.518109Z","shell.execute_reply":"2021-08-29T00:06:23.564757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:23.567795Z","iopub.execute_input":"2021-08-29T00:06:23.568113Z","iopub.status.idle":"2021-08-29T00:06:23.585053Z","shell.execute_reply.started":"2021-08-29T00:06:23.568082Z","shell.execute_reply":"2021-08-29T00:06:23.583733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['body'] = df['body'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:23.593023Z","iopub.execute_input":"2021-08-29T00:06:23.593427Z","iopub.status.idle":"2021-08-29T00:06:23.600686Z","shell.execute_reply.started":"2021-08-29T00:06:23.593397Z","shell.execute_reply":"2021-08-29T00:06:23.599443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now we'll remove all punctuation and set all letters to lower case","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load the regular expression library\n# Remove punctuation\ndf['body_processed'] = df['body'].map(lambda x: re.sub('[,\\.!?]', '', x))\n# Convert the titles to lowercase\ndf['body_processed'] = df['body_processed'].map(lambda x: x.lower())\n# Print out the first rows of papers\ndf['body_processed'].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:23.618018Z","iopub.execute_input":"2021-08-29T00:06:23.618552Z","iopub.status.idle":"2021-08-29T00:06:23.690535Z","shell.execute_reply.started":"2021-08-29T00:06:23.618517Z","shell.execute_reply":"2021-08-29T00:06:23.68964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see which words most appear in the dataset, we'll plot a WordCloud with them.","metadata":{}},{"cell_type":"code","source":"long_string = ','.join(list(df['body_processed'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a; word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nplt.figure( figsize=(15,10) )\nplt.imshow(wordcloud)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:23.69204Z","iopub.execute_input":"2021-08-29T00:06:23.69234Z","iopub.status.idle":"2021-08-29T00:06:25.512067Z","shell.execute_reply.started":"2021-08-29T00:06:23.692312Z","shell.execute_reply":"2021-08-29T00:06:25.511342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll remove the stopwords, remove words with len < 5 and remove other words that may mess our analysis (that we saw in the word cloud), like 'https' and tokenize the words.","metadata":{}},{"cell_type":"code","source":"\nnltk.download('stopwords')\nen_stop = set(nltk.corpus.stopwords.words('english'))\n\nen_stop.update(['nan','https', 'http','wwwredditcom', 'match_thread_', 'comments', 'threads', 'wwwredit'])\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        # deacc=True removes punctuations\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if word not in en_stop] for doc in texts]\n\ndef filter_size(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if len(word) > 4 ] for doc in texts]\n\ndata = df['body_processed'].values.tolist()\ndata_words = list(sent_to_words(data))\n# remove stop words\ndata_words = remove_stopwords(data_words)\ndata_words = filter_size(data_words)\nprint(data_words[:1][0][:30])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:25.513641Z","iopub.execute_input":"2021-08-29T00:06:25.514106Z","iopub.status.idle":"2021-08-29T00:06:27.902756Z","shell.execute_reply.started":"2021-08-29T00:06:25.514076Z","shell.execute_reply":"2021-08-29T00:06:27.901755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we convert the tokenized object into a corpus and dictionary.","metadata":{}},{"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n# Create Corpus\ntexts = data_words\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n# View\nprint(corpus[:1][0][:30])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:27.904269Z","iopub.execute_input":"2021-08-29T00:06:27.904565Z","iopub.status.idle":"2021-08-29T00:06:28.231316Z","shell.execute_reply.started":"2021-08-29T00:06:27.904536Z","shell.execute_reply":"2021-08-29T00:06:28.23023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic.","metadata":{}},{"cell_type":"code","source":"# number of topics\nnum_topics = 10\n# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics)\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:28.232851Z","iopub.execute_input":"2021-08-29T00:06:28.233145Z","iopub.status.idle":"2021-08-29T00:06:31.720041Z","shell.execute_reply.started":"2021-08-29T00:06:28.233117Z","shell.execute_reply":"2021-08-29T00:06:31.718836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have a trained model letâ€™s visualize the topics for interpretability","metadata":{}},{"cell_type":"code","source":"lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, sort_topics=False)\npyLDAvis.enable_notebook()\npyLDAvis.display(lda_display)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T00:06:31.721465Z","iopub.execute_input":"2021-08-29T00:06:31.721807Z","iopub.status.idle":"2021-08-29T00:06:35.041203Z","shell.execute_reply.started":"2021-08-29T00:06:31.721775Z","shell.execute_reply":"2021-08-29T00:06:35.040036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's it! Thank you for checking this notebook. It was largely inspired by these two tutorials:\n1. https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n2. https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0","metadata":{}}]}