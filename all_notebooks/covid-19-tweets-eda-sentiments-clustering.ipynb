{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Analysing COVID19 Tweets!**\n# Hello Data Scientist, Welcome to my notebook!\n### <div align=\"left\">  Today we shall perform EDA, Clustering using KMeans and Sentiment analysis on the Tweets (related to #Covid19) to answer the following questions:\nLet us begin : \n</div>\n\n\n1. Which countries tweeted the most? (based on Generalised Content)\n> 1.1 which countries tweeted the most based on #Covid19 or tweets that are related to Corona/Covid19\n2. Was the tweet based on Corona if so how frequent was the tweet based on \n> 2.1 User Accounts / User Names\n>> 2.2 Country / Location \n>>> 2.3 Source / Device they tweeted from\n3. How much did the Verified profile people tweeted in the year 2019 and 2020 based out of the country?\n4. Monthly tweet trend of 2019 vs 2020\n5. Who are the top 40 people/accounts that has most followers and their tweet count?\n6. What are the most common words in the tweets?\n7. Clustering the most commonly used tweet words into 5 groups\n> 7.1 Group 1\n>> 7.2 Group 2\n>>> 7.3 Group 3\n>>>> 7.4 Group 4\n>>>>> 7.5 Group 5\n8. What are the positive and negative sentiment words?\n> Word-cloud of Positive and Negative twitter bird\n9. What are the top 10 countries that tweeted with positive sentiments?\n10. How is the data (countries, people, tweet content, month) distributed with sentiments?\n> 10.1 In the Year 2019\n>> 10.2 In the Year 2020\n\n<div align=\"center\"> \n    If you loved my notebook, please upvote to encourage me! \n</div> "},{"metadata":{},"cell_type":"markdown","source":"###### Pre-requirements"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''\nnumpy library\npandas library\nmatplot library\nplotly library\nwordcloud library\nnltk library\nPIL library\nseaborn library\nre library\nscikit learn library\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Statements"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#stop=set(stopwords.words('english'))\n\nfrom PIL import Image\nimport seaborn as sns\nimport re\nfrom sklearn.datasets import make_blobs\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport chardet\n\n# tried building a nlp model for sentiments, but it was a time killer as i had prior exposure only on ANN and CNN \n#So made use of prebuild nlp model that predicts sentiments\n\n'''\n#from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom collections import Counter\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filepath of the dataset"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"filepath = r'../input/covid19-tweets/covid19_tweets.csv'\ninfo_path = r'../input/covid-19-tweet-supporting-files/columns.txt'\n#sentiment_filepath = r'C:/Users/riaz/learning/user_sentiment.csv'\ndf = pd.read_csv(filepath)\n#sentiment = pd.read_csv(sentiment_filepath,encoding = 'latin',header=None,names=['target','id','time','query','usr','text'])\ninfo = pd.read_csv(info_path,delimiter='->',names=['title','description'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation of given datas with heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,3))\nsns.heatmap(df.corr(),annot=True,linecolor='white',linewidths=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting unique features in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_features(data_frame):\n    unique_dataframe = pd.DataFrame()\n    unique_dataframe['features'] = data_frame.columns\n    uniques = []\n    for col in data_frame.columns:\n        u = data_frame[col].nunique()\n        uniques.append(u)\n    unique_dataframe['uniques'] = uniques\n    return unique_dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = unique_features(df)\n#print(features)\n\nplt.figure(figsize=(10,7))\nfeatures = features.sort_values(by='uniques',ascending=False)\nsns.barplot(x='uniques',y='features',data=features,palette='Dark2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CONVERTING TIME STAMPS"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['user_created'] = pd.to_datetime(df['user_created'])\ndf['only_date'] = pd.to_datetime(df['user_created']).dt.date\ndf['created_year'] = df['user_created'].apply(lambda date : date.year)\ndf['created_month'] = df['user_created'].apply(lambda date : date.month)\ndf['created_day'] = df['user_created'].apply(lambda date : date.day)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"###### Checking for empty objects / nan "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.isnull().sum().sort_values(ascending=False)[:4]\nplt.figure(figsize=(3,3))\ndata.plot(kind='barh',grid=False,sort_columns=True,title='total_missing_values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Grouping / cleaning unknown dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['user_location'].fillna('Unknown', inplace=True)\ndf['user_description'].fillna('Unknown', inplace=True)\ndf['source'].fillna('Unknown', inplace=True)\ndf['hashtags'].fillna('None', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Cleaning and Extracting  country name from \"user_ locations\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting a string into 2 words, where the first word corresponds to city name and second word to the country name\n#this acts only if the given str has a ',' in it else it would return the same\ndf['country'] = df['user_location'].apply(lambda x: x.split(\",\")[-1].strip() if (\",\" in x) else x)\ndf['city'] = df['user_location'].apply(lambda x: x.split(\",\")[0].strip() if (\",\" in x) else x)\n\n#replacing the two digit US city names with USA except UK and EU\ndf['country'] = df['country'].apply(lambda x: 'USA' if (len(x.lower().strip())<3) and ((x!='uk')|(x!='eu')) else x)\n#replacing lower case country names with standard ones\ndf['country'] = df['country'].apply(lambda x: 'USA' if x.lower().strip() in (\"united states,Alabama, Alaska, American Samoa, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, District of Columbia, Florida, Georgia, Guam, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Northern Mariana Islands, Ohio, Oklahoma, Oregon, Pennsylvania, Puerto Rico, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, U.S. Virgin Islands, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming california, texas, usa,new york, us\") else x)\ndf['country'] = df['country'].apply(lambda x: 'Canada' if x.lower().strip() in (\"ontario,toronto,quebec,montreal,quebec city,vancouver\") else x)\ndf['country'] = df['country'].apply(lambda x: 'UK' if x.lower().strip() in (\"united kingdom,london, england,uk, britain,great britain\") else x)\ndf['country'] = df['country'].apply(lambda x: 'India' if x.lower().strip() in (\"india,mumbai,tamil nadu,chennai,karnataka,bengaluru,kerala,thiruvanandhipuram,kochi,patna,delhi,new delhi,uttar pradesh,andhra pradesh,telengana,vishakapatinam,hyderabad,himachal pradesh,goa,jammu,jammu and kashmir,ladhak\") else x)\ndf['country'] = df['country'].apply(lambda x: 'N/A' if x.lower().strip() in (\"worldwide, earth, global\") else x)\n\n\n#if country name is found in city name,\ncountry_list = ['finland','netherlands','ireland','sweden','germany','denmark','switzerland','norway','france','spain','canada','bulgaria','belgium','estonia','uk','luxembourg','newzealand','austria','italy','australia','latvia','cyprus','singapore','japan','northmacedonia','southkorea','moldova','slovakia','romania','portugal','poland','czechrepublic','slovenia','costarica','chile','iceland','lithuania','georgia','hungary','usa','russia','greece','india','malaysia','armenia','southafrica']\n\ndef checker(x):\n    #checking for city names in country list\n    if x['city'].lower().strip() in country_list:\n        #if city name is not same as country name but is in country list\n        if x['city'].lower().strip() != x['country'].lower().strip():\n            return x['city']\n        else:\n            return x['country']\n    else:\n        return x['country']\n\ndf['country'] = df[['city','country']].apply(checker,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Hashtage cleaner fuction, which groups tweets that are specific to Covid 19 "},{"metadata":{"run_control":{"marked":true},"trusted":true},"cell_type":"code","source":"corona_keys = ['covid19','covid_19','covid','pandemic','covid-19','corona','virus','coronavirus','coronavirusupdates']\n\ndef hashtag_grouper(a):\n    b = a.replace(\"\\'\",\"\").strip().strip(\"[\").strip(\"]\").lower()\n    c = b.split(',')\n    for item in c:\n        if item in corona_keys:\n            return 'Covid 19'\n        else:\n            return item\n\ndf['category'] = df['hashtags'].apply(hashtag_grouper)\ndf['covid'] = df['category'].apply(lambda x: True if x == 'Covid 19' else False)\ncoronadf = df[df['covid']>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### creating a dataFrame that has count of  total number of tweets per country"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataFrame of countries that tweeted general content\nlocation_count = pd.DataFrame(df['country'].value_counts())\nlocation_count.reset_index(inplace=True)\nlocation_count = location_count.set_axis(['country', 'count'], axis=1)\nlocation_count = location_count.sort_values(by='count',ascending=False)\n\n\n#dataFrame of countries that tweeted about covid 19\nlocation_count2 = pd.DataFrame(coronadf['country'].value_counts())\nlocation_count2.reset_index(inplace=True)\nlocation_count2 = location_count2.set_axis(['country', 'cv_count'], axis=1)\nlocation_count2 = location_count2.sort_values(by='cv_count',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Countries that tweeted the most on generalised content"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_graph = px.bar(x='country',y='count',data_frame=location_count[1:16],color='country',\n                    labels={'x':'Countries','y':'Counts'},title='Generalised Tweets')\ncount_graph.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Now, we shall see countries that specifically tweeted the most about Covid 19 and their tweet counts respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_graph2 = px.bar(x=location_count2['country'][1:16],y=location_count2['cv_count'][1:16],\n                      color=location_count2['country'][1:16],labels={'x':'Countries','y':'Counts'},\n                      title='Tweets based on corona/Covid 19')\ncount_graph2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. EDA on tweets from the year 2019 (i.e. 2019 + 2020) based on"},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph(data, feature, title, pallete):\n    f, ax = plt.subplots(1,1, figsize=(18,6))\n    total = float(len(df))\n    if feature == 'user_location' or 'country' or 'city' or 'user_names':\n        g = sns.countplot(data[feature],hue= data['covid'],order = data[feature].value_counts().index[1:11], palette=pallete)  \n    elif feature == 'source':\n        g = sns.countplot(data[feature],hue= data['covid'],order = data[feature].value_counts().index[:16], palette=pallete)\n    else:\n        g = sns.countplot(data[feature],hue= data['covid'], order = data[feature].value_counts().index[0:11], palette=pallete)\n    g.set_title(\"Number and percentage of {}\".format(title))\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n\n    plt.title('Counts & Percentage representation of {} that were actually tweeting about Corona'.format(feature))\n    plt.ylabel('Frequency', fontsize=12)\n    plt.xlabel(title, fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 This plot symbolizes how frequent did the top user tweeted and whether the content was related to Covid19"},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(df[df.created_year > 2019 ], 'user_name', 'User Names','CMRmap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 This plot symbolizes Where did the most tweets come from and whether the content was related to Covid19"},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(df[df.created_year > 2019 ], 'country', 'User Locations', 'gist_rainbow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 This plot symbolizes from which devices did the tweets originated and whether the content was related to Covid19"},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(df[df.created_year > 2019 ], 'source','Source', 'plasma_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. This EDA show us how did the \"verified_Profile\" users reacted from the top 5 countries in the year 2019 and 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"verified_corona = df[ (df.user_verified == True) & (df.covid == True) & (df.country == ('USA')) | \n             (df.country == 'India') | (df.country == 'Australia') | (df.country =='UK') | (df.country =='Canada') ]  \n\nverified_corona_20 = verified_corona[verified_corona.created_year == 2020]\nverified_corona = verified_corona[verified_corona.created_year == 2019]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of tweets from top 5 countries')\nf, axes = plt.subplots(1, 2,figsize=(16,6))\naxes[0].set_title(\"Year 2019\")\naxes[1].set_title(\"Year 2020\")\norder_type = sorted(verified_corona['created_month'].unique())\nsns.countplot(x=verified_corona.created_month, hue=verified_corona.country,palette=\"Dark2\",order=order_type,ax=axes[0])\norder_type = sorted(verified_corona_20['created_month'].unique())\nsns.countplot(x=verified_corona_20.created_month, hue=verified_corona_20.country,palette=\"Dark2\",order=order_type,ax=axes[1])\n\nf.tight_layout()\nplt.show()\n#'Number of verified profile tweets in the year 2019 vs 2020'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Finding a pattern/Spike in tweets"},{"metadata":{},"cell_type":"markdown","source":"##### From this **Line Chart**  we could see quite a spike in number of tweets(February,2020 to March,2020), that is when the WHO declared Global Pandemic and many nations started security measurments such as Lockdowns"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_from_2019 = df[df['created_year'] == 2019]\ntweet_from_2020 = df[df['created_year'] == 2020]\ntweets_month_19 =  tweet_from_2019['created_month'].value_counts().to_frame().reset_index().rename(columns={'index':'month','created_month':'count'})   \ntweets_month_20 =  tweet_from_2020['created_month'].value_counts().to_frame().reset_index().rename(columns={'index':'month','created_month':'count'})   \ntweets_month_19 = tweets_month_19.sort_values('month',ascending=True)\ntweets_month_20 = tweets_month_20.sort_values('month',ascending=True)\n\nfig=go.Figure()\nfig.add_trace(go.Scatter(x=tweets_month_19['month'], y=tweets_month_19['count'],\n                         mode='markers+lines',marker_color='firebrick',\n                         name='Year 2019',line = dict(color='grey', width=4, dash='dot')))\n\n#fig.update_layout(title_text='Tweets per Month and Date in the year 2019 and 2020 ',template=\"plotly\", title_x=0.5)\nfig.add_trace(go.Scatter(x=tweets_month_20['month'], y=tweets_month_20['count'],\n                         mode='markers+lines',marker_color='darkred',name='Year 2020',\n                        line=dict(color='orange', width=4,dash='longdashdot')))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Most popular user with thier tweet count and followers"},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = df['user_name'].value_counts().reset_index()\nds.columns = ['user_name', 'tweets_count']\nds = ds.sort_values(['tweets_count'])\ndf = pd.merge(df, ds, on='user_name')\n\ndata = df.sort_values('user_followers', ascending=False)\ndata = data.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[['user_name', 'user_followers', 'tweets_count']]\ndata = data.sort_values('user_followers')\nfig = px.bar(data.tail(40), x=\"user_followers\", y=\"user_name\",color='tweets_count',orientation='h', \n             title='Top 40 users by number of followers', width=1000, height=1000,template=\"plotly_dark\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Preprocession text from tweeted content"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\n\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    #string=\"\".join(line)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line\ndef remove_thi_amp_ha_words(string):\n    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',string)\n    return line","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean_tweet'] =  df['text'].str.lower()\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_tag(str(x)))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_mention(str(x)))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_hash(str(x)))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_newline(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_url(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_number(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_punct(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:remove_thi_amp_ha_words(x))\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x:text_strip(x))\n\ndf['text_length']=df['clean_tweet'].str.split().map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word clouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_wordcloud(var, title):\n    wordcloud = WordCloud(background_color='black',colormap=\"bwr\", \n                          stopwords=set(STOPWORDS),max_words=80, min_font_size= 8,\n                          max_font_size=40, random_state=666).generate(str(var))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=16)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Creating a word cloud for the most popular words used"},{"metadata":{},"cell_type":"markdown","source":"###### As predicted a  huge  propotion of words are about Covid!"},{"metadata":{"trusted":true},"cell_type":"code","source":"build_wordcloud(df['clean_tweet'], 'Popular keywords used in all tweets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Clustering the most commonly used word using KMeans\n Cheers! 🥂 learnt from ( https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html )"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"vec = TfidfVectorizer(stop_words=\"english\")\nvec.fit(df['clean_tweet'].values)\nfeatures = vec.transform(df['clean_tweet'].values)\n\n#taking an arbitary value say 5, to get 5 cluster values\nkmeans = KMeans(n_clusters = 5,init ='k-means++', max_iter=300, random_state=0,verbose=1)\ny_kmeans =  kmeans.fit_predict(features)\ndf['Cluster']  = y_kmeans\n#df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df[df['Cluster'] == 0].head(20)['text'].tolist()\n#df[df['Cluster'] == 1].head(20)['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 7.1 Word-cloud of Cluster 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster'] == 0]['text'], '1st word cluster using KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 7.2 Word-cloud of Cluster 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster'] == 1]['text'], '2nd Word Cluster using KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 7.3 Word-cloud of Cluster 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster'] == 2]['text'], '3rd word cluster using KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 7.4 Word-cloud of Cluster 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster'] == 3]['text'], '4th word cluster using KMeans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 7.5 Word-cloud of Cluster 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"build_wordcloud(df[df['Cluster'] == 4]['text'], 'And finally the 5th word cluster using KMeans ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Sentiment Analysis"},{"metadata":{},"cell_type":"markdown","source":"###### Here we shall use sentiments of  two types i.e. Postivie Sentiments and  Negative Sentiments and extracting sentiments from a pretrained model"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"nltk.download('vader_lexicon')\nsid = SentimentIntensityAnalyzer()\n\ndef get_score(text):\n    dict_res = sid.polarity_scores(text)\n    return dict_res[\"compound\"]\n\ndf[\"Score\"] = df[\"clean_tweet\"].apply(lambda x: get_score(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Score\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df=pd.DataFrame({'text':df['clean_tweet'],'pred_sentiment':df['Score'],'country':df['country'],'text_length':df['text_length']})\npred_df['pred_sentiment']=np.where(pred_df['pred_sentiment']>0.5,1,0)\npred_df[['text','pred_sentiment']].head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bird = np.array(Image.open('../input/covid-19-tweet-supporting-files/twitter_logo.jpg'))\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask= bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==0]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment Bird',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask= bird,colormap=\"Blues\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==1]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive Sentiment Bird',fontsize=35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Top 10 countries that had positive sentiment of tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_countries = pd.DataFrame()\nsentiment_countries[\"Score\"] = df[\"Score\"]\nsentiment_countries[\"country\"] = df[\"country\"]\n\nsentiment_countries = sentiment_countries.sort_values(by = \"Score\",ascending=False)\nsentiment_countries = sentiment_countries.groupby(\"country\").sum().sort_values(by = \"Score\",ascending=False)[:10]\n\nplt.figure(figsize=(9,10))\nsns.barplot(list(sentiment_countries.values.flatten()),sentiment_countries.index,)\nplt.title(\"Top 10 Location By Positive Score\")\nplt.xlabel(\"Sentiment Score\")\nplt.ylabel(\"Location\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10. Data distribution"},{"metadata":{},"cell_type":"markdown","source":"#### 10.1 Overview of User Sentiments among the top three countries that tweeted during 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df.drop(['user_description','user_location',\t'user_created','text','is_retweet','date', 'hashtags' ],axis=1)\ndf_clean_19 = df_clean[(df_clean.created_year == 2019)]\ntop_3 = df_clean_19[(df_clean_19.country == 'USA') | (df_clean_19.country == 'India') | (df_clean_19.country == 'UK')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.parallel_categories(top_3, dimensions=['country', 'user_verified','covid', 'created_month'],\n                color=\"Score\", color_continuous_scale=px.colors.sequential.Inferno,\n                labels={'sex':'Payer sex', 'smoker':'Smokers at the table', 'day':'Day of week'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 10.2 Overview of User Sentiments among the top three countries that tweeted during 2010"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_clean = df.drop(['user_description','user_location',\t'user_created','text','is_retweet','date', 'hashtags' ],axis=1)\ndf_clean_20 = df_clean[(df_clean.created_year == 2020)]\ntop_3_20 = df_clean_20[(df_clean_20.country == 'USA') | (df_clean_20.country == 'India') | (df_clean_20.country == 'UK')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.parallel_categories(top_3_20, dimensions=['country', 'user_verified','covid', 'created_month'],\n                color=\"Score\", color_continuous_scale=px.colors.sequential.Inferno,\n                labels={'sex':'Payer sex', 'smoker':'Smokers at the table', 'day':'Day of week'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### <div align=\"center\">   With endless possibilites of representation and uncertainity, I hereby end my assessment with good hope!\n\n<div align=\"center\"> Thanks and Regards,\n<div align=\"center\"> Mohamed Riaz | mohamed.riaz1307@gmail.com \n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}