{"cells":[{"metadata":{"_uuid":"c678adcb3777358b5c553c283b5faeaa70cdb195"},"cell_type":"markdown","source":"# Download Data"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:51:59.724269Z","start_time":"2018-12-29T19:51:54.997288Z"},"trusted":false,"_uuid":"cefb3bf311aca55a21391a5d57e4a99e261865df"},"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('../input/WA_Fn-UseC_-HR-Employee-Attrition.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48fbf7cdf38a43aaac3b90d8d089c5c88af1c9a9"},"cell_type":"markdown","source":"# Exploratory data analysis"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:02.118267Z","start_time":"2018-12-29T19:52:02.075265Z"},"trusted":false,"_uuid":"43ae531c0383340746e798261b1426f78edd1164"},"cell_type":"code","source":"# first few rows\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:04.947255Z","start_time":"2018-12-29T19:52:04.936253Z"},"trusted":false,"_uuid":"0067dfc9c12098e695717d8c376553d6e988273c"},"cell_type":"code","source":"# columns\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e17027cbc5500e62924a2e3024869fd4990ac48e"},"cell_type":"markdown","source":"__No Null values and we have 26 int types and 9 object types  mainly string. These 9 objects are categorical attributes .In total we have 35 features__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:11.201235Z","start_time":"2018-12-29T19:52:11.194232Z"},"trusted":false,"_uuid":"f6c7564d465844cf8a3d9c1c020845bd5a9d46ac"},"cell_type":"code","source":"catDf = pd.DataFrame(columns=['Categorical Features','Distinct Values','Distinct Count'])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:14.130225Z","start_time":"2018-12-29T19:52:14.125225Z"},"trusted":false,"_uuid":"3d59db5a0da2c1912bf8d18794715fbc5bcfb842"},"cell_type":"code","source":"data['Attrition'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:16.39222Z","start_time":"2018-12-29T19:52:16.357217Z"},"trusted":false,"_uuid":"75a8fcefbde11e0adb120c9576e981b2f0d2045c"},"cell_type":"code","source":"i=0\nfor col in data.select_dtypes(['object']).columns.tolist():\n    catDf.loc[i] = [col,data[col].unique(),data[col].nunique()]\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:19.759208Z","start_time":"2018-12-29T19:52:19.749207Z"},"trusted":false,"_uuid":"59ac6cac3f4776491ae04affa9f463e3d4e85cb4"},"cell_type":"code","source":"catDf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04c005826ffc1c0b1d6357802bda0185ebcf0794"},"cell_type":"markdown","source":"# Check for Outliers "},{"metadata":{"_uuid":"6819024b78ad9baa4d35ade4989090bc3ed673e8"},"cell_type":"markdown","source":"__Visualization via histograms__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:28.028181Z","start_time":"2018-12-29T19:52:23.184197Z"},"trusted":false,"_uuid":"929e64ba7f756389c171e30822931338a6cf12c7"},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndata.hist(bins=50,figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49eb93647bf4549f4c8bd02c31573cdd57f8414e"},"cell_type":"markdown","source":"__Data looks clean with no potential outliers. We can drop following features since they are constant and does not contribute to the model:__\n\n1. __Employee Count__\n2. __StanardHours__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:32.300165Z","start_time":"2018-12-29T19:52:32.188167Z"},"scrolled":false,"trusted":false,"_uuid":"ee62173a832cf70762e04f6b269a66db1aafb7da"},"cell_type":"code","source":"# stata\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66706ebafc2ab4b799402cc530f9015b77ef4123"},"cell_type":"markdown","source":"# Data visualization and EDA"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-13T02:20:31.58866Z","start_time":"2018-12-13T02:20:31.575659Z"},"trusted":false,"_uuid":"fe2b40a0a87594d52803217ecb3d0bcac2c0901a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:47.055124Z","start_time":"2018-12-29T19:52:35.719155Z"},"trusted":false,"_uuid":"18b2dd789cc306f3639ee58df015df64b5b74070"},"cell_type":"code","source":"plt.figure(figsize=(80,80))\ni=1\n\nfor col in data.select_dtypes('int64').columns.tolist():\n    plt.subplot(7,4,i)\n    x1 = list(data[data['Attrition'] == 'Yes'][col])\n    x2 = list(data[data['Attrition'] == 'No'][col])\n    colors = ['#E69F00', '#56B4E9']\n    names = ['Attrition-Yes(1)','Attrition-No(0)']\n    plt.hist([x1,x2],bins=50,label=names,color=colors)\n    plt.legend(fontsize=28)\n    plt.xlabel(col,fontsize=28)\n    plt.ylabel('# of attrition',fontsize=28)\n    plt.tick_params(labelsize=26)\n    i = i+1\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dea020f51cb73c343fe14471bea077816dbe137"},"cell_type":"markdown","source":"__Interesting observations:__\n\n1. Iteration rate is higher in the early age: twenties  \n2. Iteration rate is higher among those who lives farther from the office. \n3. Iteration rate is higher in their second year\n4. Lower jobs levels tend to have higher iteration rate.\n\n__and many more, We also observed we can drop columns such as Daily rate, Employee Conunt and standard hours for obvious reasons__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:51.995109Z","start_time":"2018-12-29T19:52:51.987107Z"},"trusted":false,"_uuid":"fe61816016be50cb253796d2f2ec66429c78075d"},"cell_type":"code","source":"# mapping the target variable to numeric \nimport numpy as np\n\ndata['Attrition'] = np.where(data['Attrition'] == 'Yes',1,0)\ndata['Attrition'].head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68fe8138eb1d2b737dc0f4aa76be210e26f58471"},"cell_type":"markdown","source":"# Split training and test data"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:52:59.162081Z","start_time":"2018-12-29T19:52:55.284096Z"},"trusted":false,"_uuid":"e3c6c27a3053e757e9efb01792842698bb2f18b9"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(data,test_size=0.2,random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5277de5b4c007d3e3d6bc0895504128070ad193d"},"cell_type":"markdown","source":"# Segregating X and y "},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:53:00.18008Z","start_time":"2018-12-29T19:53:00.175078Z"},"trusted":false,"_uuid":"6a34a592e1c5864c159ece0baa06a0e11e23c68b"},"cell_type":"code","source":"X_train = train.drop(['Attrition'],axis=1)\ny_train = train['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0a8c2d5e33e8f0d672ef7f12d14338e55e20971"},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:53:02.751069Z","start_time":"2018-12-29T19:53:02.745066Z"},"trusted":false,"_uuid":"511e43c789bab1bfceec6aae0a9228327ea522bd"},"cell_type":"code","source":"# Ref: https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:53:11.565042Z","start_time":"2018-12-29T19:53:11.543041Z"},"trusted":false,"_uuid":"95f39187c0a801ba75fdfe64bf9560b0ae459b6b"},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:53:15.272038Z","start_time":"2018-12-29T19:53:15.244031Z"},"trusted":false,"_uuid":"8cb45367087495189caffbf0b6021f4ee330f440"},"cell_type":"code","source":"print(\"Top 10 Correlations pairs\")\nprint(get_top_abs_correlations(X_train.select_dtypes(exclude=['object']), 10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9263cf8b592f727985535655cf513a06c55f041"},"cell_type":"markdown","source":"__Goal here is to have as much as less correlated features in the model. Dropping MonthlyIncome seams to be wise choice since it will drop first and third pair__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-10T02:25:08.567676Z","start_time":"2018-12-10T02:25:02.671677Z"},"_uuid":"8ba293f4f6a2b9db2a1fb90b9d61a0696de53d93"},"cell_type":"markdown","source":"# Build numerical and categorical feature pipeline "},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:53:18.981018Z","start_time":"2018-12-29T19:53:18.963024Z"},"trusted":false,"_uuid":"d75b987c560f1cd011f2e0607cddc8a233a7f195"},"cell_type":"code","source":"# We will use the below statement, lets wrap it under feature engineering class\n#data = data.drop(['DailyRate','EmployeeCount','StandardHours'],axis=1)\n\n#include Object\ncat_attributes = X_train.select_dtypes(include=['object']).columns.tolist()\n#exclude Object\nnum_attributes = X_train.select_dtypes(exclude=['object']).columns.tolist()\n\n#We also observed we can drop columns such as Daily rate, Employee Count and standard hours\nnum_attributes.remove('DailyRate')\nnum_attributes.remove('EmployeeCount')\nnum_attributes.remove('StandardHours')\nnum_attributes.remove('MonthlyIncome')\n#num_attributes.append('monthlyIncomePerUnitAge')\n\nprint('cat_attributes: ',cat_attributes)\nprint('num_attributes: ',num_attributes)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-09T14:10:49.598362Z","start_time":"2018-12-09T14:10:49.585358Z"},"_uuid":"de84c43ab0fbbde6f58d6bc8d90d3d32a2666217"},"cell_type":"markdown","source":"\n"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:53:22.251012Z","start_time":"2018-12-29T19:53:22.247011Z"},"trusted":false,"_uuid":"4bca2cbb171bce1403018262ed0c3bcc6919aa29"},"cell_type":"code","source":"len(num_attributes)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-19T12:48:57.50038Z","start_time":"2018-12-19T12:48:57.43238Z"},"trusted":false,"_uuid":"2a02f35d81f11c8dea7f0ab995221ca9f9e49f4b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91a374b8833247ecd15828eaba8977342c5afa5f"},"cell_type":"markdown","source":"__Since we don't see small outliers, Min-Max scaling technique would be ideal. Lets build a pipeline__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T19:53:25.302002Z","start_time":"2018-12-29T19:53:25.293996Z"},"trusted":false,"_uuid":"9d38fab986d5b78a375cad1188c10faf0deb8096"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\nmergeCols_pipeline = ColumnTransformer([\n         #('imputer' ,Imputer(strategy=\"median\")), we don't need in this case there is no NA\n        #('attrAdder',CombinedAttributesAdder(),num_attributes),\n       ('scaler',MinMaxScaler(),num_attributes),\n       (\"cat\", OneHotEncoder(), cat_attributes)\n    ])\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-20T21:46:41.024638Z","start_time":"2018-12-20T21:46:41.007639Z"},"_uuid":"603296733a867aed4d9588913cd8bb1cd9d85301"},"cell_type":"markdown","source":"# Model Selection - Learning Curve"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T23:25:34.149623Z","start_time":"2018-12-29T23:18:14.626094Z"},"trusted":false,"_uuid":"c81fabacbb176a79569ce2f7d4947491d4999a2b"},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nclassifiers = {\n    'Nearest Neighbors' : KNeighborsClassifier(3),\n    'Linear SVM'        :SVC(kernel=\"linear\", C=0.025),\n    'RBF SVM'           :SVC(gamma=2, C=1),\n    'Gaussian Process'  :GaussianProcessClassifier(1.0 * RBF(1.0)),\n    'Decision Tree'     :DecisionTreeClassifier(max_depth=5),\n    'Random Forest'     :RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    'Neural Net'        :MLPClassifier(alpha=1),\n    'AdaBoost'          :AdaBoostClassifier(),\n    'Naive Bayes'       :GaussianNB(),\n    'QDA'               :QuadraticDiscriminantAnalysis()\n}\n\n\nmergeColWithSelectKbest_pipeline = Pipeline([('prep',mergeCols_pipeline),\n                          ('featSelection',SelectKBest(chi2,k=10))\n                        ])\n\nX_train_prep = mergeColWithSelectKbest_pipeline.named_steps['prep'].fit_transform(X_train)\n\nX_train_featureSel = mergeColWithSelectKbest_pipeline.named_steps['featSelection'].fit_transform(X_train_prep,y_train)\n\n#ref: https://chrisalbon.com/machine_learning/model_evaluation/plot_the_learning_curve/\n\nplt.figure(figsize=(40,30))\ni=1\n\n\nfor key in classifiers:\n    \n    train_sizes, train_scores, test_scores = learning_curve(classifiers[key], \n                                                        X_train_featureSel, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=7,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.10, 1.0, 50))\n\n    # Create means and standard deviations of training set scores\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n\n    # Create means and standard deviations of test set scores\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    plt.subplot(4,3,i)\n    \n    \n    # Draw lines\n    plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n    plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n    # Draw bands\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n    # Create plot\n    plt.title(\"Learning Curve \" + key)\n    plt.xlabel(\"Training Set Size\",fontsize=30), plt.ylabel(\"Accuracy Score\",fontsize=30), plt.legend(loc=\"best\",fontsize=24)\n    plt.tight_layout()\n    \n    plt.tick_params(axis='both', which='major', labelsize=25)\n    plt.tick_params(axis='both', which='minor', labelsize=25)\n    \n    i=i+1\n    \n    #plt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T14:40:29.145953Z","start_time":"2018-12-29T14:40:29.080953Z"},"_uuid":"4fa1fc8073afa7e7dc50fdeb8084a35647107582"},"cell_type":"markdown","source":"__AdaBoost appears to perform best and consistant with this data set__"},{"metadata":{"_uuid":"ee3861947ceb78cdca8942cc00da8c837d38bb51"},"cell_type":"markdown","source":"# Evaluating number of features on the best Model [AdaBoost] - Tuning SelectKBest"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T23:42:09.228022Z","start_time":"2018-12-29T23:42:09.223017Z"},"trusted":false,"_uuid":"9620d99eb5bdcdd9b135e8f70b1f14b2abc0b8fa"},"cell_type":"code","source":"# How many features we have \nnp.shape(X_train_prep)[1]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T23:54:02.989904Z","start_time":"2018-12-29T23:53:56.323873Z"},"trusted":false,"_uuid":"fa29e4bc0613622555208f1923c342e66e33c110"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf_k_acc = pd.DataFrame(columns=['K','Accuracy'])\ni=1\nfor i in range(1,np.shape(X_train_prep)[1]+1):\n    \n    mergeColWithSelectKbest_pipeline = Pipeline([('prep',mergeCols_pipeline),\n                          ('featSelection',SelectKBest(chi2,k=i))\n                        ])\n\n    X_train_prep = mergeColWithSelectKbest_pipeline.named_steps['prep'].fit_transform(X_train)\n\n    X_train_featureSel = mergeColWithSelectKbest_pipeline.named_steps['featSelection'].fit_transform(X_train_prep,y_train)\n    \n    regressor = AdaBoostClassifier()\n    # fit\n    regressor.fit(X_train_featureSel,y_train)\n    # predict \n    train_predictions = regressor.predict(X_train_featureSel)\n    \n    \n    df_k_acc.loc[i] = [i,accuracy_score(y_train, train_predictions)]\n\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T00:04:29.662002Z","start_time":"2018-12-30T00:04:29.495004Z"},"trusted":false,"_uuid":"12e2a7ba9d3662df5c4f7e05ad4b117abc32b212"},"cell_type":"code","source":"#Plot the accuracy v/s K - number of features \nplt.plot(df_k_acc['K'],df_k_acc['Accuracy'])\nplt.xlabel('K[Number of features]')\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-29T14:40:39.086744Z","start_time":"2018-12-29T14:40:39.081742Z"},"_uuid":"9cd2611c534a0f151fbbf7bdd4d287e532ac01fd"},"cell_type":"markdown","source":"__Looks like all features are important and we will skip the feature selection step__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-15T13:57:36.97334Z","start_time":"2018-12-15T13:57:36.966343Z"},"_uuid":"1fb92facbecb137c15f09f086f5274278d091c97"},"cell_type":"markdown","source":"# Grid Search - Feature Tuning on the best model"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-20T21:02:49.430317Z","start_time":"2018-12-20T21:02:49.411312Z"},"_uuid":"d3819cda3af129c94e44499f22d7e62ea143452a"},"cell_type":"markdown","source":"__Performing grid search on AdaBoost__"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T00:21:56.106594Z","start_time":"2018-12-30T00:17:53.316176Z"},"trusted":false,"_uuid":"e498eb7d783a723bb9b5dc1a7f0ad44133146ba6"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n#list of dictionary \nparam_grid = [ {'n_estimators'  : [10,100,500,1000],\n                'learning_rate' : [0.2,0.4,0.6,0.8,1,1.2,1.4],\n                'algorithm'     : ['SAMME', 'SAMME.R']\n               }\n             ]\n\nclassifier = AdaBoostClassifier()\n\ngrid_search = GridSearchCV(classifier, param_grid, cv=5,\n                           scoring='accuracy', return_train_score=True)\n\ngrid_search.fit(X_train_prep, y_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T00:22:27.621522Z","start_time":"2018-12-30T00:22:27.61452Z"},"trusted":false,"_uuid":"83546601aae65ebced647b3527002cb4a19840b9"},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd6f8311ea8726ff3d7571854fd00847e605fd56"},"cell_type":"markdown","source":"# Evaluating accuracy on the true test data"},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T00:26:49.749972Z","start_time":"2018-12-30T00:26:49.721971Z"},"trusted":false,"_uuid":"69461b7409ce6fa84f0f20e906c76c15e5f1ad0f"},"cell_type":"code","source":"X_test = test.drop(['Attrition'],axis=1)\ny_test = test['Attrition']\n\nX_test_prep = mergeCols_pipeline.transform(X_test)\n\ntest_predictions = grid_search.best_estimator_.predict(X_test_prep)\naccuracy_score(y_test, test_predictions)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}