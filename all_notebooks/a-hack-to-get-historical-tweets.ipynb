{"cells":[{"metadata":{},"cell_type":"markdown","source":"<CENTER><H1>A hack to get historical tweets</H1></CENTER>\n\n\n<BR>\n    \n* This notebook is based on [a previous notebook](https://www.kaggle.com/keitazoumana/get-tweets-about-a-topic-from-twitter-in-5-steps) by @keitazoumana \n* Have a look to this [medium article](https://medium.com/@jcldinco/downloading-historical-tweets-using-tweet-ids-via-snscrape-and-tweepy-5f4ecbf19032) to have an intuition on snscrape\n\n<BR>\n    \nBe aware that the most of the code will not work here, since it requires you API Tweeter credentials. The pourpose of this notebook is merely didactic."},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n1. [Introduction](#section1)\n2. [Loading AP credentials from a json file](#section2)\n3. [Perform your query](#section3)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section1\"></a>\n# 1. Introduction\n\nThe current tweet API (tweepy) doesn't allow queries longer than one week. It is difficult to get tweets with a certain hashtag (a great amount of tweets) with scrapping techniques, because Tweeter removed **/i/search/timeline'** end of the advanced search window. The [snscrape module](https://github.com/JustAnotherArchivist/snscrape) allows one to scrape tweets without the restrictions of Tweepy, but using it programmatically is poorly documented. That's the reason I digged into the code and ofer a solution to include it in your Python code."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# import necessary modules\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In principe, we can get a DataFrame like the [sentiment analysis on Climate change](https://www.kaggle.com/joseguzman/climate-sentiment-in-twitter)"},{"metadata":{"trusted":true},"cell_type":"code","source":"climate = pd.read_csv('../input/climate-sentiment-in-twitter/Climate_twitter.csv', index_col= 'id')\nclimate.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install tweepy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install git+https://github.com/JustAnotherArchivist/snscrape.git","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section2\"></a>\n# 2. Load API credentials from a json file\n\nUse this video to get your API developer. Get your credentials with the [help of this video](https://www.youtube.com/watch?v=PqqXjwoDQiY) and put in a json file like this:\n\n```json\n{\n    \"_comment\": \"create you account in https://developer.twitter.com/\",\n    \"api_key\": \"YOUR_API_KEY\",\n    \"api_secret\": \"YOUR_API_SECRET\",\n    \"access_token\": \"YOUR_ACCESS_TOKEN,\n    \"access_secret\":\"YOUR_ACCESS_SECRET\"\n}\n```\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tweepy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_credentials(json_file):\n    \"\"\"\n    Reads the json file and start the tweeter API.\n    Requires a json file with 'api_key', 'api_secret', \n    'access_token' and 'access_secret'\n    \n    Argument:\n    ---------\n    json_file (str)\n        path to the json file\n        \n    Returns:\n    The tweeter API\n    \"\"\"\n    with open(jsonfile) as fp:\n        log = json.load(fp)\n        auth = tweepy.OAuthHandler(log['api_key'], log['api_secret'])\n        auth.set_access_token(log['access_token'], log['access_secret'])\n    \n    myAPI = tweepy.API(auth, wait_on_rate_limit = True)\n    \n    return( myAPI)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import snscrape.modules.twitter as sntwitter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The key now is to use the sntwitter object to obtain tweeter IDs, as follows\n\n```python\nmysearch = f'{search_item} -filter:retweets since:{since} until:{until}'\n# collect tweet ids!!\nmyscraper = sntwitter.TwitterSearchScraper(mysearch).get_items()\n```\n\nNow, you can create a function that returns a Pandas DataFrame object with the results of your query"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section3\"></a>\n# 3. Perform your query"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef search(search_item, since = '2020-01-01', until = '2020-12-15', count = 100):\n    \"\"\"\n    Searchs the item in tweets and returns a Pandas DataFrame.\n\n    Arguments:\n    ----------\n    search_item (str)\n        \n    Example:\n    search_term = \"#climate+change -filter:retweets\" to avoid retweets\n        \n    \"\"\"\n    mysearch = f'{search_item} -filter:retweets since:{since} until:{until}'\n    # collect tweet ids!!\n    myscraper = sntwitter.TwitterSearchScraper(mysearch).get_items()\n        \n    tweets_id = list()\n    for i, tweet in enumerate(myscraper):\n        if i>count:\n            break\n        tweets_id.append([tweet.id, tweet.date, tweet.content, tweet.username])\n\n    # Status object from the Tweeter API\n    mytweets = list()\n    for myid in np.array(tweets_id)[:,0]:\n        status = myAPI.get_status(myid, tweet_mode=\"extended\")\n        parsed_tweet = dict()\n        # date and time of creation\n        parsed_tweet['date'] = status.created_at\n        # times the tweet is re-tweeted\n        parsed_tweet['retweets'] = st1atus.retweet_count\n        # which platform used to post\n        parsed_tweet['source'] = status.source\n        # location where posted\n        #parsed_tweet['geo'] = tweet.geo\n        # Name of the user posting\n        parsed_tweet['author'] = status.user.name\n        # number of likes\n        parsed_tweet['likes'] = status.favorite_count\n\n        parsed_tweet['id'] = status.user.id\n        # tweet posted (without URLs)\n        parsed_tweet['text'] = remove_url( status.full_text)\n        # Screen name of the USER\n        parsed_tweet['twitter_name'] = status.user.screen_name\n        # \n        parsed_tweet['location'] = status.user.location\n        # if user is verified\n        parsed_tweet['verified'] = status.user.verified\n        # number of followers\n        parsed_tweet['followers'] = status.user.followers_count\n        # number of people the user follows\n        parsed_tweet['friends'] = status.user.friends_count\n\n        mytweets.append(parsed_tweet)\n\n    df_tweet = pd.DataFrame(mytweets)\n    # just in case, remove duplicates\n    df_tweet.drop_duplicates('text', keep='first', inplace=True)\n    df_tweet.set_index('id', inplace=True)\n        \n    return(df_tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search(search_item = \"#climate+change -filter:retweets', since = '2020-01-01', until = '2020-12-15', count = 1000):","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}