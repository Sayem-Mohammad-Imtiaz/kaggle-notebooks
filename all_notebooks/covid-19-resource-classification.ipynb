{"cells":[{"metadata":{},"cell_type":"markdown","source":"***Import Packages***"},{"metadata":{"_uuid":"91534c55-a219-4d94-b60d-f0d18fd8445a","_cell_guid":"8c3cf103-ae5a-446b-bfa5-7e7a1f726ed2","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport json\nimport os\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Load Dataset***"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Helper Functions***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Helper Functions***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dec0274-7a13-4faf-8c9b-ee73f5949ea5","_cell_guid":"8415a09d-49f4-471c-a426-2fa9b839a949","trusted":true},"cell_type":"code","source":"all_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir +\"/\"+ filename\n    file =json.load(open(filename, 'rb'))\n    all_files.append(file)\n\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\ncleaned_files = []\n    \nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n\n    ]\n\n    cleaned_files.append(features)\n\ncol_names = ['paper_id', 'title', 'authors',\n             'affiliations', 'abstract', 'text',]\n\ndf = pd.DataFrame(cleaned_files, columns=col_names)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['abstract_word_count'] = df['abstract'].apply(lambda x: len(x.strip().split()))\ndf['body_word_count'] = df['text'].apply(lambda x: len(x.strip().split()))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Drop dublicates data from text and abstract data***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(['abstract', 'text'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(['title','abstract'], inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Preprocessing***\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = df.head(10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndf['text'] = df['text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf['text'] = df['text'].apply(lambda x: re.sub('\\n\\n',' ',x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('\\n\\n',' ',x))\ndf['text'] = df['text'].apply(lambda x: re.sub('\\d+', '',x))\ndf['abstract'] = df['abstract'].apply(lambda x: re.sub('\\d+', '',x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf['text'] = df['text'].apply(lambda x: x.lower())\ndf['abstract'] = df['abstract'].apply(lambda x: x.lower())\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df.drop([\"paper_id\", \"abstract\", \"abstract_word_count\", \"body_word_count\", \"authors\", \"title\", \"affiliations\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs = []\nfor x in range(0,len(text)):\n    docs.append(str(text.iloc[x]['text']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(docs[5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Remove Stopwords***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\nwith open('../input/stopwords/englishStopwords.txt', 'r') as f:\n    myLists = [line.strip() for line in f]\n\n               \nvectorizer = TfidfVectorizer(stop_words=myLists)\nX = vectorizer.fit_transform(docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# test set size of 20% of the data and the random seed 42 <3\nX_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n\nprint(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nk = 10\nkmeans = KMeans(n_clusters=k, n_jobs=4, verbose=10)\ny_pred = kmeans.fit_predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = kmeans.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outerlist = []\nwhile len(outerlist) < k:\n    outerlist.append([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in docs:\n    Y = vectorizer.transform([x])\n    prediction = kmeans.predict(Y)\n    outerlist[int(prediction)].append(x)\n\norder_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\n\n\nindex = 0\n\n\nfor iter in outerlist:\n    print(\"DOCUMENTS GROUP %d\" % index)\n    print(iter[:1], sep=', ')\n    print(\" \")\n    \n    print(\"-----------------\")\n\n\n    print(\"GROUP DESCRIPTIVE KEYWORDS\" )\n    for ind in order_centroids[index, :10]:\n        print(' %s' % terms[ind]),\n    index = index + 1\n    print(\"-----------------\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n    \nprint(model_name, \":\\n\")\nprint(\"Accuracy Score: \", '{:,.3f}'.format(float(accuracy_score(y_test, y_pred)) * 100), \"%\")\nprint(\"F1 score: \", '{:,.3f}'.format(float(f1_score(test, pred, average='micro')) * 100), \"%\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}