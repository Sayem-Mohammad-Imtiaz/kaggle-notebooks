{"cells":[{"metadata":{"_uuid":"1162235022f4161822faea9bc4235dc3739a5392"},"cell_type":"markdown","source":"**MUSHROOMS CLASSIFICATION**\n\n<a title=\"MichaelMaggs [CC BY-SA 2.5 (https://creativecommons.org/licenses/by-sa/2.5)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Amanita_muscaria_(fly_agaric).JPG\"><img width=\"128\" alt=\"Amanita muscaria (fly agaric)\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Amanita_muscaria_%28fly_agaric%29.JPG/128px-Amanita_muscaria_%28fly_agaric%29.JPG\"></a>\n\n\nIn this kernel we will investigate the mushrooms classification database.\nSimple EDA and classification with logistic regression will be performed. Data for classification will be one-hot-encoded. Based on the analysis results features importance will be revealed. At the end we will see how the classfier will behave if we select few arbitrary selected variables (ROC will be plotted)\n\nReading necesary libraries. We will use [sklearn](https://scikit-learn.org/stable/), [pandas](https://pandas.pydata.org/), [numpy](http://www.numpy.org/) and [matplotlib](https://matplotlib.org/)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, recall_score, accuracy_score, roc_curve, precision_score, roc_auc_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99080083015f9ebc96143a49ab574eb4f2dd7e80"},"cell_type":"markdown","source":"**Reading raw data from .csv file.**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/mushrooms.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a8ffbf785cabdff125e2bdd27cca0e0c6e5d79d"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a800ecb7aacf40b027d1b56600be40a1cfa2fc8d"},"cell_type":"markdown","source":"**All columns are categorical. Let's explore them by looking at bar charts.**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"fa0445e223ee4e05628d8e4d46ce2ace0f05c9ab"},"cell_type":"code","source":"def graph(name, u):\n    data[name].value_counts().plot(kind=\"bar\",ax=u, color=colors)\n    \n    plt.setp(u.get_xticklabels(), rotation=0)\n    u.grid(True)\n    u.set_title(name.replace(\"-\",\" \"), fontsize=11, fontdict={\"fontweight\": \"bold\"})\n    \n    for p in u.patches:\n        text = str(int(p.get_height()))\n        u.annotate(text, (p.get_x()+p.get_width()/2, p.get_height()+100),\n                   ha=\"center\", va='center', fontsize=7, fontweight=\"bold\")\n\n###############################################################################\n# EXPLORATORY DATA ANALYSIS\n\nfig2, ax2 = plt.subplots(3,2, figsize=(11, 10), gridspec_kw={\"wspace\" : 0.4, \"hspace\" : 0.3, \"top\": 0.95})\n\ncolors=[\"#0019ff\",\"#f44809\",\"#af00af\",\"#00af23\",\"#00af23\"]\n\ngraph(\"stalk-shape\",ax2[0,0])\ngraph(\"stalk-root\",ax2[0,1])\ngraph(\"stalk-surface-above-ring\",ax2[1,0])\ngraph(\"stalk-surface-below-ring\",ax2[1,1])\ngraph(\"stalk-color-above-ring\",ax2[2,0])\ngraph(\"stalk-color-below-ring\",ax2[2,1])\nplt.rcParams['axes.axisbelow'] = True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"707122879e4f27c64aff405c07f1c3da1721228e"},"cell_type":"markdown","source":"**Data have to be encoded for classification. One-hot-encoding with pandas_get_dummies() will be performed. Binary labels will be encoded using sklearn LabelEncoder().**"},{"metadata":{"trusted":true,"_uuid":"a7671c868adca459de8265f3a7b43412f9d8dfb3"},"cell_type":"code","source":"X = data.drop([\"class\"], axis=1)\ny = data[\"class\"]\nX = pd.get_dummies(X)\n\nle = LabelEncoder()\ny = le.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38d8c3d87368ed77a1e16102467c875cee8dd130"},"cell_type":"markdown","source":"**Classification and metrics for logistic regression classifier**"},{"metadata":{"trusted":true,"_uuid":"7e79c2ae02de04d5952329842ad9b4c2d990dca6"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\nclf = LogisticRegression(solver=\"lbfgs\").fit(X_train,y_train)\npredicted = clf.predict(X_test)\npredicted_proba = clf.predict(X_test)\n\nprint(\"Accuracy is: \"+ str(clf.score(X_test,y_test)))\nprint(\"Recall score is: \" + str(round(recall_score(y_test, predicted),3)))\nprint(\"Precision score is: \" + str(round(precision_score(y_test, predicted),3)))\nprint(\"F1 score is: \" + str(round(f1_score(y_test, predicted),3)))\nprint(\"\\nConfusion matrix:\")\nprint(confusion_matrix(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41354d068de413d00354d8f551340ba5228ebc7e"},"cell_type":"markdown","source":"**Very good result! Let's check what are the most influential variables. Let's select TOP20 of them.**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"157c8e1d6b09f0396d7c72da81de6f85a7cf2801"},"cell_type":"code","source":"values = clf.coef_[0]\nnames = X_train.columns\n\nimportance = pd.DataFrame({\"value\": values, \"name\": names}).sort_values(\"value\")\nimportance = importance.set_index(\"name\")\n\n# TOP20 FACTORS\ntop20 = pd.concat([importance[\"value\"].head(10),importance[\"value\"].tail(10)])\n\nfig, ax = plt.subplots(figsize=(12,5), gridspec_kw={\"top\": 0.90, \"bottom\":0.05, \"left\":0.2})\n\ntop20.plot.barh(ax=ax)\n\nplt.rcParams['axes.axisbelow'] = True\nplt.ylabel(\"variable name\")\nplt.grid(True)\nplt.title(\"Classification - TOP20 features (importance)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18bc9dd5fcc2a8b346b8400a68c6c9a835466ae2"},"cell_type":"markdown","source":"**SMALL EXPERIMENT - let's choose only 4 ARBITRARY variables and see classificator performance **"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e723b7b924b2fd2eb50b80c8a5b340a94251205c"},"cell_type":"code","source":"# small experiment - arbitrary selected variables\nX2 = data[[\"bruises\",\"stalk-root\",\"stalk-shape\",\"habitat\"]]\nX2 = pd.get_dummies(X2)\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=0.2)\n\nclf2 = LogisticRegression(solver=\"lbfgs\").fit(X2_train, y_train)\n\npredicted = clf2.predict(X2_test)\n\npredicted_proba = clf2.predict_proba(X2_test)\npredicted_proba = pd.DataFrame(predicted_proba)[1]\n\nroc_auc=roc_auc_score(y_test, predicted)\n\nfpr, tpr, thresholds = roc_curve(y_test, predicted_proba, drop_intermediate=False)\n\nfig2, ax2 = plt.subplots(figsize=(10, 5), gridspec_kw={\"wspace\" : 0.4, \"hspace\" : 0.3, \"top\": 0.95})\nplt.plot(fpr,tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.grid(True)\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.xticks([i for i in np.arange(0,1.1,0.1)])\nplt.yticks([i for i in np.arange(0,1.1,0.1)])\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for 4 variables only')\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bc941ba1840b750c3fa2b87d39566951b3de36e"},"cell_type":"markdown","source":"**MORE TO COME**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}