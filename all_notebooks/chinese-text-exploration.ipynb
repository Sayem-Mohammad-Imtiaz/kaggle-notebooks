{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Chinese News Text Exploration</h1>\n\n# Content\n\n* Introduction\n* Prepare for data analysis  \n    * Load packages  \n    * Load data  \n* Data exploration  \n    * Glimpse the data\n    * Missing data\n    * Unique values  \n    * Date extraction  \n    * Chinese tokens extraction  \n    * Extract landing page  \n    * Extract image main page\n    * Data visualization  \n    * Frequent tokens  \n* References "},{"metadata":{},"cell_type":"markdown","source":"# Introduction   \n\n\nWe will explore a Chinese news dataset (with Chinese traditional, Chinese simplified as well as Western characters text) to highlight what is specific to Chinese when we are doing text exploration.  \n\nWe will introduce the use of a special library for cutting Chinese text in `words` i.e. group of ideograms that define together a concept.\n\nMain credits for the utility functions for processing Chinese text goes to <a href=\"https://www.kaggle.com/johnfarrell\">Jiazhen Xi</a> and <a href=\"https://www.kaggle.com/naivelamb\">Xuan Cao</a>, from which I got the function to represent Chinese characters with Seaborn plots and the idea to use `jieba` for cutting Chinese texts in `words`.\n\nIn the reference section of the Kernel are given the respective Kernels links."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Prepare the data analysis\n\n\n## Load packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS\nfrom joblib import Parallel, delayed\nimport tqdm\nimport jieba\nimport time\nimport matplotlib.font_manager as fm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_PATH = \"../input\"\nDATA_PATH = os.path.join(INPUT_PATH, os.listdir(INPUT_PATH)[0])\nnews_data_df = pd.read_csv(os.path.join(DATA_PATH, \"news_collection.csv\"), low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{},"cell_type":"markdown","source":"## Glimpse the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_data_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(news_data_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All data types are of type `object` besides the date, which is encoded as an `int64`. 3.83% of the descriptions `desc` are missing and 0.3% of images."},{"metadata":{},"cell_type":"markdown","source":"## Unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values(news_data_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that not only `title` but also some of the texts descriptions (`desc`) are not unique (there are duplicates). Also `image` and `url` are in a small proportion not uniques. As for `source` and `date` are like categories.\n\nLet's see first the most frequent values for each category."},{"metadata":{},"cell_type":"markdown","source":"## Most frequent values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals / total * 100, 3)\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_frequent_values(news_data_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most frequent title is the main page of `SGSME.SG` (and the article is actually a link to the landing page of this news source). Most frequent description is `...` and most frequent source is `芋傳媒` (Taro Media)). As for date, the most frequent date is March 13, 2019, with 756 entries."},{"metadata":{},"cell_type":"markdown","source":"## Date extraction\n\n\nLet's extract year, month, day from date."},{"metadata":{"trusted":true},"cell_type":"code","source":"news_data_df['year'] = news_data_df['date'].apply(lambda x: str(x)[0:4])\nnews_data_df['month'] = news_data_df['date'].apply(lambda x: str(x)[4:6])\nnews_data_df['day'] = news_data_df['date'].apply(lambda x: str(x)[6:8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the new values."},{"metadata":{"trusted":true},"cell_type":"code","source":"news_data_df[['title', 'date', 'year', 'month', 'day']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Chinese tokens extraction\n\nIn order to extraxt chinese tokens, we will use the `jieba` library."},{"metadata":{"trusted":true},"cell_type":"code","source":"def jieba_tokens(x, sep=' ', cut_all_flag=False):\n    '''\n    input: x - text in Chines to cut\n    input: sep - separator to use in the output\n    input: cut_all_flag - cut in individual ideograms rather than in concepts (groups of ideograms). \n    function: cut the text in Chinese in group of ideograms (or individual ideograms)\n    output: the text cut in group of ideograms (or ideograms)\n    '''\n    try:\n        return sep.join(jieba.cut(x, cut_all=cut_all_flag))\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start with the title."},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nnews_data_df['proc_title'] = Parallel(n_jobs=4)(delayed(jieba_tokens)(x) for x in tqdm.tqdm_notebook(news_data_df['title'].values))\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We process in the same time the description `desc`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nnews_data_df['proc_desc'] = Parallel(n_jobs=4)(delayed(jieba_tokens)(x) for x in tqdm.tqdm_notebook(news_data_df['desc'].values))\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract landing page\n\nLet's extract the main homepage for each article."},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nnews_data_df['main_url'] = news_data_df['url'].apply(lambda x: x.split('/')[2])\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_data_df[['main_url', 'url']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract image main page\n\nLet's extract in a similar way the location of main page for the image."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_main_image(x):\n    try:\n        return x.split('/')[2]\n    except:\n        return x\n\nstart_time = time.time()\nnews_data_df['main_image'] = news_data_df['image'].apply(lambda x: get_main_image(x))\nprint(f\"Total processing time: {round(time.time()-start_time,2)} sec.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_data_df[['main_image', 'image', 'main_url', 'url']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Values distribution"},{"metadata":{},"cell_type":"markdown","source":"We are using a special character set to represent the chinese categories or words. We download the "},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://github.com/adobe-fonts/source-han-sans/raw/release/SubsetOTF/SourceHanSansCN.zip\n!unzip -j \"SourceHanSansCN.zip\" \"SourceHanSansCN/SourceHanSansCN-Regular.otf\" -d \".\"\n!rm SourceHanSansCN.zip\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"font_path = './SourceHanSansCN-Regular.otf'\nfont_prop = fm.FontProperties(fname=font_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_count(feature, title, df, font_prop=font_prop, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:25], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    ax.set_xticklabels(ax.get_xticklabels(), fontproperties=font_prop);\n    plt.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Main pages of image source"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('main_image', 'Most frequent images sources (first 25 from all data)', news_data_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Main pages of news sites"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('main_url', 'Most frequent main pages of news sites (first 25 from all data)', news_data_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Source"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('source', 'Most frequent sources (first 25 from all data)', news_data_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Year"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('year', 'Year', news_data_df, size=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('month', 'Month', news_data_df, size=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Day"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('day', 'Day', news_data_df, size=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Title\n\nFor title and description we will use another mode of presentation, we will just show a table with frequency and text."},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_frequent_texts(feature, df):\n    total = float(len(df))\n    dd = pd.DataFrame(df[feature].value_counts().index[:10], columns = ['Item'])\n    dd['Frequency'] = df[feature].value_counts().values[:10]\n    dd['Source'] = df['source']\n    dd['Landing page'] = df['main_url']\n    return dd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_frequent_texts('title', news_data_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Desc (description)"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_frequent_texts('desc', news_data_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequent tokens\n\nWe continue now with exploring the frequent tokens extracted from the titles and descriptions, currently stored in `proc_title` and `proc_desc`."},{"metadata":{"trusted":true},"cell_type":"code","source":"prop = fm.FontProperties(fname=font_path, size=20)\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, font_path=font_path, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        font_path=font_path,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        prop = fm.FontProperties(fname=font_path)\n        fig.suptitle(title, fontsize=40, fontproperties=prop)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Title wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(news_data_df['proc_title'], font_path, title = 'Prevalent words in title, all data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Desc wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(news_data_df['proc_desc'], font_path, title = 'Prevalent words in desc, all data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References  \n\n[1] Jiazhen Xi, Chinese News WordCloud EDA, https://www.kaggle.com/johnfarrell/chinese-news-wordcloud-eda\n[2] Xuan Cao, Chinese News WordCloud EDA - Sometimes Naive!, https://www.kaggle.com/naivelamb/chinese-news-wordcloud-eda-sometimes-naive"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}