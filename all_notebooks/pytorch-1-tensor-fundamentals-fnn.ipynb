{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyTorch Journey Part 1\nHi all, this notebook contains the first episode of my **PyTorch** Series. \n\nThe other notebooks of this series:\n* [Part 2: CNN & Gradient Accumulation](https://www.kaggle.com/milankalkenings/pytorch-2-cnn-gradient-accumulation/edit)\n* [Part 3: (Batch) Normalization](https://www.kaggle.com/milankalkenings/pytorch-3-batch-normalization)\n\n<h1 style=\"background-color:SteelBlue; color:white\" >-> Content:</h1>\n\n## 0. [Prerequisits](#sec1)\n\n## 1. [Tensor Fundamentals](#sec2)\n#### 1.1. [Change the Appearance of Tensors](#sec21)\n#### 1.2. [Tensor Broadcasting](#sec22)\n#### 1.3. [Use Tensors on GPUs](#sec23)\n#### 1.4. [Autograd](#sec24)\n\n## 2. [Linear Layers](#sec3)\n\n## 3. [FNN & Trainloop](#sec4)\n#### 3.1. [TensorDatasets](#sec41)\n#### 3.2. [Samplers](#sec42)\n#### 3.3. [DataLoaders](#sec43)\n#### 3.4. [Create the Model & Investigate it](#sec44)\n#### 3.5. [Train and Validation Loop](#sec45)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 0. Prerequisits</h1>\n\n\n1. **python fundamentals:** [This simple & free Kaggle Course](https://www.kaggle.com/learn/python) is already enough!\n2. **notebooks & numpy:** [Chapter 1&2 of this free book](https://jakevdp.github.io/PythonDataScienceHandbook/) is probably the best way to learn it! \n\nFrom now on I expect you all to be familiar with the concepts used in the named sources. I don't expect any further python skills.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 1. Tensor Fundamentals</h1>\n\nThere are multiple ways of interpreting tensors. Physicists have a different view on them as we have. Our point of view is still valid in our use cases of them:\n\nA tensor can be interpreted as an array. We can create arrays of arrays; arrays of arrays of arrays and so on.\n\n* a scalar $\\in \\mathbb{R}$ can be interpreted as a tensor of rank 0\n* an array of scalars / a vector $\\in \\mathbb{R}^n$ can be interpreted as a tensor of rank 1\n* an array of arrays of scalars / a matrix $\\in \\mathbb{R}^{n \\text{ x } m}$ can be interpreted as a tensor of rank 2\n* an array of arrays of arrays of scalars $\\in \\mathbb{R}^{n \\text{ x } m \\text{ x } k}$ can be interpreted as a tensor of rank 3, and so on..\n\nPyTorch tensors work in the same way as numpy arrays but have some beneficial properties as we will see later on.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n# create a tensor from a list or a np array\narray = [[[1,2, 3, 4], \n          [5, 6, 7, 8],\n          [0, 0, 0, 1]],\n         \n         [[4, 3, 2, 1],\n          [8, 7, 6, 5],\n          [1, 0, 0, 0]]\n        ]\n\n# \"bridge\" to torch tensors\ntensor = torch.tensor(array)\nprint(tensor, \"\\n\\n\")\n\n# get the dimensionality of the tensor\nprint(\"size =\", tensor.size())\n\n# get the dtype of the tensor\nprint(\"dtype =\", tensor.dtype)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.756637Z","iopub.execute_input":"2021-07-03T15:33:37.757077Z","iopub.status.idle":"2021-07-03T15:33:37.766796Z","shell.execute_reply.started":"2021-07-03T15:33:37.757039Z","shell.execute_reply":"2021-07-03T15:33:37.765309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the *size* of the tensor tells us that this is a tensor of rank 3 consisting of 2 tensors of rank 2, in which 3 tensors of rank 1 are stored. In each of these tensors of rank 1 are 4 tensors of rank 0.","metadata":{}},{"cell_type":"markdown","source":"In some cases it might be necessary to bridge from tensors to numpy arrays:","metadata":{}},{"cell_type":"code","source":"print(\"tensor:\\n\", tensor.numpy())\nprint(\"\\ntype:\\n\", type(tensor.numpy()))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.768878Z","iopub.execute_input":"2021-07-03T15:33:37.769315Z","iopub.status.idle":"2021-07-03T15:33:37.789025Z","shell.execute_reply.started":"2021-07-03T15:33:37.769269Z","shell.execute_reply":"2021-07-03T15:33:37.787839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec21\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 1.1. Change the Appearance of Tensors</h2>\n\nThe tensor from the example above is of *dtype=torch.int64*. We might change this in later operations. Casting the *dtype* of a tensor is as easy as:","metadata":{}},{"cell_type":"code","source":"print(\"tensor double:\\n\", tensor.double())","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.79104Z","iopub.execute_input":"2021-07-03T15:33:37.791389Z","iopub.status.idle":"2021-07-03T15:33:37.80032Z","shell.execute_reply.started":"2021-07-03T15:33:37.791347Z","shell.execute_reply":"2021-07-03T15:33:37.799023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`tensor.view()` works siilar to `numpy.reshape()`. I use it in the folowing example to transform a tensor of rank 3 into a tensor of rank 2. Try out different sizes to get a feeling for the results!","metadata":{}},{"cell_type":"code","source":"print(\"reshaped:\\n\", tensor.reshape(2, 12))\nprint(\"\\nsize:\", tensor.reshape(2, 12).size())","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.802407Z","iopub.execute_input":"2021-07-03T15:33:37.802881Z","iopub.status.idle":"2021-07-03T15:33:37.811768Z","shell.execute_reply.started":"2021-07-03T15:33:37.802834Z","shell.execute_reply":"2021-07-03T15:33:37.810157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"reshaped:\\n\", tensor.reshape(3, 8))\nprint(\"\\nsize:\", tensor.reshape(3, 8).size())","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.813792Z","iopub.execute_input":"2021-07-03T15:33:37.81428Z","iopub.status.idle":"2021-07-03T15:33:37.824693Z","shell.execute_reply.started":"2021-07-03T15:33:37.814224Z","shell.execute_reply":"2021-07-03T15:33:37.823377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To conclude: having a rank $n$ tensor, reshaping it results in filling the desired shape beginning with the first element of the first rank $n-1$ tensor. ","metadata":{}},{"cell_type":"markdown","source":"Data oftentimes has a format similar to the following:","metadata":{}},{"cell_type":"code","source":"tensor = tensor.reshape(1, 2, 1, 12)\nprint(\"tensor:\\n\", tensor)\nprint(\"\\nsize:\", tensor.size())","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.826562Z","iopub.execute_input":"2021-07-03T15:33:37.827034Z","iopub.status.idle":"2021-07-03T15:33:37.837399Z","shell.execute_reply.started":"2021-07-03T15:33:37.826985Z","shell.execute_reply":"2021-07-03T15:33:37.836011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The method [squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html) was created to handle exactly this problem, when some dimensions contain only one value. It can be applied on one specific dimension, or on all dimensions at once.","metadata":{}},{"cell_type":"code","source":"tensor_squeezed = tensor.squeeze(dim=0)\nprint(\"tensor:\\n\", tensor_squeezed)\nprint(\"\\nsize:\", tensor_squeezed.size())","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.840144Z","iopub.execute_input":"2021-07-03T15:33:37.840482Z","iopub.status.idle":"2021-07-03T15:33:37.852418Z","shell.execute_reply.started":"2021-07-03T15:33:37.840451Z","shell.execute_reply":"2021-07-03T15:33:37.851371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor_squeezed = tensor.squeeze()\nprint(\"tensor:\\n\", tensor_squeezed)\nprint(\"\\nsize:\", tensor_squeezed.size())","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.854438Z","iopub.execute_input":"2021-07-03T15:33:37.854829Z","iopub.status.idle":"2021-07-03T15:33:37.863229Z","shell.execute_reply.started":"2021-07-03T15:33:37.854783Z","shell.execute_reply":"2021-07-03T15:33:37.862407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec22\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 1.2. Tensor Broadcasting</h2>\n\nsimilar to numpy arrays, we can add, subtract, multiply ... 2 tensors. \n\nThe following example shows that tensor operations stick to the rules of **numpy broadcasting** as explained in [this chapter](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) of the book listed in the prerequisits:\n\nif we add a tensor **a of rank 2** to a tensor **b of rank 1**, b is will be added to each tensor of rank 1 stored in a","metadata":{}},{"cell_type":"code","source":"tensor_a = torch.tensor([[1, 2, 3],\n                         [4, 5, 6]])\n\ntensor_b = torch.tensor([7, 8, 9])\n\ntensor_a + tensor_b","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.864252Z","iopub.execute_input":"2021-07-03T15:33:37.864526Z","iopub.status.idle":"2021-07-03T15:33:37.878495Z","shell.execute_reply.started":"2021-07-03T15:33:37.864499Z","shell.execute_reply":"2021-07-03T15:33:37.87732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec23\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 1.3. Use Tensors on GPUs</h2>\n\n\nOne of the most important benefits of tensors is that you can perform tensor operations on the GPU (if you have one):","metadata":{}},{"cell_type":"code","source":"# only possible if ýour NVIDEA GPU is activated \nif torch.cuda.is_available():        \n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# move them to the gpu,\n# all operations on them are performed on gpu\n# both ahve to be on the same device\ntensor_a.to(device)\ntensor_b.to(device)\n\ntensor_a + tensor_b","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:37.88021Z","iopub.execute_input":"2021-07-03T15:33:37.880608Z","iopub.status.idle":"2021-07-03T15:33:37.893812Z","shell.execute_reply.started":"2021-07-03T15:33:37.880576Z","shell.execute_reply":"2021-07-03T15:33:37.892838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec24\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 1.4. Autograd</h2>\n\nWe can describe every tensor operation as a function. PyTorch Autograd allows us to calculate the Gradient of such a function with respect to each individual input. Let me give you an example. If you are not familiar with matrix calculus, you can always refer to [this](https://en.wikipedia.org/wiki/Matrix_calculus):\n\n$$f:\\mathbb{R}^2\\rightarrow \\mathbb{R}, f(x) = x^Tx+1 = x_1^2+x_2^2 +1$$\n\nHas the following partial derivatives:\n$$\\frac{\\partial f(x)}{\\partial x_1}=2x_1$$\n\n$$\\frac{\\partial f(x)}{\\partial x_2}=2x_2$$\n\nSo we would obtain: \n\n\n$$f(\\left(\\begin{array}{c} 2 \\\\ 3 \\end{array}\\right))=2^2+3^2+1=14$$\n\n$$\\frac{\\partial f(x)}{\\partial x_1}=4$$\n\n$$\\frac{\\partial f(x)}{\\partial x_2}=6$$","metadata":{}},{"cell_type":"code","source":"def f(x):\n    return x.t()@x + 1\n\nx = torch.tensor([[2], \n                  [3]],dtype=torch.float32)\n\n# requires_grad_() enables autograd functionality for that tensor\n# the underscore in the end denotes inplace operations in PyTorch\nx.requires_grad_()\n\ny = f(x)\n\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:38.010898Z","iopub.execute_input":"2021-07-03T15:33:38.011454Z","iopub.status.idle":"2021-07-03T15:33:38.019501Z","shell.execute_reply.started":"2021-07-03T15:33:38.011419Z","shell.execute_reply":"2021-07-03T15:33:38.018259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.backward() # calculates the gradient w.r.t. every input\nx.grad","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:38.021188Z","iopub.execute_input":"2021-07-03T15:33:38.021573Z","iopub.status.idle":"2021-07-03T15:33:38.033338Z","shell.execute_reply.started":"2021-07-03T15:33:38.021535Z","shell.execute_reply":"2021-07-03T15:33:38.032351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We automatically calculated the gradient w.r.t. every input of the function.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 2. Linear Layers</h1>\n\nPyTorch combines tensors and tensor operations to modules, which can then be used in artificial neural networks. The most simple module is the [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer.\n\nLet's create such a layer from scratch!\n\nGiven some input $x$, the linear layer computes its output $y$ via $y=xA^T+b$. Both, the weight matrix $A$ and the bias $b$ will be adapted during the training process so that the output $y$ becomes as close as possible to the ground truth.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass CustomLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        A = torch.randn(size=(out_features, in_features))\n        self.weight = nn.Parameter(data=A, requires_grad=True)  # weight\n        b = torch.randn(size=(out_features,))\n        self.bias = nn.Parameter(data=b, requires_grad=True)  # bias\n        \n    def forward(self, x):\n        return x @ self.weight.t() + self.bias","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:38.035008Z","iopub.execute_input":"2021-07-03T15:33:38.035471Z","iopub.status.idle":"2021-07-03T15:33:38.044603Z","shell.execute_reply.started":"2021-07-03T15:33:38.035422Z","shell.execute_reply":"2021-07-03T15:33:38.04377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `CustomLinear` module is a simplified implementiation of the [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) module. The fundamentla functionalities (forward and backward) are the same and `CustomLinear` can be used in a fully connected layer already. Note that this layer is nothing else but a combination of tensors, some of them with `requires_grad=True`, and tensor operations. Note that `A` and `b` are `nn.Parameters`, and thus are iteratively altered during the training process to improve the performance of the module. I renamed `A` and `b` to `weight` and `bias` respectively. to match the pattern of the original linear layer.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec4\"></a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 3. FNN & Trainloop</h1>\n\nNow that we have a feeling for the deep learning perspective on tensors and how to calculate gradients from tensor operations, we can finally train our first neural network.\n\nI will use a **FNN** (Feed Forward Neural Network) to explain the most important components of the training process.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec41\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 3.1. TensorDatasets</h2>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import TensorDataset\n\n# train\ndata = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")\nX = torch.tensor(data.drop([\"label\"], axis=1).values, dtype=torch.float).to(device)\ny = torch.tensor(data[\"label\"].values, dtype=torch.long).to(device)\ndata_t = TensorDataset(X, y)\n\n# val\ndata = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\nX = torch.tensor(data.drop([\"label\"], axis=1).values, dtype=torch.float).to(device)\ny = torch.tensor(data[\"label\"].values, dtype=torch.long).to(device)\ndata_v = TensorDataset(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:38.046505Z","iopub.execute_input":"2021-07-03T15:33:38.046862Z","iopub.status.idle":"2021-07-03T15:33:43.054685Z","shell.execute_reply.started":"2021-07-03T15:33:38.04683Z","shell.execute_reply":"2021-07-03T15:33:43.053686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each entry of a TensorDataset contains the independend and the  target / dependend variable(s) of one observation.","metadata":{}},{"cell_type":"code","source":"obs_1 = data_t[0]\n\nprint(\"Type of the first observation:\\n\", type(obs_1), \"\\n\")\nprint(\"The first observation:\\n\", obs_1, \"\\n\")\nprint(\"The independend variables of the first observation:\\n\", obs_1[0], \"\\n\")\nprint(\"The target of the first observation:\\n\", obs_1[1], \"\\n\")\nprint(\"The whole dataset contains \", len(data), \" many observations.\")","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.056029Z","iopub.execute_input":"2021-07-03T15:33:43.056328Z","iopub.status.idle":"2021-07-03T15:33:43.081682Z","shell.execute_reply.started":"2021-07-03T15:33:43.056301Z","shell.execute_reply":"2021-07-03T15:33:43.079506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec42\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 3.2. Samplers</h2>\n\nSamplers allow us to define data drawing policies.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import RandomSampler\n\ntrain_sampler = RandomSampler(data_source=data_t)\nval_sampler = RandomSampler(data_source=data_v)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.083249Z","iopub.execute_input":"2021-07-03T15:33:43.083566Z","iopub.status.idle":"2021-07-03T15:33:43.089539Z","shell.execute_reply.started":"2021-07-03T15:33:43.083538Z","shell.execute_reply":"2021-07-03T15:33:43.087896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"sec43\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 3.3. DataLoaders</h2>\n\nA DataLoader uses the drawing policies defined in a `sampler` to draw `batch_size` many observations from a `dataset`. \n\nThe result is an iterable containing `int(len(dataset)/batch_size)` many batches. ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 16 \nepochs = 3\n\ntrain_loader = DataLoader(dataset=data_t, \n                          batch_size=batch_size, \n                          sampler=train_sampler)\n\nval_loader = DataLoader(dataset=data_v, \n                        batch_size=batch_size, \n                        sampler=val_sampler)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.091554Z","iopub.execute_input":"2021-07-03T15:33:43.092067Z","iopub.status.idle":"2021-07-03T15:33:43.105376Z","shell.execute_reply.started":"2021-07-03T15:33:43.092018Z","shell.execute_reply":"2021-07-03T15:33:43.104555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_x_batch, some_y_batch = next(iter(train_loader))\n\nprint(\"each batch is stored in a list.\")\nprint(\"\\nthe first entry is of type\", type(some_x_batch))\nprint(\"and has a size of\", some_x_batch.size())\nprint(\"\\nthe second entry is of type\", type(some_y_batch))\nprint(\"and has a size of\", some_y_batch.size())\nprint(\"\\nThe train_loader contains\", len(train_loader), \"many batches.\")\nprint(\"\\nThe val_loader contains\", len(val_loader), \"many batches.\")","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.106612Z","iopub.execute_input":"2021-07-03T15:33:43.107057Z","iopub.status.idle":"2021-07-03T15:33:43.131845Z","shell.execute_reply.started":"2021-07-03T15:33:43.107016Z","shell.execute_reply":"2021-07-03T15:33:43.130802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first entry of each batch contains the independend variables / features and is a second rank tensor with `batch_size` many first order tensors storing `features` many zero rank tensors.\n\nThe second entry of each batch contains the target and is a first rank tensor containing `batch_size` many zero rank tensors.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec44\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 3.4. Create the Model & Investigate it</h2>\n\n* it has to inherit `torch.nn.Module`\n* it has to implement (at least) **__init__()** and **forward()**\n* **__init__()** has to call the constructor of torch.nn.Module \n\nNote that the first two linear layers are 'CustomLinear' modules, whereas the third linear layer is a `nn.Linear` layer.","metadata":{}},{"cell_type":"code","source":"class ExampleFNN(nn.Module):\n    def __init__(self, num_feats):\n        super(ExampleFNN, self).__init__()\n        \n        # hidden layer 1\n        self.linear1 = CustomLinear(in_features=num_feats, out_features=256)\n        self.relu1 = nn.ReLU()\n        \n        # output layer \n        self.linear2 = CustomLinear(in_features=256, out_features=10)\n\n        \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu1(x)\n    \n        return self.linear2(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.13423Z","iopub.execute_input":"2021-07-03T15:33:43.134491Z","iopub.status.idle":"2021-07-03T15:33:43.143419Z","shell.execute_reply.started":"2021-07-03T15:33:43.134465Z","shell.execute_reply":"2021-07-03T15:33:43.142408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ExampleFNN(num_feats=784).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.145297Z","iopub.execute_input":"2021-07-03T15:33:43.145882Z","iopub.status.idle":"2021-07-03T15:33:43.162353Z","shell.execute_reply.started":"2021-07-03T15:33:43.14584Z","shell.execute_reply":"2021-07-03T15:33:43.160903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's investigate our model a little. Therefore, we can either use a predefined model summarizer like [torchsummery](https://pypi.org/project/torch-summary/https://pypi.org/project/torch-summary/) or write our own summarizer from scratch. The latter provides some further insights into handling the model.\n\nLet's investigate how the model parameters are initialized:","metadata":{}},{"cell_type":"code","source":"def show_weights(module):\n    print(module)\n    print(type(module))\n    if (type(module) == nn.Linear) or (type(module) == CustomLinear):\n        print(module.weight)\n    print(\"\\n\\n\")\n        \nmodel.apply(show_weights);","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.164234Z","iopub.execute_input":"2021-07-03T15:33:43.164678Z","iopub.status.idle":"2021-07-03T15:33:43.180903Z","shell.execute_reply.started":"2021-07-03T15:33:43.164617Z","shell.execute_reply":"2021-07-03T15:33:43.179607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we can use the apply method to iterate over the whole model. We can exploit this functionality to select certain module types and transform them. I.e., we can perform our own parameter initialization before training the model.\n\nThe last element containw the whole model.","metadata":{}},{"cell_type":"code","source":"from tabulate import tabulate\n\ndef summary(module, x):\n    print_list = []\n    total_params = 0\n    if len(list(module.named_children())) > 0:  # if it's a model\n        for child in module.named_children():\n            x = child[1](x)\n            param_string = \"\"\n            child_params = 0\n            for param in child[1].named_parameters():\n                shape = list(param[1].size())\n                params = 1\n                for ax in shape:\n                    params *= ax\n                child_params += params\n                param_string = param_string + f\"'{param[0]}'\" + \" shape: \" + str(shape) + \" \"\n            total_params += child_params\n            print_list.append([child[0], list(x.size()), param_string, child_params])\n            \n    else:  # if it's a single module\n        x = module(x)\n        param_string = \"\"\n        for param in module.named_parameters():\n            shape = list(param[1].size())\n            params = 1\n            for ax in shape:\n                params *= ax\n            total_params += params\n            param_string = param_string + f\"'{param[0]}'\" + \" shape: \" + str(shape) + \" \"\n        print_list.append([module, list(x.size()), param_string, total_params])\n        \n    print(f\"Using a Batch Size of {x.size(0)}:\\n\")\n    print(tabulate(print_list, headers=[\"Name\", \"Out Shape\", \"Weights\", \"Trainable Parameters\"]))\n    print(\"\\nTrainable Model Parameters:\", total_params)\n\nsummary(module=model, x=some_x_batch)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.182959Z","iopub.execute_input":"2021-07-03T15:33:43.183721Z","iopub.status.idle":"2021-07-03T15:33:43.204446Z","shell.execute_reply.started":"2021-07-03T15:33:43.183669Z","shell.execute_reply":"2021-07-03T15:33:43.202934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.linear1.weight","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.206819Z","iopub.execute_input":"2021-07-03T15:33:43.207266Z","iopub.status.idle":"2021-07-03T15:33:43.221626Z","shell.execute_reply.started":"2021-07-03T15:33:43.207227Z","shell.execute_reply":"2021-07-03T15:33:43.220818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_module = CustomLinear(in_features=784, out_features=3)\n\nsummary(module=linear_module, x=some_x_batch)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.222935Z","iopub.execute_input":"2021-07-03T15:33:43.224247Z","iopub.status.idle":"2021-07-03T15:33:43.236611Z","shell.execute_reply.started":"2021-07-03T15:33:43.223336Z","shell.execute_reply":"2021-07-03T15:33:43.235257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each layer $\\mathscr{l}$ of a FNN has $M_{\\mathscr{l}-1} \\cdot M_{\\mathscr{l}}$ many weights plus $M_{\\mathscr{l}}$ many bias terms. $M_{\\mathscr{l}}$ is the number of nodes (i.e. output size) of layer ${\\mathscr{l}}$.","metadata":{}},{"cell_type":"markdown","source":"Describing a neural network as a so called [computational graph](https://www.tutorialspoint.com/python_deep_learning/python_deep_learning_computational_graphs.htm), allows us to have a visual representation of the network. A gradient can be computed for each **Leaf Node**.\n\nWe can display such a computational using [tensorboard](https://www.youtube.com/watch?v=pSexXMdruFM).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"sec45\"></a>\n***\n<h2 style=\"background-color:SteelBlue; color:white\" >-> 3.5. Train and Validation Loop</h2>","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\n\noptimizer = Adam(model.parameters(), lr=0.002)\n\nloss_func = nn.CrossEntropyLoss()\ntotal_epochs = 10","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.237929Z","iopub.execute_input":"2021-07-03T15:33:43.238609Z","iopub.status.idle":"2021-07-03T15:33:43.247642Z","shell.execute_reply.started":"2021-07-03T15:33:43.238556Z","shell.execute_reply":"2021-07-03T15:33:43.24599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef train(model, optimizer, loss_func, train_loader):\n    model.train() \n    for batch in train_loader:\n        x_batch = batch[0] \n        y_batch = batch[1]\n        optimizer.zero_grad() \n        probas = model(x_batch) \n        loss = loss_func(probas, y_batch)\n        loss.backward()\n        optimizer.step()\n\ndef validate(model, loader):\n    model.eval()\n    acc = 0\n    for batch in loader:\n        x_batch = batch[0]\n        y_batch = batch[1]\n        with torch.no_grad():\n            probas = model(x_batch)\n        pred = np.argmax(probas.cpu().numpy(), axis=1)\n        acc += accuracy_score(y_true=y_batch.cpu().numpy(), y_pred=pred)\n    return(np.round(acc/len(loader), 4))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.249147Z","iopub.execute_input":"2021-07-03T15:33:43.24946Z","iopub.status.idle":"2021-07-03T15:33:43.262306Z","shell.execute_reply.started":"2021-07-03T15:33:43.24943Z","shell.execute_reply":"2021-07-03T15:33:43.260939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, total_epochs + 1):\n    print(f\"Epoch {epoch} / {total_epochs}:\")\n    train(model=model, optimizer=optimizer, loss_func=loss_func, train_loader=train_loader)\n    acc_train = validate(model=model, loader=train_loader)\n    acc_val = validate(model=model, loader=val_loader)\n    print(\"Train Accuracy:\", acc_train)\n    print(\"Validation Accuracy:\", acc_val)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:33:43.264109Z","iopub.execute_input":"2021-07-03T15:33:43.264593Z","iopub.status.idle":"2021-07-03T15:37:28.735293Z","shell.execute_reply.started":"2021-07-03T15:33:43.264548Z","shell.execute_reply":"2021-07-03T15:37:28.734427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weight Decay","metadata":{}},{"cell_type":"code","source":"model = ExampleFNN(num_feats=784).to(device)\n\noptimizer_w = Adam([{\"params\": model.linear1.bias},\n                   {\"params\": model.linear2.bias},\n                   {\"params\": model.linear1.weight, \"weigth_decay\": 50},\n                   {\"params\": model.linear2.weight, \"weight_decay\": 50}], lr=0.002)\n\nloss_func = nn.CrossEntropyLoss()\ntotal_epochs = 30","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:50:23.404786Z","iopub.execute_input":"2021-07-03T15:50:23.405258Z","iopub.status.idle":"2021-07-03T15:50:23.416211Z","shell.execute_reply.started":"2021-07-03T15:50:23.405228Z","shell.execute_reply":"2021-07-03T15:50:23.414659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, total_epochs + 1):\n    print(f\"Epoch {epoch} / {total_epochs}:\")\n    train(model=model, optimizer=optimizer_w, loss_func=loss_func, train_loader=train_loader)\n    acc_train = validate(model=model, loader=train_loader)\n    acc_val = validate(model=model, loader=val_loader)\n    print(\"Train Accuracy:\", acc_train)\n    print(\"Validation Accuracy:\", acc_val)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-03T15:37:28.745387Z","iopub.status.idle":"2021-07-03T15:37:28.746074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it, thank you for reading this notebook!\n\nThe other notebooks of this series:\n* [Part 2: CNN & Gradient Accumulation](https://www.kaggle.com/milankalkenings/pytorch-2-cnn-gradient-accumulation/edit)\n* [Part 3: (Batch) Normalization](https://www.kaggle.com/milankalkenings/pytorch-3-batch-normalization)","metadata":{}},{"cell_type":"markdown","source":"Helpful Videos and Blogs:\n* [Elliot Waite: Autograd](https://www.youtube.com/watch?v=MswxJw-8PvE&t=75s)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>Feel free to <span style=\"color:red\">comment</span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote</span> if you like this project.</h3>\n</div>","metadata":{}}]}