{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description project\n\nWe work here for a company specialised in E-commerce who exclusively sells handmade clothes. Management would like us to developp a tool that identifies fraudulent activities in a timely manner to prevent any theft.\n\n\n#### Scope of work\n\n- Determine user's country from his IP address\n- Build a Machine Learning prediction model allowing to categorise a transaction as fraudulent or safe.\n- Elaborate on how the model works for a non technical audience who wants to be comfortable using the model\n\n\nTo complete the work, we were provided with two datasets :\n- Fraud_data, containing a sample of users with information such as sex, age, signup_time, ip_address ... and wether they commited fraud or not\n- One table indicating country from a combination of the lower bound and upper bound of an IP address.\n\n----- \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np  \nimport pandas as pd  \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport sklearn\nimport datetime\nimport calendar\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Options\n\npd.set_option('display.max_columns', 500)\n\n\n# Function for when we want to drop a column but still keep it in memory in an other dataframe\n\ndef to_recycle_bin(column) :\n    recycle_ds[column] = dataset[column]\n    dataset.drop(column, axis=1, inplace = True)\n    \n# Function to get a column back from recycle_ds\n\ndef restore_from_bin(column) :\n    dataset[column] = dataset[column]\n    recycle_ds.drop(column, axis=1, inplace = True)\n\n# First we import the data\n\ndataset = pd.read_csv(\"../input/fraud-ecommerce/Fraud_Data.csv\")              # Users information\nIP_table = pd.read_csv(\"../input/fraud-ecommerce/IpAddress_to_Country.csv\")   # Country from IP information\nrecycle_ds = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting information from IP_table"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We want to get country related to each IP address\n\nIP_table.upper_bound_ip_address.astype(\"float\")\nIP_table.lower_bound_ip_address.astype(\"float\")\ndataset.ip_address.astype(\"float\")\n\n# function that takes an IP address as argument and returns country associated based on IP_table\n\ndef IP_to_country(ip) :\n    try :\n        return IP_table.country[(IP_table.lower_bound_ip_address < ip)                            \n                                & \n                                (IP_table.upper_bound_ip_address > ip)].iloc[0]\n    except IndexError :\n        return \"Unknown\"     \n    \n# To affect a country to each IP :\n# dataset[\"IP_country\"] = dataset.ip_address.apply(IP_to_country)\n\n# Since this code is time consuming to run, we have saved the result in a file with the following line of code :\n# dataset.to_csv(\"../kaggle/input/datasets_fraud/Fraud_data_with_country.csv\")\n\n#which allows us to simply import the data if we need to restart the kernel\ndataset = pd.read_csv(\"../input/datasets-fraud/Fraud_data_with_country.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understanding the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We look at what our dataset looks like\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We look at the unique values for the relevant\nimport pprint\nfor colonne in [\"purchase_value\",\"source\",\"browser\",\"sex\",\"age\" ] :\n    pprint.pprint(\"{} : {}\".format(colonne, dataset[colonne].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are there multiple lines with the same user_id ?\nprint(\"The user_id column includes {} duplicates\".format(dataset.duplicated(subset = \"user_id\", keep =False).sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion : each row in the table has a unique user_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see if the same device_id is sometimes used several times\n\ndup_table = pd.DataFrame(dataset.duplicated(subset = \"device_id\"))\ndup_rate = dup_table.mean()\nprint(\"{}% of the dataset is comprised of transactions from a device_id that had been previously used\".format(int(dup_rate*1000)/10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we know the same machine_id can be used for several transaction we will \n# look at how many times on average devices with duplicates are used and their distribution\n\ndataset = pd.read_csv(\"../input/datasets-fraud/Fraud_data_with_country.csv\")\n\ndevice_duplicates = pd.DataFrame(dataset.groupby(by = \"device_id\").device_id.count())  # at this moment, index column name and first column name both are equal to \"device_id\"\ndevice_duplicates.rename(columns={\"device_id\": \"freq_device\"}, inplace=True)           # hence we need to replace the \"device_id\" column name\ndevice_duplicates.reset_index(level=0, inplace= True)                                  # and then we turn device_id from index to column\n\ndupli = device_duplicates[device_duplicates.freq_device >1]\nprint(\"On average, when a device is used more than once it is used {mean} times, and the most used machine was used {maxi} times\"\n      .format(mean = int(dupli.mean()*10)/10, maxi = int(dupli.freq_device.max()*10)/10))\n\ndupli = device_duplicates[device_duplicates.freq_device >2]\nprint(\"On average, when a device is used more than twice it is used {mean} times\"\n      .format(mean = int(dupli.mean()*10)/10, maxi = int(dupli.freq_device.max()*10)/10))\n\n# finally we merge with dataset\ndataset = dataset.merge(device_duplicates, on= \"device_id\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is the proportion of fraud in the dataset\nprint(\"proportion of fraud in the dataset :\" , int(dataset[\"class\"].mean()*1000)/10,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the device frequency distribution to confirm our intuition\n\nf, ax = plt.subplots(1,2,figsize=(12,6))\ng1 =sns.distplot(dataset.freq_device[dataset.freq_device <4], ax=ax[0])\ng1.set(xticks=[1,2,3])\n\ng2 =sns.distplot(dataset.freq_device[dataset.freq_device >2], ax=ax[1])\ng2.set(xticks = range(0,21,2))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# We look at the likelihood of fraud per category\n\nf, ax = plt.subplots(1,3, figsize =(18,6))\nf2,ax2 = plt.subplots(3,1, figsize =(24,18))\nsns.barplot(dataset.groupby(by = \"source\").mean()[\"class\"].index, dataset.groupby(by = \"source\").mean()[\"class\"], ax=ax[0])\nsns.barplot(dataset.groupby(by = \"browser\").mean()[\"class\"].index, dataset.groupby(by = \"browser\").mean()[\"class\"], ax =ax[1])\nsns.barplot(dataset.groupby(by = \"sex\").mean()[\"class\"].index, dataset.groupby(by = \"sex\").mean()[\"class\"], ax = ax[2])\n\nsns.pointplot(x = \"purchase_value\", y= \"class\", data = dataset, logistic=True, ci=None, ax =ax2[0])\nsns.pointplot(x = \"age\", y= \"class\", data = dataset, logistic=True, ci=None, ax = ax2[1])\nsns.pointplot(x = \"freq_device\", y= \"class\", data = dataset, logistic=True, ci=None, ax=ax2[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overview of distribution by country of origin\n\ndataset.IP_country.value_counts()[dataset.IP_country.value_counts() >1000].plot(kind=\"bar\")\n\n# We note that we have a non negligible amount of Unknown. We keep this as an information as IP that cannot be traced back to a \n# country may be an indication of a fraudulent activity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/datasets-fraud/Fraud_data_with_country.csv\")\ndataset = dataset.merge(device_duplicates, on= \"device_id\")\n\n# --- 1 ---\n# Categorisation column freq_device\n# We see a clear correlation between freq_device and fraudulent activities. We are going to split freq_device in 7 categories\ndataset.freq_device = dataset.freq_device.apply(lambda x :\n                                                str(x) if x <5 else\n                                                \"5-10\" if x>=5 and x<=10 else\n                                                \"11-15\" if x>10 and x<=15 else\n                                                \"> 15\")\n\n# We convert signup_time and purchase_time en datetime\n\ndataset.signup_time = pd.to_datetime(dataset.signup_time, format = '%d-%m-%Y %H:%M:%S')\ndataset.purchase_time = pd.to_datetime(dataset.purchase_time, format = '%d-%m-%Y %H:%M:%S')\n\n# We add features \n\n# --- 2 ---\n# Column month\ndataset[\"month_purchase\"] = dataset.purchase_time.apply(lambda x: calendar.month_name[x.month])\n\n# --- 3 ---\n# Column week\ndataset[\"weekday_purchase\"] = dataset.purchase_time.apply(lambda x: calendar.day_name[x.weekday()])\n\n# --- 4 ---\n# Column hour_of_the_day\ndataset[\"hour_of_the_day\"] = dataset.purchase_time.apply(lambda x: x.hour)\n\n# --- 5 ---\n# Column seconds_since_signup\ndataset[\"seconds_since_signup\"]= (dataset.purchase_time - dataset.signup_time).apply(lambda x : x.total_seconds())\n\n# --- 6 ---\n# Column countries_from_device (ie. number of different countries per device_id)\n\n# We flag devices that committed purchases from different countries\n# First we groupby device_id and IP_country which will give us a DF with a sublist of country for each device_id\ncountry_count = dataset.groupby(by =[ \"device_id\",\"IP_country\"]).count().reset_index()\n\n# Then we can count the number of different countries by device_id\ncountry_count = pd.DataFrame(country_count.groupby(by=\"device_id\").count().IP_country)\n\n# Finally we can merge this to our main dataset\ndataset = dataset.merge(country_count, left_on=\"device_id\", right_index=True)\ndataset.rename(columns={\"IP_country_x\": \"IP_country\" , \"IP_country_y\":\"countries_from_device\"}, inplace = True)\n\n# --- 7 ---\n# Column risk_country which classifies each country based on historical fraud rate from these countries\n\n# We calculate the proportion of fraud by country\nrisk_country = pd.DataFrame(dataset.groupby(by=\"IP_country\").mean()[\"class\"].sort_values(ascending = False))\n\n# We classify each country between Very High risk, High risk, Medium risk and low risk\nrisk_country[\"risk_country\"] = risk_country[\"class\"].apply(lambda x : \n                                                           \"Very High risk\" if x > 0.25 else\n                                                           \"High risk\" if x > 0.05 else\n                                                           \"Medium risk\" if x > 0.01 else\n                                                           \"Low risk\")\n                                                \n# We merge with dataset\ndataset = dataset.merge(risk_country.drop(\"class\", axis = 1), left_on= \"IP_country\", right_index = True)\n\n# --- 8 ---\n# Column \"quick_purchase\" : categorise time between sign_up and purchase\ndataset[\"quick_purchase\"] = dataset.seconds_since_signup.apply(lambda x : 1 if x < 30 else 0)\n\n# --- 9 ---\n# Column freq_same_purchase : indicates how many times a given device_id purchased an item of the same value\n\n# We generate a table indicating for each line if the couple device_id / purchase_value has duplicates\nduplicate = dataset.duplicated(subset = [\"purchase_value\",\"device_id\"], keep = False)\n# We merge it with a DF containing purchase_value and device_id\nduplicate = pd.concat([dataset.loc[:,[\"purchase_value\",\"device_id\"]], duplicate],axis=1)\n# We build a DataFrame which gives us the number of duplicates for each combination of device_id / purchase_value\nduplicate = duplicate.groupby(by = [\"device_id\",\"purchase_value\"]).sum()\n# We categorise number of time the same purchase has been made \nduplicate[\"freq_same_purchase\"] = duplicate[0].apply(lambda x : \n                                                x if x < 5 else\n                                                \"5-10\" if x<=10 else\n                                                \"11-15\" if x<= 15 else\n                                                \">15\" \n                                               )\n                                            \n# We merge the result with main dataset            \ndataset.merge(duplicate.drop(0,axis=1), left_on=[\"device_id\",\"purchase_value\"], right_index = True)\n\n# --- 10 ---- \n# age categorisation\n\ndataset[\"age_category\"] = dataset.age.apply(lambda x : \n                 \"< 40\" if x < 40 else\n                 \"40 - 49\" if x < 50 else\n                 \"50 -59\" if x < 60 else\n                 \"60 - 69\" if x < 70 else\n                 \" > 70\")\n\n# ---- 11 ----\n# Hour of the day categorisation\ndataset[\"period_of_the_day\"] = dataset.hour_of_the_day.apply(lambda x : \n                                                             \"late night\" if x < 4 else\n                                                             \"early morning\" if x < 8 else\n                                                             \"morning\" if x < 12 else\n                                                             \"early arvo\" if x < 16 else\n                                                             \"arvo\" if x < 20 else\n                                                             \"evening\"\n                                                            )\n\n# ---- 12 -----\n# First_purchase \n\n# Assuming a potential scammer would want to check out the website before committing any fraudulent activity\n# we want to know if for a given device_ID if the row corresponds to a first operation or not\n\n# However the following code take a long time to run! That's why we have save the resulting Series in the file \n# Series_first_purchase.csv to prevent waiting time when we reset the kernel\n\n# Original code :\n#dataset[\"first_purchase\"] = dataset.apply(lambda x : \n #                                         1 if x.purchase_time == dataset.purchase_time[dataset.device_id == x.device_id].min() else 0,\n #                                         axis =1)\n#dataset.first_purchase.to_csv(\"datasets/Series_first_purchase.csv\",index =False)\n\ndataset[\"first_purchase\"] = pd.read_csv(\"../input/datasets-fraud/Series_first_purchase.csv\", )\n\n\ndataset.to_csv(\"../data_with_first_feature_eng.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualisation post feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"liste_col = [\"freq_device\", \"month_purchase\", \"weekday_purchase\",\n                \"countries_from_device\", \"risk_country\", \"quick_purchase\",\n                \"age_category\",\"period_of_the_day\"]\n\nf, ax = plt.subplots(len(liste_col),1, figsize = (20 , 30))\n\nparam_order = {\"freq_device\":[\"1\",\"2\",\"3\",\"4\",\"5-10\",\"11-15\",\"> 15\"],\n              \"month_purchase\":[\"January\", \"February\", \"March\", \"April\",\"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"December\"],\n              \"weekday_purchase\" : [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n              \"risk_country\": [\"Low risk\", \"Medium risk\", \"High risk\", \"Very High risk\"],\n              \"Period_of_the_day\" : [\"morning\", \"early arvo\", \"arvo\", \"evening\", \"late night\", \"early morning\"]}\n\nfor i, colonne in enumerate(liste_col) :\n    sns.catplot(x = colonne, y = \"class\", data = dataset, kind =\"point\", order= param_order.get(colonne), ax =ax[i], xlabel = colonne, figsize =(40,20))\n    plt.close(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../data_with_first_feature_eng.csv\",index_col=0)\n\n# Following the trends on the above chart we decide to add a specific variable to spot if the transaction occured in January\ndataset[\"is_january\"] = dataset.weekday_purchase.apply(lambda x : 1 if x == \"January\" else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We get rid of useless columns\n\nfor colonne in [\"user_id\",\n                \"signup_time\",\n                \"purchase_time\",\n                \"device_id\",\n                \"ip_address\",\n                \"IP_country\",\n                \"hour_of_the_day\",\n               \"seconds_since_signup\",\n               \"age\"] :\n    to_recycle_bin(colonne)\n    \ndataset.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### -----------------------------------  Linear models  ---------------------------------------------------\n\n(which need to avoid colinearity of the variable, ie get_dummies(drop_first = True)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(dataset.drop(\"class\", axis=1),drop_first= True)\ny = dataset[\"class\"]\n# we split between train and test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state = 42,\n                                                    stratify = y,\n                                                    test_size = 0.25\n                                                   )\n\n# We normalise X_train et X_test \n\nfrom sklearn.preprocessing import StandardScaler\nStdS = StandardScaler()\n\nX_train = StdS.fit_transform(X_train)\nX_test = StdS.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We test a logistic regression first\n\nfrom sklearn.linear_model import LogisticRegression\nLogR = LogisticRegression()\nLogR.fit(X_train, y_train)\n\nprint(\n    \" score sur train : \", int(LogR.score(X_train, y_train)*1000)/10, \"%\"\n    \" score sur test : \" , int(LogR.score(X_test, y_test)*1000)/10, \"%\"\n)\n\n# Set up probabilite threshold\nprob_threshold = 0.22\n\ny_pred = pd.DataFrame(LogR.predict_proba(X_test), columns = [\"proba_no_fraud\", \"proba_fraud\"]).drop(\"proba_no_fraud\", axis = 1)\ny_pred = y_pred.proba_fraud.apply(lambda x : 0 if x < prob_threshold else 1)\n\nscore_threshold_test = np.mean(np.array(y_pred) == np.array(y_test))\n\nprint(\"sum y_pred\", sum(y_pred),\n    \" with a threshold of\", prob_threshold*100, \"%\"\n    \" score sur test : \", int(score_threshold_test*1000)/10,\"%\"\n)\n\n# Confusion matrix\n\nf, ax = plt.subplots(2,1, figsize =(4,8))\n\n# Matrix with threshold by default (50%)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, LogR.predict(X_test))\nsns.heatmap(cm, annot=True, fmt=\"d\", ax = ax[0])\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\n# Matrix with personalised threshold\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\", ax = ax[1])\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, LogR.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, LogR.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First conclusion :\nBased on client's needs, we would be able to set the appropriate level of threshold to reduce the number of false negative (ie. fraud not detected) without increasing inappropriately the cost of false positive (blocked transaction which were not fraudulent)"},{"metadata":{},"cell_type":"markdown","source":"Since the USA are the most represented country, we will try to apply the logistic regression to a dataset including only US and on a dataset with other countries. Except for that, the code will be excactly the same as above"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../data_with_first_feature_eng.csv\",index_col=0)\n\n# Following the trends on the above chart we decide to add a specific variable to spot if the transaction occured in January\ndataset[\"is_january\"] = dataset.weekday_purchase.apply(lambda x : 1 if x == \"January\" else 0)\n\n# WE FILTER ON USA\ndataset = dataset[dataset.IP_country == \"United States\"]\n\n# We get rid of useless columns\n\nfor colonne in [\"user_id\",\n                \"signup_time\",\n                \"purchase_time\",\n                \"device_id\",\n                \"ip_address\",\n                \"IP_country\",\n                \"hour_of_the_day\",\n               \"seconds_since_signup\",\n               \"age\"] :\n    to_recycle_bin(colonne)\n\ndataset.dropna(inplace =True)    \n    \nX = pd.get_dummies(dataset.drop(\"class\", axis=1),drop_first= True)\ny = dataset[\"class\"]\n# we split between train and test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state = 42,\n                                                    stratify = y,\n                                                    test_size = 0.25\n                                                   )\n\n# We normalise X_train et X_test \n\nfrom sklearn.preprocessing import StandardScaler\nStdS = StandardScaler()\n\nX_train = StdS.fit_transform(X_train)\nX_test = StdS.transform(X_test)\n\n# We test a logistic regression first\n\nfrom sklearn.linear_model import LogisticRegression\nLogR = LogisticRegression()\nLogR.fit(X_train, y_train)\n\nprint(\n    \" score sur train : \", int(LogR.score(X_train, y_train)*1000)/10, \"%\"\n    \" score sur test : \" , int(LogR.score(X_test, y_test)*1000)/10, \"%\"\n)\n\n# Set up probabilite threshold\nprob_threshold = 0.22\n\ny_pred = pd.DataFrame(LogR.predict_proba(X_test), columns = [\"proba_no_fraud\", \"proba_fraud\"]).drop(\"proba_no_fraud\", axis = 1)\ny_pred = y_pred.proba_fraud.apply(lambda x : 0 if x < prob_threshold else 1)\n\nscore_threshold_test = np.mean(np.array(y_pred) == np.array(y_test))\n\nprint(\"sum y_pred\", sum(y_pred),\n    \" with a threshold of\", prob_threshold*100, \"%\"\n    \" score sur test : \", int(score_threshold_test*1000)/10,\"%\"\n)\n\n# Confusion matrix\n\nf, ax = plt.subplots(2,1, figsize =(4,8))\n\n# Matrix with threshold by default (50%)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, LogR.predict(X_test))\nsns.heatmap(cm, annot=True, fmt=\"d\", ax = ax[0])\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\n# Matrix with personalised threshold\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\", ax = ax[1])\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion \nApplying the model on USA only does not yield different results than on the whole dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To improve our score we can try to do a bit of stacking on the logistic regression\n\n# -------- On predict_proba --------\n\nLogR2 = LogisticRegression()\nLogR2.fit(X_train, y_train)\nX_train = pd.DataFrame(X_train)\nX_train_new = pd.concat([pd.DataFrame(X_train), pd.DataFrame(LogR2.predict_proba(X_train))], axis =1)\nX_test_new = pd.concat([pd.DataFrame(X_test), pd.DataFrame(LogR2.predict_proba(X_test))], axis = 1)\n\nLogR2.fit(X_train_new, y_train)\nlist_scores = list()\n\nfor _ in np.arange(1,10) :\n    list_scores.append([np.round(LogR2.score(X_train_new, y_train),5) , np.round(LogR2.score(X_test_new, y_test),5)])\n    X_train_new = pd.concat([pd.DataFrame(X_train_new), pd.DataFrame(LogR2.predict_proba(X_train_new))], axis = 1)\n    X_test_new = pd.concat([pd.DataFrame(X_test_new), pd.DataFrame(LogR2.predict_proba(X_test_new))], axis = 1)\n    LogR2.fit(X_train_new, y_train)\n\nprint (\"Stacking using predict_proba\")\nfor num, element in enumerate(list_scores) :\n    print (\"score on iteration {} for train is : {} and for test is {}\".format(num , element[0], element[1]))\n\n# ------ Same thing but on predict_ (ie. 1 or 0 )  -------\n\nLogR2 = LogisticRegression()\nLogR2.fit(X_train, y_train)\nX_train_new = pd.concat([pd.DataFrame(X_train), pd.DataFrame(LogR2.predict(X_train))], axis =1)\nX_test_new = pd.concat([pd.DataFrame(X_test), pd.DataFrame(LogR2.predict(X_test))], axis = 1)\n\nLogR2.fit(X_train_new, y_train)\nlist_scores = list()\n\nfor _ in np.arange(1,10) :\n    list_scores.append([np.round(LogR2.score(X_train_new, y_train),5) , np.round(LogR2.score(X_test_new, y_test),5)])\n    X_train_new = pd.concat([pd.DataFrame(X_train_new), pd.DataFrame(LogR2.predict(X_train_new))], axis = 1)\n    X_test_new = pd.concat([pd.DataFrame(X_test_new), pd.DataFrame(LogR2.predict(X_test_new))], axis = 1)\n    LogR2.fit(X_train_new, y_train)   \n    \nprint (\"Stacking using predict (1 or 0)\")\nfor num, element in enumerate(list_scores) :\n    print (\"score on iteration {} for train is : {} and for test is {}\".format(num , element[0], element[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion : \nStacking has close to no effect on our Logistic Regression model"},{"metadata":{},"cell_type":"markdown","source":"### -----------------------------------  NON- Linear models  ---------------------------------------------------\n\n(which need to avoid colinearity of the variable, ie get_dummies(drop_first = False)"},{"metadata":{},"cell_type":"markdown","source":"Before moving into more complex model, we are going to use a Lasso model on our set of data and use the results to strip out variables with coefficients = 0. This will hopefully allow us to drastically reduce computation time when using more complex models on this new dataset and then allow us to test more interesting approaches."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\nLasReg = LassoCV()\nLasReg.fit(X_train, y_train)\n\n# We get an array of boolean giving us the information if Lasso coefficient is equal to 0 or not\nLasso_filter = LasReg.coef_ == 0\n\n# We use that \"filter\" to reduce our X\nLasso_filter = X.loc[:,Lasso_filter].columns\n\n# We create a new set of X and y, this time keeping the first columns of the dummified variables\nX2 = pd.get_dummies(dataset.drop(\"class\", axis=1),drop_first= False)\ny2 = dataset[\"class\"]\n\n# And we now use our Lasso_filter\nX2 = X2.drop(Lasso_filter, axis = 1)\n\n# we split between train and test\n\nfrom sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2,\n                                                    random_state = 42,\n                                                    stratify = y,\n                                                    test_size = 0.3\n                                                   )\n\n# We normalise X_train et X_test \n\nfrom sklearn.preprocessing import StandardScaler\nStdS = StandardScaler()\n\nX_train2 = StdS.fit_transform(X_train2)\nX_test2 = StdS.transform(X_test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"number columns X\", len(X.columns), \"number columns X2\", len( X2.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have reduced by half the number of variables !\n##### Nota Bene :\nFor the sake of validating our approach, we have re-run our previous Linear model on X.drop(Lasso_filter) with very similar score results as with X"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We tried to do a gridseacrh on XG boost but this did not yield better scores than the logistic regression\n\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrtc = XGBClassifier(n_jobs = -1, random_state =42)\nparams = {'max_depth' : np.arange(1,6),\n          \"learning_rate\" : np.arange(0.1,0.5,0.05),\n          \"n_estimators\" : np.arange(10,100,10)}\n\ngrid =  GridSearchCV(estimator=rtc,       # On indique le type d'estimator que l'on veut tester\n                     param_grid= params,  # On indique le dictionnaire des parametres a faire varier (cf ci-dessus)\n                     cv = 10,             # Le nombre de sous echantillon sur lequel le modele va tourner\n                    n_jobs =-1)      \n\n# CAREFUL TIME TO RUN THE BELOW IS VERY LONG\n# grid.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid search on Random Forest\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nrtc = RandomForestClassifier()\nparams = {'max_depth' : np.arange(1,5),\n         'min_samples_leaf' : [1, 5, 10, 20],\n         \"n_estimators\" : np.arange(10,100,20)}\n\ngrid =  GridSearchCV(estimator=rtc,       # On indique le type d'estimator que l'on veut tester\n                     param_grid= params,  # On indique le dictionnaire des parametres a faire varier (cf ci-dessus)\n                     cv = 10,             # Le nombre de sous echantillon sur lequel le modele va tourner\n                    n_jobs =-1)           # Permet de faire tourner tous les processeurs de l'ordinateur en parallele\ngrid.fit(X_train2, y_train2)\n\nprint(\"score train: \", round(grid.score(X_train2, y_train2),5), \"score test : \", round(grid.score(X_test2, y_test2),5)) \n\nbest_params = grid.best_estimator_.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X2, y2,\n                                                    random_state = 42,\n                                                    stratify = y,\n                                                    test_size = 0.3\n                                                   )\n\n# Simple Regression Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_gini = DecisionTreeClassifier(criterion=best_params[\"criterion\"],\n                                         max_depth = 3,       \n                                         min_samples_leaf = best_params[\"min_samples_leaf\"],  \n                                        )\narbre = classifier_gini.fit(X_train3,y_train3)\nprint(\n    \" score sur train : \", np.round(classifier_gini.score(X_train3, y_train3),5),\n    \" score sur test : \" , np.round(classifier_gini.score(X_test3, y_test3),5)\n)\n\nimport graphviz\nfrom sklearn import tree\n\ngraph_arbre = tree.export_graphviz(arbre, node_ids=\"box\", feature_names = X2.columns)\ngraphviz.Source(graph_arbre)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\nWe see from that tree that :\n1. all transaction with a quick purchase = 1 are classified as fraudulent\n2. keeping \"freq_device\" as a numerical would have been better to get a clearer tree as different instances of freq_devices are used to split data between knots"},{"metadata":{},"cell_type":"markdown","source":"Ideas to go further that may or may not improve the predictions :\n- Combine models with rules like (if quick_purchase ==1 then prediction = 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}