{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Работа по анализу данных Lenta.ru","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib notebook\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport re  # For preprocessing\nimport pandas as pd  # For data handling\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\n\nimport multiprocessing\nimport spacy  # For preprocessing\nfrom gensim.models import Word2Vec, KeyedVectors\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\nfrom sklearn.manifold import TSNE\nfrom numpy import dot\nfrom numpy.linalg import norm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1-ый этап: \n* вывод нужных столбцов\n* очистка от лишних символов, цифр, приведение к одному регистру \n* запись в новый файл очищенные данные","metadata":{}},{"cell_type":"code","source":"#выделить из данных только необхдимые столбцы title, text, topic\nold_questions = pd.read_csv(\"../input/corpus-of-russian-news-articles-from-lenta/lenta-ru-news.csv\")\nkeep_col = ['title', 'text', 'topic']\nquestions = old_questions[keep_col]\nquestions.to_csv(\"newData.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions.head()#вывод 5-ти первых строк","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#проверка выводы нового док-та, в котором только нужные столбцы\ndf = pd.read_csv(\"./newData.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#вывод общих тем(планировалось использовать при кластеризации)\nquestions.groupby(\"topic\").count()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#кол-во топиков\nprint(len(questions.groupby(\"topic\")))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#очистка данных от цифр, знаков препинания, все под\n#один регистр\ndef standardize_text(df, text_field):\n    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"[^А-Яа-я() A-Za-z()\\'\\-\\`\\\"\\_\\n]\", \" \")\n    #df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n    df[text_field] = df[text_field].str.lower()\n    df[text_field] = df[text_field].apply(str)\n    return df\nquestions = df.dropna() #удаление строк с содержанием NaN\n\n#еще и без запятых\n\nquestions = standardize_text(questions, \"title\")\nquestions.to_csv(\"clean_data.csv\")\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions = df.dropna() #удаление строк с содержанием NaN\nquestions = standardize_text(questions, \"text\")\nquestions.to_csv(\"clean_data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2-ой этап: \n* токенизация очищенного файла\n* попытка убрать стоп слова\n* лемматизация и стемминг(потом не используется в пользу лемматизатора) (узнать одинаково ли хорошо они представляют слова в данной выборке)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"./clean_data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"all\"] = df[\"text\"] + \"\" + df[\"title\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"all\"].head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"all\"].tail(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#токенизация\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer('[А-я]\\w+')\n\ndf[\"all\"] = df[\"text\"].apply(lambda x: tokenizer.tokenize(x.lower()))\ndf[\"all\"].head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"all\"].to_csv(\"tokens.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/tokens/tokens (2).csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pymystem3 import Mystem\nm = Mystem()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom pymystem3 import Mystem\nm = Mystem()\nm.lemmatize(df)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"пришлось сохранить токены в один файл, тк использовать будем их и для кеггл перегруз оперативки","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nprint(stopwords.words(\"russian\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#слишком долго идет обработка от стопслов\n#не очистилось, тк долго грузило блокнот\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('russian')]\n    return words\ndf[\"big_model\"] = df[\"big_model\"].apply(lambda x: remove_stopwords(x))\ndf[\"big_model\"].head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer \nstemmer = SnowballStemmer(\"russian\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_1000 = df[\"all\"][:1000]\nfor word in first_1000:\n    print (stemmer.stem(word))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pymystem3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#лемматизация \nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    lem_text = [lemmatizer.lemmatize(i) for i in text]\n    return lem_text\n\nsent = df[\"all\"].apply(lambda x: word_lemmatizer(df[\"all\"]))\nsent.head(4)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#стемминг для сравнения, потом не используется в порзу лемматизатора\nfrom nltk.stem.porter import PorterStemmer\n\nstemmer = PorterStemmer()\n\ndef word_stemmer(text):\n    stem_text = \" \".join([stemmer.stem(i) for i in text])\n    return stem_text\ndf[\"big_model\"] = df[\"big_model\"].apply(lambda x: word_stemmer(x))\ndf[\"big_model\"].head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3-ий этап: \n* обучение модели word2vec и ее сохранение\n* вывод словаря \n* вывод векторов слов\n* сравнение слов на схожесть","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nmd = pd.read_csv(\"../input/lemmatize2/csvexample3.csv\")\nmd.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\nimport re\n\nimport numpy as np\nword2count = {}\n\nfor data in md[\"бой\"][:1000]:\n\n    words = nltk.word_tokenize(data)\n\n    for word in words:\n\n        if word not in word2count.keys():\n\n            word2count[word] = 1\n            \n        else:\n\n            word2count[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(word2count)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport heapq\n\nfreq_words = heapq.nlargest(100, word2count, key=word2count.get)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(freq_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nw2v_model = Word2Vec(\n    min_count=10,\n    window=2,\n    size=300,\n    negative=10,\n    alpha=0.03,\n    min_alpha=0.0007,\n    sample=6e-5,\n    sg=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.build_vocab(md[\"бой\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.train(md[\"бой\"], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.init_sims(replace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.save(\"word2vec.w2v_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(w2v_model.wv.vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = sorted(w2v_model.wv.vocab.keys())\nprint (words[30:50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(w2v_model.wv.vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"россия\"])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = list(w2v_model.wv.vocab)\nprint(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\n#ошибка (Your notebook tried to allocate more memory than is available. It has restarted.)\nmodel = Word2Vec(md[\"бой\"], min_count=15,size= 300,workers=8, window =5, sg = 1)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"word2vec.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#вывод словаря слов модели\nwords = list(model.wv.vocab)\nprint(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel[\"кореи\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model[\"лица\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.similarity( \"северная\", \"корея\")\n#сходство двух слов","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"евклидово подобие","metadata":{}},{"cell_type":"code","source":"model.most_similar('успешно')[:5]\n#5 самых популярных значений ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.most_similar('кореи')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#это перевод словаря в матрицу\n\nimport pandas as pd\n\n\npoints = pd.DataFrame(model[words]).T\n\nprint(points)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n\n#from sklearn.datasets.samples_generator import make_blobs\n\nkmeans = KMeans(n_clusters = len(questions.groupby(\"topic\"))) #23\n\nkmeans.fit(points)\ny_kmeans = kmeans.predict(points)\nplt.scatter(points.iloc[:, 0], points.iloc[:, 1], c = y_kmeans, s = 50, cmap = 'viridis')\n\ncenters = kmeans.cluster_centers_\n\nplt.scatter(centers[:, 0], centers[:, 1], s = 200, alpha = 0.5);\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"итогом анализа получилось графическое представление выше","metadata":{}}]}