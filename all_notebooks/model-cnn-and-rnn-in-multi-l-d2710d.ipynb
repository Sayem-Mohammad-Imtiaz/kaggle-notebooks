{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Description\nDataset contains nearly 32K tweets which are labeled based on having racist or sexist content. We are going to analyse this dataset and tweets, and by the end, create a classification model to classify tweets.   \nEach row in the dataset has 3 columns:\n* `id`: Assigned ID to this tweet by Analytics Vidhya.\n* `label`: Tweet label, 1 if tweet has hatred content and 0 otherwise.\n* `tweet`: Tweet text.  \n\nDataset is provided by [Analytics Vidhya](http://https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/)  ","metadata":{"_uuid":"e78c15c05b99a04ec5b4468299c8308041255bd0","id":"450OHICs6v8d"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n#import nltk\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\nfrom keras.layers import MaxPool1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\n%matplotlib inline\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"id":"LHPs4lSz6v8k","outputId":"f209a59e-a34e-4d76-ad39-0db6516d71e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel('../input/germny-wteet/germeval2018-3.xlsx')\ndata1 = pd.read_csv('../input/multi-language-dataset/cleaned_data_hatespeech.csv')\ndata2 = pd.read_csv('../input/tweet-english/train_E6oV3lV.csv')\ndata3 = pd.read_csv('../input/french-tweet/french_tweets-003.csv')\nRu_data= pd.read_csv('../input/russian-language-toxic-comments/labeled.csv')\nKo_data= pd.read_csv('../input/korean-dataset/hate_speech_binary_dataset.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Greek_data= pd.read_csv('../input/greekkkk/offenseval-gr-training-v1.tsv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nRu_data.columns =['tweet','label' ] \n#Ru_data.columns =['tweet','label' ] \nRu_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(Ru_data.label)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ko_data.columns =['tweet','label' ] \nKo_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(Ko_data.label)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = data3[-117382:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.drop(['lang'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.drop(['id'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncols = data2.columns.tolist()\n\n\ncols = cols[-1:] + cols[:-1]\n\ndata2 = data2[cols]\n\ndata2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns =['tweet','label' ] \ndata1.columns =['tweet','label' ] \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3.columns =['label','tweet' ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = data3.columns.tolist()\n\n\ncols = cols[-1:] + cols[:-1]\n\ndata3 = data3[cols]\n\ndata2\ndata3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df= ([data]+[data1]+[data2]+[data3])\n#df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#[data.label]+ [data1.label]+[data2.label]+[data3.label]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df = [data.label]+[data1.label]+[data2.label]+[data3.label] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How are tweets spread among these 2 classes?","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"VEwU2qgZ6v81"}},{"cell_type":"code","source":"#df.label.split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import numpy as np\n#print(\"Hatred labeled: {}\\nNon-hatred labeled: {}\".format(\n#    (data.label+data1.label+data2.label+data3.label  == 1).sum(),\n#    (data.label+data1.label+data2.label+data3.label == 0).sum()\n#))\n","metadata":{"_uuid":"62127fadccac984b3e5e5dcaad931f176f40a480","id":"9-l-JSSL6v83","outputId":"b6534e8f-fde2-45c4-9555-2b6ad5b1c47c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data.label)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')\n#german DS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data1.label)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')\n#multi language","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data2.label)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')\n# english DS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data3.label)\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')\n# french DS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_fun(labels, preds):\n    labels = label.split(' ')\n    preds = tweet.split(' ')\n    rr = (np.intersect1d(label, tweet))\n    precision = np.float(len(rr)) / len(tweet)\n    recall = np.float(len(rr)) / len(label)\n    try:\n        f1 = 2 * precision * recall / (precision + recall)\n    except ZeroDivisionError:\n        return (precision, recall, 0.0)\n    return (precision, recall, f1)\nprint(1)","metadata":{"id":"5KD1iPvV6v9C","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting features\n\n#### Hashtags and mentions\nWe'll extract hashtags for each tweet as an extra column to explore them later.   \nFor user mentions, all of the usernames have been replaced with `'user'` so we can't get any data from it, we'll just remove mentions and keep the number of mentions in each tweet as an extra features for that tweet.  ","metadata":{"_uuid":"92839e8dbabe53008a1c73f8c708fd7e675bbe03","id":"xfWszSnq6v9L"}},{"cell_type":"code","source":"# data preprocess\nhashtags = data['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\ndata.loc[:, 'hashtags'] = hashtags['hashtag']\ndata['hashtags'].fillna('', inplace=True)\n\ndata.loc[:, 'mentions'] = data['tweet'].str.count('@[a-zA-Z0-9_]+')\n\ndata.tweet = data.tweet.str.replace('@[a-zA-Z0-9_]+', '')\n\n\n","metadata":{"_uuid":"6e31e542112433b66ca725da19b647da4c71585f","id":"7WpAtlp76v9N","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data preprocess\nhashtags = Ru_data['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\nRu_data.loc[:, 'hashtags'] = hashtags['hashtag']\nRu_data['hashtags'].fillna('', inplace=True)\n\nRu_data.loc[:, 'mentions'] = Ru_data['tweet'].str.count('@[a-zA-Z0-9_]+')\n\nRu_data.tweet = Ru_data.tweet.str.replace('@[a-zA-Z0-9_]+', '')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data preprocess\nhashtags = Ko_data['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\nKo_data.loc[:, 'hashtags'] = hashtags['hashtag']\nKo_data['hashtags'].fillna('', inplace=True)\n\nKo_data.loc[:, 'mentions'] = Ko_data['tweet'].str.count('@[a-zA-Z0-9_]+')\n\nKo_data.tweet = Ko_data.tweet.str.replace('@[a-zA-Z0-9_]+', '')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data1 preprocess\nhashtags1 = data1['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\ndata1.loc[:, 'hashtags'] = hashtags1['hashtag']\ndata1['hashtags'].fillna('', inplace=True)\n\ndata1.loc[:, 'mentions'] = data1['tweet'].str.count('@[a-zA-Z0-9_]+')\n\ndata.tweet = data.tweet.str.replace('@[a-zA-Z0-9_]+', '')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data2 preprocess\nhashtags2 = data2['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\ndata2.loc[:, 'hashtags'] = hashtags2['hashtag']\ndata2['hashtags'].fillna('', inplace=True)\n\ndata2.loc[:, 'mentions'] = data2['tweet'].str.count('@[a-zA-Z0-9_]+')\n\ndata2.tweet = data2.tweet.str.replace('@[a-zA-Z0-9_]+', '')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# data3 preprocess\nhashtags3 = data3['tweet'].str.extractall('#(?P<hashtag>[a-zA-Z0-9_]+)').reset_index().groupby('level_0').agg(lambda x: ' '.join(x.values))\ndata3.loc[:, 'hashtags'] = hashtags3['hashtag']\ndata3['hashtags'].fillna('', inplace=True)\n\ndata3.loc[:, 'mentions'] = data3['tweet'].str.count('@[a-zA-Z0-9_]+')\n\ndata3.tweet = data3.tweet.str.replace('@[a-zA-Z0-9_]+', '')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing anything but the words\nNow we'll remove anything but the words (punctuations, numbers, etc). Note that this time we'll replace them with a blank space since it might be a `_` or `-` or a punctuation with no space from the next word and we don't want the words to join together.  ","metadata":{"_uuid":"ed53500474e96d939cdc5fc6c10a91b5a2dfc198","id":"Lk1BzEFp6v9W"}},{"cell_type":"code","source":"data.tweet = data.tweet.str.replace('[^a-zA-Z]', ' ')\nRu_data.tweet = Ru_data.tweet.str.replace('[^a-zA-Z]', ' ')\nKo_data.tweet = Ko_data.tweet.str.replace('[^a-zA-Z]', ' ')\n","metadata":{"_uuid":"0c62abb7ca3a8a806221ed3cae8a3d9e69faf0aa","id":"eFOlpKgX6v9b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lemmatization\nWe lemmatize tweets' words as we have the sentences and we can tag part of speeches, and will stem hashtags.  ","metadata":{"_uuid":"996dfe01552929c9fba1db37f66f822dda269984","id":"sSSJn3AW6v9k"}},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag, FreqDist, word_tokenize\n\nstemmerG = SnowballStemmer('german')\nstemmerF = SnowballStemmer('french')\nstemmerE = SnowballStemmer('english')\nlemmer = WordNetLemmatizer() \n\npart = {\n    'N' : 'n',\n    'V' : 'v',\n    'J' : 'a',\n    'S' : 's',\n    'R' : 'r'\n}\n\ndef convert_tag(penn_tag):\n    if penn_tag in part.keys():\n        return part[penn_tag]\n    else:\n        return 'n'\n\n\ndef tag_and_lem(element):\n    sent = pos_tag(word_tokenize(element))\n    return ' '.join([lemmer.lemmatize(sent[k][0], convert_tag(sent[k][1][0]))\n                    for k in range(len(sent))])\n    \n\ndata.loc[:, 'tweet'] = data['tweet'].apply(lambda x: tag_and_lem(x))\ndata.loc[:, 'hashtags'] = data['hashtags'].apply(lambda x: ' '.join([stemmerG.stem(word) for word in x.split()]))\n\ndata1.loc[:, 'tweet'] = data1['tweet'].apply(lambda x: tag_and_lem(x))\ndata1.loc[:, 'hashtags'] = data1['hashtags'].apply(lambda x: ' '.join(\n    [stemmerG.stem(word) for word in x.split()]+\n    [stemmerF.stem(word) for word in x.split()]+\n    [stemmerE.stem(word) for word in x.split()]))\n\ndata2.loc[:, 'tweet'] = data2['tweet'].apply(lambda x: tag_and_lem(x))\ndata2.loc[:, 'hashtags'] = data2['hashtags'].apply(lambda x: ' '.join([stemmerE.stem(word) for word in x.split()]))\n\ndata3.loc[:, 'tweet'] = data3['tweet'].apply(lambda x: tag_and_lem(x))\ndata3.loc[:, 'hashtags'] = data3['hashtags'].apply(lambda x: ' '.join([stemmerF.stem(word) for word in x.split()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"2jeosaKQ6v9l"}},{"cell_type":"code","source":"#pip install konlpy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from konlpy.tag import Okt\n#from konlpy.utils import pprint\n#from konlpy.corpus import kobill","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from konlpy.tag import Twitter; t = Twitter()\n#tokens_ko = t.kobill(Ko_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#okt = Okt()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ko_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pprint(okt.pos(Ko_data,norm=True, stem=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag, FreqDist, word_tokenize\n\nstemmerG = SnowballStemmer('german')\nstemmerF = SnowballStemmer('french')\nstemmerE = SnowballStemmer('english')\n\nstemmerRu = SnowballStemmer('russian')\n\n#stemmerKo = SnowballStemmer('Korean')\nlemmer = WordNetLemmatizer() \n\npart = {\n    'N' : 'n',\n    'V' : 'v',\n    'J' : 'a',\n    'S' : 's',\n    'R' : 'r'\n}\n\ndef convert_tag(penn_tag):\n    if penn_tag in part.keys():\n        return part[penn_tag]\n    else:\n        return 'n'\n\n\ndef tag_and_lem(element):\n    sent = pos_tag(word_tokenize(element))\n    return ' '.join([lemmer.lemmatize(sent[k][0], convert_tag(sent[k][1][0]))\n                    for k in range(len(sent))])\n    \n\ndata.loc[:, 'tweet'] = data['tweet'].apply(lambda x: tag_and_lem(x))\ndata.loc[:, 'hashtags'] = data['hashtags'].apply(lambda x: ' '.join([stemmerG.stem(word) for word in x.split()]))\n\ndata1.loc[:, 'tweet'] = data1['tweet'].apply(lambda x: tag_and_lem(x))\ndata1.loc[:, 'hashtags'] = data1['hashtags'].apply(lambda x: ' '.join([stemmerE.stem(word) for word in x.split()]))\n\n#data1.loc[:, 'tweet'] = data1['tweet'].apply(lambda x: tag_and_lem(x))\n#data1.loc[:, 'hashtags'] = data1['hashtags'].apply(lambda x: ' '.join([stemmerE.stem(word) for word in x.split()]))\n\n#data1.loc[:, 'tweet'] = data1['tweet'].apply(lambda x: tag_and_lem(x))\n#data1.loc[:, 'hashtags'] = data1['hashtags'].apply(lambda x: ' '.join([stemmerF.stem(word) for word in x.split()]))\n\n#data.loc[:, 'tweet'] = data1['tweet'].apply(lambda x: tag_and_lem(x))\n#data.loc[:, 'hashtags'] = data1['hashtags'].apply(lambda x: ' '.join([stemmerG.stem(word) for word in x.split()]))\n\n\n\n\n\ndata2.loc[:, 'tweet'] = data2['tweet'].apply(lambda x: tag_and_lem(x))\ndata2.loc[:, 'hashtags'] = data2['hashtags'].apply(lambda x: ' '.join([stemmerE.stem(word) for word in x.split()]))\n\ndata3.loc[:, 'tweet'] = data3['tweet'].apply(lambda x: tag_and_lem(x))\ndata3.loc[:, 'hashtags'] = data3['hashtags'].apply(lambda x: ' '.join([stemmerF.stem(word) for word in x.split()]))\n\n#Ru_data.loc[:, 'tweet'] =  Ru_data['tweet'].apply(lambda x: tag_and_lem(x))\n#Ru_data.loc[:, 'hashtags'] = Ru_data['hashtags'].apply(lambda x: ' '.join([stemmerRu.stem(word) for word in x.split()]))\n\n#Ko_data.loc[:, 'tweet'] = Ko_data['tweet'].apply(lambda x: tag_and_lem(x))\n#Ko_data.loc[:, 'hashtags'] = Ko_data['hashtags'].apply(lambda x: ' '.join([stemmerKo.stem(word) for word in x.split()]))\n","metadata":{"_uuid":"d2786200ecbb879dc9fd3e34d8766bc098cf6ad4","id":"Y3q3VvpX6v9m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"bild model cnn","metadata":{"id":"cxCKlJqH6v90"}},{"cell_type":"code","source":"texts = np.array(data1['tweet'])\nlabels = np.array(data1['label'])","metadata":{"id":"vgXb66yI6v94","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nX = data1.tweet\ny = data1.label\nle = LabelEncoder()\ny = le.fit_transform(y)\ny = y.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.042)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORDS = 10000\nMAX_SEQUENCE_LENGTH = 1000\n# the percentage of train test split to be applied\nVALIDATION_SPLIT = 0.20\n# the dimension of vectors to be used\nEMBEDDING_DIM = 100\n# filter sizes of the different conv layers \nfilter_sizes = [3,4,5]\nnum_filters = 512\nembedding_dim = 100\n# dropout probability\ndrop = 0.5\nbatch_size = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport random\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_words = 10000\nmax_len = 1000\nsamples = texts.shape[0]\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\nsequences = tokenizer.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\nword_index = tokenizer.word_index\nprint(data1.shape)\nprint(labels.shape)","metadata":{"id":"r4T7gjXf6v9_","outputId":"de0560e4-8e96-4975-aa74-43b87d7b2797","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"#x_train = data[:7925]\n#y_train = labels[:7925]\n\n#x_val = data[70925:80925]\n#y_val = labels[70925:80925]\n\n#x_test = data[8925:]\n#y_test = labels[8925:]\n\nprint(X_train.shape)\n#print(x_val.shape)\nprint(X_test.shape)\n##\n\n\n","metadata":{"id":"c8eSBFB96v-E","outputId":"3d92d68a-c665-4f8e-edae-c1fd16a2223e","trusted":true}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","metadata":{"id":"5k43-J4I6v-J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.core import Flatten, Dense, Dropout\nfrom keras.callbacks import EarlyStopping\n\nimport tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom keras import backend as K\n\nfrom keras.engine import Layer, InputSpec, InputLayer\n\nfrom keras.models import Model, Sequential\n\nfrom keras.layers import Dropout, Embedding, concatenate\nfrom keras.layers import Conv1D, MaxPool1D, Conv2D, MaxPool2D, ZeroPadding1D\nfrom keras.layers import Dense, Input, Flatten, BatchNormalization\nfrom keras.layers import Concatenate, Dot, merge, Multiply, RepeatVector\nfrom keras.layers import Bidirectional, TimeDistributed\nfrom keras.layers import SimpleRNN, LSTM, GRU, Lambda\n\nfrom keras.layers.core import Reshape, Activation\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\nfrom keras.constraints import maxnorm\nfrom keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim= X_train.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from keras.models import Sequential\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n\nfrom keras.models import Sequential\n#from keras.layers import Dense, Embedding, SimpleRNN, LSTM, GRU, Bidirectional\nmodel_1 = Sequential([(Embedding(1+len(word_index), 16)),\n    ZeroPadding1D((49,49)),\n    Conv1D(64, 50, padding=\"same\"),\n    MaxPooling1D(2),\n    Activation(\"relu\"),\n    ZeroPadding1D((24,24)),\n   # GlobalMaxPooling1D(),\n    Dropout(0.5),\n    Conv1D(64, 25, padding=\"same\"),\n    Dense(10, input_dim=input_dim, activation='sigmoid'),\n    Flatten(),\n    #Dense(1, activation='sigmoid')                  \n    \n                      \n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_1.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_1.fit(sequences_matrix,Y_train,batch_size=64,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0002)], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#max_words = 10000\n#max_len = 1000\n\n#samples = texts.shape[0]\n\n#tokenizer = Tokenizer(num_words=max_words)\n#tokenizer.fit_on_texts(X_train)\n#sequences = tokenizer.texts_to_sequences(X_train)\n#sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n#word_index = tokenizer.word_index\n\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n\naccr = model_1.evaluate(test_sequences_matrix,Y_test)\n#print(data1.shape)\n#print(labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test = model_1.predict(test_sequences_matrix )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnfmatrix(Y_test,results):\n    fp = 0.0\n    fn = 0.0\n    tp = 0.0\n    tn = 0.0\n    t = 0.0\n    n = 0.0\n    results.shape\n    for i in range(results.shape[0]):\n        if Y_test[i]==1 and results[i]==1:\n            tp+=1\n            t+=1\n        elif Y_test[i]==1 and results[i]==0:\n            fn+=1\n            t+=1\n        elif Y_test[i]==0 and results[i]==1:\n            fp+=1\n            n+=1\n        elif Y_test[i]==0 and results[i]==0:\n            tn+=1\n            n+=1\n    print(tp/results.shape[0],fp/results.shape[0])\n    print(fn/results.shape[0],tn/results.shape[0])\n    Precision  = tp/(tp+fp)\n    Recall = tp/(tp+fn)\n    print(\"Precision: \",Precision,\"Recall: \",Recall)\n    f1_score = (2*Precision*Recall)/(Precision+Recall)\n    print(\"f1score: \",f1score)\n    print(\"accuracy: \",(tp+tn)/results.shape[0])\n    print(\"hate_acc: \", (tp)/t)\n    print(\"non_hate_acc: \", (tn)/n)","metadata":{"id":"HeeZbaJw6v-f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n#embedding = embedding_layer(inputs)\n\n#print(embedding.shape)\n#reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\n#print(reshape.shape)\n\n#conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n#conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n#conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n\n#maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n#maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n#maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n\n#concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n#flatten = Flatten()(concatenated_tensor)\n#dropout = Dropout(drop)(flatten)\n#output = Dense(units=20, activation='softmax')(dropout)\n\n# this creates a model that includes\n#model = Model(inputs=inputs, outputs=output)\n\n#checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n#adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\n#model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n#model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n#model = Sequential()\n#model.add(Embedding(1+len(word_index), 16))\n##model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.5, return_sequences=True)))\n#model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.5)))\n#model.add(Dense(1, activation='sigmoid'))\n#model.summary()\n\n\n","metadata":{"id":"27A4kjUM6v-P","outputId":"55cb2936-db27-4bab-ff45-1392cc3320a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"id":"S09GZUCP6v-V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history = model.fit(sequences_matrix,Y_train,batch_size=512,epochs=10,\n#          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"id":"eMo9JdkY6v-a","outputId":"aea32f5d-9e2b-4de5-d175-fcda9d06d824","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = np.array(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"id":"TP62UVTB6v-2","outputId":"5159a067-570d-43e3-f54d-1261a35ed9c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import numpy as np\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt  # doctest: +SKIP\nfrom sklearn import datasets, model_selection, svm\nfrom numpy import interp\n\ndef plot_roc(y_true, y_probas, title='ROC Curves',plot_micro=True, plot_macro=True, classes_to_plot=None,\n                   ax=None, figsize=None, cmap='nipy_spectral',\n                   title_fontsize=\"large\", text_fontsize=\"medium\"):\n    y_true = np.array(y_true)\n    y_probas = np.array(y_probas)\n\n    classes = np.unique(y_true)\n    probas = y_probas\n\n    if classes_to_plot is None:\n        classes_to_plot = classes\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n\n    fpr_dict = dict()\n    tpr_dict = dict()\n\n    indices_to_plot = np.in1d(classes, classes_to_plot)\n    for i, to_plot in enumerate(indices_to_plot):\n        fpr_dict[i], tpr_dict[i], _ = roc_curve(y_true, probas[:, i],pos_label=classes[i])\n        if to_plot:\n            roc_auc = auc(fpr_dict[i], tpr_dict[i])\n            color = plt.cm.get_cmap(cmap)(float(i) / len(classes))\n            ax.plot(fpr_dict[i], tpr_dict[i], lw=2, color=color,\n                    label='ROC curve of class {0} (area = {1:0.2f})'\n                          ''.format(classes[i], roc_auc))\n\n    if plot_micro:\n        binarized_y_true = label_binarize(y_true, classes=classes)\n        if len(classes) == 2:\n            binarized_y_true = np.hstack(\n                (1 - binarized_y_true, binarized_y_true))\n        fpr, tpr, _ = roc_curve(binarized_y_true.ravel(), probas.ravel())\n        roc_auc = auc(fpr, tpr)\n        ax.plot(fpr, tpr,\n                label='micro-average ROC curve '\n                                      '(area = {0:0.2f})'.format(roc_auc),\n                color='deeppink', linestyle=':', linewidth=4)\n\n    if plot_macro:\n        # Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr_dict[x] for x in range(len(classes))]))\n\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(len(classes)):\n            mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n\n        # Finally average it and compute AUC\n        mean_tpr /= len(classes)\n        roc_auc = auc(all_fpr, mean_tpr)\n\n        ax.plot(all_fpr, mean_tpr,\n                label='macro-average ROC curve '\n                      '(area = {0:0.2f})'.format(roc_auc),\n                color='navy', linestyle=':', linewidth=4)\n\n    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate', fontsize=text_fontsize)\n    ax.set_ylabel('True Positive Rate', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n    ax.legend(loc='lower right', fontsize=text_fontsize)\n    return ax\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # **English dataset**\n\n1- define the label and text\nE = English dataset\nET= English test set\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nE = data2.tweet\nET = data2.label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nle = LabelEncoder()\nET = le.fit_transform(ET)\nET = ET.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#X_train,X_test,Y_train,Y_test = train_test_split(E,ET,test_size=0.30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nEtrain_train,Etrain_test,ETtest_train,ETtest_test = train_test_split(E,ET,test_size=0.15425)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ETtest_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ET= [:5000]\n#ET = data2[:4931]\n#ET = labels[:4931]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accr = model_1.evaluate(test_sequences_matrix,Y_test)\n\naccr = model_1.evaluate(test_sequences_matrix,ETtest_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ETtest_test1= model_1.predict(test_sequences_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnfmatrix(Y_test,results):\n    fp = 0.0\n    fn = 0.0\n    tp = 0.0\n    tn = 0.0\n    t = 0.0\n    n = 0.0\n    results.shape\n    for i in range(results.shape[0]):\n        if Y_test[i]==1 and results[i]==1:\n            tp+=1\n            t+=1\n        elif Y_test[i]==1 and results[i]==0:\n            fn+=1\n            t+=1\n        elif Y_test[i]==0 and results[i]==1:\n            fp+=1\n            n+=1\n        elif Y_test[i]==0 and results[i]==0:\n            tn+=1\n            n+=1\n    print(tp/results.shape[0],fp/results.shape[0])\n    print(fn/results.shape[0],tn/results.shape[0])\n    Precision  = tp/(tp+fp)\n    Recall = tp/(tp+fn)\n    print(\"Precision: \",Precision,\"Recall: \",Recall)\n    f1_score = (2*Precision*Recall)/(Precision+Recall)\n    print(\"f1score: \",f1score)\n    print(\"accuracy: \",(tp+tn)/results.shape[0])\n    print(\"hate_acc: \", (tp)/t)\n    print(\"non_hate_acc: \", (tn)/n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **French dataset**\nF = French dataset\nFT= French test set","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nF = data3.tweet\nFT = data3.label\nle = LabelEncoder()\nFT = le.fit_transform(FT)\nFT = FT.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4930/2= 2465\n170810-2465=  168345\n170810+2465=   173275","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nFtrain_train,Ftrain_test,FTtest_train,FTtest_test = train_test_split(F,FT,test_size=0.042)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FTtest_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4931 ","metadata":{}},{"cell_type":"code","source":"#accr = model_1.evaluate(test_sequences_matrix,Y_test)\n#FT_test = model_1.predict(test_sequences_matrix)\naccr = model_1.evaluate(test_sequences_matrix,FTtest_test)\n\n\n\nFTtest_test1 = model_1.predict(test_sequences_matrix)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FT_test = model_1.predict(test_sequences_matrix )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnfmatrix(Y_test,results):\n    fp = 0.0\n    fn = 0.0\n    tp = 0.0\n    tn = 0.0\n    t = 0.0\n    n = 0.0\n    results.shape\n    for i in range(results.shape[0]):\n        if Y_test[i]==1 and results[i]==1:\n            tp+=1\n            t+=1\n        elif Y_test[i]==1 and results[i]==0:\n            fn+=1\n            t+=1\n        elif Y_test[i]==0 and results[i]==1:\n            fp+=1\n            n+=1\n        elif Y_test[i]==0 and results[i]==0:\n            tn+=1\n            n+=1\n    print(tp/results.shape[0],fp/results.shape[0])\n    print(fn/results.shape[0],tn/results.shape[0])\n    Precision  = tp/(tp+fp)\n    Recall = tp/(tp+fn)\n    print(\"Precision: \",Precision,\"Recall: \",Recall)\n    f1_score = (2*Precision*Recall)/(Precision+Recall)\n    print(\"f1score: \",f1score)\n    print(\"accuracy: \",(tp+tn)/results.shape[0])\n    print(\"hate_acc: \", (tp)/t)\n    print(\"non_hate_acc: \", (tn)/n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # **German Dataset**\nG = German dataset\nGT= German test set","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nG = data.tweet\nGT = data.label\nle = LabelEncoder()\nGT = le.fit_transform(GT)\nGT = GT.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n#test_sequences = tokenizer.texts_to_sequences(Gtrain_train)\n#test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nGtrain_train,Gtrain_test,GTtest_train,GTtest_test = train_test_split(G,GT,test_size=0.9845)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GT = data[:4931]\n#GT = labels[:4931]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accr = model_1.evaluate(test_sequences_matrix,Y_test)\n#FT_test = model_1.predict(test_sequences_matrix)\naccr = model_1.evaluate(test_sequences_matrix,GTtest_test)\n\n\nGTtest_test1 = model_1.predict(test_sequences_matrix)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**other datasets** \\n\nthese datasets are not trained on our models\nwe would like to include other lnaguages \n","metadata":{}},{"cell_type":"markdown","source":"# ROC/ AUC","metadata":{}},{"cell_type":"code","source":"#from sklearn.metrics import roc_curve\n#from sklearn.metrics import auc\n#GTtest_test1 = model_1.predict(test_sequences_matrix).ravel()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GTtest_test1 = model_1.predict(test_sequences_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fpr_GT, tpr_GT, thresholds_keras = roc_curve(GTtest_test, GTtest_test1)\n#auc_GT = auc(fpr_GT, tpr_GT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.figure(1)\n#plt.plot([0, 1], [0, 1], 'k--')\n#plt.plot(fpr_GT, tpr_GT, label='test (GT CNN = {:.4f})'.format(auc_GT))\n#plt.plot(fpr_keras1, tpr_keras1, label='train (area = {:.3f})'.format(auc_keras1))\n#plt.plot(fpr_keras2, tpr_keras2, label='val (area = {:.3f})'.format(auc_keras2))\n#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n#plt.xlabel('False positive rate')\n#plt.ylabel('True positive rate')\n#plt.title('ROC curve')\n#plt.legend(loc='best')\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# > > RNN model","metadata":{}},{"cell_type":"code","source":"A = data1.tweet\nB = data1.label\nle = LabelEncoder()\nB = le.fit_transform(B)\nB = B.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nA_train,A_test,B_train,B_test = train_test_split(X,y,test_size=0.042)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_words = 10000\nmax_len = 1000\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(A_train)\nsequences = tok.texts_to_sequences(A_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(64,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{}},{"cell_type":"code","source":"history = model.fit(sequences_matrix,B_train,batch_size=128,epochs=10,\n          validation_split=0.02,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_sequences = tokenizer.texts_to_sequences(X_test)\n#test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n\ntest_sequences = tok.texts_to_sequences(A_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\naccr = model.evaluate(test_sequences_matrix,B_test)\n#accr = model.evaluate(test_sequences_matrix,B_test)\nB_test1 = model.predict(test_sequences_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnfmatrix(B_test,results):\n    fp = 0.0\n    fn = 0.0\n    tp = 0.0\n    tn = 0.0\n    t = 0.0\n    n = 0.0\n    results.shape\n    for i in range(results.shape[0]):\n        if Y_test[i]==1 and results[i]==1:\n            tp+=1\n            t+=1\n        elif Y_test[i]==1 and results[i]==0:\n            fn+=1\n            t+=1\n        elif Y_test[i]==0 and results[i]==1:\n            fp+=1\n            n+=1\n        elif Y_test[i]==0 and results[i]==0:\n            tn+=1\n            n+=1\n    print(tp/results.shape[0],fp/results.shape[0])\n    print(fn/results.shape[0],tn/results.shape[0])\n    Precision  = tp/(tp+fp)\n    Recall = tp/(tp+fn)\n    print(\"Precision: \",Precision,\"Recall: \",Recall)\n    f1_score = (2*Precision*Recall)/(Precision+Recall)\n    print(\"f1score: \",f1score)\n    print(\"accuracy: \",(tp+tn)/results.shape[0])\n    print(\"hate_acc: \", (tp)/t)\n    print(\"non_hate_acc: \", (tn)/n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# English dataset","metadata":{}},{"cell_type":"markdown","source":"ER= English rnn train\nETR= English test RNN","metadata":{}},{"cell_type":"code","source":"ER = data2.tweet\nETR = data2.label\nle = LabelEncoder()\nETR = le.fit_transform(ETR)\nETR = ETR.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_sequences = tokenizer.texts_to_sequences(X_test)\n#test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n\ntest_sequences = tok.texts_to_sequences(A_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nEtrain_train,Etrain_test,ETtest_train,ETtest_test = train_test_split(ER,ETR,test_size=0.15425)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ETR = data2[:4931]\n#ETR = labels[:4931]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr = model.evaluate(test_sequences_matrix,ETtest_test)\n\n#accr = model.evaluate(test_sequences_matrix,ETR)\nETtest_test1 = model.predict(test_sequences_matrix)\n\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# French dataset ","metadata":{}},{"cell_type":"markdown","source":"FR= french rnn train\nFTR= french test RNN","metadata":{}},{"cell_type":"code","source":"FR = data3.tweet\nFTR = data3.label\nle = LabelEncoder()\nFTR = le.fit_transform(FTR)\nFTR = FTR.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_sequences = tokenizer.texts_to_sequences(X_test)\n#test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\ntest_sequences = tok.texts_to_sequences(A_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nF_train,F_test,FT_train,FT_test = train_test_split(FR,FTR,test_size=0.042)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FTR = data3[168345:173275]\n#FTR = labels[168345:173275]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr = model.evaluate(test_sequences_matrix,FT_test)\n#accr = model.evaluate(test_sequences_matrix,FT_test)\n\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FT_test1 = model.predict(test_sequences_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Germany DATAset","metadata":{}},{"cell_type":"markdown","source":"GR= germany rnn train\nGTR= Germany test RNN","metadata":{}},{"cell_type":"code","source":"GR = data.tweet\nGTR = data.label\nle = LabelEncoder()\nGTR = le.fit_transform(GTR)\nGTR = GTR.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_sequences = tokenizer.texts_to_sequences(X_test)\n#test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\ntest_sequences = tok.texts_to_sequences(A_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nGtrain_train,Gtrain_test,GTtest_train,GTtest_test = train_test_split(GR,GTR,test_size=0.9845)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GTR = data[:4931]\n#GTR = labels[:4931]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\naccr = model.evaluate(test_sequences_matrix,GTtest_test)\n\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GTtest_test1 = model.predict(test_sequences_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnfmatrix(B_test,results):\n    fp = 0.0\n    fn = 0.0\n    tp = 0.0\n    tn = 0.0\n    t = 0.0\n    n = 0.0\n    results.shape\n    for i in range(results.shape[0]):\n        if Y_test[i]==1 and results[i]==1:\n            tp+=1\n            t+=1\n        elif Y_test[i]==1 and results[i]==0:\n            fn+=1\n            t+=1\n        elif Y_test[i]==0 and results[i]==1:\n            fp+=1\n            n+=1\n        elif Y_test[i]==0 and results[i]==0:\n            tn+=1\n            n+=1\n    print(tp/results.shape[0],fp/results.shape[0])\n    print(fn/results.shape[0],tn/results.shape[0])\n    Precision  = tp/(tp+fp)\n    Recall = tp/(tp+fn)\n    print(\"Precision: \",Precision,\"Recall: \",Recall)\n    f1_score = (2*Precision*Recall)/(Precision+Recall)\n    print(\"f1score: \",f1score)\n    print(\"accuracy: \",(tp+tn)/results.shape[0])\n    print(\"hate_acc: \", (tp)/t)\n    print(\"non_hate_acc: \", (tn)/n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End of testing and evaluating","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#prediction function \nfrom keras.models import Sequential\npredictions = model_1.predict(B_test)\n\n\n\n","metadata":{"id":"pEMXsWUl6v-l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions = model.predict(ET)\n#predictions1 = model.predict(FT_test)\n#predictions2 = model.predict(GT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt  # doctest: +SKIP\nfrom sklearn import datasets, model_selection, svm\nfrom numpy import interp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef plot_roc(y_true, y_probas, title='ROC Curves',plot_micro=True, plot_macro=True, classes_to_plot=None,\n                   ax=None, figsize=None, cmap='nipy_spectral',\n                   title_fontsize=\"large\", text_fontsize=\"medium\"):\n    y_true = np.array(y_true)\n    y_probas = np.array(y_probas)\n\n    classes = np.unique(y_true)\n    probas = y_probas\n\n    if classes_to_plot is None:\n        classes_to_plot = classes\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n\n    fpr_dict = dict()\n    tpr_dict = dict()\n\n    indices_to_plot = np.in1d(classes, classes_to_plot)\n    for i, to_plot in enumerate(indices_to_plot):\n        fpr_dict[i], tpr_dict[i], _ = roc_curve(y_true, probas[:, i],pos_label=classes[i])\n        if to_plot:\n            roc_auc = auc(fpr_dict[i], tpr_dict[i])\n            color = plt.cm.get_cmap(cmap)(float(i) / len(classes))\n            ax.plot(fpr_dict[i], tpr_dict[i], lw=2, color=color,\n                    label='ROC curve of class {0} (area = {1:0.2f})'\n                          ''.format(classes[i], roc_auc))\n\n    if plot_micro:\n        binarized_y_true = label_binarize(y_true, classes=classes)\n        if len(classes) == 2:\n            binarized_y_true = np.hstack(\n                (1 - binarized_y_true, binarized_y_true))\n        fpr, tpr, _ = roc_curve(binarized_y_true.ravel(), probas.ravel())\n        roc_auc = auc(fpr, tpr)\n        ax.plot(fpr, tpr,\n                label='micro-average ROC curve '\n                                      '(area = {0:0.2f})'.format(roc_auc),\n                color='deeppink', linestyle=':', linewidth=4)\n\n    if plot_macro:\n        # Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr_dict[x] for x in range(len(classes))]))\n\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(len(classes)):\n            mean_tpr += interp(all_fpr, fpr_dict[i], tpr_dict[i])\n\n        # Finally average it and compute AUC\n        mean_tpr /= len(classes)\n        roc_auc = auc(all_fpr, mean_tpr)\n\n        ax.plot(all_fpr, mean_tpr,\n                label='macro-average ROC curve '\n                      '(area = {0:0.2f})'.format(roc_auc),\n                color='navy', linestyle=':', linewidth=4)\n\n    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate', fontsize=text_fontsize)\n    ax.set_ylabel('True Positive Rate', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n    ax.legend(loc='lower right', fontsize=text_fontsize)\n    return ax\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nX = data.tweet\ny = data.label\nle = LabelEncoder()\ny = le.fit_transform(y)\ny = y.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = datasets.make_classification(random_state=0)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(            X, y, random_state=0)\nclf = svm.SVC(random_state=0,probability=True)\nclf.fit(X_train, y_train)\nsvm.SVC(random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probas = clf.predict_proba(X_test)\ny_true = y_test\n\nplot_roc(y_true, y_probas)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\ny_pred_keras =  model_1.predict(y_test).ravel()\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import auc\nauc_keras = auc(fpr_keras, tpr_keras)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Supervised transformation based on random forests\nrf = RandomForestClassifier(max_depth=3, n_estimators=10)\nrf.fit(X_train, Y_train)\n\ny_pred_rf = rf.predict_proba(x_test)[:, 1]\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)\nauc_rf = auc(fpr_rf, tpr_rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n# Zoom in view of the upper left corner.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nfor prediction in predictions:\n    if prediction < 0.5:\n        results.append(0)\n    else:\n        results.append(1)\n        \nresults = np.array(results)","metadata":{"id":"t7shKeD36v-p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnfmatrix(y_test, results)\n","metadata":{"id":"zG3xsK4U6v-w","outputId":"4265b5e6-5c3c-411f-a149-56c664de54cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model):\n    f1_score = lambda precision, recall: 2 * ((precision * recall) / (precision + recall))\n    nexamples, recall, precision = model.test('fasttext.test')\n    print (f'recall: {recall}' )\n    print (f'precision: {precision}')\n    print (f'f1 score: {f1_score(precision,recall)}')\n    print (f'number of examples: {nexamples}')\nprint(1)","metadata":{"id":"U4Eor7Yw6v-7","outputId":"edcb8af8-6313-4291-c98a-ed875036b1bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install plot-metric","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=1000, n_classes=2, weights=[1,1], random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\nfrom scipy.io import arff\ndata = (data)\nfrom sklearn.datasets import make_multilabel_classification\n\n# this will generate a random multi-label dataset\nX, y = make_multilabel_classification(sparse = True, n_labels = 20,\nreturn_indicator = 'sparse', allow_unlabeled = False)\n\n# using binary relevance\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB\n\n# initialize binary relevance multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = BinaryRelevance(GaussianNB())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=50, random_state=23)\nmodel = clf.fit(X_train, y_train)\n\n# Use predict_proba to predict probability of the class\ny_pred = clf.predict_proba(X_test)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  \nfrom sklearn import datasets, metrics, model_selection, svm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])#\n#history = model.fit(x_train,y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))","metadata":{"id":"krOjG8py6v_A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Frequent words\nNow let's see what words and hashtags are the most frequenst in hate tweets and in total.","metadata":{"_uuid":"7b4a8749bc8f8d6624e2c41a710df454aff0f611","id":"OMRNfvNn6v_E"}},{"cell_type":"code","source":"","metadata":{"id":"1C7-RlyZ6v_F","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##from wordcloud import WordCloud, STOPWORDS\n##stopwords = STOPWORDS.add('amp')\n##traindata = ['tweet', 'label']\n##all_words = ' '.join('tweet', 'label')\n##hatred_words = ' '.join(data[data.label == 1].tweet.values)\n\n##plt.figure(figsize=(16, 8))\n\n##cloud1 = WordCloud(width=400, height=400, background_color='white', stopwords=stopwords).generate(all_words)\n##plt.subplot(121)\n##plt.imshow(cloud1, interpolation=\"bilinear\")\n#plt.axis(\"off\")\n#plt.title('All tweets', size=20)\n\n##cloud2 = WordCloud(width=400, height=400, background_color='white', stopwords=stopwords).generate(hatred_words)\n##plt.subplot(122)\n##plt.imshow(cloud2, interpolation=\"bilinear\")\n##plt.axis(\"off\")\n##plt.title('Hatred tweets', size=20)\n##plt.show()","metadata":{"_uuid":"b8a9afebbffcfb27e6267ef5227ee6032250db48","id":"P8vCjTx36v_K","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hashtags\nNow let's see which hashtags are used the most in hatred tweets and in total.","metadata":{"_uuid":"ea2d98b94c65d8694ed867c45289a9be6adc6cee","id":"pT5xQ-l46v_P"}},{"cell_type":"code","source":"##all_hashtags = FreqDist(list(' '.join(data.hashtags.values).split())).most_common(10)\n##hatred_hashtags = FreqDist(list(' '.join(data[data.label==1].hashtags.values).split())).most_common(10)\n##plt.figure(figsize=(14, 6))\n##ax = plt.subplot(121)\n##pd.DataFrame(all_hashtags, columns=['hashtag', 'Count']).set_index('hashtag').plot.barh(ax=ax, fontsize=12)\n##plt.xlabel('# occurrences')\n##plt.title('Hashtags in all tweets', size=13)\n##ax = plt.subplot(122)\n##pd.DataFrame(hatred_hashtags, columns=['hashtag', 'Count']).set_index('hashtag').plot.barh(ax=ax, fontsize=12)\n##plt.xlabel('# occurrences')\n##plt.ylabel('')\n##plt.title('Hashtags in hatred tweets', size=13)\n##plt.show()","metadata":{"_uuid":"a40bfb2f3306029a02ab2bd66ede4cfc78a11b36","id":"JRGD1e8N6v_Q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mentions\nLet's see how many mentions are there in total and if they can be of any use","metadata":{"_uuid":"bded2c905bb43ad971c7758a40ebac58f45a5df7","id":"3sQg-TEh6v_V"}},{"cell_type":"code","source":"##print(\"Number of mentions: {}\\nNumber of tweets having a mention: {}\\nCorrelation with label: {}\".format(\n##    data.mentions.sum(),\n##    len(data[data.mentions > 0]),\n##    np.corrcoef(data.mentions, data.label)[0][1]\n##))","metadata":{"_uuid":"35a5534cf0cd55ea9d1a2f9b3cf6cd2bf883850d","id":"vk0yQYc66v_W","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no meaningful relation between number of mentions and it looks like there is not a correlation either. So we'll remove number of mentions and won't use it as a feature.  ","metadata":{"_uuid":"b025142fbec15c1b20a20b7d4f23efd491b697bf","id":"i_NhEJCP6v_d"}},{"cell_type":"code","source":"##data.drop('mentions', axis=1, inplace=True)","metadata":{"_uuid":"b0b1d3f2d665e1224cf5f863846bd4b50aede99d","id":"Zs1RPM3Z6v_e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tfidf vectorizing\nNow we use the frequency of each word in tweets as our features","metadata":{"_uuid":"4a5d7e7cf2868b0f311f4a95f8439427d2e08a5f","id":"Scnbz7Hf6v_j"}},{"cell_type":"code","source":"##from sklearn.feature_extraction.text import TfidfVectorizer\n##from nltk.corpus import stopwords\n\n##vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=10)\n##features = vectorizer.fit_transform(data.tweet)","metadata":{"_uuid":"ae7203862ad3b6df4ab284c408344c5a7b131915","id":"fnhEQn1s6v_k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classifying\nWe'll build a SVC and a LogsiticRegression model for classifying our tweets.","metadata":{"_uuid":"82037d5250db29273352848200420258d08fbc27","id":"eadRs0J06v_o"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import f1_score\n\n","metadata":{"_uuid":"67b4be23a8922ca538c2d5d112ee28222d64a791","id":"pq_y8I8o6v_p","trusted":true},"execution_count":null,"outputs":[]}]}