{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### IMPORTING MODULES","metadata":{}},{"cell_type":"code","source":"# Basic data science pakages\nimport numpy as np ### For numerical computation\nimport pandas as pd ### For working with data\n\n# For Creating visualizations \n%matplotlib inline\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\n\n# To handle imbalanced data\nfrom imblearn.over_sampling import SMOTE\n\n# For creating training and test set\nfrom sklearn.model_selection import train_test_split\n\n# For column transformation\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\n# To make pipeline (or automate all the model creation works)\nfrom sklearn.pipeline import make_pipeline\n\n# Machine learning models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# For Hyperparameter Optimization\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# For evaluating model\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n\n# Remove all kinds of warning\nfrom warnings import filterwarnings \nfilterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:04.715781Z","iopub.execute_input":"2021-05-26T05:14:04.716438Z","iopub.status.idle":"2021-05-26T05:14:06.341944Z","shell.execute_reply.started":"2021-05-26T05:14:04.716392Z","shell.execute_reply":"2021-05-26T05:14:06.340842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Type of graph that we want\nplt.style.use('seaborn-whitegrid')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:08.431452Z","iopub.execute_input":"2021-05-26T05:14:08.43185Z","iopub.status.idle":"2021-05-26T05:14:08.437627Z","shell.execute_reply.started":"2021-05-26T05:14:08.431811Z","shell.execute_reply":"2021-05-26T05:14:08.436512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Maximum number of columns and rows that it will show\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 10)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:09.691825Z","iopub.execute_input":"2021-05-26T05:14:09.692187Z","iopub.status.idle":"2021-05-26T05:14:09.697308Z","shell.execute_reply.started":"2021-05-26T05:14:09.692155Z","shell.execute_reply":"2021-05-26T05:14:09.696052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LOADING THE DATA","metadata":{}},{"cell_type":"code","source":"# Loading the dataset\ndf = pd.read_csv('../input/income-adult/adult_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:10.371432Z","iopub.execute_input":"2021-05-26T05:14:10.371825Z","iopub.status.idle":"2021-05-26T05:14:10.541231Z","shell.execute_reply.started":"2021-05-26T05:14:10.371793Z","shell.execute_reply":"2021-05-26T05:14:10.540193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing the data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:10.59154Z","iopub.execute_input":"2021-05-26T05:14:10.591942Z","iopub.status.idle":"2021-05-26T05:14:10.629568Z","shell.execute_reply.started":"2021-05-26T05:14:10.59191Z","shell.execute_reply":"2021-05-26T05:14:10.628845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the data\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:10.771371Z","iopub.execute_input":"2021-05-26T05:14:10.771801Z","iopub.status.idle":"2021-05-26T05:14:10.776584Z","shell.execute_reply.started":"2021-05-26T05:14:10.771761Z","shell.execute_reply":"2021-05-26T05:14:10.775904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stripping the unnecessary spaces in the names of columns\ndf.columns = df.columns.str.strip()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:11.031679Z","iopub.execute_input":"2021-05-26T05:14:11.032063Z","iopub.status.idle":"2021-05-26T05:14:11.039953Z","shell.execute_reply.started":"2021-05-26T05:14:11.032032Z","shell.execute_reply":"2021-05-26T05:14:11.038658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total Description of the data\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:11.12192Z","iopub.execute_input":"2021-05-26T05:14:11.1223Z","iopub.status.idle":"2021-05-26T05:14:11.166433Z","shell.execute_reply.started":"2021-05-26T05:14:11.122269Z","shell.execute_reply":"2021-05-26T05:14:11.165269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total information of data\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:11.291617Z","iopub.execute_input":"2021-05-26T05:14:11.291999Z","iopub.status.idle":"2021-05-26T05:14:11.339413Z","shell.execute_reply.started":"2021-05-26T05:14:11.291966Z","shell.execute_reply":"2021-05-26T05:14:11.338253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stripping out unnecessary spaces in the values of categorical columns\nfor i in df.columns:\n    if df[i].dtype == object:\n        df[i] = df[i].str.strip()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:11.511677Z","iopub.execute_input":"2021-05-26T05:14:11.512061Z","iopub.status.idle":"2021-05-26T05:14:11.690942Z","shell.execute_reply.started":"2021-05-26T05:14:11.512028Z","shell.execute_reply":"2021-05-26T05:14:11.689667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking into each columns individually\nfor i in df.columns:\n    print('*' * 100)\n    print('{}:- {}\\n{}\\n'.format(i, df[i].nunique(), df[i].unique()))\n    print(pd.DataFrame({'count': df[i].value_counts(), '%': df[i].value_counts(normalize = True)}))\n    print('/' * 100, '\\n\\n')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-26T05:14:11.762159Z","iopub.execute_input":"2021-05-26T05:14:11.76255Z","iopub.status.idle":"2021-05-26T05:14:12.128424Z","shell.execute_reply.started":"2021-05-26T05:14:11.762514Z","shell.execute_reply":"2021-05-26T05:14:12.127295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Depict same information for education (so, removing it)\ndf.drop('education-num', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:12.130041Z","iopub.execute_input":"2021-05-26T05:14:12.130361Z","iopub.status.idle":"2021-05-26T05:14:12.142851Z","shell.execute_reply.started":"2021-05-26T05:14:12.130329Z","shell.execute_reply":"2021-05-26T05:14:12.141562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of values in columns\nfor i in df.columns:\n    print(i)\n    try: \n        df[i].plot.hist(bins = 30)\n        plt.show()\n    except:\n        plt.barh(df[i].value_counts().index, df[i].value_counts().values)\n        plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-26T05:14:12.16156Z","iopub.execute_input":"2021-05-26T05:14:12.161997Z","iopub.status.idle":"2021-05-26T05:14:15.273067Z","shell.execute_reply.started":"2021-05-26T05:14:12.161962Z","shell.execute_reply":"2021-05-26T05:14:15.271799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HANDLING MISSING VALUES (?)\nIn this dataset, we don't have missing values in the form of NaN values but in the form of a string values, i.e; **'?'**.","metadata":{}},{"cell_type":"code","source":"# Finding which columns have the missing values\nmissing_val = []\nfor i in df.columns:\n    if ('?' in df[i].unique()):\n        print(i)\n        missing_val.append(i)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.276898Z","iopub.execute_input":"2021-05-26T05:14:15.277247Z","iopub.status.idle":"2021-05-26T05:14:15.313612Z","shell.execute_reply.started":"2021-05-26T05:14:15.277215Z","shell.execute_reply":"2021-05-26T05:14:15.312556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking into each columns which have missing values\nfor i in missing_val:\n    print(i, ':-')\n    print('_'*20)\n    print(df[i].value_counts())\n    print('*'*20, '\\n\\n')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-26T05:14:15.315441Z","iopub.execute_input":"2021-05-26T05:14:15.315762Z","iopub.status.idle":"2021-05-26T05:14:15.355416Z","shell.execute_reply.started":"2021-05-26T05:14:15.315713Z","shell.execute_reply":"2021-05-26T05:14:15.3537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling missing values of native-country\ndf['native-country'].replace('?', df['native-country'].value_counts().index[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.357109Z","iopub.execute_input":"2021-05-26T05:14:15.357427Z","iopub.status.idle":"2021-05-26T05:14:15.37379Z","shell.execute_reply.started":"2021-05-26T05:14:15.357395Z","shell.execute_reply":"2021-05-26T05:14:15.372607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column (native-country) after filling the missing values\ndf['native-country'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.375196Z","iopub.execute_input":"2021-05-26T05:14:15.375518Z","iopub.status.idle":"2021-05-26T05:14:15.398638Z","shell.execute_reply.started":"2021-05-26T05:14:15.37549Z","shell.execute_reply":"2021-05-26T05:14:15.397703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling misssing values of occupation\nreplace_occ = df.loc[df['occupation'] != '?', 'occupation'].sample(len(df.loc[df['occupation'] == '?', 'occupation']))\n\nreplace_occ.index = df.loc[df['occupation'] == '?'].index\n\ndf.loc[df['occupation'] == '?', 'occupation'] = replace_occ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.400379Z","iopub.execute_input":"2021-05-26T05:14:15.400821Z","iopub.status.idle":"2021-05-26T05:14:15.438363Z","shell.execute_reply.started":"2021-05-26T05:14:15.400781Z","shell.execute_reply":"2021-05-26T05:14:15.437403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column (occupation) after filling the missing values\ndf['occupation'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.440195Z","iopub.execute_input":"2021-05-26T05:14:15.440662Z","iopub.status.idle":"2021-05-26T05:14:15.458722Z","shell.execute_reply.started":"2021-05-26T05:14:15.440613Z","shell.execute_reply":"2021-05-26T05:14:15.457741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling missing values of workclass\nreplace_wkc = df.loc[df['workclass'] != '?', 'workclass'].sample(len(df.loc[df['workclass'] == '?', 'workclass']))\n\nreplace_wkc.index = df.loc[df['workclass'] == '?'].index\n\ndf.loc[df['workclass'] == '?', 'workclass'] = replace_wkc","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.462603Z","iopub.execute_input":"2021-05-26T05:14:15.463146Z","iopub.status.idle":"2021-05-26T05:14:15.49797Z","shell.execute_reply.started":"2021-05-26T05:14:15.463108Z","shell.execute_reply":"2021-05-26T05:14:15.496948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column (workclass) after filling the missing values\ndf['workclass'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.499834Z","iopub.execute_input":"2021-05-26T05:14:15.500245Z","iopub.status.idle":"2021-05-26T05:14:15.51764Z","shell.execute_reply.started":"2021-05-26T05:14:15.500211Z","shell.execute_reply":"2021-05-26T05:14:15.516388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HANDLING OUTLIERS","metadata":{}},{"cell_type":"code","source":"# It is a class which will help in checking outliers in different columns of the data\n# Here, It uses three techniques to find the outliers (we can use anyone of them)\n# The techniques are:-\n# * IQR\n# * Z score\n# * Standard Deviation\n\nclass Outliers(object):\n    def __init__(self, df, col):\n        self.df = df\n        self.col = col\n        self.min = df[col].min()\n        self.max = df[col].max()\n        self.mean = df[col].mean()\n        self.std = df[col].std()\n        self.median = df[col].median()\n        self.quantile_25 = df[col].quantile(0.25)\n        self.quantile_75 = df[col].quantile(0.75)\n    \n    @property\n    def info(self):\n        \n        print('{}:- '.format(self.col))\n        print('Minimum:- {}'.format(self.min))\n        print('Maximum:- {}'.format(self.max))\n        print('Mean:- {}'.format(self.mean))\n        print('Median:- {}'.format(self.median))\n        print('Standard Deviation:- {}'.format(self.std))\n        print('First Quantile:- {}'.format(self.quantile_25))\n        print('Third Quantile:- {}'.format(self.quantile_75))\n        \n        \nclass IQR(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col) \n        \n        self.IQR = self.quantile_75 - self.quantile_25\n        self.lower_bound = self.quantile_25 - (1.5 * self.IQR)\n        self.upper_bound = self.quantile_75 + (1.5 * self.IQR)\n        \n    def iqr_outliers(self):\n        \n        return self.df.loc[(self.df[self.col] < self.lower_bound) | (self.df[self.col] > self.upper_bound), self.col].values\n    \n    def removed_outliers(self):\n        return self.df.loc[(self.df[self.col] > self.lower_bound) & (self.df[self.col] < self.upper_bound)]\n    \n\nclass Z_score(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col)\n        \n        pass\n    \n    def z_score_outliers(self):\n        outlier = []\n        for i in self.df[self.col]:\n            z = (i - self.mean) / self.std\n            if abs(z) > 3:\n                outlier.append(i)\n                \n        return outlier\n    \n    def removed_outliers(self):\n        \n        df_copy = self.df\n        for i in self.z_score_outliers():\n            df_copy = df_copy.loc[df_copy[self.col] != i]\n            \n        return df_copy\n    \nclass StandardDeviation(Outliers):\n    def __init__(self, df, col):\n        super().__init__(df, col)\n        pass\n    \n    @property\n    def std_calc(self):\n        lower_std = self.mean - (3 * self.std)\n        upper_std = self.mean + (3 * self.std)\n        \n        return lower_std, upper_std\n    \n    def std_outliers(self):\n        lower_std, upper_std = self.std_calc\n        return self.df.loc[(self.df[self.col] < lower_std) | (self.df[self.col] > upper_std), self.col].values\n    \n    def removed_outliers(self):\n        lower_std, upper_std = self.std_calc\n        return self.df.loc[(self.df[self.col] > lower_std) & (self.df[self.col] < upper_std)]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.51887Z","iopub.execute_input":"2021-05-26T05:14:15.519204Z","iopub.status.idle":"2021-05-26T05:14:15.542206Z","shell.execute_reply.started":"2021-05-26T05:14:15.519174Z","shell.execute_reply":"2021-05-26T05:14:15.541085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It will show the outliers of the columns\n\nfor i in df.columns:\n    if df[i].dtype != object:\n        \n        out = Outliers(df, i)\n        iqr = IQR(df, i)\n        z_score = Z_score(df, i)\n        std = StandardDeviation(df, i)\n        \n        print('Column:- {}\\n'.format(i))\n        print('INFO:- \\n')\n        out.info\n        \n        print('\\nOutlier with IQR:- {}\\n'.format(i))\n        print(iqr.iqr_outliers())\n        print('----------> dataset shape after removing outliers with iqr:- {}\\n'.format(iqr.removed_outliers().shape))\n        \n        print('\\nOutlier with Z_score:- {}\\n'.format(i))\n        print(z_score.z_score_outliers())\n        print('----------> dataset shape after removing outliers with z_score:- {}\\n'.format(z_score.removed_outliers().shape))\n        \n        print('\\nOutlier with Standard deviation:- {}\\n'.format(i))\n        print(std.std_outliers())\n        print('----------> dataset shape after removing outliers with Standard Deviation:{}\\n'.format(std.removed_outliers().shape))\n        print('*'*100)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:15.543393Z","iopub.execute_input":"2021-05-26T05:14:15.543713Z","iopub.status.idle":"2021-05-26T05:14:35.738581Z","shell.execute_reply.started":"2021-05-26T05:14:15.543673Z","shell.execute_reply":"2021-05-26T05:14:35.737307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, after looking at the outliers we came to the point where we will remove outliers of\n# ---> age (z_score or Standard deviation)\n# ---> fnlwgt (z_score or standard deviation)\n# ---> hours per week (z_score or standard deviation)\n# ---> capital gain (average of different groups)\n# ---> capital loss (average of different groups)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:35.740335Z","iopub.execute_input":"2021-05-26T05:14:35.74081Z","iopub.status.idle":"2021-05-26T05:14:35.745993Z","shell.execute_reply.started":"2021-05-26T05:14:35.74076Z","shell.execute_reply":"2021-05-26T05:14:35.744528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing outliers of age, fnlwgt, hours-per-week\ndf = Z_score(df, 'age').removed_outliers()\ndf = StandardDeviation(df, 'fnlwgt').removed_outliers()\ndf = Z_score(df, 'hours-per-week').removed_outliers()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:35.747826Z","iopub.execute_input":"2021-05-26T05:14:35.748163Z","iopub.status.idle":"2021-05-26T05:14:40.040916Z","shell.execute_reply.started":"2021-05-26T05:14:35.74813Z","shell.execute_reply":"2021-05-26T05:14:40.039792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.reset_index().rename({'index': 'new_index'}, axis = 1).drop('new_index', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:40.042272Z","iopub.execute_input":"2021-05-26T05:14:40.042576Z","iopub.status.idle":"2021-05-26T05:14:40.074082Z","shell.execute_reply.started":"2021-05-26T05:14:40.042546Z","shell.execute_reply":"2021-05-26T05:14:40.073046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Working on outliers of capital-gain 1\n# making the group in which we will substitute the mean values of that group\ncap_gn = {}\nfor i in np.arange(1, 110000, 10000):\n    \n        cap_gn[str(i) + ' - ' + str(i + 10000)] = df.loc[(df['capital-gain'] >= i) & (df['capital-gain'] < i + 10000), 'capital-gain'].mean()\n        \nfor i,j in cap_gn.items():\n    if j is np.nan:\n        cap_gn[i] = 0\n        \ncap_gn","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:40.07521Z","iopub.execute_input":"2021-05-26T05:14:40.075497Z","iopub.status.idle":"2021-05-26T05:14:40.104509Z","shell.execute_reply.started":"2021-05-26T05:14:40.075469Z","shell.execute_reply":"2021-05-26T05:14:40.103724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Working on outliers of capital-gain 2\n# Substituing mean values in capital-gain\nfor i in range(len(df['capital-gain'])):\n    for j,k in cap_gn.items(): \n        t = int(j.split(' ')[0])\n        r = int(j.split(' ')[-1])\n        if (df.loc[i, 'capital-gain'] >= t) & (df.loc[i, 'capital-gain'] < r):\n            df.loc[i, 'capital-gain'] = k\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:40.106768Z","iopub.execute_input":"2021-05-26T05:14:40.10738Z","iopub.status.idle":"2021-05-26T05:14:51.599445Z","shell.execute_reply.started":"2021-05-26T05:14:40.107344Z","shell.execute_reply":"2021-05-26T05:14:51.598696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# capital-gain after handling outliers\n# Here, it still don't look good, but i'm gonna keep it (because we can't always remove outliers)\n# You can change/remove if you want\n\n\ndf['capital-gain'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:51.601721Z","iopub.execute_input":"2021-05-26T05:14:51.602194Z","iopub.status.idle":"2021-05-26T05:14:51.612258Z","shell.execute_reply.started":"2021-05-26T05:14:51.602148Z","shell.execute_reply":"2021-05-26T05:14:51.611199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Doing same as like the capital-loss\ncap_ls = {}\nfor i in np.arange(1, 6000, 1000):\n    \n        cap_ls[str(i) + ' - ' + str(i + 1000)] = df.loc[(df['capital-loss'] >= i) & (df['capital-loss'] < i + 1000), 'capital-loss'].mean()\n        \nfor i,j in cap_ls.items():\n    if j is np.nan:\n        cap_ls[i] = 0\n        \ncap_ls","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:51.613651Z","iopub.execute_input":"2021-05-26T05:14:51.614263Z","iopub.status.idle":"2021-05-26T05:14:51.633623Z","shell.execute_reply.started":"2021-05-26T05:14:51.614217Z","shell.execute_reply":"2021-05-26T05:14:51.632822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same as capital-gain\nfor i in range(len(df['capital-loss'])):\n    for j,k in cap_ls.items(): \n        t = int(j.split(' ')[0])\n        r = int(j.split(' ')[-1])\n        if (df.loc[i, 'capital-loss'] >= t) & (df.loc[i, 'capital-loss'] < r):\n            df.loc[i, 'capital-loss'] = k\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:51.634728Z","iopub.execute_input":"2021-05-26T05:14:51.63521Z","iopub.status.idle":"2021-05-26T05:14:57.922334Z","shell.execute_reply.started":"2021-05-26T05:14:51.635177Z","shell.execute_reply":"2021-05-26T05:14:57.921573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Here also, it don't look good, but i'm gonna keep it (because we can't always remove outliers)\n# You can change/remove outliers if you want\n\ndf['capital-loss'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:57.923344Z","iopub.execute_input":"2021-05-26T05:14:57.923752Z","iopub.status.idle":"2021-05-26T05:14:57.931306Z","shell.execute_reply.started":"2021-05-26T05:14:57.923707Z","shell.execute_reply":"2021-05-26T05:14:57.930382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HANDLING IMBALANCED DATA","metadata":{}},{"cell_type":"code","source":"# Here, we can see that we have imbalanced data\npd.DataFrame({'count': df.salary.value_counts(), '%': df.salary.value_counts(normalize = True)})","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:57.932604Z","iopub.execute_input":"2021-05-26T05:14:57.932929Z","iopub.status.idle":"2021-05-26T05:14:57.967868Z","shell.execute_reply.started":"2021-05-26T05:14:57.932889Z","shell.execute_reply":"2021-05-26T05:14:57.966843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It will help in balancing imbalanced data\nsmote = SMOTE()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:57.969361Z","iopub.execute_input":"2021-05-26T05:14:57.969671Z","iopub.status.idle":"2021-05-26T05:14:57.973711Z","shell.execute_reply.started":"2021-05-26T05:14:57.969641Z","shell.execute_reply":"2021-05-26T05:14:57.972723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The imbalanced data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:57.975051Z","iopub.execute_input":"2021-05-26T05:14:57.975338Z","iopub.status.idle":"2021-05-26T05:14:58.00593Z","shell.execute_reply.started":"2021-05-26T05:14:57.975304Z","shell.execute_reply":"2021-05-26T05:14:58.004711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming columns into numerical from categorical values\n# as it supports numerical and not categorical values\n# And also scaling the existing numerical values \n\nOrdenc = OrdinalEncoder()\nlabenc = LabelEncoder()\nscale = StandardScaler()\n\nnum_enc = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\ncat_enc = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\ncat_enc_tar = ['salary']\n\ndf[cat_enc] = Ordenc.fit_transform(df[cat_enc])\ndf[cat_enc_tar] = labenc.fit_transform(df[cat_enc_tar])\ndf[num_enc] = scale.fit_transform(df[num_enc])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:58.007605Z","iopub.execute_input":"2021-05-26T05:14:58.007993Z","iopub.status.idle":"2021-05-26T05:14:58.323414Z","shell.execute_reply.started":"2021-05-26T05:14:58.007959Z","shell.execute_reply":"2021-05-26T05:14:58.322399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balancing the data\nX_train, y_train = smote.fit_resample(df.loc[:, :'salary'], df['salary'])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:58.324615Z","iopub.execute_input":"2021-05-26T05:14:58.324982Z","iopub.status.idle":"2021-05-26T05:14:58.865876Z","shell.execute_reply.started":"2021-05-26T05:14:58.32495Z","shell.execute_reply":"2021-05-26T05:14:58.864886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data before balanced\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:58.867524Z","iopub.execute_input":"2021-05-26T05:14:58.867981Z","iopub.status.idle":"2021-05-26T05:14:58.891602Z","shell.execute_reply.started":"2021-05-26T05:14:58.867935Z","shell.execute_reply":"2021-05-26T05:14:58.890871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the balanced data into dataframe\nX_train = pd.DataFrame(X_train, columns = df.columns[:-1])\ny_train = pd.DataFrame(y_train, columns = ['salary'])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:58.892582Z","iopub.execute_input":"2021-05-26T05:14:58.893007Z","iopub.status.idle":"2021-05-26T05:14:58.90976Z","shell.execute_reply.started":"2021-05-26T05:14:58.892975Z","shell.execute_reply":"2021-05-26T05:14:58.908888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the balanced dataframe \n\na = pd.DataFrame(Ordenc.inverse_transform(X_train[cat_enc]), columns = cat_enc)\nb = pd.DataFrame(labenc.inverse_transform(y_train), columns = ['salary'])\nc = pd.DataFrame(scale.inverse_transform(X_train[num_enc]), columns = num_enc)\n\ndf = pd.merge(a, c, left_index = True, right_index = True)\ndf = pd.merge(df, b, left_index = True, right_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:58.910974Z","iopub.execute_input":"2021-05-26T05:14:58.911389Z","iopub.status.idle":"2021-05-26T05:14:58.962767Z","shell.execute_reply.started":"2021-05-26T05:14:58.911359Z","shell.execute_reply":"2021-05-26T05:14:58.961708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balanced data frame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:58.964867Z","iopub.execute_input":"2021-05-26T05:14:58.965314Z","iopub.status.idle":"2021-05-26T05:14:58.989386Z","shell.execute_reply.started":"2021-05-26T05:14:58.965268Z","shell.execute_reply":"2021-05-26T05:14:58.988215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the balanced dataframe\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:58.990811Z","iopub.execute_input":"2021-05-26T05:14:58.991258Z","iopub.status.idle":"2021-05-26T05:14:59.007415Z","shell.execute_reply.started":"2021-05-26T05:14:58.991217Z","shell.execute_reply":"2021-05-26T05:14:59.006344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balanced data for salary column\ndf.salary.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:59.011251Z","iopub.execute_input":"2021-05-26T05:14:59.011718Z","iopub.status.idle":"2021-05-26T05:14:59.035623Z","shell.execute_reply.started":"2021-05-26T05:14:59.011682Z","shell.execute_reply":"2021-05-26T05:14:59.034525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# age cannot be of float type\ndf['age'] = df['age'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:59.037577Z","iopub.execute_input":"2021-05-26T05:14:59.037895Z","iopub.status.idle":"2021-05-26T05:14:59.050052Z","shell.execute_reply.started":"2021-05-26T05:14:59.037867Z","shell.execute_reply":"2021-05-26T05:14:59.049098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"###### Correlation between columns","metadata":{}},{"cell_type":"code","source":"col = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'salary']\ntotal_col = ['workclass', 'education', 'marital-status', 'occupation',\n             'relationship', 'race', 'sex', 'native-country', 'salary', 'age', 'fnlwgt', 'capital-gain', 'capital-loss', \n             'hours-per-week']\n\ntrans_col = make_column_transformer((OrdinalEncoder(), col),\n                                    remainder = 'passthrough')\n\ncorr_df = trans_col.fit_transform(df)\ncorr_df = pd.DataFrame(corr_df, columns = total_col)\ncorr_df","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:59.051562Z","iopub.execute_input":"2021-05-26T05:14:59.052175Z","iopub.status.idle":"2021-05-26T05:14:59.317131Z","shell.execute_reply.started":"2021-05-26T05:14:59.05213Z","shell.execute_reply":"2021-05-26T05:14:59.316384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 10))\n\nsns.heatmap(corr_df.corr(), annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:59.318519Z","iopub.execute_input":"2021-05-26T05:14:59.318844Z","iopub.status.idle":"2021-05-26T05:15:00.698171Z","shell.execute_reply.started":"2021-05-26T05:14:59.318812Z","shell.execute_reply":"2021-05-26T05:15:00.697027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Salary distribution in each age group","metadata":{}},{"cell_type":"code","source":"age_dis = pd.cut(df['age'], [10, 20, 30,40,50,60,70,80,90, 100])\ntemp_df = df.copy()\ntemp_df['age_dis'] = age_dis\n\nviz_age_dis_sal = temp_df.groupby(['age_dis', 'salary'])[['salary']].count().unstack()['salary']\n\nage_d = np.arange(len(viz_age_dis_sal.index))\nwidth = 0.3\n\nplt.figure(figsize = (10, 6))\n\nplt.barh(age_d - width/2, viz_age_dis_sal.loc[:, '<=50K'], height = width, label = '<=50K', alpha = 0.75, edgecolor = 'black')\nplt.barh(age_d + width/2, viz_age_dis_sal.loc[:, '>50K'], height = width, label = '>50K', alpha = 0.75, edgecolor = 'black')\n\nplt.title('Salary distribution in each age group\\n', fontsize = 25)\nplt.xlabel('\\nFrequency', fontsize = 20)\nplt.ylabel('Age group\\n', fontsize = 20)\n\nplt.legend(frameon = True, fontsize = 15, shadow = True)\nplt.yticks(np.arange(0,7), age_dis.unique().sort_values())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:00.699672Z","iopub.execute_input":"2021-05-26T05:15:00.700026Z","iopub.status.idle":"2021-05-26T05:15:00.975395Z","shell.execute_reply.started":"2021-05-26T05:15:00.699993Z","shell.execute_reply":"2021-05-26T05:15:00.97456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Popular Occupation","metadata":{}},{"cell_type":"code","source":"occ_gr = df['occupation'].value_counts()\n\nplt.figure(figsize = (10, 10))\n\nplt.pie(occ_gr.values, labels = occ_gr.index, \n        wedgeprops = {'edgecolor': 'black'}, \n        textprops = {'fontsize': 15},\n        autopct = '%1.2f%%', \n        shadow = True, \n        explode = np.full(len(occ_gr), 0.05))\n\nplt.title('Popular Occupations\\n', fontsize = 35)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:00.976501Z","iopub.execute_input":"2021-05-26T05:15:00.976959Z","iopub.status.idle":"2021-05-26T05:15:01.334682Z","shell.execute_reply.started":"2021-05-26T05:15:00.976912Z","shell.execute_reply":"2021-05-26T05:15:01.333604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Salary as compared to the working hours","metadata":{}},{"cell_type":"code","source":"wk_hr_dis = pd.cut(df['hours-per-week'], 10)\n\ntemp_wk = df.copy()\ntemp_wk['work_hr_dis'] = wk_hr_dis\n\nviz_wk_hr_sal = temp_wk.groupby(['work_hr_dis', 'salary'])[['salary']].count().unstack()['salary']\n\nplt.figure(figsize = (15, 7))\n\nhr_d = np.arange(len(wk_hr_dis.unique()))\nwidth = 0.4\n\nplt.bar(hr_d - width / 2, viz_wk_hr_sal['<=50K'], width = width, alpha = 0.75, edgecolor = 'black', label = '<=50k')\nplt.bar(hr_d + width / 2, viz_wk_hr_sal['>50K'], width = width, alpha = 0.75, edgecolor = 'black', label = '>50k')\n\nplt.title('Salary as compared to the working hours\\n', fontsize = 25)\nplt.xlabel('\\nWorking hour range', fontsize = 20)\nplt.ylabel('Frequency\\n', fontsize = 20)\n\nplt.xticks(hr_d, wk_hr_dis.unique().sort_values())\nplt.legend(frameon = True, shadow = True, fontsize = 15, loc = 'best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:01.33633Z","iopub.execute_input":"2021-05-26T05:15:01.336957Z","iopub.status.idle":"2021-05-26T05:15:01.649204Z","shell.execute_reply.started":"2021-05-26T05:15:01.336909Z","shell.execute_reply":"2021-05-26T05:15:01.648326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Work Time according to Age","metadata":{}},{"cell_type":"code","source":"age_wk_hr = df.groupby('age')[['hours-per-week']].aggregate(np.mean)\nwk_hr_median = df['hours-per-week'].median()\n\nplt.figure(figsize = (8, 5))\n\nplt.plot(age_wk_hr.index, age_wk_hr.iloc[:, 0], linewidth = 5, color = 'steelblue', alpha = 0.5)\n\nplt.fill_between(age_wk_hr.index, age_wk_hr.iloc[:, 0], wk_hr_median, \n                 where = (age_wk_hr.iloc[:, 0] > wk_hr_median),\n                 interpolate= True, alpha = 0.25, color = 'green')\n\nplt.fill_between(age_wk_hr.index, age_wk_hr.iloc[:, 0], wk_hr_median, \n                 where = (age_wk_hr.iloc[:, 0] < wk_hr_median),\n                 interpolate= True, alpha = 0.25, color = 'red')\n\n\nplt.axhline(wk_hr_median, color = 'red', linewidth = 2, \n            label = 'Working hour Median')\n\nplt.title('Work Time according to Age\\n', fontsize = 25)\nplt.xlabel('\\nAge', fontsize = 20)\nplt.ylabel('Hours per Week\\n', fontsize = 20)\n\nplt.legend(loc = 'best', frameon = True, shadow = True, fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:01.650581Z","iopub.execute_input":"2021-05-26T05:15:01.651241Z","iopub.status.idle":"2021-05-26T05:15:01.8589Z","shell.execute_reply.started":"2021-05-26T05:15:01.65119Z","shell.execute_reply":"2021-05-26T05:15:01.8581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Chance of earning salary more than less than 50K","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()\n\ndf_temp['salary_>50K'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\ndf_temp['salary_<=50K'] = df_temp['salary'].map({'<=50K': 1, '>50K': 0})\n\nage_sal = df_temp.groupby('age')[['salary_<=50K', 'salary_>50K']].mean()\n\nplt.figure(figsize = (10, 5))\n\nplt.plot(age_sal.index, age_sal['salary_<=50K'], label = 'Chance of earning less than 50K')\nplt.plot(age_sal.index, age_sal['salary_>50K'], label = 'Chance of earning more than 50K')\n\nplt.title('Chance of earning salary more than less than 50K\\n', fontsize = 25)\nplt.xlabel('\\nAge', fontsize = 20)\nplt.ylabel('Chance of earning salary\\n', fontsize = 20)\n\nplt.legend(loc = 'best', frameon = True, shadow = True, fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:01.860276Z","iopub.execute_input":"2021-05-26T05:15:01.860919Z","iopub.status.idle":"2021-05-26T05:15:02.091707Z","shell.execute_reply.started":"2021-05-26T05:15:01.86087Z","shell.execute_reply":"2021-05-26T05:15:02.090481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Age|Work Hour|Sex|Salary","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()\n\ndf_temp['salary'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\n\nage_wkhr_sal = df_temp.groupby(['age', 'hours-per-week'])[['salary']].aggregate(np.mean).unstack().fillna(0)['salary']\n\nplt.figure(figsize = (10, 7))\n\nplt.scatter(df_temp['age'], df_temp['hours-per-week'], c = df_temp['sex'].map({'Female': 1, 'Male': 2}),\n            s = df_temp['salary'].map({0: 1, 1: 2}) * 10, cmap = 'summer')\n\nplt.title('Age|Work Hour|Sex|Salary\\n', fontsize = 25)\nplt.xlabel('\\nAge', fontsize = 20)\nplt.ylabel('Work Hour\\n', fontsize = 20)\n    \nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:02.093071Z","iopub.execute_input":"2021-05-26T05:15:02.093381Z","iopub.status.idle":"2021-05-26T05:15:03.063852Z","shell.execute_reply.started":"2021-05-26T05:15:02.093352Z","shell.execute_reply":"2021-05-26T05:15:03.062807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Percent of people earning income more than 50K","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()\n\ndf_temp['salary'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\n\nms_sal = df_temp.groupby('marital-status')[['salary']].aggregate(np.mean).sort_values(by = 'salary', ascending = True)\n\nplt.figure(figsize = (10, 5))\n\nplt.barh(ms_sal.index, ms_sal.iloc[:, 0], color = '#FFBAA0', edgecolor = 'black', alpha = 0.75)\n\nplt.title('Income according to marital-status\\n', fontsize = 25)\nplt.xlabel('\\nPercent of people earning income more than 50K', fontsize = 20)\nplt.ylabel('Marital-Status\\n', fontsize = 20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:03.06527Z","iopub.execute_input":"2021-05-26T05:15:03.065578Z","iopub.status.idle":"2021-05-26T05:15:03.268216Z","shell.execute_reply.started":"2021-05-26T05:15:03.065548Z","shell.execute_reply":"2021-05-26T05:15:03.266862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Earning's more than 50K according to gender","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()\n\ndf_temp['salary'] = df_temp['salary'].map({'<=50K': 0, '>50K': 1})\n\ngen_sal = df_temp.groupby('sex')[['salary']].mean()\n\nplt.figure(figsize = (8, 10))\n\nplt.bar(gen_sal.index, gen_sal['salary'], color = '#CCFFA0', alpha = 0.75)\n\nplt.title('Earning\\'s more than 50K\\n', fontsize = 25)\nplt.xlabel('\\nGender', fontsize = 20)\nplt.ylabel('Percentange\\n', fontsize = 20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:03.270012Z","iopub.execute_input":"2021-05-26T05:15:03.270468Z","iopub.status.idle":"2021-05-26T05:15:03.438172Z","shell.execute_reply.started":"2021-05-26T05:15:03.270417Z","shell.execute_reply":"2021-05-26T05:15:03.437015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Salary according to Occupation","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()\n\ndf_temp['salary'] = df['salary'].map({'<=50K': 0, '>50K': 1})\n\nocc_sal = df_temp.groupby('occupation')[['salary']].mean().sort_values(by = 'salary', ascending = True)\n\nplt.barh(occ_sal.index, occ_sal['salary'], alpha = 0.5, color = 'steelblue', edgecolor = 'black')\n\nplt.title('Salary according to Occupation\\n', fontsize = 25)\nplt.xlabel('\\nPerecent of people earning more than 50K', fontsize = 20)\nplt.ylabel('Occupation\\n', fontsize = 20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:15:03.439934Z","iopub.execute_input":"2021-05-26T05:15:03.440398Z","iopub.status.idle":"2021-05-26T05:15:03.68727Z","shell.execute_reply.started":"2021-05-26T05:15:03.44035Z","shell.execute_reply":"2021-05-26T05:15:03.686168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MODEL CREATION","metadata":{}},{"cell_type":"code","source":"class Tuning_types():\n    grid = GridSearchCV\n    random = RandomizedSearchCV\n    \nclass Classifiers():\n\n    # I've manually set some parameters of log because of some errors in my system\n    # (you can try doing it without setting it manually)\n    log = LogisticRegression(solver = 'liblinear', max_iter = 1000) \n    dt = DecisionTreeClassifier()\n    rf = RandomForestClassifier()\n    svc = SVC()\n    gnb = GaussianNB()\n    knn = KNeighborsClassifier()\n    \n    \n\n\n\nclass Model(object):\n    \n    target_col = df.columns[-1]\n    test_size = 0.25\n    \n    def __init__(self, df):\n        \n        self.df = df\n        self.X = df.drop(Model.target_col, axis = 1).copy()\n        self.y = df[Model.target_col].copy()\n        \n        \n    def desc_cols(self):    \n        oe = []\n        ohe = []\n        \n        for i in self.df.columns[:-1]:\n            if self.df[i].dtype == object:\n                if self.df[i].nunique() <= 7:\n                    oe.append(i)\n\n                else:\n                    ohe.append(i)\n                    \n        return oe, ohe\n        \n        \n    @property\n    def train_test_set(self):\n  \n        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, stratify = self.y, \n                                                            test_size = Model.test_size, random_state = 0)\n        \n        return X_train, X_test, y_train, y_test\n    \n    def transform_columns(self, ss = None, oe = [], ohe = []):\n        \n        def ss_choice():\n            if ss == None:\n                return make_column_selector(dtype_include = ['int', 'float'])\n            else:\n                return ss\n            \n            \n        temp = ''\n        \n        if ((oe == []) & (ohe == [])):\n            oe, ohe = self.desc_cols()\n            \n        elif (ohe == []) & (oe != []):\n            temp, ohe = self.desc_cols()\n            \n        elif (oe == []) & (ohe != []):\n            oe, temp = self.desc_cols()\n        \n        trans_col = make_column_transformer((StandardScaler(), ss_choice()), \n                                           (OneHotEncoder(handle_unknown = 'ignore', sparse = False), ohe), \n                                           (OrdinalEncoder(), oe), \n                                           remainder  = 'passthrough')\n        \n        return trans_col\n    \n    @property\n    def target_col_trans_col(self):\n        \n        return make_column_transformer((LabelEncoder(), Model.target_col), \n                                       remainder = 'passthrough')\n\n\n\n\n\n\nclass Logistic_Regression(Model):\n    def __init__(self, df = df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_log = Classifiers().log\n        self.tune = Tuning_types()\n        \n    def pipe_log(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_log)\n        \n        return pipe\n    \n    def tune_params_log(self,\n                    tuning = 'grid',\n                    penalty = ['l1', 'l2'],\n                    dual = [False],\n                    tol = [0.0001, 0.001, 0.01, 0.1], \n                    C = [1, 2, 4, 5],\n                    fit_intercept = [True],\n                    intercept_scaling = [1],\n                    class_weight = [None],\n                    random_state = [None],\n                    multi_class = ['auto'],\n                    verbose = [0],\n                    warm_start = [False],\n                    n_jobs = [None],\n                    l1_ratio = [None],\n                                   ):\n        \n        params = {\n            'logisticregression__penalty': penalty,\n            'logisticregression__dual': dual,\n            'logisticregression__tol': tol,\n            'logisticregression__C': C,\n            'logisticregression__fit_intercept': fit_intercept,\n            'logisticregression__intercept_scaling': intercept_scaling,\n            'logisticregression__class_weight': class_weight,\n            'logisticregression__random_state': random_state,\n            'logisticregression__multi_class': multi_class,\n            'logisticregression__verbose': verbose,\n            'logisticregression__warm_start': warm_start,\n            'logisticregression__n_jobs': n_jobs,\n            'logisticregression__l1_ratio': l1_ratio,\n        }\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_log(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_log(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n\n            \n            \nclass Decision_Tree(Model):\n    def __init__(self, df = df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_dt = Classifiers().dt\n        self.tune = Tuning_types()\n    \n    \n    def pipe_dt(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_dt)\n        \n        return pipe\n    \n    def tune_params_dt(self, \n                    tuning = 'grid', \n                    criterion = ['entropy', 'gini'],\n                    splitter = ['best'],\n                    max_depth = [None],\n                    min_samples_split = np.arange(2, 11, 2),\n                    min_samples_leaf = np.arange(1,6),\n                    min_weight_fraction_leaf = [0.0],\n                    max_features = [None],\n                    random_state = [None],\n                    max_leaf_nodes = [None],\n                    min_impurity_decrease = [0.0],\n                    min_impurity_split = [None],\n                    class_weight = [None],\n                    ccp_alpha = [0.0]):\n    \n        params = {\n            'decisiontreeclassifier__criterion': criterion,\n            'decisiontreeclassifier__splitter': splitter,\n            'decisiontreeclassifier__max_depth': max_depth,\n            'decisiontreeclassifier__min_samples_split': min_samples_split,\n            'decisiontreeclassifier__min_samples_leaf': min_samples_leaf,\n            'decisiontreeclassifier__min_weight_fraction_leaf': min_weight_fraction_leaf,\n            'decisiontreeclassifier__max_features': max_features,\n            'decisiontreeclassifier__random_state': random_state,\n            'decisiontreeclassifier__max_leaf_nodes': max_leaf_nodes,\n            'decisiontreeclassifier__min_impurity_decrease': min_impurity_decrease,\n            'decisiontreeclassifier__min_impurity_split': min_impurity_split,\n            'decisiontreeclassifier__class_weight': class_weight,\n            'decisiontreeclassifier__ccp_alpha': ccp_alpha,\n        }\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_dt(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_dt(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n\n\n\n\n        \n        \nclass Random_Forest(Model):\n            \n\n    def __init__(self, df = df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_rf = Classifiers().rf\n        self.tune = Tuning_types()\n    \n    \n    def pipe_rf(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_rf)\n        \n        return pipe\n    \n    def tune_params_rf(self, \n                    tuning = 'grid', \n                    n_estimators = [50, 100, 150],\n                    criterion = ['entropy', 'gini'],\n                    max_depth = [None],\n                    min_samples_split = np.arange(2, 11, 2),\n                    min_samples_leaf = [1],\n                    min_weight_fraction_leaf = [0.0],\n                    max_features = ['auto'],\n                    max_leaf_nodes = [None],\n                    min_impurity_decrease = [0.0],\n                    min_impurity_split = [None],\n                    bootstrap = [True],\n                    oob_score = [False],\n                    n_jobs = [None],\n                    random_state = [None],\n                    verbose = [0],\n                    warm_start = [False],\n                    class_weight = [None],\n                    ccp_alpha = [0.0],\n                    max_samples = [None]\n                   ):\n        \n        RandomForestClassifier()\n        params = {\n            'randomforestclassifier__n_estimators': n_estimators,\n            'randomforestclassifier__criterion': criterion,\n            'randomforestclassifier__max_depth': max_depth,\n            'randomforestclassifier__min_samples_split': min_samples_split,\n            'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n            'randomforestclassifier__min_weight_fraction_leaf': min_weight_fraction_leaf,\n            'randomforestclassifier__max_features': max_features,\n            'randomforestclassifier__max_leaf_nodes': max_leaf_nodes,\n            'randomforestclassifier__min_impurity_decrease': min_impurity_decrease,\n            'randomforestclassifier__min_impurity_split': min_impurity_split,\n            'randomforestclassifier__bootstrap': bootstrap,\n            'randomforestclassifier__oob_score': oob_score,\n            'randomforestclassifier__n_jobs': n_jobs,\n            'randomforestclassifier__random_state': random_state,\n            'randomforestclassifier__verbose': verbose,\n            'randomforestclassifier__warm_start': warm_start,\n            'randomforestclassifier__class_weight': class_weight,\n            'randomforestclassifier__ccp_alpha': ccp_alpha,\n            'randomforestclassifier__max_samples': max_samples\n        }\n        \n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_rf(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_rf(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n        \n        \nclass Support_Vector_Machine(Model):\n    \n        \n    def __init__(self, df = df):\n        super().__init__(df)\n\n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_svc = Classifiers().svc\n        self.tune = Tuning_types()\n    \n    \n    def pipe_svc(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_svc)\n        \n        return pipe\n    \n    def tune_params_svc(self, \n                    tuning = 'grid', \n                    C = [1.0],\n                    kernel = ['linear', 'rbf', 'poly'],\n                    degree = [3],\n                    gamma = ['scale'],\n                    coef0 = [0.0],\n                    shrinking = [True],\n                    probability = [False],\n                    tol = [1e-3],\n                    cache_size = [200],\n                    class_weight = [None],\n                    verbose = [False],\n                    max_iter = [-1],\n                    decision_function_shape = ['ovr'],\n                    break_ties = [False],\n                    random_state = [None]\n                   ):\n        \n        params = {\n            'svc__C': C,\n            'svc__kernel': kernel,\n            'svc__degree': degree,\n            'svc__gamma': gamma,\n            'svc__coef0': coef0,\n            'svc__shrinking': shrinking,\n            'svc__probability': probability,\n            'svc__tol': tol,\n            'svc__cache_size': cache_size,\n            'svc__class_weight': class_weight,\n            'svc__verbose': verbose,\n            'svc__max_iter': max_iter,\n            'svc__decision_function_shape': decision_function_shape,\n            'svc__break_ties': break_ties,\n            'svc__random_state': random_state\n        }\n\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_svc(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_svc(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n    \nclass Gaussian_Naive_Bayes(Model):\n    \n    def __init__(self, df = df):\n        super().__init__(df)\n\n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_gnb = Classifiers().gnb\n        self.tune = Tuning_types()\n    \n    \n    def pipe_gnb(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_gnb)\n        \n        return pipe\n    \n    def tune_params_gnb(self, \n                    tuning = 'grid', \n                    var_smoothing = [1e-10, 1e-09, 1e-8],\n                    priors = [None],\n                    ):\n        \n        params = {\n            'gaussiannb__var_smoothing': var_smoothing,\n            'gaussiannb__priors': priors\n        }\n    \n    \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_gnb(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_gnb(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n        \n        \nclass KNearest_Neighbor(Model):\n    \n    def __init__(self, df = df):\n        super().__init__(df)\n\n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n        self.clf_knn = Classifiers().knn\n        self.tune = Tuning_types()\n        \n    def pipe_knn(self, ss = None, oe = [], ohe = []):\n        pipe = make_pipeline(self.transform_columns(ss = None, oe = oe, ohe = ohe), self.clf_knn)\n        \n        return pipe\n    \n    \n    def tune_params_knn(self, \n                    tuning = 'grid', \n                    n_neighbors = np.arange(5, 31, 5),\n                    weights = ['uniform'],\n                    algorithm = ['auto'],\n                    leaf_size = [30],\n                    p = [1, 2],\n                    metric = ['minkowski'],\n                    metric_params = [None],\n                    n_jobs = [None]\n                   ):\n        \n        params = {\n            'kneighborsclassifier__n_neighbors': n_neighbors,\n            'kneighborsclassifier__weights': weights,\n            'kneighborsclassifier__algorithm': algorithm,\n            'kneighborsclassifier__leaf_size': leaf_size,\n            'kneighborsclassifier__p': p,\n            'kneighborsclassifier__metric': metric,\n            'kneighborsclassifier__metric_params': metric_params,\n            'kneighborsclassifier__n_jobs': n_jobs\n        }\n        \n        if tuning == 'grid':\n            return self.tune.grid(self.pipe_knn(), params, cv = 5, verbose = 10)\n        \n        elif tuning == 'random':\n            return self.tune.random(self.pipe_knn(), params, cv = 5, verbose = 10)\n        \n        else:\n            return \"ERROR: Invalid tuning type.\\nSet tuning as in ['grid', 'random'] in the parameter.\"\n        \n        \n\nclass Evaluate(Logistic_Regression, \n               Decision_Tree, \n               Random_Forest, \n               Support_Vector_Machine, \n               Gaussian_Naive_Bayes, \n               KNearest_Neighbor):\n\n    def __init__(self, df):\n        super().__init__(df)\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = self.train_test_set\n\n\n    def show(self, y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test):\n\n        accuracy_tr_nm = accuracy_score(self.y_train, y_pred_nm_train)\n        accuracy_ts_nm = accuracy_score(self.y_test, y_pred_nm_test)\n        \n        accuracy_tr_tm = accuracy_score(self.y_train, y_pred_tm_train)\n        accuracy_ts_tm = accuracy_score(self.y_test, y_pred_tm_test)\n        \n        acc = pd.DataFrame({'Training Set': [accuracy_tr_nm, accuracy_tr_tm], \n                      'Testing Set': [accuracy_ts_nm, accuracy_ts_tm]},\n                     index = ['Before parameter optimization', \n                              'After parameter optimization'])\n        \n        print('/' * 100, '\\n')\n        print('Acccuracy Score:- ')\n        print('*' * 50)\n        print(acc)\n        print('*' * 50, '\\n')\n        \n        \n        # Confusion Matrix\n        confusion_matrix_tr_nm = confusion_matrix(self.y_train, y_pred_nm_train)\n        confusion_matrix_ts_nm = confusion_matrix(self.y_test, y_pred_nm_test)\n        \n        confusion_matrix_tr_tm = confusion_matrix(self.y_train, y_pred_tm_train)\n        confusion_matrix_ts_tm = confusion_matrix(self.y_test, y_pred_tm_test)\n        print('Confusion Matrix:- ')\n        print('*' * 50)\n        print('Confusion Matrix (Training Set):- ')\n        print('Before parameter tuning:- \\n', confusion_matrix_tr_nm)\n        print('After parameter tuning:- \\n', confusion_matrix_tr_tm)\n        print('\\n')\n        print('Confusion Matrix (Testing Set):- ')\n        print('Before parameter tuning:- \\n', confusion_matrix_ts_nm)\n        print('After parameter tuning:- \\n', confusion_matrix_ts_tm)\n        print('*' * 50, '\\n')\n        \n        # Classification Report\n        classification_report_tr_nm = classification_report(self.y_train, y_pred_nm_train)\n        classification_report_ts_nm = classification_report(self.y_test, y_pred_nm_test)\n        \n        classification_report_tr_tm = classification_report(self.y_train, y_pred_tm_train)\n        classification_report_ts_tm = classification_report(self.y_test, y_pred_tm_test)\n        \n        print('Classification Report:- ')\n        print('*' * 50)\n        print('Classification Report (Training Set):- ')\n        print('Before parameter tuning:- \\n', classification_report_tr_nm)\n        print('After parameter tuning:- \\n', classification_report_tr_tm)\n        \n        print('\\n')\n        print('Classification Report (Testing Set):- ')\n        print('Before parameter tuning:- \\n', classification_report_ts_nm)\n        print('After parameter tuning:- \\n', classification_report_ts_tm)\n        print('*' * 50, '\\n')\n        \n        print('/' * 100)\n        \n\n\n    \n    @property\n    def evaluate_log(self):\n        \n        normal_model = self.pipe_log().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_log().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n\n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n        \n        \n    @property\n    def evaluate_dt(self):\n        \n        normal_model = self.pipe_dt().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_dt().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n\n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n    @property\n    def evaluate_rf(self):\n        \n        normal_model = self.pipe_rf().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_rf().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n\n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n        \n    @property\n    def evaluate_svc(self):\n        \n        normal_model = self.pipe_svc().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_svc().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n        \n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n        \n    @property\n    def evaluate_gnb(self):\n        \n        normal_model = self.pipe_gnb().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_gnb().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n        \n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n    \n    \n    @property\n    def evaluate_knn(self):\n        \n        normal_model = self.pipe_knn().fit(self.X_train, self.y_train)\n        tuned_model = self.tune_params_knn().fit(self.X_train, self.y_train)\n        \n        y_pred_nm_train = normal_model.predict(self.X_train)\n        y_pred_nm_test = normal_model.predict(self.X_test)\n        \n        y_pred_tm_train = tuned_model.predict(self.X_train)\n        y_pred_tm_test = tuned_model.predict(self.X_test)\n        \n        self.show(y_pred_nm_train, y_pred_nm_test, y_pred_tm_train, y_pred_tm_test)\n        \n        \n    \n    # This method will evaluate all the model\n    def evaluate_all(self, *models):\n        \n        for i in models:\n            if i == 'logisticregression':\n                self.evaluate_log\n                \n            elif i == 'decisiontreeclassifier':\n                self.evaluate_dt\n                \n            elif i == 'randomforestclassifier':\n                self.evaluate_rf\n                \n            elif i == 'supportvectorclassifier':\n                self.evaluate_svc\n                \n            elif i == 'gaussiannaivebayes':\n                self.evaluate_gnb\n                \n            elif i == 'knearestneighbor':\n                self.evaluate_knn\n            \n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:18:55.390069Z","iopub.execute_input":"2021-05-26T05:18:55.390537Z","iopub.status.idle":"2021-05-26T05:18:55.661348Z","shell.execute_reply.started":"2021-05-26T05:18:55.390502Z","shell.execute_reply":"2021-05-26T05:18:55.660503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ev = Evaluate(df)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:18:58.071423Z","iopub.execute_input":"2021-05-26T05:18:58.071974Z","iopub.status.idle":"2021-05-26T05:18:58.640226Z","shell.execute_reply.started":"2021-05-26T05:18:58.071937Z","shell.execute_reply":"2021-05-26T05:18:58.639377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 1) Logistic Regression","metadata":{}},{"cell_type":"code","source":"ev.evaluate_log","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:19:00.591876Z","iopub.execute_input":"2021-05-26T05:19:00.59242Z","iopub.status.idle":"2021-05-26T05:20:28.998755Z","shell.execute_reply.started":"2021-05-26T05:19:00.592379Z","shell.execute_reply":"2021-05-26T05:20:28.997476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 2) Decision Tree","metadata":{}},{"cell_type":"code","source":"ev.evaluate_dt","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:20:29.000867Z","iopub.execute_input":"2021-05-26T05:20:29.00135Z","iopub.status.idle":"2021-05-26T05:22:48.810426Z","shell.execute_reply.started":"2021-05-26T05:20:29.001299Z","shell.execute_reply":"2021-05-26T05:22:48.809305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3) Random Forest ","metadata":{}},{"cell_type":"code","source":"ev.evaluate_rf","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:22:48.812635Z","iopub.execute_input":"2021-05-26T05:22:48.813024Z","iopub.status.idle":"2021-05-26T05:34:08.893493Z","shell.execute_reply.started":"2021-05-26T05:22:48.812967Z","shell.execute_reply":"2021-05-26T05:34:08.892223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 4) Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"ev.evaluate_svc","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:34:08.895653Z","iopub.execute_input":"2021-05-26T05:34:08.896121Z","iopub.status.idle":"2021-05-26T06:02:06.974506Z","shell.execute_reply.started":"2021-05-26T05:34:08.896067Z","shell.execute_reply":"2021-05-26T06:02:06.973412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5) Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"ev.evaluate_gnb","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:02:06.976054Z","iopub.execute_input":"2021-05-26T06:02:06.976374Z","iopub.status.idle":"2021-05-26T06:02:17.699367Z","shell.execute_reply.started":"2021-05-26T06:02:06.976343Z","shell.execute_reply":"2021-05-26T06:02:17.698043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 6) KNearest Neighbors","metadata":{}},{"cell_type":"code","source":"ev.evaluate_knn","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:02:17.70078Z","iopub.execute_input":"2021-05-26T06:02:17.70121Z","iopub.status.idle":"2021-05-26T06:18:45.601389Z","shell.execute_reply.started":"2021-05-26T06:02:17.701165Z","shell.execute_reply":"2021-05-26T06:18:45.600217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can change or set different hyperparameters (Its not necessary that mine is perfect.)\n# Changing hyperparameters may give better results\n\n# But I have kept it like this because of time and system barriers.","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:19:08.729976Z","iopub.execute_input":"2021-05-26T06:19:08.730344Z","iopub.status.idle":"2021-05-26T06:19:08.734728Z","shell.execute_reply.started":"2021-05-26T06:19:08.730312Z","shell.execute_reply":"2021-05-26T06:19:08.733582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}