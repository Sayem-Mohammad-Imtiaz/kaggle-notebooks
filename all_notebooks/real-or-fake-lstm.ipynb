{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text classification using LSTM\n\nIn this notebook I will make use of Long Short Term Memory (LSTM) architecture to classify newspaper articles as either real or fake, using Keras. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\n\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"os.chdir(\"/home/leon/Documents/projects\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"true.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.iloc[0]['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create single, labeled dataframe\n\nTo merge the two dataframes I add a label column to each and stack the frames on top of each other. This way I combined the data and added the labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"true['label']  = pd.Series([0] * len(true)) # add label column | 0 == True, 1 == Fake\nfake['label']  = pd.Series([1] * len(fake))\n\ntrue['label'] = pd.Categorical(true['label']) # make label categorical\nfake['label'] = pd.Categorical(fake['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full = true.append(fake) # create single df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning\n\n### The following functions clean the text data (punctuation, stopwords, make lowercase etc.) \n\nThe nice thing is that the very last function 'full_clean' can be adapted by removing or adding the other functions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers: word_tokenize('ebrahim^hazrati')'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n\n#remove one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\ndef full_clean(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply the cleaning functions on text and export the cleaned text to save memory."},{"metadata":{"trusted":false},"cell_type":"code","source":"full['clean'] = [full_clean(i) for i in full['text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full.to_csv('fake_real.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load in cleaned data"},{"metadata":{"trusted":true},"cell_type":"code","source":"full = pd.read_csv(\"../input/fakereal/fake_real.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"articles = full['clean'].dropna().to_list()\narticles[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(articles))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see The United States and especially Trump is a common theme in the articles. International affairs (north korea) and U.S. elections (hillary clinton) are also featured often.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow import keras\n\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras import optimizers\n\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately my computer cannot handle the full data, so I needed to subsample and work with only 10% of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def balanced_subsample(y, size=None):\n    \n    '''Sample from data and keep the classes balanced'''\n\n    subsample = []\n\n    if size is None:\n        n_smp = y.value_counts().min()\n    else:\n        n_smp = int(size / len(y.value_counts().index))\n\n    for label in y.value_counts().index:\n        samples = y[y == label].index.values\n        index_range = range(samples.shape[0])\n        indexes = np.random.choice(index_range, size=n_smp, replace=False)\n        subsample += samples[indexes].tolist()\n\n    return subsample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample 10% of the data\nsample = balanced_subsample(full['label'], len(full)*0.1) \n\n# extract the indices picked by the function to create subsample\nsample = full.iloc[sample, :]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = sample['clean'].astype(str)\ndocs = text.to_list()\n\nlabels = pd.array(sample['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(docs))\n# print('\\n')\nprint(len(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check number of unique words to estimate a reasonable vocab size\n\none_str = ''.join(text)\nunique_words = Counter(one_str.split())\nlen(unique_words) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 46856  # 20000 for the entire dataset\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs[:1]) # list of lists","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = []\n\nfor i in encoded_docs:\n    x.append(len(i))\n    \nprint(\"In my sample the largest document has\", max(x), \"words.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now every document will be represented by a vector of the same length: 3000 values / 'words'\n\nmax_length = 2996\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = padded_docs\ny = sample['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim = vocab_size, output_dim = 32, input_length = max_length))\n\nmodel.add(Bidirectional(LSTM(64, activation='linear')))\nmodel.add(Dense(32, activation='linear'))\nmodel.add(Dense(1, activation='sigmoid'))\n   \nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n   \nmodel.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# fit the model\nhistory = model.fit(X_train, y_train, epochs=5, verbose=1, batch_size=30, validation_split = 0.2)\n# evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.style.use(\"fivethirtyeight\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"tf-gpu","language":"python","name":"tf-gpu"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":4}