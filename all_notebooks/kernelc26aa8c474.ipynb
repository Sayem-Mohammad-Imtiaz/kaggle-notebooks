{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\n\nimport tensorflow as tf\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import text_to_word_sequence, Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Optimizer\nfrom keras import callbacks\n\nimport matplotlib as mpl\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom keras.utils import plot_model \nfrom IPython.display import Image\n\nnp.random.seed(17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task Description\n- Sequence-to-sequence encoder-decoder model in Keras\n- Task of text summarization\n- Dataset: Amazon reviews and summaries from Kaggle https://www.kaggle.com/snap/amazon-fine-food-reviews\n- only a 100k subset of this dataset is used","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_FILE_PATH = '../input/nlp-topic-modelling/Reviews.csv' \nEMB_DIR = '../input/glove6b300dtxt/glove.6B.300d.txt'\n\nMAX_TEXT_VOCAB_SIZE = 30000\nMAX_SUMMARY_VOCAB_SIZE = 10000\nMAX_TEXT_LEN = 100\nMAX_SUMMARY_LEN = 5\n\nLSTM_DIM = 300\nEMBEDDING_DIM = 300\n\nBATCH_SIZE = 128\nN_EPOCHS = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(DATA_FILE_PATH)\nprint('Number of rows = ', df.shape[0])\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting all columns to string\ndf['Summary'] = df['Summary'].apply(lambda x: str(x))\ndf['Text'] = df['Text'].apply(lambda x: str(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_len = lambda x:len(x)\ndf['Summary_length'] = df.Summary.apply(sent_len)\ndf[df['Summary_length']<5]['Summary'].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summaries having lesser than 5 characters can be discarded - noisy data, which is about 1500 rows of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summaries having lesser than 5 characters can be discarded - noisy data\nindices = df[df['Summary_length']<5].index\ndf.drop(indices, inplace=True)\n\n# Can drop the Summary_length columns - to save memory\ndf.drop('Summary_length', inplace=True, axis=1)\n\ndf.reset_index(inplace=True, drop=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count = lambda x:len(x.split()) # Word count for each question\ndf['s_wc'] = df.Summary.apply(word_count)\ndf['t_wc'] = df.Text.apply(word_count)\n\np = 75.0\n\nprint(' Summary :{} % of the summaries have a length less than or equal to {}'.format(p, np.percentile(df['s_wc'], p)))\nprint(' Text :{} % of the texts have a length less than or equal to {}'.format(p, np.percentile(df['t_wc'], p)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the above stats to decide on the MAX number of tokens of the encoder and decoder sides","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_list = [' '.join(word_tokenize(x)[:MAX_TEXT_LEN]) for x in df['Text']]\ntext_list[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_list = [' '.join(word_tokenize(x)[:MAX_SUMMARY_LEN]) for x in df['Summary']]\nsummary_list[:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separate embedding matrices - one for encoder side (Texts), one for decoder side (Summary)\n\n- We could use a shared embedding matrix if the source and target have similar vocabularies (same size and same domain). Eg. for dialogue generation\n\n\n- If the vocabularies are different, e.g., machine translation, we definitely need to use different embedding matrices\n\n\n- In this case, the summaries (decoder side) will have a smaller vocabulary than the texts (source side). It is not efficient to use a shared vocabulary/shared embedding matrix. For example, on the decoder side if we have 30,000 words vs. 5,000 words -> it is more difficult for the decoder (the softmax) to select 1 word from 30,000 choices vs. 1 from 5,000.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preparing the word-to-index mapping\nSpecial tokens\n- PAD \n- SOS (only required for the decoder input, for the 1st timestep to inform the decoder that it can now start decoding)\n- EOS\n- UNK","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_list = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n\ntext_tokenizer = Tokenizer(filters=filter_list)\ntext_tokenizer.fit_on_texts(text_list)\nprint(\"Number of words in TEXT vocabulary:\", len(text_tokenizer.word_index))\n\nsummary_tokenizer = Tokenizer(filters=filter_list)  \nsummary_tokenizer.fit_on_texts(summary_list)\nprint(\"Number of words in SUMMARY vocabulary:\", len(summary_tokenizer.word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_word_index = {}\ntext_word_index['PAD'] = 0\ntext_word_index['UNK'] = 1\ntext_word_index['EOS'] = 2\n\nfor i, word in enumerate(dict(text_tokenizer.word_index).keys()):\n    text_word_index[word] = i+3 # Move existing indices up by 3 places\n    \ntext_tokenizer.word_index = text_word_index\nX = text_tokenizer.texts_to_sequences(text_list)\n\n# Replace OOV words with UNK token\n# Append EOS to the end of all sentences\nfor i, seq in enumerate(X):\n    if any(t>=MAX_TEXT_VOCAB_SIZE for t in seq):\n        seq = [t if t<MAX_TEXT_VOCAB_SIZE else text_word_index['UNK'] for t in seq ]\n    seq.append(text_word_index['EOS'])\n    X[i] = seq    \n    \n# Padding and truncating sequences\nX = pad_sequences(X, padding='post', truncating='post', maxlen=MAX_TEXT_LEN, value=text_word_index['PAD'])\n\n# Finalize the dictionaries\ntext_word_index = {k: v for k, v in text_word_index.items() if v < MAX_TEXT_VOCAB_SIZE} \ntext_idx_to_word = dict((i, word) for word, i in text_word_index.items()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_word_index = {}\nsummary_word_index['PAD'] = 0\nsummary_word_index['UNK'] = 1\nsummary_word_index['EOS'] = 2\nsummary_word_index['SOS'] = 3\n\nfor i, word in enumerate(dict(summary_tokenizer.word_index).keys()):\n    summary_word_index[word] = i+4 # Move existing indices up by 4 places\n    \nsummary_tokenizer.word_index = summary_word_index\nY = summary_tokenizer.texts_to_sequences(summary_list)\n\n# Replace OOV words with UNK token\n# Append EOS to the end of all sentences\nfor i, seq in enumerate(Y):\n    if any(t>=MAX_SUMMARY_VOCAB_SIZE for t in seq):\n        seq = [t if t<MAX_SUMMARY_VOCAB_SIZE else summary_word_index['UNK'] for t in seq ]\n    seq.append(summary_word_index['EOS'])\n    Y[i] = seq    \n    \n# Padding and truncating sequences\nY = pad_sequences(Y, padding='post', truncating='post', maxlen=MAX_SUMMARY_LEN, value=summary_word_index['PAD'])\n\n# Finalize the dictionaries\nsummary_word_index = {k: v for k, v in summary_word_index.items() if v < MAX_SUMMARY_VOCAB_SIZE} \nsummary_idx_to_word = dict((i, word) for word, i in summary_word_index.items()) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Train-Validation-Test Splits","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val_test, Y_train, Y_val_test = train_test_split(X, Y, test_size=0.05)\nX_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embedding Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load GloVe word embeddings \n# Download Link: https://nlp.stanford.edu/projects/glove/\nprint(\"[INFO]: Reading Word Embeddings ...\")\n# Data path\nembeddings = {}\nf = open(EMB_DIR)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1:], dtype='float32')\n    embeddings[word] = vector\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(text_word_index), EMBEDDING_DIM)) \n\nfor word, i in text_word_index.items(): # i=0 is the embedding for the zero padding\n    try:\n        embeddings_vector = embeddings[word]\n    except KeyError:\n        embeddings_vector = None\n    if embeddings_vector is not None:\n        encoder_embeddings_matrix[i] = embeddings_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(encoder_embeddings_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(summary_word_index), EMBEDDING_DIM)) \n\nfor word, i in summary_word_index.items(): # i=0 is the embedding for the zero padding\n    try:\n        embeddings_vector = embeddings[word]\n    except KeyError:\n        embeddings_vector = None\n    if embeddings_vector is not None:\n        decoder_embeddings_matrix[i] = embeddings_vector\n        \ndel embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(decoder_embeddings_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model during training\nInput: [where, do, you, live, ?, EOS] <br>\nOutput: [I, reside, in, waterloo, EOS] <br>\nWhen the model predicts wrongly, we need to correct it so that errors are not accumulated. __TEACHER FORCING__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Keras Model Building\nAdapted from Keras blog post:\nhttps://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, LSTM, Dense, Embedding\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When the datasets are huge, it is a good idea to make use of the keras fit_generator. <br>\nBasically, we need to generator function that feeds inputs and corresponding outputs in a batch_wise fashion. <br>\n\nIn our case, we have \n- encoder inputs\n- decoder inputs (which is the ground truth delayed by 1 timestep)\n- decoder outputs \n\nFor decoder outputs for each token, we need to predict a softmax of the vocabulary |V|. If we have _m_ tokens and a training data size of _n_, we need to store a huge tensor of size _n_ x _m_ x |V|. In our case, 100000 x 5 x 10000.\n<br>\n\nThis may not fit in memory. Even if it fits, it is not a good practice to keep such large variables, hence use the fit generator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_generator(X, Y, BATCH_SIZE):\n\n    # this line is just to make the generator infinite, keras needs that    \n    while True:\n        \n        batch_start = 0\n        batch_end = BATCH_SIZE\n\n        while batch_start < len(X):\n            \n            [X_enc, X_dec], Y_dec = prepare_data(X, Y, batch_start, batch_end) \n            \n            yield ([X_enc, X_dec], Y_dec) \n\n            batch_start += BATCH_SIZE   \n            batch_end += BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(X, Y, batch_start, batch_end):\n    \n    # Encoder input\n    X_enc = X[batch_start:batch_end] \n\n    # Decoder input\n    # Concatenate a column of 3s (i.e., SOS token) to the Y and remove the last element\n    X_dec = np.c_[3 * np.ones([len(Y[batch_start:batch_end])]), Y[batch_start:batch_end, :-1]]\n\n    # Decoder output - one hot encoded for softmax layer - 1 in |V|\n    Y_dec = np.array([to_categorical(y, num_classes=len(summary_word_index)) for y in Y[batch_start:batch_end]])\n\n    return [X_enc, X_dec], Y_dec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sequence-to-sequence model for __training__:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"K.set_learning_phase(1) # 1 for training, 0 for inference time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoder Setup\nenc_input = Input(shape=(MAX_TEXT_LEN, ), name='encoder_input')\nenc_emb_look_up = Embedding(input_dim=MAX_TEXT_VOCAB_SIZE,\n                             output_dim=EMBEDDING_DIM,\n                             weights = [encoder_embeddings_matrix], \n                             trainable=False, \n                             mask_zero=True,\n                             name='encoder_embedding_lookup')\n\nenc_emb_text = enc_emb_look_up(enc_input)\n\nencoder_lstm = LSTM(LSTM_DIM, return_state=True, name='encoder_lstm', dropout=0.2) # To return the final state of the encoder\nencoder_lstm2=LSTM(128,return_state=True,dropout=0.3)\nencoder_outputs, state_h, state_c = encoder_lstm2(enc_emb_text)\nencoder_states = [state_h, state_c] # Discard encoder_outputs (at each time step) and only keep the final states.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decoder Setup\ndec_input = Input(shape=(None, ), name='decoder_input') # Specify None instead of MAX_SUMMARY_LEN\n# So that we can use the decoder with one-token prediction at a time\n# By specifying MAX_SUMMARY_LEN, we limit this capability of the decoder\n# That is, if MAX_SUMMARY_LEN is specified, we always have to provide MAX_SUMMARY_LEN tokens as input to the decoder\n# By not specifying, we can dynamically adjust it, i.e., MAX_SUMMARY_LEN during training and 1 during inference\n\ndec_emb_look_up = Embedding(input_dim=MAX_SUMMARY_VOCAB_SIZE,\n                             output_dim=EMBEDDING_DIM,\n                             weights = [decoder_embeddings_matrix], \n                             trainable=False, \n                             mask_zero=True,\n                             name='decoder_embedding_lookup')\n\ndec_emb_text = dec_emb_look_up(dec_input)\n\n# We set up our decoder to return full output sequences,\n# and to return internal LSTM states (h, c) as well. We don't use the \n# return states in the training model, but we will use them during inference.\ndecoder_lstm = LSTM(LSTM_DIM, return_sequences=True, return_state=True, name='decoder_lstm', dropout=0.3)\ndecoder_lstm2 = LSTM(128, return_sequences=True, return_state=True, dropout=0.3)\n# Dropout needs to be set back to 0.0 for the inference time\n\n# Hidden state initialization using `encoder_states` as initial state.\ndecoder_outputs, _, _ = decoder_lstm2(dec_emb_text,\n                                     initial_state=encoder_states)\n\ndecoder_dense = Dense(MAX_SUMMARY_VOCAB_SIZE, activation='softmax', name='output_layer')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([enc_input, dec_input], decoder_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set optimizer and loss function \noptimizer = keras.optimizers.Adam(lr=0.002) # Try a different learning rate\n\nloss = 'categorical_crossentropy'\n\nfilepath=\"saved_models/seq2seq_textsummarization_{epoch:02d}_{val_loss:.4f}.h5\"\ncheckpoint = callbacks.ModelCheckpoint(filepath, \n                                       monitor='val_loss', \n                                       verbose=0, \n                                       save_best_only=False)\ncallbacks_list = [checkpoint]\n\nmodel.compile(optimizer=optimizer, loss=loss)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"__Note:__ It is possible to write your own callback functions by instructing what needs to be done at the end of each epoch. <br>\n\nHomework - write a callback function to calculate BLEU scores on validation set and print generated samples at the end of each epoch <br>\nCheck out https://keunwoochoi.wordpress.com/2016/07/16/keras-callbacks/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\nVAL_STEPS = len(X_val)//BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(batch_generator(X_train, Y_train, BATCH_SIZE), \n                    steps_per_epoch = STEPS_PER_EPOCH, \n                    epochs = 50,\n                    validation_data = batch_generator(X_val, Y_val, BATCH_SIZE), \n                    validation_steps = VAL_STEPS, \n                    callbacks = callbacks_list,\n                   )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Note__: It is best to write the above code onto a .py file and execute it directly on the command line. \n\nWe may need to train the model for over 100 epcohs to get good sentence generations. \n\nIn this case, the model was trained for 100 epochs, about 300 seconds (~5mins) per epoch on a single GPU.","execution_count":null},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Load Saved Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_STEPS = len(X_test)//BATCH_SIZE\n\n# Obtain the last layer output, i.e., the softmax probabilities\npreds = model.predict_generator(batch_generator(X_test, Y_test, BATCH_SIZE), steps = TEST_STEPS) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Greedy Decoding\noutputs = [np.argmax(p, axis = -1) for p in preds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_summary_sentence(gen_ids):\n    pad = summary_word_index['PAD']\n    eos = summary_word_index['EOS']\n    print(' '.join([summary_idx_to_word[w] for w in gen_ids if w not in [pad, eos]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_input_text(inp_ids):\n    pad = text_word_index['PAD']\n    eos = text_word_index['EOS']\n    print(' '.join([text_idx_to_word[w] for w in inp_ids if w not in [pad, eos]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5,35):\n    show_input_text(X_test[i])\n    print()\n    print('Summary - ', end='')\n    show_summary_sentence(outputs[i])\n    print('\\n----------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, gen in enumerate(outputs[10:20]):\n    print(i, end=', ')\n    show_summary_sentence(gen)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bi lstm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoder Setup\n\n# # Encoder Setup\n# enc_input = Input(shape=(MAX_TEXT_LEN, ), name='encoder_input')\n# enc_emb_look_up = Embedding(input_dim=MAX_TEXT_VOCAB_SIZE,\n#                              output_dim=EMBEDDING_DIM,\n#                              weights = [encoder_embeddings_matrix], \n#                              trainable=False, \n#                              mask_zero=True,\n#                              name='encoder_embedding_lookup')\n\n\nencoder_embedding_layer = Embedding(input_dim = MAX_TEXT_VOCAB_SIZE, \n                                    output_dim =EMBEDDING_DIM,\n                                    \n                                    weights = [encoder_embeddings_matrix],\n                                    trainable = False,\n                                    mask_zero=True)\nenc_input = Input(shape=(MAX_TEXT_LEN, ), name='encoder_input')\nenc_emb_look_up = encoder_embedding_layer(encoder_inputs)\nencoder_LSTM = LSTM(300, return_state=True)\nencoder_LSTM_R = LSTM(128, return_state=True, go_backwards=True)\nencoder_outputs_R, state_h_R, state_c_R = encoder_LSTM_R(enc_emb_look_up)\nencoder_outputs, state_h, state_c = encoder_LSTM(enc_emb_look_up)\nfinal_h = Add()([state_h, state_h_R])\nfinal_c = Add()([state_c, state_c_R])\nencoder_states = [final_h, final_c]\n\ndecoder_inputs = Input(shape=(MAX_SUMMARY_LEN,))\ndecoder_embedding = decoder_embedding_layer(decoder_inputs)\ndecoder_LSTM = LSTM(300, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states) \ndecoder_dense = Dense(MAX_SUMMARY_VOCAB_SIZE,activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\nmodel= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decoder Setup\ndec_input = Input(shape=(None, ), name='decoder_input') # Specify None instead of MAX_SUMMARY_LEN\n# So that we can use the decoder with one-token prediction at a time\n# By specifying MAX_SUMMARY_LEN, we limit this capability of the decoder\n# That is, if MAX_SUMMARY_LEN is specified, we always have to provide MAX_SUMMARY_LEN tokens as input to the decoder\n# By not specifying, we can dynamically adjust it, i.e., MAX_SUMMARY_LEN during training and 1 during inference\n\ndec_emb_look_up = Embedding(input_dim=MAX_SUMMARY_VOCAB_SIZE,\n                             output_dim=EMBEDDING_DIM,\n                             weights = [decoder_embeddings_matrix], \n                             trainable=False, \n                             mask_zero=True,\n                             name='decoder_embedding_lookup')\n\ndec_emb_text = dec_emb_look_up(dec_input)\n\n# We set up our decoder to return full output sequences,\n# and to return internal LSTM states (h, c) as well. We don't use the \n# return states in the training model, but we will use them during inference.\ndecoder_lstm = Bidirectional(LSTM(LSTM_DIM, return_state=True, name='decoder_bi_lstm', dropout=0.3)) # To return the final state of the encoder\ndecoder_lstm2 = Bidirectional(LSTM(128, return_state=True, name='decoder_bi_lstm2', dropout=0.2))\n# Dropout needs to be set back to 0.0 for the inference time\n\n# Hidden state initialization using `encoder_states` as initial state.\ndecoder_outputs= decoder_lstm2(dec_emb_text,initial_state=encoder_states)\n\ndecoder_dense=Dense(MAX_SUMMARY_VOCAB_SIZE, activation='softmax', name='output_layer')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([enc_input, dec_input], decoder_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}