{"cells":[{"metadata":{},"cell_type":"markdown","source":"  **Customer Segmentation using RFM analysis**\n* Successful companies are the ones that know their customers so well that they can anticipate their needs.\n* Segmenting the customers to better serve them.\n* By using online retail ecommerce dataset containing anonymized customer transactions.\n* Will prepare the segments created,by using RFM Analysis.\n* Understanding of practical customer behavioral analytics and segmentation techniques.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import required libraries\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime as dt\nimport seaborn as sns # used for plot interactive graph. \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importig dataset \n\ndata = pd.read_csv('../input/online-retail-ii-uci/online_retail_II.csv')\n\n#check first 5 rows \n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets  do some basic data preparation **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# No of records and columns \ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for any duplicate record first\n\ndata.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove duplicate items \ndata = data[~data.duplicated()]\n\n# check no of records now \n\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for the missing values \n\ntotal = data.isnull().sum().sort_values(ascending=False)\n\npercent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 22 % for cust ID missing and quite few no of Discriptions , so could remove the missing cust ID as there is no way to impute in dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Don't need records with Null customer id,so deleting them from the dataframe  \n\ndata.dropna(axis = 0, subset = ['Customer ID'], inplace = True)\n\ndata.shape  # so no of records are reduced now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the data types of variables \ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change the datatypes of InvoiceDate to datetime object and Customer ID to string\n\n# Convert remaining Customer Ids to string type\n\ndata['Customer ID']= data['Customer ID'].astype(str)\n\ndata['InvoiceDate'] = pd.to_datetime(data ['InvoiceDate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the descriptive statstics of numerical variables \n\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is not normalize and huge difference in min and maximum values and Std so before applying any machine learning technique has to unskew the variables.\n\nSome value of Quantity is in negative,so need more investigation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are negative quantities and corresponding Invoice no is started with string C (Cancelled items)\n\ndisplay(data.sort_values('Quantity')[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check and remove transactions with cancelled items.\n\ndata_new = data[~data.Invoice.str.contains('C', na=False)]\n\n# check no of records, further reduced \n\ndata_new.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now there is no negative value of Quantity variable \n\ndisplay(data_new.sort_values('Quantity')[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check which are countries with maximum no of transactions \n\ncustomer_country=data_new[['Country','Customer ID']].drop_duplicates()\n\ncustomer_country.groupby(['Country'])['Customer ID'].aggregate('count').reset_index().sort_values('Customer ID', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So more than 90% of the customers in the data are from the United Kingdom","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**RFM analysis: How recently, How often , How much customer buy oftenly**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets change the format of InvoiceDate \n\nimport time\nfrom datetime import datetime, date, time, timedelta\n\ndata_new['InvoiceDate'] = pd.to_datetime(data_new['InvoiceDate']).dt.date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oldest and latest date \nprint('Min:{}; Max:{}'.format(min(data_new.InvoiceDate),\nmax(data_new.InvoiceDate)))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a hypothetical snapshot_day data as if we're doing analysis recently.\n\nsnapshot_date = max(data_new.InvoiceDate) + timedelta(days=1)\nprint(snapshot_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new columns called TotalSum column = Quantity x UnitPrice.\n\ndata_new['TotalSum'] = data_new['Quantity'] * data_new['Price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new.head() # to check new column ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RFM Customer Segmentation**\n\nLets calculate RFM metrics to built a dataset with Recency, Frequency, and Monetary values!\n\nCreate five customer-level features that will then use in predicting next month's customer transactions. \n\nThese features capture highly predictive customer behavior patterns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate data on a customer level\ndatamart = data_new.groupby(['Customer ID']).agg({'InvoiceDate': lambda x: (snapshot_date - x.max()).days,'Invoice': 'count','TotalSum': 'sum'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename columns for easier interpretation # term used as recency, frequency and Monetary\ndatamart.rename(columns = {'InvoiceDate': 'Recency','Invoice': 'Frequency','TotalSum': 'MonetaryValue'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the first few rows\ndatamart.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Will calculate quartile value for each column (Recency, Frequency & Monetary) and name then R, F, M\n* Recency - days since last customer transaction\n* Frequency - number of transactions in the last 12 months\n* Monetary Value - total spend in the last 12 months","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" \n#Recency with a decreasing range of 4 through 1\n\nr_labels = range(4, 0, -1)  # create generators \n\n# Create a spend quartile with 4 groups and pass the previously created labels \nrecency_quartiles = pd.qcut(datamart['Recency'], q=4, labels=r_labels)\n\n# Assign the quartile values to the Recency_Quartile column in `data`\ndatamart['Recency_Quartile'] = recency_quartiles \n\n# Print `data` with sorted Recency_Days values\nprint(datamart.sort_values('Recency'))              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So can check most recent has assigned 1 status in Recency_Quartile and least recent has status of 0(more depends on the business goal)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Frequency and Monetary quartiles  and Create labels for Frequency and monetray\nf_labels = range(1,5)\nm_labels = range(1,5)\n\n#Assign these labels to 4 equal percentile groups based on Frequency.\nf_quartiles = pd.qcut(datamart['Frequency'], 4, labels = f_labels)\n\n## Assign these labels to 4 equal percentile groups\nm_quartiles = pd.qcut(datamart['MonetaryValue'], 4, labels = m_labels)\n\n#Create new columns F and M\n\ndatamart = datamart.assign(F = f_quartiles.values)\ndatamart = datamart.assign(M = m_quartiles.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check first few rows with new columns   # \ndatamart.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now Build RFM Segment and RFM Score\n* Concatenate RFM quartile values to RFM_Segment\n* Sum RFM quartiles values to RFM_Score : RFM Score which is a sum of the R (here Recency_Quartile), F, and M values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Concatenate RFM quartile values to RFM_Segment and converted to string\n\ndef join_rfm(x): return str(x['Recency_Quartile']) + str(x['F']) + str(x['M'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#group the customers into three separate groups based on Recency,Frequency and monetary values\n\ndatamart['RFM_Segment'] = datamart.apply(join_rfm, axis=1)\n\n#Sum RFM quartiles values to RFM_Score\n\ndatamart['RFM_Score'] = datamart[['Recency_Quartile','F','M']].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndatamart.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So with final Results having 2 new columns : RFM_Segment (combine RFM value is RFM_segment) & RFM_Score is addition of RFM_segment for eg.(2+2+4):8 is RFM_score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing RFM segments : Largest 10 RFM segments\n\ndatamart.groupby('RFM_Segment').size().sort_values(ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering on RFM segments \n\n# Select bottom RFM segment \"111\" and view top 5 rows\n\ndatamart[datamart['RFM_Segment']=='111'][:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple selection of customer based on their score can be done to find out worth of different customers and Who are the top 10 of our best customers!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Summary metrics per RFM Score\n\ndatamart.groupby('RFM_Score').agg({'Recency': 'mean','Frequency': 'mean','MonetaryValue': ['mean', 'count'] }).round(1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Each mean RFM value is better with higher RFM score\n\n* For example with RFM_Score of 12.0 ,recency mean is 10.7 ,Frequecy mean 560.7 and Monetrary value 14708.7\n\n* So to conclude those who have high frequency mean have high Monetrary value and higher RFM_Score and and  less mean of Recency.\n\n* Therefore high RFM_score,very recent and high monetary value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Now grouping into named segments finally.\n\n* Creating custom segments based on RFM score.\n\n* Creating a function to build segmentation and then assign it to each customer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use RFM score to group customers into Gold, Silver and Bronze segments.\n\ndef segment_me(df):\n    if df['RFM_Score'] >= 9:\n       return 'Gold'\n    elif (df['RFM_Score'] >= 5) and (df['RFM_Score'] < 9):\n       return 'Silver'\n    else:\n       return 'Bronze'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new variable called Segment_Level\n\ndatamart['Segment_level'] = datamart.apply(segment_me, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analyze average values of Recency, Frequency and MonetaryValue for the custom segments created.\n\ndatamart.groupby('Segment_level').agg({'Recency': 'mean','Frequency': 'mean','MonetaryValue': ['mean', 'count']}).round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So by analyzing custom segments with different segement level with mean of RFM values can check in Gold segment customer has lowest mean of Recency (latest recency) and highest mean value frequency and Monetary.\n\nFor Further analysis RFM matrix can be used for Kmeans clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}