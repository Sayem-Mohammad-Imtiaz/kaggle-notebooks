{"cells":[{"metadata":{},"cell_type":"markdown","source":"Sentiment Classification \nAccuarcy : ~77%"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('twitter_samples')\n!pip install -U trax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import twitter_samples\nimport trax\nimport trax.fastmath as tnp\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport os\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n  all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n  all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n  return all_positive_tweets,all_negative_tweets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split data into training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_tweets ,negative_tweets = load_data()\npos_tweet_train = positive_tweets[:3000]\nneg_tweet_train = negative_tweets[:3000]\npos_tweet_val = positive_tweets[3000:]\nneg_tweet_val = negative_tweets[3000:]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenize and create Word Vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train = pos_tweet_train + neg_tweet_train\nrandom.shuffle(all_train)\ntokenizer = Tokenizer(num_words=1000,oov_token='<OOV>')\ntokenizer.fit_on_texts(all_train)\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_seq_train = tokenizer.texts_to_sequences(pos_tweet_train)\nneg_seq_train = tokenizer.texts_to_sequences(neg_tweet_train)\npos_seq_val = tokenizer.texts_to_sequences(pos_tweet_val)\nneg_seq_val = tokenizer.texts_to_sequences(neg_tweet_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Generator for Training and Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_gen(pos,neg,batch_size,epoch_num):\n    assert batch_size %2 ==0\n    \n    pos_index_line = list(range(len(pos)))\n    neg_index_line = list(range(len(neg)))\n    pos = pad_sequences(pos,maxlen = 25,padding = 'post')\n    neg = pad_sequences(neg,maxlen = 25,padding = 'post')\n    batch = []\n    n_to_take = batch_size //2\n    \n            \n    for epoch in range(epoch_num):\n        random.shuffle(pos_index_line)\n        random.shuffle(neg_index_line)\n        pos_tmp = []\n        neg_tmp =[]\n        index = 0\n        \n        while index < len(pos) or index < len(neg):\n            for i in range(n_to_take):\n                pos_tmp.append(pos[pos_index_line[index]])\n                neg_tmp.append(neg[neg_index_line[index]])\n                index += 1\n             \n            input_ = np.concatenate((pos_tmp,neg_tmp),axis=0)\n            target = np.append(np.ones(len(pos_tmp)) , np.zeros(len(neg_tmp)) )\n            example_weights = np.ones_like(target)\n            yield np.array(input_) , target, example_weights\n\ndef train_data_gen(batch_size,epoch):\n    return data_gen(pos_seq_train,neg_seq_train,batch_size,epoch)\n   \n\n\ndef val_data_gen(batch_size,epoch):\n    return data_gen(pos_seq_val,neg_seq_val,batch_size,epoch)\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from trax import layers as tl\n\nmodel = tl.Serial(\n    tl.Embedding(vocab_size = len(word_index),d_feature=500),\n    tl.Mean(axis = 1),\n    tl.Dense(50),\n    tl.Dense(50),\n    tl.Dense(20),\n    tl.Dense(2),\n    tl.LogSoftmax()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ouput directory for saving model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir output_dir","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"from trax.supervised import training\n\ntrain_task = training.TrainTask(\n    labeled_data = train_data_gen(300,100),\n    loss_layer = tl.CrossEntropyLoss(),\n    optimizer = trax.optimizers.Adam(0.0001),\n    n_steps_per_checkpoint = 50,\n)\n\neval_task = training.EvalTask(\n    labeled_data = val_data_gen(20,100),\n    metrics = [tl.CrossEntropyLoss() , tl.Accuracy()],\n    n_eval_batches = 20,\n)\n\noutput_dir = os.path.expanduser('~/output_dir/')\n\ntraining_loop = training.Loop(\n  model,\n  train_task,\n  eval_tasks = [eval_task],\n  output_dir= output_dir\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loop.run(600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}