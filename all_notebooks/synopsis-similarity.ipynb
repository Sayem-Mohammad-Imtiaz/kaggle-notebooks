{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# Synopsis Similarity\n\nCalculate the synopsis similarity and save it as csv file.\n\n## Import library","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom collections import Counter\nimport math","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c227de852c13080175a329c06b0e346b051ffe76"},"cell_type":"markdown","source":"## Cosine similarity function","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"567ceb5ebfd963f8d280110078f9b874a5c7b434"},"cell_type":"code","source":"def get_cosine(vec1, vec2):\n     intersection = set(vec1.keys()) & set(vec2.keys())\n     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n     if not denominator:\n        return 0.0\n     else:\n        return 1 - (float(numerator) / denominator)\n    \ndef ConstructMatrixSynopsis(data):\n    dsyn = np.zeros((len(data),len(data)))\n    for i,dti in enumerate(data):\n        #create frequency dictionary\n        counter_i=Counter(dti)\n        for j,dtj in enumerate(data[0:i+1]):\n            counter_j=Counter(dtj)\n            if i == j:\n                dsyn[i][j] = -1\n            else:\n                dsyn[i][j] = float(\"{0:.5f}\".format(get_cosine(counter_i,counter_j)))\n    msyn = np.matrix(dsyn)\n    newsyn = msyn + np.transpose(msyn)\n    dfnewsyn = pd.DataFrame(newsyn,index=df['Anime_ID'],columns=df['Anime_ID'])\n    return dfnewsyn\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edcfc0241c3bc83b17027f53457e4644deefbd86"},"cell_type":"markdown","source":"## Initiating Tokenizer and Stemmer","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7c0bc64787753986a9d48fd7435471e16df274cc","collapsed":true,"trusted":true},"cell_type":"code","source":"pattern = r'\\b[^\\d\\W]+\\b'\ntokenizer = RegexpTokenizer(pattern)\nen_stop = get_stop_words('en')\nlemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d31cdb262af5004a5f6aad07c0eec92420b55b8f"},"cell_type":"markdown","source":"## Read the data","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"89c7f6e136e767e45c5cc0f2395d2dc1d5012a1d"},"cell_type":"code","source":"# Input from csv\ndf = pd.read_csv('../input/datasynopsis-all-share-new.csv',sep='|')\n\n# sample data\nprint(df['Synopsis'].head(2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f7d29531bbe13a5ae3dfe8ce4d9f8400c59fc5b"},"cell_type":"markdown","source":"## Perform Tokenization, words removal, and Lemmatization","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"f2a80e531cc21bdb0598a39cc312a4d435d69935"},"cell_type":"code","source":"# list for tokenized documents in loop\ntexts = []\n\n# loop through document list\nfor i in df['Synopsis'].iteritems():\n    # clean and tokenize document string\n    raw = str(i[1]).lower()\n    tokens = tokenizer.tokenize(raw)\n\n    # remove stop words from tokens\n    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n    \n    # lemmatize tokens\n    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n    \n    # remove one character\n    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n\n       \n    # add tokens to list\n    texts.append(new_lemma_tokens)\n\n# sample data\nprint(texts[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b43cafd3d749151a1e536fa5326d82622d96f2c"},"cell_type":"markdown","source":"## Calculate SYNOPSIS similarity","outputs":[],"execution_count":null},{"metadata":{"_uuid":"86455ef30b83921863d8b8a36bb1261d7db52f42","scrolled":true,"trusted":true},"cell_type":"code","source":"synopsis_matrix = ConstructMatrixSynopsis(texts)\nprint(synopsis_matrix.head(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd41bc924eac7e7d63dfe43baf195a076f374d6"},"cell_type":"markdown","source":"## Save to temporary file","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c1a092566ea8400ff8d0d40a31fb4501f5b523dd","collapsed":true,"trusted":true},"cell_type":"code","source":"synopsis_matrix.to_csv('sim_synopsis.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}