{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nposts_df = pd.read_csv(\"../input/reddit-selfposts/rspct.tsv\", sep='\\t')\nsubred_df = pd.read_csv(\"../input/reddit-selfposts/subreddit_info.csv\").set_index(['subreddit'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = posts_df.join(subred_df, on='subreddit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_mapping = {\n    'id': 'id',\n    'subreddit': 'subreddit',\n    'title': 'title',\n    'selftext': 'text',\n    'category_1': 'category',\n    'category_2': 'subcategory',\n    'category_3': None, # no data\n    'in_data': None, # not needed\n    'reason_for_exclusion': None # not needed\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define remaining columns\ncolumns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n\n# select and rename those columns\ndf = df[columns].rename(columns=column_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['category'] == 'autos']\n\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(1).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = None ###\ndf.sample(1, random_state=7).T\npd.options.display.max_colwidth = 200 ###","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle(\"reddit_dataframe.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sqlite3\ndb_name = \"reddit-selfposts.db\"\n\ncon = sqlite3.connect(db_name)\ndf.to_sql(\"posts\" , con , index=False , if_exists=\"replace\")\ncon.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"\"\"\nAfter viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\nit got me thinking about the best match ups.\n<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\nCaptain America<lb>\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nRE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n\ndef impurity(text,min_len=10):\n    \n    if text is None or len(text) < min_len:\n        return 0\n    else:\n        return len(RE_SUSPICIOUS.findall(text))/len(text)\n    \nprint(impurity(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add new column to data frame\ndf['impurity'] = df['text'].apply(impurity , min_len=10)\n\ndf[['text' , 'impurity']].sort_values(by='impurity' , ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n\ndef count_words(df, column='tokens', preprocess=None, min_freq=2):\n\n    # process tokens and update counter\n    def update(doc):\n        tokens = doc if preprocess is None else preprocess(doc)\n        counter.update(tokens)\n\n    # create counter and run through all data\n    counter = Counter()\n    df[column].progress_map(update)\n\n    # transform counter into data frame\n    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n    freq_df = freq_df.query('freq >= @min_freq')\n    freq_df.index.name = 'token'\n    return freq_df.sort_values('freq', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing noise through Regular expressions\nimport html\n\ndef clean(text):\n    # convert html escapes like &amp; to characters.\n    text = html.unescape(text)\n    # tags like <tab>\n    text = re.sub(r'<[^<>]*>', ' ', text)\n    # markdown URLs like [Some text](https://....)\n    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n    # text or code in brackets like [0]\n    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n    # standalone sequences of specials, matches &# but not #cool\n    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n    # standalone sequences of hyphens like --- or ==\n    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n    # sequences of white spaces\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text = clean(text)\nprint(clean_text)\nprint(\"Impurity:\", impurity(clean_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean_text'] = df['text'].map(clean)\ndf['impurity'] = df['clean_text'].apply(impurity,min_len=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['text' , 'impurity']].sort_values(by='impurity',ascending=False).head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install textacy\nimport textacy.preprocessing as tprep\ndef normalize(text):\n    text = tprep.normalize_hyphenated_words(text)\n    text = tprep.normalize_quotation_marks(text)\n    text = tprep.normalize_unicode(text)\n    text = tprep.remove_accents(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntext = \"The caf√© ‚ÄúSaint-Rapha√´l‚Äù is loca-\\nted on C√¥te d ºAzur.\"\n\nprint(normalize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textacy.preprocessing.resources import RE_URL\n\ncount_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textacy.preprocessing.replace import replace_urls\n\ntext = \"Check out https://spacy.io/usage/spacy-101\"\n\nprint(replace_urls(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean_text'] = df['clean_text'].map(replace_urls)\ndf['clean_text'] = df['clean_text'].map(normalize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)\ndf.drop(columns=['impurity'], inplace=True)\n\ncon = sqlite3.connect(db_name)\ndf.to_sql(\"posts_cleaned\", con, index=False, if_exists=\"replace\")\ncon.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"\"\"\n2019-08-10 23:32: @pete/@louis - I don't have a well-designed\nsolution for today's problem. The code of module AC68 should be -1.\nHave to think a bit... #goodnight ;-) üò©üò¨\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = re.findall(r'\\w\\w+' , text)\nprint(*tokens , sep='|')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RE_TOKEN = re.compile(r\"\"\"\n               ( [#]?[@\\w'‚Äô\\.\\-\\:]*\\w     # words, hashtags and email addresses\n               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n               )\n               \"\"\", re.VERBOSE)\n\ndef tokenize(text):\n    return RE_TOKEN.findall(text)\n\ntokens = tokenize(text)\nprint(*tokens, sep='|')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp.pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for token in doc:\n    print(token , end='|')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_func(doc , include_punct=False):\n    rows = []\n    for i, t in enumerate(doc):\n        if not t.is_punct or include_punct:\n            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_,\n                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n                   'pos_': t.pos_, 'dep_': t.dep_,\n                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n            rows.append(row)\n            \n    df = pd.DataFrame(rows).set_index('token')\n    df.index.name = None\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = display_func(doc , include_punct=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import textacy\ndef extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n    patterns = []\n    for pos in preceding_pos:\n        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n    spans = textacy.extract.matches(doc, patterns=patterns)\n    return [sep.join([t.lemma_ for t in s]) for s in spans]\n\nprint(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_name = \"reddit-selfposts.db\"\ncon = sqlite3.connect(db_name)\ndf = pd.read_sql(\"select * from posts_cleaned\", con)\ncon.close()\n\ndf['text'] = df['title'] + ': ' + df['text']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if spacy.prefer_gpu():\n    print(\"Working on GPU.\")\nelse:\n    print(\"No GPU found, working on CPU.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_nlp(doc):\n    return {\n    'lemmas'          : extract_lemmas(doc,\n                                     exclude_pos = ['PART', 'PUNCT',\n                                        'DET', 'PRON', 'SYM', 'SPACE'],\n                                     filter_stops = False),\n    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n    'noun_phrases'    : extract_noun_phrases(doc, ['NOUN']),\n    'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = textacy.extract.words(doc,\n            filter_stops = True,           # default True, no stopwords\n            filter_punct = True,           # default True, no punctuation\n            filter_nums = True,            # default False, no numbers\n            include_pos = ['ADJ', 'NOUN'], # default None = include all\n            exclude_pos = None,            # default None = exclude none\n            min_freq = 1)                  # minimum frequency of words\n\nprint(*[t for t in tokens], sep='|')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_lemmas(doc, **kwargs):\n    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n\nlemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\nprint(*lemmas, sep='|')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)\n\npatterns = [\"POS:ADJ POS:NOUN:+\"]\nspans = textacy.extract.matches(doc, patterns=patterns)\nprint(*[s.lemma_ for s in spans], sep='|')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n    patterns = []\n    for pos in preceding_pos:\n        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n    spans = textacy.extract.matches(doc, patterns=patterns)\n    return [sep.join([t.lemma_ for t in s]) for s in spans]\n\nprint(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_entities(doc, include_types=None, sep='_'):\n\n    ents = textacy.extract.entities(doc,\n             include_types=include_types,\n             exclude_types=None,\n             drop_determiners=True,\n             min_freq=1)\n\n    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 50\n\nfor i in range(0 , len(df) , batch_size):\n    docs = nlp.pipe(df['text'][i:i+batch_size])\n    \n    for j in enumerate(docs):\n        for col, values in extract_nlp(doc).items():\n            df[col].iloc[i+j] = values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}