{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook contains some experiments with topic modeling and SciBERT embeddings "},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{},"cell_type":"markdown","source":"Code from: https://www.kaggle.com/cogitae/create-corona-csv-file to create single csv file with data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob\nimport sys\nsys.path.insert(0, \"../\")\n\n#root_path = '/kaggle/input/CORD-19-research-challenge/2020-03-13'\nroot_path = '/kaggle/input/CORD-19-research-challenge'\n\n# Get all the files saved into a list and then iterate over them like below to extract relevant information\n# hold this information in a dataframe and then move forward from there. \n\n# Just set up a quick blank dataframe to hold all these medical papers. \n\ncorona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n                  \"abstract\": [None], \"text_body\": [None]}\ncorona_df = pd.DataFrame.from_dict(corona_features)\n\n# Cool so dataframe now set up, lets grab all the json file names. \n\n# For this we can use the very handy glob library\n\njson_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)\n\n# Now we just iterate over the files and populate the data frame. \n\ndef return_corona_df(json_filenames, df, source):\n\n    for file_name in json_filenames:\n\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n              \"abstract\": None, \"text_body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [data['abstract'][x]['text'] for x in range(len(data['abstract']) - 1)]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. For some reason I am getting an index error\n            # In one of the Json files, so rather than have it wrapped in a lovely list\n            # comprehension I've had to use a for loop like a neanderthal. \n            \n            # Needless to say this bug will be revisited and conquered. \n            \n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"BIORXIV\"\n            elif source == \"c\":\n                row['source'] = \"COMMON_USE_SUB\"\n            elif source == \"n\":\n                row['source'] = \"NON_COMMON_USE\"\n            elif source == \"p\":\n                row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df\n\ncorona_df = return_corona_df(json_filenames, corona_df, 'b')\ncorona_out = corona_df.to_csv('kaggle_covid-19_open_csv_format.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ncorona_df = pd.read_csv('kaggle_covid-19_open_csv_format.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"# clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstract = corona_df['abstract']\nabstract.fillna(\"\",inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=20000, stop_words='english')\nX = tfidf.fit_transform(abstract)\n\nclustered = KMeans(n_clusters=6, random_state=0).fit_predict(X)\n\n#with_clusters=pd.DataFrame(clusters)\ncorona_df['cluster_abstract']=clustered\n\ngrouped=corona_df.groupby('cluster_abstract')\nfor gp_name, gp in grouped:\n    display(gp)\n\ngrouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"# LDA, NMF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\nX_tfidf = tfidf.fit_transform(abstract)\ntfidf_feature_names = tfidf.get_feature_names()\n\nvectorizer = CountVectorizer(stop_words='english', max_features=1000)\nX_tf = vectorizer.fit_transform(abstract)\ntf_feature_names = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_topics = 15\n\n# Run NMF\nnmf = NMF(n_components=no_topics).fit(X_tfidf)\n\n# Run LDA\nlda = LatentDirichletAllocation(n_components=no_topics).fit(X_tf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract topics\ndef display_topics(model, feature_names, no_top_words):\n    topics=[]\n    for topic_idx, topic in enumerate(model.components_):\n        #rint (\"Topic %d:\" % (topic_idx))\n        topic_words=\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n        #rint(topic_words)\n        topics.append(topic_words)\n    return topics\n\nno_top_words = 5\n#rint(\"NMF: \")\ntopics_nmf=display_topics(nmf, tfidf_feature_names, no_top_words)\n#rint(\"\\nLDA: \")\ntopics_lda=display_topics(lda, tf_feature_names, no_top_words)\n\n#rint(topics_nmf)\n#rint(topics_lda)\n\npred_lda=lda.transform(X_tf)\npred_nmf=nmf.transform(X_tfidf)\n\nres_lda=[topics_lda[np.argmax(r)] for r in pred_lda]\nres_nmf=[topics_nmf[np.argmax(r)] for r in pred_nmf]\n\ncorona_df['topic_lda']=res_lda\ncorona_df['topic_nmf']=res_nmf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped=corona_df.groupby('topic_lda')\nfor gp_name, gp in grouped:\n    display(gp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_nmf=corona_df.groupby('topic_nmf')\nfor gp_name, gp in grouped_nmf:\n    display(gp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_nmf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"based on https://www.kaggle.com/isaacmg/scibert-embeddings\n\nTranslated to Tensorflow and optimized with batch processing"},{"metadata":{},"cell_type":"markdown","source":"# SciBERT embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorona_df.drop(corona_df.index[0], inplace=True)\ncorona_df.dropna(subset=['title'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import *\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_version = 'allenai/scibert_scivocab_uncased'\ndo_lower_case = True\nmodel = TFBertModel.from_pretrained(model_version, from_pt=True)\ntokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_titles = corona_df['title'].values\ntitles_encoded=[tokenizer.encode(l ,return_tensors='tf',max_length=200, pad_to_max_length=True) for l in list_titles]\n\nbatch_size=100\nbatched=[]\nfor i in range(int(len(titles_encoded)/batch_size)+1):\n  b = titles_encoded[i*batch_size:(i+1)*batch_size]\n  batched.append(tf.concat(b,axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nembed =[]\nfor i in tqdm(range(len(batched))):\n#for i in range(200):\n  #print(i)\n  e = model(batched[i])[0]\n  mean_e = tf.reduce_mean(e, axis=1)\n  embed.append(mean_e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nembed_titles = np.concatenate(embed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization of embeddings (https://www.kaggle.com/isaacmg/scibert-embeddings)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install umap-learn\nimport umap\nreducer = umap.UMAP()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\nfrom bokeh.palettes import Spectral10, Category20c\nfrom bokeh.palettes import magma\nimport pandas as pd\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red = reducer.fit_transform(embed_titles[:200])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_plot(red, title_list, number, color = True):\n    digits_df = pd.DataFrame(red, columns=('x', 'y'))\n    digits_df['digit'] = title_list\n    datasource = ColumnDataSource(digits_df)\n    plot_figure = figure(\n    title='UMAP projection of the article title embeddings',\n    plot_width=890,\n    plot_height=600,\n    tools=('pan, wheel_zoom, reset')\n    )\n\n    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n    <div>\n    <div>\n        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n    </div>\n    <div>\n        <span style='font-size: 10px; color: #224499'></span>\n        <span style='font-size: 10px'>@digit</span>\n    </div>\n    </div>\n    \"\"\"))\n    if color:\n        color_mapping = CategoricalColorMapper(factors=title_list, palette=magma(number))\n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit', transform=color_mapping),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\n    else:\n        \n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit'),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_plot(red, list_titles[:200], 200)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"TensorFlow-GPU","language":"python","name":"tf-gpu"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}