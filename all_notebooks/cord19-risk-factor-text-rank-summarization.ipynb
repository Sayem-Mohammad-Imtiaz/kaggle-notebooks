{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Created by a [TransUnion](www.transunion.com) data scientist that believes that information can be used to change our world for the better. #InformationForGood**\n\n### Our goal here is to summarize abstracts that are related to  covid19 risk factors using text rank. Joint work with [Karen](https://www.kaggle.com/kejinqian/find-answers-using-lda-and-skip-thoughts ).\n\n# COVID-19: extractive text summarization using text rank ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pipeline\n\n1. For each subtopic(smoking and pulmonary diseases, pregnancy and neonates, co-infections and commorbidities, etc)  generate a new dataframe for related abstracts using syntax. [These syntax or keyword were found using an LDA model output from karen's link here](https://www.kaggle.com/kejinqian/find-answers-using-lda-and-skip-thoughts )\n2. For each abstract   taken [from here](kkk.com), perform an extractive text summarization (text rank) using the top 3 sentences\n    1. Tokenize into sentences and cleaning,\n    2. Create sentence representations  using GLOVE word embeddings,\n    3. Construct similarity matrix using cosine similarity,\n    4. Build sentences network and apply page rank to find an importance score fro each sentence\n    5. Select top 3 sentences to build the new summary.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport nltk\nnltk.download('punkt')\nimport re\nimport networkx as nx\nfrom termcolor import colored \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words=stopwords.words('english')\nstop_words.extend(['abstract','background','summary','introduction'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On Smoking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke=pd.read_csv(\"../input/subtopic/smokpaper.csv\",sep=',')\ndf=smoke.loc[[0]]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## tokenize sentences\nfrom nltk.tokenize import sent_tokenize\nsentences = []\nfor s in df['abstract']:\n    sentences.append(sent_tokenize(s))\n\nsentences= [y for x in sentences for y in x] \nsentences[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## word embed from glove \nword_embedings = {}\nf= open('../input/glovewordembed/glove.6B.100d.txt',encoding='utf-8')\n\nfor line in f:\n    values=line.split()\n    word=values[0]\n    coefs=np.asarray(values[1:],dtype='float32')\n    word_embedings[word]= coefs\nf.close()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## remove special char\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_sentences[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## remove stop words\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_sentences[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##sentence to vectors using word embeddings\nsentence_vectors = []\nfor i in clean_sentences:\n  if len(i) != 0:\n    v = sum([word_embedings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n  else:\n    v = np.zeros((100,))\n  sentence_vectors.append(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_vectors[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])\nfrom sklearn.metrics.pairwise import cosine_similarity\nfor i in range(len(sentences)):\n  for j in range(len(sentences)):\n    if i != j:\n      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)\n## each node represents a sentence\nnx.draw(nx_graph,pos=nx.spring_layout(nx_graph),with_labels = True)\nnx_graph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Score and sentence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\nsummary=pd.DataFrame(ranked_sentences).drop_duplicates(subset=1,keep='first')\nsummary=summary.rename(columns={0:'score',1:\"sentence\"})\nsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 3 sentences to create new summary\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked= summary['sentence'].values.tolist()\n#top 20\n#for i in range(len(ranked)):\n  #print(i+1,\")\", ranked[i], \"\\n\")\n \n#print(colored(list(smoke['abstract']) ,'green'))\n#first=sentences[0]\n#aa=ranked[:3]\n#aa.append(first)\n#'.'.join(aa)   ## the first sentence  may be usefull\n\nprint(colored(list(df['abstract']) ,'green'))\n'.'.join(ranked[:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On pregancy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preg= pd.read_csv(\"../input/subtopic/pregnantpaper.csv\",sep=',')\ndf2=preg.loc[[10]]\ndf2\nfrom nltk.tokenize import sent_tokenize\nsentences = []\nfor s in df2['abstract']:\n    sentences.append(sent_tokenize(s))\n\nsentences= [y for x in sentences for y in x]  \n\n## remove special char\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]\n\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new\n\n# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n\n\nsentence_vectors = []\nfor i in clean_sentences:\n  if len(i) != 0:\n    v = sum([word_embedings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n  else:\n    v = np.zeros((100,))\n  sentence_vectors.append(v)\n\nsim_mat = np.zeros([len(sentences), len(sentences)])\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfor i in range(len(sentences)):\n  for j in range(len(sentences)):\n    if i != j:\n      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)    \n\n## ranking \nranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\nsummary=pd.DataFrame(ranked_sentences).drop_duplicates(subset=1,keep='first')\nsummary=summary.rename(columns={0:'score',1:\"text\"})\n\n## output\nsummary=pd.DataFrame(ranked_sentences).drop_duplicates(subset=1,keep='first')\nsummary=summary.rename(columns={0:'score',1:\"text\"})\n\nranked= summary['text'].values.tolist()\n#top 20\n#for i in range(len(ranked)):\n  #print(i+1,\")\", ranked[i], \"\\n\")\n#for i in range(4):    \n    #print( ranked[i])\nprint(colored(list(df2['abstract']) ,'green'))\n'.'.join(ranked[:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### On cardiovascular and cerebrovascular","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"card= pd.read_csv(\"../input/subtopic/respiratory_cardio_paper.csv\",sep=',')\ndf3=card.loc[[2]]\nfrom nltk.tokenize import sent_tokenize\nsentences = []\nfor s in df3['abstract']:\n    sentences.append(sent_tokenize(s))\n\nsentences= [y for x in sentences for y in x]  \n\n## remove special char\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]\n\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new\n\n# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n\n\nsentence_vectors = []\nfor i in clean_sentences:\n  if len(i) != 0:\n    v = sum([word_embedings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n  else:\n    v = np.zeros((100,))\n  sentence_vectors.append(v)\n\nsim_mat = np.zeros([len(sentences), len(sentences)])\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfor i in range(len(sentences)):\n  for j in range(len(sentences)):\n    if i != j:\n      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)    \n\n## ranking \nranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\nsummary=pd.DataFrame(ranked_sentences).drop_duplicates(subset=1,keep='first')\nsummary=summary.rename(columns={0:'score',1:\"text\"})\n\n## output\nsummary=pd.DataFrame(ranked_sentences).drop_duplicates(subset=1,keep='first')\nsummary=summary.rename(columns={0:'score',1:\"text\"})\n\nranked= summary['text'].values.tolist()\n#top 20\n#for i in range(len(ranked)):\n  #print(i+1,\")\", ranked[i], \"\\n\")\n#for i in range(4):    \n    #print( ranked[i])\nprint(colored(list(df3['abstract']) ,'green'))\n'.'.join(ranked[:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finally, we could just encapsulate all this into a function that receives as inputs an abstract and outputs a summary**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}