{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sign language recognition using CNN","metadata":{}},{"cell_type":"markdown","source":"* This Notebook inspirde by the course Scientific computation in Python from IDC and Dr. Yoav Ram","metadata":{}},{"cell_type":"markdown","source":"In this Exercise we would train a CNN to clasify images of sign languge.\nThis Notebook has 4 main parts:\n\n    1. Loading the Data.\n    2. Prepering the data for the model(images and labels).\n    3. Creating and training the model.\n    4. Presinting the results.","metadata":{}},{"cell_type":"markdown","source":"First lets do some imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pandas as pd\nimport keras\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers.normalization import BatchNormalization","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-24T15:56:46.993743Z","iopub.execute_input":"2021-05-24T15:56:46.99443Z","iopub.status.idle":"2021-05-24T15:56:49.290693Z","shell.execute_reply.started":"2021-05-24T15:56:46.994324Z","shell.execute_reply":"2021-05-24T15:56:49.28975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The First step would be loading the data from the Dataset folder, we load all the images. \nWe also load the data from the csv files which would help us to crop the images.","metadata":{}},{"cell_type":"code","source":"def getFileName(path):\n    return (path.split(\"../input/sign-language-images/Dataset/\")[1])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:49.292261Z","iopub.execute_input":"2021-05-24T15:56:49.292725Z","iopub.status.idle":"2021-05-24T15:56:49.296499Z","shell.execute_reply.started":"2021-05-24T15:56:49.292676Z","shell.execute_reply":"2021-05-24T15:56:49.295666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = {}\nmetaData = []\nfolders = next(os.walk('../input/sign-language-images/Dataset'))[1]\nfor folder in folders:\n    csvFile = pd.read_csv(\"../input/sign-language-images/Dataset/%s/%s_loc.csv\" %(folder,folder))\n    metaData.append(np.array(csvFile))\n    for img in glob.glob(\"../input/sign-language-images/Dataset/%s/*.jpg\" %folder):\n        n= mpimg.imread(img)\n        picName = getFileName(img)\n        images.update({picName : n})","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:49.297747Z","iopub.execute_input":"2021-05-24T15:56:49.298157Z","iopub.status.idle":"2021-05-24T15:56:52.962408Z","shell.execute_reply.started":"2021-05-24T15:56:49.298111Z","shell.execute_reply":"2021-05-24T15:56:52.961578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After loading the data we crop the images so we would have only the \"interesting\" part of the images, and resize all the images to the same size(keras requirment).","metadata":{}},{"cell_type":"code","source":"def preprocess(image,box):\n    return cv2.resize(image[box[0]:box[1],box[2]:box[3]], (90,90))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:52.963838Z","iopub.execute_input":"2021-05-24T15:56:52.964287Z","iopub.status.idle":"2021-05-24T15:56:52.969612Z","shell.execute_reply.started":"2021-05-24T15:56:52.964242Z","shell.execute_reply":"2021-05-24T15:56:52.968498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagesAfterCrop = {}\nfor userImages in metaData:\n    for img in userImages:\n        imagesAfterCrop.update({img[0] : preprocess(images[img[0]],[img[2], img[4], img[1], img[3]])})","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:52.973163Z","iopub.execute_input":"2021-05-24T15:56:52.973567Z","iopub.status.idle":"2021-05-24T15:56:53.082006Z","shell.execute_reply.started":"2021-05-24T15:56:52.973526Z","shell.execute_reply":"2021-05-24T15:56:53.080844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=imagesAfterCrop[\"user_10/A1.jpg\"]\nimgplot = plt.imshow(img)\nplt.show()\nimagesAfterCrop[\"user_10/A1.jpg\"].shape","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:53.084222Z","iopub.execute_input":"2021-05-24T15:56:53.084655Z","iopub.status.idle":"2021-05-24T15:56:53.2826Z","shell.execute_reply.started":"2021-05-24T15:56:53.08461Z","shell.execute_reply":"2021-05-24T15:56:53.281341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we create the labels array, using the pictures names.\nThe labels are one hot encoding, we would make sure everything fine by printing the shape of the labels array.","metadata":{}},{"cell_type":"code","source":"def createLabelsArray(images):\n    counter = 0;\n    labels = np.zeros((len(images),26))\n    for img in images:\n        labels[counter][(ord(img.split('/')[1][0]) - ord('A'))] = 1 \n        counter = counter + 1\n    return labels\n    \n        \ny = createLabelsArray(imagesAfterCrop)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:53.284506Z","iopub.execute_input":"2021-05-24T15:56:53.284942Z","iopub.status.idle":"2021-05-24T15:56:53.296264Z","shell.execute_reply.started":"2021-05-24T15:56:53.284898Z","shell.execute_reply":"2021-05-24T15:56:53.295099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We almost there!\nNow we split the data to train and test, and rescale all the x(pixels) values to be between 0 to 1(keras requirment)","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(list(imagesAfterCrop.values()), y, test_size=0.2, random_state = 0)\nx_train = np.array(x_train)\nx_test = np.array(x_test)\nx_train = x_train / 255\nx_test = x_test / 255","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:53.297713Z","iopub.execute_input":"2021-05-24T15:56:53.298119Z","iopub.status.idle":"2021-05-24T15:56:53.440959Z","shell.execute_reply.started":"2021-05-24T15:56:53.298083Z","shell.execute_reply":"2021-05-24T15:56:53.439859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finaly we can create and train our model!\nfor better results we use BatchNormalization and the Adam optimizer.\nWe use the softmax function for the classification.","metadata":{}},{"cell_type":"code","source":"num_classes = 26\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(32, (3, 3), input_shape=(90, 90, 3)))\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(keras.layers.Conv2D(32, (3, 3)))\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(keras.layers.Conv2D(32, (3, 3)))\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(1024))\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(keras.layers.Dense(num_classes))\nmodel.add(keras.layers.Activation('softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:53.442254Z","iopub.execute_input":"2021-05-24T15:56:53.44253Z","iopub.status.idle":"2021-05-24T15:56:53.601938Z","shell.execute_reply.started":"2021-05-24T15:56:53.442502Z","shell.execute_reply":"2021-05-24T15:56:53.600881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=keras.optimizers.RMSprop(lr=0.0001, decay=1e-6),\n              metrics=['accuracy'])\n\nhistory = model.fit(\n    x_train, y_train,\n    batch_size=16,\n    epochs=10,\n    validation_data=(x_test, y_test)\n).history","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:56:53.603286Z","iopub.execute_input":"2021-05-24T15:56:53.603597Z","iopub.status.idle":"2021-05-24T15:58:47.423242Z","shell.execute_reply.started":"2021-05-24T15:56:53.603567Z","shell.execute_reply":"2021-05-24T15:58:47.422225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Score trained model.\nloss, acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', loss)\nprint('Test accuracy:', acc)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:58:47.424552Z","iopub.execute_input":"2021-05-24T15:58:47.42483Z","iopub.status.idle":"2021-05-24T15:58:48.178888Z","shell.execute_reply.started":"2021-05-24T15:58:47.424804Z","shell.execute_reply":"2021-05-24T15:58:48.177235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great!\nwe get ~90% accuracy on the test set.\nhere are some examples of our predictions.","metadata":{}},{"cell_type":"code","source":"def decodeReal(yHat):\n    return chr(np.argmax(yHat) + 97).upper()\n\ndef decodePrediction(number):\n    return chr(number + 97).upper()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:58:48.180291Z","iopub.execute_input":"2021-05-24T15:58:48.180644Z","iopub.status.idle":"2021-05-24T15:58:48.186402Z","shell.execute_reply.started":"2021-05-24T15:58:48.180609Z","shell.execute_reply":"2021-05-24T15:58:48.184801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(15, 15))\ncolumns = 3\nrows = 3\nfor i in range(1, columns*rows +1):\n    img = x_test[0+ i]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.axis('off')\n    print(\"The predicted Letter for Example#\"+str(i)+\" is \"+decodePrediction(model.predict_classes(np.array([x_test[i],]))))\n    print(\"The real Letter for Example#\"+str(i)+ \" is \" +decodeReal(y_test[i]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:58:48.188177Z","iopub.execute_input":"2021-05-24T15:58:48.188676Z","iopub.status.idle":"2021-05-24T15:58:49.613239Z","shell.execute_reply.started":"2021-05-24T15:58:48.188567Z","shell.execute_reply":"2021-05-24T15:58:49.612163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allLettersTestArray = np.zeros(26)\nallLettersTestMistakesArray = np.zeros(26)\ncounter = 0\nfor example in x_test:\n    yhat = np.argmax(y_test[counter])\n    allLettersTestArray[yhat] += 1\n    if(model.predict_classes(np.array([x_test[counter],])) != yhat):\n        allLettersTestMistakesArray[yhat] += 1\n    counter += 1\nallLettersTestArray[allLettersTestArray == 0] = -1    \nmistakesPerLetter = allLettersTestMistakesArray / allLettersTestArray\nlettersArray = np.array(list(map(chr, range(97, 123))))\ndf = pd.DataFrame({'leters' : lettersArray, 'mistakes' : mistakesPerLetter})\ndf['colors'] = 'b'\ndf.loc[df.mistakes > 0.15, 'colors'] ='r'","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:58:49.615118Z","iopub.execute_input":"2021-05-24T15:58:49.615507Z","iopub.status.idle":"2021-05-24T15:59:03.618376Z","shell.execute_reply.started":"2021-05-24T15:58:49.615467Z","shell.execute_reply":"2021-05-24T15:59:03.617391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(18,5))\nplt.bar(df.leters, df.mistakes, color=df.colors)\nax.set_title('Mistake Frequency by letter', fontsize=18)\nax.set_ylabel('Mistake Frequency', fontsize=18)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T15:59:03.619846Z","iopub.execute_input":"2021-05-24T15:59:03.620333Z","iopub.status.idle":"2021-05-24T15:59:03.912019Z","shell.execute_reply.started":"2021-05-24T15:59:03.62029Z","shell.execute_reply":"2021-05-24T15:59:03.910497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see the Mistakes frequncy per letter,\nthe red ones are the letters which had the most mistakes.\nThere could be many reasons why we had more mistakes on those letters(maybe similarity to other letters), but thats out of our scope :)","metadata":{}},{"cell_type":"markdown","source":"# Thank you for reading","metadata":{}}]}