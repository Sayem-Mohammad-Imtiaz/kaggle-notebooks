{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py","version":"3.6.1","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"cd5a2b7513778f528d5f27bb782ec6ead5ad23cd","collapsed":true,"_cell_guid":"7630b3cd-b3ec-4992-ac6c-52e092203cbb"},"outputs":[],"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sympy import *\nimport matplotlib.pyplot as plt\nimport operator\n\nfrom IPython.core.display import display\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.utils.data as data_utils\nimport torch.nn.init as init\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ninit_printing(use_unicode=True)","execution_count":1},{"metadata":{"_uuid":"37462a65f8e510f98117b2668ebec73fda768128","_cell_guid":"4798b3c4-8914-4c00-ac08-2049794fbcfa"},"cell_type":"markdown","source":"First up, lets's read in the data and see what's in it."},{"metadata":{"_uuid":"9a97b8b2426b66749960b07c41a3602ebaf5fc26","collapsed":true,"_cell_guid":"fcde61c7-00f7-42c5-a839-9873537ea472"},"outputs":[],"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\")","execution_count":2},{"metadata":{"_uuid":"92ee2344e3b2773a4ea539bd27c57f819177557d","_cell_guid":"690584e4-d6ad-4321-b2a5-bff89bc4fd80"},"outputs":[],"cell_type":"code","source":"data.head()","execution_count":3},{"metadata":{"_uuid":"b1d152e44e1532d7457baed864bdc25874e51551","_cell_guid":"1494b763-f661-4787-9e95-873dc5396c84"},"cell_type":"markdown","source":"Let's prepare out training and test sets.\n\nThe diagnosis column contains the train labels, so let's extract that and turn the strings into 1 and 0.\n\nThe id and \"Unamed: 32\" columns won't help us in the prediction so lets drop them."},{"metadata":{"_uuid":"5632bb0c21081ded1832d98d7367061d03ecb511","collapsed":true,"_cell_guid":"e84bc4f5-110f-4868-957d-f438834f20a9"},"outputs":[],"cell_type":"code","source":"x = data.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)\ndiag = { \"M\": 1, \"B\": 0}\ny = data[\"diagnosis\"].replace(diag)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=85)","execution_count":4},{"metadata":{"_uuid":"f83fa497e38d80e158eac22baf1af5e505a853fc","_cell_guid":"60494dfb-3a2d-467d-b329-885a5ea6f7c1"},"cell_type":"markdown","source":"Now lets scale the data ready for entry into the network and load it in the format expected by pytorch."},{"metadata":{"_uuid":"cccbf2dc492486c6696b5b347a3af1ee20948b50","collapsed":true,"_cell_guid":"02e37990-2412-4921-881d-75391bfa7527"},"outputs":[],"cell_type":"code","source":"scaler = StandardScaler()\ntransformed = scaler.fit_transform(x_train)\n\ntrain = data_utils.TensorDataset(torch.from_numpy(transformed).float(),\n                                 torch.from_numpy(y_train.as_matrix()).float())\ndataloader = data_utils.DataLoader(train, batch_size=128, shuffle=False)","execution_count":5},{"metadata":{"_uuid":"c161bc9252fa41860b68d65557b553426a06ab6e","_cell_guid":"bb62f9ba-19c6-4985-94db-93d85d2c03f0"},"cell_type":"markdown","source":"To create the pytorch model, we'll first define a function that can create a sequential network of any size. This might come in useful later if we want to search for the optimal hyper parameters."},{"metadata":{"_uuid":"2c6f37027e0547b66658bcd9b77fec3103057288","collapsed":true,"_cell_guid":"a923f8cc-4ed7-4095-b34d-c366f5910d18"},"outputs":[],"cell_type":"code","source":"def create_model(layer_dims):\n    model = torch.nn.Sequential()\n    for idx, dim in enumerate(layer_dims):\n        if (idx < len(layer_dims) - 1):\n            module = torch.nn.Linear(dim, layer_dims[idx + 1])\n            init.xavier_normal(module.weight)\n            model.add_module(\"linear\" + str(idx), module)\n        else:\n            model.add_module(\"sig\" + str(idx), torch.nn.Sigmoid())\n        if (idx < len(layer_dims) - 2):\n            model.add_module(\"relu\" + str(idx), torch.nn.ReLU())\n\n    return model","execution_count":6},{"metadata":{"_uuid":"12725d4a292d6676ac59e0be6117b757aa5eb2e8","_cell_guid":"d19563f8-802f-4de1-81d9-6f66e4744900"},"cell_type":"markdown","source":"In a similar manor to the train set, let's now scale and prepare a test set to let us know how our predictions are going."},{"metadata":{"_uuid":"de166a325c4c0c5e366398226a891be4307e0942","collapsed":true,"_cell_guid":"07353879-1675-4932-a9b2-03e17d7543af"},"outputs":[],"cell_type":"code","source":"scaler = StandardScaler()\ntransformed = scaler.fit_transform(x_test)\n\ntest_set = torch.from_numpy(transformed).float()\ntest_valid = torch.from_numpy(y_test.as_matrix()).float()","execution_count":7},{"metadata":{"_uuid":"82bc18e91cc671a628927039dd7fade484139b53","_cell_guid":"90ebd855-aea1-48c8-bb22-10897c10e681"},"cell_type":"markdown","source":"Finally, we can now specify the hyperparameters and iterate through our train data to train the model.\n\nFor now we'll use a fairly arbitrary network with 2 hidden layers of 20 and 10 nodes and a learning rate of 0.0007. For optimization, it uses Adam."},{"metadata":{"_uuid":"a441ca7a77f056bbe682540cf995059f60fd01d1","_cell_guid":"db99f018-4ddc-43ec-8884-e697cdd75dd8"},"outputs":[],"cell_type":"code","source":"## Create model and hyper parameters\ndim_in = x_train.shape[1]\ndim_out = 1\n\nlayer_dims = [dim_in, 20, 10, dim_out]\n\nmodel = create_model(layer_dims)\n\nloss_fn = torch.nn.MSELoss(size_average=False)\nlearning_rate = 0.0007\nn_epochs = 300\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n## Now run model\nhistory = { \"loss\": [], \"accuracy\": [], \"loss_val\": [], \"accuracy_val\": [] }\nfor epoch in range(n_epochs):\n    loss = None\n\n    for idx, (minibatch, target) in enumerate(dataloader):\n        y_pred = model(Variable(minibatch))\n\n        loss = loss_fn(y_pred, Variable(target.float()))\n        prediction = [1 if x > 0.5 else 0 for x in y_pred.data.numpy()]\n        correct = (prediction == target.numpy()).sum()\n        \n        # This can be uncommented for a per mini batch feedback\n        #history[\"loss\"].append(loss.data[0])\n        #history[\"accuracy\"].append(100 * correct / len(prediction))\n        \n        y_val_pred = model(Variable(test_set))\n        loss_val = loss_fn(y_val_pred, Variable(test_valid.float()))\n        prediction_val = [1 if x > 0.5 else 0 for x in y_val_pred.data.numpy()]\n        correct_val = (prediction_val == test_valid.numpy()).sum()\n        \n        # This can be uncommented for a per mini batch feedback\n        #history[\"loss_val\"].append(loss_val.data[0])\n        #history[\"accuracy_val\"].append(100 * correct_val / len(prediction_val))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    history[\"loss\"].append(loss.data[0])\n    history[\"accuracy\"].append(100 * correct / len(prediction))\n    history[\"loss_val\"].append(loss_val.data[0])\n    history[\"accuracy_val\"].append(100 * correct_val / len(prediction_val))\n        \n    print(\"Loss, accuracy, val loss, val acc at epoch\", epoch + 1,history[\"loss\"][-1], \n          history[\"accuracy\"][-1], history[\"loss_val\"][-1], history[\"accuracy_val\"][-1] )\n\n\nindex, value = max(enumerate(history[\"accuracy_val\"]), key=operator.itemgetter(1))\n\nprint(\"Best accuracy was {} at iteration {}\".format(value, index))","execution_count":8},{"metadata":{"_uuid":"48c370e5ebddff2b65e368e04ad0af168101b2a2","_cell_guid":"30fa805b-7572-455d-9387-e757f00369a0"},"cell_type":"markdown","source":"Not bad at all! Around %98 accuracy with no optimization at this point.\n\nLet's plot the loss and accuracy to see if everything looks good."},{"metadata":{"_uuid":"f59edaf0011781acd73317bbda2c2ea223aadb9c","_cell_guid":"ac8600be-3329-4996-a4a7-17c60ae297be"},"outputs":[],"cell_type":"code","source":"plt.plot(history['accuracy'])\nplt.plot(history['accuracy_val'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":9},{"metadata":{"_uuid":"71f5044ddaef085b61be3e106d268bfcc3539622","collapsed":true,"_cell_guid":"baa4231a-9c00-4eb5-b55c-3911cb3b9bd6"},"outputs":[],"cell_type":"code","source":"plt.plot(history['loss'])\nplt.plot(history['loss_val'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null},{"metadata":{"_uuid":"c8d0c960e22a4c1d2c6a51be0b6608ba22d3b43d","_cell_guid":"3712db24-abb3-44b6-85a3-f10865b02224"},"cell_type":"markdown","source":"Obviously a few things to work on but not bad for a first attempt."},{"metadata":{"_uuid":"63c7325066487152fdde7d2c533c32b4fb830cfe","collapsed":true,"_cell_guid":"fb7a8e82-4864-47a4-a871-f2e8b45b93bf"},"outputs":[],"cell_type":"code","source":"","execution_count":null}],"nbformat":4}