{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Context\n+ According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n### Attribute Information\n+ id: unique identifier\n+ gender: \"Male\", \"Female\" or \"Other\"\n+ age: age of the patient\n+ hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n+ heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n+ ever_married: \"No\" or \"Yes\"\n+ work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n+ Residence_type: \"Rural\" or \"Urban\"\n+ avg_glucose_level: average glucose level in blood\n+ bmi: body mass index\n+ smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n+ stroke: 1 if the patient had a stroke or 0 if not\n\n+ Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install lux-api --quiet\n!pip install lux-widget --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing some basic libraries for exploring and visualizing the dataset.\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport lux\nimport luxwidget\n\n# Importing library for balancing the data.\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\n# Importing libraries for splitting and scaling the data.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing machine learning libraries.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import classification_report\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Colleecting the data.\n\ncollected_dataset = '/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the data.\n\ndf = pd.read_csv(collected_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis using pandas_profiling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Pandas Profiling fearure we can do all kind EDA and basic visualization also.\n\npp.ProfileReport(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treating missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(value = df['bmi'].median(), inplace = True)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns = 'id', inplace = True)\nd = df[df['gender'] == 'Other'].index\ndf.drop(d, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing using Lux"},{"metadata":{"trusted":true},"cell_type":"code","source":"!jupyter nbextension install --py luxwidget --quiet\n!jupyter nbextension enable --py luxwidget","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ Lux is a latest visualization library in python. With the help lux we can make an automatic visualization and it is an attractive and informative visualization library. For using lux we don't need to import matplotlib.\n\n+ Here, we can see there are three tabs which are Correlation, Distribution and Occurrence. Correlation is showing correlation between each numerical features. Distribution is showing distribution of each numerical features. Occurrence is showing count of all integer data type fearures."},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ Here, I have performed on some features and we can see two tabs which are Enhance, Filter. Enhance is showing based on a specific feature find all other features. Filter is filtering on a specific feature with help all categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.intent = ['gender']\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.intent = ['hypertension']\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_intent(['ever_married'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_intent(['smoking_status'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_intent(['stroke'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_intent(['heart_disease'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_intent(['age'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_intent(['avg_glucose_level'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_intent(['bmi'])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seaborn Distplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(2,2, figsize=(15 ,10))  \nsns.distplot(df['age'], ax = ax[0,0], color = 'b') \nsns.distplot(df['avg_glucose_level'], ax = ax[0,1], color = 'b')\nsns.distplot(df['bmi'], ax = ax[1,0], color = 'b')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ With the help of dist plot we can get a rough idea on distribution of each numerical features. A Distplot or distribution plot, depicts the variation in the data distribution. Seaborn Distplot represents the overall distribution of continuous data variables. The Seaborn module along with the Matplotlib module is used to depict the distplot with different variations in it. Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. This article deals with the distribution plots in seaborn which is used for examining univariate and bivariate distributions."},{"metadata":{},"cell_type":"markdown","source":"### Seaborn Violinplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(2,2, figsize=(15,10))  # 'ax' has references to all the four axes\nsns.violinplot(x = 'stroke', y = 'age', hue = 'gender', palette = 'rocket', data = df, ax = ax[0,0]) \nsns.violinplot(x = 'stroke', y = 'avg_glucose_level', hue = 'gender', palette = 'rocket',data = df, ax = ax[0,1])\nsns.violinplot(x = 'stroke', y = 'bmi', hue = 'gender', palette = 'rocket', data = df, ax = ax[1,0]) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ A violin plot plays a similar activity that is pursued through whisker or box plot do. As it shows several quantitative data across one or more categorical variables. It can be an effective and attractive way to show multiple data at several units. A “wide-form” Data Frame helps to maintain each numeric column which can be plotted on the graph. It is possible to use NumPy or Python objects, but pandas objects are preferable because the associated names will be used to annotate the axes."},{"metadata":{},"cell_type":"markdown","source":"### Seaborn Pairplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue = 'stroke', palette = 'icefire');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ To plot multiple pairwise bivariate distributions in a dataset, you can use the pairplot() function. This shows the relationship for (n, 2) combination of variable in a DataFrame as a matrix of plots and the diagonal plots are the univariate plots. Data Visualization is the presentation of data in pictorial format. It is extremely important for Data Analysis, primarily because of the fantastic ecosystem of data-centric Python packages. And it helps to understand the data, however, complex it is, the significance of data by summarizing and presenting a huge amount of data in a simple and easy-to-understand format and helps communicate information clearly and effectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize = (20, 15))\nsns.heatmap(df[top_corr_features].corr(), annot = True, cmap = \"Blues\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ Here, I performed the correlation analysis for all the numerical features including qualitative and quantitative features. From the above correlation plot, I found there were several features have strong positive correlation relationships. The strongest positive correlation relationship is between the \"age\" and \"bmi\", which means that a higher if age increase then bmi will also be increased. Apart from that there are some other features which are highly correlated with \"age\" which are \"stroke\", \"heart_disease\", \"hypertension\", \"avg_glucose_level\"."},{"metadata":{},"cell_type":"markdown","source":"### Seaborn Boxplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(2,2, figsize=(15,10))  # 'ax' has references to all the four axes\nsns.boxplot(x = df['stroke'], y = df['age'], hue = df['gender'], palette = 'rocket', ax = ax[0,0]) \nsns.boxplot(x = df['stroke'], y = df['avg_glucose_level'], hue = df['gender'], palette = 'rocket', ax = ax[0,1])\nsns.boxplot(x = df['stroke'], y = df['bmi'], hue = df['gender'], palette = 'rocket', ax = ax[1,0]) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ From box plot we can understand that if age is more than 60 then chance of stroke is higher than below age of 60 and it is applicable for male and female and If avg_glucose_level is high then chance of stroke is also high for male and female but male have slightly higher than female."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I applied different types of encoding.\n\ndef encoding(data, feature):\n    \n    data = data.copy()\n    \n    # One-Hot Encoding\n    dummies = pd.get_dummies(data[feature], prefix = feature)\n    data = pd.concat([data, dummies], axis = 1)\n    data = data.drop(feature, axis = 1)\n    \n    #Binary Encoding\n    data['gender'] = data['gender'].replace({'Female' : 0, 'Male' : 1})\n    data['ever_married'] = data['ever_married'].replace({'No' : 0, 'Yes' : 1})\n    data['Residence_type'] = data['Residence_type'].replace({'Rural' : 0, 'Urban' : 1})\n    \n    #Lebel Encoding\n    lebels = {'smokes' : 1, 'formerly smoked' : 2, 'never smoked' : 3, 'Unknown' : 4}\n    data['smoking_status_lebels'] = data['smoking_status'].map(lebels)\n    data = data.drop(columns = 'smoking_status', axis = 1)\n    \n    return data\n\ncolumns = ['work_type']\ndf1 = encoding(df, columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df1.drop('stroke', axis = 1)\ny = df1['stroke']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ My target feature is imbalanced that's why I made it balanced using \"Random Over Sampler\". If I don't make it balanced data then my model could give biased output. We can see also before and after applting it how the dataset is changed in shape. "},{"metadata":{"trusted":true},"cell_type":"code","source":"os = RandomOverSampler(sampling_strategy = 1)\nx_ros, y_ros = os.fit_resample(x, y)\nprint(x_ros.shape, y_ros.shape)\nprint('Original dataset shape {}'.format(Counter(y)))\nprint('Resampled dataset shape {}'.format(Counter(y_ros)))\nx_ros = pd.DataFrame(x_ros)\ny_ros = pd.DataFrame(y_ros)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I splitted the dataset into 75% of training and 25% of testing. I applied \"Satandard Scaler\" for scalling the data.\n\ndef feature_scalling(data):\n    #Split data into x and y\n    x = x_ros\n    y = y_ros\n    \n    #Applying Train-Test-Split\n    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.75, shuffle = True, random_state = 1)\n    \n    #Appling Standard Scalling\n    scaler = StandardScaler()\n    scaler.fit(x_train)\n    x_train = pd.DataFrame(scaler.fit_transform(x_train), index = x_train.index, columns = x_train.columns)\n    x_test = pd.DataFrame(scaler.fit_transform(x_test), index = x_test.index, columns = x_test.columns)\n    \n    return x_train, x_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = feature_scalling(df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I applied \"Logistic Regression\" and found \"Confusion Matrix, Accuracy Score, Classification Report\".\n\nmodel_lgr = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, Y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(Y_test, lr_predict)\nlr_acc_score = accuracy_score(Y_test, lr_predict)\n\ndf1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index = [\"Not Stroke\", \"Stroke\"], data = lr_conf_matrix )\nf,ax = plt.subplots(figsize = (6, 3))\nsns.heatmap(df1, annot = True, cmap = \"viridis\", fmt = '.0f', linewidths = 5, ax = ax, cbar = False)\nplt.xlabel(\"Predicted Label\")\nplt.xticks(size = 12)\nplt.yticks(size = 12, rotation = 0)\nplt.ylabel(\"Actual Label\")\nplt.title(\"Confusion Matrix\", size = 12)\nplt.show()\n\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(Y_test, lr_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Findout the minimum Kvalue at which maximum accuracy.\n\nfor K in range(1, 10):\n    K_value = K + 1\n    neighbor = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm = 'auto')\n    neighbor.fit(X_train, Y_train) \n    y_pred = neighbor.predict(X_test)\n    print( \"Accuracy is \", accuracy_score(Y_test, y_pred)*100,\"% for K-Value:\",K_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ Here, I am getting highest accuracy with value of k = 2. That's why I will consider k = 2 for next tasks in K -NeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I applied \"K-Neighbors Classifier\" and found \"Confusion Matrix, Accuracy Score, Classification Report\".\n\nmodel_knn = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(X_train, Y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(Y_test, knn_predicted)\nknn_acc_score = accuracy_score(Y_test, knn_predicted)\n\ndf1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index = [\"Not Stroke\", \"Stroke\"], data = knn_conf_matrix )\nf,ax = plt.subplots(figsize = (6, 3))\nsns.heatmap(df1, annot = True, cmap = \"magma\", fmt= '.0f', linewidths = 5, ax = ax, cbar = False)\nplt.xlabel(\"Predicted Label\")\nplt.xticks(size = 12)\nplt.yticks(size = 12, rotation = 0)\nplt.ylabel(\"Actual Label\")\nplt.title(\"Confusion Matrix\", size = 12)\nplt.show()\n\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(Y_test, knn_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I applied \"Random Forest Classifier\" and found \"Confusion Matrix, Accuracy Score, classification_report\".\n\nmodel_rfc = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators = 10, random_state = 101, max_depth = 5)\nrf.fit(X_train, Y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(Y_test, rf_predicted)\nrf_acc_score = accuracy_score(Y_test, rf_predicted)\n\ndf1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index = [\"Not Stroke\", \"Stroke\"], data = rf_conf_matrix )\nf,ax = plt.subplots(figsize = (6, 3))\nsns.heatmap(df1, annot = True, cmap = \"rocket_r\", fmt = '.0f', linewidths = 5, ax = ax, cbar = False)\nplt.xlabel(\"Predicted Label\")\nplt.xticks(size = 12)\nplt.yticks(size = 12, rotation = 0)\nplt.ylabel(\"Actual Label\")\nplt.title(\"Confusion Matrix\", size = 12)\nplt.show()\n\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(Y_test, rf_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I applied \"Decision Tree Classifier\" and found \"Confusion Matrix, Accuracy Score, classification_report\".\n\nmodel_dtc = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy', random_state=0, max_depth = 5)\ndt.fit(X_train, Y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(Y_test, dt_predicted)\ndt_acc_score = accuracy_score(Y_test, dt_predicted)\n\ndf1 = pd.DataFrame(columns = [\"Not Stroke\", \"Stroke\"], index= [\"Not Stroke\", \"Stroke\"], data= dt_conf_matrix )\nf,ax = plt.subplots(figsize = (6, 3))\nsns.heatmap(df1, annot = True, cmap = \"flare\", fmt = '.0f', linewidths = 5, ax = ax, cbar = False)\nplt.xlabel(\"Predicted Label\")\nplt.xticks(size = 12)\nplt.yticks(size = 12, rotation = 0)\nplt.ylabel(\"Actual Label\")\nplt.title(\"Confusion Matrix\", size = 12)\nplt.show()\n\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(Y_test,dt_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I have created a barplot on accuracy score for each models.\n\nmodel_ev = {'Model' : ['Logistic Regression', 'K-NeighborsClassifier', 'Random Forest Classfier', 'DecisionTreeClassifier'], \n            'Accuracy' : [lr_acc_score*100, knn_acc_score*100, rf_acc_score*100, dt_acc_score*100]}\ncolors = ['gold', 'silver', 'blue', 'cyan']\nplt.figure(figsize = (12, 6))\nplt.title(\"Barplot Represent Accuracy of different models\")\nplt.xlabel(\"Accuracy %\")\nplt.xticks(rotation = 90)\nplt.ylabel(\"Algorithms\")\nplt.bar(model_ev['Model'], model_ev['Accuracy'], color = colors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### This target feature is imbalanced. I did not know about it after applying machine learning models. I saw every models are showing only 'True Negative' and 'False Negative' values in classification report and all models are showing more than 90%  of accuracy. Then I understood my feature is not balanced. After that for making it balanced I used 'Random Over Sampler' method for 'over sampling' the data and I applied it because I didn't want to loose any data that's why I didn't use 'under sampling'. After applying this again I fit the data into models. Here, I used four models for which one of them is giving higher  accuracy and f1-score.\n#####  My first reason to choose \"K-Neighbors Classifier\" is \"K-Neighbors Classifier\" is simple and easy to implement. There’s no need to tune several parameters, or make additional assumptions and it is versatile. It can be used for classification, regression.  My second reason for choosing it is that here, I saw \"K-Neighbors Classifier\" is giving more than 97% of accuracy and f1-score with the k value of 2. Here, \"Random Forest Classfier\" is giving more than 80% and others are giving lees than 80%. That's why I have chosen \"K-Neighbors Classifier\" for predicting on stroke."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}