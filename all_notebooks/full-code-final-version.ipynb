{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Split imbalanced dataset into train and test sets with stratification: #Only used in imbalanced classification probmlems\n# This will make sure that in train and test there will be about 8% defaulters. # Remaining structure of original data.\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# Making sure all numbers would not have scientific notaion (e+8 for example):\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Extra options for convenient\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading data\napp_df = pd.read_csv(\"../input/loan-defaulter/application_data.csv\")\napp_df.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading previous applications data\nprevious = pd.read_csv(\"../input/loan-defaulter/previous_application.csv\")\nprevious.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking size of application and previous data (examples, columns):\nprint(\"app_df shape:\", app_df.shape, \",\", \"previous_df shape:\", previous.shape)\nprint(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic statistics about TARGET features:\nprint(app_df['TARGET'].value_counts())\n\nprint(\"Defaulters percantage: \", (app_df[app_df['TARGET'] == 1]['TARGET'].count() / app_df[app_df['TARGET'] != 2]['TARGET'].count())*100,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notice we're working with imbalanced data as just 8.07% are defaulters","metadata":{}},{"cell_type":"code","source":"# Creating a function that we're going to use during the project\n# Function takes a feature from data as an argument, splits it by the different values and returns the probabilty that\n## example with this value will default\n\ndef groupby_target(column):\n    columnXtarget = app_df[[column, 'TARGET']].groupby(column, as_index=False).mean()\n    columnXtarget = columnXtarget.sort_values('TARGET')\n    return columnXtarget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with nulls:\n1. Saving all columns with more than 25% nulls\n2. Changing categorial columns with high nulls % \n3. Examine correlation between these columns and TARGET. Delete the uncorrelated.","metadata":{}},{"cell_type":"code","source":"# Lets examine features in data \n## 65 floats, 41 ints, 16 objects\napp_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking null % value in each column:\nround(app_df.isnull().sum() / app_df.shape[0] * 100.00,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization of null % per colums in app df:\n\nnull_app_df = pd.DataFrame((app_df.isnull().sum())*100/app_df.shape[0]).reset_index() #Creating new df with 2 columns. columns from app_df saved in rows, base on \"coloumn name\"\nnull_app_df.columns = ['Column Name', 'Null Values Percentage'] #Adding names to\nfig = plt.figure(figsize=(18,6))\nax = sns.pointplot(x=\"Column Name\",y=\"Null Values Percentage\",data=null_app_df,color='b', alpha=1)\nplt.xticks(rotation =90,fontsize =7)\nax.axhline(25, ls='--',color='red') ## Red line for over 25 % of nulls\nplt.title(\"Percentage of Missing values in app data\")\nplt.ylabel(\"Null Values PERCENTAGE\")\nplt.xlabel(\"COLUMNS\")\nax.set_facecolor(\"k\")\nfig.set_facecolor(\"lightgrey\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Saving high % nulls features in a list\nhigh_null = null_app_df[null_app_df['Null Values Percentage'] > 25]\nhigh_null","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a new column that is the sum of all avg information about the appartment. alot of null in these features. \n## We want to make sure we can delete all these columns\napp_df['BUILDING_TOTAL_AVG'] = app_df.loc[:, 'APARTMENTS_AVG': 'NONLIVINGAREA_MEDI'].sum(axis=1)\napp_df['BUILDING_TOTAL_AVG'].describe() # Statitstics about column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we can check how many examples has 0 value in all appartment feauters\nlen(app_df[app_df['BUILDING_TOTAL_AVG'] == 0]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Almost half of application dont have info about building. We'll delete these columns due to too many missing values.","metadata":{}},{"cell_type":"markdown","source":"## Examiming \"OCCUPATION_TYPE\" feature. The only categrical feature with high null %","metadata":{}},{"cell_type":"code","source":"# Checking values of occupation_type\napp_df['OCCUPATION_TYPE'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We might want to save this feature since it has many values that can be predictive\n\n??# Lets classify similiar occupation types\nfirst, lets check which types of occupations has higher avg income","metadata":{}},{"cell_type":"code","source":"# Using the function we've built and plotting the results. Y-axis is the probablity of being a defaulter \ngroupby_target('OCCUPATION_TYPE').plot(kind='bar', x='OCCUPATION_TYPE', color='g')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights: Big difference between highest to lowest probabilities\n### Decision: Undelete occupation type column and complete missing values to \"other\"","metadata":{}},{"cell_type":"code","source":"# Changing all 30% missing values in occupation type feature to \"other\"\napp_df['OCCUPATION_TYPE'].replace(np.nan, 'Other',regex=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We've done examining high null % columns Lets check some statistics about flag documents \n### Sum all flag documents together to one column\n\n","metadata":{}},{"cell_type":"code","source":"app_df['FLAG_DOC_TOTAL'] = app_df.loc[:, 'FLAG_DOCUMENT_2': 'FLAG_DOCUMENT_21'].sum(axis=1)\napp_df['FLAG_DOC_TOTAL'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It looks like all the examples have only one variable marked \"1\"  \n#### Flag document has binary value. Lets check the distribution inside these binary features","metadata":{}},{"cell_type":"code","source":"flags_df = app_df.loc[:, 'FLAG_DOCUMENT_2': 'FLAG_DOCUMENT_21']\nflags_df.mean() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### All features besides flag document 3 can be deleted due to all values are 0.","metadata":{}},{"cell_type":"code","source":"# Saving \"Flag\" features in a list to be deleted\nFlagDocument_list = [col for col in app_df if col.startswith('FLAG_DOC')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FlagDocument_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deleting features with high % nulls or unnecessary from app_df","metadata":{}},{"cell_type":"code","source":"delete_col_app = high_null['Column Name'].tolist() + FlagDocument_list \ndelete_col_app.remove('FLAG_DOCUMENT_3')\ndelete_col_app.remove('OCCUPATION_TYPE')\nlen(delete_col_app) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 69 features to be delteted","metadata":{}},{"cell_type":"code","source":"# Lets delete columns and examine shape before and after:\nprint(\"app_df shape before deleting:\", app_df.shape)\napp_df.drop(labels=delete_col_app, axis=1, inplace=True)\nprint(\"app_df shape after deleting:\", app_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deleting EXT_SOURCE 2 and 3 from data because of uncertainty what it means\napp_df = app_df.drop(labels=['EXT_SOURCE_2', 'EXT_SOURCE_3'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking correlation between more unclear columns to \"TARGET\" feature in order to delete\n## these fatures if they're uncorrelated.\nobs_before_app = ['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'TARGET']\ndoc_corr = app_df[obs_before_app].corr()\nfig = plt.figure(figsize=(15,10))\nax = sns.heatmap(doc_corr,\n            xticklabels=doc_corr.columns,\n            yticklabels=doc_corr.columns,\n            annot = True,\n            cmap =\"RdYlGn\",\n            linewidth=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights: No correlation between features to TARGET\n## Decision: Deleting these columns","metadata":{}},{"cell_type":"code","source":"app_df = app_df.drop(labels=['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE'], axis=1)\napp_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking Statistics about AMT_REQ_CREDIT_BUREAU features. they had 13.5% missing values and we might want to delete them\n## since they are hard to complete and might not be predictive.\na = groupby_target('AMT_REQ_CREDIT_BUREAU_YEAR') #Using built function to save probabilites of defaulting per value in df\nplt.xticks(rotation=-45)\nsns.barplot(x='AMT_REQ_CREDIT_BUREAU_YEAR', y='TARGET', data=a) #Visualization of results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights: After examining AMT_REQ_CREDIT_BUREAU_YEAR feature (that has more unique values than the others), it seems like there is no much of a correlation between this feature to TARGET feature\n### Decision: Due to graph result and the fact that these features has 13.5% missing values we'll delete these features","metadata":{}},{"cell_type":"code","source":"app_df = app_df.drop(labels=['AMT_REQ_CREDIT_BUREAU_YEAR', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_HOUR'], axis=1)\napp_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We've done dealing with missing values. Just 42 features left as we had to delete 3/4 of features due to high null %","metadata":{}},{"cell_type":"markdown","source":"# Staitistics about loan amount per repayers and defaulters","metadata":{}},{"cell_type":"code","source":"repayers = app_df[app_df['TARGET'] == 0]\ndefaulters = app_df[app_df['TARGET'] == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting distribution\ntxt = ' Red - Repayers\\n Blue - Defaulters'\nfig = plt.figure(figsize=(12,7))\nyx = sns.distplot(repayers[['AMT_ANNUITY']], hist=False, rug=True, color='b', bins=2000)\nyx = sns.distplot(defaulters[['AMT_ANNUITY']], hist=False, rug=True, color='r', bins=2000)\nplt.title('Distribution Of Loan Amount', fontdict={'fontsize':26} )\nplt.xlim(0,200000)\nfig.text(.01,.01,txt)\nyx.set_facecolor(\"k\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets Examine basic statistics about numeric features in order to get more information about their distribution","metadata":{}},{"cell_type":"code","source":"app_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observations:\n1. Max children value is much higher than 3rd quarter value. Maybe we should band children feature \n2. Amount income total and amount annuity has huge maximum value. Should check they are outliers\n3. Features Days: (BIRTH, EMPLYOYED, REGISTRAION, ID_PUB, LAST_PHONE_CHANGE) has negative values. Well change them with \"abs\" function\n4. Days birth and days emoloyed will be changed to Age, Year Employed\n5. Days employed has a maximum value of 365243, which are 1000 years. we should check what those outliers meaning as they might be unemployed/ pensioners\n6. Max family members value is much higher than 3rd quarter value. Consider band feature  ","metadata":{}},{"cell_type":"markdown","source":"## First lets create repayers and defaulters df's for ongoing statistics","metadata":{}},{"cell_type":"code","source":"repayers = app_df[app_df['TARGET'] == 0]\ndefaulters = app_df[app_df['TARGET'] == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistics about children count:\n\nprint(app_df[['CNT_CHILDREN', 'TARGET']].groupby('CNT_CHILDREN', as_index=0).mean())\nprint(\"\")\nprint('# It seems like some children count are unique and thats why their target value mean is 0 or 1. Lets check it out! #')\nprint(\"\")\nprint(\"CHILDREN COUNT VALUES:\")\nprint(app_df['CNT_CHILDREN'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we thought, just 555 examples has more than 3 children, and there is some unique values like 11 that has just 1 example with that value. we gonna have to band it apppropriately.\n\n### Lets try and rank it. we gonna rank: 0, 1-2, 3-4, 5-6, 7+ ranks. Hopefully this is a good rank. We might change it later on ","metadata":{}},{"cell_type":"code","source":"# Creating new rank children column, base on total examples in data:\napp_df.loc[app_df['CNT_CHILDREN'] < 1, 'CNT_CHILDREN_RANK'] = 1\napp_df.loc[(app_df['CNT_CHILDREN'] > 0) & (app_df['CNT_CHILDREN'] <= 2), 'CNT_CHILDREN_RANK'] = 2\napp_df.loc[(app_df['CNT_CHILDREN'] > 2) & (app_df['CNT_CHILDREN'] <= 4), 'CNT_CHILDREN_RANK'] = 3\napp_df.loc[(app_df['CNT_CHILDREN'] > 4) & (app_df['CNT_CHILDREN'] <= 6), 'CNT_CHILDREN_RANK'] = 4\napp_df.loc[app_df['CNT_CHILDREN'] > 6 , 'CNT_CHILDREN_RANK'] = 5\n\ngroupby_target('CNT_CHILDREN_RANK')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### it seems to be more reasonable now, as we might think that more children make it harder to be a repayer.","metadata":{}},{"cell_type":"markdown","source":"## Lets do the same with family members","metadata":{}},{"cell_type":"code","source":"# Statistics about FAM_MEMBERS:\nprint(app_df[['CNT_FAM_MEMBERS', 'TARGET']].groupby('CNT_FAM_MEMBERS', as_index=0).mean())\nprint(\"\")\n\nprint(\"FAM_MEMBERS COUNT VALUES:\")\nprint(app_df['CNT_FAM_MEMBERS'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We're going to do the same with fam_members. rank it base on values","metadata":{}},{"cell_type":"code","source":"app_df.loc[app_df['CNT_FAM_MEMBERS'] < 2, 'CNT_FAM_MEMBERS_RANK'] = 1\napp_df.loc[(app_df['CNT_FAM_MEMBERS'] > 1) & (app_df['CNT_FAM_MEMBERS'] <= 2), 'CNT_FAM_MEMBERS_RANK'] = 2\napp_df.loc[(app_df['CNT_FAM_MEMBERS'] > 2) & (app_df['CNT_FAM_MEMBERS'] <= 4), 'CNT_FAM_MEMBERS_RANK'] = 3\napp_df.loc[(app_df['CNT_FAM_MEMBERS'] > 4) & (app_df['CNT_FAM_MEMBERS'] <= 6), 'CNT_FAM_MEMBERS_RANK'] = 4\napp_df.loc[app_df['CNT_FAM_MEMBERS'] > 6, 'CNT_FAM_MEMBERS_RANK'] = 5\n\napp_df[['CNT_FAM_MEMBERS_RANK', 'TARGET']].groupby('CNT_FAM_MEMBERS_RANK', as_index=0).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It does seems like more fam members = higher chance being a deaulter. But, its not as obvious as it was with \"CNT_CHILDREN\".","metadata":{}},{"cell_type":"markdown","source":"## Next, we're going to change all features that count by 'DAYS', which counts by negative values to positive values","metadata":{}},{"cell_type":"code","source":"# Saving all columns starting with 'DAYS' since those are the columns we want to change\nminus_col = [col for col in app_df if col.startswith('DAYS')]\nminus_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying \"abs\" (absolut) function to all columns starting with 'DAYS'. \napp_df[minus_col]= abs(app_df[minus_col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Changing relevant days counted features to years to observe some statistics about them. (Only 2 at the moment(!))\napp_df['Age'] = app_df['DAYS_BIRTH'] / 365\napp_df['Years Employed'] = app_df['DAYS_EMPLOYED'] / 365 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for outliers in Years Employed, as we observed before:\nplt.figure(figsize=(8,4))\nax = sns.distplot(app_df['Years Employed'], color=\"y\")\nplt.title('Distribution of Years Employed', fontdict={'fontsize':20} )\nax.set_facecolor(\"k\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights: Alot of examples has 1000 years of working value. should treat it\n## Lets check who are those examples","metadata":{}},{"cell_type":"code","source":"app_df[app_df['Years Employed'] > 900]['NAME_INCOME_TYPE'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision: We'll change pensioners years employed value to 25 and and unemployed years employed to 0","metadata":{}},{"cell_type":"code","source":"#Using python mask built in function, we can change years employed of pensioners and unemployed easily and deal\n## with those outliers smartly.\n\n#Pensioners to 25\napp_df['Years Employed'] = app_df[\"Years Employed\"].mask((app_df[\"Years Employed\"] > 900) & (app_df['NAME_INCOME_TYPE'] == 'Pensioner'), 25)\n#Unemployed to 0     \napp_df['Years Employed'] = app_df[\"Years Employed\"].mask((app_df[\"Years Employed\"] > 900) & (app_df['NAME_INCOME_TYPE'] == 'Unemployed'), 0)\n\nlen(app_df[app_df['Years Employed'] > 900]) #Prints 0 if no more outliers                                                                                                             ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets deal with data about cell phone, work phone and so on","metadata":{}},{"cell_type":"code","source":"print(groupby_target('FLAG_EMP_PHONE'))\nprint(\"------------------------------\")\nprint(groupby_target('FLAG_CONT_MOBILE'))\nprint(\"------------------------------\")\nprint(groupby_target('FLAG_PHONE'))\nprint(\"------------------------------\")\nprint(groupby_target('FLAG_EMAIL'))\nprint(\"------------------------------\")\nprint(groupby_target('FLAG_MOBIL'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(app_df['FLAG_MOBIL'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### At this point we'll only drop the flag_mobil column since only 1 example did not provide his phone (We can observe distribution around target in the last section of above code). No reason to delete other columns since they has good spread and may contribute to the model","metadata":{}},{"cell_type":"code","source":"app_df = app_df.drop(['FLAG_MOBIL'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking some statistics about clients address validity:","metadata":{}},{"cell_type":"code","source":"print(app_df['REG_REGION_NOT_LIVE_REGION'].value_counts())\nprint('-'*50)\nprint(app_df['REG_REGION_NOT_WORK_REGION'].value_counts())\nprint('-'*50)\nprint(app_df['LIVE_REGION_NOT_WORK_REGION'].value_counts())\nprint('-'*50)\nprint(app_df['REG_CITY_NOT_LIVE_CITY'].value_counts())\nprint('-'*50)\nprint(app_df['REG_CITY_NOT_WORK_CITY'].value_counts())\nprint('-'*50)\nprint(app_df['LIVE_CITY_NOT_WORK_CITY'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It seems like we can work with this columns\n##### We can also think about synthethic feature that sums all these together.","metadata":{}},{"cell_type":"code","source":"# Lets observe null % again:\nround(app_df.isnull().sum() / app_df.shape[0] * 100.00,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets clean some last nulls in:\n- AMT_ANNUITY\n- CNT_FAM_MEMBERS_RANK\n- AMT_GOODS_PRICE\n- DAYS_LAST_PHONE_CHANGED","metadata":{}},{"cell_type":"code","source":"# Basic fill of nulls by mean or median:\napp_df['AMT_ANNUITY'] = app_df['AMT_ANNUITY'].fillna(app_df['AMT_ANNUITY'].mean())\napp_df['CNT_FAM_MEMBERS_RANK'] = app_df['CNT_FAM_MEMBERS_RANK'].fillna(app_df['CNT_FAM_MEMBERS_RANK'].median())\napp_df['DAYS_LAST_PHONE_CHANGE'] = app_df['DAYS_LAST_PHONE_CHANGE'].fillna(app_df['DAYS_LAST_PHONE_CHANGE'].mean())\n\n# Sophisticated fill of nulls:\n# For each example that has null in (AMT_GOODS_PRICE), we'll take the mean of amt_goods_price that is \n##relevant to the age ot the null example\napp_df['AMT_GOODS_PRICE'] = app_df['AMT_GOODS_PRICE'].fillna(app_df.groupby('Age')['AMT_GOODS_PRICE'].transform('mean'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# At this point we'll delete some more numeric features: \n## Days birth and employed changed to years. Family members count changed to ranks. ","metadata":{}},{"cell_type":"code","source":"print(app_df.shape)\napp_df = app_df.drop(labels=['DAYS_BIRTH', 'DAYS_EMPLOYED', 'BUILDING_TOTAL_AVG', 'CNT_FAM_MEMBERS'], axis=1)\nprint(app_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# All other continuous values (and some others) will be normalized later # ","metadata":{}},{"cell_type":"markdown","source":"# --------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Finished Numeric features. Lets start dealing with some categorical features","metadata":{}},{"cell_type":"code","source":"# Basic statistics about categorical columns:\napp_df.describe(include=['O']) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Changing XNA 4 values in gender feature to the more common one (women)\napp_df['CODE_GENDER'].replace({'XNA': \"F\"}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pie plots for statistics about genders","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1,3)\n\nfor d in [\n    {'df': app_df, 'ax': ax1, 'title': 'Gender Distribution in data', 'cp': 'bright'},\n    {'df': repayers, 'ax': ax2, 'title': 'Distribution of repayers per gender', 'cp': 'deep'},\n    {'df': defaulters, 'ax': ax3, 'title': 'Distribution of defaulters per gender', 'cp': 'deep'},\n]:\n    d['ax'].set_title(d['title'], fontsize=15)\n    d['df'][\"CODE_GENDER\"].value_counts().plot.pie(\n        ax=d['ax'], autopct=\"%1.0f%%\", fontsize=15, figsize=(15,8),\n        colors=sns.color_palette(d['cp']),\n        wedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"},shadow=False\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Insights: Distribution within repayers preserves almost same original gaps within genders.\n# Distribution of defaulters is different. percentage gap between genders is smaller. the 1% diff from the original distribution making a big change within defaulters","metadata":{}},{"cell_type":"code","source":"# Lets visuazlize some statitstics about genders and incomes:\nplt.figure(figsize=(10,6))\nax = sns.pointplot(x='TARGET', y='AMT_INCOME_TOTAL', hue='CODE_GENDER', data=app_df)\nax.set_facecolor(\"#f2f2f2\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights:\n1. Men: In the transition from repayers to defaulters, average income is decreasing.\n2. Women: In the transition from repayers to defaulters, average income stays the same.","metadata":{}},{"cell_type":"code","source":"#Changing genders and other features to binary\napp_df['CODE_GENDER'].replace({'F':0, 'M':1}, inplace=True)\napp_df['NAME_CONTRACT_TYPE'].replace({'Cash loans':0, 'Revolving loans':1}, inplace=True)\napp_df['FLAG_OWN_CAR'].replace({'N':0, 'Y':1}, inplace=True)\napp_df['FLAG_OWN_REALTY'].replace({'Y':0, 'N':1}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df.describe(include=['O']) #Checking relevant columns have been changed from categorical.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with \"NAME_TYPE_SUITE\" feature","metadata":{}},{"cell_type":"code","source":"#Checking for missing values\napp_df['NAME_TYPE_SUITE'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking values count\napp_df['NAME_TYPE_SUITE'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing nulls to unaccompanied (most common feature)\napp_df['NAME_TYPE_SUITE'].replace(np.nan, 'Unaccompanied',regex=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with \"NAME_INCOME_TYPE\" feature. There are 8 different types there. Lets check values count, and avg target per type.","metadata":{}},{"cell_type":"code","source":"app_df['NAME_INCOME_TYPE'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4 Types have low values amount. We will change these to \"WORKING\" since its the most common one.","metadata":{}},{"cell_type":"code","source":"app_df['NAME_INCOME_TYPE'].replace(['Businessman', 'Student', 'Unemployed', 'Maternity leave'], 'Working', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistics about income type\ngroupby_target('NAME_INCOME_TYPE').plot(kind='bar', x='NAME_INCOME_TYPE', color='#ff5522') # Visual of avg target per type after changing.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights: Difference between probabilities to default are small\n\n## Decision: We will use one hot encoding method later on","metadata":{}},{"cell_type":"markdown","source":"# Dealing with \"EDUCATION_TYPE feature","metadata":{}},{"cell_type":"code","source":"# Visualization\nplt.figure(figsize=(12.5,8.5))\napp_df[\"NAME_EDUCATION_TYPE\"].value_counts().plot.pie(autopct = \"%1.0f%%\",fontsize=10,\ncolors = sns.color_palette(\"cubehelix\"),\nwedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"},shadow =False)\n\nplt.title(\"Distribution of education type\",color=\"g\", fontsize=9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights: Amount of examples that has academic degree and applies for loan in app_df is lower than 1%","metadata":{}},{"cell_type":"code","source":"# Visualization of avg target per education type.\nax = sns.barplot(x='NAME_EDUCATION_TYPE', y='TARGET', data=app_df, color='y')\nplt.xticks(rotation=-45)\nax.set_facecolor(\"k\")\nplt.title(\"Odds of being a defaulter per education type\", fontsize=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights: Examples with academic degree or higher educataion tend to default less\n\n## Decision: We will use one hot encoding method later on","metadata":{}},{"cell_type":"markdown","source":"# Dealing with family status feature","metadata":{}},{"cell_type":"code","source":"app_df['NAME_FAMILY_STATUS'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Changing unknown to married since its the most common value\napp_df['NAME_FAMILY_STATUS'].replace('Unknown', 'Married',regex=True, inplace=True)\n#Later on well use one hot method with this feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with \"NAME_HOUSING_TYPE\" value","metadata":{}},{"cell_type":"code","source":"app_df['NAME_HOUSING_TYPE'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spread in values is okay. We'll use one-hot encoding later on","metadata":{}},{"cell_type":"markdown","source":"# Dealing with \"ORGANIZATION_TYPE\" feature","metadata":{}},{"cell_type":"code","source":"# Visualization of probailites being a defaulter pre each value\ngroupby_target('ORGANIZATION_TYPE').plot(kind='bar', figsize=(12.5,6), x='ORGANIZATION_TYPE', y='TARGET', color='#000000' ,alpha=1)\nplt.xticks(rotation=-90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Insights:\n1. 12% Difference between highest to lowest occupatios.\n2. this should be a good feature to out model since it has 58 different values. \n# Decision:\n1. We will use one-hot encoding later on.","metadata":{}},{"cell_type":"markdown","source":"# Were going to use features from previous df in our model:\n1. Creating smalls df's that contains 2 columns: \"SK_ID_CURR\", \"COUNT\"\n##### It will measure the amount of times that applier from current application requested loan in previous data. We will also devide it to 4 different type of counts base on the previous loan decision (approved/canceled...)\n2. We'll use pandas join function on id. This will add new columns to app_df base on our counts.\n##### since some examples didnt apply for loan in the past (e.g dont have a row in previous df), their value will be \"nan\" in loan count column\n3. Replacing nans in the new columns to 0, since nan is default applied for examples who didnt request loan in previous data","metadata":{}},{"cell_type":"code","source":"loan_counter = pd.DataFrame(previous['SK_ID_CURR'].value_counts()).reset_index() # Creating small df base on id count from previous df\nloan_counter.columns = ['SK_ID_CURR', 'Total Loan Count'] #Adding names to counter\n\nrefused_df = pd.DataFrame(previous[previous['NAME_CONTRACT_STATUS'] == 'Refused'])\nrefused_counter = pd.DataFrame(refused_df['SK_ID_CURR'].value_counts()).reset_index() # Creating small df base on refused loan and id count from previous dfrefused_counter.columns = ['SK_ID_CURR', 'Refused Count'] #Adding names to counter\nrefused_counter.columns = ['SK_ID_CURR', 'Refused Count'] #Adding names to refused counter\n\napproved_df = pd.DataFrame(previous[previous['NAME_CONTRACT_STATUS'] == 'Approved'])\napproved_counter = pd.DataFrame(approved_df['SK_ID_CURR'].value_counts()).reset_index() # Creating small df base on approved loan and id count from previous dfrefused_counter.columns = ['SK_ID_CURR', 'Refused Count'] #Adding names to counter\napproved_counter.columns = ['SK_ID_CURR', 'Approved Count'] #Adding names to approved counter\n\ncanceled_df = pd.DataFrame(previous[previous['NAME_CONTRACT_STATUS'] == 'Canceled'])\ncanceled_counter = pd.DataFrame(canceled_df['SK_ID_CURR'].value_counts()).reset_index() # Creating small df base on canceled loan and id count from previous dfrefused_counter.columns = ['SK_ID_CURR', 'Refused Count'] #Adding names to counter\ncanceled_counter.columns = ['SK_ID_CURR', 'Canceled Count'] #Adding names to canceled counter\n\nunused_df = pd.DataFrame(previous[previous['NAME_CONTRACT_STATUS'] == 'Unused offer'])\nunused_counter = pd.DataFrame(unused_df['SK_ID_CURR'].value_counts()).reset_index() # Creating small df base on un-used loan and id count from previous dfrefused_counter.columns = ['SK_ID_CURR', 'Refused Count'] #Adding names to counter\nunused_counter.columns = ['SK_ID_CURR', 'Unused Count'] #Adding names to un-used counter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df = app_df.join(loan_counter.set_index('SK_ID_CURR'), on='SK_ID_CURR') # Using pandas join function on id, to add loan count\napp_df = app_df.join(refused_counter.set_index('SK_ID_CURR'), on='SK_ID_CURR') # Using pandas join function on id, to add refused count\napp_df = app_df.join(approved_counter.set_index('SK_ID_CURR'), on='SK_ID_CURR') # Using pandas join function on id, to add approved count\napp_df = app_df.join(canceled_counter.set_index('SK_ID_CURR'), on='SK_ID_CURR') # Using pandas join function on id, to add cancelled count\napp_df = app_df.join(unused_counter.set_index('SK_ID_CURR'), on='SK_ID_CURR') # Using pandas join function on id, to add unused count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding app_df a column that describes if client requested insurance in his last application.\n### We'll take only the last application using min \"Days_Decision\" value ","metadata":{}},{"cell_type":"code","source":"prev_insurance = previous.groupby('SK_ID_CURR', as_index=False).min('DAYS_DECISION')[['SK_ID_CURR', 'NFLAG_INSURED_ON_APPROVAL']]\napp_df = app_df.join(prev_insurance.set_index('SK_ID_CURR'), on='SK_ID_CURR')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In previous_df, we have \"DAYS_DECISION feature which measures how many days has past since final payment date of application from previous_df, to application request in app_df \n### Days are relative to current application\n#### Since some examples has more than one loan request in previous_df, we have to smartly take only the days since last application","metadata":{}},{"cell_type":"code","source":"previous['DAYS_DECISION'] = abs(previous['DAYS_DECISION']) # Converting and then adding -min- days decision to app_df\nmin_days_decision = previous.groupby('SK_ID_CURR', as_index=False).min('DAYS_DECISION')[['SK_ID_CURR', 'DAYS_DECISION']]\napp_df = app_df.join(min_days_decision.set_index('SK_ID_CURR'), on='SK_ID_CURR') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Synthethic features:\n## 3 Synthethic features to be added to try and make model more predictive","metadata":{}},{"cell_type":"code","source":"# Making synthteic features for total income devided by annuity amount. same for credit amount:\napp_df['INC_ANNUITY'] = app_df['AMT_INCOME_TOTAL'] / app_df['AMT_ANNUITY']\napp_df['CRED_ANNUITY'] = app_df['AMT_CREDIT'] / app_df['AMT_ANNUITY']\n\n# Synthethic feature for age / (age+children). We want to make model know combined statistics about age and children's of client\napp_df['ageXchildren'] = app_df['Age'] / (app_df['Age'] + app_df['CNT_CHILDREN']) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can delete Children count feature now since its been ranked to new feature\napp_df = app_df.drop(['CNT_CHILDREN'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We have to deal with more null values added from previous_df\n## We want to save these new features since they might be predictive\n### But, we will complete nulls only after standard scaling these feature so that null values will be 0. ","metadata":{}},{"cell_type":"code","source":"app_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df['ORGANIZATION_TYPE'].replace(['Industry: type 8', 'Trade: type 5', 'Trade: type 4','Industry: type 13', 'Religion', 'Industry: type 10', 'Industry: type 6'], 'Other', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 51 Features for model to use, before one-hot encoding","metadata":{}},{"cell_type":"code","source":"# dummies for all + deleting IMMEDIATELY original columns\napp_df = pd.get_dummies(data=app_df, columns=['OCCUPATION_TYPE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_HOUSING_TYPE', 'ORGANIZATION_TYPE', 'NAME_FAMILY_STATUS', 'WEEKDAY_APPR_PROCESS_START', 'CNT_FAM_MEMBERS_RANK', 'CNT_CHILDREN_RANK'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using sklearn standard scaler function\napp_df[['DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'HOUR_APPR_PROCESS_START', 'DAYS_LAST_PHONE_CHANGE', 'Age', 'Years Employed', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'Total Loan Count', 'Refused Count', 'Approved Count', 'Canceled Count', 'Unused Count', 'DAYS_DECISION', 'INC_ANNUITY', 'CRED_ANNUITY', 'ageXchildren', 'NFLAG_INSURED_ON_APPROVAL']] = StandardScaler().fit_transform(app_df[['DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'HOUR_APPR_PROCESS_START', 'DAYS_LAST_PHONE_CHANGE', 'Age', 'Years Employed', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'Total Loan Count', 'Refused Count', 'Approved Count', 'Canceled Count', 'Unused Count', 'DAYS_DECISION', 'INC_ANNUITY', 'CRED_ANNUITY', 'ageXchildren', 'NFLAG_INSURED_ON_APPROVAL']])\n\n# Lets complete all nulls to 0 after we normazlied them. now all the nulls will have 0 values and wont effect distribution.\napp_df.fillna({'Total Loan Count':0,'Refused Count':0, 'Approved Count':0, 'Unused Count':0, 'Canceled Count':0, 'DAYS_DECISION':0, 'NFLAG_INSURED_ON_APPROVAL':0, 'SQR DecisionEmployed':0},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making sure that there is no more missing values","metadata":{}},{"cell_type":"code","source":"app_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deleting final outliers, and completing missing values for new added features","metadata":{}},{"cell_type":"code","source":"delete_outliers_columns = ['AMT_INCOME_TOTAL', 'AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'Total Loan Count', 'Refused Count', 'Approved Count', 'Canceled Count', 'Unused Count']\nfor column in delete_outliers_columns:\n    app_df = app_df[app_df[column] < 35]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We're ready for predictions!\n# Thats it for now. Modeling and predictions will be writtten in separate code","metadata":{}}]}