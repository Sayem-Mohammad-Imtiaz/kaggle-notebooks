{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-07T15:16:05.135802Z","iopub.execute_input":"2021-08-07T15:16:05.136289Z","iopub.status.idle":"2021-08-07T15:16:05.15986Z","shell.execute_reply.started":"2021-08-07T15:16:05.136174Z","shell.execute_reply":"2021-08-07T15:16:05.158701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:05.161872Z","iopub.execute_input":"2021-08-07T15:16:05.162215Z","iopub.status.idle":"2021-08-07T15:16:06.262687Z","shell.execute_reply.started":"2021-08-07T15:16:05.162181Z","shell.execute_reply":"2021-08-07T15:16:06.261423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Loading","metadata":{}},{"cell_type":"markdown","source":"For this project we load data set from sensor reading to predict building energy consumption. Besides sensor reading we get the data from weather station which is temperature and humidity on each measurement was conducted. Sensor data were temperature and humidity of building, in this case is two floor house. ","metadata":{}},{"cell_type":"code","source":"path = '../input/appliances-energy-prediction-data-set/energydata_complete.csv'\ndataLoad = pd.read_csv(path, index_col='date')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:06.264854Z","iopub.execute_input":"2021-08-07T15:16:06.265286Z","iopub.status.idle":"2021-08-07T15:16:06.448509Z","shell.execute_reply.started":"2021-08-07T15:16:06.265223Z","shell.execute_reply":"2021-08-07T15:16:06.447465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Data Exploration","metadata":{}},{"cell_type":"markdown","source":"On the basic data exploration result we can know that the data consist of 28 features and 19735 rows. In the data itself the missing values are not found.","metadata":{}},{"cell_type":"code","source":"dataLoad.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:06.450083Z","iopub.execute_input":"2021-08-07T15:16:06.45046Z","iopub.status.idle":"2021-08-07T15:16:06.490277Z","shell.execute_reply.started":"2021-08-07T15:16:06.450428Z","shell.execute_reply":"2021-08-07T15:16:06.489349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataLoad.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:55:59.155718Z","iopub.execute_input":"2021-08-07T16:55:59.156116Z","iopub.status.idle":"2021-08-07T16:55:59.162886Z","shell.execute_reply.started":"2021-08-07T16:55:59.156081Z","shell.execute_reply":"2021-08-07T16:55:59.1618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataLoad.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:06.491744Z","iopub.execute_input":"2021-08-07T15:16:06.492032Z","iopub.status.idle":"2021-08-07T15:16:06.515045Z","shell.execute_reply.started":"2021-08-07T15:16:06.492004Z","shell.execute_reply":"2021-08-07T15:16:06.514317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataLoad.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:06.516022Z","iopub.execute_input":"2021-08-07T15:16:06.516431Z","iopub.status.idle":"2021-08-07T15:16:06.618501Z","shell.execute_reply.started":"2021-08-07T15:16:06.516391Z","shell.execute_reply":"2021-08-07T15:16:06.617436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataLoad.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:06.620914Z","iopub.execute_input":"2021-08-07T15:16:06.62122Z","iopub.status.idle":"2021-08-07T15:16:06.632497Z","shell.execute_reply.started":"2021-08-07T15:16:06.621192Z","shell.execute_reply":"2021-08-07T15:16:06.631044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"Then we do more exploratory data analysis to know the distribution of dataset. I use univariate distribution analysis for each features in the data sets. Also, I do bivarite analysis to know the correlation between target variable which in here is Appliances column and the predictor variable. In the bivariate analysis I also make correlation heatmap to understand correlation between each feature. From the exploratory data analysis we know that some predictor almost have normal distribution. ","metadata":{}},{"cell_type":"markdown","source":"### Univariate","metadata":{}},{"cell_type":"code","source":"dataVisual = dataLoad.drop(['rv1', 'rv2'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:06.634374Z","iopub.execute_input":"2021-08-07T15:16:06.634683Z","iopub.status.idle":"2021-08-07T15:16:06.643541Z","shell.execute_reply.started":"2021-08-07T15:16:06.634655Z","shell.execute_reply":"2021-08-07T15:16:06.642299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution plot for each features\n\nls = []\ncolName = [j for j in dataVisual.columns]\na = 0\nwhile a < 7:\n    for i in range(5):\n        ls.append((a, i))\n    a+=1\nfig, ax = plt.subplots(6, 5, figsize=(28, 30))\nfor k in range(26):\n    sns.histplot(ax=ax[ls[k][0], ls[k][1]], x=colName[k], data=dataVisual)\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:06.644837Z","iopub.execute_input":"2021-08-07T15:16:06.645274Z","iopub.status.idle":"2021-08-07T15:16:15.700811Z","shell.execute_reply.started":"2021-08-07T15:16:06.645224Z","shell.execute_reply":"2021-08-07T15:16:15.699809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 0\nwhile a < 7:\n    for i in range(5):\n        ls.append((a, i))\n    a+=1\nfig, ax = plt.subplots(6, 5, figsize=(28, 30))\nfor k in range(26):\n    sns.boxplot(ax=ax[ls[k][0], ls[k][1]], y=colName[k], data=dataVisual)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:15.702288Z","iopub.execute_input":"2021-08-07T15:16:15.702861Z","iopub.status.idle":"2021-08-07T15:16:19.120879Z","shell.execute_reply.started":"2021-08-07T15:16:15.702815Z","shell.execute_reply":"2021-08-07T15:16:19.11979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bivariate","metadata":{}},{"cell_type":"code","source":"b = 0\nwhile b < 7:\n    for i in range(5):\n        ls.append((b, i))\n    b+=1\nfig, ax = plt.subplots(5, 5, figsize=(28, 30))\nfor k in range(25):\n    sns.scatterplot(ax=ax[ls[k][0], ls[k][1]], x=colName[k+1], y=colName[0], data=dataVisual)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:19.122154Z","iopub.execute_input":"2021-08-07T15:16:19.122471Z","iopub.status.idle":"2021-08-07T15:16:25.374153Z","shell.execute_reply.started":"2021-08-07T15:16:19.122441Z","shell.execute_reply":"2021-08-07T15:16:25.373098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = dataVisual.corr()\nplt.figure(figsize=(30, 30))\nsns.heatmap(corr, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:25.375438Z","iopub.execute_input":"2021-08-07T15:16:25.375751Z","iopub.status.idle":"2021-08-07T15:16:29.462126Z","shell.execute_reply.started":"2021-08-07T15:16:25.375719Z","shell.execute_reply":"2021-08-07T15:16:29.461135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing Data","metadata":{}},{"cell_type":"markdown","source":"Before the data were pushed to model, I do some preprocessing to avoid poor performance of the model. I do normalization to make distribution of the features become normal. Also, I try to remove outliers of the data. It is because for predictive model that I will use sensitive to outliers and it can make poor performance model. After removing outliers I have found many missing value of the features. I encouter it with filling missing value with imputation method. I use mean value of each feature to fill the missing value.","metadata":{}},{"cell_type":"code","source":"## Data Normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndataTrans = pd.DataFrame(scaler.fit_transform(dataVisual), columns=dataVisual.columns)\ndataTrans.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.4635Z","iopub.execute_input":"2021-08-07T15:16:29.463803Z","iopub.status.idle":"2021-08-07T15:16:29.613143Z","shell.execute_reply.started":"2021-08-07T15:16:29.463773Z","shell.execute_reply":"2021-08-07T15:16:29.612149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eliminating outliers\nQ1=dataTrans.quantile(0.25)\nQ3 = dataTrans.quantile(0.75)\nIQR = Q3-Q1\n\ndataClean = dataTrans[~((dataTrans < (Q1-IQR * 1.5)) |(dataTrans > (Q3 + IQR*1.5)))]\ndataClean.head()\ndataClean.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.614577Z","iopub.execute_input":"2021-08-07T15:16:29.61485Z","iopub.status.idle":"2021-08-07T15:16:29.643599Z","shell.execute_reply.started":"2021-08-07T15:16:29.614823Z","shell.execute_reply":"2021-08-07T15:16:29.642452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='mean')\ndataImp = pd.DataFrame(imputer.fit_transform(dataClean), columns=dataClean.columns)\ndataImp.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.644908Z","iopub.execute_input":"2021-08-07T15:16:29.645197Z","iopub.status.idle":"2021-08-07T15:16:29.871784Z","shell.execute_reply.started":"2021-08-07T15:16:29.645167Z","shell.execute_reply":"2021-08-07T15:16:29.870635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataImp.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.873139Z","iopub.execute_input":"2021-08-07T15:16:29.873527Z","iopub.status.idle":"2021-08-07T15:16:29.884681Z","shell.execute_reply.started":"2021-08-07T15:16:29.873495Z","shell.execute_reply":"2021-08-07T15:16:29.883372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Selection","metadata":{}},{"cell_type":"markdown","source":"Then I do feature selection to make sure I use only importance features for training the model and predicting the energy consumption with small error. I try two diffrence approaches for it. First i am using univariate selection with F value. Then, I am also using features importance to find importance feature. In both method I decide to select the 10 best features.","metadata":{}},{"cell_type":"markdown","source":"### Univariate Selection","metadata":{}},{"cell_type":"code","source":"dataTarget = dataImp['Appliances']\ndataPred = dataImp.drop('Appliances', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.886048Z","iopub.execute_input":"2021-08-07T15:16:29.886416Z","iopub.status.idle":"2021-08-07T15:16:29.893309Z","shell.execute_reply.started":"2021-08-07T15:16:29.886381Z","shell.execute_reply":"2021-08-07T15:16:29.892275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(dataPred, dataTarget, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.89469Z","iopub.execute_input":"2021-08-07T15:16:29.894988Z","iopub.status.idle":"2021-08-07T15:16:29.911579Z","shell.execute_reply.started":"2021-08-07T15:16:29.89496Z","shell.execute_reply":"2021-08-07T15:16:29.910397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\nselector = SelectKBest(score_func=f_regression, k=10)\nfit = selector.fit(dataPred, dataTarget)\ndfscores = pd.DataFrame(fit.scores_)\ndfpred = pd.DataFrame(dataPred.columns)\nfeatScore = pd.concat([dfpred, dfscores], axis=1)\nfeatScore.columns = ['Feature', 'Score']\nfeatScore.nlargest(10, 'Score')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.914582Z","iopub.execute_input":"2021-08-07T15:16:29.914971Z","iopub.status.idle":"2021-08-07T15:16:29.959624Z","shell.execute_reply.started":"2021-08-07T15:16:29.914936Z","shell.execute_reply":"2021-08-07T15:16:29.958437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:29.96176Z","iopub.execute_input":"2021-08-07T15:16:29.962227Z","iopub.status.idle":"2021-08-07T15:16:30.030768Z","shell.execute_reply.started":"2021-08-07T15:16:29.962179Z","shell.execute_reply":"2021-08-07T15:16:30.02955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ExtraTreesRegressor()\nmodel.fit(x_train, y_train)\nfeat_importance = pd.Series(model.feature_importances_, index=x_train.columns)\nfeat_importance.nlargest(10).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:16:30.036103Z","iopub.execute_input":"2021-08-07T15:16:30.038984Z","iopub.status.idle":"2021-08-07T15:16:38.838288Z","shell.execute_reply.started":"2021-08-07T15:16:30.03891Z","shell.execute_reply":"2021-08-07T15:16:38.837273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Splitting","metadata":{}},{"cell_type":"markdown","source":"I am splitting data into two parts training and test data set for each variable. I do this with proportion 80 % of total data are training dataset and the rest are test dataset. Then I split the training dataset again in to two parts training and validation dataset. The second spltting I use proportion 80% of the training data are training data and the rest of data are validation dataset.","metadata":{}},{"cell_type":"code","source":"y = dataImp['Appliances']\nX = dataImp[['T2', 'T6', 'T8', 'T1', 'T_out', 'T4','RH_9', 'RH_6', 'RH_8', 'RH_out', 'lights']]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:24:35.311853Z","iopub.execute_input":"2021-08-07T15:24:35.312289Z","iopub.status.idle":"2021-08-07T15:24:35.324604Z","shell.execute_reply.started":"2021-08-07T15:24:35.312234Z","shell.execute_reply":"2021-08-07T15:24:35.323296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:25:43.422844Z","iopub.execute_input":"2021-08-07T15:25:43.423272Z","iopub.status.idle":"2021-08-07T15:25:43.435715Z","shell.execute_reply.started":"2021-08-07T15:25:43.423223Z","shell.execute_reply":"2021-08-07T15:25:43.434513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:27:03.259403Z","iopub.execute_input":"2021-08-07T15:27:03.259856Z","iopub.status.idle":"2021-08-07T15:27:03.269488Z","shell.execute_reply.started":"2021-08-07T15:27:03.259818Z","shell.execute_reply":"2021-08-07T15:27:03.268136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model","metadata":{}},{"cell_type":"markdown","source":"For baseline model I am using linear regression and random forest regressor. For the linear regression model I keep the parameter deafult. For the random forest regressor I vary the n_estimators parameter from 1 until 100. The metrics to evaluate each model performance is mean absolute error. ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:29:08.226961Z","iopub.execute_input":"2021-08-07T15:29:08.227463Z","iopub.status.idle":"2021-08-07T15:29:08.232176Z","shell.execute_reply.started":"2021-08-07T15:29:08.227409Z","shell.execute_reply":"2021-08-07T15:29:08.231381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regmodel = LinearRegression()\nregmodel = regmodel.fit(xTrain, yTrain)\npredReg = regmodel.predict(xVal)\nmaeReg = mean_absolute_error(yVal, predReg)\nprint('This is MAE score for Linear Regression : ', round(maeReg, 3))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:32:43.890961Z","iopub.execute_input":"2021-08-07T15:32:43.891416Z","iopub.status.idle":"2021-08-07T15:32:43.911802Z","shell.execute_reply.started":"2021-08-07T15:32:43.89138Z","shell.execute_reply":"2021-08-07T15:32:43.910508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modelRF(n, xTrain, yTrain, xVal, yVal):\n    modelRF = RandomForestRegressor(n_estimators=n, criterion='mae')\n    modelRF.fit(xTrain, yTrain)\n    predRF = modelRF.predict(xVal)\n    maeRF = mean_absolute_error(yVal, predRF)\n    return print('This is MAE score for Random Forest Regressor model', n, 'estimators : ', round(maeRF, 3))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:41:54.506033Z","iopub.execute_input":"2021-08-07T15:41:54.506505Z","iopub.status.idle":"2021-08-07T15:41:54.512931Z","shell.execute_reply.started":"2021-08-07T15:41:54.506465Z","shell.execute_reply":"2021-08-07T15:41:54.511841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = [1, 5, 10, 20, 25, 50, 75, 100]\n\nfor i in n:\n    modelRF(i, xTrain, yTrain, xVal, yVal)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T15:41:56.903739Z","iopub.execute_input":"2021-08-07T15:41:56.904499Z","iopub.status.idle":"2021-08-07T16:13:47.867289Z","shell.execute_reply.started":"2021-08-07T15:41:56.904446Z","shell.execute_reply":"2021-08-07T16:13:47.866241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model building ","metadata":{}},{"cell_type":"markdown","source":"Finally I build the model with best performance that I know from baseline model. In here I decide to use random forest regressor model with parameter n_estimators are 20 and the criterion parameter is 'mae'.","metadata":{}},{"cell_type":"code","source":"modelFinal = RandomForestRegressor(n_estimators=20, criterion='mae')\nmodelFinal.fit(x_train, y_train)\npredFinal = modelFinal.predict(x_test)\nmaeFinal = mean_absolute_error(y_test, predFinal)\nprint('This is MAE score of final model :', round(maeFinal, 3))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:31:55.93477Z","iopub.execute_input":"2021-08-07T16:31:55.935278Z","iopub.status.idle":"2021-08-07T16:35:19.453218Z","shell.execute_reply.started":"2021-08-07T16:31:55.935199Z","shell.execute_reply":"2021-08-07T16:35:19.452071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}