{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Tutorial: Wine EDA, Clustering and PCA**"},{"metadata":{},"cell_type":"markdown","source":"1. Make and EDA for the dataset in order to explain the current variables in the dataset and how they affect the diferent values.\n2. Develop a clustering model to compare the results vs the variable \"Customer_Segment\".\n3. Develop a predictic model using PCA and test the output."},{"metadata":{},"cell_type":"markdown","source":"**EDA (Exploratory Data Analysis)**"},{"metadata":{},"cell_type":"markdown","source":"As first step for the EDA, we load the data in order to identify all the variables and values inside our file."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Load Data\ndf = pd.read_csv(\"../input/wine-pca/Wine.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Null values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As second step, we proceed to remove \"Customer_Segment\" from our data, because that colum is the result that we want to get, so for this first process we dont need that value."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"Customer_Segment\"], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density plot for each attribute\nfig, ax = plt.subplots(5,3, figsize=(14,12))\naxes_ = [axes_row for axes in ax for axes_row in axes]\nfor i,c in enumerate(df.columns):\n    sns.distplot(df[c], ax = axes_[i], color = 'red')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot for each attribute\nfig, ax = plt.subplots(5,3, figsize=(14,12))\naxes_ = [axes_row for axes in ax for axes_row in axes]\nfor i,c in enumerate(df.columns):\n    sns.boxplot(df[c], ax = axes_[i], color = 'orange')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\nf,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(df.corr(method='spearman'),annot=True,fmt=\".1f\",linewidths=1,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr(method='spearman')\nth = 0.6\ncorr[corr > th]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After this first analysis we notice the following:\n* There is a high linear relation between Total_Phenols, Flavanoids, Proanthocyanins and OD280.\n* There is a high linear realtion between Alcohol, Color_Intensity and Proline.\n\nNow we can remove some of those variables in order to avoid redundant information which can affect the results of our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"Flavanoids\",\"Proanthocyanins\",\"Color_Intensity\",\"OD280\",\"Proline\"], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clustering**\n\nBefore select any clustering model we have to standardize our data because clustering algorithms need all data in the same scale."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Scalation\nfrom sklearn import preprocessing\nnew_df= preprocessing.StandardScaler().fit_transform(df)\nnew_df = pd.DataFrame(new_df, columns=['Alcohol', 'Malic_Acid', 'Ash', 'Ash_Alcanity', 'Magnesium','Total_Phenols','Nonflavanoid_Phenols','Hue'])\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of scaled and unscaled data\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n\nax1.set_title('Before Scaling')\nsns.kdeplot(df['Alcohol'], ax=ax1)\nsns.kdeplot(df['Malic_Acid'], ax=ax1)\nsns.kdeplot(df['Ash'], ax=ax1)\nsns.kdeplot(df['Ash_Alcanity'], ax=ax1)\nsns.kdeplot(df['Magnesium'], ax=ax1)\nsns.kdeplot(df['Total_Phenols'], ax=ax1)\nsns.kdeplot(df['Nonflavanoid_Phenols'], ax=ax1)\nsns.kdeplot(df['Hue'], ax=ax1)\nax2.set_title('After Standard Scaler')\nsns.kdeplot(new_df['Alcohol'], ax=ax2)\nsns.kdeplot(new_df['Malic_Acid'], ax=ax2)\nsns.kdeplot(new_df['Ash'], ax=ax2)\nsns.kdeplot(new_df['Ash_Alcanity'], ax=ax2)\nsns.kdeplot(new_df['Magnesium'], ax=ax2)\nsns.kdeplot(new_df['Total_Phenols'], ax=ax2)\nsns.kdeplot(new_df['Nonflavanoid_Phenols'], ax=ax2)\nsns.kdeplot(new_df['Hue'], ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to idetify the best # of cluster for our data using the elbow method"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_df[[\"Alcohol\",\"Malic_Acid\"]].values  \nfrom sklearn.cluster import KMeans\ndef elbow_method(epsilon, figure=False):\n    wcss = [] \n    diff = np.inf\n    i = 1\n    \n    while diff > epsilon:\n        print(\"Iteration Nº Clusters: k: {k}\".format(k=i))\n        kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300,n_init = 10, random_state = 0)\n        kmeans.fit(X)\n        \n        if diff == np.inf:\n            diff = kmeans.inertia_\n        elif kmeans.inertia_ == 0:\n            wcss.append(kmeans.inertia_)\n            break\n        else:\n            diff = (wcss[-1] - kmeans.inertia_)/wcss[-1]\n        wcss.append(kmeans.inertia_)\n        i += 1\n        \n    if figure:\n        plt.plot(range(0,len(wcss)), wcss)\n        plt.title('Elbow Method')\n        plt.xlabel('Clusters Number')\n        plt.ylabel('WCSS')\n        plt.show()\n    k = i-1\n    return wcss, k\n\n# Results Plot\nepsilon = 0.05 \nwcss, _ = elbow_method(epsilon, figure=True)    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimal K value\nepsilon = 0.33\n_, k = elbow_method(epsilon, figure=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-means\nkmeans = KMeans(n_clusters = k, init= 'k-means++', max_iter = 300, n_init =10, random_state = 0)\ny_kmeans = kmeans.fit_predict(X)\n\n### Plot clusters \nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'blue',label = 'C1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'red',label = 'C2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green',label = 'C3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan',label = 'C4')\n\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('WinesClusters')\nplt.xlabel('X1: Alcohol')\nplt.ylabel('X2: Malic_Acid')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA Model**"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv(\"../input/wine-pca/Wine.csv\")\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2.drop([\"Customer_Segment\"], axis=1)\ndf2.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df2.iloc[:, 0:len(df2.columns)-1].values\ny = df2.iloc[:, len(df2.columns)-1].values\n\n# Train/Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n### PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None) \n\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nexplained_variance = pca.explained_variance_ratio_\nprint(\"Varianza Explicada por cada PC\")\nprint(explained_variance)\nvar_exp = np.round(np.sum(explained_variance[0:5]),4)\nprint(\"Con 5 PC se explicaría el {var}% de la varianza\".format(var=var_exp*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Se entrena solo para esas 5 componentes principales\npca = PCA(n_components = 5)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(\"Varianza Explicada por cada PC\")\nprint(explained_variance)\nprint(\"Parámetros del Modelo\")\nprint(pca.components_)\n# Visualizacion de las PC\nsns.barplot(x='PC',y=\"var\",\n     data=pd.DataFrame({'var':explained_variance, 'PC':['PC1','PC2','PC3','PC4', 'PC5']}), color=\"c\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Modelo de Regresión\n# Regresion Lineal\nimport statsmodels.api as sm\nmodel = sm.OLS(y_train, X_train_pca).fit()\nmodel.summary() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RF\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100)\nmodel.fit(X_train_pca, y_train)\nprint(\"Relevancia de los parámetros\")\nprint(model.feature_importances_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando 2 PC\npca = PCA(n_components = 2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(\"Varianza Explicada por cada PC\")\nprint(explained_variance)\nprint(\"Parámetros del Modelo\")\nprint(pca.components_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(max_depth=5, random_state=0,\n n_estimators=100)\nmodel.fit(X_train_pca, y_train)\ny_pred = model.predict(X_test_pca)\nr2 = r2_score(y_test, y_pred)\nmae = mean_squared_error(y_test, y_pred)\nprint(\"r2: \", r2, \"mae: \", mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_train_pca[:,0], X_train_pca[:,1])\nplt.ylabel(\"PC1\")\nplt.xlabel(\"PC2\")\nplt.title(\"Representación Gráfica de las PC\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}