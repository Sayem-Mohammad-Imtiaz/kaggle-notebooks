{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing My Work Environemnet \n# 1. Import Data Analysis & Visulaization Tools \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline \n# 2. Import Machine Learning Models \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report\n# 3. Import Other Assisting Libraries \nimport math        #For mathematical operations \nimport time        #For time calculations \nimport joblib      #For Saving & Loading My final model \nimport warnings    #For Dealing with system warnings \nimport os          #For Working with operating system  \n# 4. Import Work DataSets \ndf_train = pd.read_csv('../input/imbalanced-data-practice/aug_train.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info() # Get more information about the data ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Analysis & Prediction Strategy \n1. Clean the data :      \n    a. Ensure that there are no null values (Checked)    \n    b. Check for duplicate values    \n    c. Turn all categorical data into numerical \n2. Choose the important features for prediction and eliminate all useless ones. \n3. Applying Machine Learning Models to the training data and choose the best estimator. \n4. Evaluating & Improving the best estimator predictions (Tuning Hyperparameters). \n5. Saving the model.\n6. Applying all the above on test set to make predictions. \n7. Send to \"kaggle.com\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.b Check for duplicate values \nif len(df_train) == len(df_train['id'].unique()):\n    print ('There are no duplicated values .. You may proceed')\nelse : \n    print ('There\\'s Duplication!! Please Check!!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.c Turn all categorical values into numerical \ndf = df_train.copy()\nfor label, content in df.items(): \n    if pd.api.types.is_string_dtype(content): \n        df[label] = pd.Categorical(content.astype('category')).codes \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Choosing the important features \n# The most clear way to view importance of features is throw visulaization of correlation \nfig, ax = plt.subplots(figsize=(20,10))\nax = sns.heatmap(df.corr(),cmap = 'YlGnBu', annot = True, fmt = '.2f', linewidths=1)\nfig.savefig('Correlation Heatmap.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way than the heatmap for calculating feature importance is through bar graph \ny = df.corr()['Response']\nx = df.columns\nfig, ax = plt.subplots(figsize=(20,10))\nax = plt.barh(x,y)\nfig.savefig('Correlation (Bars).png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_feats = ['id', 'Gender', 'Age','Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium',\n       'Policy_Sales_Channel']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply Machine learning algorithms & Choosing best estimator \nnp.random.seed(30)\nx, y = df[imp_feats], df['Response']\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.1)\nmodels = {'Log_Regression' : LogisticRegression(),\n         'KNN' : KNeighborsClassifier(),\n          'DecisionTree' : DecisionTreeClassifier(criterion= 'gini', max_depth= 8, max_features= 'log2', min_samples_leaf= 2, min_samples_split= 4)}\ndef fit_and_score (x_train, x_val, y_train, y_val, models): \n    for label, model in models.items() :\n        start = time.time()\n        print (f'{label} Model has started training .....')\n        model.fit(x_train, y_train)\n        y_preds = model.predict(x_val)\n        print (f'{label} Model has finalized training in {(time.time()-start):.2f} seconds with roc auc score of {(roc_auc_score(y_val, y_preds)):.2f}')\n        print (f'Also Here is the classification Report !!')\n        print (classification_report(y_val, y_preds))\n        print (f'<----------------------------------------------->')\nfit_and_score(x_train, x_val, y_train, y_val, models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My Main choice would be the decision tree .. for the following : \n- It gives better recall in positive predictions meaning less amount of people predicted false audience while they are actual customers (Better roc_auc_score)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# Tuning HyperParameters \nnp.random.seed(30)\nstart = time.time()\ngs_params = {'criterion' : ['gini','entropy'],\n            'max_depth' : np.arange(6,15), \n            'min_samples_split' : np.arange(2,10),\n            'min_samples_leaf' : np.arange(2,10),\n            'max_features' : ['auto', 'sqrt', 'log2']}\ndtc = DecisionTreeClassifier()\ngs_dtc = GridSearchCV(dtc, param_grid=gs_params, cv=10, scoring = 'recall', verbose= 10 )\ngs_dtc.fit (x, y)\nfilename = 'finalized_tree_model.pkl'\njoblib.dump(gs_dtc.best_estimator_, filename)\nprint ('Tree Model has been Saved to my directory...')\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}