{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Risk \n\n## Objective:\n\nThe objective is to build machine learning models based on given dataset to predict whether a particular customer will repay the loan or not. \n\n### Applicability:\n* Target Label is known before Supervised Learning Models could build upon dataset.\n* Target classes are discrete so any Classifier model can be built\n* Here, we considered traditional Classifer models- Logistic Regression, Decision Tree Classifer, RandomForestClassifier\n    \n \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import necessary modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit= pd.read_csv('/kaggle/input/credit-risk/original.csv')\ncredit.head()        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Understanding the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(credit.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"age cannot be negative. some of the values in age column is negative. we need to make this negative values to either '0' or 'NaN'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets replace this negative values with nan values\ncredit.loc[~(credit['age'] > 0), 'age']=np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_vals= {\n    k: credit[k].unique()\n    for k in credit.columns\n    \n}\n\nunique_vals","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Clientid has unique values for each observations. keeping it for modeling will make our model complex. we will ignore this column\n2. income, age, loan looks like numerical columns\n3. default have only two values. The objective is to create model if the customer is default or not. This column is our output variable. Since we know the Target value and target variable is discrete. We need to build a Supervised Learning model\n4. Convert the default column to category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop clientid from dataset\ncredit= credit.drop('clientid', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6 missing values in 2000 records is roughly 1.2% of total records. we will drop null values\ncredit= credit.dropna()\ncredit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit['default']= credit['default'].astype('category')\ncredit['age']=credit['age'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploratory Data Analytics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"income and loan have high variance between them compared to age. we need to scale them before we fit our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.var()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as stated above, variance is high in income and loan.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"credit['default'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.groupby('default').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"youngsters will repay their loan soon. \nbecause of their repayment power, they are offered with higher loans","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So, age is important feature for outcome variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.groupby('age').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visual EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\ncredit.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3)= plt.subplots(1,3)\ncredit['age'].plot(kind='box', ax=ax1, figsize=(12,6))\ncredit['income'].plot(kind='box', ax=ax2, figsize=(12,6))\ncredit['loan'].plot(kind='box', ax=ax3, figsize=(12,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like there are outliers present in loan","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.barplot(y='age', x='default', data=credit)\nplt.xlabel('Defaults')\nplt.ylabel('age of defaulters')\nplt.title('Average age of defaulters on Loan', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of all people defaulted to loan, most of them are above age 40. Because of their extended lifetime, People of age around 20-30 are very keen on repaying loan","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=credit, hue='default',diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We couldnt find any pairwise relationships between features. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the mean and standard dev\nstd = credit['loan'].std()\nmean = credit['loan'].mean()\n# Calculate the cutoff\ncut_off = std * 3\nlower, upper = mean - cut_off, mean + cut_off\n# Trim the outliers\ntrimmed_df = credit[(credit['loan'] < upper) \\\n                           & (credit['loan'] > lower)]\ntrimmed_df.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# The trimmed box plot\ntrimmed_df[['loan']].boxplot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no statistically differnce in removing outlier value from dataframe","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will use all the features of original dataframe to build our base model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the independent and outcome variable\nX= credit.iloc[:,0:3]\ny=credit.iloc[:,3]\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target labels have uneven distribution; test and training sets might not be representative samples of our data and could bias the model we are trying to train. We will use stratified Sampling to split up the dataset according to the y dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets split to training and test set for training the model and validating the model\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test= train_test_split(X, y, random_state=9999, stratify=y)\n#stratify is used since the target class distribution is imbalanced\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets perform scaling. all our features are numerical columns\n#it is important that we need to have our features to be in same scale.\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc= StandardScaler()\nX_train= sc.fit_transform(X_train)\nX_test=sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building- Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n#instantiate LogisticRegression model\nlogreg= LogisticRegression(solver='lbfgs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#perform cross validation to ensure the model is good model\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores= cross_val_score(logreg, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on CV score, We have our benchmark accuracy score for our Logistic Regression model. if our test set accuracy is between 90-96, we can safely assume that our model is best model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the linear regression model to training data\nlogreg.fit(X_train, y_train)\n\n# Predict the test set\ny_pred = logreg.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Validation- Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ncm = confusion_matrix(y_test,y_pred)\nacc_score = accuracy_score(y_test, y_pred)\n\nprint(f\"Accuracy = {acc_score*100:.2f}%\")\nprint(f\"Confusion matrix = \\n{cm}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Training and Test Set Accuracy\n\ntraining_accuracy= logreg.score(X_train, y_train)\ntest_accuracy= logreg.score(X_test, y_test)\n\nprint(f\"Training Set accuracy = {training_accuracy*100:.2f}%\")\nprint(f\"Test Set accuracy = {test_accuracy*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training and Test Set accuracy are high and they are almost same. Thus, there is no chance of Overfitting and Underfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complete classification report\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coefficients of the model and its intercept\nprint(dict(zip(X.columns, abs(logreg.coef_[0]).round(2))))\nprint(logreg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of feature coefficients are close to zero. So, There is no need to drop any of these features.\n\nHowever, we can perform RFE to understand if the accuracy is improved by dropping any features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score\n\n# Create the RFE with a LogisticRegression estimator and 2 features to select\nrfe = RFE(estimator=logreg, n_features_to_select=2, verbose=1)\n# Fits the eliminator to the data\nrfe.fit(X_train, y_train)\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X.columns, rfe.ranking_)))\n# Print the features that are not eliminated\nprint(X.columns[rfe.support_])\n# Calculates the test set accuracy\nacc = accuracy_score(y_test, rfe.predict(X_test))\nprint(\"{0:.1%} accuracy on test set.\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping features doesnot improve our model accuracy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model Evaluation:\n\nEvaluate model performance by plotting an ROC curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n#compute predicted probabilities: y_pred_prob\ny_pred_prob= logreg.predict_proba(X_test)[:,1]\n\n#Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Calculate the AUC\n\nroc_auc = auc(fpr, tpr)\nprint ('ROC AUC: %0.3f' % roc_auc )\n\n#Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC for both the test and train samples when run on my logistic regression demonstrates relatively strong power of separation between positive and negative occurences (repay - 1, default - 0)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Comparing with other ML models\n\nWe will build our data with other Classifier models and compare which model best fit to dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Model Building- RandomForestClassifier","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#instantiate RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc= RandomForestClassifier(n_estimators=10, max_depth=3)\n\n#Fit the RandomForest model to training data\nrfc.fit(X_train, y_train)\n\n# Predict the test set\ny_pred_rfc = rfc.predict(X_test)\ny_pred_rfc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix\ncm_rfc = confusion_matrix(y_test,y_pred_rfc)\nacc_score_rfc = accuracy_score(y_test, y_pred_rfc)\n\nprint(f\"Accuracy = {acc_score_rfc*100:.2f}%\")\nprint(f\"Confusion matrix = \\n{cm_rfc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Training and Test Set Accuracy\n\ntraining_accuracy_rfc= rfc.score(X_train, y_train)\ntest_accuracy_rfc= rfc.score(X_test, y_test)\n\nprint(f\"Training Set accuracy = {training_accuracy_rfc*100:.2f}%\")\nprint(f\"Test Set accuracy = {test_accuracy_rfc*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model underfits on test set. we will perform hyperparameter tuning to get best params to fit our model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Hyperparameter Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\n\n\n#lets get hyperparameters defined in our model\nrfc.get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 18 hyperparameters present in rfc model. We can use most important hyperparameter which affects model accuracy.\nWe will tune four parameters out of 18 parameters\n        * max_depth\n        * max_leaf_nodes\n        * min_samples_split\n        * n_estimators","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid= {\"max_depth\": [2, 4, 6, 8, 10],\n            \"max_leaf_nodes\": [2, 4, 6],\n            \"min_samples_split\":[2, 4, 6, 8],\n            \"n_estimators\": [10, 50, 100, 150]}\n\n#create scoring parameter as accuracy_score. There are some default scoring methods defined. however if we want to create we can create using make_Scorer\n#Here i am using Accuracy score as scorring method. we can also use recall_score etc\nscorer= make_scorer(accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rcv =RandomizedSearchCV(estimator=rfc,param_distributions=param_grid,n_iter=10,cv=5,scoring=scorer)\nrcv.fit(X, y)\n\n# print the mean test scores:\nprint('The accuracy for each run was: {}.'.format(rcv.cv_results_['mean_test_score']))\n# print the best model score:\nprint('The best accuracy for a single model was: {}'.format(rcv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the best params and reinstantiate RandomForestClassifier model\nmodel=RandomForestClassifier(n_estimators= 50, min_samples_split= 2, max_leaf_nodes= 6, max_depth= 10)\n\n#fit the training set to model\nmodel.fit(X_train, y_train)\n\n# Making the confusion matrix\ncm_rfc2 = confusion_matrix(y_test,model.predict(X_test))\nacc_score_rfc2 = accuracy_score(y_test, model.predict(X_test))\n\nprint(f\"Accuracy = {acc_score_rfc2*100:.2f}%\")\nprint(f\"Confusion matrix = \\n{cm_rfc2}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Training and Test Set Accuracy\n\ntraining_accuracy_rfc2= model.score(X_train, y_train)\ntest_accuracy_rfc2= model.score(X_test, y_test)\n\nprint(f\"Training Set accuracy = {training_accuracy_rfc2*100:.2f}%\")\nprint(f\"Test Set accuracy = {test_accuracy_rfc2*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though the Test set accuracy is lower than base model, Training set accuracy subsequently increases with hyper parameter tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model Estimation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n#compute predicted probabilities: y_pred_prob\ny_pred_prob_rfc= model.predict_proba(X_test)[:,1]\n\n#Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_rfc)\n\n# Calculate the AUC\n\nroc_auc = auc(fpr, tpr)\nprint ('ROC AUC: %0.3f' % roc_auc )\n\n#Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of RandomForest Model')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building- Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt= DecisionTreeClassifier(max_depth=4, random_state=9999)\n\ndt.fit(X_train, y_train)\n\n#fit the training set to model\ndt.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix\ncm_dt = confusion_matrix(y_test,dt.predict(X_test))\nacc_score_dt = accuracy_score(y_test, dt.predict(X_test))\n\nprint(f\"Accuracy = {acc_score_dt*100:.2f}%\")\nprint(f\"Confusion matrix = \\n{cm_dt}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Training and Test Set Accuracy\n\ntraining_accuracy_dt= dt.score(X_train, y_train)\ntest_accuracy_dt= dt.score(X_test, y_test)\n\nprint(f\"Training Set accuracy = {training_accuracy_dt*100:.2f}%\")\nprint(f\"Test Set accuracy = {test_accuracy_dt*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complete classification report\nprint(classification_report(y_test,dt.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Estimation","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#compute predicted probabilities: y_pred_prob\ny_pred_prob_dt= dt.predict_proba(X_test)[:,1]\n\n#Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_dt)\n\n# Calculate the AUC\n\nroc_auc = auc(fpr, tpr)\nprint ('ROC AUC: %0.3f' % roc_auc )\n\n#Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of Decision Tree Model')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets draw decision tree\n\nfrom sklearn import tree\n\ndecision_tree= tree.export_graphviz(dt, out_file='tree.dot', feature_names=credit.iloc[:, :3].columns, \n                                    max_depth=4, filled=True, rounded=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!dot -Tpng tree.dot -o tree.png","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image= plt.imread('tree.png')\nplt.figure(figsize=(20, 20))\nplt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion: \n    By all means, Decision Tree performs better than LogisticRegression and RandomForest Classifier models","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}