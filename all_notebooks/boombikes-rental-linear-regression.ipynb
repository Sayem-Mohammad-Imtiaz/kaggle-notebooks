{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Supress Warnings and import all the relevant packages and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary packages and libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 50)\n\nfrom sklearn.model_selection import train_test_split\n\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading and preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\n\nbike = pd.read_csv('/kaggle/input/boombikes/day.csv')\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of the dataframe\n\nbike.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the info for all the columns\n\nbike.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Comments:-\nIt is observed from the above, that the entire dataset does not contain any missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the unnecessary columns\n\nbike = bike.drop(['instant','dteday','casual','registered','atemp'], axis=1)\nbike.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the columns for better understanding\n\nbike.rename(columns = {'yr':'year','mnth':'month','hum':'humidity','cnt':'count'}, inplace = True) \nbike.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identifying the categorical and continuous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"bike.nunique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Categorical Variables - year, holiday, workingday, weathersit, season, weekday, month\n\nThe rest of the variables are continuous in nature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping variables season, month, weekday, weathersit\n\nbike['season'] = bike.season.map({1: 'spring', 2: 'summer',3:'fall', 4:'winter'})\n\nbike['month'] = bike.month.map({1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'June', 7:'July', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'})\n\nbike['weekday']  =bike.weekday.map({0:'Sun', 1:'Mon', 2:'Tue', 3:'Wed', 4:'Thu',  5:'Fri', 6:'Sat'})\n\nbike['weathersit'] = bike.weathersit.map({1: 'Clear', 2:'Cloudy', 3:'Light Snow', 4:'Heavy Rain'})\n\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising the numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot of all the numeric variables\n\nsns.pairplot(bike, vars=['count', 'temp', 'humidity', 'windspeed'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nThe count of total rental bikes, is more correlated to temperature."},{"metadata":{},"cell_type":"markdown","source":"### Visualising the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot for some of the categorical variables with respect to the target varibale 'count'\n\nplt.figure(figsize=(20, 12))\nplt.subplot(2,4,1)\nsns.boxplot(x = 'year', y = 'count', data = bike)\nplt.subplot(2,4,2)\nsns.boxplot(x = 'holiday', y = 'count', data = bike)\nplt.subplot(2,4,3)\nsns.boxplot(x = 'workingday', y = 'count', data = bike)\nplt.subplot(2,4,4)\nsns.boxplot(x = 'weathersit', y = 'count', data = bike)\nplt.subplot(2,4,5)\nsns.boxplot(x = 'season', y = 'count', data = bike)\nplt.subplot(2,4,6)\nsns.boxplot(x = 'weekday', y = 'count', data = bike)\nplt.subplot(2,4,7)\nsns.boxplot(x = 'month', y = 'count', data = bike)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observations:-\n\n1) Bike Rentals are more in the year 2019, compared to 2018.\n2) Bike Rentals are more on the clear weather days.\n3) Bike Rentals are the most during the Fall season.\n4) Bike Rentals are the most on Saturdays.\n5) Bike Rentals are the most in the month of September."},{"metadata":{},"cell_type":"markdown","source":"### Analysis between the target variable - count, and the other variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis between weathersit and count\n\nplt.figure(figsize=(10,4))\nsns.barplot('weathersit','count',data=bike)\nplt.title('Bike Rentals in different Weather Situations',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nBike Rentals are the most during the Clear weather, and the least during the Cloudy weather.    "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis between season and count\n\nplt.figure(figsize=(10,4))\nsns.barplot('season','count',data=bike)\nplt.title('Bike Rentals in different Seasons',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nBike Rentals are the most during the Fall season, and the least during the Spring season."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of the Bike Rentals for each month of both the years\n\nplt.figure(figsize=(10,5))\nsns.barplot('month','count',hue='year',data=bike)\nplt.title('Bike Rentals in different Months of both the Years',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\n\n1) Bike Rentals are the most in the month of June, for 2018, and September, for 2019.\n2) Bike Rentals are the least in the month of January, for both the years."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of Bike Rentals with Temperature\n\nsns.scatterplot(x='temp',y='count' ,data=bike)\nplt.title('Temp vs Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nHigher the temperature, bike rentals are higher too."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of Bike Rentals with Humidity\n\nsns.scatterplot(x='humidity',y='count' ,data=bike)\nplt.title('Humidity vs Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\nHigher the humidity, bike rentals are higher too."},{"metadata":{},"cell_type":"markdown","source":"### Correlation between the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap to visualise the correlation between the variables\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(bike.corr(), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Correlation between the variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observation:-\n'Temperature' and 'year' are most correlated with the target variable 'count'."},{"metadata":{},"cell_type":"markdown","source":"Both the pairplot and heatmap, above, help to interpret the data well and identify the variables that can turn out to be useful in building the model. Hence, Linear Regression Model can be used."},{"metadata":{},"cell_type":"markdown","source":"### Creating Dummy Variables"},{"metadata":{},"cell_type":"markdown","source":"Creating dummy variables for - month, season, weathersit and weekday"},{"metadata":{"trusted":true},"cell_type":"code","source":"month_dummy = pd.get_dummies(bike.month,drop_first=True)\nweekday_dummy = pd.get_dummies(bike.weekday,drop_first=True)\nweathersit_dummy = pd.get_dummies(bike.weathersit,drop_first=True)\nseason_dummy = pd.get_dummies(bike.season,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the dummy variables to the original dataframe\n\nbike = pd.concat([month_dummy,weekday_dummy,weathersit_dummy,season_dummy,bike],axis=1)\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the original columns - month, weekday, weathersit and season, since dummy variables have already been created for them\n\nbike.drop(['season','month','weekday','weathersit'], axis = 1, inplace = True)\nbike.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Data into Training and Testing Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\n\nbike_train, bike_test = train_test_split(bike, train_size = 0.7, random_state = 100)\n\nprint(bike_train.shape)\nprint(bike_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the Features using MinMax Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list of numeric variables\n\nnum_vars=['temp','humidity','windspeed','count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the data\n\nbike_train[num_vars] = scaler.fit_transform(bike_train[num_vars])\nbike_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking numeric variables after scaling the features\nbike_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dividing into X and y sets for the model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = bike_train.pop('count')\nX_train = bike_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building our model\n\nWe will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE - Recursive Feature Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing RFE and LinearRegression\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 15\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# Running RFE\nrfe = RFE(lm, 15)             \nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns for which rfe_support is true\n\ncol = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model using statsmodel, for the detailed statistics"},{"metadata":{},"cell_type":"markdown","source":"### Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the first dataframe model with RFE selected variables\nX_train_1 = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \n\nimport statsmodels.api as sm  \nX_train_1 = sm.add_constant(X_train_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_1).fit() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of our linear model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_1 = X_train_1.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be following the below rule to drop the variables one by one, as per the priorities mentioned by their sequences:-\n\n* We will first check the summary and VIF\n* If a variable has got high p-value(>0.05) as well as high VIF(>5), we need to drop that first\n* If a variable has got high p-value(>0.05) but low VIF(<5), then we need to drop such\n* Still if we have a variable with low p-value(<0.05) but high VIF(>5), we need to drop such at the very end"},{"metadata":{},"cell_type":"raw","source":"In Model 1, 'Sat' has got high p-value but low VIF, but 'humidity' has got low p-value but high VIF.\n\nSince, 'Sat' is insignificant with respect to the other variables, hence this is dropped, and not 'humidity'."},{"metadata":{},"cell_type":"markdown","source":"### Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Sat'\n\nX_train_2 = X_train_1.drop(['Sat'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_2 = sm.add_constant(X_train_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_2).fit() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_2 = X_train_2.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 2, none of the variables are insignificant. But, 'humidity' has got very high VIF. Hence, this should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"### Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'humidity'\n\nX_train_3 = X_train_2.drop(['humidity'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_3 = sm.add_constant(X_train_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_3).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_3 = X_train_3.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 3, none of the variables are insignificant, but 'temp' has the highest VIF value. But, we cannot remove 'temp' as this highly correlated with our target variable - 'count', as obtained from our previous observations. Hence, we will rebuild a model by removing 'windspeed', the variable with the next highest VIF."},{"metadata":{},"cell_type":"markdown","source":"### Model 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'windspeed'\n\nX_train_4 = X_train_3.drop(['windspeed'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_4 = sm.add_constant(X_train_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_4).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_4 = X_train_4.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 4, 'Jan' can be dropped now since it has the highest p-value, although lesser than 0.05."},{"metadata":{},"cell_type":"markdown","source":"### Model 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Jan'\n\nX_train_5 = X_train_4.drop(['Jan'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_5 = sm.add_constant(X_train_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_5).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_5 = X_train_5.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_5\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 5, 'Dec' is the insignificant variable, hence this should be removed now."},{"metadata":{},"cell_type":"markdown","source":"### Model 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Dec'\n\nX_train_6 = X_train_5.drop(['Dec'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_6 = sm.add_constant(X_train_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_6).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the const variable\n\nX_train_6 = X_train_6.drop(['const'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_6\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"In Model 6, 'Nov' is insignificant, hence this should be dropped now."},{"metadata":{},"cell_type":"markdown","source":"### Model 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model without 'Nov'\n\nX_train_7 = X_train_6.drop(['Nov'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the contsant variable\n\nX_train_7 = sm.add_constant(X_train_7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_7).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the new model\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_7\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif = vif[vif['Features']!='const']    # Ignoring to display the vif of 'const'\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Model 7 seems to be the best model achieved so far, with the p-values of all 9 dependent variables are 0, and the VIF values are less than 4.\nHence, this model can be finalised, for making the predictions."},{"metadata":{},"cell_type":"markdown","source":"### Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_7).fit()  #As obtained previously\ny_train_count = lm.predict(X_train_7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nsns.distplot((y_train - y_train_count), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"It is observed that the error terms are normally distributed."},{"metadata":{},"cell_type":"markdown","source":"### Applying the scaling on the test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars=['temp','humidity','windspeed','count']\n\n# Fit and transform operations are done on the training data but only transform operation will be done on the test data\n\nbike_test[num_vars] = scaler.transform(bike_test[num_vars])\nbike_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dividing into X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = bike_test.pop('count')\nX_test = bike_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding constant variable to test dataframe\n\nX_test_m7 = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test_m7 dataframe by dropping variables which were removed till our final Model 7 in the training dataset\n\nX_test_m7 = X_test_m7.drop(['Aug','Feb','June','Mar','May','Oct','Mon','Sun','Thu','Tue','Wed','summer','workingday','Sat','humidity','windspeed','Jan','Dec','Nov'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions using the seventh model\n\ny_pred_m7 = lm.predict(X_test_m7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred_m7)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression plot\n\nsns.regplot(x = y_test, y = y_pred_m7, fit_reg=True,scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n\nplt.title('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculation of R-square and Adjusted R-square values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate R-square for test dataset\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_pred_m7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjusted R^2\n# adj r2 = 1-((1-R2)*(n-1)/(n-p-1))\n\n# n = sample size (in this case the value is 220, as yielded before)\n# p = number of independent variables(in this case the value is 9)\n\nAdj_r2 = 1 - ((1 - 0.8065842474886509) * 219 / (220-9-1))\nprint(Adj_r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the training dataset, the R^2 value was 0.822 and adjusted R^2 value was 0.819.\n\nFor the testing dataset, the R^2 value obtained is 0.809 and adjusted R^2 value obtained is 0.798.\n\n\n\n\nHence the equation of our best fitted line is:-\n\n$ count = 0.0654 \\times Sep + 0.0534 \\times winter + 0.2332 \\times year + 0.4695 \\times temp - 0.0690 \\times July - 0.0781 \\times Cloudy - 0.2993 \\times Light Snow - 0.1122 \\times spring - 0.1006 \\times holiday $\n\n\nOverall we have a decent model, but we also acknowledge that we could do better. "},{"metadata":{},"cell_type":"markdown","source":"### Interpretations\n\n* We have arrived at a very decent model for the the demand for shared bikes with the significant variables.\n\n* We can see that temperature variable is having the highest coefficient of 0.4695, which means if the temperature increases by one unit, the number of bike rentals increases by 0.4695 units.\n\n* The other significant variables having positive coefficients are September and Winter.\n\n* There are some variables with negative coefficients too, like July, Cloudy, Light Snow, Spring and Holiday. A negative coefficient suggests that, as the independent variable increases, the dependent variable tends to decrease,and vice-versa."},{"metadata":{},"cell_type":"markdown","source":"### Business Goals\n\n* The demand for bikes is observed to be more in 2019, than its previous year 2018. The recent dip in business may be due to the ongoing pandemic situation.\n\n* The demand for bike rentals are found to be the least during the Spring season. Hence, special discounts may be considered during this season to get back the demand.\n\n* The demand for bike rentals are mostly on Saturdays, but very less on the Sundays, probably due to the weekend holiday.\n\n* January and February are the months, in which demands have been observed to be at the bottom, whereas September and its adjacent months show the highest demands. This is probably due to the vacation time during those two impacted months.\n\n* The rental demands are the most during the clear days. But sharp fall in the demand have been observed during the adverse weather conditions during snowing, raining and thunderstorms.\n\n* Temparature plays a cruical role in the bike rental demand. The demand seems to be pretty decent on a day with a medium to high temperature. Lower the temperature, lower the demand.\n\n* The company needs to provide special discounts to attract more rental demands during the Spring season, or during the months of January and February, on Sundays, or on a particular day when it is snowing or raining."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}