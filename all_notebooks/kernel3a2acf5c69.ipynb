{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Customer Churn"},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\nThe dataset consists of a telecom dataset with 7043 observations and 19 features. The goal of this analysis is to create a model that predicts customer churn while maximizing sensitivity, f1, and AUC. Sensitivity is the primary measurement to target as many accurate churns as possible. False positives are acceptable, but would preferably be minimized. With appropriate profit data these measurements may change."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# model prep\nfrom sklearn import cluster\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n!pip install prince\nfrom prince import PCA\nfrom statsmodels.regression import linear_model\n\n# models\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# model evaluation\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import cross_val_predict\n\n# visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n%matplotlib inline\nstyle.use('seaborn-white')\nimport seaborn as sns\nsns.set_style('white')\n\n!pip install jupyterthemes\nfrom jupyterthemes import jtplot\njtplot.style(theme = 'onedork', grid = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration & Prep\n\nThe TotalCharges feature has some missing values. These observations also have a tenure of 0, indicating new customers who may not have accumulated total charges yet. Therefore, I replaced the missing observations with 0.\n\nThe customerID field is not appropriate for creating a model since it's a unique identifier and may lead to overfitting.\n\nNone of the numerical features are normally distributed, which isn't a problem since none of the classification methods we're using require normality.\n\nThe numerical variables also do not have any outliers that need to be addressed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While there are no null values in the dataset, as seen above, there are 11 blank values in **TotalCharges** whenever **tenure** is 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Blanks in TotalCharges')\nprint(df.TotalCharges.str.isspace().value_counts())\n\nprint('\\nTenure where TotalCharges is blank')\nprint(df[df.TotalCharges.str.isspace()][['tenure', 'TotalCharges']])\n\n# clean missing values\n# when TotalCharges is blank, the tenure is 0\ndf.TotalCharges = df.TotalCharges.replace(' ', '0')\n\n# TotalCharges is read initially as a string because of the blank values. This field should be a float.\ndf.TotalCharges = pd.to_numeric(df.TotalCharges, downcast='float')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **customerID** field is a unique value that could lead to overfitting if included in the analysis. I've also removed **gender** from the analysis to avoid gender bias."},{"metadata":{"trusted":true},"cell_type":"code","source":"# customerID is a unique identifier that provides no predictive value\ndf = df.drop(['customerID', 'gender'], axis = 1)\n\ncategorical_columns = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', \\\n                      'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \\\n                      'Contract', 'PaperlessBilling', 'PaymentMethod']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace({'No phone service': 'No'}, regex=True)\ndf.head()\ndf.to_excel(r'kaggle\\working\\clean_data.xlsx', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 quantitative features, none of which are normally distributed. **MonthlyCharges** and **tenure** are both bimodal, while **TotalCharges** is skewed to the right."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots_adjust(left=1, bottom=1, right=3, top=2, wspace=.3, hspace=None)\nplt.subplot(1,3,1)\nsns.distplot(df['MonthlyCharges'], bins = 7)\nplt.subplot(1,3,2)\nsns.distplot(df['TotalCharges'], bins = 7)\nplt.subplot(1,3,3)\nsns.distplot(df['tenure'], bins = 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots_adjust(left=1, bottom=1, right=3, top=2, wspace=.3, hspace=None)\nplt.subplot(1, 3, 1)\nsns.boxplot(y=df['MonthlyCharges'])\nplt.subplot(1, 3, 2)\nsns.boxplot(y=df['TotalCharges'])\nplt.subplot(1, 3, 3)\nsns.boxplot(y=df['tenure'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\n#### Collinearity\n**TotalCharges** has a linear relationship to both **tenure** and **MonthlyCharges**, which makes sense since someone with high tenure would be charged more over time and a customer has high monthly charges, that will increase the total charges."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[['MonthlyCharges', 'TotalCharges', 'tenure']], diag_kind = 'kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df[['MonthlyCharges', 'TotalCharges', 'tenure']].corr().round(2)\nsns.heatmap(corr,fmt='', annot=True, cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove TotalCharges since it's highly correlated with tenure and MonthlyCharges\ndf = df.drop('TotalCharges', 1)\ncorr = df[['MonthlyCharges', 'tenure']].corr().round(2)\nsns.heatmap(corr,fmt='', annot=True, cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare the features for inital analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode categorical features\nle = LabelEncoder()\nX = df.drop('Churn', 1).apply(le.fit_transform)\ny = df.Churn\n\n# prepare target variable 'Churn' for decision tree model\ny.replace('Yes', 1, inplace = True)\ny.replace('No', 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the most important categorical features are **Contract**, **OnlineSecurity**, **PaymentMethod**, and **TechSupport**. OnlineSecurity and TechSupport both have a level for No internet service, which could be combined with the No level."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nclf = RandomForestClassifier(class_weight='balanced', \n                           n_estimators=100).fit(X_train,y_train)\n\nfeature_imp = pd.DataFrame(clf.feature_importances_, index=X.columns)\nfeature_imp = feature_imp[0].sort_values(ascending=False)\n\nsns.barplot(x=feature_imp[:20], y=feature_imp[:20].index, color = '#3274A1')\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Important Features - Random Forest\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2)\nplt.subplots_adjust(left=1, bottom=1, right=3, top=2, wspace=.5, hspace=.5)\ndf.Contract.value_counts().plot(kind = 'barh', ax = axes[0,0], title = 'Contract')\ndf.OnlineSecurity.value_counts().plot(kind = 'barh', ax = axes[0,1], title = 'OnlineSecurity')\ndf.PaymentMethod.value_counts().plot(kind = 'barh', ax = axes[1,0], title = 'PaymentMethod')\ndf.TechSupport.value_counts().plot(kind = 'barh', ax = axes[1,1], title = 'TechSupport')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is a little imbalanced with 27% of the target (**Churn**) being positive. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(100. * df.Churn.value_counts() / len(df.Churn)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of features can be reduced by identifying feature clusters. In the 5 cluster set, the 2nd cluster of features were not very important according to the random forest modeled above. Therefore, I'll consolidate the features to 4 clusters and use **Contract** as the most important feature in cluster 1."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def cluster_features(n):\n    agglo = cluster.FeatureAgglomeration(n_clusters = n)\n    agglo.fit(X)\n    for i, label in enumerate(set(agglo.labels_)):\n        features_with_label = [j for j, lab in enumerate(agglo.labels_) if lab == label]\n        clustered_features = []\n        for feature in features_with_label:\n            clustered_features.append(X.columns[feature])\n        print('Cluster {}: {}'.format(i + 1, clustered_features))\n\nprint('5 Clusters')\ncluster_features(5)\nprint('\\n4 Clusters')\ncluster_features(4)\n\nclustered_features = ['Contract', 'MonthlyCharges', 'tenure', 'PaymentMethod']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interaction Features\n153 interaction terms are identified"},{"metadata":{"trusted":true},"cell_type":"code","source":"#generating interaction terms\nx_interaction = PolynomialFeatures(2, interaction_only=True, include_bias=False).fit(X)\ninteraction_df = pd.DataFrame(x_interaction.transform(X), columns = x_interaction.get_feature_names(X.columns))\ninteraction_model = linear_model.OLS(y, interaction_df).fit()\n\nX = interaction_df\nlen(X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance\nAfter creating interaction features, 25 features are significant at the .05 level, while 19 are significant at the .01 level."},{"metadata":{"trusted":true},"cell_type":"code","source":"interaction_pvalues_05 = interaction_model.pvalues[interaction_model.pvalues < 0.05].sort_values(ascending=True)\ninteraction_pvalues_01 = interaction_model.pvalues[interaction_model.pvalues < 0.01].sort_values(ascending=True)\ninteraction_features_05 = interaction_pvalues_05.index[:]\ninteraction_features_01 = interaction_pvalues_01.index[:]\nprint(len(interaction_features_05))\nprint(len(interaction_features_01))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nclf = RandomForestClassifier(class_weight='balanced', \n                           n_estimators=100).fit(X_train,y_train)\n\nfeature_imp = pd.DataFrame(clf.feature_importances_, index=X.columns)\nfeature_imp = feature_imp[0].sort_values(ascending=False)\n\nsns.barplot(x=feature_imp[:20], y=feature_imp[:20].index, color = '#3274A1')\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Important Features - Random Forest\")\nplt.legend()\nplt.show()\n\nsns.barplot(x=interaction_pvalues_05, y=interaction_features_05, color = '#3274A1')\nplt.xlabel('P-Values')\nplt.ylabel('Features')\nplt.title(\"Important Features - Polinomial Features\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# improve performance by limiting to the most important features\nX_regduced = X[interaction_features_05] \n\n# Dummy variables for logistic regression\nX_dummy = pd.get_dummies(X, drop_first = True)\nX_dummy_reduced = pd.get_dummies(X[interaction_features_05], drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X)\nscaled_X = scaler.transform(X)\n\npca = PCA(n_components=2)\npca_X = pca.fit(scaled_X).transform(scaled_X)\n\nplt.figure()\nplt.figure(figsize=(8,8))\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal Component - 1',fontsize=20)\nplt.ylabel('Principal Component - 2',fontsize=20)\nplt.title(\"Principal Component Analysis of Customer Churn\",fontsize=20)\ntargets = [0, 1]\nfor target in targets:\n    indicesToKeep = df['Churn'] == target\n    plt.scatter(pca_X.loc[indicesToKeep, 0]\n               , pca_X.loc[indicesToKeep, 1], cmap = 'coolwarm', s = 10)\n\n\nplt.legend(['No Churn', 'Churn'],prop={'size': 15})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_confusion_matrix(y_test, y_pred, predicted_proba, title):\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = ['{0:0.0f}'.format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in\n                         cf_matrix.flatten()/np.sum(cf_matrix)]\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    \n    plt.title(title +  \n              '\\nRecall ' + str(round(recall_score(y_test, y_pred), 2)) +\n              '\\nPrecision ' + str(round(precision_score(y_test, y_pred), 2)) + \n              '\\nAUC ' + str(round(roc_auc_score(y_test, predicted_proba[:,1]), 2)) +\n              '\\nF1 ' + str(round(f1_score(y_test, y_pred), 2)) +\n              '\\nAccuracy ' + str(round(accuracy_score(y_test, y_pred), 2)))\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(class_weight='balanced', probability=True)\n\nplt.subplots_adjust(left=1, bottom=.5, right=4, top=1, wspace=.5, hspace=None)\n\nplt.subplot(1, 4, 1)\npredicted_proba = cross_val_predict(svm, pca_X, y, cv=5, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'PCA')\n\nplt.subplot(1, 4, 2)\npredicted_proba = cross_val_predict(svm, X[interaction_features_05], y, cv=5, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Interaction Features p<.05')\n\nplt.subplot(1, 4, 3)\npredicted_proba = cross_val_predict(svm, X[interaction_features_01], y, cv=5, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Interaction Features p<.01')\n\nplt.subplot(1, 4, 4)\npredicted_proba = cross_val_predict(svm, X[clustered_features], y, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Important Features Clustered')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(class_weight='balanced', \n                            max_iter=1000)\n\nplt.subplots_adjust(left=1, bottom=.5, right=4, top=1, wspace=.5, hspace=None)\n\nplt.subplot(1, 4, 1)\npredicted_proba = cross_val_predict(lr, pca_X, y, cv=5, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'PCA')\n\nplt.subplot(1, 4, 2)\npredicted_proba = cross_val_predict(lr, X[interaction_features_05], y, cv=5, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Interaction Features p<.05')\n\nplt.subplot(1, 4, 3)\npredicted_proba = cross_val_predict(lr, X[interaction_features_01], y, cv=5, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Interaction Features p<.01')\n\nplt.subplot(1, 4, 4)\npredicted_proba = cross_val_predict(lr, X[clustered_features], y, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Important Features Clustered')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(class_weight='balanced', \n                               max_depth=5, \n                               n_estimators=50)\n\nplt.subplots_adjust(left=1, bottom=.5, right=4, top=1, wspace=.5, hspace=None)\n\nplt.subplot(1, 4, 1)\npredicted_proba = cross_val_predict(rf, pca_X, y, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'PCA')\n\nplt.subplot(1, 4, 2)\npredicted_proba = cross_val_predict(rf, X[interaction_features_05], y, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Interaction Features p<.05')\n\nplt.subplot(1, 4, 3)\npredicted_proba = cross_val_predict(rf, X[interaction_features_01], y, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Interaction Features p<.01')\n\nplt.subplot(1, 4, 4)\npredicted_proba = cross_val_predict(rf, X[clustered_features], y, method='predict_proba')\ny_pred = (predicted_proba[:,1] >= 0.5).astype('int')\ncreate_confusion_matrix(y, y_pred, predicted_proba, 'Important Features Clustered')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Comparison"},{"metadata":{},"cell_type":"markdown","source":"The SVM models are significantly slower and perform worse than the logistic regression and random forest models."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params = {\n    'logistic_regression': {\n        'model': LogisticRegression(class_weight = 'balanced', max_iter=1000),\n        'params': {\n            'C': range(1, 20)\n        }\n    },\n    'random_forest': {\n        'model': RandomForestClassifier(class_weight = 'balanced'),\n        'params': {\n            'max_depth': range(1, 20),\n            'n_estimators': range(1, 100)\n        }\n    }\n}\n\nscores = []\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfor model_name, mp in model_params.items():\n    clf = RandomizedSearchCV(mp['model'], mp['params'], scoring = 'roc_auc', return_train_score = False)\n    clf.fit(X[interaction_features_01], y)\n    scores.append({\n        'model': model_name,\n        'best_auc_score': clf.best_score_,\n#         'results': clf.cv_results_\n        'best_params': clf.best_params_\n    })\n    \nfor model_name, mp in model_params.items():\n    clf = RandomizedSearchCV(mp['model'], mp['params'], scoring = 'recall', return_train_score = False)\n    clf.fit(X[interaction_features_01], y)\n    scores.append({\n        'model': model_name,\n        'best_recall_score': clf.best_score_,\n#         'results': clf.cv_results_\n        'best_params': clf.best_params_\n    })","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do a sanity check and see if we can find a better model with hyperparameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nThe model that performed the best was the random forest with interaction features significant at the .01 level. This model maximized recall, while minimizing the loss of precision and had the maximum AUC and F1 scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"# chosen features\nprint(interaction_features_01)\nprint(len(interaction_features_01))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depending on the cost of marketing materials, a higher or lower recall may be desirable and can be obtained by adjusting the .05 threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots_adjust(left=1, bottom=.5, right=4, top=1, wspace=.5, hspace=None)\n\nfor i in range(3):\n    threshold = round((i / 10) + 0.4, 1)\n    plt.subplot(1, 3, i + 1)\n    predicted_proba = cross_val_predict(rf, X[interaction_features_01], y, method='predict_proba')\n    y_pred = (predicted_proba[:,1] >= threshold).astype('int')\n    create_confusion_matrix(y, y_pred, predicted_proba, str(threshold) + ' Threshold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":4}