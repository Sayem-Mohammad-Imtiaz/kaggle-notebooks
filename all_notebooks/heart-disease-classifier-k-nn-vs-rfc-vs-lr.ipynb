{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predictiong heart disease using various ML models\nThis notebook looks into using various Python-based machine learning machine learning and adata science libraries in an attempt to build a machine learning model capable of predictiong whether or notsomeone has haert disease based on their medical attributes.\n\n<Font color=\"salmon\">I hope you find this kernel helpful and some UPVOTES would be very much appreciated</Font>"},{"metadata":{},"cell_type":"markdown","source":"# Features \n* age = age in years\n* sex = (1 = male; 0 = female)\n* cp = chest pain type\n* trestbps =  = resting blood pressure (in mm Hg on admission to the hospital)\n* chol = serum cholestoral in mg/dl\n* fbs = (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n* restecg =  resting electrocardiographic results\n* thalach = maximum heart rate achieved\n* exang = exercise induced angina (1 = yes; 0 = no)\n* oldpeak = ST depression induced by exercise relative to rest\n* slope = the slope of the peak exercise ST segment\n* ca = number of major vessels (0-3) colored by flourosopy\n* thal = 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target = 1 or 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing essential tools\n# Regular EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# preprocessor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score,cross_validate,KFold,ShuffleSplit\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import f1_score, recall_score, precision_score, plot_roc_curve,roc_curve,roc_auc_score\n# Feature selection\nfrom sklearn.feature_selection import SelectFromModel\n#Pipeline\nfrom sklearn.pipeline import make_pipeline,Pipeline\nplt.style.use('seaborn-whitegrid')\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/heart-disease-uci/heart.csv\",sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape #(rows,columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration (Exploring data analysis or EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.target.value_counts() # balanced dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.target.value_counts().plot.bar();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() # No-Null values and all numerical features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum() # another way of checking missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heart Disease frequency according to sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sex.value_counts() # 1=male , 0-female","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparing target `column` with `sex` column\npd.crosstab(df.sex,df.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\n\npd.crosstab(df.sex, df.target).plot(kind=\"bar\",\n                                    color=[\"salmon\", 'lightblue'],\n                                    figsize=(10, 6),\n                                    ax=ax);\n\nax.set(xlabel=\"Sex (Female-0 , Male-1)\",\n       ylabel=\"Heart Disease Frequeny\",\n       title=\"Heart disease frequency for sex\");\n\nplt.xticks(rotation=0);\n\nax.legend(['Negative','Positive'],title =\"Target\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Age vs Max Heart rate for Heart Disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\n\nscatter = ax.scatter(x=df.age,\n           y= df.thalach,\n           c=df.target,\n               cmap='winter');\n\nax.set(xlabel=\"Age\",ylabel=\"Max Heaer Rate Achieved\",title=\"Heart Disease in function of Age and Max_Heart_Rate \")\nax.legend(*scatter.legend_elements(),title=\"Target\");\nplt.xticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.age.hist(bins= 15); # Helps in checking the ouliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heart disease frequency per chest pain type "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.ca,df.target,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\n\npd.crosstab(df.cp,df.target,).plot.bar(color=[\"salmon\",\"lightblue\"],ax=ax)\n\nax.set(xlabel=\"Chest Pain type\",\n       ylabel=\"Heart Disease Frequeny\",\n       title=\"Heart Disease frequency per chest pain type\");\n\nplt.xticks(rotation=0);\n\nax.legend(['Negative','Positive'],title =\"Heart Disease\");\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                linewidths=0.5,\n                fmt=\".2f\",\n                cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"## Prepare data for machine learning model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = df.drop(\"target\",axis=1),df.target\nnp.random.seed(24)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing various models"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Testing various models to check which model works best"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"models = {\n    \"Random Forest Classifier    \" : RandomForestClassifier(),\n    \"Logistic Regression\" : LogisticRegression(),\n    \"Knn\" : KNeighborsClassifier()\n}\nscore_dict = {}\nfor i in models:\n    models[i].fit(X_train,y_train)\n    score_dict[i] = cross_val_score(models[i],X,y).mean();\nscore_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df = pd.DataFrame(score_dict,index=['cross_val_score']).T\nscore_df.plot(kind='bar',figsize=(10,6))\nplt.xticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LogisticRegression has best baseline score**"},{"metadata":{},"cell_type":"markdown","source":"# Tuning and Evaluating model"},{"metadata":{},"cell_type":"markdown","source":" `tuned_score` to store the cross-validation scores of various models"},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_score = {} ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function to comapre models based upon their cross-validation-scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef evaluate(model,a,b):\n    metrics=['accuracy','f1','recall','precision','roc_auc']\n    \n    # cross-validatins the model to calculate scores\n    cv = ShuffleSplit(n_splits=5, test_size=0.2,random_state=99)\n    score = cross_validate(model,a,b,scoring=metrics,cv= cv)\n    \n    # rounding off the scores to 2 decimal places\n    for i in score:\n        score[i] = round(np.array(score[i]).mean(),ndigits=2)\n    \n    #plotting the cross-validates scores\n    score_df = pd.DataFrame(score,index=[\"score\"]).iloc[:,2:].T\n    score_df.plot.bar(figsize=(6,5),color=['salmon'],width=0.2)\n    plt.xticks(rotation=0) \n    \n    return (score) # returning the dictionery of crossvalidation scores for all metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## k-Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data into features and target\nX,y = df.drop(\"target\",axis=1),df.target\n\n# converting the categorical features into dummies and dropping 1 column out of each of them\ncat_features = ['cp','ca','slope','thal','sex']\nX = pd.get_dummies(X,columns=cat_features)\nX = X.drop(['cp_0','ca_0','slope_0','thal_0','sex_0'],axis=1)\n\n# splitting dataset into train and test set\nnp.random.seed(24)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n\n# scaling the features usng Standard Scaler\nscaler = StandardScaler()\nX_train_encoded_scaled = scaler.fit_transform(X_train);\nX_test_encoded_scaled = scaler.transform(X_test);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### k-NN - Tuning Hyperparameters by hand"},{"metadata":{"trusted":true},"cell_type":"code","source":"# variables for storing train and test score\ntrain_score = np.array([])\ntest_score = np.array([])\n\nneighbors = np.arange(1,21)\nfor i in neighbors :\n    # initializing model variable with diffrent values of n_neighbors parameter\n    knn = KNeighborsClassifier(n_neighbors=i)\n    \n    # fitting training set data\n    knn.fit(X_train_encoded_scaled,y_train)\n    \n    # appending scores in the respective arrays\n    train_score =  np.append(train_score , [knn.score(X_train_encoded_scaled,y_train)])\n    test_score =  np.append(test_score , [knn.score(X_test_encoded_scaled,y_test)])\n    \n# plotting scores \nplt.plot(neighbors,train_score,'b',label=\"train\")\nplt.plot(neighbors,test_score,'g',label=\"test\")\nplt.legend()\nplt.xticks(np.arange(1,21))\nplt.title(f'max_score is {test_score.max()}   at  n_neighbor ={np.argmax(test_score)+1}');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### k-NN - Tuning Hyperparameters using GridSearchCV"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# list of parameters and their possible values\nparameters = {\n    \"kneighborsclassifier__n_neighbors\" : np.arange(1,15),\n    \"kneighborsclassifier__leaf_size\" : np.arange(1,5),\n    \"kneighborsclassifier__weights\" : [\"uniform\",\"distance\"]\n}\n\n# initializing model variable and implementing GridSearchCV\nknn = KNeighborsClassifier()\n\n# creating a pipeline with sacler and estimator \nknn_scaling_pipeline = make_pipeline(StandardScaler(),KNeighborsClassifier())\n# implementing GridSearchCV on the pipeline\nknn_cv = GridSearchCV(knn_scaling_pipeline,param_grid=parameters,n_jobs=-1,cv=5)\n\n# fitting the training data to the \nknn_cv.fit(X_train,y_train)\nbest_estimator =  knn_cv.best_estimator_\n\n# creating pipeline with scaler and best_estimator so as to calculate the cross-validation scores in the \"evaluate\" function\nknn_scaling_pipeline = make_pipeline(StandardScaler(), best_estimator)\n\nsc = evaluate(knn_scaling_pipeline,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['k-NN'] = sc\nsc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForestClassifier (RFC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating features and label set\nX,y = df.drop(\"target\",axis=1),df.target\n\n# splitting dataset into train and test set\nnp.random.seed(24)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RFC - Tuning hyperparameters of RandomForestClassifier using GridSearchCV"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# list of possible values of the parameters\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2],\n    'min_samples_leaf': [5],\n    'min_samples_split': [ 12],\n    'n_estimators': np.arange(0,300,50)\n}\n# initializing model \nrfc =RandomForestClassifier(random_state=42)\n\n# implimenting GridSeachCV and fitting training set to it\nrfc_cv = GridSearchCV(rfc,n_jobs=-1,param_grid=param_grid,cv=5)\nrfc_cv.fit(X_train,y_train)\nbest_estimator = rfc_cv.best_estimator_\n\n#evaluating model\nsc = evaluate(best_estimator,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['RFC'] = sc\nsc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RFC - Selecting important features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"rfc_cv.best_estimator_.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing the transformer \nsfm = SelectFromModel(best_estimator,threshold=0.03,prefit=True)\n# since estimator is pre-fitted therfore no need to fit it again with the training set data , just make prefit = True \n \n# transforming the train and test dataset , features with importance value above threshold will be selected\nX_important = sfm.transform(X)\nX_train_important = sfm.transform(X_train)\nX_test_important = sfm.transform(X_test)\n\n# fittting the GridSeachCV model with important features\nrfc_cv.fit(X_train_important,y_train)\n\n# evaluating model\nsc = evaluate(rfc_cv.best_estimator_,X_important,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['RFC_imp_features'] = sc\nsc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression (LR)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = df.drop(\"target\",axis=1),df.target\nnp.random.seed(24)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n\nscaler = StandardScaler()\nX_train__scaled = scaler.fit_transform(X_train);\nX_test__scaled = scaler.transform(X_test);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LR - With scaled features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing model and evaluating results on the the scaled data\nlr = LogisticRegression()\nlr.fit(X_train__scaled,y_train)\nprint(\"test-score = \",lr.score(X_test__scaled,y_test),\"\\n\")\n\n# pipeline with scaler and estimator \nlr_scaling_pipeline = make_pipeline(StandardScaler(), lr)\nsc = evaluate(lr_scaling_pipeline,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['LR-scaled'] = sc\nsc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LR - Tuning hyperparametrs using GridSearchCV"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"param_grid = {'logisticregression__C': np.logspace(-4,4,30),'logisticregression__solver':['liblinear'],'logisticregression__penalty':['l1','l2']}\n\n\n# implementing GridSearchCV\nlr = LogisticRegression()\n\n# pipeline with scaler and estimator to prevent data leakage while scaling\nlr_pipeline = make_pipeline(StandardScaler(), lr)\nlr_cv = GridSearchCV(lr_pipeline,cv=5,n_jobs=-1,param_grid=param_grid)\n\n# fitting the training data to lr_cv\nlr_cv.fit(X_train,y_train)\n\n# evaluating model using pipleine\nlr_cv_pipeline = make_pipeline(StandardScaler(),lr_cv.best_estimator_)\nsc = evaluate(lr_cv_pipeline,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['LR-Tuned'] =sc\nsc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the final report"},{"metadata":{"trusted":true},"cell_type":"code","source":"# variable for storing the 2-dimensional array\nscore_array=np.ndarray((5,5))\n\n# converting the scores in the tuned)score dict into 2-dimensional np.array\nfor i,j in enumerate(tuned_score):\n    score_array[i,:] = list(tuned_score[j].values())[2:]\n\n# initializing scaler object\nscaler = MinMaxScaler(feature_range=(0,1))\n\n# MinMax Scaled cross-validation scores\nscaled_score_array = np.transpose(scaler.fit_transform(score_array))\n\n# cross-validation scores ( columns - models)(index - metrics)\nscore_array = np.transpose(score_array)\nprint(score_array)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Dataframe with cross-validation scores\nscore_df = pd.DataFrame(score_array,columns=list(tuned_score.keys()),index= ['accuracy','f1','recall','precision','roc_auc'])\n\n# Dataframe with MinMax scaled cross-validation scores\nscaled_score_df = pd.DataFrame(scaled_score_array,columns=list(tuned_score.keys()),index= ['accuracy','f1','recall','precision','roc_auc'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"score_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_score_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross validation Scores"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plotting scores\nscore_df.plot.bar(figsize=(15,4));\nplt.yticks(np.linspace(0,1,11,endpoint=True));\nplt.xticks(rotation=0,fontsize=12)\nplt.xlabel(\"Metrics\",fontsize=15)\nplt.ylabel(\"CV-Score\",fontsize=15)\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n          ncol=3, fancybox=True, shadow=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Relative Cross-validation scores\nMinMax scaled scores - easy to comapre performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_score_df.plot.bar(figsize=(15,4));\nplt.yticks(np.linspace(0,1,11,endpoint=True));\nplt.xticks(rotation=0,fontsize=12)\nplt.xlabel(\"Metrics\",fontsize=15)\nplt.ylabel(\"Relative CV-Score\",fontsize=15)\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n          ncol=3, fancybox=True, shadow=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nknn and RFC are the best peforming models with comaparable scores"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":4}