{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We would like to predict Placement","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Exploration\nfilepath = '/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv'\ndf = pd.read_csv(filepath, nrows = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_raw = tf.data.experimental.make_csv_dataset(filepath, batch_size = 5, label_name = 'status', num_epochs = 1, shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for features, labels in ds_raw.take(1):\n    for key, values in features.items():\n        print(f'{key:20}: {values}')\n    print(labels.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First let's encode the status label\nds = ds_raw.map(lambda features, label: (features, tf.map_fn(lambda x: 1 if x == 'Placed' else 0, label, dtype = tf.int32)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for features, labels in ds.take(1):\n    for key, values in features.items():\n        print(f'{key:20}: {values}')\n    print(labels.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Before we go into the next step, we would like to deal with NaN values, and only salary feature has lots of NaNs. \n#All NaNs were loaded as 0 by default in tf.data.experimental.make_csv_dataset\n#we would just drop salary feature because of the amount of NaNs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def popping(input_dict, feature_name):\n    input_dict.pop(feature_name)\n    return input_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = ds.map(lambda features, label: (popping(features, 'salary'), label))\nds = ds.map(lambda features, label: (popping(features, 'sl_no'), label)) #drop Serial Number, too","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next we would like to get the numeric columns apart from categorical columns\ncolumn_dtypes = {}\nfor key, value in ds.element_spec[0].items():\n    column_dtypes[key] = value.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = [column for column, dtype in column_dtypes.items() if dtype in [tf.int32, tf.float32]]\ncategorical_features = [column for column, dtype in column_dtypes.items() if dtype == tf.string]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For numeric features, we need to standardize, here we need to go back to the original Pandas dataframe and grab the mean and std\nmetadata = df.drop(['salary', 'sl_no'], axis = 1).describe().T\nmetadata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next we pack all the numeric features\nclass PackNumericFeatures():\n    def __init__(self, names):\n        self.names = names\n        \n    def __call__(self, features, label):\n        numeric = [features.pop(name) for name in self.names]\n        numeric = [tf.cast(item, tf.float32) for item in numeric]\n        numeric = tf.stack(numeric, -1)\n        features['numeric'] = numeric\n        \n        return features, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = ds.map(PackNumericFeatures(numeric_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for features, labels in ds.take(1):\n    for key, values in features.items():\n        print(f'{key:20}: {values}')\n    print(labels.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we can actually perform train test split\nprint(f'There are {len(df)} records.')\n\n#The reason we only take 40 below is because each batch contains 5 records\nds_train = ds.take(40)\nds_test = ds.skip(40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_test.reduce(0, lambda x, _: x + 1).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, we need a Standardization function for the numeric columns\ndef Standardize(tensor, mean, std):\n    return (tensor - mean)/std\n\nmean = metadata['mean'].to_numpy()\nstd = metadata['std'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import functools\n\nStandardize_partial = functools.partial(Standardize, mean = mean, std = std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Next we would build feature columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#for numeric columns\nnumeric_column = tf.feature_column.numeric_column('numeric', shape = (len(numeric_features), ), normalizer_fn = Standardize_partial)\nnumeric_columns = [numeric_column]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for categorical columns\nprint(categorical_features)\n\ncategories = {'gender': ['M', 'F'],\n             'ssc_b': ['Central', 'Others'],\n              'hsc_b': ['Central', 'Others'],\n              'hsc_s': ['Commerce', 'Science', 'Arts'],\n              'degree_t': ['Comm&Mgmt', 'Sci&Tech', 'Others'],\n              'workex': ['Yes', 'No'],\n              'specialisation': ['Mkt&Fin', 'Mkt&HR']\n             }\n\ncategorical_columns = [tf.feature_column.categorical_column_with_vocabulary_list(key, values) for key, values in categories.items()]\ncategorical_columns = [tf.feature_column.indicator_column(column) for column in categorical_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = numeric_columns + categorical_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Next we would train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = tf.estimator.DNNClassifier(hidden_units = [32, 32, 32], feature_columns = feature_columns, model_dir = '/estimator', n_classes = 2, activation_fn = 'relu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def input_fn(ds_train):\n    return ds_train.unbatch().shuffle(1000).batch(5).repeat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#estimator.train(input_fn = lambda: input_fn(ds_train)) #does not work","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So we probably need to create the EagerTensor within the function, so let's try the alternative to define a function that creates the EagerTensor starting from the beginning\ndef input_fn(numeric_features, train = True, batch_size = 5, num_epochs = 1, shuffle = False, steps = 40):\n    ds_raw = tf.data.experimental.make_csv_dataset(filepath, batch_size = batch_size, label_name = 'status', num_epochs = num_epochs, shuffle = shuffle)\n    ds = ds_raw.map(lambda features, label: (features, tf.map_fn(lambda x: 1 if x == 'Placed' else 0, label, dtype = tf.int32)))\n    \n    ds = ds.map(lambda features, label: (popping(features, 'salary'), label))\n    ds = ds.map(lambda features, label: (popping(features, 'sl_no'), label)) #drop Serial Number, too\n    \n    ds = ds.map(PackNumericFeatures(numeric_features))\n    \n    if train:\n        ds = ds.take(steps)\n        return ds.unbatch().shuffle(1000).batch(batch_size).repeat()        \n    else:\n        ds = ds.skip(steps)\n        return ds.unbatch().shuffle(1000).batch(batch_size)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator.train(input_fn = lambda: input_fn(numeric_features, train = True), steps = 400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_result = estimator.evaluate(input_fn = lambda: input_fn(numeric_features, train = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The accuracy of 1.0 seems too good to be true...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Manually building a Keras Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = tf.keras.layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential()\n\nmodel.add(input_layer)\nmodel.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train = ds_train.unbatch().shuffle(1000).batch(5)\nmodel.fit(ds_train, epochs = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_results2 = model.evaluate(ds_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So this time we have a lower test accuracy, we can still predict the result\npredictions = model.predict(ds_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Sigmoid(x):\n    return 1/(1+np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sigmoid(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = predictions.ravel()\npred_prob = Sigmoid(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"actual = np.array([])\nfor batch in ds_test:\n    actual = np.append(actual, batch[1].numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for p, a in zip(pred_prob, actual):\n    print(f'The predicted Placement probability is {p:.2%}, the actual result of the placement is {\"Placed\" if a == 1 else \"Not Placed\"}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}