{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"n5ka4SsKkGe5","colab_type":"text"},"cell_type":"markdown","source":"#**Data** **Preprocessing**"},{"metadata":{"id":"Q-acVsSPz7WR","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nimport random\nfrom sklearn.svm import SVC\nimport sklearn.metrics as sk\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"id":"8BA8YEtm7jBG","colab_type":"code","outputId":"ba22de91-3757-4f91-c1bc-a87bb3f73278","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#change the dataset location\ndf = pd.read_csv('/kaggle/input/bank-marketing/bank-additional-full.csv', sep = ';')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"STEMeQrf0sPb","colab_type":"code","outputId":"2e2850ce-7565-4dc1-87f9-62c7af5e5d3d","colab":{"base_uri":"https://localhost:8080/","height":195},"trusted":true},"cell_type":"code","source":"#viewing data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SKWVMRHz0uNQ","colab_type":"code","outputId":"b828ce64-90a3-40ea-9e86-3b0e777b0c2e","colab":{"base_uri":"https://localhost:8080/","height":386},"trusted":true},"cell_type":"code","source":"#data info\ndf.info()\n#No null values in the data","execution_count":null,"outputs":[]},{"metadata":{"id":"foXImJEPUEky","colab_type":"code","outputId":"cad2c016-f421-4e13-9b2f-9912e41393c5","colab":{"base_uri":"https://localhost:8080/","height":402},"trusted":true},"cell_type":"code","source":"#Removing non-relevant variables\ndf1=df.drop(columns=['day_of_week','month','contact','poutcome','pdays'],axis=1)\ndf1","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"9e0006b8-6ea3-43e6-d8ee-f15cb5834413","id":"be-AehEwc42c","colab":{"base_uri":"https://localhost:8080/","height":402},"trusted":true},"cell_type":"code","source":"#Replacing all the binary variables to 0 and 1\ndf1.y.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.default.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.housing.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.loan.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1","execution_count":null,"outputs":[]},{"metadata":{"id":"gbHut2dZcB-z","colab_type":"code","outputId":"8a044179-0462-4787-cf10-71580ae80b10","colab":{"base_uri":"https://localhost:8080/","height":232},"trusted":true},"cell_type":"code","source":"#creating Dummies for categorical variables\ndf2 = pd.get_dummies(df1)\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"eS3kDX710wSt","colab_type":"code","outputId":"4dafce21-c40e-4272-8106-2c56d214fe05","colab":{"base_uri":"https://localhost:8080/","height":817},"trusted":true},"cell_type":"code","source":"#Removing extra dummy variables & checking descriptive stats\ndf3=df2.drop(columns=['job_unknown','marital_divorced','education_unknown'],axis=1)\ndf3.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"4TWrAOMtdaoT","colab_type":"code","outputId":"0d99fc1a-f18b-4299-a22e-d78f3c52d0b7","colab":{"base_uri":"https://localhost:8080/","height":585},"trusted":true},"cell_type":"code","source":"#Correlation plot\nplt.figure(figsize=(14,8))\ndf3.corr()['y'].sort_values(ascending = False).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"id":"brHw-bqF8tgf","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#Creating binary classification target variable\ndf_target=df3[['y']].values\ndf_features=df3.drop(columns=['y'],axis=1).values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"uSiofyyZergw","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nx1_train = sc.fit_transform(x1_train)\nx1_test = sc.transform(x1_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"BBw_RnuwOkRK","colab_type":"text"},"cell_type":"markdown","source":"#Run SVM\n"},{"metadata":{"id":"SPvhfLpfN2Fo","colab_type":"code","outputId":"d36bc321-2620-47f3-d54f-883e948cf59b","colab":{"base_uri":"https://localhost:8080/","height":591},"trusted":true},"cell_type":"code","source":"#Linear SVM\nprint('Linear Model',end='\\n')\nlsvclassifier = SVC(kernel='linear')\nlsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = lsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_linear=accuracies.mean()\nstd_svm_linear=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_linear*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_linear*100,end='\\n')\n\n#Predict SVM\ny_predl = lsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predl))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predl))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predl, normalize=True, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"id":"emcij_ZBhFN6","colab_type":"code","outputId":"064a0fd4-45ce-490f-c939-f0a6e9959218","colab":{"base_uri":"https://localhost:8080/","height":558},"trusted":true},"cell_type":"code","source":"#Polynomial SVM\nprint('Polynomial Model',end='\\n')\npsvclassifier = SVC(kernel='poly')\npsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = psvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_poly=accuracies.mean()\nstd_svm_poly=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_poly*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_poly*100,end='\\n')\n\n#Predict SVM\ny_predp = psvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predp))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predp))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predp, normalize=True, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"id":"b4USM3HPhE2c","colab_type":"code","outputId":"931cec18-0f70-46ec-8d24-dff61dc98e27","colab":{"base_uri":"https://localhost:8080/","height":860},"trusted":true},"cell_type":"code","source":"#RBF SVM\nprint('RBF Model',end='\\n')\nrsvclassifier = SVC(kernel='rbf')\nrsvclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = rsvclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_svm_rbf=accuracies.mean()\nstd_svm_rbf=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_svm_rbf*100,end='\\n')\nprint('Standard deviation of Accuracies',std_svm_rbf*100,end='\\n')\n\n#Predict SVM\ny_predr = rsvclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test,y_predr))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test,y_predr))\nprint('Accuracy: ',sk.accuracy_score(y1_test, y_predr, normalize=True, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"id":"DTZiu0pzx14m","colab_type":"code","outputId":"4148229d-2232-4347-8015-66ff8375a39e","colab":{"base_uri":"https://localhost:8080/","height":350},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(rsvclassifier, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kKdh-uZ97gDL","colab_type":"text"},"cell_type":"markdown","source":"#Decision Trees\n"},{"metadata":{"id":"tscXhQZC7ftV","colab_type":"code","outputId":"23993d98-868c-4714-8b79-4821b8f58c59","colab":{"base_uri":"https://localhost:8080/","height":319},"trusted":true},"cell_type":"code","source":"#Entropy Model\neclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\neclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = eclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_e=accuracies.mean()\nstd_dt_e=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_e*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_e*100,end='\\n')\n\n#predict y\ny_pred = eclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"h_24T5lAivNH","colab_type":"code","outputId":"921d939e-8794-4494-e0df-b58f1cc3fb9a","colab":{"base_uri":"https://localhost:8080/","height":319},"trusted":true},"cell_type":"code","source":"#Gini Model\ngclassifier = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\ngclassifier.fit(x1_train, y1_train)\n\n#Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = gclassifier, X = x1_train, y = y1_train, cv = 5)\nmean_dt_g=accuracies.mean()\nstd_dt_g=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_dt_g*100,end='\\n')\nprint('Standard deviation of Accuracies',std_dt_g*100,end='\\n')\n\n#predict y\ny_pred = gclassifier.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_pred))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_pred))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"KJRmE8iUkHsI","colab_type":"code","outputId":"cc30d59e-353e-4b98-c359-ac9adbb5b4a9","colab":{"base_uri":"https://localhost:8080/","height":50},"trusted":true},"cell_type":"code","source":"#Pruning the better tree - Entropy Tree\nparameters = [{'criterion': ['entropy'],'min_samples_leaf':[5,10,20,50,100],'max_depth':[5,10,20,50,100]}] \ngrid_search = GridSearchCV(estimator = eclassifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"rOLl3URy9gPQ","colab_type":"code","outputId":"4e40fc9f-4306-4de0-fc2c-62a882560ee7","colab":{"base_uri":"https://localhost:8080/","height":401},"trusted":true},"cell_type":"code","source":"cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(grid_search, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"XCjKi6_PXAci","colab_type":"text"},"cell_type":"markdown","source":"#Boosting"},{"metadata":{"id":"sg5QMF4lXCRd","colab_type":"code","outputId":"80fd5e91-01c2-4d89-ef05-5b9b2b2db23d","colab":{"base_uri":"https://localhost:8080/","height":406},"trusted":true},"cell_type":"code","source":"# Boosting via Gradient Boost\nfrom sklearn.ensemble import GradientBoostingClassifier\nclassifiergb = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\nclassifiergb.fit(x1_train, y1_train)\n\n# Applying k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifiergb, X = x1_train, y = y1_train, cv = 10,n_jobs=-1)\nmean_boosting=accuracies.mean()\nstd_boosting=accuracies.std()\n\n#After using 5 fold cross validation\nprint('After 5 fold cross validation:')\nprint('Mean of Accuracies: ',mean_boosting*100,end='\\n')\nprint('Standard deviation of Accuracies',std_boosting*100,end='\\n')\n\n# Predicting the Test set results\ny_predgb = classifiergb.predict(x1_test)\n\n#Confusion Matrix\nprint('Test Output:')\nprint('Confusion Matrix:')\nprint(sk.confusion_matrix(y1_test, y_predgb))\nprint('Classification Report:')\nprint(sk.classification_report(y1_test, y_predgb))\nprint('Accuracy: ',sk.accuracy_score(y1_test,y_predgb))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"W7HUvbDjXJDG","colab_type":"code","outputId":"887c609a-159a-4ed4-e825-0ca86bada728","colab":{"base_uri":"https://localhost:8080/","height":138},"trusted":true},"cell_type":"code","source":"#playing around with the pruning to get the best boosting tree\n# Applying Grid Search to find the best model and the best parameters\nfrom sklearn.ensemble import AdaBoostClassifier\nclassifier_AdaBoost = AdaBoostClassifier(random_state=1)\nclassifier_AdaBoost.fit(x1_train, y1_train)\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators': [50,100,200,300,500,1000,1500]}] \ngrid_search = GridSearchCV(estimator = classifier_AdaBoost,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x1_train, y1_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint('Accuracy: ',best_accuracy,end='\\n')\nprint('Best Parameters: ',best_parameters,end='\\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"9NcgpfPe9pdT","colab_type":"code","outputId":"56eb0758-6e02-4f94-f360-72744029142c","colab":{"base_uri":"https://localhost:8080/","height":401},"trusted":true},"cell_type":"code","source":"cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier_AdaBoost, \n                                                        df_features, \n                                                        df_target,\n                                                        # Number of folds in cross-validation\n                                                        cv=cv,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"ML_assignment_2_Banking.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}