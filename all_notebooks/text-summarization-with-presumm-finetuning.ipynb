{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"This kernel demonstrates how to use the open source code [PreSumm](http://github.com/nlpyang/PreSumm) released by **yang liu** and to conduct text summarization task on your own dataset. It covers the following contents\n\n- 1. Process the data by using **process.py** from PreSumm repo and [**StanfordCoreNLP**](https://stanfordnlp.github.io/CoreNLP/download.html) tool\n- 2. Fine-tuning the text summarization model and save weights\n\n***Notice that:** The Presumm code used here has been modified to run in kaggle and process the customized data.*"},{"metadata":{},"cell_type":"markdown","source":"# Setup \nInstall the required package from **requirements.txt** in presummy folder"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# !pip install -r /kaggle/input/pre-summy/PreSumm/requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the raw description and summary data"},{"metadata":{},"cell_type":"markdown","source":"Glimpse on our News summary data released by **Kondalarao Vonteru** in [here](https://www.kaggle.com/sunnysai12345/news-summary#news_summary_more.csv)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nsummary_df = pd.read_csv('/kaggle/input/news-summary/news_summary_more.csv')\nsummary_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocess"},{"metadata":{},"cell_type":"markdown","source":"Our data process includes the following four steps:  \n- 1. Convert our raw data to **.story** format and store in */kaggle/working/news_sum/* . This enables us to use **preprocess.py** code to do further process  \n- 2. Tokenize **.story** file via using [**StanfordCoreNLP**](https://stanfordnlp.github.io/CoreNLP/index.html). Stanford CoreNLP is written in **Java**; recent releases require Java **1.8+**. You need to have Java installed to run CoreNLP. However, you can interact with CoreNLP via the command-line or its web service; many people use CoreNLP while writing their own code in Javascript, Python, or some other language.  \n\n    (*Here we activate Stanford CoreNLP by the code in **preprocess.py***)\n- 3. Convert tokenized data as **json** format \n- 4. Convert **json** file to **.pt** format for model finetuning\n"},{"metadata":{},"cell_type":"markdown","source":"## Create required folder for the preprocess and modeling fine-tuning"},{"metadata":{},"cell_type":"markdown","source":"## **folder name -> data file**\n* **news_sum** -> .story data  \n* **merged_stories_tokenized** -> tokenized data  \n* **json_data** ->  json files\n* **bert_data** ->  .pt files\n* **logs** ->  for storing logs information during preprocess and finetuning\n* **temp** -> cache model config data\n* **bertsumextabs** ->  save finetuning model weights****\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nos.chdir('/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir news_sum\n!mkdir merged_stories_tokenized\n!mkdir json_data\n!mkdir bert_data\n!mkdir logs\n!mkdir temp\n!mkdir bertsumextabs ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Make raw description and summary data as *.story*** files\nHere we preprocess only 100 files to save kernel commit time "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# def create_story_files(df,num_file=100,filename=None):\n#     for i in range(num_file):\n#         doc = df['text'][i] + '\\n'*2 + '@highlight' + '\\n'*2 + df['headlines'][i]\n#         file_name = os.path.join(filename,(str(i) + '.story'))\n#         with open(file_name,'w') as story_file:\n#             story_file.write(doc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create_story_files(summary_df,num_file=30000,filename='/kaggle/working/news_sum')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Make  *.story* files as tokenized data file via StandfordCoreNLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir('/kaggle/input/pre-summy/PreSumm/src/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python preprocess.py -mode tokenize -raw_path /kaggle/working/news_sum -save_path /kaggle/working/merged_stories_tokenized  -log_file /kaggle/working/logs/cnndm.log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Make  *tokenized data* files as *.json file* "},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python preprocess.py -mode format_to_lines -raw_path /kaggle/working/merged_stories_tokenized -save_path /kaggle/working/json_data/news -n_cpus 1 -use_bert_basic_tokenizer false -log_file /kaggle/working/logs/cnndm.log ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Make .json file as .pt file for BERT model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python preprocess.py -mode format_to_bert -raw_path /kaggle/working/json_data -save_path /kaggle/working/bert_data  -lower -n_cpus 1 -log_file /kaggle/working/logs/cnndm.log ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/')\n\n!rm -r news_sum\n!rm -r merged_stories_tokenized\n!rm -r json_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Finetuning"},{"metadata":{},"cell_type":"markdown","source":"* The code here uses a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences.   \n* The original [paper](https://arxiv.org/abs/1908.08345) includes a two-staged fine-tuning approach *(finetuning on extractive BERT and then abstractive BERT)* can further boost the quality of the generated summaries, but here we only do the second stage finetuning on the **abstractive summarization model**."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir('/kaggle/input/pre-summy/PreSumm/src/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python train.py  -task abs -mode train -train_from /kaggle/input/absbert-weights/model_step_149000.pt -bert_data_path /kaggle/working/bert_data/news  -dec_dropout 0.2  -model_path /kaggle/working/bertsumextabs -sep_optim true -lr_bert 0.002 -lr_dec 0.02 -save_checkpoint_steps 1000 -batch_size 140 -train_steps 150000 -report_every 100 -accum_count 5 -use_bert_emb true -use_interval true -warmup_steps_bert 1000 -warmup_steps_dec 500 -max_pos 512 -visible_gpus 0  -temp_dir /kaggle/working/temp -log_file /kaggle/working/logs/abs_bert_cnndm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation - Rouge-1 , Rouge-L "},{"metadata":{},"cell_type":"markdown","source":"We can get Average **Rouge-1: 0.522**,  **Rouge-L:0.487** after finetuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/pre-summy/PreSumm/src')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python train.py -task abs -mode test -model_path /kaggle/working/bertsumextabs -test_from /kaggle/working/bertsumextabs/model_step_150000.pt -batch_size 100 -test_batch_size 100 -bert_data_path /kaggle/working/bert_data/news -temp_dir /kaggle/working/temp -log_file /kaggle/working/logs/abs_bert_cnndm  -sep_optim true -use_interval true -visible_gpus 0 -max_pos 512 -max_length 200 -alpha 0.82 -min_length 10 -result_path /kaggle/working/logs/abs_bert_cnndm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/working/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('logs/abs_bert_cnndm.150000.gold','r') as s:\n#     summary = s.readlines()\n\n# for i in range(10):\n#     print(summary[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('logs/abs_bert_cnndm.150000.candidate','r') as s:\n#     cand = s.readlines()\n\n# for i in range(10):\n#     print(cand[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install rouge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from rouge import Rouge\n# rouge = Rouge()\n# rouge_1= 0\n# rouge_l=0\n# for i in range(len(summary)):\n#     scores = rouge.get_scores(summary[i],cand[i])\n#     rating = 'good' if scores[0]['rouge-l']['r']>0.5 else 'bad'\n#     rouge_1+=float(scores[0]['rouge-1']['r'])\n#     rouge_l+=float(scores[0]['rouge-l']['r'])\n    \n# rouge_1/=len(summary)\n# rouge_l/=len(summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Average Rouge-1: {},  Rouge-L:{}'.format(rouge_1 ,rouge_l))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove generate data to save commit time"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -r bert_data\n!rm -r logs\n!rm -r temp\n!rm -r bertsumextabs ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}