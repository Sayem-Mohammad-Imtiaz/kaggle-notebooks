{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2><center>Imbalanced Classification</center></h2>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nImbalanced classification involves developing predictive models on classification datasets that have a severe class imbalance. The challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n</div>    "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import kurtosis, skew\nfrom scipy import stats\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.metrics import brier_score_loss, make_scorer\n\nfrom sklearn.dummy import DummyClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_recall_curve\n\nfrom sklearn.model_selection import GridSearchCV\n%matplotlib inline\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")\n#plt.style.use('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n\ndef DataDesc(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef CalOutliers(df_num): \n    '''\n    \n    Leonardo Ferreira 20/10/2018\n    Set a numerical value and it will calculate the upper, lower and total number of outliers\n    It will print a lot of statistics of the numerical feature that you set on input\n    \n    '''\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print(color.BOLD+f'Lower outliers: {len(outliers_lower)}'+ color.END) # printing total number of values in lower cut of outliers\n    print(color.BOLD+f'Upper outliers: {len(outliers_higher)}'+ color.END) # printing total number of values in higher cut of outliers\n    print(color.BOLD+f'Total outliers: {len(outliers_total)}'+ color.END) # printing total number of values outliers of both sides\n    print(color.BOLD+f'Non - outliers: {len(outliers_removed)}'+ color.END) # printing total number of non outlier values\n    print(color.BOLD+f'% of Outliers : {round((len(outliers_total) / len(outliers_removed) )*100, 4)}'+ color.END ) # Percentual of outliers in points","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><center>1. Reading Data </center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"FILE_PATH = '../input/habermans-survival-data-set/haberman.csv'\ncolumns = ['age', 'year', 'nodes', 'class']\n\nhaberman_df = pd.read_csv(FILE_PATH, header=None, names=columns)\n\n# Setting class values to 1 & 0 for +ve & -ve classes\nhaberman_df['class'].replace({1:0,2:1}, inplace=True)\nhaberman_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><center>2. Exploring Data </center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"DataDesc(haberman_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>2.1. Age Distribution & Outliers</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"display(haberman_df['age'].describe())\n\nplt.figure(figsize=(15,5))\nplt.suptitle('Age Distribution', fontsize=30)\n_ = sns.countplot(data=haberman_df, x='age', color='#963559')\n_ = plt.ylabel('Count', fontsize=20)\n_ = plt.xlabel('Age', fontsize=20)\n\n\nprint(\"\\n\")\ndisplay(CalOutliers(haberman_df['age']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<h4 style=\"font-family:verdana; word-spacing:1.5px;font-size:16px\">Analysis of feature \"Age\" :</h4>\n<ul><li>Entropy/Uncertainty of column age is the highest(5.34), having 49 unique values out of 306 total samples.\n    <li>Mean age value is 52 and the distribution is similar to a normal Distribution.\n        <li> As of now we have not detected any outliers using naive calculation.\n    </ul>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<h3>2.2 Distribution of Age Based on Cancer</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\n\nax1 = fig.add_subplot(121)\n_ = sns.distplot(haberman_df[haberman_df['class'] == 0]['age'], bins=24, color='#f5dd90', ax=ax1)\n_ = ax1.set_title('Non-Cancer', fontsize=20)\n_ = ax1.set_xlabel(\"Age\",fontsize=15)\n_ = ax1.set_ylabel(\"\")\n\nax2 = fig.add_subplot(122)\n_ = sns.distplot(haberman_df[haberman_df['class'] == 1]['age'], bins=24, color='#0d3b66', ax=ax2)\n_ = ax2.set_title('Cancer', fontsize=20)\n_ = ax2.set_xlabel(\"Age\",fontsize=15)\n_ = ax2.set_ylabel(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>2.3 Nodes Distribution</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"display(haberman_df['nodes'].describe())\n\nplt.figure(figsize=(15,5))\nplt.suptitle('Nodes Distribution', fontsize=20)\n_ = sns.countplot(data=haberman_df, x='nodes', color='#963559')\n_ = plt.ylabel('Count', fontsize=15)\n_ = plt.xlabel('Nodes', fontsize=15)\n\n\nprint(\"\\n\")\ndisplay(CalOutliers(haberman_df['nodes']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>2.4 Distribution of nodes Based on Cancer</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\n_ = sns.swarmplot(data=haberman_df, x='class', y='nodes', palette=['#9fb8ad','#475841'])\n_ = plt.ylabel('Nodes', fontsize=15)\n_ = plt.xlabel('Class', fontsize=15)\n_ = plt.xticks([0,1],['Non-Cancer','Cancer'], fontsize=15)\n_ = plt.title('Nodes vs Cancer', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A cancer patient cannot be directly identified by number of nodes"},{"metadata":{},"cell_type":"markdown","source":"<h3>2.5 Cancer vs Non-Cancer</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"_ = plt.figure(figsize=(8,6))\n_ = sns.countplot(haberman_df['class'], palette=['#9fb8ad','#475841'])\n_ = plt.xticks([0,1],['Non-Cancer','Cancer'], fontsize=15)\n\ntarget = haberman_df['class'].values\ncounter = Counter(target)\nfor k,v in counter.items():\n    per = v / len(target) * 100\n    print('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><center>3. Model Test and Baseline Result</center></h3>"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"font-family:verdana; word-spacing:1.5px;font-size:16px\">\nWe are interested in predicting a probability of survival, we need a performance metric that evaluates the skill of a model based on the predicted probabilities. In this case, we will use the Brier score that calculates the mean squared error between the predicted probabilities and the expected probabilities.<br><br><br>\n    We need a Brier score for a reference prediction. A reference prediction for a problem in which we are predicting probabilities is the probability of the positive class label in the dataset. In this case, the positive class label represents non-survival and occurs about 26% in the dataset. Therefore, predicting about 0.26471 represents the worst-case or baseline performance for a predictive model on this dataset. Any model that has a Brier score better than this has some skill, where as any model that as a Brier score lower than this has no skill.<br><br>\n    We will evaluate the baseline strategy of predicting the distribution of positive examples in the training set as the probability of each case in the test set.<br> This can be implemented automatically using the DummyClassifier class and setting the strategy to ‘prior’ that will predict the prior probability of each class in the training dataset, which for the positive class we know is about 0.26471.\n    </div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def brier_skill_score(y_true, y_prob):\n    ref_probs = [0.26471 for _ in range(len(y_true))]\n    \n    bs_ref = brier_score_loss(y_true, ref_probs)\n    bs_model = brier_score_loss(y_true, y_prob)\n    \n    return 1.0 - (bs_model / bs_ref)\n\ndef evaluate_model(X, y, model):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    metric = make_scorer(brier_skill_score, needs_proba=True)\n    \n    scores = cross_val_score(model, X, y, cv=cv, scoring=metric, n_jobs=-1)\n    \n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = haberman_df.values[:,:-1], haberman_df.values[:,-1]  \n\nmodel = DummyClassifier(strategy='prior')\n\nscores = evaluate_model(X, y, model)\n\nprint(f'Mean BSS {np.mean(scores)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><center>4. Evaluate Probabilistic Models</center></h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;font-size:16px\">\n    <ul>\n        <li>Logistic Regression (LR)\n        <li>Linear Discriminant Analysis (LDA)\n        <li>Quadratic Discriminant Analysis (QDA)\n        <li>Gaussian Naive Bayes (GNB)\n        <li>Multinomial Naive Bayes (MNB)\n        <li>Gaussian Process (GPC)\n     </ul>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<h3>4.1. Probabilistic Algorithm Evaluation</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_models():\n    models, names = list(), list()\n    models.append(LogisticRegression(solver='lbfgs')) \n    names.append('LR')\n    \n    models.append(LinearDiscriminantAnalysis()) \n    names.append('LDA')\n    \n    models.append(QuadraticDiscriminantAnalysis()) \n    names.append('QDA')\n    \n    models.append(GaussianNB()) \n    names.append('GNB')\n    \n    models.append(MultinomialNB()) \n    names.append('MNB')\n    \n    models.append(GaussianProcessClassifier()) \n    names.append('GPC')\n    \n    return models, names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models, names = get_models() \nresults = list()\n\nfor i in range(len(models)):\n    scores = evaluate_model(X, y, models[i])\n    results.append(scores)\n\n    print('Model : %s, Mean : %.3f, STD : %.3f' % (names[i], np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blockquote>\nInterestingly, most if not all algorithms show a spread indicating that they may be unskillful on some of the runs. The distribution between the two top-performing models appears roughly equivalent, so choosing a model based on mean performance might be a good start.</blockquote>"},{"metadata":{},"cell_type":"markdown","source":"<h3>4.2. Model Evaluation With Scaled Inputs </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"models, names = get_models() \nresults = list()\n\nfor i in range(len(models)):\n    steps = [('t',StandardScaler()),('m',models[i])]\n    pipeline = Pipeline(steps=steps)\n    scores = evaluate_model(X, y, pipeline)\n    results.append(scores)\n\n    print('Model : %s, Mean : %.3f, STD : %.3f' % (names[i], np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4.3. Model Evaluation With Power Transform</h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;font-size:16px\">\nPower transforms, such as the Box-Cox and Yeo-Johnson transforms, are designed to change the distribution to be more Gaussian. This will help with the age input variable in our dataset and may help with the nodes variable and un-bunch the distribution slightly.<br><br>\nThe power transform may make use of a log() function, which does not work on zero values. We have zero values in our dataset, therefore we will scale the dataset prior to the power transform using a MinMaxScaler.</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"models, names = get_models() \nresults = list()\n\nfor i in range(len(models)):\n    steps = [('scale',MinMaxScaler()),('powert',PowerTransformer()),('model',models[i])]\n    pipeline = Pipeline(steps=steps)\n    scores = evaluate_model(X, y, pipeline)\n    results.append(scores)\n\n    print('Model : %s, Mean : %.3f, STD : %.3f' % (names[i], np.mean(scores), np.std(scores)))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blockquote>\n    Box and whisker plots are created for the results from each algorithm, suggesting perhaps a smaller and more focused spread for LR compared to the LDA, which was the second-best performing method. All methods still show skill on average, however the distribution of scores show runs that drop below 0.0 (no skill) in some cases.\n    </blockquote>"},{"metadata":{},"cell_type":"markdown","source":"<h3><center>5. Selecting Logistic regression & Fitting</center></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1)\n\nmodel = LogisticRegression(solver='lbfgs')\n\nsteps = [('scaler',MinMaxScaler()), ('powert',PowerTransformer()), ('model',model)]\npipeline = Pipeline(steps=steps)\n\nc_values = [100, 10, 1.0, 0.1, 0.01]\ngrid = dict(model__C=c_values)\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\ngrid_search = GridSearchCV(estimator=pipeline, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Prediction</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = grid_result.best_estimator_\n\ny_pred = best_model.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blockquote> Predictions are very poor especially for the positive class making all the predictions wrong</blockquote>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Using Heuristic(balanced) classw weights in Logistic regression</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=1)\n\nmodel = LogisticRegression(solver='lbfgs', class_weight='balanced')\n\nsteps = [('scaler',MinMaxScaler()), ('powert',PowerTransformer()), ('model',model)]\npipeline = Pipeline(steps=steps)\n\nc_values = [100, 10, 1.0, 0.1, 0.01]\ngrid = dict(model__C=c_values)\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n\ngrid_search = GridSearchCV(estimator=pipeline, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = grid_result.best_estimator_\n\ny_pred = best_model.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Predicting Probablities</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Survival Cases\nprint('Survival Cases:')\ndata = [[31,59,2], [31,65,4], [34,60,1]]\nfor row in data:\n    yhat = best_model.predict_proba([row])\n    p_survive = yhat[0, 0] * 100\n    print('>data=%s, Survival=%.3f%%' % (row, p_survive)) \n\n\n# some non-survival cases\nprint('Non-Survival Cases:')\ndata = [[44,64,6], [34,66,9], [38,69,21]]\n\nfor row in data:\n    yhat = best_model.predict_proba([row])\n    p_survive = yhat[0, 0] * 100\n    print('>data=%s, Survival=%.3f%%' % (row, p_survive))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}