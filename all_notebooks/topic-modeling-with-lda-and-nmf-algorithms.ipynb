{"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":2,"cells":[{"metadata":{"_uuid":"f588fa0865180a8e5cff7e5cf0a760a57e5a50eb","_cell_guid":"dfbc3da9-4ddc-4e69-b2da-daa67c7dfa7e"},"source":"# Data imports\n\nWe import Pandas, numpy and scipy for data structuresWe use gensim for LDA, and sklearn for NMF","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"216e35647b2e7a57dfeff81150733c091bd667ed","collapsed":true,"trusted":false,"_cell_guid":"5651f0f5-0875-4a8f-9a0d-92a70e7ce2bc","_execution_state":"idle"},"source":"import pandas as pd;\nimport numpy as np;\nimport scipy as sp;\nimport sklearn;\nimport sys;\nfrom nltk.corpus import stopwords;\nimport nltk;\nfrom gensim.models import ldamodel\nimport gensim.corpora;\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\nfrom sklearn.decomposition import NMF;\nfrom sklearn.preprocessing import normalize;\nimport pickle;","outputs":[],"cell_type":"code","execution_count":1},{"metadata":{"_uuid":"8c9345920c049ef38fd1fd89f7ce8072bd0d8ffe","_cell_guid":"c03f6c11-fe37-4ba2-aaff-4386b919cb20"},"source":"# Loading the data\n\nWe are using the ABC News headlines dataset. Some lines are badly formatted (very few), so we are skipping those.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"764d50cf171c841be1145c6761d0e1d099a435cf","collapsed":true,"trusted":false,"_cell_guid":"d747ed57-54f7-46f1-9364-38527109888c","_execution_state":"idle"},"source":"data = pd.read_csv('../input/abcnews-date-text.csv', error_bad_lines=False);","outputs":[],"cell_type":"code","execution_count":2},{"metadata":{"_uuid":"56452bb7f3335e6a5bbd2ffe76e01f68a32986b8","collapsed":true,"trusted":false,"_cell_guid":"330682b0-76c2-4052-bac9-8df9a24aa853","_execution_state":"idle"},"source":"#We only need the Headlines_text column from the data\ndata_text = data[['headline_text']];","outputs":[],"cell_type":"code","execution_count":3},{"metadata":{"_uuid":"83d0b19011d04fd80a81c79cb448df983536ac68","collapsed":false,"_execution_state":"idle"},"source":"#it takes too long to run it on the entire dataset on Kaggle, so reducing the size\nnp.random.seed(1024);\ndata_text = data_text.iloc[np.random.choice(len(data_text), 10000)];","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e0facf6b4f56ffa82660e247045b5392b4cc91c3","_cell_guid":"8ae5c98f-3b17-401b-a532-02e29b8b3e3c"},"source":"We need to remove stopwords first. Casting all values to float will make it easier to iterate over.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"c291d777b1168e810cd6e01d2df992929c8de281","collapsed":true,"trusted":false,"_cell_guid":"65dfea70-43de-42cb-baf6-bcb1b3a8705b","_execution_state":"idle"},"source":"data_text = data_text.astype('str');","outputs":[],"cell_type":"code","execution_count":21},{"metadata":{"_uuid":"d0da68359e6f2869c242b964bfa6458583a738a4","trusted":false,"_cell_guid":"20cd79ca-22d9-4460-9c66-8d9300bbcd78","scrolled":true,"_execution_state":"idle"},"source":"for idx in range(len(data_text)):\n    \n    #go through each word in each data_text row, remove stopwords, and set them on the index.\n    data_text.iloc[idx]['headline_text'] = [word for word in data_text.iloc[idx]['headline_text'].split(' ') if word not in stopwords.words()];\n    \n    #print logs to monitor output\n    if idx % 1000 == 0:\n        sys.stdout.write('\\rc = ' + str(idx) + ' / ' + str(len(data_text)));","outputs":[],"cell_type":"code","execution_count":22},{"metadata":{"_uuid":"d4ac86dfb7a6880543e78b2279edc739fcdc263c","collapsed":true,"trusted":false,"_cell_guid":"17b5ec46-74c4-443e-b422-f5345e78bb7f"},"source":"#save data because it takes very long to remove stop words\npickle.dump(data_text, open('data_text.dat', 'wb'))","outputs":[],"cell_type":"code","execution_count":105},{"metadata":{"_uuid":"6de821342df79a66168d2868a17dde0bc5dde61e","collapsed":true,"trusted":false,"_cell_guid":"d4c0c760-721d-4a51-a76f-74c1055f560d","_execution_state":"idle"},"source":"#get the words as an array for lda input\ntrain_headlines = [value[0] for value in data_text.iloc[0:].values];","outputs":[],"cell_type":"code","execution_count":24},{"metadata":{"_uuid":"1cb072ce110fd961ebadd0828a021dd303297237","collapsed":true,"trusted":false,"_cell_guid":"35ac0795-553c-4bbc-beaf-19aa5817956c","_execution_state":"idle"},"source":"#number of topics we will cluster for: 10\nnum_topics = 10;","outputs":[],"cell_type":"code","execution_count":26},{"metadata":{"_uuid":"0b28e2a222748a2f02916f5e4fe509f2159febb2","_cell_guid":"6d7316ea-c19f-4130-a28a-a80115e353e1"},"source":"# LDA\n\nWe will use the gensim library for LDA. First, we obtain a id-2-word dictionary. For each headline, we will use the dictionary to obtain a mapping of the word id to their word counts. The LDA model uses both of these mappings.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"9da710becdc0becdfbcbaf587a001de252c86143","collapsed":true,"trusted":false,"_cell_guid":"5fc5ea20-484b-445c-84a5-625b95008da8","_execution_state":"idle"},"source":"id2word = gensim.corpora.Dictionary(train_headlines)","outputs":[],"cell_type":"code","execution_count":27},{"metadata":{"_uuid":"6144609f82545dd604b28bf3485cb3e9abdfa931","collapsed":true,"trusted":false,"_cell_guid":"ac25d164-f694-48ed-ad83-72c6148e38cf","_execution_state":"idle"},"source":"corpus = [id2word.doc2bow(text) for text in train_headlines]","outputs":[],"cell_type":"code","execution_count":28},{"metadata":{"_uuid":"015c826e1f3e38e4ca0f4a5d2307e8f55a7db275","collapsed":true,"trusted":false,"_cell_guid":"b0d573a1-50d9-4ade-9713-65ef17c07a88","_execution_state":"idle"},"source":"lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)","outputs":[],"cell_type":"code","execution_count":30},{"metadata":{"_uuid":"ee8858f023eac1ab256e4871f4eac99ac1895f93","_cell_guid":"6dcdeab6-2ee6-46d4-b31d-2abeeefd3bd3"},"source":"# generating LDA topics\n\nWe will iterate over the number of topics, get the top words in each cluster and add them to a dataframe.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"2193410f3b9be711eb7d176e510fb6aefd8eaa36","collapsed":true,"trusted":false,"_cell_guid":"bb8055e4-82ed-4102-87f5-ca37ad66dac8","_execution_state":"idle"},"source":"def get_lda_topics(model, num_topics):\n    word_dict = {};\n    for i in range(num_topics):\n        words = model.show_topic(i, topn = 20);\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n    return pd.DataFrame(word_dict);","outputs":[],"cell_type":"code","execution_count":31},{"metadata":{"_uuid":"42160604385486ac2fa0f70571fe2a983dee48c7","trusted":false,"_cell_guid":"28054448-4467-4c7a-9693-2d2c3f764f90","_execution_state":"idle"},"source":"get_lda_topics(lda, num_topics)","outputs":[],"cell_type":"code","execution_count":33},{"metadata":{"_uuid":"27487714b2ebee6b8bbfe7d2b57751fb7b921480","collapsed":true,"_cell_guid":"8356cc1e-4c42-4023-844a-77738af13427"},"source":"# NMF\n\nFor NMF, we need to obtain a design matrix. To improve results, I am going to apply TfIdf transformation to the counts.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"48eb95d71a20f24e921fa1907799f43b1b5a6bac","collapsed":true,"trusted":false,"_cell_guid":"a00e72db-0df3-49db-bff0-a45cf6d93ccc","_execution_state":"idle"},"source":"#the count vectorizer needs string inputs, not array, so I join them with a space.\ntrain_headlines_sentences = [' '.join(text) for text in train_headlines]","outputs":[],"cell_type":"code","execution_count":34},{"metadata":{"_uuid":"f4d0a3614c4ca2b9bddde8fea49f94d3c41d7433","collapsed":true,"trusted":false,"_cell_guid":"23a6e9b1-e9bb-4f60-908b-79d05b4ef467","_execution_state":"idle"},"source":"#obtain a Counts design matrix. Because the size of the matrix will be large, we can set the max_features to 5000.\nvectorizer = CountVectorizer(analyzer='word', max_features=5000);\nx_counts = vectorizer.fit_transform(train_headlines_sentences);","outputs":[],"cell_type":"code","execution_count":35},{"metadata":{"_uuid":"d34c383385fe6ba016ffd40ee0ef3030d3f254e9","trusted":false,"_cell_guid":"6a4a3a2b-fc24-4491-ac6a-07df1c09cd70","_execution_state":"idle"},"source":"#set a TfIdf transformer, and transfer the counts with the model.\ntransformer = TfidfTransformer(smooth_idf=False);\nx_tfidf = transformer.fit_transform(x_counts);","outputs":[],"cell_type":"code","execution_count":36},{"metadata":{"_uuid":"bbdf244d2c9ef2b94abc2e39444ea4d29865c6b2","trusted":false,"_cell_guid":"40a8ce98-7482-4093-82ef-2ff7fe08dbb2","_execution_state":"idle"},"source":"#normalize the TfIdf values so each row has unit length.\nxtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)","outputs":[],"cell_type":"code","execution_count":37},{"metadata":{"_uuid":"d3908709458c87850e031b79ca5469e8d6c2b3af","collapsed":true,"trusted":false,"_cell_guid":"e43ab7dd-d7a5-4080-a99c-5b41ac2eca99","_execution_state":"idle"},"source":"#obtain a NMF model.\nmodel = NMF(n_components=num_topics, init='nndsvd');","outputs":[],"cell_type":"code","execution_count":38},{"metadata":{"_uuid":"240e4f115320cb99c0d210e6a13d61390f036a2b","trusted":false,"_cell_guid":"bc6eb7d6-2ddc-4bb4-a63b-e2306971413f","_execution_state":"idle"},"source":"#fit the model\nmodel.fit(xtfidf_norm)","outputs":[],"cell_type":"code","execution_count":39},{"metadata":{"_uuid":"1fabf9b2891da6a46fabb87a366165224402974d","collapsed":true,"trusted":false,"_cell_guid":"8155d7f8-6adc-4eb3-8632-e7e22e540ba9","_execution_state":"idle"},"source":"def get_nmf_topics(model, n_top_words):\n    \n    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n    feat_names = vectorizer.get_feature_names()\n    \n    word_dict = {};\n    for i in range(num_topics):\n        \n        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n        words = [feat_names[key] for key in words_ids]\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n    \n    return pd.DataFrame(word_dict);","outputs":[],"cell_type":"code","execution_count":40},{"metadata":{"_uuid":"251c6c267be6b58362bd7ec3da705b6ac2c9616e","trusted":false,"_cell_guid":"c0f4a422-f3ec-414f-94f8-f48b6d9a983a","_execution_state":"idle"},"source":"get_nmf_topics(model, 20)","outputs":[],"cell_type":"code","execution_count":41}]}