{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_data = pd.read_csv('/kaggle/input/students-performance-in-exams/StudentsPerformance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_data.nunique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**let's add a copy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = csv_data.copy() # Create a copy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is there a correlation between gender and scores?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('gender').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mean = df.groupby(\"gender\").agg([np.mean, np.std])  # Let's make a table that only contains the gender and average score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mean # It worked!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting this table"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"darkgrid\")\ndf_mean['math score'].plot(kind = 'barh', y = 'mean', legend = False, color = ['pink', 'blue'],figsize=(9, 6),fontsize = 16,xerr='std')\nplt.title(\"Average scores of the Math Test by gender\",fontsize = 16)\nplt.xlabel(\"Gender\",fontsize = 16)\nplt.ylabel(\"Mean\",fontsize = 16)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"darkgrid\")\ndf_mean['reading score'].plot(kind = 'barh', y = 'mean', legend = False, color = ['pink', 'blue'],figsize=(9, 6),fontsize = 16,xerr='std')\nplt.title(\"Average scores of the Reading Test by gender\",fontsize = 16)\nplt.xlabel(\"Gender\",fontsize = 16)\nplt.ylabel(\"Mean\",fontsize = 16)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"darkgrid\")\ndf_mean['writing score'].plot(kind = 'barh', y = 'mean', legend = False, color = ['pink', 'blue'],figsize=(9, 6),fontsize = 16,xerr='std')\nplt.title(\"Average scores of the Writing Test by gender\",fontsize = 16)\nplt.xlabel(\"Gender\",fontsize = 16)\nplt.ylabel(\"Mean\",fontsize = 16)\nplt.xticks(rotation=0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It seems females do considerably better at the Writing and Reading tests**"},{"metadata":{},"cell_type":"markdown","source":"# Now let's check the correlation between race group and scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = df.columns.str.replace(' ', '_')  # replace whitespace with _ to make it cleaner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_mean = df.groupby(['race/ethnicity']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_mean.head() # I don't want race/ethnicity to be the index.. let's reset that","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_mean = score_mean.reset_index()\nscore_mean.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now let's plot this"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df.groupby(['race/ethnicity']).mean())\nsns.set_style(\"darkgrid\")\nplt.legend(['Math Score','Reading Score','Writing Score'])\nplt.title(\"Correlation between the race/ethnicity and test scores\",fontsize = 16)\nplt.xlabel(\"Race/Ethnicity\",fontsize = 16)\nplt.ylabel(\"Mean test score\",fontsize = 16)\nplt.xticks(rotation=0, fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.figure(figsize=(12,6))\nplt.rcParams['figure.figsize'] = (9,5)\nplt.show()\n# Seems that group E has the best average","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(score_mean['math_score'],\n       labels = score_mean['race/ethnicity'].values,\n       autopct = '%.2f%%',\n        textprops = {\n            'color' : 'black',     \n        }\n       )\nplt.title('Math Scores by race/ethnicity', fontsize = '16')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(score_mean['reading_score'],\n       labels = score_mean['race/ethnicity'].values,\n       autopct = '%.2f%%',\n        textprops = {\n            'color' : 'black',     \n        }\n       )\nplt.title('Reading Scores by race/ethnicity', fontsize = '16')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(score_mean['writing_score'],\n       labels = score_mean['race/ethnicity'].values,\n       autopct = '%.2f%%',\n        textprops = {\n            'color' : 'black',     \n        }\n       )\nplt.title('Writing Scores by race/ethnicity', fontsize = '16')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's create a 'total' score column\nscore_mean['total'] = score_mean['math_score']+score_mean['reading_score']+score_mean['writing_score']\nscore_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(x = score_mean['race/ethnicity'],\n       height = score_mean['total'], \n        color = 'midnightblue')\nplt.title('Total score number comparison', fontsize = '16')\nplt.xlabel(\"Race/ethnicity\")\nplt.ylabel(\"Total score\")\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Does having free/reduced lunch correlate with the test scores?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lunch_mean = df.groupby(df['lunch']).mean()\nlunch_mean = lunch_mean.reset_index()\nlunch_mean # People with free/reduced lunch averages a lower score than those with standard lunch!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lunch_sum =  df.groupby(df['lunch']).sum() # I'll also create the sum of all scores\nlunch_sum['total'] = lunch_sum['math_score'] + lunch_sum['reading_score'] + lunch_sum['writing_score']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lunch_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2,2, figsize = (12,10))\naxes[0,0].bar(x = lunch_mean['lunch'],\n             height = lunch_mean['math_score'],\n             color = 'darkred')\naxes[0,0].set_title(\"Average Math score comparision\")\naxes[0,1].bar(x = lunch_mean['lunch'],\n             height = lunch_mean['reading_score'],\n             color = 'midnightblue')\naxes[0,1].set_title(\"Average Reading score comparision\")\naxes[1,0].bar(x = lunch_mean['lunch'],\n             height = lunch_mean['writing_score'],\n             color = 'darkgreen')\naxes[1,0].set_title(\"Average Writing score comparision\")\naxes[1,1].bar(x = lunch_mean['lunch'],\n             height = lunch_sum['total'],\n             color = 'purple')\naxes[1,1].set_title(\"Total score comparision\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}