{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction of CARVAN DATABASE-** This database is an real time insurance dataset. One important thing about this dataset is this dataset have all the columns are of Categorical in Nature i.e. unique values are from 1 to 40.\n\nSince all the dataset is of the type categorical data we are using Target Encoding instead of One Hot Encoding as One Hot encoding increases the database size drastically as whole dataset is categorical type.\n\n**Objective of this Notebook -** In the present notebook we analysed this classification problem using supervised learning models. In present notebook we analysed the Model using following supervised modeling techniques\n**- Decision Tree Model **\n**- Random Forest Model**\n**- Ada Boost Model**\n**- Gradient Boosting Model**\n\nWe are predicting the dataset using various models and find the prediction score of all the models.\n\n**Feature Importance** is another important area of this notebook in this we analyse various features of dataset using Random Forest Classifier."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# 1.1 Call Libraries and import Dataset.\n%reset -f\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.parsers import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error as MSE\n\n#2.0 Import OS directory and import data from CSV file\nimport os          \ncarvan = read_csv('../input/caravan-insurance-challenge/caravan-insurance-challenge.csv')\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Data more easily exporable -** Since this dataset is in coded language columns we first convert this dataset into easily understandable."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#2.1 Rename column names for better understanding of database\n\ncarvan_column_names  = {\n'ORIGIN':     'Datacategory',\n'MOSTYPE':    'Customer_Subtype',\n'MAANTHUI':   'Number_of_houses',\n'MGEMOMV':    'Avg_size_household',\n'MGEMLEEF':   'Avg_age',\n'MOSHOOFD':   'Customer_main_type',\n'MGODRK':     'Roman_catholic',\n'MGODPR':     'Protestant',\n'MGODOV':     'Other_religion',\n'MGODGE':     'No_religion',\n'MRELGE':     'Married',\n'MRELSA':     'Living_together',\n'MRELOV':     'Other_relation',\n'MFALLEEN':   'Singles',\n'MFGEKIND':   'Household_without_children',\n'MFWEKIND':   'Household_with_children',\n'MOPLHOOG':   'High_level_education',\n'MOPLMIDD':   'Medium_level_education',\n'MOPLLAAG':   'Lower_level_education',\n'MBERHOOG':   'High_status',\n'MBERZELF':   'Entrepreneur',\n'MBERBOER':   'Farmer',\n'MBERMIDD':   'Middle_management',\n'MBERARBG':   'Skilled_labourers',\n'MBERARBO':   'Unskilled_labourers',\n'MSKA':       'Social_class_A',\n'MSKB1':      'Social_class_B1',\n'MSKB2':      'Social_class_B2',\n'MSKC':       'Social_class_C',\n'MSKD':       'Social_class_D',\n'MHHUUR':     'Rented_house',\n'MHKOOP':     'Home_owners',\n'MAUT1':      '1_car',\n'MAUT2':      '2_cars',\n'MAUT0':      'No_car',\n'MZFONDS':    'National_Health_Service',\n'MZPART':     'Private_health_insurance',\n'MINKM30':    'Income_<_30000',\n'MINK3045':   'Income_30-45000',\n'MINK4575':   'Income_45-75000',\n'MINK7512':   'Income_75-122000',\n'MINK123M':   'Income_>123000',\n'MINKGEM':    'Average_income',\n'MKOOPKLA':   'Purchasing_power_class',\n'PWAPART':    'Contribution_private_third_party_insurance',\n'PWABEDR':    'Contribution_third_party_insurance_firms',\n'PWALAND':    'Contribution_third_party_insurane_agriculture',\n'PPERSAUT':   'Contribution_car_policies',\n'PBESAUT':    'Contribution_delivery_van_policies',\n'PMOTSCO':    'Contribution_motorcycle-scooter_policies',\n'PVRAAUT':   'Contribution_lorry_policies',\n'PAANHANG':   'Contribution_trailer_policies',\n'PTRACTOR':   'Contribution_tractor_policies',\n'PWERKT':     'Contribution_agricultural_machines_policies',\n'PBROM':      'Contribution_moped_policies',\n'PLEVEN':     'Contribution_life_insurances',\n'PPERSONG':   'Contribution_private_accident_insurance_policies',\n'PGEZONG':    'Contribution_family_accidents_insurance_policies',\n'PWAOREG':   'Contribution_disability_insurance_policies',\n'PBRAND':     'Contribution_fire_policies',\n'PZEILPL':    'Contribution_surfboard_policies',\n'PPLEZIER':   'Contribution_boat_policies',\n'PFIETS':     'Contribution_bicycle_policies',\n'PINBOED':   'Contribution_property_insurance_policies',\n'PBYSTAND':  'Contribution_social_security_insurance_policies',\n'AWAPART':   'Number_of_private_third_party_insurance_1-12',\n'AWABEDR':   'Number_of_third_party_insurance_firms',\n'AWALAND':   'Number_of_third_party_insurance_agriculture',\n'APERSAUT':  'Number_of_car_policies',\n'ABESAUT':   'Number_of_delivery_van_policies',\n'AMOTSCO':   'Number_of_motorcycle-scooter_policies',\n'AVRAAUT':   'Number_of_lorry_policies',\n'AAANHANG':  'Number_of_trailer_policies',\n'ATRACTOR':  'Number_of_tractor_policies',\n'AWERKT':   'Number_of_agricultural_machines_policies',\n'ABROM':      'Number_of_moped_policies',\n'ALEVEN':     'Number_of_life_insurances',\n'APERSONG':  'Number_of_private_accident_insurance_policies',\n'AGEZONG':   'Number_of_family_accidents_insurance_policies',\n'AWAOREG':   'Number_of_disability_insurance_policies',\n'ABRAND':     'Number_of_fire_policies',\n'AZEILPL':   'Number_of_surfboard_policies',\n'APLEZIER':  'Number_of_boat_policies',\n'AFIETS':     'Number_of_bicycle_policies',\n'AINBOED':   'Number_of_property_insurance_policies',\n'ABYSTAND':  'Number_of_social_security_insurance_policies',\n'CARAVAN':   'Number_of_mobile_home_policies_0-1'\n                        }\ncarvan.rename(\n         columns = carvan_column_names,\n         inplace = True\n         )\ncarvan['Number_of_mobile_home_policies_0-1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset is Highly Imbalanced\nWhile analysing the target variable column CARAVAN we find dataset is highly imbalance as huge difference in count of target variable i.e.\nCount of Target Variable Caravan with 0 = 9236\nCount of Target Variable Caravan with 1 =  586 \nIn this case accuracy is not very useful matrics. We have to work on confusion matrics and analyse performance better."},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.2 Divide the Data into Training and Testing Data. \n#After division Carvan_train is my training dataset and Carvan_test is my testing dataset.\ncarvan_train = carvan.loc[carvan['Datacategory'] == 'train']\ncarvan_train = carvan_train.drop(['Datacategory'],axis =1)\ncarvan_test = carvan.loc[carvan['Datacategory'] == 'test']\ncarvan_test = carvan_test.drop(['Datacategory'],axis =1)\n\n#2.3 Divide the carvan_train and carvan_test data into X_train,y_train,X_test and y_test\n\ny_train = carvan_train.pop(\"Number_of_mobile_home_policies_0-1\")\nX_train = carvan_train\ny_test = carvan_test.pop(\"Number_of_mobile_home_policies_0-1\")\nX_test = carvan_test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking of Unique Values in Dataset -** We analysed whole dataset consists of Categorical Variables from unique values in range 2 to 40. Since dataset has all the unique values in the range of 2 to 40, so we treat all the variables as Categorical in nature and not Numerical as dataset actually all the variables are categorical.\n\n**Usage of Target Encoder instead of One Hot Encoder**\nSince all the variables in this database of type categorical range from 2 to 40 unique values, we use Target Encoder in our notebook as One Hot Encoder increase the size of Database to very high extent."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"colunique=X_train.nunique\ncols= (X_train.nunique() < 41 )\ncols\ncat_cols = cols[cols==True].index.tolist()\nnum_cols = cols[cols==False].index.tolist()\ncat_cols\nnum_cols\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysing the Data using Decision Tree Classification Model** The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.5 First Doing decision Tree Classification we find the score of dataset.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth=6)\ndt.fit(X_train,y_train)\ny_predict_dt = dt.predict(X_test)\nscore_dt =np.sum(y_predict_dt ==y_test)/len(y_test)\nscore_dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysing the dataset using Random Forest Classification Model -** Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our modelâ€™s prediction For data cleaning and random forest we are using Pipeling concept in this model and encoding done using Target Encoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6 Since All the Columns in this Dataset is of type Categorical having nunique values less than 50 \n#we now use Target Encoder for better results.\nimport pandas as pd\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import StandardScaler\nct = ColumnTransformer([('cde',TargetEncoder(),cat_cols)],remainder =\"passthrough\")\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf= RandomForestClassifier(n_estimators =400,oob_score = True,bootstrap=True)    \n#2.5 Define Pipeline\npipe_rf = Pipeline([('ct',ct),('rf',rf)])\npipe_rf.fit(X_train,y_train)\ny_predict_rf = pipe_rf.predict(X_test)\nscore_rf =np.sum(y_predict_rf ==y_test)/len(y_test)\nscore_rf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6 Evaluate the performance of model by Confusion Metrics and Classification Report\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\ncf_rf =confusion_matrix(y_test,y_predict_rf)\ncf_rf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cr_rf = classification_report(y_test,y_predict_rf)\ncr_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysing the dataset using Ada Boost Classifier Model** - Ada Boost classification model is a favourite model of most of Kaggle competitions and give very robust results.Ada Boost means adaptive boosting algorithm it focusses on classification problem and aims to convert set of week classifier into strong ones. In this model we do a series of predictions and next prediction uses errors happens in previous one. This process iterates for number of estimators given. In this output of this model is better."},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.0 Evaluating the Dataset Using Adaboost Classifier Model\n\n#3.1 Import Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n#Import AdaBoost Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\nct = ColumnTransformer([('cde',TargetEncoder(),cat_cols)],remainder =\"passthrough\")\n\n#Instantiate Decision Tree Classifier\ndt = DecisionTreeClassifier(max_depth =1,random_state =1) \n\n#Instantiate AdaBoost Classifier\nada = AdaBoostClassifier(base_estimator =dt,n_estimators =100,random_state = 1)\n\n#3.1 Define Pipeline\npipe_ada = Pipeline([('ct',ct),('ada',ada)])\npipe_ada.fit(X_train,y_train)\ny_predict_ada = pipe_ada.predict(X_test)\nscore_ada =np.sum(y_predict_ada ==y_test)/len(y_test)\nscore_ada\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6 Evaluate the performance of model by Confusion Metrics and Classification Report for AdaBoost Model\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\ncf_ada =confusion_matrix(y_test,y_predict_ada)\ncf_ada\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cr_ada = classification_report(y_test,y_predict_ada)\ncr_ada","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting Model -** This model is also a boosting model is a tree based model. In this model prediction done on n number of trees and error of predition done on each tree is used in next prediction tree. This algorithm gives best results and favourite in lot of Kaggle competitions."},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.0 Evaluating the Dataset Using Gradiant Boosting Model\n\n#Import and Instantiate Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators = 100,max_depth = 1, random_state =2)\npipe_gb = Pipeline([('ct',ct),('gb',gb)])\npipe_gb.fit(X_train,y_train)\ny_predict_gb = pipe_gb.predict(X_test)\nscore_gb =np.sum(y_predict_gb ==y_test)/len(y_test)\nscore_gb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6 Evaluate the performance of model by Confusion Metrics and Classification Report using Gradient Boosting Model\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\ncf_gb =confusion_matrix(y_test,y_predict_gb)\ncf_gb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cr_gb = classification_report(y_test,y_predict_gb)\ncr_gb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generation of ROC Curve and AUC Curve Using Logistic Regression ** \nWe are ananlysing this classification problem using Logestic Regression technique and find the scores of ROC and AUC."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# 2.7 Generating ROC Curve\nimport pandas as pd\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\n\nct = ColumnTransformer([('cde',TargetEncoder(),cat_cols)],remainder =\"passthrough\")\nlogreg = LogisticRegression(max_iter =5000)\n#2.5 Define Pipeline\npipe_logreg = Pipeline([('ct',ct),('logreg',logreg)])\n\npipe_logreg.fit(X_train,y_train)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Computing predicted probablities of this dataset\ny_predictprob_lr = logreg.predict_proba(X_test)[:,1]\n\n#Generate ROC Curve\n\nfpr_lr,tpr_lr,thresholds = roc_curve(y_test,y_predictprob_lr)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_lr,tpr_lr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding of Area Under the Curve (AUC score) of the metrics\nfrom sklearn.metrics import roc_auc_score\n\nauc = roc_auc_score(y_test,y_predictprob_lr)\nauc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations of AUC curve**\nWe observe that AUC curve have results more than 72% gives quite good for Logistics Regresssion."},{"metadata":{},"cell_type":"markdown","source":"**Finding Important Features using Random Forest Classifier and Important Features model.** Through this step we analyse the important features of Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#5.0 Finding Important Features of Carvan Dataset.\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot\n# define dataset\nX_train, y_train = make_classification(n_samples=1000, n_features=100, n_informative=5, n_redundant=5, random_state=1)\n# define the model\nmodel = RandomForestClassifier()\n# fit the model\nmodel.fit(X_train, y_train)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\npyplot.bar([x for x in range(len(importance))], importance)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final Conclusion** Carvan Insurance problem is realtime insurance classification problem with an objective to learn insurance dataset and find result of target column carvan. This is a supervised learning classification problem. We analysed this problem using various tree based models and we are using pipelining concept in this model to provide numerical data for a model.\n\nWhile analysing with four model Models i.e. Decision Tree Classifier, Random Forest Model, Adaboost Classifier Model and Gradient Boosting Model we observed results of all the models are more than 90 percent and Gradient Boosting Model gives best results.\n\nAnother important step in this notebook we are analysing important features in dataset using Random Forest classifier."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}