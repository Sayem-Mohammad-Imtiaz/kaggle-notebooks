{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=+3 color=\"#3D3D3D\"><center><b>Predicting Customer Churn using Machine Learning üè¶üí∞</b></center></font>\n\n<img src=\"https://images.unsplash.com/photo-1549728662-1499eff84059?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80\" width = 300>\n<center><em>Photo by Dustin Tramel (Unsplash)</em></center>\n\n**Table of Contents**\n\n1. [Introduction](#Introduction)\n2. [Objective](#Objective)\n3. [Libraries and Functions](#Libraries-and-Functions)\n4. [A Quick Look at our Data](#A-Quick-Look-at-our-Data)\n5. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n6. [Data Preprocessing](#Data-Preprocessing)\n7. [Building Machine Learning Models](#Building-Machine-Learning-Models)\n8. [Results](#Results)\n9. [Evaluating the Test Set](#Evaluating-the-Test-Set)\n10. [Bibliography](#Bibliography)\n11. [Future Development](#Future-Development)\n12. [Conclusions](#Conclusions)\n\n\n# Introduction\n\nCustomer churn (also known as customer attrition) occurs when a customer stops using a company's products or services. \n\nCustomer churn affects profitability, especially in industries where revenues are heavily dependent on subscriptions (e.g. banks, telephone and internet service providers, pay-TV companies, insurance firms, etc.) [[1](https://www.investopedia.com/terms/c/churnrate.asp), [2](https://en.wikipedia.org/wiki/Customer_attrition)]. It is estimated that acquiring a new customer can cost up to five times more than retaining an existing one [[3](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.278.4171&rep=rep1&type=pdf)].\n\nTherefore, customer churn analysis is essential as it can help a business:\n\n- identify problems in its services (e.g. poor quality product/service, poor customer support, wrong target audience, etc.), and \n- make correct strategic decisions that would lead to higher customer satisfaction and consequently higher customer retention [[4](https://www.optimove.com/resources/learning-center/customer-attrition)]. "},{"metadata":{},"cell_type":"markdown","source":"# Objective\n\nThe goal of this notebook is to understand and predict customer churn for a bank. Specifically, we will initially perform **Exploratory Data Analysis** (**EDA**) to identify and visualise the factors that contribute to customer churn. This analysis will later help us build **Machine Learning** models to predict whether a customer will churn or not. \n\nThis is a typical **classification** problem. The task does not specify which performance metric to use for optimising our machine learning models. I decided to use **recall** since correctly classifying elements of the positive class (customers who churned) is more important for the bank.\n\n<br>\n\n*Skills: Exploratory Data Analysis, Data Visualisation, Data Preprocessing (Feature Selection, Encoding Categorical Features, Feature Scaling), Addressing Class Imbalance (SMOTE), Model Tuning.*\n\n*Models Used: Logistic Regression, Support Vector Machines, Random Forests, Gradient Boosting, XGBoost, and Light Gradient Boosting Machine.*\n"},{"metadata":{},"cell_type":"markdown","source":"# Libraries and Functions\n\nWe start by importing the necessary libraries and setting some parameters for the whole notebook (such as parameters for the plots, etc.). We will mainly use:\n\n- Pandas for handling and analysing data,\n- Seaborn and Matplotlib for data visualization, and\n- Scikit-learn for building Machine Learning models."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:53.59513Z","start_time":"2020-12-21T11:41:45.797511Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action = 'ignore', category = FutureWarning)\n\nimport numpy as np\nimport pandas as pd\npd.set_option('precision', 3)\npd.options.mode.chained_assignment = None\n\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom scipy.stats import chi2_contingency\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, learning_curve\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, auc, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport scikitplot as skplt\n\nlabel_size = 17\n\nplt.rcParams['axes.labelsize'] = label_size\nplt.rcParams['xtick.labelsize'] = label_size - 2\nplt.rcParams['ytick.labelsize'] = label_size - 2\nplt.rcParams['axes.titlesize'] = label_size\nplt.rcParams['legend.fontsize'] = label_size - 2\n\nrandom_state = 42\nscoring_metric = 'recall'\ncomparison_dict = {}\ncomparison_test_dict = {}\n\nprint ('Libraries Loaded!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will repeatedly use some functions which will be defined now:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:53.647209Z","start_time":"2020-12-21T11:41:53.599096Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_continuous(feature):\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n    \n    sns.distplot(df_remained[feature], bins = 15, color = colors[0], label = 'Remained', hist_kws = dict(edgecolor = 'firebrick', linewidth = 1), ax = ax1, kde = False)\n    sns.distplot(df_churned[feature], bins = 15, color = colors[1], label = 'Churned', hist_kws = dict(edgecolor = 'firebrick', linewidth = 1), ax = ax1, kde = False)\n    ax1.set_title('{} distribution - Histogram'.format(feature))\n    ax1.set_ylabel('Counts')\n    ax1.legend()\n\n    sns.boxplot(x = 'Exited', y = feature, data = train_df, palette = colors, ax = ax2)\n    ax2.set_title('{} distribution - Box plot'.format(feature))\n    ax2.set_xlabel('Status')\n    ax2.set_xticklabels(['Remained', 'Churned'])\n\n    plt.tight_layout();\n    \n    \ndef plot_categorical(feature):\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n    sns.countplot(x = feature, hue = 'Exited', data = train_df, palette = colors, ax = ax1)\n    ax1.set_ylabel('Counts')\n    ax1.legend(labels = ['Retained', 'Churned'])\n    \n    sns.barplot(x = feature, y = 'Exited', data = train_df, palette = colors2 , ci = None, ax = ax2)\n    ax2.set_ylabel('Churn rate')\n    \n    if (feature == 'HasCrCard' or feature == 'IsActiveMember'):\n        ax1.set_xticklabels(['No', 'Yes'])\n        ax2.set_xticklabels(['No', 'Yes'])\n    \n    plt.tight_layout();\n    \ndef plot_learning_curve(estimator, estimator_name, X, y, cv = None, train_sizes = np.linspace(0.1, 1.0, 5)):\n                 \n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv = cv, n_jobs = -1, \n                                                            train_sizes = train_sizes, scoring = 'accuracy')\n    \n    train_scores_mean, train_scores_std = np.mean(train_scores, axis = 1), np.std(train_scores, axis = 1)\n    test_scores_mean, test_scores_std = np.mean(test_scores, axis = 1), np.std(test_scores, axis = 1)\n            \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha = 0.1, color = 'dodgerblue')\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha = 0.1, color = 'darkorange')\n    \n    plt.plot(train_sizes, train_scores_mean, color = 'dodgerblue', marker = 'o', linestyle = '-', label = 'Training Score')\n    plt.plot(train_sizes, test_scores_mean, color = 'darkorange', marker = 'o', linestyle = '-', label = 'Cross-validation Score')\n    plt.title(estimator_name)\n    plt.xlabel('Training Examples')\n    plt.ylabel('Accuracy Score')\n    plt.legend(loc = 'best')\n            \n    plt.tight_layout();\n    \ndef plot_conf_mx(cm, classifier_name, ax):\n    sns.heatmap(cm, annot = True, cmap = 'Blues', annot_kws = {'fontsize': 24}, ax = ax)\n    ax.set_title('{}'.format(classifier_name))\n    ax.set_xlabel('Predicted Label')\n    ax.set_xticks([0.5, 1.5])\n    ax.set_xticklabels(['Remained', 'Churned'])\n    ax.set_ylabel('True Label') \n    ax.set_yticks([0.25, 1.25])\n    ax.set_yticklabels(['Remained', 'Churned']);\n    \ndef plot_feature_imp(classifier, classifier_name, color, ax):\n\n    importances = pd.DataFrame({'Feature': X_train.columns,\n                                'Importance': np.round(classifier.best_estimator_.feature_importances_, 3)})\n\n    importances = importances.sort_values('Importance', ascending = True).set_index('Feature')\n\n    importances.plot.barh(color = color, edgecolor = 'firebrick', legend = False, ax = ax)\n    ax.set_title(classifier_name)\n    ax.set_xlabel('Importance');\n    \ndef clf_performance(classifier, classifier_name, classifier_name_abv):\n    print('\\n', classifier_name)\n    print('-------------------------------')\n    print('   Best Score ({}): '.format(scoring_metric) + str(np.round(classifier.best_score_, 3)))\n    print('   Best Parameters: ')\n    for key, value in classifier.best_params_.items() :\n        print ('      {}: {}'.format(key, value))\n    \n    y_pred_pp = cross_val_predict(classifier.best_estimator_, X_train, y_train, cv = 5, method = 'predict_proba')[:, 1]\n    y_pred = y_pred_pp.round()\n    \n    cm = confusion_matrix(y_train, y_pred, normalize = 'true')\n    \n    fpr, tpr, _ = roc_curve(y_train, y_pred_pp)\n    \n    comparison_dict[classifier_name_abv] = [accuracy_score(y_train, y_pred), \n                                            precision_score(y_train, y_pred),\n                                            recall_score(y_train, y_pred),\n                                            roc_auc_score(y_train, y_pred_pp),\n                                            fpr, tpr]    \n\n    fig, ax = plt.subplots(figsize = (5, 4))\n    \n    plot_conf_mx(cm, '', ax)    \n    plt.tight_layout();\n    \ndef test_func(classifier, classifier_name):\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred, normalize = 'true')\n    \n    comparison_test_dict[classifier_name] = [accuracy_score(y_test, y_pred), \n                                             precision_score(y_test, y_pred),\n                                             recall_score(y_test, y_pred)]\n    \n    plt.title(classifier_name)\n    sns.heatmap(cm, annot = True, annot_kws = {'fontsize': 18}, cmap = 'Blues')\n    plt.xlabel('Predicted Label')\n    plt.xticks([0.5, 1.5], ['Remained', 'Churned'])\n    plt.ylabel('True Label') \n    plt.yticks([0.2, 1.4], ['Remained', 'Churned']);\n    \nprint ('Functions defined!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A Quick Look at our Data\n\nWe start by importing the dataset into a Pandas DataFrame. We can also take a look at the top five rows using the `head()` method:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:53.723097Z","start_time":"2020-12-21T11:41:53.652168Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv')\n\nprint ('This dataset contains {} rows and {} columns.'.format(df.shape[0], df.shape[1]))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataframe has 14 features and 10K customers/instances. The last feature, '**Exited**', is the **target variable** and indicates whether the customer has churned (0 = No, 1 = Yes). The meaning of the rest of the features can be easily inferred from their name.\n\nThe `info()` method can give us valuable information such as the number of non-null values and the type of each feature:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:53.757818Z","start_time":"2020-12-21T11:41:53.732521Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thankfully, there are **no** missing values in our dataset. Columns 'RowNumber', 'CustomerID' and 'Surname' are specific to each customer and can be dropped: "},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:53.773192Z","start_time":"2020-12-21T11:41:53.763272Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1, inplace = True)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `describe()` method gives us a statistical summary of the numerical features:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:53.862978Z","start_time":"2020-12-21T11:41:53.780632Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most important things to note are:\n\n- The age of customers ranges from 18 to 92, with a mean value approximately equal to 40,\n- The mean (and median) tenure is 5 years, so the majority of customers is loyal (tenure > 3), and\n- Approximately 50% of customers are active.\n\nEDA will help us understand our dataset better. However, before we look at the data any further, we need to create a **test set**, put it aside, and use it only for evaluating our Machine Learning models. This ensures that evaluation will be performed using unseen data and protects our models from *data snooping bias* (you can read more on page 51 of [[1](#Bibliography)]).\n\n## Creating a Test Set\n\nWe will split our dataset into a train and test set by using scikit-learn's `train_test_split()` function, which implements random sampling. Our dataset is large enough (especially relative to the number of features), so we do **not** risk introducing *sampling bias*."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:53.883305Z","start_time":"2020-12-21T11:41:53.866442Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size = 0.2, random_state = 42)\n\ntrain_df.reset_index(drop = True, inplace = True)\ntest_df.reset_index(drop = True, inplace = True)\n\nprint('Shape')\nprint('Train set: {} rows x {} columns'.format(train_df.shape[0], train_df.shape[1]))\nprint(' Test set: {} rows x {} columns'.format(test_df.shape[0], test_df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n## Target Variable: Exited\n\nAs we mentioned earlier, our target variable is already encoded:\n\n- Exited = 0, for non-churned customers, and\n- Exited = 1, for churned customers."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:54.420473Z","start_time":"2020-12-21T11:41:53.888266Z"},"code_folding":[],"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"colors = ['#00A5E0', '#DD403A']\n\nfig = plt.figure(figsize = (5, 5))\nsns.countplot(x = 'Exited', data = train_df, palette = colors)\n\nfor index, value in enumerate(train_df['Exited'].value_counts()):\n    label =  '{}%'.format(round( (value/train_df['Exited'].shape[0])*100, 2)) \n    plt.annotate(label, xy = (index - 0.18, value - 800), color = 'w', fontweight = 'bold', size = label_size)\n\nplt.title('Number of Retained and Churned Customers')\nplt.xticks([0, 1], ['Remained', 'Churned'])\nplt.xlabel('Status')\nplt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bank kept 80% of its clientele. \n\nNotice that our dataset is **skewed/imbalanced** since the number of instances in the 'Remained' class outnumbers the number of instances in the 'Churned' class by a lot. Therefore, accuracy is probably not the best metric for model performance. \n\n<br>\n\nDifferent visualisation techniques apply to different types of variables, so it's useful to differentiate between continuous and categorical variables and look at them separately."},{"metadata":{},"cell_type":"markdown","source":"## Continuous Variables\n\nBy calling the `hist()` method we can plot a histogram for each of the four continuous numeric features:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:56.014618Z","start_time":"2020-12-21T11:41:54.426426Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"continuous = ['Age', 'CreditScore', 'Balance', 'EstimatedSalary']\ntrain_df[continuous].hist(figsize = (10, 8), bins = 20, layout = (2, 2), \n                          color = 'steelblue', edgecolor = 'firebrick', linewidth = 1.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'Age' is slightly tail-heavy, i.e. it extends more further to the right of the median than to the left,\n- Most values for 'CreditScore' are above 600,\n- If we ignore the fist bin, 'Balance' follows a fairly normal distribution, and\n- The distribution of 'EstimatedSalary' is more or less uniform and provides little information.\n\n### Looking for Correlations\n\nWe can compute the standard correlation coefficient between every pair of (continuous) features using the pandas' `corr()` method and plot it as a matrix:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:56.922794Z","start_time":"2020-12-21T11:41:56.019082Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (7, 6))\nax = sns.heatmap(train_df[continuous].corr(), annot = True, annot_kws = {'fontsize': 16}, cmap = 'Blues')\n\nax.tick_params(axis = 'x', rotation = 45)\nax.tick_params(axis = 'y', rotation = 360);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant intercorrelation between our features, so we do **not** have to worry about multicollinearity.\n\nLet's look at these features in greater detail."},{"metadata":{},"cell_type":"markdown","source":"### Age"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:58.093355Z","start_time":"2020-12-21T11:41:56.926763Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_churned = train_df[train_df['Exited'] == 1]\ndf_remained = train_df[train_df['Exited'] == 0]\n\nplot_continuous('Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, there is a clear difference between age groups since older customers are more likely to churn. This could potentially indicate that preferences change with age, and the bank hasn't adapted its strategy to meet the requirements of older customers."},{"metadata":{},"cell_type":"markdown","source":"### Credit Score"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:41:59.279788Z","start_time":"2020-12-21T11:41:58.100795Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_continuous('CreditScore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant difference between retained and churned customers in terms of their credit score. "},{"metadata":{},"cell_type":"markdown","source":"### Balance"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:00.361068Z","start_time":"2020-12-21T11:41:59.286236Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_continuous('Balance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the two distributions are quite similar. There is a big percentage of non-churned customers with a low account balance."},{"metadata":{},"cell_type":"markdown","source":"### Estimated Salary"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:01.589663Z","start_time":"2020-12-21T11:42:00.368509Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_continuous('EstimatedSalary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both churned and retained customers display a similar uniform distribution for their salary. Consequently, we can conclude that salary doesn't have a significant effect on the likelihood to churn."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Variables\n\nLet's plot a countplot for each categorical feature:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:03.594991Z","start_time":"2020-12-21T11:42:01.594125Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"colors2 = ['#B0A8B9', '#C34A36', '#4B4453', '#845EC2', '#FF8066', '#D5CABD', '#38c4e3', '#8f9aaa', '#d4cebb',  '#63BAAA', '#9D88B3']\n\ncat_vars = ['Geography', 'Gender', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']\ndf_cat = train_df[cat_vars]\n\nfig, ax = plt.subplots(2, 3, figsize = (12, 8))\n\nfor index, column in enumerate(df_cat.columns):\n\n    plt.subplot(2, 3, index + 1)\n    sns.countplot(x = column, data = train_df, palette = colors2)\n    \n    plt.ylabel('Count')\n    if (column == 'HasCrCard' or column == 'IsActiveMember'): \n        plt.xticks([0, 1], ['No', 'Yes'])\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Important points:\n\n- The bank has customers in three countries (France, Spain, and Germany). Most customers are in France.\n- There are more male customers than females,\n- Only a small percentage leaves within the first year. The count of customers in tenure years between 1 and 9 is almost the same,\n- Most of the customers have purchased 1 or 2 products, while a small portion has purchased 3 and 4,\n- A significant majority of customers has a credit card, and\n- Almost 50% of customers are not active.\n\nAgain, we will look at these features in greater detail."},{"metadata":{},"cell_type":"markdown","source":"### Geography"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:04.233342Z","start_time":"2020-12-21T11:42:03.59747Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_categorical('Geography')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers in Germany are more likely to churn than customers in the other two countries (the churn rate is almost double compared to Spain and France). Many reasons could explain this finding such as higher competition or different preferences for German customers."},{"metadata":{},"cell_type":"markdown","source":"### Gender"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:04.795806Z","start_time":"2020-12-21T11:42:04.239295Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_categorical('Gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Female customers are more likely to churn."},{"metadata":{},"cell_type":"markdown","source":"### Tenure"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:05.743169Z","start_time":"2020-12-21T11:42:04.798287Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_categorical('Tenure')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of years (tenure) doesn't seem to affect the churn rate."},{"metadata":{},"cell_type":"markdown","source":"### Number of Products"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:06.362672Z","start_time":"2020-12-21T11:42:05.746143Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_categorical('NumOfProducts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, having 3 or 4 products significantly increases the likelihood of churn. I not sure how to interpret this result. It could potentially mean that the bank is unable to properly support customers with more products which in turn increases customer dissatisfaction. "},{"metadata":{},"cell_type":"markdown","source":"### Card Holders"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:06.949935Z","start_time":"2020-12-21T11:42:06.365647Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_categorical('HasCrCard')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having a credit card doesn't seem to affect the churn rate."},{"metadata":{},"cell_type":"markdown","source":"### Active Members"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.502519Z","start_time":"2020-12-21T11:42:06.952911Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_categorical('IsActiveMember')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's not a surprise that inactive customers are more likely to churn. A big portion of the clientele is inactive; therefore, the bank will definitely benefit from changing its policy so that more customers become active. "},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\nData preprocessing is the process of converting raw data into a well-readable format that is suitable for building and training Machine Learning models.\n\nLet's complete this process step-by-step:\n\n## Feature Selection\n\nWe have already performed feature selection by dropping columns 'RowNumber', 'CustomerId', and 'Surname' at the beginning of our notebook. EDA revealed several more features that can be dropped as they do not provide any value in predicting our target variable:\n\n- 'EstimatedSalary' displays a uniform distribution for both types of customers and can be dropped. \n- The categories in 'Tenure' and 'HasCrCard' have a similar churn rate and are deemed redundant. This can be confirmed from a chi-square test [[2](#Bibliography)]:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.62648Z","start_time":"2020-12-21T11:42:07.509424Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"chi2_array, p_array = [], []\nfor column in cat_vars:\n    \n    crosstab = pd.crosstab(train_df[column], train_df['Exited'])\n    chi2, p, dof, expected = chi2_contingency(crosstab)\n    chi2_array.append(chi2)\n    p_array.append(p)\n\ndf_chi = pd.DataFrame({'Variable': cat_vars, \n                       'Chi-square': chi2_array, \n                       'p-value': p_array})\ndf_chi.sort_values(by = 'Chi-square', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Tenure' and 'HasCrCard' have a small chi-square and a p-value greater than 0.05 (the standard cut-off value), which confirms our initial hypothesis that these two features do not convey any useful information.\n\nWe can use the `drop()` method to remove these three features from the train set:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.643344Z","start_time":"2020-12-21T11:42:07.633426Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features_drop = ['Tenure', 'HasCrCard', 'EstimatedSalary']\ntrain_df.drop(features_drop, axis = 1, inplace = True) \n\nprint ('Features dropped!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Categorical Features\n\nMachine learning algorithms usually require that all input (and output) features are numeric. Consequently, categorical features need to be converted (encoded) to numbers before using them for building models. \n\nOur dataset contains two features that require encoding. \n\n- For 'Gender', we will use scikit-learn's `LabelEncoder()` which maps each unique label to an integer (Male --> 1 and Female --> 0). \n- For 'Geography', we will manually map values so that customers in Germany have the value of 1 and all other customers (France and Spain) have zero. I chose this method since the churn rate for customers in the other two countries is almost equal and considerably lower than in Germany. Therefore, it makes sense to encode this feature so that it differentiates between German and non-German customers. Additionally, I tried one-hot encoding (`get_dummies()`) this feature, and the two new features for France and Spain had small feature importance. "},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.665664Z","start_time":"2020-12-21T11:42:07.649296Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_df['Gender'] = LabelEncoder().fit_transform(train_df['Gender'])\n\ntrain_df['Geography'] = train_df['Geography'].map({'Germany': 1, 'Spain': 0, 'France': 0})\n\nprint ('Features encoded!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling\n\nFeature scaling is a technique used to normalise the range of features in a dataset. Some algorithms are sensitive to feature scaling (e.g. SVMs) while others are invariant to it (e.g. Random Forests). \n\nI decided to use `StandardScaler()` which standardises features by subtracting the mean and dividing by the standard deviation. This results in features with zero mean and unit variance."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.698895Z","start_time":"2020-12-21T11:42:07.669137Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nscl_columns = ['CreditScore', 'Age', 'Balance']\ntrain_df[scl_columns] = scaler.fit_transform(train_df[scl_columns])\n\nprint ('Features scaled!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will split the train set into 'X_train' and 'y_train' sets:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.712784Z","start_time":"2020-12-21T11:42:07.701378Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"y_train = train_df['Exited']\nX_train = train_df.drop('Exited', 1)\n\nprint ('Sets created!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Addressing Class Imbalance\n\nAs we have seen previously, there is an imbalance in the classes to be predicted, with one class (0 ‚Äì remained) much more prevalent than the other (1 - churned):"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.727666Z","start_time":"2020-12-21T11:42:07.716753Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"before_sm = Counter(y_train)\nprint(before_sm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Class imbalance is usually a problem and occurs in many real-world tasks. Classification using imbalanced data is biased in favor of the majority class, meaning that machine learning algorithms will likely result in models that do little more than predict the most common class. Additionally, common metrics can be misleading when handling class-imbalanced data (e.g. if a dataset contains 99.9% 0s and 0.01% 1s, a classifier that always predicts 0 will have 99.9% accuracy).\n\nThankfully, some strategies can address this problem. I decided to use the SMOTE ('Synthetic Minority Oversampling Technique') algorithm which, as we read in [[2](#Bibliography)], <br>\n'*finds a record that is similar to the record being upsampled and creates a synthetic record that is a randomly weighted average of the original record and the neighboring record, where the weight is generated separately for each predictor*'.\n\nI‚Äôll use the `SMOTE` function from [imblearn](https://imbalanced-learn.readthedocs.io/en/stable/api.html) with the `sampling_strategy` set to 'auto'."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:07.783715Z","start_time":"2020-12-21T11:42:07.731632Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"over = SMOTE(sampling_strategy = 'auto', random_state = 42)\nX_train, y_train = over.fit_resample(X_train, y_train)\n\nafter_sm = Counter(y_train)\nprint(after_sm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Machine Learning Models\n\nWe are now ready to start building machine learning models. The 6 classifiers I have selected are the following:\n\n1) [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), <br>\n2) [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), <br>\n3) [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), <br> \n4) [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), <br>\n5) [Xtreme Gradient Boosting Classifier](https://xgboost.readthedocs.io/en/latest/), and <br>\n6) [Light Gradient Boosting Machine](https://lightgbm.readthedocs.io/en/latest/).\n\nI won't go into detail about how these algorithms work. You can read more in [[1](#Bibliography)] or the corresponding documentation.\n\nUsing default hyperparameters usually results in non-optimised models that overfit or underfit the dataset. **Hyperparameter tuning** is the process of finding the set of hyperparameter values that achieves optimal performance. For this purpose, we will first define which hyperparameters we want to experiment with, and what values to try out. We will pass this information to Scikit-Learn‚Äôs `GridSearchCV` which then evaluates all the possible combinations of hyperparameter values. As mentioned in the [Objective](#Objective), **recall** will be used as the scoring metric for optimising our models. \n\n`GridSearchCV` evaluates performance by performing **k-fold cross-validation**. The idea behind k-fold cross-validation, which is illustrated in [this figure](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png), is simple: it splits the (training) set into k subsets/folds, trains the models using k-1 folds, and evaluates the model on the remaining one fold. This process is repeated until every fold is tested once."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:42:15.120051Z","start_time":"2020-12-21T11:42:07.786689Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state = random_state)\n\nparam_grid = {'max_iter' : [100],\n              'penalty' : ['l1', 'l2'],\n              'C' : [0.001, 0.01, 0.1, 1, 10],\n              'solver' : ['lbfgs', 'liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, scoring = scoring_metric, \n                      cv = 5, verbose = False, n_jobs = -1)\n\nbest_clf_lr = clf_lr.fit(X_train, y_train)\nclf_performance(best_clf_lr, 'Logistic Regression', 'LR')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Classifier"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:49:47.304661Z","start_time":"2020-12-21T11:42:15.123029Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"svc = SVC(probability = True, random_state = random_state)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'],\n                                  'gamma': ['scale', 'auto'],\n                                  'C': [.1, 1, 2]},\n                                 {'kernel': ['linear'], \n                                  'C': [.1, 1, 10]}]\n\nclf_svc = GridSearchCV(svc, param_grid = param_grid, scoring = scoring_metric, cv = 5, verbose = False, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train, y_train)\nclf_performance(best_clf_svc, 'Support Vector Classifier', 'SVC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:50:19.781314Z","start_time":"2020-12-21T11:49:47.307609Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = random_state)\nparam_grid = {'n_estimators': [50],\n              'criterion': ['entropy', 'gini'],\n              'bootstrap': [True],\n              'max_depth': [6],\n              'max_features': ['auto','sqrt'],\n              'min_samples_leaf': [2, 3, 5],\n              'min_samples_split': [2, 3, 5]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, scoring = scoring_metric, cv = 5, verbose = False, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train, y_train)\nclf_performance(best_clf_rf, 'Random Forest Classifier', 'RF')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting Classifier"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:52:45.644366Z","start_time":"2020-12-21T11:50:19.784182Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state = random_state)\nparam_grid = {'n_estimators': [600],\n              'subsample': [0.66, 0.75],\n              'learning_rate': [0.001, 0.01],\n              'max_depth': [3],                       # default=3\n              'min_samples_split': [5, 7],\n              'min_samples_leaf': [3, 5],\n              'max_features': ['auto', 'log2', None],\n              'n_iter_no_change': [20],\n              'validation_fraction': [0.2],\n              'tol': [0.01]}\n                                  \nclf_gbc = GridSearchCV(gbc, param_grid = param_grid, scoring = scoring_metric, cv = 5, verbose = False, n_jobs = -1)\nbest_clf_gbc = clf_gbc.fit(X_train, y_train)\nclf_performance(best_clf_gbc, 'Gradient Boosting Classifier', 'GBC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of estimators after early stopping is: "},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:52:45.653788Z","start_time":"2020-12-21T11:52:45.647837Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best_clf_gbc.best_estimator_.n_estimators_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Classifier"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:54:50.859552Z","start_time":"2020-12-21T11:52:45.657756Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(random_state = random_state)\n\nparam_grid = {'n_estimators': [50],\n              'learning_rate': [0.001, 0.01],\n              'max_depth': [3, 4],                # default=6\n              'reg_alpha': [1, 2],\n              'reg_lambda': [1, 2],\n              'subsample': [0.5, 0.75],\n              'colsample_bytree': [0.50, 0.75],\n              'gamma': [0.1, 0.5, 1],\n              'min_child_weight': [1]}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, scoring = scoring_metric, cv = 5, verbose = False, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train, y_train)\nclf_performance(best_clf_xgb, 'XGBoost Classifier', 'XGBC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBMClassifier"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:55:09.321637Z","start_time":"2020-12-21T11:54:50.862491Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"lgbmc = LGBMClassifier(random_state = random_state)\n\nparam_grid = {'max_depth': [5],\n              'num_leaves': [5, 10],\n              'learning_rate': [0.001, 0.01],\n              'n_estimators': [200],\n              'feature_fraction': [0.5],\n              'min_child_samples': [5, 10],\n              'reg_alpha': [0.1, 0.5],\n              'reg_lambda': [0.1, 0.5]} \n\nclf_lgbmc = GridSearchCV(lgbmc, param_grid = param_grid, verbose = False,\n                         scoring = scoring_metric, cv = 5, n_jobs = -1)\n\nbest_clf_lgbmc = clf_lgbmc.fit(X_train, y_train)\nclf_performance(best_clf_lgbmc, 'LGBMClassifier', 'LGBMC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Learning\n\nWe can combine the predictions of all these classifiers to see if we get better predictive performance compared to each constituent individual classifier. This is the main motivation behind Ensemble Learning.\n\nSpecifically, I will use **Soft Voting**. In this case, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T11:56:26.005566Z","start_time":"2020-12-21T11:55:09.324117Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"estimators = [('LR', best_clf_lr.best_estimator_),\n              ('SCV', best_clf_svc.best_estimator_),\n              ('RF', best_clf_rf.best_estimator_),\n              ('GBC', best_clf_gbc.best_estimator_),\n              ('XGB', best_clf_xgb.best_estimator_),\n              ('LGBMC', best_clf_lgbmc.best_estimator_)]\n\ntuned_voting_soft = VotingClassifier(estimators = estimators[1:], voting = 'soft', n_jobs = -1)\nestimators.append(('SoftV', tuned_voting_soft))\n\ny_pred_pp = cross_val_predict(tuned_voting_soft, X_train, y_train, cv = 5, method = 'predict_proba')[:, 1]\ny_pred = y_pred_pp.round()\n\ncm = confusion_matrix(y_train, y_pred, normalize = 'true')\nfpr, tpr, _ = roc_curve(y_train, y_pred_pp)\ncomparison_dict['SVot'] = [accuracy_score(y_train, y_pred),\n                           precision_score(y_train, y_pred),\n                           recall_score(y_train, y_pred),\n                           roc_auc_score(y_train, y_pred_pp), fpr, tpr]\n\nprint('Soft Voting\\n-----------------')\nprint('  Recall: ', np.round(recall_score(y_train, y_pred), 3))\n\nfig, ax = plt.subplots(figsize = (5, 4))\n\nplot_conf_mx(cm, 'Soft Voting', ax)    \nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\n## Learning Curves\n\nLearning curves are plots of a model‚Äôs performance on the training set and the validation set as a function of the training set size. They can help us visualise overfitting/underfitting and the effect of the training size on a model's error."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:00:04.128464Z","start_time":"2020-12-21T11:56:26.008543Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize = (15, 15))\n\nfor i in range(len(estimators)):\n    plt.subplot(3, 3, i + 1)\n    plot_learning_curve(estimators[i][1], estimators[i][0], X_train, y_train)\n    \nplt.tight_layout()\nax[2,1].set_axis_off()\nax[2,2].set_axis_off();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all models, there is a tiny gap between the two curves at the end of training. This indicates that we do **not** overfit the training set. "},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance\n\nSome classifiers allow us to visualise feature importance:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:00:05.448823Z","start_time":"2020-12-21T12:00:04.131405Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"color_ = ['steelblue', 'darkgray', 'cadetblue', 'bisque']\n\nfig = plt.subplots(2, 2, figsize = (10, 8))\n\nfor i, (name, clf) in enumerate(zip(['RF', 'GB', 'XGB', 'LGBM'], \n                                    [best_clf_rf, best_clf_gbc, best_clf_xgb, best_clf_lgbmc])):\n    \n    ax = plt.subplot(2, 2, i + 1)\n    plot_feature_imp(clf, name, color_[i], ax)\n    plt.ylabel('')\n    \nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Age' and 'NumOfProducts' seem like the most useful features for all classifiers, followed by 'IsActiveMember' and 'Balance'. On the other hand, 'CreditScore' is the least important feature with a small value close to zero for all estimators apart from LGBM. "},{"metadata":{},"cell_type":"markdown","source":"## Performance Comparison\n\nInitially, we can compare the performance of our classifiers in terms of four individual metrics (Accuracy, precision, recall, and area under the ROC curve or simply AUC):"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:00:05.558683Z","start_time":"2020-12-21T12:00:05.452253Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comparison_matrix = {}\nfor key, value in comparison_dict.items():\n    comparison_matrix[str(key)] = value[0:4]\n\ncomparison_df = pd.DataFrame(comparison_matrix, index = ['Accuracy', 'Precision', 'Recall', 'AUC']).T\ncomparison_df.style.highlight_max(color = 'indianred', axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:00:06.052698Z","start_time":"2020-12-21T12:00:05.561658Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comparison_df.plot(kind = 'bar', figsize = (10, 5), fontsize = 12, color = ['#5081DE', '#A7AABD', '#D85870', '#424656'])\n\nplt.legend(loc = 'upper center', ncol = len(comparison_df.columns), bbox_to_anchor = (0.5, 1.11))\nplt.xticks(rotation = 0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y = 0.75, color = 'red', linestyle = '--')\nplt.text(x = -0.45, y = 0.77, s = '0.75', size = label_size + 2, color = 'red');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apart from LR, all other classifiers have a recall higher than 75%. The Soft Voting classifier is the model with the highest recall (78.5 %). However, the LGBM classifier has the best overall performance with the highest accuracy, precision, and AUC. \n\nUsing single metrics is not the only way of comparing the predictive performance of classification models. The ROC curve (Receiver Operating Characteristic curve) is a graph showing the performance of a classifier at different classification thresholds. It plots the true positive rate (another name for recall) against the false positive rate."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:00:06.891431Z","start_time":"2020-12-21T12:00:06.056665Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"colors = ['steelblue', 'seagreen', 'black', 'darkorange', 'purple', 'firebrick', 'slategrey']\n\nfig = plt.figure(figsize = (8, 5))\n\nfor index, key in enumerate(comparison_dict.keys()):\n    auc, fpr, tpr = comparison_dict[key][3], comparison_dict[key][4], comparison_dict[key][5]\n    plt.plot(fpr, tpr, color = colors[index], label = '{}: {}'.format(key, np.round(auc, 3)))\n\nplt.plot([0, 1], [0, 1], 'k--', label = 'Baseline')\n\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.xticks([0, 0.25, 0.5, 0.75, 1])\nplt.ylabel('False Positive Rate')\nplt.yticks([0, 0.25, 0.5, 0.75, 1])\nplt.legend(fontsize = 14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dashed diagonal line represents a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).\n\nIn our case, all classifiers, apart from Logistic Regression, perform similarly. It seems that LGBM performs marginally better as evidenced by the slightly higher AUC (0.888)."},{"metadata":{},"cell_type":"markdown","source":"Recently, I came across another tool for assessing the performance of a classifier model. Simply put, a Cumulative Gain shows the percentage of targets reached when considering a certain percentage of the population with the highest probability to be target according to the model (see [here](https://towardsdatascience.com/meaningful-metrics-cumulative-gains-and-lyft-charts-7aac02fc5c14) and [here](http://mlwiki.org/index.php/Cumulative_Gain_Chart)). The `scikitplot` library offers an easy way of plotting this chart:"},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:01:21.097678Z","start_time":"2020-12-21T12:00:06.893912Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print ('Soft Voting:')\n\ny_pred = cross_val_predict(tuned_voting_soft, X_train, y_train, cv = 5, method = 'predict_proba')\nskplt.metrics.plot_cumulative_gain(y_train, y_pred)\n\nplt.plot([0.5, 0.5], [0, 0.8], color = 'firebrick')\nplt.plot([0.0, 0.5], [0.8, 0.8], color = 'firebrick')\nplt.text(0.15, 0.81, '80%', size = label_size, color = 'firebrick');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This chart shows that if we target 50% of the customers most likely to churn (according to the model), the model will pick 80% of customers who will actually churn, while the random pick would pick only 50% of the targets."},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the Test Set\n\nNow is the time to evaluate the models on unseen data. First, we need to perform the same preprocessing steps as the training set. "},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:01:21.146783Z","start_time":"2020-12-21T12:01:21.102155Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df.drop(features_drop, axis = 1, inplace = True)\n\ntest_df['Gender'] = LabelEncoder().fit_transform(test_df['Gender'])\ntest_df['Geography'] = test_df['Geography'].map({'Germany': 1, 'Spain': 0, 'France': 0})\n\ntest_df[scl_columns] = scaler.transform(test_df[scl_columns])  # not fit_transform, scaler has already been trained\n\ny_test = test_df['Exited']\nX_test = test_df.drop('Exited', 1)\n\nprint ('Preprocessing completed!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the same method for comparing our classifiers as we did in the training set."},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:01:51.601693Z","start_time":"2020-12-21T12:01:21.151246Z"},"scrolled":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tuned_voting_soft.fit(X_train, y_train)\nfig = plt.subplots(7, 1, figsize = (5, 25))\n\nfor i, (name, clf) in enumerate(zip(['LR', 'SVC', 'RF', 'GB', 'XGB', 'LGBM', 'SVot'],\n                                    [best_clf_lr.best_estimator_, best_clf_svc.best_estimator_, best_clf_rf.best_estimator_, best_clf_gbc.best_estimator_, best_clf_xgb.best_estimator_, best_clf_lgbmc.best_estimator_, tuned_voting_soft])):\n    \n    plt.subplot(7, 1, i + 1)\n    test_func(clf, name)\n    \nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:01:51.642368Z","start_time":"2020-12-21T12:01:51.604672Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comparison_test_df = pd.DataFrame(comparison_test_dict, index = ['Accuracy', 'Precision', 'Recall']).T\ncomparison_test_df.style.highlight_max(color = 'indianred', axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-12-21T12:01:52.171103Z","start_time":"2020-12-21T12:01:51.646335Z"},"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comparison_test_df.plot(kind = 'bar', figsize = (10, 5), fontsize = 12, color = ['#5081DE', '#A7AABD', '#D85870'])\n\nplt.legend(loc = 'upper center', ncol = len(comparison_test_df.columns), bbox_to_anchor = (0.5, 1.11))\nplt.xticks(rotation = 0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y = 0.75, color = 'red', linestyle = '--')\nplt.text(x = -0.45, y = 0.77, s = '0.75', size = label_size + 2, color = 'red');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two main points we need to note:\n\n1) The last three estimators (XGB, LGBM, and Soft Voting) seem like the best options as they display a high recall (around 77.1%). LGBM has the highest accuracy, precision, and recall. However, small variations are expected due to the small size of the test set and LGBM's performance is not significantly different than XGB and Soft Voting.\n\n2) Performance on the test set is fairly similar to the training set which proves that we do **not** have a problem with overfitting."},{"metadata":{},"cell_type":"markdown","source":"# Bibliography\n\nThe main resources I used are the following two books:\n\n[1] [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), by Aur√©lien G√©ron (2019)\n\n[2] [Practical Statistics for Data Scientists, 2nd Edition](https://www.oreilly.com/library/view/practical-statistics-for/9781492072935/), by Peter Bruce, Andrew Bruce, Peter Gedeck (2020)\n\n<br>\n\nThe following resources also helped me in my analysis:\n\n[3] [Bank Customer Churn](https://rstudio-pubs-static.s3.amazonaws.com/565148_6e82a5c320f14869bf63e23bcf59ce9b.html#compare-models-performance), by Zicheng Shi (same dataset but analysis in R)\n\n[4] [Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html)\n\n[5] [Easy Guide To Data Preprocessing In Python](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html), by Ahmad Anis.\n\n[6] [Meaningful Metrics: Cumulative Gains and Lyft Charts](https://towardsdatascience.com/meaningful-metrics-cumulative-gains-and-lyft-charts-7aac02fc5c14) by Raffi Sahakyan (2019)"},{"metadata":{},"cell_type":"markdown","source":"# Future Development\n\n- I tried combining existing features to produce more useful ones (Feature engineering). However, this didn‚Äôt increase the predictive performance of my models. Feature engineering can prove quite important when done right, so it is worth exploring it in a future version of this notebook."},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nOur notebook just came to an end! Our final report to the bank should be based on two main points:\n\n- **EDA** can help us identify which features contribute to customer churn. Additionally, **feature importance** analysis can quantify the importance of each feature in predicting the likelihood of churn. Our results reveal that the most significant feature is age (older customers are more likely to churn), followed by the number of products (having more products increases a customer‚Äôs likelihood to churn). The bank could use our findings to **adapt** and **improve its services** in a way that increases satisfaction for those customers more likely to churn.\n\n- We can build several **machine learning models** with **recall** approximately equal to **78%**. Perhaps, adding more features or/and records could help us improve predictive performance.\n\n<br>\n<br>\n\nPlease feel free to make any suggestions for improving my analysis. Also,  please consider <font size=+0 color=\"red\"><b>upvoting</b></font> if you found this notebook useful. Thank you! üòâ"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}