{"cells":[{"metadata":{"_uuid":"6407780609a842ef2eb5c899d4023af1d75886d8"},"cell_type":"markdown","source":"## ACL Accepted Papers Analysis\n\nLet's analyze its tendency of the research theme from the title of paper.\n\n1. Read papers dataset\n2. Calculate frequent words\n3. Analyze ACL 2018 abstracts\n\n### Read papers dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\npapers = pd.read_csv(\"../input/acl_papers.csv\", delimiter=\"\\t\")\npapers.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Calculate frequent words\n\nCount the frequent word by [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n"},{"metadata":{"trusted":true,"_uuid":"5c32a744eb4a1b0e4f6c76fa27b435bdc9e05051"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n\nyear_counts = {}\nall_papers = pd.concat([papers[\"title\"][papers[\"year\"] == y] for y in [2016, 2017, 2018]], ignore_index=True)\nvectorizer = CountVectorizer(stop_words=\"english\", max_df=0.8, min_df=7, ngram_range=(1, 2))\nvectorizer.fit(all_papers)\n\nwords = vectorizer.get_feature_names()\nfor y in [2016, 2017, 2018]:\n    vectors = vectorizer.transform(papers[\"title\"][papers[\"year\"] == y])\n    counts = vectors.toarray().sum(axis=0)\n    word_counts = {}\n    for i in range(len(words)):\n        word_counts[words[i]] = counts[i]\n    year_counts[y] = word_counts\n\nyear_count_df = pd.DataFrame(year_counts)\nyear_count_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1721bbc23f1550758b9e460ad84560946067ca41"},"cell_type":"code","source":"def show_rate(year_freq_df, max_limit=0.5, min_limit=0):\n    num_columns = year_freq_df.shape[1]\n    total = np.repeat(year_freq_df.sum(axis=1).values.reshape(-1, 1), num_columns, axis=1)\n    year_freq_df_rate = (year_freq_df / total)\n    first_index = year_freq_df.columns[0]\n    year_freq_df_rate.sort_values(by=first_index, inplace=True)\n    limited = year_freq_df_rate[(year_freq_df_rate.max(axis=1) > max_limit) & (year_freq_df_rate.min(axis=1) > min_limit)]\n    limited.plot.barh(stacked=True, figsize=(8, 12))\n    return limited\n\nyear_count_rate_df = show_rate(year_count_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ddfaa6b0a5c5195e85a943fb4425f52ed548262"},"cell_type":"markdown","source":"Evaluate by TF-IDF"},{"metadata":{"trusted":true,"_uuid":"e4d81421d803b5e42cab0d7473a7d990e0439795"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\nyear_idfs = {}\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.8, min_df=7, ngram_range=(1, 2))\nvectorizer.fit(all_papers)\n\nwords = vectorizer.get_feature_names()\nfor y in [2016, 2017, 2018]:\n    vectors = vectorizer.transform(papers[\"title\"][papers[\"year\"] == y])\n    idfs = vectors.toarray().mean(axis=0)\n    word_idfs = {}\n    for i in range(len(words)):\n        word_idfs[words[i]] = idfs[i]\n    year_idfs[y] = word_idfs\n\nyear_idf_df = pd.DataFrame(year_idfs)\nyear_idf_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35c669341bf4593e44f8a13c631562ab49975687"},"cell_type":"code","source":"year_idf_rate_df = show_rate(year_idf_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f68719cb1ebb92ba9954d28f4f039ed17229fe2e"},"cell_type":"markdown","source":"Both are similar result.\n\n\n### Analyze ACL 2018 abstracts\n\nWe can use abstracts of ACL 2018 papers (because these are published on official site).\nSo let's analyze it by above way.\n"},{"metadata":{"trusted":true,"_uuid":"0b2d491948fe539ce093f424fe5b47edea9395f8"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\nacl_2018s = papers[\"title\"][papers[\"year\"] == 2018] + papers[\"summary\"][papers[\"year\"] == 2018]\nvectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.6, min_df=0.01, ngram_range=(1, 2))\nvectors = vectorizer.fit_transform(acl_2018s)\n\nwords = vectorizer.get_feature_names()\nacl2018_idfs = {}\nfor i in range(len(words)):\n    if not words[i].isdigit():\n        acl2018_idfs[words[i]] = vectorizer.idf_[i]\n\nacl2018_idfs= pd.Series(acl2018_idfs)\nacl2018_idfs.nlargest(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0340f304003ba8e4a043a3dd79fb1df68e15bf44"},"cell_type":"markdown","source":"#### Clustering"},{"metadata":{"trusted":true,"_uuid":"5626153941b5d2688ad88699662d555f4317452f"},"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score\n\n\nscores = []\nnum_clusters = range(2, 10)\nfor n_clusters in num_clusters:\n    labels = SpectralClustering(n_clusters=n_clusters).fit_predict(vectors)\n    score = silhouette_score(vectors, labels)\n    scores.append(score)\n\npd.Series(data=scores, index=list(num_clusters)).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b939ffee879bf6dbae51feacad7842c71c417458"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\n\nnum_cluster = 7\ntransformed = TruncatedSVD(n_components=2).fit_transform(vectors)\nlabels = SpectralClustering(n_clusters=num_cluster).fit_predict(transformed)\n\nax = None\ncolors = [\"salmon\", \"lightskyblue\", \"mediumaquamarine\", \"wheat\", \"gray\", \"violet\", \"darkblue\", \"lime\", \"cadetblue\"]\nfor cluster in range(num_cluster):\n    c = pd.DataFrame(transformed[labels == cluster], columns=[\"pc1\", \"pc2\"])\n    if ax is None:\n        ax = c.plot.scatter(x=\"pc1\", y=\"pc2\", color=colors[cluster], label=\"cluster:{}\".format(cluster))\n    else:\n        ax = c.plot.scatter(x=\"pc1\", y=\"pc2\", color=colors[cluster], label=\"cluster:{}\".format(cluster), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2666fc51f3c2794c14d492d1763d905ea1369c13"},"cell_type":"markdown","source":"There is no explicit cluster."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}