{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic\nimport numpy as np \nimport pandas as pd \n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n# train test split\nfrom sklearn.model_selection import train_test_split\n\n# Making Polynomial Features\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Importing models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Regression Metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n# To build optimal model using Backward Elimination\nimport statsmodels.api as sm\n\n# Cross validation\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv', index_col = False)\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA (wrt. to Salary)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see that the salary for 67 students are not given as they were not placed.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Remove the rows that have data of unplaced students because we want to develop a model that predicts the salary of a placed student.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dropna(axis=0, inplace=True)\nprint(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we now have (215 - 67 = ) 148 entries. We have successfully removed the null salary entries. \n\nWe can also remove the 'status' column as we are predicting salaries of the students assuming they were already placed. So, that column would be 'Placed' for every student.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(columns = ['status'], axis=1, inplace=True)\ndataset.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Salary Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a = dataset['salary'])\nplt.title('Salary Distribution')\nplt.xlabel('Salary')\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\nThe salary distribution is **centered around 250k**. The range of salaries that are mostly given out lie in the region **200k - 400k**. \n\nWe can also see some outliers >400k.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Gender","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.violinplot(x = 'gender', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['gender'])['salary'].median().values\nnobs = dataset['gender'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Gender vs Salary')\nplt.grid(b=True, which='major', axis='both', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', axis='y', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\n1. The median salary is slightly higher for males than females. \n2. The distribution is also skewed in terms of females getting placed more than males.\n3. The range of salaries being offered to males is much higher than the reange of salaries being offered to females.\n4. Maximum salary offered to a male is 10,00,000 and that to a female is 7,00,000.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2. ssc_p (Senior Secondary Percentage)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.regplot(x='ssc_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs SSC Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\nThere seems to be almost 0 correlation between the senior secondary percentage and the salaries offered to the students.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3. ssc_b (Senior Secondary Board)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.violinplot(x = 'ssc_b', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['ssc_b'])['salary'].median().values\nnobs = dataset['ssc_b'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs SSC Board')\nplt.grid(b=True, which='major', axis='both', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', axis='y', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\n1. The median lies around the same value for both the boards.\n2. The range of salaries offered to students of the Central board is more than the range offered to the students of Other boards. \n\nWe can't particularly say if it is a good indicator for the salary. We will check the correlation later and also in the model refinement process.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4. hsc_p (Higher Secondary Percentage)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.regplot(x='hsc_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs HSC Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\nThe correlation between salary and HSC percentage can be visualised to be very less. This will not be a very good indicator for the salary offered to the student.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5. hsc_b (Higher Secondary Board)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.violinplot(x = 'hsc_b', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['hsc_b'])['salary'].median().values\nnobs = dataset['hsc_b'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs HSC Board')\nplt.grid(b=True, which='major', axis='both', color='#666666', linestyle='-')\nplt.minorticks_on()\nplt.grid(b=True, which='minor', axis='y', color='#999999', linestyle='-', alpha=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\n1. The median lies around the same value for both the boards.\n2. The range of salaries offered to students of the Central board is more than the range offered to the students of Other boards. \n\nWe can't particularly say if it is a good indicator for the salary. We will check the correlation later and also in the model refinement process.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 6. hsc_s (Higher Secondary Subject)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'hsc_s', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['hsc_s'])['salary'].median().values\nnobs = dataset['hsc_s'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs HSC Subjects')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\n1. The range of salaries offered are much greater for Students who took Commerce, and then Science. Arts students have a very small range of salaries offered.\n2. The correlation needs to be checked to check if this is a good indicator for salary offered.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 7. degree_p (Graduation Degree Percentage)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.regplot(x='degree_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs Degree Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\nThe correlation is either very less or 0 between the percentage of the degree and the salary offered.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 8. degree_t (Graduation Specialisation)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'degree_t', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['degree_t'])['salary'].median().values\nnobs = dataset['degree_t'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs Graduate Specialisation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'degree_t', y = 'salary', data = dataset, hue='gender')\nplt.title('Salary vs Graduate Specialisation and gender')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\n1. The range of salaries offered are in the order Comm&Mgmt > Sci&Tech > Others\n2. Correlation between the degree specialisation and the salary offered needs to be checked to check if this is a good indicator or not.\n3. The range of salaries offered to females (Sci&Tech and Comm&Mgmt fields) was very less compared to the range of salaries offred to males.\n4. Females who pursued Other fields were offered a decent range of salaries whereas for males in the same field, there was an absence of the same.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 9. workex (Work Experience)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'workex', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['workex'])['salary'].median().values\nnobs = dataset['workex'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs Work Experience')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'workex', y = 'salary', data = dataset, hue='gender')\nplt.title('Salary vs Work Experience and Gender')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\n1. The median of salary offered to students who had some work experience is slightly more than the ones who did not have a work experience.\n2. The range of salaries offered to students who had some work experience is a lot higher than the range offered to the ones who did not have work experience.\n3. Females with or without work experience were offered a range of salary less than the males.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 10. etest_p (Employability Test Percentage)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.regplot(x='etest_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs Employability Test Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\nA less significant positive correlation can be visualised between the salary offered and the employability test percentage.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 11. specialisation (in MBA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'specialisation', y = 'salary', data = dataset)\n\nmedians = dataset.groupby(['specialisation'])['salary'].median().values\nnobs = dataset['specialisation'].value_counts().values\nnobs = [str(x) for x in nobs.tolist()]\nnobs = ['n = ' + i for i in nobs]\n\npos = range(len(nobs))\nfor tick,label in zip(pos, ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick]+0.04, nobs[tick], horizontalalignment='center', size='x-small', color='w', weight='semibold')\n\nplt.title('Salary vs specialisation in MBA')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.violinplot(x = 'specialisation', y = 'salary', data = dataset, hue='gender')\nplt.title('Salary vs specialisation in MBA and gender')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\n1. The median of salary offered to students who pursued Mkt&Fin is slighty more than the salary offered to students who pursued Mkt&HR in MBA\n2. The range of salary offered to students who pursued Mkt&Fin is mugh larger than the salary offered to students who pursued Mkt&HR in MBA.\n3. There could be some correlation. \n4. The females who studied in either of the fields were offered a smaller range of salaries than the males.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 12. mba_p (Percentage in MBA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.regplot(x='mba_p', y='salary', data = dataset)\nplt.minorticks_on()\nplt.grid(b=True, which='both', axis='both', alpha=0.1)\nplt.title('Salary vs MBA Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION:\n\nA less significant correlation can be visualised between the MBA percentage and the salary offered to the student.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Steps we need to take:\n1. Remove the first column of serial numbers.\n2. We need to change all the categorical variables into hot encoded values.\n3. Drop the original categorical variable columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping first column\n\ndataset.drop(columns=['sl_no'], axis=1, inplace=True)\ndataset.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender: F coded as 0 and M as 1\ndummy = pd.get_dummies(dataset['gender'])\ndummy.rename(columns={'M':'Gender'}, inplace=True)\n\n# drop original column \ndataset.drop(\"gender\", axis = 1, inplace=True)\n\n# merge data frame \"dataset\" and \"dummy_variable_1: Gender column\" \ndf = pd.concat([dummy['Gender'], dataset], axis=1)\n\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ssc_b: Central as 1 and Others as 0\ndummy = pd.get_dummies(dataset['ssc_b'])\ndummy.rename(columns={'Central':'ssc_b'}, inplace=True)\n\ndf.drop(\"ssc_b\", axis = 1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:2], dummy['ssc_b'], df.iloc[:, 2:]], axis=1)\n\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hsc_b: Central as 1 and Others as 0\ndummy = pd.get_dummies(dataset['hsc_b'])\ndummy.rename(columns={'Central':'hsc_b'}, inplace=True)\n\ndf.drop(\"hsc_b\", axis = 1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:4], dummy['hsc_b'], df.iloc[:, 4:]], axis=1)\n\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Higher Secondary Specialisation: Science: 10 and Commerce: 01 and Arts: 00\ndummy = pd.get_dummies(df['hsc_s'])\ndummy.rename(columns={'Science': 'HS_Sci', 'Commerce': 'HS_Comm'}, inplace=True)\ndummy = pd.concat([dummy['HS_Sci'], dummy['HS_Comm']], axis=1)\ndummy.head()\n\n# drop original\ndf.drop('hsc_s', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:5], dummy, df.iloc[:, 5:]], axis=1)\n\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Undergrad specialisation: Sci&Tech: 10 and Comm&Mgmt: 01 and Others: 00\ndummy = pd.get_dummies(df['degree_t'])\ndummy.rename(columns={'Sci&Tech': 'UG_Sci', 'Comm&Mgmt': 'UG_Comm'}, inplace=True)\ndummy = pd.concat([dummy['UG_Sci'], dummy['UG_Comm']], axis=1)\ndummy.head()\n\n# drop original\ndf.drop('degree_t', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:8], dummy, df.iloc[:, 8:]], axis=1)\n\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Work experience: Yes as 1 nd No as 0\ndummy = pd.get_dummies(df['workex'])\ndummy.rename(columns={'Yes': 'workex'}, inplace=True)\n# dummy.head()\n\n# drop original\ndf.drop('workex', axis=1, inplace=True)\n\n# merge data\ndf = pd.concat([df.iloc[:, 0:10], dummy['workex'], df.iloc[:, 10:]], axis=1)\n\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specialisation: Mkt&Fin as 1 and Mkt&HR as 0\ndummy = pd.get_dummies(df['specialisation'])\ndummy.rename(columns={'Mkt&Fin': 'specialisation'}, inplace=True)\n# dummy.head()\n\n# drop original data\ndf.drop('specialisation', axis=1, inplace=True)\n\n# merge data\ndf= pd.concat([df.iloc[:, 0:12], dummy['specialisation'], df.iloc[:, 12:]], axis=1)\n\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation between all variables**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 12))\nsns.heatmap(df.corr(), annot=True)\nplt.title('Correlation between all features and salary offered')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# acquiring data for model\n\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nprint('X_shape {}'.format(X.shape))\nprint('y_shape {}'.format(y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint('Shape of training set: {} and test set: {}'.format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Construction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We will try models to see which model suits the data best.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Multiple Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making regressor\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train, y_train))  \nprint('Test Score: ', regressor.score(X_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Polynomial Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Polynomial Features\npoly_reg = PolynomialFeatures(degree = 3)\nX_train_poly = poly_reg.fit_transform(X_train)\nX_test_poly = poly_reg.fit_transform(X_test)\n\n# Fitt PolyReg to training set\nregressor = LinearRegression()\nregressor.fit(X_train_poly, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_poly)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train_poly, y_train))  \nprint('Test Score: ', regressor.score(X_test_poly, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Support Vector Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying feature scaling for this\n\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc = sc.fit_transform(X_test)\n\nprint('Scaled Successfully')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = SVR(kernel='rbf')\nregressor.fit(X_train_sc, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test_sc)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train_sc, y_train))  \nprint('Test Score: ', regressor.score(X_test_sc, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Decision Tree Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = DecisionTreeRegressor()\nregressor.fit(X_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train, y_train))  \nprint('Test Score: ', regressor.score(X_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Random Forest Regression\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = RandomForestRegressor(n_estimators = 10)\nregressor.fit(X_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_train, y_train))  \nprint('Test Score: ', regressor.score(X_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of all the models tried above:\n\n**Multiple Linear Regression works best**, \n\nfollowed by **Random Forest Regression**, and \n\nthen **SVR**.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will thus, go forward with the **Multiple Linear Regression** and **Random Forest Regression** and will try to better the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Backward Elimination for Multiple LR","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will use Backward Elimination to find out the variables right for our multiple Linear Regression Model.\n\nSTEPS:\n1. Select significance level to stay in the model (SL = 0.05)\n2. Fit the full model will all predictors\n3. Consider predictor with highest p-value. If p>SL, go to S4, else FINISH.\n4. Remove the predictor.\n5. Fit model without the predictor. Go back to S3.\n\nFINISH.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_0 has to be given here explicitly because this package does not take in the b_0 constant otherwise.\nX_new = df.iloc[:, :-1].values\nX_new = np.append(arr = np.ones((148,1)).astype(int), values = X_new, axis = 1)\n\nprint(X_new.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_new[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13, 14]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,5,6,7,8,9,10,11,12,13,14]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,3,4,5,6,8,9,10,11,12]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,4,5,6,7,8,9,10,11]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,3,4,5,6,8,9,10]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,3,4,5,6,7,8,9]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 7","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,3,4,5,7,8]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 8","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [1,2,3,4,5,6,7]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 9","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,4,5,6]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 10","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,2,3,4,5]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 11","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,1,2,4]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iteration 12","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt = X_opt[:, [0,2,3]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SCORES**\n\n**R-squared value --> 0.914**\n\n**Adjusted R-squared value --> 0.912**\n\nThis score shows that the model performs better with the three features in X_opt. \n\nHowever, \n\nwe do see that the p value for feature **x2** has exceeded the value we set for our p value. We will chuck that feature and see if our model performs better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Iteration 13","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# S0. Create a new set of features that will be our optimal set of features\nX_opt_final = X_opt[:, [0,2]]\n\n# S1. SL chosen 0.05\n\n# S2. Taken X_opt. Fit multiple LR\nregressor_OLS = sm.OLS(endog = y, exog = X_opt_final).fit()\n\n# S3. predictor with highest p-value. p > SL\nregressor_OLS.summary()\n\n# S4. Remove predictor if p > SL and highest p-value. Go to S0.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SCORES**\n\n**R-squared value --> 0.913**\n\n**Adjusted R-squared value --> 0.911**\n\nThis score shows that the model **FALLS** in performance when we **drop the x2 feature from X_opt.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thus, we will keep the **three features we selected in Iteration 12.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of optimal values or X: ', X_opt.shape)\n\n# Let's visualise the first 3 values to see, which of the features have we selected\nX_opt[0:5,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On comparing the original dataset with the optimal table of X, we can see that the features that best give the salary prediction are:\n\n1. Gender\n2. UnderGraduate Degree (Comm/Sci or Others)\n3. Percentage in MBA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Final Model (Multiple Linear Regression)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final = df.iloc[:, [0,9,13]].values\ny = df.iloc[:, 14].values\n\nprint('X_shape {}'.format(X_final.shape))\nprint('y_shape {}'.format(y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting\nX_final_train, X_final_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=0)\n\nprint('Shape of training set: {} and test set: {}'.format(X_final_train.shape, X_final_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making regressor\nregressor = LinearRegression()\nregressor.fit(X_final_train, y_train)\n\n# Predicting test values\ny_pred = regressor.predict(X_final_test)\n\n# Model performance through metrics\nprint('Train Score: ', regressor.score(X_final_train, y_train))  \nprint('Test Score: ', regressor.score(X_final_test, y_test)) \nprint()\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\nprint('R2 score: ', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation\nreg_score = cross_val_score(regressor, X_final_train, y_train, cv=10)\n\nprint('Cross Validation Scores across all 10 iterations: ', reg_score)\nprint('Multiple Linear Regression: ', np.mean(reg_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thoughts","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The model here performs quite poorly even after choosing out the best features. That makes me wonder:\n\n1. If the salary offered does really depend on the specifics of the student's biodata, or \n2. Does it depend on the company policies/existing salaries and posts in the company?\n3. Does it depend in how the interview of the candidate was?\n\nAs visible, **none** of the features showed a very strong negative or positive correlation with the salary offered. This did imply that of the features given, **no particular feature** was a strong predictor for the salaries. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Further Work","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Further work would include, \n1. Working on Random Forest model to see if that model gives better predictions or not.\n2. Finding a better comination of variables that give us better salary predictions with RF model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Previous Work: Check out [here](http://www.kaggle.com/mani97/placed-or-not-eda-classification-88-8) for EDA and Classification modelling to predict if a student was placed or not!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Note:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Do comment and let me know any ideas that I missed out on or if I could better my thought process regarding this in any way.\n\nThank you!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}