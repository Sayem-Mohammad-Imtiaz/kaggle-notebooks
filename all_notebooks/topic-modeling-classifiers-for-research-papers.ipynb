{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Experimenting with Topic Modeling using Word Embeddings\n\nThe data set being used contains research paper titles and abstracts as well as labels as either Computer Science, Physics, Mathematics, Statistics, Quantitative Biology, Quantitative Finance, or some combination of those labels.  The approach that I am taking is to convert the text to a vector using word embeddings trained on this data set, then I will train a classifier for each of the labels, separately.  At the end I am going to create a function that when text is inputed will return the likely topic(s) of the title or abstract.\n\nThis function will evaluate the inputed text on each of the classifiers separately, then return an array with the results of each one in the same order that they appear in the columns in the training dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom gensim.models import Word2Vec\nimport gensim.utils\n\nimport tensorflow as tf\n\n%matplotlib notebook\nprint('You\\'re running python %s' % sys.version.split(' ')[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the training data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/topic-modeling-for-research-articles/train.csv',keep_default_na=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Take a look at the training data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create array with labels for later:"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance'],dtype='str')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Break up dataset into lists that can be used for training and testing sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this row number has a title that has no strings after simple_preprocess, so I removed it\nissueRowNumber = 8270\n\n# X's\ninputAbstracts = train['ABSTRACT'].tolist()\ninputTitles = train['TITLE'].tolist()\ninputAbstracts.pop(issueRowNumber)\ninputTitles.pop(issueRowNumber)\n\n# y's or labels\nlabelColumns = [None]*len(labels)\nfor i in range(len(labels)):\n    col = train[labels[i]].tolist()\n    col.pop(issueRowNumber)\n    labelColumns[i] = col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tokenize titles and abstracts:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenize titles:\ninputTitleTokens = []\nfor title in inputTitles:\n    tokens = gensim.utils.simple_preprocess(title)\n    inputTitleTokens.append(tokens)\n    \n#tokenize abstracts:   \ninputAbstractTokens = []\nfor abstract in inputAbstracts:\n    tokens = gensim.utils.simple_preprocess(abstract)\n    inputAbstractTokens.append(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create Word Embeddings for article titles using Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"W2V_model_title = Word2Vec(inputTitleTokens, min_count=1,size=100,workers=3, window=5, sg=1)\nW2V_model_abstract = Word2Vec(inputAbstractTokens, min_count=1,size=100,workers=3, window=5, sg=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vectorize article titles using Word Embeddings:"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizedTitles = [None]*len(inputTitleTokens)\nfor i in range(len(inputTitleTokens)):\n    title=[]\n    for word in inputTitleTokens[i]:\n        try:\n            title.append(W2V_model_title.wv[word])\n        except:\n            'do nothing'\n    title_avg = np.mean(np.array(title, dtype='f'),axis=0)\n    vectorizedTitles[i]=title_avg\n\nvectorizedAbstracts = [None]*len(inputAbstractTokens)\nfor i in range(len(inputAbstractTokens)):\n    abstract=[]\n    for word in inputAbstractTokens[i]:\n        try:\n            abstract.append(W2V_model_abstract.wv[word])\n        except:\n            'do nothing'\n    abstract_avg = np.mean(np.array(abstract, dtype='f'),axis=0)\n    vectorizedAbstracts[i]=abstract_avg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split up testing and training sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = len(inputTitles)//5\ntrain_size = len(inputTitles)-test_size\nprint('Testing set size: '+str(test_size),'|','Training set size: '+str(train_size),'|','Total size: '+str(test_size+train_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the X test and training matricies for the article titles\ntemp = np.array(vectorizedTitles)\nX_title_test,X_title_train = temp[train_size:],temp[:train_size]\n#create the X test and training matricies for the article abstracts\ntemp = np.array(vectorizedAbstracts)\nX_abstract_test,X_abstract_train = temp[train_size:],temp[:train_size]\n\n#create the Y test and training arrays for the article labels (list of \"np.array columns\")\nY_train,Y_test = [None]*len(labelColumns),[None]*len(labelColumns)\nfor colNumber in range(len(labelColumns)):\n    temp = np.array(labelColumns[colNumber])\n    Y_test[colNumber],Y_train[colNumber]  = temp[train_size:],temp[:train_size]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create random forest classifiers for each label:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TITLES:')\ntitle_classifiers = [None]*len(Y_train)\nfor colNumber in range(len(Y_train)):\n    temp = RandomForestClassifier(max_depth=6,n_estimators=10)\n    temp.fit(X_title_train, Y_train[colNumber])\n    title_classifiers[colNumber] = temp\n    print(colNumber,labels[colNumber])\n    print('Training accuracy:',np.sum(temp.predict(X_title_train)==Y_train[colNumber])/len(X_title_train))\n    print('Testing accuracy:',np.sum(temp.predict(X_title_test)==Y_test[colNumber])/len(X_title_test))\n    print()\n\nprint('ABSTRACTS:')\nabstract_classifiers = [None]*len(Y_train)\nfor colNumber in range(len(Y_train)):\n    temp = RandomForestClassifier(max_depth=6,n_estimators=10)\n    temp.fit(X_abstract_train, Y_train[colNumber])\n    abstract_classifiers[colNumber] = temp\n    print(colNumber,labels[colNumber])\n    print('Training accuracy:',np.sum(temp.predict(X_title_train)==Y_train[colNumber])/len(X_title_train))\n    print('Testing accuracy:',np.sum(temp.predict(X_title_test)==Y_test[colNumber])/len(X_title_test))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create classifier function that evaluates input text on all five labels:\none for the title, and one for the abstract."},{"metadata":{"trusted":true},"cell_type":"code","source":"def title_classifier(title):\n    global title_classifiers\n    tokenTitle = gensim.utils.simple_preprocess(title)\n    vecTitle = []\n    for word in tokenTitle:\n        try:\n            vecTitle.append(W2V_model_title.wv[word])\n        except:\n            'do nothing'\n    vecTitle = np.mean(np.array(vecTitle, dtype='f'),axis=0)\n    preds = [None]*len(title_classifiers)\n    for index in range(len(title_classifiers)):\n        preds[index] = int(title_classifiers[index].predict(vecTitle.reshape(1, -1))[0])\n    return np.array(preds)\n\ndef abstact_classifier(abstract):\n    global abstract_classifiers\n    tokenAbstract = gensim.utils.simple_preprocess(abstract)\n    vecAbstract = []\n    for word in tokenAbstract:\n        try:\n            vecAbstract.append(W2V_model_title.wv[word])\n        except:\n            'do nothing'\n    vecAbstract = np.mean(np.array(vecAbstract, dtype='f'),axis=0)\n    preds = [None]*len(abstract_classifiers)\n    for index in range(len(abstract_classifiers)):\n        preds[index] = int(abstract_classifiers[index].predict(vecTitle.reshape(1, -1))[0])\n    return np.array(preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Try out classifier on some made up article name inputs:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"articleName = \"New Methods for KNN with text data\"\npreds = title_classifier(articleName)\nprint('Output vector:',preds,'|','Predicited Label(s):',labels[preds==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articleName = \"Pi used in new formula\"\npreds = title_classifier(articleName)\nprint('Output vector:',preds,'|','Predicited Label(s):',labels[preds==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articleName = \"New prime number discovered\"\npreds = title_classifier(articleName)\nprint('Output vector:',preds,'|','Predicited Label(s):',labels[preds==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articleName = \"New Data distribution used to speed up training\"\npreds = title_classifier(articleName)\nprint('Output vector:',preds,'|','Predicited Label(s):',labels[preds==1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary:\n\nWhile this ensemble classifier does not work perfectly, it does a fairly descent job of classifying the papers correctly. With the way thes models have been trained, it seems that the title gives more of an indication of the field of the paper, rather than the abstract.  Having thought about this a bit, I think maybe the abstract classifiers would be more accurate if the window of the word embeddings for them was larger, since the abstracts have more words, and so maybe a context longer than 5 words.  However, overall the accuracy of 80%+ test and training accuracy for the the titles is a good indication that this classification can be done well.  I am sure it is possible to improve the accuracy a bit.  I believe a better method of combining the word embeddings, instead of a simple average as I have done here, might achieve greater accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}