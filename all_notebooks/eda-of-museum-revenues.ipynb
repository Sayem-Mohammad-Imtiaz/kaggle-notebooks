{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes=True)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/museum-directory/museums.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis\n\nIn this section, we are going to perform an Exploratory Data Analysis of Museums data. In this regards, we will undertake \ndifferent descriptive and statistical analysis of the data. We will try to identify the missing values, when present, clean \nthem and try to make meaning of the remaining data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll look at the head in order to understand labels of the data. \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see for the general information about the data. \n#It seems, there are a lot of missing values in columns such as Institution Name column.\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the statistical values of the numerical columns. \nAccording to the data, in the first quartile the income and revenue for some museums or such institutions seems to be 0. \nThese institutions either have not provided their earnings or are completely subsidized by the government, \nwhich exempts them to publish their earnings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's look at the statistical values from a differnet perspective. \ndf.describe(include='O').transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the check the shape of the dataframe.\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing the irrelevant columns, duplicates and missing valuesÂ¶\n\nIn this part, we are going to first remove the irrelevant columns. Depending on the data, the removal of the columns can be different. But in this case, we have identified that there are overlapping information such as institution addresses or names. This information are not going to add value to our analysis. Moreover, some of the columns have more missing values. Therefore, we decided to drop those columns. Another column that we drop is the column of the Phone Numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's get the column names' first\n\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now, let's drop the columns.\n\ndf = df.drop(['Museum ID','Legal Name', 'Alternate Name', 'Street Address (Physical Location)', 'City (Physical Location)',\n       'State (Physical Location)', 'Phone Number', 'Employer ID Number'], axis=1)\n\n# And check the head again. \n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let' check the shape of dataframe again. \n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's remove the number of rows before removing the duplicates.\n\ndf = df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's count the number of rows after removing the duplicates.\n\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The next thing is to identify the number of missing values. \n#According to our dataframe, columns such as Institution Name or \n#Zip Code (Physical Location) have the most missing values.\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find the columns, which have more than 50% missing values.\n\nmost_missing_cols = set(df.columns[df.isnull().mean() > 0.50])\n\nmost_missing_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this case let's drop the \"Institution Name\" and Zip Code (Physical Location) columns, since we believe that \n#ommision of these columns will not much differnce to our analysis. \n\ndf = df.drop(['Institution Name', 'Zip Code (Physical Location)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the head of our dataframe again to make sure that the last two columns have been dropped.\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The next step is to drop the missing values. \n\ndf = df.dropna()\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the columns with 0 missing values\n\nno_nulls = set(df.columns[df.isnull().mean()==0])\nno_nulls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though Revenue column does appear in the above snippet, in the code below, we have identified rows that \nhave 0s rather than missing values. We believe that these institutions with 0 Revenues are the ones\nthat receive support from the government and are not required to file for taxes. 0s affect the mean of our calculations. \nTherefore, we will filter out the rows with 0s as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# When we plot the Revenue column with 0s included, the mean is close to zero and almost invisible. \nsns.boxplot(x=df['Revenue'], showfliers=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, we are going to filter out the rows that have 0 values and assign the result to a new variable.\nno_zeros = df[df['Revenue']!=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the spread of the revenue with 0s filtered out. As you can observe the mean, \n#even though slightly, has moved to the right. \nsns.boxplot(x=no_zeros['Revenue'] , showfliers=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In terms of normal distribution, we have a right skewed histogram. \nfig, ax = plt.subplots()\nax.hist(x=no_zeros['Revenue'], bins =2)\nax.set(title = 'Normal distribution of museum revenues');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's count the number of museum types available in the dataframe.\n\ndf['Museum Type'].value_counts().nlargest(20).plot(figsize=(10,5), kind = 'bar')\nplt.title(\"Number of museum types in the United States\")\nplt.ylabel(\"Number of museums\")\nplt.xlabel(\"Types of museums\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see what type of museums attract the most visitors in terms of revenue.\n\ntype_rev = df.groupby(['Museum Type']).agg({'Revenue':'sum'})\ntype_rev = type_rev.sort_values(by='Revenue', ascending=False)\ntype_rev.plot(kind='bar', figsize=(10,5));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How about the states that earn the majority of Revenues?\nstate_rev = df.groupby(['State (Administrative Location)']).agg({'Revenue':'sum'})\nstate_rev = state_rev.sort_values(by='Revenue', ascending=False)\nstate_rev[:20].plot(kind='bar', figsize=(10,5))\nplt.ylabel('Revenue (million $)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the top grossing museums in terms of their city locations. \ncity_rev = df.groupby(['City (Administrative Location)']).agg({'Revenue':'sum'})\ncity_rev = city_rev.sort_values(by='Revenue', ascending=False)\ncity_rev[:20].plot(kind='bar', figsize=(10,5))\nplt.ylabel('Revenue (million $)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this section we are going to create a column based on revenue \n#column in order to rank the museums in terms of their revenues. \n\ncols = ['Revenue']\n\nno_zeros['Rank'] = no_zeros.sort_values(cols, ascending=False).groupby(cols, sort=False).ngroup()+1\nno_zeros.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_zeros.sort_values('Rank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we can see that the Museums, that are sorted by their revenues. When we compare with the Rank column\n#we see different museum names. It's probably due to the rank method.\n\nmuseum_rev = df[['Museum Name','Revenue']].sort_values(by='Revenue', ascending=False)\nmuseum_rev.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nThe aim of this exploratory analysis was to perform statistical analysis on Museums, Aquariums and Zoo data. As we have seen \nthe dataset contains many null and zero values. In order to get an accurate result, we have dropped null values and ommited \n0 values from our calculations. The dataset contained duplicate or overlapping values as well. We have identified these values \nand dropped before starting to analyze the data. We remaining data, we were able to analyze identify the number of revenues, high earning \ncities and states and plot the results on bar charts. For the analysis to be more accurate, we need more data to evaluate further. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}