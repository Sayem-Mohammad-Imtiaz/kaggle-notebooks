{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries and Packages","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV\nsns.set()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function Definitions\n* ***processDataFrame(file_name)***: takes a file name, loads data in CSV format, and outputs a data frame\n* ***dataCleaning(df)***: takes a data frame, encodes categorical variables, imputes missing values, and outputs the updated data frame\n* ***countplotConstructor(df)***: takes a data frame and constructs countplots for the categorical variables\n* ***histogramConstructor(df)***: takes a data frame, constructs histograms for the quantitative variables, and fits a kernel density estimate to the histogram\n* ***scatterplotConstructor(df, a, c)***: takes a data frame, transparency parameter, and a color parameter; it creates six scatter plots, one per predictor variable\n* ***printMetrics(y_test, predictions)***: takes the actual and the predicted diamond prices and calculates some relevant performance metrics\n* ***residualPlotConstructor(y_test, predictions, a, c1, c2)***: takes the actual and predicted diamond prices, a transparency parameter, and two color parameters; it constructs a residual plot","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def processDataFrame(file_name):\n    df = pd.read_csv(file_name)\n    print(df.info())\n    print(df.describe())\n    \n    return df\n    \ndef dataCleaning(df):\n    cut_map = {'Fair':1, 'Good':2, 'Very Good':3, 'Premium':4, 'Ideal':5}\n    color_map = {'D':7, 'E':6, 'F':5, 'G':4, 'H':3, 'I':2, 'J':1}\n    clarity_map = {'I1':1, 'SI1':2, 'SI2':3, 'VS1':4, 'VS2':5, 'VVS1':6, 'VVS2':7, 'IF':8}\n        \n    df.cut = df.cut.replace(cut_map)\n    df.color = df.color.replace(color_map)\n    df.clarity = df.clarity.replace(clarity_map)\n    \n    df.x[df.x == 0.0] = df.x.mean()\n    df.y[df.y == 0.0] = df.y.mean()\n    df.z[df.z == 0.0] = df.z.mean()\n    \n    df.dropna()\n    df.reset_index(drop = True)\n    \n    return df\n\ndef countplotConstructor(df):\n    fig, axes = plt.subplots(nrows = 3, figsize = (15, 15))\n    sns.countplot(x = 'cut', data = df, ax = axes[0], order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'])\n    sns.countplot(x = 'color', data = df, ax = axes[1], order = ['J', 'I', 'H', 'G', 'F', 'E', 'D'])\n    sns.countplot(x = 'clarity', data = df, ax = axes[2], order = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'])\n    \n    plt.tight_layout()\n    \ndef histogramConstructor(df):\n    columns = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n    fig, axes = plt.subplots(nrows = len(columns), figsize = (18, 18))\n    for i in range(len(columns)):\n        sns.distplot(df[columns[i]], kde = True, ax = axes[i])\n        axes[i].set(xlabel = columns[i], ylabel = 'frequency', title = columns[i])\n    \n    plt.tight_layout()    \n\ndef boxplotConstructor(df):\n    columns = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n    fig, axes = plt.subplots(nrows = len(columns), figsize = (18, 18))\n    for i in range(len(columns)):\n        sns.boxplot(df[columns[i]], ax = axes[i])\n    \n    plt.tight_layout()    \n\ndef scatterplotConstructor(df, ylabel, a, c):\n    fig, axes = plt.subplots(nrows = 2, ncols = 3, figsize = (18, 18))\n    axes[0, 0].scatter(df.carat, df.price, alpha = a, color = c)\n    axes[0, 1].scatter(df.depth, df.price, alpha = a, color = c)\n    axes[0, 2].scatter(df.table, df.price, alpha = a, color = c)\n    axes[1, 0].scatter(df.x, df.price, alpha = a, color = c)\n    axes[1, 1].scatter(df.y, df.price, alpha = a, color = c)\n    axes[1, 2].scatter(df.z, df.price, alpha = a, color = c)\n\n    axes[0, 0].set(xlabel = 'carat', ylabel = ylabel, title = 'price vs carat')\n    axes[0, 1].set(xlabel = 'depth', ylabel = ylabel, title = 'price vs depth')\n    axes[0, 2].set(xlabel = 'table', ylabel = ylabel, title = 'price vs table')\n    axes[1, 0].set(xlabel = 'x-dimension', ylabel = ylabel, title = 'price vs x-dimension')\n    axes[1, 1].set(xlabel = 'y-dimension', ylabel = ylabel, title = 'price vs y-dimension')\n    axes[1, 2].set(xlabel = 'z-dimension', ylabel = ylabel, title = 'price vs z-dimension')\n\n    plt.tight_layout()\n\ndef printMetrics(y_test, predictions):\n    mae = metrics.mean_absolute_error(y_test, predictions)\n    mse = metrics.mean_squared_error(y_test, predictions)\n    mape = np.mean(np.abs(y_test - predictions) / np.abs(y_test))\n    \n    print('Mean Absolute Error: ', mae)\n    print('Mean Squared Error: ', mse)\n    print('Root Mean Squared Error: ', np.sqrt(mse))\n    print('Mean Absolute Percentage Error: ', round(mape * 100, 2), '%')\n    print('Accuracy: ', round(100 * (1 - mape), 2), '%')\n    \ndef residualPlotConstructor(y_test, predictions, a, c1, c2):\n    fig, ax = plt.subplots()\n    ax.set(xlabel = 'Predicted Price', ylabel = 'Actual Price - Predicted Price', title = 'Residual Plot')\n    ax.scatter(predictions, y_test - predictions, alpha = a, color = c1)\n    ax.axhline(y = 0, color = c2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THE DATA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\nThe data frame has a 53940x10 shape. There are no obvious missing values. However, a brief inspection of the statistical summary shows that there are some hidden missing values. The minimum values for the variables ***x***, ***y***, ***z*** are ***0.0***. Since the three variables represent physical dimensions, a measure of ***0.0*** does not make sense. The ***0.0*** is likely a placeholder for missing values. Compared to the overall size of the data set, the number of missing values is negligible; however, I still imputed the missing values by replacing the ***0.0***'s in each column with the average of the corresponding variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name = '/kaggle/input/diamonds/diamonds.csv'\ndf = processDataFrame(file_name)\ndf[df == 0.0].count(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of the variables — except for ***depth*** — are highly correlated with ***price***. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.cut.unique())\nprint(df.color.unique())\nprint(df.clarity.unique())\ncountplotConstructor(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I retained all predictors except for ***Unnamed: 0*** (the index column) and ***depth***.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataCleaning(df)\ny = df.price\nX = df.drop(columns = ['Unnamed: 0', 'depth', 'price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The variables ***carat,*** ***price,*** and ***x*** are skewed to the right. The distributions of the variables ***table,*** ***y,*** and ***z*** are jagged. The variable ***depth*** is fairly symmetric, but even that distribution has long tails on each side, suggesting potential extreme values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"histogramConstructor(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The boxplots make more explicit what the histograms and kde's suggest. Each variable has a considerable number of outliers, either on one or both ends of the distribution. There is nothing about the data to suggest that the outliers are typos or incorrect in some other way. Removing them might lead to a better model fit, but doing so would also remove some of the nuance and variation of the data. Regular multivariate regression and other techniques can be sensitive to extreme values. To avoid this problem, I retained the outliers but used a random forest regressor, which is more robust to outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplotConstructor(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterplotConstructor(df, 'price', a = 0.2, c = 'tan')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THE MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\nregressor = RandomForestRegressor()\nregressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Metrics & Residual Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"printMetrics(y_test, predictions)\nresidualPlotConstructor(y_test, predictions, 0.15, 'firebrick', 'maroon')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For low predicted prices, the residuals are closely centered around y = 0. As predicted price increases, the residuals start diverging from y = 0, but still mostly concentrate around the horizontal line.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}