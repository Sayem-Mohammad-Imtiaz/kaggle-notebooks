{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n> This book is part of a detailed writeup on analyzing the [CORD 19 Research Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) dataset . Please [visit this link](https://www.kaggle.com/finalepoch/cord-19-research-dataset-analysis-visualization) for the main page which contains more information and context.\n\nIn this notebook I have tried to create a Named Entity Recognition system for analyzing the [CORD 19 Research Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) dataset. I am tyring to use Spacy's [custom named entity recognition](https://spacy.io/usage/training/) to train a model that can detect mentions of three things in text:\n* MedicalCondition\n* Medicine\n* Pathogen\n\nI have currently used [LightTag](https://www.lighttag.io/) for creating a manually tagged corpus from randomly selected text from Wikipedia. The dataset is available here : [https://www.kaggle.com/finalepoch/medical-ner](https://www.kaggle.com/finalepoch/medical-ner)\n"},{"metadata":{},"cell_type":"markdown","source":"## Load the annotated corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nwith open(\"/kaggle/input/medical-ner/Corona2.json\") as f:\n    annotation = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert the tagged data to a format understood by Spacy. Remove anything that has spaces and does **not** have   *\"human_annotations\"* . LightTag gui has bugs which give overlapping entites that results in error during training"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA  = []\nfor e in annotation[\"examples\"]:\n    content = e[\"content\"]\n    entities = []\n    for an in e[\"annotations\"]:        \n        if len(an[\"value\"]) == len(an[\"value\"].strip()):          \n            if len(an['human_annotations']) == 0:\n                continue\n            info = (an[\"start\"],an[\"end\"],an[\"tag_name\"])\n            entities.append(info)\n            #print(an[\"start\"],an[\"end\"],an[\"tag_name\"])\n    if len(entities) > 0:\n        TRAIN_DATA.append(([content,{\"entities\":entities}]))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Spacy model\nI am currently using a blank spacy model. I have tried using 'en_core_web_sm' and 'en_core_web_lg', but it doesnt result in any dramatic improvement. \n\n> There is a lot of scope for experimenting here. When I get time, I will try out more things !"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function\nimport random\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\nimport spacy\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.util.use_gpu(0)\ndef train_model(model=None, output_dir=\"/kaggle/working/medical-ner\", n_iter=1000):\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        if model is None:\n            nlp.begin_training(device=0)\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 64.0, 1.2))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  \n                    annotations,  \n                    drop=0.20, \n                    losses=losses\n                   \n                )\n            print(\"Losses\", losses)\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predections on CORD-19-research-challenge dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp2 = spacy.load(\"/kaggle/working/medical-ner\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport random\n\nfiles = []\nfor dirname, _, filenames in os.walk('/kaggle/input/CORD-19-research-challenge/'):\n    for filename in filenames:\n        if \".json\" in filename:           \n            fpath = os.path.join(dirname, filename)\n            if len(files) < 300:\n                files.append(fpath)\nrandom.shuffle(files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = []\nentities = []\nfor i in range(0,len(files)):\n    if i%100 == 0:\n        print('completed ', i)\n    with open(files[i]) as f:\n        file_data = json.load(f)        \n    for o in file_data[\"body_text\"]: \n            doc = nlp2(o[\"text\"],disable=['parser','tagger'])\n            for ent in doc.ents:\n                if len(ent.text) > 2:\n                    entities.append((ent.text, ent.label_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot the results on sample dataset (where count > n)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [12, 6]\npathogens = [l[0] for l in entities if l[1] == 'Pathogen']\ncounts = Counter(pathogens)\ncounts = {x : counts[x] for x in counts if counts[x] >= 20}\nplt.title(\"Pathogens detected so far !\")\nplt.xticks(rotation='vertical')\nplt.bar(counts.keys(),counts.values())\nplt.show()\nplt.savefig('path.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medical_conds = [l[0] for l in entities if l[1] == 'MedicalCondition']\ncounts = Counter(medical_conds)\ncounts = {x : counts[x] for x in counts if counts[x] >=20 and len(x) > 4}\nplt.xticks(rotation='vertical')\nplt.title(\"Medical Conditions detected so far !\")\nplt.bar(counts.keys(),counts.values(),color =\"g\")\nplt.show()\nplt.savefig('mc.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"medicines = [l[0] for l in entities if l[1] == 'Medicine']\ncounts = Counter(medicines)\ncounts = {x : counts[x] for x in counts if counts[x] >=35 and len(x) > 4}\n#plt.xticks(counts.keys(),rotation='vertical')\nplt.xticks(rotation='vertical')\nplt.title(\"Medicines detected so far !\")\nplt.bar(counts.keys(),counts.values(),color=\"y\")\nplt.show()\nplt.savefig('med.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Things to improve upon\n\n* An accurate NER tagger is key to my approach. However, it is difficult to create a manually tagged corpus single handedly. Need to collobrate on this with community !\n\n* Create a scheme to augument the manually labelled data and expand training data\n\n* Spacy does not give any 'confidence' measure along with NER tagging. This makes it impossible to apply a threshould and filter out unimportant or wrongly identified entities. Maybe using some library as a base may help !\n\n* Implement mechanisms to check the accuracy of NER tagging after training. Currently it does work, but not sure how well !"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}