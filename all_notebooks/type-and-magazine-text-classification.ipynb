{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python"}},"cells":[{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import OrderedDict","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"df = pd.read_csv('../input/ISIS Religious Texts v1.csv', encoding = \"ISO-8859-1\")\ndf.head()","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"import re\ndef clearstring(string):\n    try:\n        string = re.sub('[^A-Za-z0-9 .]+', '', string)\n        string = string.split(' ')\n        string = filter(None, string)\n        string = [y.strip() for y in string]\n        string = ' '.join(string)\n    except:\n        print(string)\n    return string","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"# remove some unnecessary symbols \nfor i in range(df.shape[0]):\n    df['Quote'].iloc[i] = clearstring(df['Quote'].iloc[i])","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"# remove last row, empty row\ndf = df.iloc[:-1, :]\ndf.head()","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"# I just want verbs and nouns in a sentence\ndef get_clean_text(string):\n    blob = TextBlob(string).tags\n    tags = []\n    # you can add more\n    accept = ['NNP', 'NN', 'NNS', 'NNPS', 'VBZ', 'VBN', 'VB']\n    for k in blob:\n        if k[1] in accept:\n            tags.append(k[0])\n            \n    return list(OrderedDict.fromkeys(tags))","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"# we need to split by ('.') to save our memory during speech tagging process to build the tree\nfor i in range(df.shape[0]):\n    texts = df['Quote'].iloc[i].split('. ')\n    tags = []\n    for t in texts:\n        tags += get_clean_text(t)\n    df['Quote'].iloc[i] = ' '.join(list(OrderedDict.fromkeys(tags)))","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"df['Quote'].iloc[1]","cell_type":"code"},{"metadata":{},"source":"It cleans already\n\nBelow I want to get freq of unique for certain columns i want, you can do bar graph","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"df['Type'].value_counts()","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"df['Purpose'].value_counts()","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"df['Magazine'].value_counts()","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"df['Source'].value_counts()","cell_type":"code"},{"metadata":{},"source":"How about we predict Magazine from Source?","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"labels = df['Magazine'].values.copy()\n\n# get unique label\nunique_labels = np.unique(labels)\n# change into int\nlabels = LabelEncoder().fit_transform(labels)\ntexts = df['Quote'].values.copy()","cell_type":"code"},{"metadata":{},"source":"I will use traditional Bag-of-Word and tf-idf for changing text to vectors space","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"bag_counts = CountVectorizer().fit_transform(texts)\nbag_counts.shape","cell_type":"code"},{"metadata":{},"source":"That means, there are 8829 unique of word, every words in a sentence will add value by 1 if got in the vectors.\n\nExample, sentence is 'I LOVE YOU YOU YOU', our vector got [0, 0, 0] represent 'I LOVE YOU',\n\nthat mean our vector for 'I LOVE YOU YOU YOU' is [1, 1, 3]","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"bag_counts_tdidf = TfidfTransformer(use_idf = True).fit_transform(bag_counts)\nbag_counts_tdidf.shape","cell_type":"code"},{"metadata":{},"source":"This is the function of tf-idf\n![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/10109d0e60cc9d50a1ea2f189bac0ac29a030a00)","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"# just want to show example\ndel bag_counts_tdidf, bag_counts","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"# i use pipeline to do automate processing to feed into my classifier. bag of word -> tf-idf -> SGD\nmagazine_clf = Pipeline([('vect', CountVectorizer()), \n                         ('tfidf', TfidfTransformer()), \n                         ('clf', SGDClassifier(loss = 'modified_huber', \n                                               penalty = 'l2', alpha = 1e-4, \n                                               n_iter = 100, random_state = 42))])","cell_type":"code"},{"metadata":{},"source":"How about some visualization for our text data?","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ncurrent_palette = sns.color_palette()\n# blue and red from seaborn\ncolors = [current_palette[0], current_palette[2]]\n\n# visualize 20% of our data\n_, x, _, y = train_test_split(texts, labels, test_size = 0.2)\n\nplt.rcParams[\"figure.figsize\"] = [10, 10]\nax = plt.subplot(111)\n\nX = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),]).fit_transform(x).todense()\ntsne = TSNE(n_components = 2).fit_transform(X)\nfor no, _ in enumerate(np.unique(unique_labels)):\n    ax.scatter(tsne[y == no, 0], tsne[y == no, 1], c = colors[no], label = unique_labels[no])\n    \nbox = ax.get_position()\nax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\nax.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.05), fancybox = True, shadow = True, ncol = 5)\nplt.show()","cell_type":"code"},{"metadata":{},"source":"We will got low accuracy for this if reduce the dimension","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size = 0.2)\nmagazine_clf.fit(x_train, y_train)\npredicted = magazine_clf.predict(x_test)\nprint (np.mean(predicted == y_test))","cell_type":"code"},{"metadata":{},"source":"We got 81 accuracy for validation set! how about f1 score?","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"print(metrics.classification_report(y_test, predicted, target_names = unique_labels))","cell_type":"code"},{"metadata":{},"source":"Good enough! how about we do discrimination on 'Type'?","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"types = df['Type'].values.copy()\n\n# get unique label\nunique_types = np.unique(types)\n# change into int\ntypes = LabelEncoder().fit_transform(types)","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"current_palette = sns.color_palette(n_colors = unique_types.shape[0])\n\n# visualize 20% of our data\n_, x, _, y = train_test_split(texts, types, test_size = 0.2)\n\nplt.rcParams[\"figure.figsize\"] = [10, 10]\nax = plt.subplot(111)\n\nX = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),]).fit_transform(x).todense()\ntsne = TSNE(n_components = 2).fit_transform(X)\nfor no, _ in enumerate(np.unique(unique_types)):\n    ax.scatter(tsne[y == no, 0], tsne[y == no, 1], c = current_palette[no], label = unique_types[no])\n    \nbox = ax.get_position()\nax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\nax.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.05), fancybox = True, shadow = True, ncol = 5)\nplt.show()","cell_type":"code"},{"metadata":{},"source":"It clustered and sticked nearly each others according to population, good!","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"# i use pipeline to do automate processing to feed into my classifier. bag of word -> tf-idf -> SGD\ntype_clf = Pipeline([('vect', CountVectorizer()), \n                         ('tfidf', TfidfTransformer()), \n                         ('clf', SGDClassifier(loss = 'modified_huber', \n                                               penalty = 'l2', alpha = 1e-4, \n                                               n_iter = 100, random_state = 42))])","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"x_train, x_test, y_train, y_test = train_test_split(texts, types, test_size = 0.2)\ntype_clf.fit(x_train, y_train)\npredicted = type_clf.predict(x_test)\nprint(np.mean(predicted == y_test))\nprint(metrics.classification_report(y_test, predicted, target_names = unique_types))","cell_type":"code"},{"metadata":{},"source":"['Bible', 'Fatwa', 'Fiqh', 'Hadith Commentary'] seems overfit, anyways, it is good enough to do text classification!","cell_type":"markdown"}],"nbformat_minor":1,"nbformat":4}