{"cells":[{"metadata":{"id":"IU_X2U0YvQ0S"},"cell_type":"markdown","source":"## Importing required libraries","execution_count":null},{"metadata":{"id":"jV2IxXDJ3kMn","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.model_selection import train_test_split , StratifiedKFold,GridSearchCV\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import accuracy_score,recall_score , precision_score,make_scorer,confusion_matrix,precision_recall_curve,mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.svm import SVR\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"P9saLyuUvBx_"},"cell_type":"markdown","source":"## Task 1 : Predicting the outcome of a new patient","execution_count":null},{"metadata":{"id":"7kXuftnNYHKC"},"cell_type":"markdown","source":"##1. Loading the data","execution_count":null},{"metadata":{"id":"eCuJeBH0LFfE","trusted":false},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"_JUSSUpvYaUM"},"cell_type":"markdown","source":"#2. Data preprocessing","execution_count":null},{"metadata":{"id":"xvcbdOFXBaeJ","outputId":"ef0e4911-33bb-4e94-bcd4-937b4e2f317e","trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"VJhAjZNB39HE","outputId":"35e64668-ed9a-4769-8ad5-4d3b8f77eaec","trusted":false},"cell_type":"code","source":"df.isnull().sum()  # checking for null values ","execution_count":null,"outputs":[]},{"metadata":{"id":"ANdN39pgKgF5","trusted":false},"cell_type":"code","source":"df=df.drop(['Unnamed: 32'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"AJ-mer94Y7Tz"},"cell_type":"markdown","source":"#3.Visualizing data","execution_count":null},{"metadata":{"id":"VhkTIAuAZCVf"},"cell_type":"markdown","source":"##a). Checking for outliers by plotting boxplot taking 10 features at a time","execution_count":null},{"metadata":{"id":"L8sVAFv6CXrw","outputId":"466beefa-0a0b-4a91-cbd0-aa5a618a5d21","trusted":false},"cell_type":"code","source":"# for mean features\n\ndata = pd.concat([df.diagnosis,df.iloc[:,2:12]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",var_name=\"features\",value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\",whis=2.5, data=data)\nplt.xticks(rotation=90)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ykC3Qf_PUHYl","outputId":"a58c5ae7-22f7-4df0-992b-b7b034b33b91","trusted":false},"cell_type":"code","source":"#for std_dev features\n\ndata2 = pd.concat([df.diagnosis,df.iloc[:,12:22]],axis=1)\ndata2 = pd.melt(data2,id_vars=\"diagnosis\",var_name=\"features\",value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", whis=2.5,data=data2)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"id":"R-hz-NZSWoho","outputId":"87df1a2f-1c42-4f79-a970-d153dfacbe47","trusted":false},"cell_type":"code","source":"# for worst features\n\ndata3 = pd.concat([df.diagnosis,df.iloc[:,22:32]],axis=1)\ndata3 = pd.melt(data3,id_vars=\"diagnosis\",var_name=\"features\",value_name='value')\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", whis=2.5,data=data3)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"id":"zHFqDSUgoeho"},"cell_type":"markdown","source":"By looking at above 3 plots , we will clip the area parameters .\n","execution_count":null},{"metadata":{"id":"s1sKOT2JOJjl","outputId":"b7b5c6f1-8b15-4e98-f21a-275a9f2a4d49","trusted":false},"cell_type":"code","source":"df=df[df.area_mean<2000]\ndf=df[df.area_se<300]\ndf=df[df.area_worst<4000]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"03uMywJwZTNQ"},"cell_type":"markdown","source":"##b). Checking correlation between features by plotting heatmap\n\n","execution_count":null},{"metadata":{"id":"EGOExD-9oq1H","outputId":"85ff9f71-32cc-481b-8be6-dbbbc2d3643b","trusted":false},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True,linewidths=.5, fmt= '.2g',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"A-8_zjE2Z5IY"},"cell_type":"markdown","source":"#4. Feature selection","execution_count":null},{"metadata":{"id":"-YTu9uEoAAhr","outputId":"c0cb8dd8-cc87-4b20-a1bf-4a2ea681432f","trusted":false},"cell_type":"code","source":"#correlation between all mean and worst features\n\ncols_mean=list(df.columns)[2:12]\ncols_worst=list(df.columns)[22:32]\nfor i in range(0,10):\n    corr, _ = pearsonr(df[cols_mean[i]],df[cols_worst[i]])\n    print(cols_mean[i],'-',cols_worst[i],'=',corr)","execution_count":null,"outputs":[]},{"metadata":{"id":"xNqVk6O4bB5-"},"cell_type":"markdown","source":"As we can see in heatmap compactness_mean, concavity_mean and concave_points_mean are correlated with each other so we remove compactness_mean and concave_points_mean also area_mean, perimeter_mean and radius_mean are correlated with each other so we keep only area_mean .Similarly we remove compactness_se,concave points_se and keep concavity_se , remove  radius_std_dev, perimeter_std_dev\nand keep only area_std_dev , remove compactness_worst, concave_points__worst and keep only concavity_worst , remove radius_worst , perimeter_worst and keep only area_worst.","execution_count":null},{"metadata":{"id":"oV7G88r8yK2k","trusted":false},"cell_type":"code","source":"#dropping unwanted features as discussed above \n\ndrop_list = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\ndf=df.drop(drop_list,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"cXPHySg_UYiK","outputId":"c16ea03a-d15b-42d7-fdcd-4639889408c7","trusted":false},"cell_type":"code","source":"f1,ax1 = plt.subplots(figsize=(9,9))\nsns.heatmap(df.corr(), annot=True,linewidths=.5, fmt= '.2g',ax=ax1)  # after removing correlated features","execution_count":null,"outputs":[]},{"metadata":{"id":"3F-xwF8e-f7_"},"cell_type":"markdown","source":"#5.Model selection and validation","execution_count":null},{"metadata":{"id":"Je6yBnW--qLu"},"cell_type":"markdown","source":"We are splitting the given data in the ratio 7:3 or we can say 70% is our training data and rest 30% is our test data on which we will make predictions , calculate accuracy and submit the results ","execution_count":null},{"metadata":{"id":"IW0Y6fvd2LV9","trusted":false},"cell_type":"code","source":"# splitting the data  \n\nY=df['diagnosis']      # target variable\nen=LabelEncoder()\nY=en.fit_transform(Y)   # encode 'N' to 0 and 'R' to 1\nX=df.drop(['diagnosis'],axis=1)\nX_train , X_test , y_train , y_test=train_test_split(X,Y,test_size=0.3,random_state=44)\nids1=X_test['id']\nX_train=X_train.drop(['id'],axis=1)       # training data\nX_test=X_test.drop(['id'],axis=1)         # test data","execution_count":null,"outputs":[]},{"metadata":{"id":"JS1_YZm6A4n0"},"cell_type":"markdown","source":"We have to minimize false negatives(FN) because predicting the disease even if it is not present is less risky as compared to predicting disease free but actually it is present \nOr we can say we have to maximise recall score of our classifier which is formulated as :\n\n            Recall = TP/(TP+FN)","execution_count":null},{"metadata":{"id":"7UyMi0cVCIN_","trusted":false},"cell_type":"code","source":"# we are building custom function which fits the model with best hyperparametrs using grid search , calculates accuracy and recall score\n# We optimize our model to maximize recall score\n# It also plots precision recall curve and confusion matrix \n\ndef fit_model(model,Xtrain,Xtest,ytrain,ytest,features,param_grid):\n  scorers = {'recall_score': make_scorer(recall_score)}\n  kf = StratifiedKFold(n_splits=3)\n  grids = GridSearchCV(model,param_grid,scoring=scorers, refit='recall_score',cv=kf)\n  grids.fit(Xtrain[features],ytrain)\n  pred = grids.predict(Xtest)\n  print('Best parameters:',grids.best_params_)\n  accuracy = accuracy_score(pred,ytest)\n  print('Accuracy :',accuracy)\n  rscore=recall_score(ytest, pred, average='binary')\n  print('Recall-score:',rscore)\n  fpr, tpr, thresholds = precision_recall_curve(ytest, pred)\n  plt.subplot(1,2,1)\n  plt.plot(fpr,tpr, marker='.')\n  cm = confusion_matrix(ytest,pred)\n  plt.subplot(1,2,2)\n  sns.heatmap(cm,annot=True,fmt=\"d\")\n  return grids\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"0ZYMmFUiAOJp"},"cell_type":"markdown","source":"a) Using logistic regression\n\nSince target variable is binary hence our first approach would be Logistic Regression","execution_count":null},{"metadata":{"id":"D2u5ojjiCKjw","outputId":"6178296a-79c7-4f68-b4e9-9d00fd292ae8","trusted":false},"cell_type":"code","source":"feats = X_train.columns\nss= MinMaxScaler(feature_range=(0,1))  # scaling features for logistic regression\nX_train1=pd.DataFrame(ss.fit_transform(X_train),columns=feats)\nX_test1=pd.DataFrame(ss.fit_transform(X_test),columns=feats)\nparam_grid = {'C': [1],'max_iter':[100,200,300]}\n\nlr=LogisticRegression(random_state=8)\nfitted=fit_model(lr,X_train1,X_test1,y_train,y_test,feats,param_grid)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"uEI3wc6QAaJK"},"cell_type":"markdown","source":"b) Using random forests","execution_count":null},{"metadata":{"id":"I7chMZny7WmP","outputId":"5225d30a-5254-40aa-ab53-21508077033f","trusted":false},"cell_type":"code","source":"feats = X_train.columns\nparam_grid1 = {'n_estimators': [70,80],'min_samples_split':[4,5],'max_depth':[7,9],'max_features':[9]}\nrf = RandomForestClassifier(random_state=8)\ngrid_search_clf=fit_model(rf,X_train,X_test,y_train,y_test,feats,param_grid1)","execution_count":null,"outputs":[]},{"metadata":{"id":"q91JlbeDCVnF"},"cell_type":"markdown","source":"c) By adjusting decision threshold of above Random forest model (default is 0.5)","execution_count":null},{"metadata":{"id":"YRdjoP3nwYig","outputId":"e99e9fcc-ee1d-4ac6-fd4d-6f104d6cf5c0","trusted":false},"cell_type":"code","source":"#checking class distribution\n\ndf2 = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv') \nsns.catplot(x=\"diagnosis\", kind=\"count\", palette=\"ch:.75\", data=df2);\n","execution_count":null,"outputs":[]},{"metadata":{"id":"OGOkoRzYKuM-"},"cell_type":"markdown","source":"As we can see in above plot , our data is class imbalanced hence predictions would be much baised towards 'B' class.\nSo we can tune our classification threshold for random forest\nuntill we get minimum False negatives ","execution_count":null},{"metadata":{"id":"qd8uLXl2XExc","trusted":false},"cell_type":"code","source":"y_probs = grid_search_clf.predict_proba(X_test)[:, 1]\np, r, thresholds = precision_recall_curve(y_test, y_probs)\n\n\ndef threshold_tuner(y_scores, t):    # for setting threshold\n    \n    return [1 if y >= t else 0 for y in y_scores]\n\ndef confusion_m(p, r, thresholds, t):     # plot confusion matrix after setting threshold and return new predictions\n    \n    y_pred_adj = threshold_tuner(y_probs, t)\n    cm1 = confusion_matrix(y_test,y_pred_adj)\n    sns.heatmap(cm1,annot=True,fmt=\"d\")    \n    predicted=y_pred_adj\n    print('Accuracy:',accuracy_score(predicted,y_test))\n    return predicted","execution_count":null,"outputs":[]},{"metadata":{"id":"hgGryFCgTR65"},"cell_type":"markdown","source":"We will tune the threshold until false negatives reach minimum","execution_count":null},{"metadata":{"id":"_fsfFINIXTWf","outputId":"4d7bca44-790b-4ffe-f1cb-058f6e52e7ea","trusted":false},"cell_type":"code","source":"pr=confusion_m(p, r, thresholds, 0.51)","execution_count":null,"outputs":[]},{"metadata":{"id":"93FT-paZVk5U"},"cell_type":"markdown","source":"So our optimum threshold is t=0.51","execution_count":null},{"metadata":{"id":"dks8OYLTnk6g","trusted":false},"cell_type":"code","source":"#Reverse encoding 1 to 'M' and 0 to 'B'\n\ntask1_pred=[]             \nfor x in range(0,len(pr)):\n  if pr[x]==0:\n    task1_pred.append('B')\n  elif pr[x]==1:\n    task1_pred.append('M')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"qDGM6sYL2qsr","trusted":false},"cell_type":"code","source":"# creating csv file of test predictions\n\nsub1 = pd.DataFrame({\"ID\": ids1, \"Outcome\": task1_pred})\nsub1.to_csv('task_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}