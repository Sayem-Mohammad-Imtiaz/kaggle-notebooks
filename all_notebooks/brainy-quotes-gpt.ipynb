{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\nimport math\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\ntqdm.pandas()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T04:42:23.154208Z","iopub.execute_input":"2021-07-29T04:42:23.154562Z","iopub.status.idle":"2021-07-29T04:42:29.954596Z","shell.execute_reply.started":"2021-07-29T04:42:23.154486Z","shell.execute_reply":"2021-07-29T04:42:29.953789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BrainyQuoteDataset(torch.utils.data.Dataset):\n    def __init__(self,text_list,tokenizer,max_len):\n        self.text_list = text_list\n        self.tokenizer = tokenizer\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.max_len = max_len\n    def __getitem__(self,index):\n        text = self.text_list[index]\n        return tokenizer.encode_plus(text,tokenizer.eos_token,padding=\"max_length\",max_length=self.max_len,return_tensors='pt')\n    def __len__(self):\n        return len(self.text_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:42:29.956014Z","iopub.execute_input":"2021-07-29T04:42:29.956336Z","iopub.status.idle":"2021-07-29T04:42:29.96191Z","shell.execute_reply.started":"2021-07-29T04:42:29.956302Z","shell.execute_reply":"2021-07-29T04:42:29.961129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([pd.read_csv(f) for f in glob.glob(\"../input/brainyquote-topics/\"+'*.csv')])\ntrain_df,test_df = train_test_split(df,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:42:29.963516Z","iopub.execute_input":"2021-07-29T04:42:29.964046Z","iopub.status.idle":"2021-07-29T04:42:31.202177Z","shell.execute_reply.started":"2021-07-29T04:42:29.964009Z","shell.execute_reply":"2021-07-29T04:42:31.201327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = GPT2LMHeadModel.from_pretrained('distilgpt2')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:42:31.203472Z","iopub.execute_input":"2021-07-29T04:42:31.203808Z","iopub.status.idle":"2021-07-29T04:42:45.660794Z","shell.execute_reply.started":"2021-07-29T04:42:31.203773Z","shell.execute_reply":"2021-07-29T04:42:45.659766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len = 0\n# def find_max_len(text):\n#     global max_len\n#     curr_len = len(tokenizer.encode(text))\n#     if curr_len > max_len:\n#         max_len = curr_len\n# df['title'].progress_apply(find_max_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:42:47.167212Z","iopub.execute_input":"2021-07-29T04:42:47.16753Z","iopub.status.idle":"2021-07-29T04:42:47.171502Z","shell.execute_reply.started":"2021-07-29T04:42:47.1675Z","shell.execute_reply":"2021-07-29T04:42:47.170412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 128\nbatch_size = 8\ntrain_dataset = BrainyQuoteDataset(train_df['title'].tolist(),tokenizer,max_len)\ntest_dataset = BrainyQuoteDataset(test_df['title'].tolist(),tokenizer,max_len)\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\ntest_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n# Define two sets of parameters: those with weight decay, and those without\noptimizer_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n]\nepochs = 5\noptimizer = AdamW(optimizer_parameters, lr=5e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=math.floor(len(train_dataloader)*epochs/2), num_training_steps=len(train_dataloader)*epochs\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:42:47.93944Z","iopub.execute_input":"2021-07-29T04:42:47.93978Z","iopub.status.idle":"2021-07-29T04:42:47.966974Z","shell.execute_reply.started":"2021-07-29T04:42:47.939748Z","shell.execute_reply":"2021-07-29T04:42:47.965851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:42:49.477708Z","iopub.execute_input":"2021-07-29T04:42:49.478075Z","iopub.status.idle":"2021-07-29T04:42:49.484869Z","shell.execute_reply.started":"2021-07-29T04:42:49.478042Z","shell.execute_reply":"2021-07-29T04:42:49.483839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_loss = 9999\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_loss_ls = []\navg_meter = AverageMeter()\nmodel.to(device)\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0\n    avg_meter.reset()\n    tk =  tqdm(train_dataloader)\n    for data in tk:\n        optimizer.zero_grad()\n        input_ids = data['input_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n        out = model(input_ids,labels=input_ids,attention_mask=attention_mask)\n        loss = out[0]\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        train_loss += loss.item()\n        train_loss_ls.append(loss.item())\n        avg_meter.update(loss.item(),input_ids.shape[0])\n        tk.set_postfix({'loss':avg_meter.avg})\n    test_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for data in tqdm(test_dataloader):\n            input_ids = data['input_ids'].to(device)\n            attention_mask = data['attention_mask'].to(device)\n            out = model(input_ids,labels=input_ids,attention_mask=attention_mask)\n            loss = out[0]\n            test_loss += loss.item()\n    if test_loss<best_loss:\n        best_loss = test_loss\n        torch.save(model.state_dict(), 'best_brainyquotegpt2.pth')\n    print(f\"epoch: {epoch} train loss: {train_loss/len(train_dataloader)} test loss: {test_loss/len(test_dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:42:51.306012Z","iopub.execute_input":"2021-07-29T04:42:51.306325Z","iopub.status.idle":"2021-07-29T04:43:28.492808Z","shell.execute_reply.started":"2021-07-29T04:42:51.306297Z","shell.execute_reply":"2021-07-29T04:43:28.490689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_loss_ls)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T18:01:31.615733Z","iopub.execute_input":"2021-07-28T18:01:31.616277Z","iopub.status.idle":"2021-07-28T18:01:31.750036Z","shell.execute_reply.started":"2021-07-28T18:01:31.61623Z","shell.execute_reply":"2021-07-28T18:01:31.749223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nprompt = \"Inspiration is\"\nwith torch.no_grad():\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n    input_ids = input_ids.to(device)\n    outputs = model.generate(\n        input_ids,\n        max_length=1024, \n        do_sample=True, \n        top_k=50, \n        top_p=0.95, \n    )\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T18:01:36.3289Z","iopub.execute_input":"2021-07-28T18:01:36.329219Z","iopub.status.idle":"2021-07-28T18:01:36.930002Z","shell.execute_reply.started":"2021-07-28T18:01:36.32919Z","shell.execute_reply":"2021-07-28T18:01:36.928959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'brainyquotegpt2.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}