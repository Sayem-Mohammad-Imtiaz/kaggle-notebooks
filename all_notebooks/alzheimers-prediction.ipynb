{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport sklearn as sk\nimport pandas as pd\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\ndf = pd.read_csv('/kaggle/input/alzheimers-clinical-data/clinical-data-for Alzheimers.csv')\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cleaning data\ndf['mmse'] = df['mmse'].str.replace(r'?', '72')\ndf['memory'] = df['memory'].str.replace(r'?','1' )\n\ndf = df.dropna()\ndf.isnull().sum().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data to variables and target\nX = df\nY = df['dx1']\ndel df\n\n#Remove unnecessary columns (features), remove first 9 columns and 'Dx codes for submission'\nremove_columns = list(X.columns)[0:1]\nremove_columns.append('dx1')\n\nprint('Removing columns:', remove_columns)\n\nX = X.drop(remove_columns, axis=1)\n\nfeatures = list(X.columns)\nX.head(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classify variables into numerical ans catergorical variables\nnumerical_vars = ['ageAtEntry', 'mmse', 'cdr','memory']\ncategorical_vars = list(set(features) - set(numerical_vars))\n\nprint('Categorical variable distributions:\\n')\n\nfor var in categorical_vars:\n    print('\\nDistribution of', var)\n    \n    print(X[var].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize  data\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nprint('Numerical Variable Distributions:\\n')\n\nfor var in numerical_vars:\n    plt.hist(X[var], bins=10)\n    plt.title(var + ' Distribution')\n    plt.show()\n    \n    # descriptive stats\n    print(X[var].describe())\n    print(X[var].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(Y.value_counts().index, Y.value_counts())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pre-processing\n#for each categorical var, convert to 1-hot encoding\nfor var in categorical_vars:\n    print('Converting', var, 'to 1-hot encoding')\n    \n    #get 1-hot and replace original column with the >= 2 categories as columns\n    one_hot_df = pd.get_dummies(X[var])\n    X = pd.concat([X, one_hot_df], axis=1)\n    X = X.drop(var, axis=1)\n    \nX.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n\nprint('X_train:', X_train.shape, '\\ty_train:', y_train.shape)\nprint('X_test:', X_test.shape, '\\ty_test:', y_test.shape)\nnum_test = X_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#buliding model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.exceptions import FitFailedWarning\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_testlog_clf = LogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000000, multi_class='multinomial')\nprint('Validation Accuracy = ', format(cross_val_score(log_clf, X_train, y_train, cv=5).mean(), '.2%'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=200)\nprint('Validation Accuracy = ', format(cross_val_score(rf_clf, X_train, y_train, cv=5).mean(), '.2%'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf = KNeighborsClassifier(n_neighbors=10)\nprint('Validation Accuracy = ', format(cross_val_score(knn_clf, X_train, y_train, cv=5).mean(), '.2%'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_clf = MLPClassifier(hidden_layer_sizes=(15, 10), alpha=3, learning_rate='adaptive', max_iter=100000)\nprint('Validation Accuracy = ', format(cross_val_score(mlp_clf, X_train, y_train, cv=5).mean(), '.2%'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVC_clf=SVC()\nprint('Validation Accuracy = ', format(cross_val_score(SVC_clf, X_train, y_train, cv=5).mean(), '.2%'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_clf=GaussianNB()\nprint('Validation Accuracy = ', format(cross_val_score(NB_clf, X_train, y_train, cv=5).mean(), '.2%'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDT_clf= DecisionTreeClassifier()\nprint('Validation Accuracy = ', format(cross_val_score(DT_clf, X_train, y_train,cv=5).mean(), '.2%'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate feature importances given by Random Forest\n\nrf_clf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = pd.DataFrame(rf_clf.feature_importances_, index=X_train.columns, \n                                   columns=['Importance']).sort_values('Importance', ascending=False)\nprint(feature_importances[:10])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}