{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chargement et stockage des données","metadata":{}},{"cell_type":"markdown","source":"**Le type de données objet peut contenir plusieurs types. Par exemple, la colonne peut inclure des entiers, des réels et des chaînes de caractére qui sont collectivement étiquetés comme un objet**","metadata":{}},{"cell_type":"code","source":"# importer la librairie\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#telecharger la fonction word_tokenize de la Bibliothèques nltk.tokenize, qui permet de couper les messages en unité linguistique individuelle donc en mot.\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# charger les données du fichier sms spam.\nsms = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\nsms.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Renommer les colonnes 1 et 2 en Label et message","metadata":{}},{"cell_type":"code","source":"# grâce à rename on renomme les différentes colonnes\nsms= sms.rename(columns={\"v1\":\"label\",\"v2\":\"message\"})\nsms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retirer les colonnes 3, 4 et 5","metadata":{}},{"cell_type":"code","source":"# grâce à dropna, on retire les 3 colonnes qui sont sans informations donc inutiles\nsms = sms.dropna(axis=\"columns\")\nsms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Afficher le diagramme de la colonne Label","metadata":{}},{"cell_type":"code","source":"#importer la bibliothèque pour les diagramme\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#diagramme qui affiche les spams et les hams    \nsns.countplot(sms.label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# afficher le nombre de ligne et de colonne de la data frame sms\nsms.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Nettoyage de la colonne message","metadata":{}},{"cell_type":"code","source":"# passage des lettres en majuscule à des lettres en minuscule\nsms['message']=sms['message'].str.lower()\nsms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importation de la variable pontuation afin de faire apparaitre toutes les ponctuations\nimport string\nstring.punctuation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punct(result):\n    #diviser texte en deux morceaux\n    result_tok=word_tokenize(result)\n    l=[]\n    for token in result_tok:\n        \n    # test si token n'est pas dans ponc\n     if not token in string.punctuation:\n        l.append(token)\n    \n    resultat=\" \".join(l)\n    \n    return resultat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Dans le programme précedent, nous réalisons dans un premier temps le word tokenize, qui permet de couper les messages en unités linguistiques individuelles, donc en mot.\nDans un second temps nous créons une variable l qui est vide.\nEnsuite pour un mot qui fait parti de nos mots, nous vérifions si il fait parti de la pontuation ou pas. Si le token est un mot, il est implanté dans la varible L. Si c'est une ponctuation, elle ne rentre pas dans L.\nLa ligne resultat= \"\".join(l) signifie que nous reformons les phrases.","metadata":{}},{"cell_type":"code","source":"# retirer les ponctuations sur la colonne message\nsms['message']=sms.message.apply(remove_punct)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# afficher le fichier modifié\nsms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Les stop-words sont les mots très courants dans la langue étudiée (\"et\", \"à\", \"le\"... en français) qui n'apportent pas de valeur informative pour la compréhension du \"sens\" d'un document. Ils sont très fréquents et ralentissent notre travail : nous souhaitons donc les supprimer.**","metadata":{}},{"cell_type":"code","source":"#Télècharger les stopwords grace à la librairie\nfrom nltk.corpus import stopwords\nBadword=set(stopwords.words('english'))\n#stocker tout dans la variable Badword\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Supprimer les stop word\ndef remove_stopword(result):\n    \n    result_tok=word_tokenize(result)\n    l=[]\n    for token in result_tok:\n    # test si token n'est pas dans ponc\n     if not token in Badword:\n        l.append(token)\n    \n    resultat=\" \".join(l)\n    \n    \n    return resultat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Dans le programme précedent, nous allons supprimer les stopwords,c'est à dire les petits mots qui ne sont pas utiles pour la compréhension du texte.\nNous avons donc défini la fonction remove stopwords, nous allons donc dans un premier temps séparé toute les phrases en mot. Puis nous créons une variable l vide.\ndans la boucle for, nous vérifions si les mots sont des badword donc des stopwords ou non. Si le mot est un badword, il n'est pas ajouté à la varible liste l. Si le mot n'est pas un badword, il est alors placé dans la variable l.\nLa ligne resultat=\" \".join(l) nous permet ensuite de reformer les phrases","metadata":{}},{"cell_type":"code","source":"# appliquer la fonction stopword sur la colonne message\nsms['message']=sms.message.apply(remove_stopword)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# afficher sms avec les stopword en moins.\nsms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**On remplace les mots par leur forme canonique**","metadata":{}},{"cell_type":"code","source":"# On importe la fonction nltk\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\ncano=WordNetLemmatizer()\n# remplace les mots au pluriel en un mot au singulier par exemple ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def canonic(result):\n    \n    result_tok= word_tokenize(result)\n    l=[]\n    \n    for token in result_tok : \n        l.append(cano.lemmatize(token))\n    resultat= \" \".join(l)\n    \n    \n    return resultat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Dans ce programme, nous réalisons les mêmes deux premières étapes que pour les fonction remove punct et remove stopword.\nNous avons ensuite une boucle for, celle-ci permet de dire que lorsqu'un mot est dans nos messages, il faut réaliser le lemmatizer, donc la transformation en sa forme canonique.","metadata":{}},{"cell_type":"code","source":"# Appliquer la fonction canonic sur la colonne message\nsms.message=sms.message.apply(canonic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Afficher le sms modifié \nsms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectoriser la colonne des messages avec la technique de Bag of words","metadata":{}},{"cell_type":"code","source":"# définir la base de donnée\ncorpus=sms['message'].values\ncorpus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n#count car elle fait le comptage de tous les mots \nbw_vect = CountVectorizer()\n\n# tokenize et construire le vocabulaire\n#.fit veut dire je vais l'apppliquer sur\nbw_fit=bw_vect.fit(corpus)\n\n# vectoriser les mots,donc construire le tableau\nbw_corpus = bw_fit.transform(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bw_corpus.shape\n# voir sa dimension grace à .shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bw_fit.get_feature_names()\n# .get_feature_names affiche tous les mots différents ici nous l'avons mis avec un # car la liste est très longue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bw_corpus.toarray()\n# afficher les valeurs, le 1 veut dire que la phrase contient une fois le mot, il peut également y avoir des 2,3,4....","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_data=pd.DataFrame(bw_corpus.toarray(),columns=bw_fit.get_feature_names())\ncv_data\n# Assigner dans les colonnes les mots et afficher dans les lignes les valeurs matricielles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectoriser la colonne des messages avec la technique de TF-IDF\n","metadata":{}},{"cell_type":"markdown","source":"**C'est une mesure statistique permet d'évaluer l'importance d'un terme contenu dans un document, relativement à une collection ou un corpus. Le poids augmente proportionnellement au nombre d'occurrences du mot dans le document. Il varie également en fonction de la fréquence du mot dans le corpus.**","metadata":{}},{"cell_type":"code","source":"#telecharger la fonction TfidfVectorizer de la Bibliothèques sklearn.feature_extraction.text\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialiser les paramètres du vectoriseur\n# 10000 veut dire que nous avons laissé la visibilité sur 10000 mots. Ici ca n'a pas \n# d'importance puisque nous avons moins de mots, dans le cas ou nous aurions eu plus de mots, ca aurait réduit la quantité.\ntf_vect = TfidfVectorizer(max_features=10000)\n#Apprendre le vocabulaire du vectoriseur basé sur le paramètre initialisé\ntfidf_fit=tf_vect.fit(corpus)\n#Vectoriser le corpus\ntfidf_corpus= tfidf_fit.transform(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tfidf_fit.get_feature_names()\n#afficher les valeurs, ici avec un # car trop long","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Afficher le tableau \ntfidf_data=pd.DataFrame(tfidf_corpus.toarray(),columns=tfidf_fit.get_feature_names())\ntfidf_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Préparation des données pour l'algorithme apprentissage","metadata":{}},{"cell_type":"code","source":"# créer deux listes une d'entrainement et une de test\nfrom sklearn.model_selection import train_test_split\n# La fonction train_test_split permet de décomposer le jeu de données en 2 groupes: les données pour l'apprentissage et les données pour les tests. Le paramètre test_size indique la taille du jeu du teste\nX = cv_data\n# X contient le tableau avec les chiffres de bag of words\nY = sms.label\n# Y contient les infos spam ou ham\n# Split train / test data :\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n# X_train et Y_Train son pour l'apprentissage et l'arbre de décision, X_Text et Y_test sont les vraie valeuer et permettront la comparaison des résultats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Construction de l'arbre de decision","metadata":{}},{"cell_type":"markdown","source":"**Un arbre de décision est un arbre orienté dont les noeuds sont étiquetés par un test et les arcs contiennent les résultats du test. On choisit de faire un test sur la variable qui disperse le mieux les classes.**","metadata":{}},{"cell_type":"code","source":"#importe les données pour l'arbre\nfrom sklearn import tree\ntree_model = tree.DecisionTreeClassifier()\n#tree_model = tree.DecisionTreeClassifier(max_depth = 2)\n# apprendre au logiciel qu'on utilise les deux classes pour construire l'arbre\ntree_model=tree_model.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Programme pour les dimensions de l'arbre\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,10))\nnames = ['Spam', 'Ham']\ntree.plot_tree(tree_model,feature_names = X.columns, \n               class_names=names,\n               filled = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# défini la variable de prédiction qui en fonction des X text nous permet de voir si on peut prédir ou non en fonction de l'arbre\nY_predict=tree_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation de l'arbre de décision","metadata":{}},{"cell_type":"code","source":"# Plot the Confusion Matrix :\nfrom sklearn.metrics import accuracy_score, confusion_matrix \nmat = confusion_matrix(Y_predict, Y_test)\nprint(mat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.figure(figsize=(15,5))\nsns.heatmap(mat, annot=True,  xticklabels=names, yticklabels=names)\nplt.xlabel('Test')\nplt.ylabel('Predicted')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy: c'est le pourcentage de bonne réponse\na_CART = accuracy_score(Y_test,Y_predict)\nprint(\"L'accuracy score du modèle CART est de : \",a_CART)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"Le tableau de prédiction à une marge d'erreur.\nEn conclusion, nous avons pu réaliser tout au long de nos tp des codes afin de simplifier des recherches ou de réaliser des présentation de réponses. \nNous avons découvert le traitement naturel du langage, aussi appelé Natural Language Processing ou NLP en anglais, qui est une technologie permettant aux machines de comprendre le langage humain grâce à l’intelligence artificielle.\nNous avons ensuite pu trier des variables, grâce à des arbres de décisions ou encore des fôret d'arbre de décision. ","metadata":{}},{"cell_type":"markdown","source":"#  La méthode Gridsearch pour déterminer la meilleure profondeur entre 10 et 40 pour l'algorithme de l'arbre de décision","metadata":{}},{"cell_type":"markdown","source":"**Permet de déterminer n'importe quel paramètres et fait la recherche. Elle choisit le paramètre qui maximize la réponse**","metadata":{}},{"cell_type":"code","source":"# On importe des données et la fonction Gridsearch, qui permet de déterminer la meilleur profondeur pour déterminer si le message est un spam ou un ham\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndepths = np.arange(10, 40,5) # La profondeur maximale de l'arbre, toutes les valeurs entre 10 et 40 avec un pas de 5\nnum_features = np.arange(1,X.shape[1]) # organiser \n\nparam_grid = [{'max_depth':depths}]\n# variable pour trouver la bonne profondeur","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# détermine la meilleur profondeur en calaculant l'accuracy\ngrid_tree= GridSearchCV(estimator=tree.DecisionTreeClassifier(),param_grid=param_grid,scoring='accuracy',cv=10)\ngrid_tree.fit(X_train, Y_train)\n# .fit relation entre X et Y train \nbest_model_tree = grid_tree.best_estimator_\n# la fonction best estimator, permet de selectionner la meilleure profondeur possible","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_grid=best_model_tree.predict(X_test)\n# on prend le meilleur arbre de X_test pour le mettre dans Y_grid\n# On utilise cet arbre pour prédir si c'est un spam ou ham\n\n# Accuracy: pourcentage de bonne réponse de la machine par rapport à son apprentissage\naccuracy_score(Y_test, Y_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# afficher le meilleur paramètre de profondeur\ngrid_tree.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Réaliser une fôret d'arbre pour Y-train et X-train","metadata":{}},{"cell_type":"markdown","source":"Si on a un nombre important de variables explicatives (features). On utilise la Forêt d'arbres qui fonctionne comme le suivant:\n* on prend des sous ensembles de données et des sous ensembles de variables explicatives.\n* on applique l'Arbre de décision sur chaque sous ensemble.\n* la prédiction de la forêt aléatoire est alors un simple vote majoritaire des arbes construites.","metadata":{}},{"cell_type":"code","source":"# importer la bibliothèque pour la fôret\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Réalise une fôret d'arbre pour avoir un plus grand nombre de réponse\nRf_model = RandomForestClassifier()\nRf_model=Rf_model.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_predict=Rf_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prend la majorité des arbres \nmat = confusion_matrix(Y_predict, Y_test)\nsns.heatmap(mat, annot=True,  xticklabels=names, yticklabels=names)\nplt.xlabel('Test')\nplt.ylabel('Predicted')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prendre un texte et dire si il est Spam ou Ham","metadata":{}},{"cell_type":"markdown","source":"**Le type de données objet peut contenir plusieurs types. Par exemple, la colonne peut inclure des entiers, des réels et des chaînes de caractére qui sont collectivement étiquetés comme un objet### Chatbot**","metadata":{}},{"cell_type":"code","source":"text = 'Free entry in 2 a wkly comp to win FA Cup'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classer(text):\n\n    text=remove_punct(text)\n    text=remove_stopword(text)\n    text=canonic(text)\n    tfidf_text=tfidf_fit.transform([text])\n    \n    resultat = Rf_model.predict(tfidf_text)\n    \n    return resultat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = classer(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vérification du code","metadata":{}},{"cell_type":"code","source":"a=str(input())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while True:\n    text = str(input(\"Input: \"))\n    if text== \"exit\":\n        print(\"Response: Exiting.....\")\n        break\n    print(\"Response:\",classer(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}