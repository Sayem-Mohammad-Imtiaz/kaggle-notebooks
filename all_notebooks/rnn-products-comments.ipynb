{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nwords = ['LaoWang has a Wechat account.','He is not a nice person.','Be careful.']\ntokenizer = Tokenizer(num_words=30)\ntokenizer.fit_on_texts(words)\nsequences = tokenizer.texts_to_sequences(words)\none_hot_matrix = tokenizer.texts_to_matrix(words, mode='binary')\nword_index = tokenizer.word_index\nprint(len(word_index))\nprint(sequences)\nprint(one_hot_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:40:08.68765Z","iopub.execute_input":"2021-06-18T09:40:08.687975Z","iopub.status.idle":"2021-06-18T09:40:08.695646Z","shell.execute_reply.started":"2021-06-18T09:40:08.687944Z","shell.execute_reply":"2021-06-18T09:40:08.694778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd # 导入Pandas\nimport numpy as np # 导入NumPy\ndir = '../input/product-comments/'\ndir_train = dir+'Clothing Reviews.csv'\ndf_train = pd.read_csv(dir_train) # 读入训练集\ndf_train.head() # 输出部分数据","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:44:26.227962Z","iopub.execute_input":"2021-06-18T09:44:26.22833Z","iopub.status.idle":"2021-06-18T09:44:26.407898Z","shell.execute_reply.started":"2021-06-18T09:44:26.228288Z","shell.execute_reply":"2021-06-18T09:44:26.40703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer # 导入分词工具\nX_train_lst = df_train[\"Review Text\"].values # 将评论读入张量(训练集)\ny_train = df_train[\"Rating\"].values # 构建标签集\ndictionary_size = 20000 # 设定词典的大小\ntokenizer = Tokenizer(num_words=dictionary_size) # 初始化词典\ntokenizer.fit_on_texts( X_train_lst ) # 使用训练集创建词典索引\n# 为所有的单词分配索引值，完成分词工作\nX_train_tokenized_lst = tokenizer.texts_to_sequences(X_train_lst)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:45:43.642061Z","iopub.execute_input":"2021-06-18T09:45:43.642445Z","iopub.status.idle":"2021-06-18T09:45:45.497742Z","shell.execute_reply.started":"2021-06-18T09:45:43.642404Z","shell.execute_reply":"2021-06-18T09:45:45.496912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_tokenized_lst[0])\nprint(X_train_tokenized_lst[1])\nprint(X_train_tokenized_lst[2])\nprint(y_train)\nprint(len(y_train))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:50:05.731604Z","iopub.execute_input":"2021-06-18T09:50:05.731941Z","iopub.status.idle":"2021-06-18T09:50:05.738525Z","shell.execute_reply.started":"2021-06-18T09:50:05.73191Z","shell.execute_reply":"2021-06-18T09:50:05.73745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt # 导入matplotlib\nword_per_comment = [len(comment) for comment in X_train_tokenized_lst]\nplt.hist(word_per_comment, bins = np.arange(0,500,10)) # 显示评论长度分布\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:49:32.338265Z","iopub.execute_input":"2021-06-18T09:49:32.338824Z","iopub.status.idle":"2021-06-18T09:49:32.764579Z","shell.execute_reply.started":"2021-06-18T09:49:32.338776Z","shell.execute_reply":"2021-06-18T09:49:32.763587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences \nmax_comment_length = 100 # 设定评论输入长度为100，并填充默认值(如字数少于100)\nX_train = pad_sequences(X_train_tokenized_lst, maxlen=max_comment_length)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:50:33.824339Z","iopub.execute_input":"2021-06-18T09:50:33.824672Z","iopub.status.idle":"2021-06-18T09:50:34.177992Z","shell.execute_reply.started":"2021-06-18T09:50:33.824639Z","shell.execute_reply":"2021-06-18T09:50:34.177115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train[0])\nprint(X_train[1])\nprint(X_train[2])","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:51:12.04503Z","iopub.execute_input":"2021-06-18T09:51:12.045369Z","iopub.status.idle":"2021-06-18T09:51:12.052015Z","shell.execute_reply.started":"2021-06-18T09:51:12.045338Z","shell.execute_reply":"2021-06-18T09:51:12.050172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential # 导入贯序模型\nfrom keras.layers.embeddings import Embedding #导入词嵌入层\nfrom keras.layers import Dense #导入全连接层\nfrom keras.layers import SimpleRNN #导入SimpleRNN层\nembedding_vecor_length = 60 # 设定词嵌入向量长度为60\nrnn = Sequential() # 贯序模型\nrnn.add(Embedding(dictionary_size, embedding_vecor_length, \n          input_length=max_comment_length)) # 加入词嵌入层\nrnn.add(SimpleRNN(100)) # 加入SimpleRNN层\nrnn.add(Dense(10, activation='relu')) # 加入全连接层\nrnn.add(Dense(6, activation='softmax')) # 加入分类输出层\nrnn.compile(loss='sparse_categorical_crossentropy', #损失函数\n            optimizer='adam', # 优化器\n            metrics=['acc']) # 评估指标\nprint(rnn.summary()) #打印网络模型","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:53:12.06452Z","iopub.execute_input":"2021-06-18T09:53:12.064853Z","iopub.status.idle":"2021-06-18T09:53:14.571653Z","shell.execute_reply.started":"2021-06-18T09:53:12.064821Z","shell.execute_reply":"2021-06-18T09:53:14.570956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = rnn.fit(X_train, y_train, \n                    validation_split = 0.3, \n                    epochs=10, \n                    batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:53:55.612129Z","iopub.execute_input":"2021-06-18T09:53:55.612488Z","iopub.status.idle":"2021-06-18T09:57:17.796523Z","shell.execute_reply.started":"2021-06-18T09:53:55.612458Z","shell.execute_reply":"2021-06-18T09:57:17.795655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential # 导入贯序模型\nfrom keras.layers.embeddings import Embedding #导入词嵌入层\nfrom keras.layers import Dense #导入全连接层\nfrom keras.layers import LSTM #导入LSTM层\nembedding_vecor_length = 60 # 设定词嵌入向量长度为60\nlstm = Sequential() # 贯序模型\nlstm.add(Embedding(dictionary_size, embedding_vecor_length, \n          input_length=max_comment_length)) # 加入词嵌入层\nlstm.add(LSTM(100)) # 加入LSTM层\nlstm.add(Dense(10, activation='relu')) # 加入全连接层\nlstm.add(Dense(6, activation='softmax')) # 加入分类输出层\nlstm.compile(loss='sparse_categorical_crossentropy', #损失函数\n             optimizer = 'adam', # 优化器\n             metrics = ['acc']) # 评估指标\nhistory = rnn.fit(X_train, y_train, \n                    validation_split = 0.3,\n                    epochs=10, \n                    batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:58:24.593743Z","iopub.execute_input":"2021-06-18T09:58:24.594058Z","iopub.status.idle":"2021-06-18T10:01:44.871938Z","shell.execute_reply.started":"2021-06-18T09:58:24.594027Z","shell.execute_reply":"2021-06-18T10:01:44.871254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}