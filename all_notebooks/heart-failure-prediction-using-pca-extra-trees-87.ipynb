{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like a very ideal dataset<br>\nAll the values are numeric<br>\nThere are no null values to impute<br>\nThe platelets column needs scaling before applying ML algorithms, but lets find out if that feature actually matters<br>\nLets proceed to visualization<br>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(df.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusions from the heatmap :**\n* Death event looks to be highly correlated with serum_creatinine and age\n* Survival event looks to be highly correlated with time, ejection_fraction, serum_sodium\n* Sex and smoking have the least correlation with DEATH_EVENT, we can consider dropping these features","metadata":{}},{"cell_type":"code","source":"df.corr()['DEATH_EVENT'].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distplot of all the features\nTo understand how all the features are distributed","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20, 25))\nfeature_num = 1\nfor i in df:\n    if feature_num < 13:\n        ax = plt.subplot(4, 4, feature_num)\n        sns.distplot(df[i])\n        plt.xlabel(i, fontsize = 12)\n        \n    feature_num += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference from the distplot** :<br>\nThere is a notable skew in certain features like platelets, creatinine_phosphokinase <br>\nWe can overcome this using log transformation but will skip the same as these features have less impact on survival","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling the features using Standard Scaler<br>\nML algorithms are very sensitive differences in scales of various features<br>\nIn this particular case the platelets feature has a huge magnitude when compared to other features<br>\nThis can considerably offset the accuracy of our results<br>\nHence scaling is necessary<br>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ny = df['DEATH_EVENT']\ndf_scaled = ss.fit_transform(df.drop(('DEATH_EVENT'),axis=1))\ndf_scaled = pd.DataFrame(data=df_scaled,columns=df.columns[:-1])\ndf_scaled = pd.concat([df_scaled,y],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Boxplots to find outliers","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = df_scaled, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Treating Outliers with Principal Component Analysis<br>\nPrincipal component analysis can be used to find the features that explain the most of the variance(95-100%) in the dataset.<br>\nMost of the times we do not need all the features in the input dataset to explain the variance.<br>\n\nHence this can be used to :<br>\n1. Drop unwanted features <br>\n2. Drop unwanted rows that act as outliers<br>\n\nThe inbuilt sklearn pca does not give us the top features(best features to use), the location of outliers in the dataset<br>\nThere is this cool library called pca that does both and hence we will be using the same.","metadata":{}},{"cell_type":"code","source":"pip install pca","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pca import pca\nmodel = pca(n_feat=12,n_components=12) #considering all the 12 features as I do not want to eliminate columns\ndf_scaled_x = model.fit_transform(df_scaled.drop(('DEATH_EVENT'),axis=1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled_x['topfeat']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers = df_scaled_x['outliers']\noutliers_ = outliers[outliers['y_bool_spe']==True]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers_.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers_index = outliers_['index']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers_index #gives the index of the outliers in the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us plot the pca model<br>\nAll the points outside the green zone gives us the outliers","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nmodel.biplot(legend=True, SPE=True, hotellingt2=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us drop the outliers","metadata":{}},{"cell_type":"code","source":"df_new = df_scaled.drop(outliers_index,axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us see if there are some more outliers in the dataset after PCA","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = df_new, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there are some more outliers and we have to clean them up","metadata":{}},{"cell_type":"code","source":"df_exp = df_scaled[df_scaled['platelets'] < df_scaled['platelets'].quantile(0.95)]\ndf_exp = df_exp[df_exp['platelets'] > df_exp['platelets'].quantile(.1)]\ndf_exp = df_exp[df_exp['serum_sodium'] > df_exp['serum_sodium'].quantile(.1)]\ndf_exp = df_exp[df_exp['serum_creatinine'] < df_exp['serum_creatinine'].quantile(0.9)]\ndf_exp = df_exp[df_exp['creatinine_phosphokinase'] < df_exp['creatinine_phosphokinase'].quantile(0.91)]\nfig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = df_exp, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have cleaned up most of the outliers, do not want to drop more samples as it may reduce the amount of data input<br>\nto ML algorithms for classification","metadata":{}},{"cell_type":"code","source":"df_exp.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exp['DEATH_EVENT'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test train split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df_exp.drop(('DEATH_EVENT'),axis=1), df_exp['DEATH_EVENT'], test_size=0.3, random_state=42,stratify=df_exp['DEATH_EVENT'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output labels are imbalanced and we have overcome this via oversampling in order to avoid model bias towards majority class","metadata":{}},{"cell_type":"markdown","source":"# Oversampling with SMOTE","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy={0:150,1:150})\nX,y = sm.fit_resample(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Choosing the best model based on F1-score","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, balanced_accuracy_score\nfrom sklearn.model_selection import KFold\nmodels=[(\"XGboost\", XGBClassifier()),\n        (\"Stochastic Gradient Descent\", SGDClassifier()),\n        (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n        (\"Decision Tree\", DecisionTreeClassifier()),\n        (\"Random Forest\", RandomForestClassifier()),\n        (\"Extra Trees\", ExtraTreesClassifier()),\n        (\"Gradient Boosting\", GradientBoostingClassifier()),\n        (\"KNeighbors\", KNeighborsClassifier()),\n        (\"SVM\", SVC()),\n        (\"Naive Bayes\", GaussianNB()),\n        (\"Cat Boost\", CatBoostClassifier(verbose=False)),\n        (\"Ada Boost\", AdaBoostClassifier())]\n\nf1 = []\nvariance = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10)\n    results = cross_val_score(model, X, y, cv=kfold, scoring='f1')\n    f1.append(results.mean())\n    variance.append(results.std())\n    names.append(name)\n    print('Model name : {}, F1 score : {},  variance : {}'.format(name,results.mean(),results.std()))\n\nf1 = pd.Series(data=f1,name='f1-score')\nvariance = pd.Series(data=variance,name='variance')\nnames = pd.Series(data=names,name='names')\ndf_f1 = pd.concat([f1,variance],axis=1)\n\ndf_f1.set_index(keys=names,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_f1.sort_values('f1-score',ascending=False,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.barplot(y=df_f1.index,x=df_f1['f1-score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use **Cat boost , Extra trees, Random forests** as they seem to have the best F1-scores","metadata":{}},{"cell_type":"markdown","source":"# Feature Selection\nLet us use extra trees to do some feature selection and eliminate unwanted features to boost model accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\next = ExtraTreesClassifier(criterion='entropy', max_depth=8)\next.fit(X,y)\npred_ext = ext.predict(X_test)\nprint(confusion_matrix(pred_ext,y_test))\nprint(classification_report(pred_ext,y_test))\nsns.barplot(x=ext.feature_importances_,y=X_test.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most important features are **age, ejection_fraction, serum_creatinine,time**<br>\nWe will be dropping all the other features","metadata":{}},{"cell_type":"code","source":"X.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.drop(['anaemia', 'creatinine_phosphokinase', 'diabetes',\n        'high_blood_pressure', 'platelets','serum_sodium', 'sex', 'smoking'],inplace=True,axis=1)\nX_test.drop(['anaemia', 'creatinine_phosphokinase', 'diabetes',\n        'high_blood_pressure', 'platelets','serum_sodium', 'sex', 'smoking'],inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape,X_test.shape,y.shape,y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper parameter tuning for Extra trees classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams_ext = [{'criterion' : [\"gini\", \"entropy\"],\n              'min_samples_split': [2,4,6,8], \n              'max_depth': [2,4,6,8],\n              'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n              'n_estimators': [100,200,400,600],\n          }]\nclassifier_ = ExtraTreesClassifier()\ngrid_search_ext = GridSearchCV(classifier_,params_ext,cv=3,n_jobs=150,scoring='f1',verbose=10)\ngrid_search_ext.fit(X,y)\nprint(grid_search_ext.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search_ext.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction using Extra trees","metadata":{}},{"cell_type":"code","source":"ext = ExtraTreesClassifier(max_depth=8, max_features='sqrt', min_samples_split=4)\next.fit(X,y)\npred_ext = ext.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(pred_ext,y_test))\nprint(classification_report(pred_ext,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper parameter tuning for random forests","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nparams_rfc = [{'criterion' : [\"gini\", \"entropy\"],\n              'min_samples_split': [2,4,6,8], \n              'max_depth': [2,4,6,8],\n              'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n              'n_estimators': [100,200,400,600],\n          }]\nclassifier_ = RandomForestClassifier()\ngrid_search_rfc = GridSearchCV(classifier_,params_rfc,cv=3,n_jobs=150,scoring='f1',verbose=10)\ngrid_search_rfc.fit(X,y)\nprint(grid_search_rfc.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search_rfc.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(criterion='entropy', max_depth=8)\nrfc.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction using random forests","metadata":{}},{"cell_type":"code","source":"pred_rfc = rfc.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(pred_rfc,y_test))\nprint(classification_report(pred_rfc,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Cat Boost Classifier","metadata":{}},{"cell_type":"code","source":"cat = CatBoostClassifier(verbose=False)\ncat.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction using cat boost","metadata":{}},{"cell_type":"code","source":"pred_cat = cat.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(pred_cat,y_test))\nprint(classification_report(pred_cat,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"So the best model to use would be **Extra trees and cat boost** , because they both have an **F1-score of 0.82** <br>\nAnd also an **accuracy of 0.87**","metadata":{}},{"cell_type":"markdown","source":"**Please upvote if this notebook was helpful and if you liked it<br>\nComments for improvements are welcome**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}