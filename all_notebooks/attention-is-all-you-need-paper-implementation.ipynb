{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction:\n\nHello, everyone! In this kernel I'll be implementing the code from \"The Annotated Transformer\" post in http://nlp.seas.harvard.edu/2018/04/03/attention.html","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math, copy, time\nfrom torch.autograd import Variable\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set_context(context=\"talk\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Base encoder architecture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n        \n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n    \n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)    \n    \n    def forward(self, src, tgt, src_mask, tgt_mask):\n        'take in and process masked src and target sequences'\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    'define standard linear + softmax generation step'\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n        \n    def forward(self, x):\n        return F.log_softmax(self.proj(x), dim=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking things up","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clones(module, N):\n    'produce N identical layers'\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer,N)\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n        \n    def forward(self, x):\n        #print(x)\n        #print(type(x))\n        #print(x.shape)\n        #mean = np.mean(x)\n        #std = np.std(x)\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SublayerConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n        \n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n        \n    def forward(self, x, memory, src_mask, tgt_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def subsequent_mask(size):\n    attn_shape = (1, size, size)\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n    return torch.from_numpy(subsequent_mask) == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(subsequent_mask(20)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n        \n        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n        \n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n        \n        return self.linears[-1](x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model,d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n        \n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(1000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n        return self.dropout(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\npe = PositionalEncoding(20, 0)\ny = pe.forward(Variable(torch.zeros(1, 100, 20)))\nplt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\nplt.legend(['dim %d'%p for p in [4, 5, 6, 7]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        \n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        Generator(d_model, tgt_vocab)\n    )\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform(p)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Batch:\n    def __init__(self, src, trg=None, pad=0):\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if trg is not None:\n            self.trg = trg[:, :-1]\n            self.trg_y = trg[:, 1:]\n            self.trg_mask = self.make_std_mask(self.trg, pad)\n            self.ntokens = (self.trg_y != pad).data.sum()\n            \n    @staticmethod\n    def make_std_mask(tgt, pad):\n        #print(tgt.shape, pd.shape)\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        #print(tgt_mask.shape)\n        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n        return tgt_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_epoch(data_iter, model, loss_compute):\n    start = time.time()\n    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    for i, batch in enumerate(data_iter):\n        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n        total_loss += loss\n        total_tokens += batch.ntokens\n        tokens += batch.ntokens\n        if i % 50 == 1:\n            elapsed = time.time() - start\n            print(\"Epoch step: \", i, \" Loss: \", loss/batch.ntokens, \" Tokens per sec: \", tokens/elapsed)\n            start = time/time()\n            tokens = 0\n    return total_loss / total_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global max_src_in_batch, max_tgt_in_batch\ndef batch_size_fn(new, count, sofar):\n    global max_src_in_batch, max_tgt_in_batch\n    if count == 1:\n        max_src_in_batch = 0\n        max_tgt_in_batch = 0\n    max_src_in_batch = max(max_src_in_batch, len(new.src))\n    max_tgt_in_batch = max(max_tgt_in_batch, len(new.trg) + 2)\n    src_elements = count * max_src_in_batch\n    tgt_elements = count * max_tgt_in_batch\n    return max(src_elements, tgt_elements)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class NoamOpt:\n    def __init__(self, model_size, factor, warmup, optimizer):\n        self.otptimizer = optimizer\n        self._step = 0\n        self.warmup = warmup\n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n        \n    def step(self):\n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n        self.optimizer.step()\n        \n    def rate(self, step=None):\n        if step is None:\n            step = self._step\n        return self.factor * (self.model_size **(-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n    \ndef get_std_opt(model):\n    return NoamOpt(model.src_embed[0].d_model, 2, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opts = [NoamOpt(512, 1, 4000, None),\n      NoamOpt(512, 1, 8000, None),\n      NoamOpt(256, 1, 4000, None)]\n\nplt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\nplt.legend(['512:4000', \"512:8000\", \"256:4000\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regularization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabelSmoothing(nn.Module):\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(size_average=False)\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        \n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing/ (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n            self.true_dist = true_dist\n            return self.criterion(x, Variable(true_dist, requires_grad=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crit = LabelSmoothing(5, 0, 0.4)\npredict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                             [0, 0.2, 0.7, 0.1, 0],\n                             [0, 0.2, 0.7, 0.1, 0]])\nv = crit(Variable(predict.log()),\n        Variable(torch.LongTensor([2, 1, 0])))\nplt.imshow(crit.true_dist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"crit = LabelSmoothing(5, 0, 0.1)\n\ndef loss(x):\n    d = x + 3 * 1\n    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d], ])\n    return crit(Variable(predict.log()), Variable(torch.tensor([1], dtype=torch.long))).data[0]\n\nplt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchtext spacy\n!python -m spacy download en\n!python -m spacy download fr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loading","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/language-translation-englishfrench/eng_-french.csv')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext import data, datasets\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy_en = spacy.load('en')\nspacy_fr = spacy.load('fr')\n\nprint(spacy_fr)\nprint('\\n')\nprint(spacy_en)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_en(text):\n    return [tok.text for tok in spacy_en.tokenizer(text)]\n\ndef tokenize_fr(text):\n    return [tok.text for tok in spacy_fr.tokenizer(text)]\n\nBOS_WORD = '<s>'\nEOS_WORD = '</s>'\nBLANK_WORD = '<blank>'\n\nSRC = data.Field(tokenize=tokenize_en, pad_token=BLANK_WORD)\nTGT = data.Field(tokenize=tokenize_fr, init_token=BOS_WORD, eos_token=EOS_WORD, pad_token=BLANK_WORD)\n\nMAX_LEN = 100\n#dataset = datasets.IWSLT.splits(exts=('.fr', '.en'), fields=(SRC, TGT), filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\ntrain, val, test = datasets.IWSLT.splits(exts=('.fr', '.en'), fields=(SRC, TGT), filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\nMIN_FREQ = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SRC.build_vocab(train.src, min_freq=MIN_FREQ)\nTGT.build_vocab(train.trg, min_freq=MIN_FREQ)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyIterator(data.Iterator):\n    def create_batches(self):\n        if self.train:\n            def pool(d, random_shuffler):\n                for p in data.batch(d, self.batch_size * 100):\n                    p_batch = data.batch(sorted(p, key=self.sort_key), self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        yield b\n            self.batches = pool(self.data(), self.random_shuffler)\n            \n        else:\n            self.batches = []\n            for b in data.batch(self.data(), self.batch_size, self.batch_size_fn):\n                self.batches.append(sorted(b, key=self.sort_key))\n                \ndef rebatch(pad_idx, batch):\n    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n    return Batch(src, trg, pad_idx)\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multi GPU Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiGPULossCompute:\n    def __init__(self, generator, criterion, devices, opt=None, chunk_size = 5):\n        self.generator = generator\n        self.criterion = nn.parallel.replicate(criterion, devices=devices)\n        self.optimizer = opt\n        self.devices = devices\n        self.chunk_size = chunk_size\n        \n    def __call__(self, out, targets, normalize):\n        total = 0.0\n        generator = nn.parallel.replicate(self.generator, devices=self.devices)\n        out_scatter = nn.parallel.scatter(targets, target_gpus=self.devices)\n        out_grad = [[] for _ in out_scatter]\n        targets = nn.parallel.scatter(targets, target_gpus=self.devices)\n        \n        chunk_size = self.chunk_size\n        for i in range(0, out_scatter[0].size(1), chunk_size):\n            out_column = [[Variable(o[:, i:i+chunk_size].data, requires_grad=self.optimizer is not None)] for o in out_scatter]\n            gen = nn.parallel.parallel_apply(generator, out_column)\n            \n            y = [(g.contiguous().view(-1, g.size(-1)), t[:, i:i+chunk_size].contiguous().view(-1)) for g, t in zip(gen, targets)]\n            loss = nn.parallel.parallel_apply(generator, out_column)\n            \n            l = nn.parallel.gather(loss, target_device=self.devices[0])\n            l = l.sum()[0] / normalize\n            total += l.data[0]\n            \n            if self.optimizer is not None:\n                l.backward()\n                for j, l in enumerate(loss):\n                    out_grad[j].append(out_column[j][0].grad.data.clone())\n                    \n        if self.optimizer is not None:\n            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n            o1 = out\n            o2 = nn.parallel.gather(out_grad, target_device=self.devices[0])\n            \n            o1.backward(gradient=o2)\n            self.optimizer.step()\n            self.optimizer.optimizer.zero_grad()\n        return total * normalize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"devices = [0]\nif True:\n    pad_idx = TGT.vocab.stoi[\"<blank>\"]\n    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n    model.cuda()\n    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n    criterion.cuda()\n    BATCH_SIZE = 100\n    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0, repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)), batch_size_fn=batch_size_fn, train=True)\n    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0, repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)), batch_size_fn=batch_size_fn, train=False)\n    model_par = nn.DataParallel(model, device_ids=devices)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the system","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if True:\n    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n    \n    for epoch in range(10):\n        model_par.train()\n        run_epoch((rebatch(pad_idx, b) for b in train_iter), model_par, MultiGPULossCompute(model.generator, criterion, devices=devices, opt=model_opt))\n        \n        model_par.eval()\n        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), model_par, MultiGPULossCompute(model.generator, criterion, devices=devices, opt=None))\n        print(loss)\nelse:\n    model = torch.load('iwslt.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, batch in enumerate(valid_iter):\n    src = batch.src.transpose(0, 1)[:1]\n    src_mask = (src != SRC.vocab.stoi['<blank>']).unsqueeze(-2)\n    out = greedy_decode(model, src, src_mask, max_len=60, start_symbol=TGT.vocab.stoi['<s>'])\n    \n    print('Translation: ', end='\\t')\n    for i in range(1, out.size(1)):\n        sym = TGT.vocab.itos[out[0, i]]\n        if sym == '</s>': break\n        print(sym, end = \" \")\n        \n    print()\n    print('Target: ', end=\"\\t\")\n    for i in range(1, batch.trg.size(0)):\n        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n        if sym == \"</s>\": break\n        print(sym, end=\" \")\n    print('\\n')\n    print('Target:\\n')\n    for i in range(1, batch.tgr.size(0)):\n        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n        if sym == '</s>': break\n        print(sym, end = \" \")\n        print('\\n')\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://s3.amazonaws.com/opennmt-models/en-de-model.pt\nmodel, SRC, TGT = torch.load(\"en-de-model.pt\")\nmodel.eval()\nsent = \"▁The ▁log ▁file ▁can ▁be ▁sent ▁secret ly ▁with ▁email ▁or ▁FTP ▁to ▁a ▁specified ▁receiver\".split()\nsrc = torch.LongTensor([[SRC.stoi[w] for w in sent]])\nsrc = Variable(src)\nsrc_mask = (src != SRC.stoi[\"<blank>\"]).unsqueeze(-2)\nout = greedy_decode(model, src, src_mask, \n                    max_len=60, start_symbol=TGT.stoi[\"<s>\"])\nprint(\"Translation:\", end=\"\\t\")\ntrans = \"<s> \"\nfor i in range(1, out.size(1)):\n    sym = TGT.itos[out[0, i]]\n    if sym == \"</s>\": break\n    trans += sym + \" \"\nprint(trans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}