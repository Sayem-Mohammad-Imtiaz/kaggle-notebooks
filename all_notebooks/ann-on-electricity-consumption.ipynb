{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"f7a4189f-ef08-461a-a868-c28f5253c8e3","_uuid":"f5de3e8c5fdf8ce1469d437b500cb1f259ce2d38"}},{"source":"**My Functions**","cell_type":"markdown","metadata":{}},{"source":"def datetounix(df):\n    # Initialising unixtime list\n    unixtime = []\n    \n    # Running a loop to convert Date to seconds\n    for date in df['datetime']:\n        unixtime.append(time.mktime(date.timetuple()))\n    \n    # Replacing Date with unixtime list\n    df['datetime'] = unixtime\n    return(df)","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"source":"**Import Libraries**","cell_type":"markdown","metadata":{}},{"source":"# import libs\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport time\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport operator\nfrom sklearn.preprocessing import StandardScaler\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\n# read train dataframe\n# file_path = os.path.join(os.path.abspath(''), 'train.csv')\ndf_train = pd.read_csv(\"../input/train.csv\", encoding='ISO-8859-1', engine='c')\n\n# read test dataframe\n# file_path = os.path.join(os.path.abspath(''), 'test.csv')\ndf_test = pd.read_csv(\"../input/test.csv\", encoding='ISO-8859-1', engine='c')\ndf_train.info()","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Data Cleaning**\n\nUploading and cleaning of data.","cell_type":"markdown","metadata":{}},{"source":"# Converting to datetime\ndf_train['datetime'] = pd.to_datetime(df_train['datetime'])\ndf_test['datetime'] = pd.to_datetime(df_test['datetime'])\ndf_test.info()","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"# Creating features from DateTime for train data\n\ndf_test['Weekday'] = [datetime.weekday(date) for date in df_test.datetime]\ndf_test['Year'] = [date.year for date in df_test.datetime]\ndf_test['Month'] = [date.month for date in df_test.datetime]\ndf_test['Day'] = [date.day for date in df_test.datetime]\ndf_test['Time'] = [((date.hour*60+(date.minute))*60)+date.second for date in df_test.datetime]\ndf_test['Week'] = [date.week for date in df_test.datetime]\ndf_test['Quarter'] = [date.quarter for date in df_test.datetime]\n\n# Creating features from DateTime for test data\n\ndf_train['Weekday'] = [datetime.weekday(date) for date in df_train.datetime]\ndf_train['Year'] = [date.year for date in df_train.datetime]\ndf_train['Month'] = [date.month for date in df_train.datetime]\ndf_train['Day'] = [date.day for date in df_train.datetime]\ndf_train['Time'] = [((date.hour*60+(date.minute))*60)+date.second for date in df_train.datetime]\ndf_train['Week'] = [date.week for date in df_train.datetime]\ndf_train['Quarter'] = [date.quarter for date in df_train.datetime]","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"source":"**Data Preparation**","cell_type":"markdown","metadata":{}},{"source":"# Create Dummy Variables for Train set\ndf_train.loc[df_train.var2 == 'A', 'var2A'] = 1\ndf_train.loc[df_train.var2 == 'B', 'var2B'] = 1\n\ndf_train['var2A'].fillna(0, inplace=True)\ndf_train['var2B'].fillna(0, inplace=True)\n\ndf_train.drop(['var2'], axis=1, inplace=True)\n\n# Create Dummy Variables for Test set\ndf_test.loc[df_test.var2 == 'A', 'var2A'] = 1\ndf_test.loc[df_test.var2 == 'B', 'var2B'] = 1\n\ndf_test['var2A'].fillna(0, inplace=True)\ndf_test['var2B'].fillna(0, inplace=True)\n\ndf_test.drop(['var2'], axis=1, inplace=True)\n\n# Creating X_test\nX_test = datetounix(df_test).drop(['ID'], axis=1).values\n\n# Remove target column from the df\ndf_train_features = df_train.drop(['electricity_consumption', 'ID'], axis=1)\n\n# Convet timestamp to seconds\ndf_train_features = datetounix(df_train_features)\n\n# store features in X array\nX = df_train_features.values\ny = df_train['electricity_consumption'].values","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"source":"**Visualisation of features**","cell_type":"markdown","metadata":{}},{"source":"# create an instance for tree feature selection\ntree_clf = ExtraTreesClassifier()\n\n# fit the model\ntree_clf.fit(X, y)\n\n# Preparing variables\nimportances = tree_clf.feature_importances_\nfeature_names = df_train_features.columns.tolist()\n\nfeature_imp_dict = dict(zip(feature_names, importances))\nsorted_features = sorted(feature_imp_dict.items(), key=operator.itemgetter(1), reverse=True)\n\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"feature %d : %s (%f)\" % (indices[f], sorted_features[f][0], sorted_features[f][1]))\n\n# Plot the feature importances of the forest\nplt.figure(0)\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"############ Data Scaling ###################\nsc = StandardScaler()\nX = sc.fit_transform(X)\nX_test = sc.transform(X_test)\n\ny_norm = (y - min(y))/(max(y) - min(y))\ny_norm","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Implementing ANN**","cell_type":"markdown","metadata":{}},{"source":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu', input_dim = 14))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['mae'])\n\n# Fitting the ANN to the training set\nclassifier.fit(X, y_norm, batch_size = 10, epochs = 100)\n\n# Part 3 - Making the predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred * (max(y) - min(y))) + min(y)\n\npredictions = [int(elem) for elem in y_pred]\n\ndf_solution = pd.DataFrame()\ndf_solution['ID'] = df_test.ID\n\n# Prepare Solution dataframe\ndf_solution['electricity_consumption'] = predictions\ndf_solution['electricity_consumption'].unique()\n\ndf_solution","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}}],"nbformat_minor":1,"nbformat":4}