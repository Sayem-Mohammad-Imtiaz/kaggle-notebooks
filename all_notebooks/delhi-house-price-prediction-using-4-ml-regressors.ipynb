{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis notebook makes use of the Delhi house price dataset and aims to apply and subsequently evaluate different Machine Learning (ML) methods based upon their ability to perform a regression capable of predicting house prices in Delhi by considering the values of other variables within the dataset. \n\nSpecifically, a (i) Support Vector Regressor, (ii) Random Forest Regressor, (iii) K-Nearest Neighbour Regressor and (iv) Deep Learning Multilayer Perceptron (MLP) Regressor are applied. Of the methods, the Random Forest Regressor performed the best over 5 re-runs, followed very closely by the Deep Learning MLP Regressor, and then the Support Vector Regressor and K-Nearest Neighbour Regressor.\n\nThis notebook is split up into five different stages as follows:\n\n**1) Stage 1: Data Description & Data Exploration**\n\n**2) Stage 2: Data Preprocessing**\n\n**3) Stage 3: Splitting Training/Testing subsets**\n\n**4) Stage 4: Model Fitting and Evaluation**\n\n**5) Stage 5: Model Comparison**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stage 1: Data Description/Exploration\n\nThe first stage involves describing and exploring the data provided. Section 1.1 details the description of data, whilst 1.2 covers the data exploration"},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Data Description\n\nThe data provided is described below, with the use of the Pandas '.head()' method, which prints the first few rows of the dataframe, the '.describe()' method which provides a statistical description of a dataframe, the '.dtypes' method which outlines the formats within which respective columns within a dataframe are stored, and finally the '.shape' method which details the dimensions of the dataframe."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load in the delhi house price dataframe\ndf = pd.read_csv('../input/delhi-house-price-prediction/MagicBricks.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect the first few rows of the dataframe\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the dataset\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain datatypes\nprint(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outline the shape of the dataset\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Data Exploration\n\nThis section provides a brief exploration of the data provided in the form of a Pearson's correlation matrix which examines the relationships between all variables where such a linear correlation can be applied appropriately, and an examination of the distributions of variables.\n\nThe computed correlation matrix below indicates that of the variables used, 'Area', 'BHK' (Bedroom, Hall, Kitchen), 'Bathroom' and 'Per_Sqft' exhibit positive correlations with the house price variable that is to be predicted. However, the 'Parking' variable exihibits a negative correlation with house prices which is perhaps unexpected, as more parking spaces would perhaps be expected to increase the value of a house."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import seaborn and matplotlib.pyplot for data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# produce a correlation matrix between variables where applicable\ncorr = df.corr()\n\n# generate a custom diverging colormap\ncmap = sns.diverging_palette(150, 275, as_cmap=True)\n\n# draw the heatmap with the mask and correct aspect ratio\nsns.set(rc={'figure.figsize': (17.0, 8.0)}, font_scale=1.2)\nsns.heatmap(corr, cmap=cmap, vmax=1.0,vmin=min(corr.min()), center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.title(\"A correlation matrix to show the Pearson's \\ncorrelation between variables\", fontsize=20)\nplt.yticks(rotation=0)\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions of variables are also examined below with histograms, with 'Area' and 'Per_Sqft' and particularly the 'Parking' and 'Price' variables displaying exponential distributions, whilst variables 'BHK' and 'Bathroom' display comparatively more normal distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf, ax = plt.subplots(3, 2, constrained_layout=True)\nax[0,0].hist(df['Area'], color='red', bins=50, edgecolor='black', linewidth=1.2)\nax[0,1].hist(df['BHK'], color='blue', bins=10, edgecolor='black', linewidth=1.2)\nax[1,0].hist(df['Bathroom'], color='green', bins=10, edgecolor='black', linewidth=1.2)\nax[1,1].hist(df['Parking'], color='purple', bins=50, edgecolor='black', linewidth=1.2)\nax[2,0].hist(df['Price'], color='yellow', bins=50, edgecolor='black', linewidth=1.2)\nax[2,1].hist(df['Per_Sqft'], color='pink', bins=50, edgecolor='black', linewidth=1.2)\n\n# set all y-labels\nplt.setp(ax[:, :], ylabel='Frequency')\n\n# individually set subplot titles and x axis labels\nplt.setp(ax[0, 0], xlabel='Area')\nax[0, 0].set_title('Area value frequency histogram', fontsize=20)\n\nplt.setp(ax[0, 1], xlabel='BHK')\nax[0, 1].set_title('BHK value frequency histogram', fontsize=20)\n\n\nplt.setp(ax[1, 0], xlabel='Bathroom')\nax[1, 0].set_title('Bathroom value frequency histogram', fontsize=20)\n\n\nplt.setp(ax[1, 1], xlabel='Parking')\nax[1, 1].set_title('Parking value frequency histogram', fontsize=20)\n\n\nplt.setp(ax[2, 0], xlabel='Price')\nax[2, 0].set_title('Price value frequency histogram', fontsize=20)\n\n\nplt.setp(ax[2, 1], xlabel='Per_Sqft')\nax[2, 1].set_title('Per_Sqft value frequency histogram', fontsize=20)\n\nf.tight_layout(pad=1.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stage 2: Data Preprocessing\n\nWith the data introduced and briefly explored, Stage 2 involves preprocessing the data so that it is in a format that can be interpreted by a machine. This involves first cleaning the data (section 2.1), converting the datatypes (section 2.2) and finally scaling the data (section 2.3)."},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Data cleaning\n\nThe data is cleaned by removing the rows which contains NA values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove rows if they contain NA values\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Datatype Conversions\n\nThe 'BHK' and 'Price' variables are converted to floats, as such a format enables it to be understood by a machine."},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert integers to floats\ndf['BHK'] = df['BHK'].astype(float)\ndf['Price'] = df['Price'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns with categorical variables are also One Hot Encoded, which takes a column with n-number of categorical values and 'splits' this into n-number of new columns which correspond to each unique categorical value, with each row within these new columns containing either a 0 or 1, depending on whether each row contains a specific column value. This enables these categorical values to be appropriately understood by a machine.\n\nThe One Hot Encoding is done using the Pandas '.get_dummies()' method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get dummies for one hot encoding\nfurn_dummies = pd.get_dummies(df['Furnishing'], dtype=float)\nloca_dummies = pd.get_dummies(df['Locality'], dtype=float)\nstat_dummies = pd.get_dummies(df['Status'], dtype=float)\ntran_dummies = pd.get_dummies(df['Transaction'], dtype=float)\ntype_dummies = pd.get_dummies(df['Type'], dtype=float)\n\n# remove old columns\ndf = df.drop(['Furnishing', 'Locality', 'Status', 'Transaction', 'Type'], axis=1)\n\n# concat the one hot encoded dataframes onto the main dataframe\ndf = pd.concat([df, furn_dummies, loca_dummies, stat_dummies, tran_dummies, type_dummies], axis=1)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Data Scaling\n\nIn order to accelerate the calculations made using the various Regressors which are to be applied, the data is scaled using a Min-Max scaler which will scale all values to between 1 and 2. However, prior to scaling between values of 1 and 2 with the Min-Max scaler, the variables 'Area', 'Parking', 'Per_Sqft' and 'Price' are scaled logarithmically since the exploratory analysis indicated that these variables exhibited an exponential distribution. The 'BHK' variable did not have the logarithmic scaling applied, and all remaining variables (One Hot Encoded variables etc.) were not scaled as it was not deemed necessary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the min-max scaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# define our scaler\nscaler = MinMaxScaler(feature_range=(1, 2))\n\n# no reverse transformation required with these columns, so we can use a fit_transform()\ndf['Area'] = scaler.fit_transform(np.expand_dims(np.log(df['Area']), axis=1))\ndf['BHK'] = scaler.fit_transform(np.expand_dims(df['BHK'], axis=1))\ndf['Parking'] = scaler.fit_transform(np.expand_dims(np.log(df['Parking']), axis=1))\ndf['Per_Sqft'] = scaler.fit_transform(np.expand_dims(np.log(df['Per_Sqft']), axis=1))\n\n# we will ned to reverse transform the price values for evaluating the models,\n# so we will separately define a scaler and store it as a variable for this variable\nprice_scaler = scaler.fit(np.expand_dims(np.log(df['Price']), axis=1))\ndf['Price'] = price_scaler.transform(np.expand_dims(np.log(df['Price']), axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stage 3: Splitting Training/Testing sets\n\nThe data was divided into testing and training sets with an 80/20 split."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the train_test_split function\nfrom sklearn.model_selection import train_test_split\n\nX_values = df.drop(['Price'], axis=1)\ny_values = df['Price']\n\n# now split our x and y values into train/test sets with an 80/20 percentage split\nX_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size=0.2)\nprint(\"X_train shape is\", X_train.shape)\nprint(\"X_test shape is\", X_test.shape)\nprint(\"y_train shape is\", y_train.shape)\nprint(\"y_test shape is\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stage 4: Model Fitting and Evaluation\n\nStage 4 involved applying four Machine Learning (ML) regressors, which specifically were the Support Vector Regressor (section 4.1), Random Forest Regressor (section 4.2), K-Nearest Neighbour Regressor (4.3) and a custom Deep Learning Multilayer Perceptron (MLP) Regressor (section 4.4). With each Regressor, the model was fit using the training data and its performance briefly evaluated using an r-squared and mean squared error evaluation metric."},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Support Vector Regressor\n\nThe Support Vector Regressor makes use of Support Vector Machines.\n\nThe Support Vector Regressor is defined and fit as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import sklearn's Support Vector Regression model\nfrom sklearn.svm import SVR\n\n# define the model\nsv_regressor = SVR(kernel='linear')  # linear kernel achieved best results\n\n# fit the model\nsv_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Support Vector Regressor is applied on unseen testing X data and evaluated as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the r2 and mse evaluation metrics\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# make predictions with unseen testing data\nsv_preds = sv_regressor.predict(X_test)\n\n# calculate r-squared on inverse transformed data\nsv_r2 = r2_score(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(np.expand_dims(sv_preds, axis=1))))\n\n# calculate mse on inverse transformed data\nsv_mse = mean_squared_error(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(np.expand_dims(sv_preds, axis=1))))\n\nprint('R-Squared: ', sv_r2)\nprint('Mean Squared Error: ', np.format_float_scientific(np.asarray(sv_mse), precision=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R-squared and Mean Squared Error observed between the Support Vector Regressor testing data outputs and ground truth values were within ranges of 0.710-0.816 (R-squared) and 1.194x10$^{14}$-3.050x10$^{14}$ (Mean Squared Error) at means of 0.764 and 1.839x10$^{14}$ respectively (5 re-runs in total). NOTE: these were calculated from 5 re-runs, and later re-runs may record values outside of these ranges due to the random nature of the train/test splitting method."},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Random Forest Regressor\n\nThe Random Forest Regressor fits a number of decision trees on various sub-samples of the dataset. Averaging is applied to improve the predictive accuracy and control over-fitting.\n\nThe Random Forest Regressor is defined and fit as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load in the Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# define the model\nrf_regressor = RandomForestRegressor(n_estimators=120)  # 120 estimators optimised performance\n\n# fit the model\nrf_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Random Forest Regressor is applied on unseen testing X data and evaluated as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions with unseen testing data\nrf_preds = rf_regressor.predict(X_test)\n\n# calculate r-squared on inverse transformed data\nrf_r2 = r2_score(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(np.expand_dims(rf_preds, axis=1))))\n\n# calculate mse on inverse transformed data\nrf_mse = mean_squared_error(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(np.expand_dims(rf_preds, axis=1))))\n\nprint('R-Squared: ', rf_r2)\nprint('Mean Squared Error: ', np.format_float_scientific(np.asarray(rf_mse), precision=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R-squared and Mean Squared Error observed between the Random Forest Regressor testing data outputs and ground truth values were within ranges of 0.827-0.927 (R-squared) and 4.665x10$^{13}$-1.843x10$^{14}$ (Mean Squared Error) at means of 0.876 and 9.919x10$^{13}$ respectively (5 re-runs in total). NOTE: these were calculated from 5 re-runs, and later re-runs may record values outside of these ranges due to the random nature of the train/test splitting method.\n\nThe higher mean R-squared and lower mean MSE indicates that this Regressor is more accurate than the Support Vector counterpart."},{"metadata":{},"cell_type":"markdown","source":"### 4.3 K-Nearest Neighbour Regressor\n\nThe K-Nearest Neighbour Regressor performs a regression using K-Nearest Neighbours.\n\nThe K-Nearest Neighbour Regressor is defined and fit as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load in the Random Forest Regressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# define the model\nkn_regressor = KNeighborsRegressor(n_neighbors=4, algorithm='auto')  # 4 neighbours and 'auto' algorithm optimum\n\n# fit the model\nkn_regressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The K-Nearest Neighbour Regressor is applied on unseen testing X data and evaluated as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions with unseen testing data\nkn_preds = kn_regressor.predict(X_test)\n\n# calculate r-squared on inverse transformed data\nkn_r2 = r2_score(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(np.expand_dims(kn_preds, axis=1))))\n\n# calculate mse on inverse transformed data\nkn_mse = mean_squared_error(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(np.expand_dims(kn_preds, axis=1))))\n\nprint('R-Squared: ', kn_r2)\nprint('Mean Squared Error: ', np.format_float_scientific(np.asarray(kn_mse), precision=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R-squared and Mean Squared Error observed between the K-Nearest Neighbour Regressor testing data outputs and ground truth values were within ranges of 0.635-0.777 (R-squared) and 1.611x10$^{14}$-3.881x10$^{14}$ (Mean Squared Error) at means of 0.693 and 2.403x10$^{14}$ respectively (5 re-runs in total). NOTE: these were calculated from 5 re-runs, and later re-runs may record values outside of these ranges due to the random nature of the train/test splitting method.\n\nThe K-Nearest Neighbour Regressor exhibits a lower mean R-squared value and higher mean MSE value which suggests that it is less accurate than the Support Vector and Random Forest counterparts."},{"metadata":{},"cell_type":"markdown","source":"### 4.4 A Deep Learning MLP Regressor\n\nA Deep Learning MLP Regressor was also applied. This was undertaken using Pytorch.\n\nFirst, the data is converted to PyTorch tensors as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# tensorize our x/y train/test data to form pytorch tensors\nX_train_tensor = torch.from_numpy(X_train.to_numpy()).float()\nX_test_tensor = torch.from_numpy(X_test.to_numpy()).float()\ny_train_tensor = torch.from_numpy(y_train.to_numpy()).float()\ny_test_tensor  = torch.from_numpy(y_test.to_numpy()).float()\nprint(\"X_train_tensor shape is\", X_train_tensor.shape)\nprint(\"X_test_tensor shape is\", X_test_tensor.shape)\nprint(\"y_train_tensor shape is\", y_train_tensor.shape)\nprint(\"y_test_tensor shape is\", y_test_tensor.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Specifically, the model (below) was built with two hidden layers, the first of which features a width of 4000 neurons, and the second a width of 1000 neurons which map the 318 input variables to a single output. ReLU layers were also applied to introduce non-linearity and a small dropout of 0.02 was used in order to reduce overfitting. The Deep Learning MLP Regressor is constructed and defined as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct the deep learning MLP regressor\nclass MLP_Regressor(nn.Module):\n    def __init__(self, input_dim, layer_sizes, dropout):\n        super(MLP_Regressor, self).__init__()\n        self.mlp = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(input_dim, layer_sizes[0]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(layer_sizes[0], layer_sizes[1]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(layer_sizes[1], 1),\n        )        \n        \n    # the forward pass through the network\n    def forward(self, input_tensor):\n        \n        output_tensor = self.mlp(input_tensor)  # pass the input tensor through the mlp\n        \n        return output_tensor\n    \n# now lets define the model\nmlp_regressor = MLP_Regressor(X_train_tensor.shape[1],\n                               [4000, 1000],\n                               0.02)\nprint(mlp_regressor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_function = nn.MSELoss()  # mse loss function\noptimizer = torch.optim.Adam(mlp_regressor.parameters(), lr=0.0001)  # adam's optimiser\nepochs = 1000  # number of epochs\nloss_vals_train = []  # hold the training loss values\nloss_vals_valid = []  # hold the validation loss values\n\nfor i in range(epochs):\n    y_pred_tensor = mlp_regressor(X_train_tensor)  # obtain y predictions\n    single_loss = loss_function(y_pred_tensor[:-20], torch.unsqueeze(y_train_tensor[:-20], dim=1))  # calculate training loss\n    loss_vals_train.append(single_loss.item())\n    \n    # now calculate the validation loss\n    with torch.no_grad():  # disable the autograd engine\n        val_loss = loss_function(y_pred_tensor[-20:], torch.unsqueeze(y_train_tensor[-20:], dim=1))  # calculate validation loss\n        loss_vals_valid.append(val_loss.item())\n    \n    optimizer.zero_grad()  # zero the gradients\n    single_loss.backward()  # backpropagate through the model\n    optimizer.step()  # update parameters\n    \n    if i%25 == 0:\n        print(f'epoch: {i:5} training loss: {single_loss.item():10.8f} validation loss: {val_loss.item():10.8f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training and validation losses are plotted against the number of epochs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize': (45.0, 20.0)})\nsns.set(font_scale=8.0)\nsns.set_context(\"notebook\", font_scale=5.5, rc={\"lines.linewidth\": 1.0})\nx_vals = np.arange(0, epochs, 1)\nax = sns.lineplot(x=x_vals, y=loss_vals_train)\nax = sns.lineplot(x=x_vals, y=loss_vals_valid)\nax.set_ylabel('Loss', labelpad=20, fontsize=75)\nax.set_xlabel('Epochs', labelpad=20, fontsize=75)\nplt.legend(labels=['Training loss', 'Validation loss'], facecolor='white', framealpha=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Deep Learning MLP Regressor is applied on unseen testing X data and evaluated as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# activate the evaluation mode for the model\nmlp_regressor.eval()\n\n# make predictions with the mlp model\nmlp_preds = mlp_regressor(X_test_tensor).detach().numpy()\n\n# calculate r-squared on inverse transformed data\nmlp_r2 = r2_score(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(mlp_preds)))\n\n# calculate mse on inverse transformed data\nmlp_mse = mean_squared_error(np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1))),\n                 np.exp(price_scaler.inverse_transform(mlp_preds)))\n\nprint('R-Squared: ', mlp_r2)\nprint('Mean Squared Error: ', np.format_float_scientific(np.asarray(mlp_mse), precision=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R-squared and Mean Squared Error observed between the Deep Learning MLP Regressor testing data outputs and ground truth values were within ranges of 0.845-0.896 (R-squared) and 6.857x10$^{13}$-1.653x10$^{14}$ (Mean Squared Error) at means of 0.872 and 1.004x10$^{14}$ respectively. NOTE: these were calculated from 5 re-runs, and later re-runs may record values outside of these ranges due to the random nature of the train/test splitting method and the way that the MLP weights are initialised.\n\nThe mean R-squared and mean MSE values from this Regressor are higher and lower respectively than the Support Vector and K-Nearest Neighbour equivalents, indicating that this Regressor is more accurate. The mean R-squared and MSE values are very similar to that of the Random Forest Regressor, although slightly lower and higher respectively which may indicate slightly lower accuracy. However, it is important to note that the MLP Regressor may appear to be a bit more consistent with its performance, as the range of the R-squared recorded over 5 re-runs is slightly lower than the Random Forest Regressor."},{"metadata":{},"cell_type":"markdown","source":"## Stage 5: Model Comparison\n\nThe last stage aims to compare and further evaluate each model. Specifically, the R-squared and MSE of each model's predictions run with the current kernel are plotted as barplots. Additionally, the testing data predictions are plotted against the ground truth in the form of a scatterplot, and the residuals of each model are plotted as histograms."},{"metadata":{"trusted":true},"cell_type":"code","source":"# store revelant information as lists\nr2_vals = [sv_r2, rf_r2, kn_r2, mlp_r2]\nmse_vals = [sv_mse, rf_mse, kn_mse, mlp_mse]\nmodels = ['Support Vector \\nRegression', 'Random Forest \\nRegression',\n          'K-Nearest Neighbour \\nRegression', 'Deep Learning MLP \\nRegression']\n\n# convert lists into a single dataframe\naccuracy_df = pd.DataFrame({'Model': models, 'R-Squared': r2_vals, 'MSE': mse_vals})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A barplot to compare the R-Squared of each model's testing data predictions is produced as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot r2 score as a barplot\nsns.set_context(\"notebook\", font_scale=4.5, rc={\"lines.linewidth\": 0.5})\nax = sns.barplot(x=\"Model\", y=\"R-Squared\", data=accuracy_df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment='right')\nax.set_ylabel('R-Squared', labelpad=50, fontsize=85)\nax.set_xlabel('Model', labelpad=50, fontsize=85)\n\nplt.title(\"A Barplot comparing the testing data R-Squared \\nof all four regressors\", fontsize=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A barplot to compare the MSE of each model's testing data predictions is generated as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot MSE score as a barplot\nsns.set_context(\"notebook\", font_scale=4.5, rc={\"lines.linewidth\": 0.5})\nax = sns.barplot(x=\"Model\", y=\"MSE\", data=accuracy_df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment='right')\nax.set_ylabel('MSE', labelpad=50, fontsize=85)\nax.set_xlabel('Model', labelpad=50, fontsize=85)\n\nplt.title(\"A Barplot comparing the testing data MSE \\nof all four regressors\", fontsize=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain the ground truth and predictions of each regressor as lists\nground_truth = np.exp(price_scaler.inverse_transform(np.expand_dims(y_test, axis=1)))\nsv_preds = np.exp(price_scaler.inverse_transform(np.expand_dims(sv_preds, axis=1)))\nrf_preds = np.exp(price_scaler.inverse_transform(np.expand_dims(rf_preds, axis=1)))\nkn_preds = np.exp(price_scaler.inverse_transform(np.expand_dims(kn_preds, axis=1)))\nmlp_preds = np.exp(price_scaler.inverse_transform(mlp_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The predictions obtained with the testing data of each model are then plotted against the ground truth using the code outlined below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf, ax = plt.subplots(2, 2)\nsns.scatterplot(x=ground_truth.flatten(), y=sv_preds.flatten(), ax=ax[0, 0], color='red', s=150)\nsns.scatterplot(x=ground_truth.flatten(), y=rf_preds.flatten(), ax=ax[0, 1], color='blue', s=150)\nsns.scatterplot(x=ground_truth.flatten(), y=kn_preds.flatten(), ax=ax[1, 0], color='green', s=150)\nsns.scatterplot(x=ground_truth.flatten(), y=mlp_preds.flatten(), ax=ax[1, 1], color='purple', s=150)\n\n# add subfigure titles\nax[0, 0].set_title('Support Vector Regressor', fontsize=60)\nax[0, 1].set_title('Random Forest Regressor', fontsize=60)\nax[1, 0].set_title('K-Nearest Neighbour Regressor', fontsize=60)\nax[1, 1].set_title('Deep Learning MLP Regressor', fontsize=60)\n\n# generate annotations\nannotations = []\nfor i, row in accuracy_df.iterrows():\n    annotations.append('R2: ' + str(round(row[1], 3)) + '\\nMSE: ' + str(np.format_float_scientific(np.asarray(row[2]), precision=3)))\n\n# annotate the subfigures with the R-squared and MSE values\nax[0,0].annotate(annotations[0], xy=(0, max(sv_preds)*0.65), fontsize=40, color='dimgrey')\nax[0,1].annotate(annotations[1], xy=(0, max(rf_preds)*0.65), fontsize=40, color='dimgrey')\nax[1,0].annotate(annotations[2], xy=(0, max(kn_preds)*0.65), fontsize=40, color='dimgrey')\nax[1,1].annotate(annotations[3], xy=(0, max(mlp_preds)*0.65), fontsize=40, color='dimgrey')\n\n# set all y-labels\nplt.setp(ax[:, :], ylabel='Predicted \\nvalues', xlabel='Observed values')\n\nf.tight_layout(pad=2.0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By plotting the observed values against the predicted values of each Regressor, the similarity between the two values can be visualised, whereby an a tighter spread of points within the two-dimensional space and greater adherence to an x=y relationship indicates greater similarity between the observed and predicted values (and thus higher accuracy). With this in mind, the K-Nearest Neighbour Regressor appears to have the least tight spread of points, followed by the Support Vector Regressor, whilst both the Deep Learning MLP Regressor and Random Forest Regressor exhibit the tighter spreads (although in this individual run the Random Forest Regressor did display a slightly higher R-squared). The R-squared values and MSE values also follow this ordering."},{"metadata":{},"cell_type":"markdown","source":"Residuals are calculated, and then plotted as histograms as shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function to calculate residuals\ndef calculate_residuals(truth: list, preds: list):\n    \n    residuals = []\n    for i in range(len(truth)):\n        res = truth[i] - preds[i]\n        residuals.append(res)\n    \n    return residuals\n\n# calculate the residuals from the predictions of each regressor\nsv_res = calculate_residuals(ground_truth, sv_preds)\nrf_res = calculate_residuals(ground_truth, rf_preds)\nkn_res = calculate_residuals(ground_truth, kn_preds)\nmlp_res = calculate_residuals(ground_truth, mlp_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf, ax = plt.subplots(2, 2)\nax[0,0].hist(np.asarray(sv_res), color='red', edgecolor='black', linewidth=1.2)\nax[0,1].hist(np.asarray(rf_res), color='blue', edgecolor='black', linewidth=1.2)\nax[1,0].hist(np.asarray(kn_res), color='green', edgecolor='black', linewidth=1.2)\nax[1,1].hist(np.asarray(mlp_res), color='purple', edgecolor='black', linewidth=1.2)\n\n# add subfigure titles\nax[0, 0].set_title('Support Vector Regressor', fontsize=60)\nax[0, 1].set_title('Random Forest Regressor', fontsize=60)\nax[1, 0].set_title('K-Nearest Neighbour Regressor', fontsize=60)\nax[1, 1].set_title('Deep Learning MLP Regressor', fontsize=60)\n\n# annotate the subfigures with the median values\nax[0,0].annotate(\"Median: \\n\" + str(np.format_float_scientific(np.median(np.sort(np.asarray(sv_res))),\n                                                             precision=3)), xy=(max(sv_res)*0.3, 40),\n                 fontsize=45)\n\nax[0,1].annotate(\"Median: \\n\" + str(np.format_float_scientific(np.median(np.sort(np.asarray(rf_res))),\n                                                             precision=3)), xy=(max(rf_res)*0.3, 40),\n                 fontsize=45)\n\nax[1,0].annotate(\"Median: \\n\" + str(np.format_float_scientific(np.median(np.sort(np.asarray(kn_res))),\n                                                             precision=3)), xy=(max(kn_res)*0.3, 40),\n                 fontsize=45)\n\nax[1,1].annotate(\"Median: \\n\" + str(np.format_float_scientific(np.median(np.sort(np.asarray(mlp_res))),\n                                                             precision=3)), xy=(max(mlp_res)*0.3, 40),\n                 fontsize=45)\n\n\n# set all x and y-labels\nplt.setp(ax[:, :], ylabel='Frequency', xlabel='Residual values')\n\nf.tight_layout(pad=1.4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The residual histograms above show the distributions of residuals, with the median residual values providing insight into whether the models have a tendency to over or underpredict values.\n\nIn the kernel used in this notebook, the Support Vector Regressor, Random Forest Regressor and Deep Learning MLP Regressor appear to have a tendency to underpredict values (positive median residual value) whilst the K-Nearest Neighbour Regressor appears to have a tendency to overpredict values (negative median residual value).\n\nIt is also worth noting that both the Random Forest Regressor and Deep Learning MLP Regressor both exhibit a higher frequency of residual values closer to 0, whilst the Support Vector Regressor, and especially the K-Nearest Neighbour Regressor display a slightly wider range of residual values and fewer residuals closer to the 0 mark."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nRegarding the four Regressors applied, it is first of all most clear that the K-Nearest Neighbour Regressor performed the worst. This is evidenced by the low R-squared values and high MSE values, both in the context of this kernel and the means of the 5 re-runs as well as the non-tight spread of points within the observed vs predicted values feature space and the comparatively lwoer frequency of residuals near the 0 value. \n\nWith the same indicators in mind, the Support Vector Regressor exhibited the second worse performance as it displayed lower R-squared values and higher MSE values than the Random Forest and Deep Learning MLP Regressors both in terms of this kernel and the means of the 5 re-runs as well as the point spread of the scatterplot and the distribution of residuals.\n\nSeparating the Random Forest Regressor and Deep Learning MLP Regressor and determining which takes the top spot is not quite as obvious as the two exhibited similar performances in terms of R-squared, MSE, spread of points on the scatterplot and distribution of residuals. However, the Random Forest Regressor did display a slightly higher (0.459%) mean R-squared value and slightly lower (1.205%) mean MSE value than the Deep Learning MLP Regressor as well as slightly higher R-squared and lower MSE values within this kernel. As such, the Random Forest Regressor is considered the best Regressor of the four, with the Deep Learning MLP Regressor closely following it. However, it is worth noting that over the 5 re-runs the Deep Learning MLP Regressor did display slightly higher consistency with regards to its lower range of R-squared values."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}