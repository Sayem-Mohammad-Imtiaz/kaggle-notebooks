{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\n\nHi, this kernel will try to find what are the most important features when it come to Employee Attrition.\n\nTo be able to do so, I will perform a RandomForestClassifier before building a bar plot of the features importance. "},{"metadata":{},"cell_type":"markdown","source":"# IMPORTING MODULES"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic module\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Preparation module\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n#Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA EXPLORATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Info\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numerical variables summary statistics\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for null values in the dataset\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Null model\n\nThis null model tell us that if we had to random guess if an employee will leave or not, we will be right 83.87% of the  time. We will compare this results with the prediction from our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#return the random guess if we had to predict if and employee will leave or not\nrandom_guess = 1-len(df[df['Attrition']=='Yes'])/df.shape[0]\nrandom_guess","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### EDA takeaways\n\n* the dataset has 1470 observation and 35 features. The small number of row could be problematic.\n* There are 26 features of type integers and 9 of type Object.\n* The dataset has no missing data.\n* The model should be higher then 83% if we want it to be good or accepteble.\n"},{"metadata":{},"cell_type":"markdown","source":"# VISUALISATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the object features #\nplt.style.use('ggplot')\n#Create a loop that print all categorical variable against the attrition variable\nfor col in df.select_dtypes('object'):\n    plt.figure(figsize=(8,6))\n    sns.countplot(x=col,hue='Attrition',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the int features \nfor col in df.select_dtypes('int64'):\n    plt.figure(figsize=(10,8))\n    sns.boxplot(x='Attrition', y=col,data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create heat map to see correlated features\nplt.figure(figsize=(10,8))\nsns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURES SELECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look for variables with low variances\ndf.var(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graphics above, we can already delete some columns that are usesless.\n* Employee count\n* Employee number\n* StandardHours\n* Over 18\n* Perfomance rating\n* Stock Option level\n* Job Involvement\n"},{"metadata":{},"cell_type":"markdown","source":"# Prepare the data\n\nIn the next section, we will delete a few number of rows with low variance and then split the data into categorical and numerical features.\nThe purpose of this step is to Encode and Scale the features. Finaly we transform everything back to a dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the features with low variance\nto_drop = ['StandardHours','EmployeeCount','EmployeeNumber','Over18','PerformanceRating','StockOptionLevel','JobInvolvement']\ndf.drop(to_drop,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split X and y \nX= df.drop('Attrition',axis=1)\ny=df['Attrition'].replace({'Yes':1,'No':0})\n\n#split categorical , numerical and ordinal features\ncategorical = list(X.columns[X.dtypes=='object'])\nordinal = ['Education','EnvironmentSatisfaction','JobLevel','JobSatisfaction','WorkLifeBalance','RelationshipSatisfaction']\nnumerical = list(X.drop(categorical + ordinal,axis=1))\n\n#Transform numerical and categorical features\nX_cat = pd.get_dummies(X[categorical]) #Transform categorical into 0 and 1\nX_num = StandardScaler().fit_transform(X[numerical])\nX_num = pd.DataFrame(X_num,columns=X[numerical].columns) #Transform the array back to a dataframe for future use\n\n#Create the new X object and look at it\nX_new = pd.concat([X_num,X_cat],axis=1)\nX_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far so good, everything has been transform with no problem. We now have 42 features.\nI only scale the reel numerical features. Those in the ordinal features have a rank among them. So it shouldn't be a problem for the model to compare these variables."},{"metadata":{},"cell_type":"markdown","source":"## Split the data in train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test= train_test_split(X_new,y,test_size=0.40,shuffle=True)\n\nprint('X_train shape',X_train.shape)\nprint('X_test shape',X_test.shape)\nprint('y_train shape',y_train.shape)\nprint('y_test shape',y_test.shape)\n\n#I decided to put the test_size at 40% because the dataset have very few observation.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality reduction using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA to reduce the dimension and plot the graph\npca = PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd=np.argmax(cumsum>0.95)+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the variance curve against the number of features\nplt.plot(cumsum)\nplt.xlabel('Number of features')\nplt.ylabel('Explained Variance')\nplt.plot(d,0.95,marker='d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the number of features to keept\nd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This section of code extracted the number of features that keeps 95% variance. In other words, 22 features could predict 95% of the attrition prediction. Although PCA algorithm normaly change the features and comprese those with high correlation together. I used it to simply show me how many feature I should keep. The next lines of code will use a SelectKBest model to choos 22 features among the 42 that are the best."},{"metadata":{"trusted":true},"cell_type":"code","source":"k=21\n#Changing the Train set\nselector = SelectKBest(f_classif,k=k)\nselector.fit(X_train,y_train)\n\n# Keep only the selected features into a new variable X_train_reduced\ncol=selector.get_support(indices=True)\nX_train_reduced = X_train.iloc[:,col]\n\n#Changing the Test set\nselector.fit(X_test,y_test)\n\n#Same as above\ncol=selector.get_support(indices=True)\nX_test_reduced = X_test.iloc[:,col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Quick look at the new data\nX_train_reduced","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create fit and score the model\nrfc = RandomForestClassifier(n_estimators=700,max_depth=10,n_jobs=-1,random_state=123)\nrfc_model = rfc.fit(X_train_reduced,y_train)\n\nrfc_scores = cross_val_score(rfc,X_train_reduced,y_train,scoring='accuracy',cv=5)\nprint('This is train score',rfc_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict the model\ny_pred_rfc = rfc_model.predict(X_test_reduced)\nprint('This is test score: ',accuracy_score(y_pred_rfc,y_test))\n\n#Print the confusion_matrix\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test,y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sadly, my model have accuracy of 85 which is pretty close to the null model. I am still happy with the precision score of it. I few things could still be done to enhance the model performance. \n* First the target feature is imbalance and a SMOTE algorithm could fix that problem. \n* Get more data because 1470 observation is pretty small.\n* Remove Outlier from the dataset."},{"metadata":{},"cell_type":"markdown","source":"# Features Importance\n\nIn this section we will finaly find wich features are responsible for attrition, rank by importance."},{"metadata":{},"cell_type":"markdown","source":"## Plot the importance of each column in attrition rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model that \ndef plot_feature_importance(importance,names,model_type): \n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data) \n    \n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True) \n    \n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance(rfc.feature_importances_,X_train_reduced.columns,'RANDOM FOREST')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION\n\n#### From the features importance graph above, the companie would be able to dig deeper into the most relevent features or attributes related to attrition among their employee. Starting probably with the top 5 causes.\n\n\n\nPlease upvote if find this kernel usefull. Feel free to comment my code if you thing something could have been done differently.\n\nThank you."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}