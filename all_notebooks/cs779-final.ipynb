{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cloning Indic NLP library\n!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n!pip install Morfessor\n!pip install -U nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing all the libraries\nimport csv\nimport sys\nfrom spacy.lang.en import English\nfrom io import open\nimport string\nimport re\nimport random\nimport nltk\nimport time\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch import optim\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom nltk.tokenize import word_tokenize\nimport operator\nfrom queue import PriorityQueue\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import single_meteor_score\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\n\n\nnltk.download('punkt')  \nnltk.download('wordnet')\n\n# This variable which store whether gpu is available or not\nuse_cuda = torch.cuda.is_available()\n# input path for train data\ninput_path = '/kaggle/input/hineng/train/train.csv'\n\nINDIC_NLP_LIB_HOME=r\"/kaggle/working/indic_nlp_library\"\nINDIC_NLP_RESOURCES=\"/kaggle/working/indic_nlp_resources\"\nsys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n\nfrom indicnlp import common\nfrom indicnlp import loader\nfrom indicnlp.tokenize import indic_tokenize \nfrom indicnlp.normalize.indic_normalize import BaseNormalizer\n\ncommon.set_resources_path(INDIC_NLP_RESOURCES)\nloader.load()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the data from the csv file \nhindi_sent = []\nenglish_sent = []\nwith open(input_path, 'r') as file:\n    my_reader = csv.reader(file, delimiter=',')\n    header = next(my_reader)\n    for row in my_reader:\n        hindi_sent.append(row[1])\n        english_sent.append(row[2])\npairs = list(zip(hindi_sent, english_sent))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ukn = 0 # Unknown index\nsos = 1 # Start of Sequence index\neos = 2 # End of Sequence index\npad = 3 # Padding index\nteacher_forcing = 0.5 # teacher forcing ratio\nMAX_LENGTH = 60 # Max length for input sentence while training\nMAX_LENGTH_OUT = 100 # Max length output sentence generated\nMIN_FREQUENCY = 1 # Min frequency of word to be counted in vocabulary\nnon_word_idxs = [0,1,2,3] # non-word indexes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To normalize hindi sentences\ndef normalize_sentence(s):\n    normalizer = BaseNormalizer(\"hi\", remove_nuktas=False)\n    output_text=normalizer.normalize(s)\n    return output_text\n\n# To tokenize sentence\ndef tokenize_sentence(s, lang):\n    if(lang.name == 'English'):\n        tokens = list(word_tokenize(s))\n    else:\n        tokens = list(indic_tokenize.trivial_tokenize(normalize_sentence(s)))\n    return tokens\n\n\n# get list of indexes of each token corresponding to sequence\ndef indexesFromSentence(lang, s):\n    if(lang.name == 'English'):\n        return [lang.word2index.get(str(word),ukn) for word in list(word_tokenize(s))]\n    else:\n        return [lang.word2index.get(word, ukn) for word in list(indic_tokenize.trivial_tokenize(s))]\n\n# returns the indexes as above just adds the <sos> and <eos>\ndef variableFromSentence(lang, s):\n    indexes = indexesFromSentence(lang, s)\n    indexes = [sos]+indexes+[eos]\n    return indexes\n    \n# this returns a pair of list of indexes corresponding to hindi and english sentence\ndef variablesFromPair(hindi_lang,english_lang,pair):\n    input_variable = variableFromSentence(hindi_lang, pair[0])\n    target_variable = variableFromSentence(english_lang, pair[1])\n    return (input_variable, target_variable)\n\n# This does the required padding\ndef do_padding(pair, max_leng):\n    if(len(pair) > max_leng):\n        pair = pair[:max_leng]\n    else:\n        sz = len(pair)\n        for i in range(max_leng-sz):\n            pair.append(pad)\n    return pair\n\n\n# Creating out language using the data\ndef prepare_data(pairs):\n    hindi_lang = Language('Hindi')\n    english_lang = Language('English')\n    for pair in pairs:\n        for token in tokenize_sentence(pair[0], hindi_lang):\n            hindi_lang.addWord(token)\n        for token in tokenize_sentence(pair[1], english_lang):\n            english_lang.addWord(str(token))\n            \n    print(f'English has {english_lang.vocab} words')\n    print(f'Hindi has {hindi_lang.vocab} words')\n    \n    return (hindi_lang, english_lang)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a Language class \nclass Language:\n  def __init__(self, name):\n    self.name = name # name of language\n    self.word2index = {\"<sos>\": 1, \"<eos>\": 2,\"<ukn>\": 0, \"<pad>\": 3} # word to index dictionary\n    self.index2word = {1: \"<sos>\", 2: \"<eos>\", 0: \"<ukn>\", 3: \"<pad>\"} # index to word dictionary\n    self.vocab = 4 # vocabulary of the language\n    self.minfrequency = MIN_FREQUENCY # minimum frequency of each word required to be added to the vocab\n    self.wordcount = {\"<sos>\": self.minfrequency, \"<eos>\": self.minfrequency,\"<ukn>\": self.minfrequency, \"<pad>\": self.minfrequency} # dictionary to store the count of each word\n\n  def addWord(self, word): # adding word to language\n    if word not in self.wordcount:\n        self.wordcount[word] = 0\n    self.wordcount[word]+=1\n    if((self.wordcount[word] >= self.minfrequency) and (word not in self.word2index)):\n      self.word2index[word] = self.vocab\n      self.index2word[self.vocab] = word\n      self.vocab += 1\n\n## Used in beam search\nclass BeamNode(object):\n    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n        self.leng = length\n        self.h = hiddenstate\n        self.wordid = wordId\n        self.prevNode = previousNode\n        self.logp = logProb\n        \n    def eval(self):\n        return self.logp / float(self.leng - 1 + 1e-6) \n    def __gt__(self, other):\n        return self.leng > other.leng\n    def __lt__(self, other):\n        return self.leng < other.leng ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The language object for Hindi and English language\nhindi_lang = Language('Hindi')\nenglish_lang = Language('English')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here I remove the special characters from out english sentences\ntemp = pairs\npairs = []\nfor pair in temp:\n    curr = pair[1].lower()\n    split_true = list(filter(None, re.split(r'[\\s!\"#$%&\\()+,-./:;<=>?@\\\\^_`{|}~]+', curr)))\n    pairs.append((pair[0], ' '.join(split_true)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here I am doing the train-dev split \nrandom.seed(44)\nrandom.shuffle(pairs)\ntrain_length = int(0.98*len(pairs))\ntrain_split = pairs[:train_length]\ndev_split = pairs[train_length:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here I am filtering the train split, and will use only those hindi sentences whose length <= MAX_LENGTH\ntemp = train_split\ntrain_split = []\nfor pair in temp:\n    if(len(tokenize_sentence(pair[0], hindi_lang))<=MAX_LENGTH):\n        train_split.append(pair)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hindi_lang, english_lang = prepare_data(train_split)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## In this cell we are converting our sentences into indexes, doing padding and then creating the dataloader object for both train and dev set\ntraining_pairs = []\neval_pairs = []\ntrue_sentences = []\nfor i in range(len(train_split)): # train_split is a list of pairs of sentences, (hindi sentence, english sentences)\n    training_pairs.append(variablesFromPair(hindi_lang, english_lang, train_split[i]))# variables from pair returns a pair, (list of index of hindi sentence, list of index of english sentence)\n\nfor i in range(len(dev_split)):\n    eval_pairs.append(variablesFromPair(hindi_lang, english_lang, dev_split[i]))\n    true_sentences.append(dev_split[i][1])\n\nx_train = []\ny_train = []\nx_eval = []\ny_eval = []\nfor pair in training_pairs:\n    x_train.append(do_padding(pair[0], MAX_LENGTH))\n    y_train.append(do_padding(pair[1], MAX_LENGTH_OUT))\n\nfor pair in eval_pairs:\n    x_eval.append(do_padding(pair[0], MAX_LENGTH))\n    y_eval.append(do_padding(pair[1], MAX_LENGTH_OUT))\n\nx_train = torch.tensor(x_train)\ny_train = torch.tensor(y_train)\n\nx_eval = torch.tensor(x_eval)\ny_eval = torch.tensor(y_eval)\n\ntrain = torch.utils.data.TensorDataset(x_train,y_train)\ntrain_data = torch.utils.data.DataLoader(train, batch_size=32)\n\ndev = torch.utils.data.TensorDataset(x_eval,y_eval)\ndev_data = torch.utils.data.DataLoader(dev, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, n_layers=1, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.input_size = input_size  # The vocabulary size of hindi\n        self.hidden_size = hidden_size  # The hidden size in our case 300\n        self.embed_size = embed_size  # The embedding size in our case 256\n        self.embed = nn.Embedding(input_size, embed_size) # Using the embedding layer to get our embedding corresponding to our sentence \n        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout, bidirectional=True) # Passing the embedding through a bi-directional GRU\n\n    def forward(self, src, hidden=None):\n        embedded = self.embed(src)  # Using the embedding layer on our input\n        outputs, hidden = self.gru(embedded, hidden)  # Using the GRU layer on the embeddings of the sequence\n        outputs = (outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]) # As the GRU is bidirectional, adding both the diirections output\n        return outputs, hidden\n\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.hidden_size = hidden_size # The hidden size in our case 300\n        self.v = nn.Parameter(torch.rand(hidden_size))\n        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n        self.v.data.uniform_(-1.0/math.sqrt(self.v.size(0)), 1.0/math.sqrt(self.v.size(0))) \n\n    def forward(self, hidden, encoder_outputs):\n        steps = encoder_outputs.size(0)\n        h = hidden.repeat(steps, 1, 1).transpose(0, 1) \n        encoder_outputs = encoder_outputs.transpose(0, 1) \n        energy = F.relu(self.attn(torch.cat([h, encoder_outputs], 2)))\n        value = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1) \n        energy = torch.bmm(value, energy.transpose(1,2)).squeeze(1) \n        \n        return F.softmax(energy, dim=1).unsqueeze(1) \n\n\nclass Decoder(nn.Module):\n    def __init__(self, embed_size, hidden_size, output_size, n_layers=1, dropout=0.2):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size \n        self.n_layers = n_layers \n        self.output_size = output_size \n        self.embed_size = embed_size \n\n        self.embed = nn.Embedding(output_size, embed_size)\n        self.dropout = nn.Dropout(dropout, inplace=True)\n        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout)\n        self.out = nn.Linear(hidden_size * 2, output_size)\n        self.attention = Attention(hidden_size)\n\n    def forward(self, input, last_hidden, encoder_outputs): \n        embedded = self.embed(input).unsqueeze(0) # Get the embedding of the current input word (last output word)\n        embedded = self.dropout(embedded) # add some dropout\n        attn_weights = self.attention(last_hidden[-1], encoder_outputs)  # Calculate attention weights and apply to encoder outputs\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)).transpose(0, 1)  # Carry batch matrix multiplication with attention weights\n        input_to_gru = torch.cat([embedded, context], 2) # Combine embedded input word and attended context, run through RNN\n        output, hidden = self.gru(input_to_gru, last_hidden) \n        output = self.out(torch.cat([output.squeeze(0), context.squeeze(0)], 1)) \n        output = F.log_softmax(output, dim=1) ## using softmax to get probabilities for each word in vocab\n        return output, hidden, attn_weights  \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is just the combination of the decoder, encoder and attention module class\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        max_len = trg.size(0) \n        batch_size = src.size(1)\n        vocab_size = self.decoder.output_size\n        if(use_cuda):\n            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n        else:\n            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))\n        encoder_output, hidden = self.encoder(src)\n        hidden = hidden[:self.decoder.n_layers] \n        all_attention = []\n        output = Variable(trg.data[0, :])\n        for t in range(1, max_len):\n            output, hidden, attn_weights = self.decoder(\n                output, hidden, encoder_output) \n            all_attention.append(attn_weights)\n            outputs[t] = output\n            top1 = output.data.max(1)[1]\n            is_teacher = random.random() < teacher_forcing_ratio\n            if(use_cuda):\n                output = Variable(trg.data[t] if is_teacher else top1).cuda()\n            else:\n                output = Variable(trg.data[t] if is_teacher else top1)\n        return outputs, all_attention\n\n    # Here we find the decoded sentences given a batch of hindi sentences after indexing\n    def decode(self, src, strategy='beam'):\n        encoder_output, hidden = self.encoder(src) \n        hidden = hidden[:self.decoder.n_layers] \n        if strategy == 'beam':\n            return self.beam_search(hidden, encoder_output)\n        else:\n            return self.greedy_search(hidden, encoder_output)\n        \n        \n    def infer(self, input_sent, strategy = 'beam'): # input should be a tensor of shape [sent_len, 1]\n        encoder_output, hidden = self.encoder(input_sent)\n        hidden = hidden[:self.decoder.n_layers]\n        if(strategy == 'beam'):\n            return self.beam_search(hidden, encoder_output)\n        else:\n            return self.greedy_search(hidden, encoder_output)\n\n    # Greedy strategy of decoding i.e at every step we pick the word with the maximum probability\n    def greedy_search(self, decoder_hidden, encoder_outputs):\n        batch_size = decoder_hidden.shape[1]\n        seq_len = MAX_LENGTH_OUT\n        decoded_batch = torch.zeros((batch_size, seq_len))\n        if(use_cuda):\n            decoder_input = Variable(torch.tensor([1]*batch_size)).cuda()  # sos\n        else:\n            decoder_input = Variable(torch.tensor([1]*batch_size))\n        for t in range(seq_len):\n            decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.data.topk(1) \n            topi = topi.view(-1)\n            decoded_batch[:, t] = topi\n            decoder_input = topi.detach().view(-1)\n\n        return decoded_batch\n\n    # Beam strategy of decoding here we pick a set of top k words s.t the overall probability is maximized rather than taking maximum probability word at each step. \n    # This decoding takes a lot of time so during training we use greedy to check model performance on evaluation. In test set decoding we use this with beam size of 5\n    def beam_search(self, decoder_hiddens, encoder_outputs):\n        beam_size = 5 # The beam size\n        topk = 1  # number of sentences we need to infer for a particular hindi sentence\n        decoded_batch = [] # This will store the decoded sentences for entire batch\n        seq_len = MAX_LENGTH_OUT # The maximum length of inferred sentence\n        batch_size = decoder_hiddens.shape[1]\n#         print(batch_size)\n        # This strategy works when we do one sentence at a time so loop over the batch size\n        for idx in range(batch_size):  # batch_size \n            encoder_output = encoder_outputs[:, idx, :].unsqueeze(1) \n            decoder_hidden = decoder_hiddens[:, idx, :].unsqueeze(0) \n            # Check if cuda is available\n            if(use_cuda):\n                decoder_input = torch.LongTensor([sos]).cuda()\n            else:\n                decoder_input = torch.LongTensor([sos])\n\n            # Number of sentence to generate\n            end_nodes = []\n            if((topk + 1) < topk - len(end_nodes)):\n                req_num = topk + 1\n            else:\n                req_num = topk - len(end_nodes)\n            node_queue = PriorityQueue() # Using a priority queue to create the queue\n            # starting node -  hidden vector, previous node, word id, logp, length\n            node = BeamNode(decoder_hidden, None, decoder_input, 0, 1)\n            node_queue.put((-node.eval(), node))\n            size_of_queue = 1\n            limit = 2000\n            # start beam search\n            while True:\n                # give up when decoding takes too long\n                if size_of_queue > limit:\n                    break\n                # Here we extract the best node\n                score, n = node_queue.get()\n                decoder_hidden = n.h\n                decoder_input = n.wordid\n\n                if (n.wordid.item() == eos):\n                    if (n.prevNode != None):\n                        end_nodes.append((score, n))\n                        # if required number is reached then break\n                        if len(end_nodes) >= req_num:\n                            break\n                        else:\n                            continue\n                # decode for one step using decoder\n                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output)\n                log_probability, indexes = torch.topk(decoder_output, beam_size)\n                n_next = []\n                for new_k in range(beam_size):\n                    decoded_t = indexes[0][new_k].view(-1)\n                    log_prob = log_probability[0][new_k].item()\n                    node = BeamNode(decoder_hidden, n, decoded_t, n.logp + log_prob, n.leng + 1)\n                    score = -node.eval()\n                    n_next.append((score, node))\n\n                # put them into queue\n                for i in range(len(n_next)):\n                    score, nn = n_next[i]\n                    node_queue.put((score, nn))\n                    # increase size_of_queue\n                size_of_queue += len(n_next) - 1\n            # Choose the best paths for back-tracking\n            if len(end_nodes) == 0:\n                end_nodes = [node_queue.get() for _ in range(topk)]\n            inferred = []\n            for score, nn in sorted(end_nodes, key=operator.itemgetter(0)):\n                possibilities = []\n                possibilities.append(nn.wordid)\n                # Back tracking\n                while nn.prevNode != None:\n                    nn = nn.prevNode\n                    possibilities.append(nn.wordid)\n                possibilities = possibilities[::-1]\n                inferred.append(possibilities)\n            decoded_batch.append(inferred)\n\n        return decoded_batch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## initialising the hyperparameters\nepochs = 10\nbatch_size = 32\nlr = 0.0001\ngrad_clip = 10.0\nembed_size = 256\nhidden_size = 300","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Initialize the model parameters\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.normal_(param.data, mean=0, std=0.05)\n\n\n## Function to train the model\ndef train(epoch, model, optimizer, train_iter, grad_clip):\n    vocab_size = english_lang.vocab\n    model.train()\n    total_loss = 0\n    strt = time.time()\n    overall_loss = 0\n    for idx, batch in enumerate(train_iter):\n        src = batch[0]\n        trg = batch[1]\n        src = src.permute(1,0)\n        trg = trg.permute(1,0)\n  \n        if(use_cuda):\n          src, trg = src.cuda(), trg.cuda()\n        optimizer.zero_grad()\n        output, _ = model(src, trg)\n        loss = F.nll_loss(output[1:].view(-1, vocab_size), trg[1:].contiguous().view(-1), ignore_index=pad)\n        loss.backward()\n        clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        total_loss += loss.data.item()\n        overall_loss += loss.data.item()\n\n        if idx % 100 == 0 and idx != 0:\n            total_loss = total_loss / 100\n            print(\"[%d][loss:%5.2f][time:%5.2f]\" %(idx, total_loss, time.time()-strt))\n            strt = time.time()\n            total_loss = 0\n    return overall_loss/len(train_iter)\n\n## Function to get attention weights\ndef get_attention_weights(model, src, trg): #src --> [length, 1], trg --> [length, 1]\n    with torch.no_grad():\n        model.eval()\n        if(use_cuda):\n            src = src.cuda()\n            trg = trg.cuda()\n        output, attention_wts = model(src, trg, teacher_forcing_ratio=1.0)\n        return attention_wts\n## Returns sentences from list of indexes\ndef indexes_to_sentences(idxs):\n    sequence = []\n    for idx in idxs:\n        if(idx.item() not in non_word_idxs):\n            sequence.append(english_lang.index2word[idx.item()])\n    return TreebankWordDetokenizer().detokenize(sequence)\n\n# Returns sentences from list of indexes\ndef indexes_to_sentences_beam(idxs):\n    sequence = []\n    for idx in idxs[0]:\n        if(idx.item() not in non_word_idxs):\n            sequence.append(english_lang.index2word[idx.item()])\n    return TreebankWordDetokenizer().detokenize(sequence)\n\n## evaluate the model\ndef evaluate(model, val_iter):\n    vocab_size = english_lang.vocab\n    with torch.no_grad():\n        model.eval()\n        total_loss = 0\n        for idx, batch in enumerate(val_iter):\n            src = batch[0]\n            trg = batch[1]\n            src = src.permute(1,0)\n            trg = trg.permute(1,0)\n            if(use_cuda):\n                src = src.data.cuda()\n                trg = trg.data.cuda()\n            output, _ = model(src, trg, teacher_forcing_ratio=0.0)\n            loss = F.nll_loss(output[1:].view(-1, vocab_size), trg[1:].contiguous().view(-1), ignore_index=pad)\n            total_loss += loss.data.item()\n        return total_loss / len(val_iter)\n    \n## evaluate model and get bleu score on dev set    \ndef evaluate_for_score(true_sentences, val_iter):\n    pred_sentences_greedy = []\n    pred_sentences_beam = []\n    for idx, batch in enumerate(val_iter):\n        src = batch[0].permute(1,0)\n        if(use_cuda):\n            src = src.data.cuda()\n        decoded_greedy = seq2seq.decode(src, 'greedy')\n        for i in range(len(decoded_greedy)):\n            pred_sentences_greedy.append(indexes_to_sentences(decoded_greedy[i]))\n#         decoded_beam = seq2seq.decode(src)\n#         for i in range(len(decoded_beam)):\n#             pred_sentences_beam.append(indexes_to_sentences_beam(decoded_beam[i]))\n    print('Scores for greedy decoding')\n    greedy_score =  get_scores(true_sentences, pred_sentences_greedy)\n    beam_score = 0\n#     print('Scores for beam decoding')\n#     beam_score = get_scores(true_sentences, pred_sentences_beam)\n    return (max(greedy_score, beam_score), pred_sentences_beam)\n    \n## the evaluation script \ndef get_scores(true_sentences, pred_sentences):\n    \n    if len(true_sentences) != len(pred_sentences):\n        print(f'E: Number of sentences do not match. True: {len(true_sentences)} Pred: {len(pred_sentences)}')\n        sys.exit()\n\n    for i in range(len(true_sentences)):\n        true_sentences[i]=true_sentences[i].lower()\n        pred_sentences[i]=pred_sentences[i].lower()\n    \n    true_sentences_joined, pred_sentences_joined = [], []\n\n    for i in range(len(true_sentences)):\n        # some punctuations from string.punctuation\n        split_true = list(filter(None, re.split(r'[\\s!\"#$%&\\()+,-./:;<=>?@\\\\^_`{|}~]+', true_sentences[i])))\n        split_pred = list(filter(None, re.split(r'[\\s!\"#$%&\\()+,-./:;<=>?@\\\\^_`{|}~]+', pred_sentences[i])))\n        true_sentences_joined.append(' '.join(split_true))\n        pred_sentences_joined.append(' '.join(split_pred))\n    \n    scores = {}\n    # Macro-averaged BLEU-4 score.\n    scores['bleu_4_macro'] = 0\n    for ref, hyp in zip(true_sentences, pred_sentences):\n        scores['bleu_4_macro'] += sentence_bleu(\n            [ref.split()],\n            hyp.split(),\n            smoothing_function=SmoothingFunction().method2\n        )\n    scores['bleu_4_macro'] /= len(true_sentences)\n\n    # BLEU-4 score.\n    scores['bleu_4'] = corpus_bleu(\n        [[ref.split()] for ref in true_sentences],\n        [hyp.split() for hyp in pred_sentences],\n        smoothing_function=SmoothingFunction().method2\n    )\n\n    # METEOR score.\n    scores['meteor'] = 0\n    for ref, hyp in zip(true_sentences, pred_sentences):\n        scores['meteor'] += single_meteor_score(ref, hyp)\n    scores['meteor'] /= len(true_sentences)\n\n    print(f'D: Scores: {scores}')\n    \n    return scores['bleu_4']\n\n## this function uses the model to get the translations on the test set and forms the answer.txt file\ndef get_test_result(model, test_path):\n    # Printing the converted sentences to a new anster.txt file\n    output_path = 'answer.txt' # output path of the translated test sentences\n    # Reading the test sentences from csv files which needs to be translated\n    test_sentences = []\n    with open(test_path, 'r') as file:\n        my_reader = csv.reader(file, delimiter=',')\n        header = next(my_reader)\n        for row in my_reader:\n            test_sentences.append(row[2])\n\n    pred_new = []\n    count = 0\n    start = time.time()\n    for sent in test_sentences:\n        pred_new.append(indexes_to_sentences_beam(model.infer(torch.tensor(do_padding(variableFromSentence(hindi_lang, sent), 100)).view(-1,1).cuda())[0]))\n        count+=1\n        if(count%100 == 0):\n            print(f'{count} sentences translated {pred_new[-1]}, time taken: {time.time()-start}')\n\n    all_text = \"\"\n    for sent in pred_new:\n        all_text+=sent+\"\\n\"\n\n    # Getting the final translated sentences and outputing it\n    f = open(output_path,\"w\")\n    f.write(all_text)\n    f.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Declaring the model\n\nencoder = Encoder(hindi_lang.vocab, embed_size, hidden_size, n_layers=2, dropout=0.5)\ndecoder = Decoder(embed_size, hidden_size, english_lang.vocab, n_layers=1, dropout=0.5)\nif use_cuda:\n    seq2seq = Seq2Seq(encoder, decoder).cuda()\nelse:\n    seq2seq = Seq2Seq(encoder, decoder)\noptimizer = optim.Adam(seq2seq.parameters(), lr=lr)\n\n## Initializing the model\nseq2seq.apply(init_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To train a new model run this cell otherwise skip this\n\nmax_bleu = 0\nmodel_name = 'CS779_seq2seq_best.pth'\nstart_time = time.time()\ntrain_losses = []\nval_losses = []\nbleu_scores = []\nfor e in range(1, epochs+1):\n    train_iter = iter(train_data)\n    train_loss = train(e, seq2seq, optimizer, train_iter, grad_clip)\n    train_losses.append(train_loss)\n    val_iter = iter(dev_data)\n    val_loss = evaluate(seq2seq, val_iter)\n    val_losses.append(val_loss)\n    print(\"[Epoch:%d] time:%5.3f val_loss:%5.3f\" % (e, (time.time()-start_time), val_loss))\n    start_time = time.time()\n    val_iter = iter(dev_data)\n    score, _ = evaluate_for_score(true_sentences, val_iter)\n    bleu_scores.append(score)\n    print(f'bleu score: {score}')\n    if(score > max_bleu):\n        if(os.path.exists(model_name)):\n            os.remove(model_name)\n        torch.save(seq2seq, model_name)\n        print('Saving Model ...')\n        max_bleu = score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To load the pre-traineed model run this cell\nsaved_model_path = '/kaggle/input/cs779-pretrained-model-2/CS779_seq2seq_best_filtered.pth'\nseq2seq = torch.load(saved_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## use the model to get score on dev set\nval_iter = iter(dev_data)\nscore, pred = evaluate_for_score(true_sentences, val_iter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## use model on test set to get score\ntest_path = '/kaggle/input/testphase/testhindistatements.csv' ## path to test file\nget_test_result(seq2seq, test_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}