{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis is a basic data cleaning notebook for the aircraft pricing dataset. The data was scraped at the beginning of July of 2020, using the Scrapy framework for Python. \n\nI created this notebook mainly for practice and to improve my skills using various numpy, pandas, and regex methods/functions. This notebook is great for beginners as I explain my logic every step of the way. Feel free to reach out with any questions or suggestions. \n\nLets get started! ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# DATA CLEANING - Aircraft Pricing Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libariries \n\nimport numpy as np\nimport pandas as pd\nimport re\n\npd.options.mode.chained_assignment = None # Ignore certain warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import dataset\n\ndf = pd.read_csv('/kaggle/input/used-aircraft-pricing/aircraft_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot of missing data. \n\nKeep in mind that the dataset was scraped from 2 different websites. The following columns - 'Engine 1 Hours', 'Engine 2 Hours', 'Prop 1 Hours', 'Prop 2 Hours', 'Total Seats' and 'Flight Rules' - were only available for some aircraft and on only one of the sites scraped, hence the missing data. \n\nHowever, we also have missing data in the following columns - 'Condition', 'Currency', 'Location', 'Total Hours' and 'National Origin'.\n\nLet's dive in and clean the data one column at a time, starting with the easiest columns first. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Make - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Luckily, there isn't any missing data here.\n\ndf['Make'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Make'].value_counts()[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Make'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'Make' data seems fine at first glance. There are 187 different makes of aircraft throughout the dataset, Cessna being the most popular. Let's make all of the entries uppercase to stay consistent. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are a total of {}/{} uppercase rows in this column'.format((df['Make'].str.isupper().sum()), (len(df))))\ndf['Make'] = df['Make'].str.upper()\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Make'].str.isupper().sum()), (len(df))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing data.\n\ndf['Model'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Model'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying upper method to the Model column. Some aircraft models are stricly numbers, which is why not all have been converted\n# to uppercase. \n\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Model'].str.isupper().sum()), (len(df))))\ndf['Model'] = df['Model'].str.upper()\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Model'].str.isupper().sum()), (len(df))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are a total of 962 different models of aircraft in our dataset.\n\ndf['Model'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Model'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## National Origin - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['National Origin'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter for rows that have NaN values. \n\ndf.loc[df['National Origin'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. [MICCO](http://aso.com/seller/5487/old/micco.htm) - A quick google search revealed that this is a US company.\n2. [STAUDACHER](https://alumni.msu.edu/stay-informed/magazine/article.cfm?id=253) - US manufacturer. \n3. [HOMEBUILT](https://en.wikipedia.org/wiki/Homebuilt_aircraft) - Homebuilt aircraft can vary significantly from one another. The specs depend on the person building the aircraft. I'm going to drop all 'HOMEBUILT' aircraft since they aren't built by a specific aircraft manufacturer. \n4. MINX - Limited information available on this manufacturer.\n5. BONSALL DC - Limited information available on this manufacturer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### MICCO - change origin to US","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[137, ['National Origin']] = df.loc[137, ['National Origin']].replace(np.nan, 'United States')\ndf.loc[137]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### STAUDACHER - change origin to US","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[711, ['National Origin']] = df.loc[137, ['National Origin']].replace(np.nan, 'United States')\ndf.loc[711]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### HOMEBUILT - drop all","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confirm that all 4 were dropped\n\nprint(len(df.loc[df['Make'] == 'HOMEBUILT']))\ndf = df.drop(df.loc[df['Make'] == 'HOMEBUILT'].index)\nprint(len(df.loc[df['Make'] == 'HOMEBUILT']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### MINX & BONSALL DC - drop both","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is only 1 listing for each manufacturer. We can go ahead and drop both as not having them shouldn't \n# notably impact our dataset. \n\nprint(len(df.loc[df['Make'] == 'MINX']))\nprint(len(df.loc[df['Make'] == 'BONSALL DC']))\ndf = df.drop(df.loc[df['Make'] == 'MINX'].index)\ndf = df.drop(df.loc[df['Make'] == 'BONSALL DC'].index)\nprint(len(df.loc[df['Make'] == 'MINX']))\nprint(len(df.loc[df['Make'] == 'BONSALL DC']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confirm that the right amount of rows were dropped. \n\nprint('length of dataset before dropping 6 rows: {}'.format(len(df)))\nprint('length of dataset after dropping 6 rows: {}'.format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similar to the other columns, apply the upper method to stay consistent.\n\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['National Origin'].str.isupper().sum()), (len(df))))\ndf['National Origin'] = df['National Origin'].str.upper()\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['National Origin'].str.isupper().sum()), (len(df))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['National Origin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initially when going through this dataset I didn't notice that Switzerland is listed twice above. The first entry is spelt 'Switzerland' and the second 'Swtizerland'. This is obviously an error and we need to correct the spelling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the index value of the row that has the incorrect spelling of Switzerland\n\ndf.loc[df['National Origin'] == 'SWTIZERLAND']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update and confirm the results\n\ndf.loc[186, 'National Origin'] = df.loc[186, 'National Origin'].replace('SWTIZERLAND', 'SWITZERLAND')\ndf['National Origin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# According to our dataset, most aircraft listings are made in the United States.\n\nprint('Aircraft found in the dataset are manufactured in {} different countries.'.format(df['National Origin'].nunique()))\nprint('')\nprint(df['National Origin'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Lastly,  I'm going to rename this column to 'Country of Origin'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns={'National Origin': 'Country of Origin'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Category - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing data \n\ndf['Category'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned previously, this data was scarped from various websites and therefore isn't consistent. I'm going to merge a few of the categories because they are duplicates. \n\n1. Single Engine Piston = Single Piston \n2. Multi Engine Piston = Twin Piston\n3. Turboprop = Turboprops","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Category'] = np.where((df['Category'] == 'Single Piston'), 'Single Engine Piston', df['Category'])\ndf['Category'] = np.where((df['Category'] == 'Twin Piston'), 'Multi Engine Piston', df['Category'])\ndf['Category'] = np.where((df['Category'] == 'Turboprops'), 'Turboprop', df['Category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amend 'Gliders | Sailplanes' to 'Gliders/Sailplanes' to stay consistent with the 'Military/Classic/Vintage' name format.\n\ndf['Category'] = np.where((df['Category'] == 'Gliders | Sailplanes'), 'Gliders/Sailplanes', df['Category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensure the changes have been applied\n\ndf['Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similar to the rest of the columns, apply the upper method to stay consistent.\n\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Category'].str.isupper().sum()), (len(df))))\ndf['Category'] = df['Category'].str.upper()\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Category'].str.isupper().sum()), (len(df))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see what the dataframe looks like after making various changes to the Category, \n# Make, Model, and Country of Origin columns\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Year - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing data in this column\n\ndf['Year'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The data is for 94 different years\ndf['Year'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Year'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look at the 'Not listed' and '-' entries. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} rows that have 'Not listed' entered in the year column.\".format(len(df[df['Year'] == 'Not Listed'])))\nprint('')\ndf[df['Year'] == 'Not Listed'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} rows that have '-' entered in the year column.\".format(len(df[df['Year'] == '-'])))\nprint('')\ndf[df['Year'] == '-'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the columns that don't have an actual year. Total rows to drop = 34 + 44 = 78 \n\nprint('Length of dataset prior to dropping rows with missing data: {}'.format(len(df)))\ndf = df.drop(df.loc[df['Year'] == 'Not Listed'].index)\ndf = df.drop(df.loc[df['Year'] == '-'].index)\nprint('Length of dataset after dropping rows with missing data: {}'.format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensure the right amount of rows were dropped\n\n2524-2446","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looks good.\n\ndf['Year'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last step is to convert the 'Year' column to type = integer\n\ndf['Year'] = df['Year'].astype(np.int64)\ndf['Year'].dtype","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Total Hours - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Total Hours'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Total Hours'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### First, address the 'NaN', '-', and '0' hours. Convert 'NaN' and '-' to 0 hours.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df.loc[df['Total Hours'].isnull()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of NaN: {} and '-': {} BEFORE converting to 0.\".format((len(df.loc[df['Total Hours'].isnull()])), len(df.loc[df['Total Hours'] == '-'])))\n\ndf['Total Hours'] = np.where((df['Total Hours'] == '-'), 0, df['Total Hours'])\ndf['Total Hours'] = np.where((df['Total Hours'].isnull()), 0, df['Total Hours'])\ndf['Total Hours'] = np.where((df['Total Hours'] == '0'), 0, df['Total Hours'])\n\nprint(\"Total number of NaN: {} and '-': {} AFTER converting to 0.\".format((len(df.loc[df['Total Hours'].isnull()])), len(df.loc[df['Total Hours'] == '-'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The data is very messy. There are letters, commas, colons, and periods within the data. Let's clean it up.\n\ndf['Total Hours'].value_counts()[-20:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's filter for rows that aren't pure digits. \n\ndf_messy_hours = df.loc[~df['Total Hours'].astype(str).str.isdigit()]\nprint(len(df_messy_hours))\ndf_messy_hours","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Search for rows that have 'h' or 'H' within them.\n\ncontains_h = df_messy_hours[df_messy_hours['Total Hours'].str.contains('h')]\nprint(len(contains_h))\ncontains_h.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contains_H = df_messy_hours[df_messy_hours['Total Hours'].str.contains('H')]\nprint(len(contains_H))\ncontains_H.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all letters from these rows so that only numbers remain\n\ncontains_h['Total Hours'] = contains_h['Total Hours'].astype(str).str.replace('[^0-9]', '')\ncontains_H['Total Hours'] = contains_H['Total Hours'].astype(str).str.replace('[^0-9]', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop row 1928 since it already exists in the 'contains_h' subset\n\ncontains_H.drop(1928, inplace=True)\nlen(contains_H)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contains_H","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After dropping the letters, some numbers look too big. For example row 2397 - 1883145 hours.\nBefore dropping the letters, the row stated: \"1883tt 145 since O/H\". We'll have to review the total hours before/after dropping the letters to make sure that the correct number of hours is reflected in the 'Total Hours' column.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amending total hours to correct number.\n\ncontains_H.loc[2325, 'Total Hours'] = contains_H.loc[2325, 'Total Hours'].replace('122200', 'NaN')\ncontains_H.loc[2372, 'Total Hours'] = contains_H.loc[2372, 'Total Hours'].replace('8207', '821')\ncontains_H.loc[2397, 'Total Hours'] = contains_H.loc[2397, 'Total Hours'].replace('1883145', 'NaN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update our df with the correct values from above. \n\ndf.update(contains_h)\ndf.update(contains_H)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[~df['Total Hours'].astype(str).str.isdigit()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter rows that aren't pure digits again\n\nmessy_hours2 = df.loc[~df['Total Hours'].astype(str).str.isdigit()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_letters = messy_hours2[pd.to_numeric(messy_hours2['Total Hours'], errors='coerce').notnull()]\nprint(len(no_letters))\nno_letters[30:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I reviewed the Total Hours column above to ensure that when each value is converted to a float data type, they'll be converted correctly. \n\nExample - row 1929 - '1.978' would be converted to 2 hours (1.978 rounds to 2), instead of 2000. Because of this I had to manually convert this to '1978'. See this and other manual changes I had to make in the cell below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"no_letters.loc[1929, 'Total Hours'] = no_letters.loc[1929, 'Total Hours'].replace('1.978', '1978')\nno_letters.loc[2023, 'Total Hours'] = no_letters.loc[2023, 'Total Hours'].replace('5.198', '5198')\nno_letters.loc[2093, 'Total Hours'] = no_letters.loc[2093, 'Total Hours'].replace('5.74', '574')\nno_letters.loc[2115, 'Total Hours'] = no_letters.loc[2115, 'Total Hours'].replace('5.497', '5497')\nno_letters.loc[2127, 'Total Hours'] = no_letters.loc[2127, 'Total Hours'].replace('5.615', '5615')\nno_letters.loc[2230, 'Total Hours'] = no_letters.loc[2230, 'Total Hours'].replace('1.06', '106')\nno_letters.loc[2331, 'Total Hours'] = no_letters.loc[2331, 'Total Hours'].replace('10.72', '1072')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the numbers to integers\n\nno_letters['Total Hours'] = no_letters['Total Hours'].astype(float).round().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update the dataframe with the updated rows from the 'no_letters' df. \n\ndf.update(no_letters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We currently don't have any values that are null in the 'Total Hours' column\n\ndf['Total Hours'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to integers\n\ndf['Total Hours'] = pd.to_numeric(df['Total Hours'], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the interest of time, I'm not going to look into the rest of the 'Total Hours' rows that aren't pure digits. I will convert any value that can be converted to an integer, while the remaining rows will be converted to np.nan format. I will drop the NaN rows and continue cleaning the rest of the data. \n\nSince this is just for fun/practice I don't want to spend too much time on this one column especially since the amount of rows that will be dropped is around 100, which will leave more than 2000 to work with. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# After the integer conversion, 109 rows will need to be dropped.\n\nprint(\"There are {} rows with NaN that need to be dropped\". format(df['Total Hours'].isnull().sum()))\ndf.dropna(subset=['Total Hours'], inplace=True)\nprint(\"There are {} rows with NaN remaining in the 'Total Hours' column\". format(df['Total Hours'].isnull().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Condition - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Condition'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are lots of NaN values in the Condition column. This should be an easy fix if we make a few assumptions.\n\n1. Any aircraft that has 0 Total Hours and manufactured between 2018-2020 will be considered a new aircraft.  \n2. Aircraft with Total Hours > 0 but a NaN value in the Condition column will be considered Used. \n\nLet's begin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's filter for aircraft with 0 Total Hours, manufactured between 2018-2020 and don't have a listed condition. \n# These should all be listed as New according to the assumptions above. \n\nprint(len(df.loc[(df['Total Hours'] == 0) & (df['Year'] >= 2018) & (df['Condition'].isnull())]))\ndf['Condition'] = np.where((df['Total Hours'] == 0) & (df['Year'] >= 2018) & (df['Condition'].isnull()) , 'New', df['Condition'])\nprint(len(df.loc[(df['Total Hours'] == 0) & (df['Year'] >= 2018) & (df['Condition'].isnull())]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 12 rows were updated, 600 remain. \n\ndf['Condition'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter for aircraft that are listed as having Total Hours greater than 0 and the condition listed as NaN.\n# Change these all to Used\n\nprint(len(df.loc[(df['Total Hours'] != 0) & (df['Condition'].isnull())]))\ndf['Condition'] = np.where((df['Total Hours'] != 0) & (df['Condition'].isnull()) , 'Used', df['Condition'])\nprint(len(df.loc[(df['Total Hours'] != 0) & (df['Condition'].isnull())]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 575 rows were updated, 25 remain. \n\ndf['Condition'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the remaining listings\n\ndf[df['Condition'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to drop the 25 listings above so that we have more accurate data to work with. I believe that some or even most of the aircraft above have been rebuilt, hence are showing 0 Total Hours. I believe these are anomolies and don't reflect the majority of the data in the dataset.\n\nAdditionally, I'll drop aircraft stated as Used but have 0 Total Hours.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Length of dataset prior to dropping NaN values from the Condition column: {}'.format(len(df)))\ndf.dropna(subset=['Condition'], inplace=True)\nprint('Length of dataset after dropping NaN values from the Condition column: {}'.format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Condition'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop used aircraft with 0 total hours:\n\nlen(df[(df['Total Hours'] == 0) & (df['Condition'] == 'Used')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Length of dataset prior to dropping NaN values from the Condition column: {}'.format(len(df)))\ndf = df.drop(df[(df['Total Hours'] == 0) & (df['Condition'] == 'Used')].index)\nprint('Length of dataset after dropping NaN values from the Condition column: {}'.format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensure the correct amount of rows were dropped.\n\n2312-2241","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Project aircraft, similar to homebuilt aircraft mentioned above can vary widely and there isn't sufficient data\n# to carry out a meaningful analysis. But for the sake of curiousity I'll leave this for now. \n\ndf[df['Condition'] == 'Project'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looks good.\n\ndf['Condition'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One more time - apply the upper method on the entire column.\n\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Condition'].str.isupper().sum()), (len(df))))\ndf['Condition'] = df['Condition'].str.upper()\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Condition'].str.isupper().sum()), (len(df))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Price & Currency - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The data is a little messy.\n\ndf['Price'].value_counts().tail(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing data in the Price column. Let's check the currency column.\n\ndf['Price'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 415 NaN values in the Currency column\n\ndf['Currency'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are 5 different currencies within our dataset.\n\ndf['Currency'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Currency'].isnull()].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the data didn't transfer correctly from the csv input file. \n\nA few key points:\n\n1. Rows that only have an integer value in the Price column and no matching Currency are USD\n2. Rows that have the following symbol - 'â‚¬' - are Euros. \n3. Rows that have the following symbol - 'Â£' - are GBP. \n\nI'm going to start by filtering by the different currencies and only extracting the integers from the each row. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Currency'] == 'EUR'].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Currency'] == 'GBP'].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a variable for EUR\n\nprice_eur = df[df['Price'].str.contains(r'â‚¬', flags=re.IGNORECASE, regex=True, na=False)]\nprice_eur","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update the Currency column for the listing below\n\nprice_eur[price_eur['Currency'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[2361, ['Currency']] = df.loc[2361, ['Currency']].replace(np.nan, 'EUR')\ndf.loc[2361, 'Currency']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can now go ahead and remove the unwanted characters within the Price column for Euros\n\nprice_eur['Price'] = price_eur['Price'].str.replace('Price:', '')\nprice_eur['Price'] = price_eur['Price'].str.replace('â‚¬', '')\nprint(len(price_eur))\nprice_eur","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.update(price_eur)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's repeat the steps above for GBP now. \n\nprice_gbp = df[df['Price'].str.contains(r'£', flags=re.IGNORECASE, regex=True, na=False)]\nprint(len(price_gbp))\nprice_gbp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing values for Currency = GBP\n\nprice_gbp[price_gbp['Currency'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all unwaned characters within the GBP rows in the Price column\n\nprice_gbp['Price'] = price_gbp['Price'].str.replace('Price: ', '')\nprice_gbp['Price'] = price_gbp['Price'].str.replace('Â£', '')\nprint(len(price_gbp))\nprice_gbp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.update(price_gbp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lastly, follow the steps above for USD\n\nprice_usd = df[df['Price'].str.contains(r'USD', flags=re.IGNORECASE, regex=True, na=False)]\nprint(len(price_usd))\nprice_usd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace the NaN rows in the Currency column with 'USD'\n\nprice_usd['Currency'] = price_usd['Currency'].replace(np.nan, 'USD')\nprice_usd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all unwanted characters\n\nprice_usd['Price'] = price_usd['Price'].str.replace('Price: USD', '')\nprice_usd['Price'] = price_usd['Price'].str.replace('$', '')\nprice_usd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update the main df\n\ndf.update(price_usd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only 1 NaN remains. \n# Drop row with index 2260 since it doesn't have a price, nor currency.\n\nprint(df['Currency'].isnull().sum())\ndf[df['Currency'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Length of dataset before dropping row at index 2260: {}'.format(len(df)))\ndf.drop(2260, inplace=True)\nprint('Length of dataset before dropping row at index 2260: {}'.format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Currency'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Currency'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's quickly take a look at the other currencies\n# CAD looks good\n\ndf[df['Currency'] == 'CAD']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's quickly take a look at the other currencies\n# CHF looks good\n\ndf[df['Currency'] == 'CHF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Price'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lastly, drop unwanted characters such as the '$' symbol from the entire Price column.\n\ndf['Price'] = df['Price'].str.replace('$', '')\ndf['Price'] = df['Price'].str.replace(' ', '')\ndf['Price'] = df['Price'].str.replace(',', '')\n\ndf['Price'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price column looks good now, we can move on. \n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Location - column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This column was definitely the most time consuming to clean up. I had to apply a lot of manual changes. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 11 missing rows in this column\n\ndf['Location'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Location'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the 11 rows with NaN values in the Location column.\n\nprint('Length of dataset before dropping NaN values from the Location column: {}'. format(len(df)))\ndf.dropna(subset=['Location'], inplace=True)\nprint('Length of dataset before dropping NaN values from the Location column: {}'. format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove blank spaces from Location column\n\ndf.Location = df.Location.str.replace(' ', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I manually kept adding countries to this list as I kept looking through the Location column. \n\ncountry_list = ['UnitedKingdom','Monaco', 'United Kingdom', 'USA', 'Canada', 'Luxembourg', 'Germany', 'Austria',\n                    'Monaco', 'Poland', 'Belgium', 'Russian Federation', 'Netherlands', 'Sweden',\n                    'Norway', 'Switzerland', 'France', 'Spain','Denmark', 'Lithuania', 'Turkey', 'Italy', \n                'Iceland', 'SouthAfrica', 'UnitedStates', 'CzechRepublic', 'NewZealand', 'Brazil', 'Australia', \n                'Bulgaria', 'CostaRica', 'RussianFederation', 'Chile', 'Nigeria', 'Pakistan', 'Indonesia', \n                'Venezuela', 'Malaysia', 'Congo', 'NewGuinea', 'UnitedArabEmirates', 'Singapore', 'CAN', 'POL',\n                'DEU', 'FRA', 'ITA', 'ZAF', 'AUS', 'ARG', 'SRB', 'CZE', 'NLD', 'MEX', 'ESP', 'AUS', 'URY', 'KEN', 'CHE']\n\npattern = '|'.join(country_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to search through the Location column and extract country names. A Country column is created with the \n# individual Country names\n\ndef pattern_search(search_str:str, search_list:str):\n\n    search_obj = re.search(search_list, search_str)\n    if search_obj :\n        return_str = search_str[search_obj.start(): search_obj.end()]\n    else:\n        return_str = np.nan\n    return return_str\n\ndf['Country'] = df['Location'].astype(str).apply(lambda x: pattern_search(search_str=x, search_list=pattern))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In the interest of time I'm going to completely drop these rows.\n\ndf.dropna(subset=['Country'], inplace=True)\ndf['Country'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some of the countries are repeated. I'll have to manually update these. \n\ndf['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manual changes to ensure that countries aren't double counted and have a consistent format.\n\ndf['Country'] = np.where((df['Country'] == 'FRA'), 'France', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'MEX'), 'Mexico', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'URY'), 'Uruguay', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'KEN'), 'Kenya', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'ITA'), 'Italy', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'ESP'), 'Spain', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'NLD'), 'Netherlands', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'CZE'), 'Czech Republic', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'SRB'), 'Serbia', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'ARG'), 'Argentina', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'CzechRepublic'), 'Czech Republic', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'CostaRica'), 'Costa Rica', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'UnitedArabEmirates'), 'United Arab Emirates', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'RussianFederation'), 'Russia', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'POL'), 'Poland', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'DEU'), 'Germany', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'AUS'), 'Australia', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'SouthAfrica'), 'South Africa', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'ZAF'), 'South Africa', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'UnitedKingdom'), 'United Kingdom', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'CHE'), 'Switzerland', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'CAN'), 'Canada', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'NewGuinea'), 'New Guinea', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'United States'), 'USA', df['Country'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Double check the data. \n\ndf[df['Country'] == 'Canada']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like the Country values aren't correct. Using row 2508 as an example. The Country column indicates Canada, but it should actually be the United States, according to the Location column. This is because the Search Function takes the first country name from the 'country_list' and matches it to the first country it comes across in the Location column.\n\nWe'll need to look at each country individually to ensure that our data is 100% accurate. Let's do that, but first I'll add 'US - State' column. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy/Paste from https://gist.github.com/JeffPaine/3083347\n\nstates = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n\npattern = '|'.join(states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['US - State'] = df['Location'].astype(str).apply(lambda x: pattern_search(search_str=x, search_list=pattern))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manual changes to correct the Country column\n\ndf['Country'] = np.where((df['Country'] == 'Canada') & (df['Location'] == 'NorthAmerica+Canada,Mexico'), 'Mexico', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'Canada') & (df['Location'] == 'NorthAmerica+Canada,UnitedStates'), 'USA', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'Canada') & (df['Location'] == 'NorthAmerica+Canada,Canada'), 'Canada', df['Country'])\ndf['Country'] = np.where((df['Country'] == 'Canada') & (df['US - State'] != 'CA') & (df['US - State'].notnull()), 'USA', df['Country'])\ndf['US - State'] = np.where((df['Country'] == 'Canada') & (df['US - State'].notnull()), np.nan, df['US - State'])\ndf['Country'] = np.where((df['Country'] == 'Canada') & (df['Location'] == 'NorthAmerica+Canada,UnitedStates-CA'), 'USA', df['Country'])\ndf['US - State'] = np.where((df['Country'] == 'USA') & (df['Location'] == 'NorthAmerica+Canada,UnitedStates-CA'), 'CA', df['US - State'])\ndf['US - State'] = np.where((df['Country'] != 'USA') & (df['US - State'].notnull()), np.nan, df['US - State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# While cross checking each country using the code below (I appplied the filter below for all countries, one at a time), \n# I noticed that Uruguay was incorrectly entered as the Country for some of the listings. \n\ndf[(df['Country'] == 'Uruguay')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amendments to correct Uruguay:\n\ndf['Country'] = np.where((df['Country'] == 'Uruguay') & (df['Location'] == 'HAWKESBURY,\\n\\tON\\n\\tCAN'), 'Canada', df['Country']) \ndf['Country'] = np.where((df['Country'] == 'Uruguay') & (df['Location'] == 'HAWKESBURY,\\n\\tQC\\n\\tCAN'), 'Canada', df['Country']) \ndf['Country'] = np.where((df['Country'] == 'Uruguay') & (df['Location'] == 'HAWKESBURY\\n\\t\\n\\tUSA'), 'USA', df['Country'])\ndf[(df['Country'] == 'Uruguay')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Australia also has several errors. See cell below for amendments. \n\ndf[(df['Country'] == 'Australia')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Australia amendments\n\ndf['Country'] = np.where((df['Country'] == 'Australia') & (df['Location'] == 'Australia&NZ,NewZealand'), 'New Zealand', df['Country']) \ndf['Country'] = np.where((df['Country'] == 'Australia') & (df['Location'] == 'AUSTIN,\\n\\tTX\\n\\tUSA'), 'USA', df['Country']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['US - State'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['US - State'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last step - Applying upper method\n\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Country'].str.isupper().sum()), (len(df))))\ndf['Country'] = df['Country'].str.upper()\nprint('There are a total of {}/{} uppercase rows in this column'.format((df['Country'].str.isupper().sum()), (len(df))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA CLEANING - FINAL STEPS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few last steps:\n\n1. Drop the Location column because we've created a Country and US - State columns\n2. Drop Engine 1 Hours, Engine 2 Hours, Prop 1 Hours, Prop 2 Hours, Total Seats, Flight Rules columns since there is too much missing data. I might decide to add them back later. But, I won't be using those columns for the initial analysis.  \n3. Drop S/N and REG columns. These columns won't be very useful in analyzing aircraft prices. The Serial and Registration numbers are unique to each aircraft and have no significance on the price. These numbers are similar to a automobiles license plate, which doesn't have any impact on the price. \n4. Rearrange and rename certain columns \n5. Convert the data into correct data types. For example, the Price and Total Hours columns should be Dtype = integer.\n6. Convert Currency to USD to make it easier to work with the data\n7. Create output csv file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop - Location column\n\nlocation_df = df['Location']\ndf.drop(['Location'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop - Engine 1 Hours, Engine 2 Hours, Prop 1 Hours, Prop 2 Hours, Total Seats, Flight Rules\n\nunused_columns = df[['Engine 1 Hours', 'Engine 2 Hours', 'Prop 1 Hours', 'Prop 2 Hours', 'Total Seats', 'Flight Rules']]\ndf.drop(['Engine 1 Hours', 'Engine 2 Hours', 'Prop 1 Hours', 'Prop 2 Hours', 'Total Seats', 'Flight Rules'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop S/N and REG columns\n\nsn_reg = df[['S/N', 'REG']]\ndf.drop(['S/N', 'REG'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename and Rearrange the columns\n\ndf = df.rename(columns={'Country': 'Location - Country', 'US - State': 'Location - US State'})\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearrange the columns\n\nrearrange_columns = df.columns.tolist()\nrearrange_columns = [\n 'Condition',\n 'Category',\n 'Year',\n 'Make',\n 'Model',\n 'Country of Origin',\n 'Total Hours',\n 'Location - Country',\n 'Location - US State',\n 'Price',\n 'Currency', \n ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[rearrange_columns]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert columns to correct data types\n\ndf['Total Hours'] = df['Total Hours'].astype(np.int64)\ndf['Price'] = df['Price'].astype(np.int64)\ndf['Year'] = pd.to_datetime(df['Year'], format='%Y').dt.year\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert the Price column to USD. I will use the July 1st close rate.\n\n- [1 USD = 0.8888 EUR](https://www.poundsterlinglive.com/best-exchange-rates/us-dollar-to-euro-exchange-rate-on-2020-07-01)\n- [1 USD = 0.8023 GBP](https://www.poundsterlinglive.com/best-exchange-rates/us-dollar-to-british-pound-exchange-rate-on-2020-07-01)\n- [1 USD = 1.3591 CAD](https://www.poundsterlinglive.com/best-exchange-rates/us-dollar-to-canadian-dollar-exchange-rate-on-2020-07-01)\n- [1 USD = 0.9458 CHF](https://www.poundsterlinglive.com/best-exchange-rates/us-dollar-to-swiss-franc-exchange-rate-on-2020-07-01)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert all Prices to USD\n\ndf.loc[df['Currency'] == 'EUR', 'Price'] = df.loc[df['Currency'] == 'EUR', 'Price']*(1/0.8888)\ndf.loc[df['Currency'] == 'GBP', 'Price'] = df.loc[df['Currency'] == 'GBP', 'Price']*(1/0.8023)\ndf.loc[df['Currency'] == 'CAD', 'Price'] = df.loc[df['Currency'] == 'CAD', 'Price']*(1/1.3591)\ndf.loc[df['Currency'] == 'CHF', 'Price'] = df.loc[df['Currency'] == 'CHF', 'Price']*(1/0.9458)\ndf['Price'] = df['Price'].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can now drop the Currency column since all of prices are in USD\n\ndf.drop(['Currency'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean Dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Last Step! \n\nCreate a csv output of the clean dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('clean_aircraft_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank you for going through this notebook!\n\nFeel free to leave any questions or comments below.  ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}