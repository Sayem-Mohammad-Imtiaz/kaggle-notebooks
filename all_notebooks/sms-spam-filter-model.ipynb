{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SMS Spam Filtering using NLP\n\n* Agenda of this notebook is to create a model that classify messages as spam or ham. The model should have acceptable accuracy and presision to do the classification.\n\n* I have used RandomForest classifier here","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets load some basic Libraries and start the engine xD**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re\nimport nltk\nnltk.download('stopwords')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now lets see how the data looks**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sms=pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\",encoding='latin-1')\nprint(sms.shape)\nsms.head(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as we can see above how the data looks and to exactly specify. There are 5572 messages.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here I have dropped the columns with all NaN, makes no sense to keep that because it gives us 0 insights.\n\nAlso naming the columns v1,v2 as Label and messages respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\nsms.columns=['Label','Messages']\nsms.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sms.isnull().sum())\nprint(sms.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not the way I expected to be honest. Quite clean xD","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['Messages'].head(15)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets see how mant ham and spam messages are there in the dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'ham= {len(sms[sms[\"Label\"] == \"ham\"])}')\nprint(f'spam= {len(sms[sms[\"Label\"] == \"spam\"])}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 4825 ham messages and 747 spam messages.\n\n**Lets see the same with some visualization**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax1=plt.subplots(figsize=(7,5))\nsns.countplot(x=\"Label\",data=sms)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Lets check some attibutes or insights from the data. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Creating some additionals features that may help the model to understand better. Like: **Text length** and **punctuations used**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['Text_len']= sms['Messages'].apply(lambda x:len(x))\nsms.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets see the distribution of spam and ham message's length**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (28, 7))\n\nsns.distplot(sms[sms[\"Label\"] == \"spam\"][\"Text_len\"], bins = 50, ax = ax[0],rug=True,kde_kws={\"color\": \"r\"},rug_kws={\"color\": \"g\"})\nax[0].set_xlabel(\"Spam Message Word Length\")\n\nsns.distplot(sms[sms[\"Label\"] == \"ham\"][\"Text_len\"], bins = 100, ax = ax[1],rug=True,kde_kws={\"color\": \"r\"},rug_kws={\"color\": \"g\"})\nax[1].set_xlabel(\"Ham Message Word Length\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spam:** Here we can see the spread little uniform when compared to ham messages. Generally spam message's length is around 110-160/170\n\n**Ham:** Its totally right skewed distribution. like 70% ham message's length is around >0 and <180.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets count the number of punctuations used, instead lets get punctuation % easier to understand**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ndef punc_count(text):\n    count_punc=sum([1 for c in text if c in string.punctuation])\n    return 100*count_punc/len(text)\n\nsms['punc_%']= sms['Messages'].apply(lambda x:punc_count(x))\nsms.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets get to text cleaning part. Here I will remove the puntuations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#I am creating a function that will iterate over all the characters in the texts and search for punctuation as mentioned in \"string.punctuation\"\n\ndef remove_punctuation(text):\n    text_nopunc=\"\".join([c for c in text if c not in string.punctuation])\n    return text_nopunc\n\n#Reason why I used join is, while I ran it I gave me\",\" between every letters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['clean_text']=sms['Messages'].apply(lambda x:remove_punctuation(x))\nsms.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As, we can see the above dataset all the puctuations are removed properly in the coloumn name \"clean_text\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now lets divide the messages in tokens. i.e single words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text):\n    tokens=re.split('\\W+',text)#W here stands for non-word and \"w\" stands for word, it will spilt on non-word\n    return tokens\n\nsms['text_tokens']=sms['clean_text'].apply(lambda x:tokenize(x.lower())) #x.lower to tell python that uppercase and lowercase with spellings are same words\n\nsms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I am now moving to remove the stopwords from the tokenised sentence**. \n\nSo stopwords are the words of a language which doesn't contribute much to the meaning of a sentence. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopword= nltk.corpus.stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    text_no_sw= [word for word in text if word not in stopword]\n    return text_no_sw\n\nsms['text_clean']=sms['text_tokens'].apply(lambda x:remove_stopwords(x))\nsms.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets move on to stemming and in particular I am using Porter Stemmer**. \n\nStemming is a process of reducing a word to its original form. like converting a word's plural or tense form to the original form. Like reducing the branches of a tree to just its stem, thus named stemming.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nps=nltk.PorterStemmer()\nprint(ps.stem('cats'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stemming(text_clean):\n    stemmed=[ps.stem(word)for word in text_clean]\n    return stemmed\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms['text_stemmed']=sms['text_clean'].apply(lambda x:stemming(x))\nsms.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorisation\n\n\nThe textual data after processing needs to be fed into the model. Since the model doesn't accept textual data and only understands numbers, this data needs to be vectorized i.e. transforming text into a meaningful vector (or array) of numbers.\n\nTo convert string data into numerical data one can use following methods\n\nÂ· Bag of words\n\nÂ· TFIDF\n\nÂ· Word2Vec\n\n\n**WE ARE GOING TO USE Tfidf TODAY**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf= TfidfVectorizer()\nX_tfidf = tfidf.fit_transform(sms['Messages'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.concat([sms['Text_len'], sms['punc_%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\nX.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above is the vectorized form of every messages and it is now readt to be fed to the ML model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=sms['Label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now divide the dataset into test and train dataset, I am keeping the test dataset size to 30% of the original size**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below I have imported the Random Forest classifier model and did fit and predict**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_jobs=-1) #n_jobs=-1 means the all processors CPU jobs will be running concurrently. \nrf.fit(X_train,Y_train)\nY_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(Y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(Y_test,Y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like we got a good accuracy using a simple Classifier, we got an accuracy of 97 %.\n\n**Steps to make the accuracy better:**\n\n* By using n-grams try identifying uni-gram,bi-gram and tri-gram and make the into one word like Thank You.\n\n* You can also try hyperparameter tuning. try RandomizedSearch CV or GridSearch CV.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}