{"cells":[{"metadata":{},"cell_type":"markdown","source":"\nThis is my first experience with Data Visualization & machine Learning. I come from totally non-coding background, so my insights & the way to intrepret the things might be different from the one with good machine learning background. Your insights & feedback are welcome.\n\n**Credit Card Fraud Detection**"},{"metadata":{},"cell_type":"markdown","source":"Importing the important Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn as sk\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc=pd.read_csv('../input/credit-card-fraud-detection/creditcard.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysing the DataSet"},{"metadata":{},"cell_type":"markdown","source":"The data set has 31 columns & 281807 rows. This set doesnt have any null values.\nWe do not have column labels except for the amount column so we do not know what the other columns signify.\n\n#### This is  unsupervised learning problem\n\n##### Our goal is to identify if the transaction is fraud or not, the class column becomes dependent variable, our Y variable.\n\n##### This is classification problem, we will use classification algorithms to build the model.\n\n\n## Lets  Begin with exploring the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analysing the dependent or Y variable\n\ncc['Class'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Non fraudulent transactions:{round(cc['Class'].value_counts()[0]/len(cc['Class'])*100,2)}%\")\nprint(f\" fraudulent transactions:{round(cc['Class'].value_counts()[1]/len(cc['Class'])*100,2)}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see most of the transactions (99%) are Non Fraud & only 1% are Fraudulent transactions. This means this is largely imbalanced dataset. \n\nIf we use this dataset to build the model we might get lot of errors & we might overfit since it will assume most of the transaction Non fraudulent.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\n#Lets analyse the 'Amount' column\nsns.distplot(cc['Amount'],ax=ax[0],color='r')\nax[0].set_title('Distribution of amount')\n\n#now seeing the time distribution\nsns.distplot(cc['Time'],ax=ax[1],color='violet')\nax[1].set_title('Distribution of Time')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that above distributions are skewed & we need use the techniques to reduce the skewness. We will see how to normalise the data in later stages.\n\n### Dealing with Imbalanced Data\n\nAs we see the data is hugely imbalance, this might lead to overfitting problem. To make out model work accurately we will need to balance the fraudulent & non- fruadulent transactions which means we need to have equal amount of both classes.\n\nTo balance the data, we will take the sub-sample of both fraudulent & non-fraudulent transactions & try to build the prediction models. Since we have only 492 fradulent transactions, we will randomply pick 492 non-fraudulent transactions to create a balanced sub-sample #or dataframe\n\n\n\n### Scaling\nIf we observe all the other variables are scaled except the Amount & time labels, we will first scale these variables using Standard Scalar\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstd_slr= StandardScaler()\ncc['sld_amt']=std_slr.fit_transform(cc['Amount'].values.reshape(-1,1))\ncc['sld_time']=std_slr.fit_transform(cc['Time'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the old columns\ncc.drop(['Amount','Time'],axis=1, inplace=True)\ncc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have scaled all the labels.\n\nNext we have to look into imbalance data before we start building the model. We need to have equal Fruad & Non-fruad transactions. In order to achieve this, we will have to use Random sampling techniques & scale the dataset to get the balanced dataset of Y variable classes.This will give us new dataset.\n\nRemember, we have to test our model on the original dataset not on new dataset achieved through sampling.\n\n### First lets split the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=cc.drop('Class',axis=1)\ny=cc['Class']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initially lets identify how the model built on imbalanced dataset will perform\n\n#Using Naive Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nmodel=GaussianNB()\nmodel.fit(x_train,y_train)\ny_pred=model.predict(x_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(x_train,y_train)\ny_pred=model.predict(x_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\nacc=accuracy_score(y_test,y_pred)\ncm=confusion_matrix(y_test,y_pred)\nprint(\"Accuracy:\",acc)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note that above accuracy score can be misleading though we have got 98% accuracy,since the data is hugely imbalanced.\n\nLike Said earlier, we need to balance the samples. we have 492 fraud transactions and we need only 492 non-fraud transactions.\nWe will perform Random Under Sampling technique to achieve the balanced sample.\n\n### Random Undersampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle the data before creating the sub-sample\n\ncc=cc.sample(frac=1) #this will randomly select all the data from dataset\n\n#extract the fraudulent & non-fraudulent transactions\nfraud_df=cc.loc[cc['Class']==1]\nnonfraud_df=cc.loc[cc['Class']==0][:492] #using slicing method to select the 492 samples\n\n#combine the datasets\nnew_df=pd.concat([fraud_df,nonfraud_df])\nnew_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the distribution of classes\nnew_df['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the correlation between the variables. We need to analyse how these variables are correlated with Y variable.\nnew_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=new_df.corr()\n\nplt.figure(figsize=(18,8))\ncorr[\"Class\"].sort_values(ascending=True)[:-1].plot(kind=\"barh\")\nplt.title(\"Correlation of variables to Class\")\nplt.xlabel(\"Correlation to Class\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nsns.heatmap(corr,annot=False, cmap=\"Blues\")\nplt.title(\"Correlation of Variables with Class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see how the model performs with undersampled data\n\nx=new_df.drop('Class',axis=1)\ny=new_df['Class']\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test1,y_train1,y_test1=train_test_split(x,y,test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Naive Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nmodel=GaussianNB()\nmodel.fit(x_train1,y_train1)\ny_pred1=model.predict(x_test1)\ny_pred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(\"Accuracy:\", metrics.accuracy_score(y_test1,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(x_train1,y_train1)\ny_pred1=model.predict(x_test1)\ny_pred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\nacc=accuracy_score(y_test1,y_pred1)\ncm=confusion_matrix(y_test1,y_pred1)\nprint(\"Accuracy:\",acc)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see the accuracy of the undersampled decreased compared the original data. \n\nThere is a catch, if you observe the we have used undersampled test data to predict our model, this can be misleading too we need test our model on the original data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing on the original Dataset\n\n#Using Naive Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nmodel=GaussianNB()\nmodel.fit(x_train1,y_train1)\ny_pred2=model.predict(x_test)\ny_pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(x_train1,y_train1)\ny_pred2=model.predict(x_test)\ny_pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\nacc=accuracy_score(y_test,y_pred2)\ncm=confusion_matrix(y_test,y_pred2)\nprint(\"Accuracy:\",acc)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have not normalised the amount & the time column(*which I stated will do at later stages;)*), which might also help develop the model with greater accuracy & can help reducing the overfitting problem. I'm not sure though. Can anyone help me with some insights?\n\n\nHope you like my first Notebook. Your comments & feedback are welcome."},{"metadata":{},"cell_type":"markdown","source":"**P.S: I have referred to multiple Notebooks to get started, thanking each one of them.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}