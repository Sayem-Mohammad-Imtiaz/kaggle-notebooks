{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore') #Supress warnings\npd.set_option('display.max_columns',200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Reading and Understanding the Data\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# import the data frame : Read & Understand data\ndf = pd.read_csv('../input/boombikes/day.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for shape of data\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for presence of null values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correcting the column names\ndf.rename(columns={'dteday':'date','yr':'year','mnth':'month','cnt':'count','hum':'humidity'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for datatype\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n    - No null values are observed\n    - Variables are converted into required data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describing the data with numeric data\ndf.describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data check"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the date column from object to date i.e., required format\ndf['date'] = pd.to_datetime(df['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() # re-check for date column dtype","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mapping the categorical values\n    \n    - mnth : month ( 1 to 12)\n    - weathersit : \n        -1: Clear, Few clouds, Partly cloudy, Partly cloudy\n        -2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n        -3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n        -4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"},{"metadata":{"trusted":true},"cell_type":"code","source":"import calendar\ndef season_func(x):\n    if x==1:\n        return ('spring')\n    elif x==2:\n        return ('Summer')\n    elif x==3:\n        return ('fall')\n    elif x==4:\n        return ('winter')\n    \ndef weather(x):\n    if x==1:\n        return ('Clear')\n    elif x==2:\n        return ('Mist')\n    elif x==3:\n        return ('Light Snow')\n    elif x==4:\n        return ('Heavy Rain')\n\ndf['season'] = df['season'].apply(season_func)\ndf['month'] = df['month'].apply(lambda x: calendar.month_name[x])\ndf['weathersit'] = df['weathersit'].apply(weather)\ndf['weekday'] = df['weekday'].apply(lambda x: calendar.day_abbr[x-1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Visualising the Data\n\n**understanding the data**.\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n\nWe'll visualise our data using `matplotlib` and `seaborn`."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Numeric variables\nnum_vars = ['temp','atemp','windspeed','humidity','casual','registered','count']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pair plots for numeric variables\nnum_vars = list(num_vars)\nsns.pairplot(df[num_vars])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continued\nsns.set_style('darkgrid')\nplt.figure(figsize=(14,10))\nfor i in range(1,len(num_vars)):\n    plt.subplot(3,3,i)\n    sns.distplot(df[num_vars[i]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n    - temp is showing good pattern with cnt in terms of linear regression\n    - temp & atemp with respect to all other variables are following almost same pattern\n    - Casual bookings i.e., new users versus registered users and total count appears to have good correlation\n    - Registered users are highly correlated with count."},{"metadata":{},"cell_type":"markdown","source":"#### Regression plot for numerical variables"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,4))\nplt.subplot(1,3,1)\nsns.regplot(data=df,x=\"windspeed\",y=\"count\",color = \"Orange\")\nplt.subplot(1,3,2)\nsns.regplot(data=df,x=\"temp\",y=\"count\",color = \"Seagreen\")\nplt.subplot(1,3,3)\nsns.regplot(data=df,x=\"humidity\",y=\"count\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference:\n    - By the above plot we can observe that correlation between temp and count is good/positive when compared to other variables\n    - Bookings are likely to happen during the days where humidity ~40+ %\n    - Booking are likely to happen during the speed of wind are lower"},{"metadata":{},"cell_type":"markdown","source":"### Data visualization for categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical variables\ncat_vars = list(df.select_dtypes(exclude=['float64','int64','datetime64']))\ncat_vars","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (16,16),ncols=3,nrows=2,sharey=True)\nplt.subplot(331)\nsns.boxplot(data=df,x=\"month\",y=\"count\")\nplt.subplot(3,3,2)\nsns.boxplot(data=df,x=\"season\",y=\"count\")\nplt.subplot(333)\nsns.boxplot(data=df,x=\"year\",y=\"count\")\nplt.subplot(334)\nsns.boxplot(data=df,x=\"holiday\",y=\"count\")\nplt.subplot(335)\nsns.boxplot(data=df,x=\"workingday\",y=\"count\")\nplt.subplot(336)\nsns.boxplot(data=df,x=\"weekday\",y=\"count\")\nplt.subplot(337)\nsns.boxplot(data=df,x=\"weathersit\",y=\"count\")\n;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the distrubution of no of days & booking over season"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='season',y='count',data=df)\nplt.title('Total No. of bookings in different season');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n    - It is more evident from the plot that there is a significant growth in 2019\n    - There is no much variance in number of bookings versus day of the week\n    - When we compare the number of days of holidays v/s booking., number of booking/day is more during holidays.\n    Hence this can be good predictor on number of bookings\n    - Number of bookings are more in season 3:fall\n    - Season is one of the good predictor"},{"metadata":{},"cell_type":"markdown","source":"#### YoY - Monthy Bookings comparision"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.subplot(2,1,1)\nsns.boxplot(data=df,x='month',y='count',hue='year')\nplt.subplot(2,1,2)\nsns.barplot(x='month',y='count',data=df,hue='year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n    - Month on month bookings are significantly increased\n    - Total number of bookings in 2019 are more than 2018 (Where lengend 0 - yr 2018 / 1 - yr 2019)"},{"metadata":{},"cell_type":"markdown","source":"#### Correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = round(df[num_vars].corr(),3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df_corr,cmap='BuGn',annot=True,square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.array(df_corr)\nmask[np.tril_indices_from(mask)]=False\nplt.figure(figsize=(10,10))\nsns.heatmap(df_corr,mask=mask,annot=True,vmax=0.8,square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n    - Temp & atemp are highly correlated amy lead to multicollinearity.\n    - wind and humidity are appearing to be negatively correlated\n    - It is observed heat maps numeric variables temp, atemp, casual, registered are showing positive correlation with count"},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Data Preparation"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Dropping the variables that are not helpful in modeling - Instant(Just acts as name tag/sr no), \n#casual & registered(sum of both is count) are also dependent variables, date(not a predicting factor) since it is an index\n# atemp - which is highly correlated (+0.99) with dependent variable temp\n\ndf1 = df.drop(['date','instant','casual','registered','atemp'],axis=1)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create dummy variables - For categorical vars"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding - Converting cat_vars into dummies\n# Let's drop the first column from status df using 'drop_first = True'\n\nseason_dummies= pd.get_dummies(df1['season'],drop_first=True)\nmonth_dummies = pd.get_dummies(df1['month'],drop_first=True)\nweather_dummies = pd.get_dummies(df1['weathersit'],drop_first=True)\nweekday_dummies = pd.get_dummies(df1['weekday'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the results to the original dataframe\ndf_boom = pd.concat([df1,season_dummies,month_dummies,weather_dummies,weekday_dummies],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_boom.head() # Checking dummy vars","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Dropping the cat_vars which are converted into dummy vars\ndf_boom = df_boom.drop(['season','month','weekday','weathersit'],axis=1)\ndf_boom.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Splitting the Data into Training and Testing Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for linear regression modelling\nimport sklearn\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting of data into train and test with 70:30 ratio\n# We specify this so that the train and test data set always have the same rows, respectively\n\ndf_train, df_test = train_test_split(df_boom, train_size = 0.7, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for shape of train & test data\n\nprint('Shape of train data : {}'.format(df_train.shape))\nprint('Shape of test data : {}'.format(df_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Linear Regression Modelling"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (16, 10))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_table = round(df_train.corr(),3)#['count']#.sort_values(by='count',ascending=False)\ncorr_table[['count']].sort_values(by='count',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference:\n    - As you might have noticed, `temp` seems to the correlated to `count` the most.\n    - We can also observe a pattern during the months `June, July, & Aug` i.e., summer are positively correlated, meanwhile as it goes towards, winter & spring starting to correlate negatively\n"},{"metadata":{},"cell_type":"markdown","source":"#### Scaling & defining of dependent variable and predicting variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling the numeric variables\nscaler = MinMaxScaler()\ndf_train[['temp','humidity','windspeed','count']] = scaler.fit_transform(df_train[['temp','humidity','windspeed','count']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining of predictor and dependent variables\n\ny_train = df_train.pop('count')\nX_train = df_train\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Building our model\n\nThis time, we will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (which is a utility from sklearn)"},{"metadata":{},"cell_type":"markdown","source":"### RFE\nRecursive feature elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_] # column / variable names in top 15 rank\ncol","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_] # Removing the false variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model using statsmodel, for the detailed statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)\nlm1 = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model\nlm1.params","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Let's see the summary of our linear model\nprint(lm1.summary())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_rfe.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_new = X_train_rfe.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****`holiday` is having high p-value `0.118`; can be dropped"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Creating of new X_train data\nX_train_new = X_train_new.drop([\"holiday\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm2 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nlm2.params","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Let's see the summary of our linear model\nprint(lm2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Sun` is having high `p-value` `0.288` ; can be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['Sun'],axis=1)\nX_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm3 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nlm3.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the summary of our linear model\nprint(lm3.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`temp` is having `10.03` so it can be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['temp'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm4 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nlm4.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the summary of our linear model\nprint(lm4.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`winter` is having higher p-value `0.829` so it can be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['winter'],axis=1)\nX_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm5 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nlm5.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm5.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm.drop(['const'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    - `July` is having high p-value `0.265`\t ; so we can drop"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['July'],axis=1)\nX_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm  \nX_train_lm = sm.add_constant(X_train_new)\nlm6 = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nlm6.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm6.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_lm.drop(['const'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    `p-value` of holiday is high > 0.05 i.e., 0.114 and low VIF of 1.04"},{"metadata":{},"cell_type":"markdown","source":"All `p-vales` < 0.05, F-statistics = 3.40e-129 which is negligible, and VIF < 5. Hence we can say the model is stable and can proceed further for residuals analysis and testing"},{"metadata":{},"cell_type":"markdown","source":"## Step 7: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_price = lm6.predict(X_train_lm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    - Error terms are distrubuted normally, we can proceed futher making predictions using final model"},{"metadata":{},"cell_type":"markdown","source":"## Step 8: Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final."},{"metadata":{},"cell_type":"markdown","source":"#### Applying the scaling on the test sets"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[['temp','humidity','windspeed','count']] = scaler.transform(df_test[['temp','humidity','windspeed','count']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Defining dependent and indepedent variables\ny_test = df_test.pop('count')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test derived from model\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_new.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_test_pred = lm6.predict(X_test_new)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 9: Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_test_pred)\nfig.suptitle('y_test vs y_test_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_test_pred', fontsize=16)                          # Y-label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R-Squared value for test set\nr2 = round(r2_score(y_test, y_test_pred),4)\nr2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    - Observed R-Squared value for train & test data set is 74 % and 79.77 % respectively. The difference is approximately 5 % which is acceptable in small difference in variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    - Lower the mean_squared_error more the stability of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    - Lower the mean_absolute_error more the stability of the model"},{"metadata":{},"cell_type":"markdown","source":"### Equation for `bestfitline` as follows:\ncount = 0.611045 + 0.232469 X year + 0.033295 X workingday -0.168256 X humidity X  -0.183157 X windspeed X  -0.286873 X spring -0.102989 X December -0.131633 X November + 0.083101 X September -0.170055 X Light Snow + 0.039602 X Sat\n    \n    - Demand increases with increase in parameters of Year, workingday & September i.e, summer season, Sat i.e, weekend\n    - Demand decreases with increase in humidity, windspeed, spring, December and Light snow "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}