{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/advertising-dataset/advertising.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.Linear regression","metadata":{}},{"cell_type":"markdown","source":"#### Feature selection","metadata":{}},{"cell_type":"code","source":"X = df.drop('Sales',axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['Sales']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear regression model creation","metadata":{}},{"cell_type":"code","source":"#from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predicting values the model has never seen before","metadata":{}},{"cell_type":"code","source":"test_predictions = model.predict(X_test)\ntest_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.Let's test the model accuracy ","metadata":{}},{"cell_type":"markdown","source":"#### MAE","metadata":{}},{"cell_type":"code","source":"mean_absolute_error(y_test,test_predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RMSE","metadata":{}},{"cell_type":"code","source":"np.sqrt(mean_squared_error(y_test,test_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### MAPE","metadata":{}},{"cell_type":"code","source":"def mape(actual,pred):\n    actual, pred = np.array(actual), np.array(pred)\n    return np.mean(np.abs((actual - pred) / actual) * 100)\n\nprint('The Mean absolute percentage error is:- ',mape(y_test,test_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### R-2 score","metadata":{}},{"cell_type":"code","source":"r2_score(y_test,test_predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see that with Linear Regression the model accuracy achieved is 91.85%","metadata":{}},{"cell_type":"markdown","source":"## Let's also see the residual plots","metadata":{}},{"cell_type":"code","source":"test_residuals = y_test - test_predictions\ntest_residuals","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7,6),dpi=90)\nsns.scatterplot(x=y_test, y=test_residuals)\nplt.ylabel('Residuals from the Linear regression model')\nplt.title('Residual plot of Linear resgression')\nplt.axhline(y=0,color='red');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With the residual plot we can see that the points are normally distributed along the regression line, though points are a bit far off from the line, but we can say that linear regression was a good choice of algo for this dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Polynomial Regssion - Selecting the degree of polynomial\n#### Though we have good accuracy with Linear regression, this can be further increased with Polynomial regression","metadata":{}},{"cell_type":"markdown","source":"#### But we need a way to decide what the order of the polynomial should be to get maximum accuracy.\n\nOne way is to run a for loop for the entire process from polynomial feature creation to finally testing the accuracy using RMSE and then plotting out them out to see which degree gives the least error so that we can create a final model it.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set_rmse = []\ntest_set_rmse = []\n\nfor d in range(1,10):\n    \n    # Creating polymial features\n    polynomial_converter = PolynomialFeatures(degree = d, include_bias=False)\n    poly_features = polynomial_converter.fit_transform(X)\n    \n    # Creating training and test set\n    X_p_train, X_p_test, y_p_train, y_p_test = train_test_split(poly_features, y , test_size=0.3, random_state=101)\n    \n    # Polynomial Model creation\n    poly_model = LinearRegression()\n    poly_model.fit(X_p_train,y_p_train)\n    \n    # Predictions of both train and test set\n    train_set_preds = poly_model.predict(X_p_train)\n    test_set_preds = poly_model.predict(X_p_test)\n    \n    # Calculating RMSE for both train and test predictions\n    train_rmse = np.sqrt(mean_squared_error(y_p_train,train_set_preds))\n    test_rmse = np.sqrt(mean_squared_error(y_p_test,test_set_preds))\n    \n    # Storing the rmse to be later used for plotting\n    train_set_rmse.append(train_rmse)\n    test_set_rmse.append(test_rmse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now we have completed the whole process \n\n#### Let's see what the RMSE values are and plot them","metadata":{}},{"cell_type":"code","source":"train_set_rmse","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the training set RMSE values we can see that the errors are constantly going down with increase in the order of the polynomial, though at one point we can see that for &th degree it shoots up and then gradually decreases which is kind of a redflag but it decreses after that.","metadata":{}},{"cell_type":"code","source":"test_set_rmse","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With the testing set RMSE values we can see that after degree 4 the errors shoot up suddenly to a very high value, giving us an idea where we should stop increasing the order","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Let's visualize them","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,6),dpi=90)\nplt.plot(range(1,6),train_set_rmse[0:5],label='Train RMSE')\nplt.plot(range(1,6),test_set_rmse[0:5],label='Test RMSE')\nplt.xlabel('Model complexity')\nplt.ylabel('RMSE values')\nplt.title('Model complexity vs RMSE on both training and testing set')\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Usually the training set performs a bit better than the tset test, the same can be seen above\n\n### So even though here we see that degree 4 has less error than degree 3 but we also have to think about whether it is worth the risk to go for degree 4 instead of degree3 since we can clearly see taking up degree 4 has a high risk for shooting up the error very badly so what is suggested is that you should go for degree 3","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. So we'll go with degree 3 for our final Polymial regression","metadata":{}},{"cell_type":"markdown","source":"### 6.Final model creation","metadata":{}},{"cell_type":"code","source":"final_poly_converter = PolynomialFeatures(degree=3,include_bias=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_poly_features = final_poly_converter.fit_transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_poly_features.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initially we had just 3 features now as a result of using degree 3 we have the additional features which include the possible squared values of the original values and possible interaction terms","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training and testing data\nX_f_train, X_f_test, y_f_train, y_f_test = train_test_split(final_poly_features, y , test_size=0.3, random_state=101)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_poly_model = LinearRegression()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_poly_model.fit(X_f_train,y_f_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_poly_predictions = final_poly_model.predict(X_f_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_poly_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.Let's test the accuracy of the Polynomial regression model and compare it with Linear Regression","metadata":{}},{"cell_type":"markdown","source":"#### MAE - polynomial regression\n\n","metadata":{}},{"cell_type":"code","source":"print('MAE of Polynomial Regression',mean_absolute_error(y_f_test,final_poly_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MAE of Linear regression - 1.21","metadata":{}},{"cell_type":"markdown","source":"#### RMSE - polynomial regression","metadata":{}},{"cell_type":"code","source":"print('RMSE of Polynomial Regression',np.sqrt(mean_squared_error(y_f_test,final_poly_predictions)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE of Linear regression - 1.51","metadata":{}},{"cell_type":"markdown","source":"We can see that both the error metrics of Polynomial regression are performing way better than Linear regression","metadata":{}},{"cell_type":"markdown","source":"#### R-2 score - Polynomial Regression","metadata":{}},{"cell_type":"code","source":"print('The r2 score is',r2_score(y_f_test,final_poly_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Polynomial Regression Model Accuracy - 98.88%\n\nLinear Regression model accuracy - 91.85%","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize the residual plots","metadata":{}},{"cell_type":"code","source":"poly_test_residuals = y_f_test - final_poly_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6),dpi=90)\nsns.scatterplot(x=y_f_test, y=poly_test_residuals)\nplt.ylabel('Residuals from the Polynomial Regression model')\nplt.title('Residual plot of Polynomial regression')\nplt.axhline(y=0,color='red');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize the residual plots of Linear regression and Polynomial regression side by side","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,5),dpi=90)\nplt.subplot(1,2,1)\nsns.scatterplot(x=y_test, y=test_residuals)\nplt.title('Residual plot of Linear regression')\nplt.ylabel('Residuals from the Linear Regression model')\nplt.axhline(y=0,color='red');\n\nplt.subplot(1, 2, 2) \nsns.scatterplot(x=y_f_test, y=poly_test_residuals)\nplt.title('Residual plot of Polynomial regression')\nplt.ylabel('Residuals from the Polynomial Regression model')\nplt.axhline(y=0,color='red');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the residual plots we can see that with less errors than Linear regression the residuals in Polynomial regression are close to the regression line, telling us that it has less errors.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hence from above metrics analysis we can clearly see that using higher order polynomial we get best accuracy than Linear Regression.","metadata":{}},{"cell_type":"markdown","source":"### We should keep in mind that this does not apply to every case, i.e. polynomial regression won't always give best accuracy, but we won't find out until we try it for ourselves.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(final_poly_model,'Sales_prediction_model_poly_reg.joblib')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dump(final_poly_converter,'Final_sales_features_converter.joblib')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}