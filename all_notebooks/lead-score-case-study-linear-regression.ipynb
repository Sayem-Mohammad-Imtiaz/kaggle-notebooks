{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# *********************************************************************************************************************\n# * @Assignment: Machine Learning 1 : Lead Scoring Case study\n# *********************************************************************************************************************\n# *\n# * @author    : Poonam Yadav and Hitesh Yevale\n# * @version   : v0.1.1\n# * @StartDate : 09-Mar-2020\n# \n# *********************************************************************************************************************","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Steps/Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"# *********************************************************************************************************************\n# * Suppress Warnings\n# *********************************************************************************************************************\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# *********************************************************************************************************************\n# * Import Libraries \n# *********************************************************************************************************************\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# *********************************************************************************************************************\n# To Scale our data\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\n\n#Model building\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n\n\n#display setting\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 200)\n\n \n# *********************************************************************************************************************","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data Reading"},{"metadata":{"trusted":true},"cell_type":"code","source":"# *********************************************************************************************************************\n# * Loading Data frames\n# *********************************************************************************************************************\nLead_score= pd.read_csv(\"../input/lead-scoring-x-online-education/Leads X Education.csv\")\nLead_score_backup= Lead_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ********************************************************************************************************************* \n# * 1. Importing required libraries\n# * 2. Data Reading\n# * 3. Data understanding \n# * 4. Data cleansing \n# * 5. Data Preparation\n# * 6. Model building\n# * 7. Final observations","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Lead_score.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Lead_score.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Data Cleansing"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Checking for duplicate rows in the dataset\nLead_score.loc[Lead_score.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Conclusion: no duplicates found","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing 'select' with a null value . The customer did not make a selection here from the given options therefore this value\nis as good as null value "},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score = Lead_score.replace('Select', np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Computing the % of null values in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# % of null values\nround(100*(Lead_score.isnull().sum()/len(Lead_score.index)), 2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Treating NULL Values**\n- Delete columns with Null Values more than 70%\n- Deal with columns with Null Values between 20-40% manually\n- Drop the other columns based on further analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score = Lead_score.drop(Lead_score.loc[:,list(round(100*(Lead_score.isnull().sum()/len(Lead_score.index)), 2)>70)].columns, 1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"round(100*(Lead_score.isnull().sum()/len(Lead_score.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets look at the following columns with Null Values >20%**\n\nCountry                                         \nSpecialization                                   \nWhat is your current occupation                  \nWhat matters most to you in choosing a course    \nTags                                             \nLead Quality                                    \nCity                                             \nAsymmetrique Activity Index                     \nAsymmetrique Profile Index                      \nAsymmetrique Activity Score                     \nAsymmetrique Profile Score                      "},{"metadata":{},"cell_type":"markdown","source":"***1. Country :***                                                                                                                   \nAs most of the leads are from India , Computing values in Country column to simplify the data by dividing the data into India and outside India; and replacing the null values with 'India'"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Lead_score['Country'] = Lead_score['Country'].apply(lambda x: 'India' if x=='India' else 'Outside India')\nLead_score['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['Country'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score.loc[pd.isnull(Lead_score['Country']), ['Country']] = 'India'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['Country'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***2. Specialization***"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"Lead_score['Specialization'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check the number of null values in this column\nLead_score['Specialization'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since the amount of null values is high , Lets compute the null values and replace them with text 'Unknown'\nLead_score['Specialization'].fillna(\"Unknown\", inplace = True)\nLead_score['Specialization'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***3. What is your current occupation***"},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['What is your current occupation'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['What is your current occupation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['What is your current occupation'].fillna(\"Unknown\", inplace = True)\nLead_score['What is your current occupation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***4. What matters most to you in choosing a course***"},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['What matters most to you in choosing a course'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"Lead_score['What matters most to you in choosing a course'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since this column does not give away a lot of information and mostly are null or 'other' we can drop the column\nLead_score = Lead_score.drop('What matters most to you in choosing a course' , axis= 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***5.Tags***                                                                                                       \nTags assigned to customers indicating the current status of the lead."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Lead_score['Tags'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# This column does not add much value to the analysis , hence dropping it. \nLead_score = Lead_score.drop('Tags',axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***6. Lead Quality***"},{"metadata":{},"cell_type":"markdown","source":"Lead Quality - Indicates the quality of lead based on the data and intuition of the employee who has been assigned to the lead."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"Lead_score['Lead Quality'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lead quality column has 5 unique values except null values, lets check what are those and how usefull are they for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(Lead_score['Lead Quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here all the null values are as good as not sure.\n\nLead_score['Lead Quality'] = Lead_score['Lead Quality'].replace(np.nan, 'Not Sure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(Lead_score['Lead Quality'])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***7. City***"},{"metadata":{},"cell_type":"markdown","source":"We will compute the null values in city column by replacing them with 'Unknown\" because we do not want to loose any data computing it any other way"},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['City'].fillna(\"unknown\",inplace = True)\nLead_score['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***8.Asymmetrique Activity Index***                                                                       \n***9.Asymmetrique Profile Index***                                                                          \n***10.Asymmetrique Activity Score***                                                                             \n***11.Asymmetrique Profile Score***"},{"metadata":{},"cell_type":"markdown","source":"- An index and score assigned to each customer based on their activity and their profile.                                    \n- We can get rid of Asymmetrique Activity Score and Asymmetrique Profile Score as we can look at the index columns for the corresponding information"},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score = Lead_score.drop(['Asymmetrique Activity Score', 'Asymmetrique Profile Score'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets replace the null values in Asymmetrique Activity Index and Asymmetrique Profile Index with 'Unknown'"},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score['Asymmetrique Activity Index'].fillna(\"Unknown\", inplace = True)\nLead_score['Asymmetrique Activity Index'].value_counts()\nLead_score['Asymmetrique Profile Index'].fillna(\"Unknown\", inplace = True)\nLead_score['Asymmetrique Profile Index'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will not look at rest of the columns and see which ones will add value to our analysis and which ones can be dropped from the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Lead_score.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Lead_score['Magazine'].value_counts())\nprint(Lead_score['Receive More Updates About Our Courses'].value_counts())\nprint(Lead_score['Update me on Supply Chain Content'].value_counts())\nprint(Lead_score['I agree to pay the amount through cheque'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since the above columns have only 1 unique value they do not add any value to the data , let's drop them and all similar\n# columns from the data ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score= Lead_score.loc[:,Lead_score.nunique()!=1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prospect ID gives out the same information as Lead Number , therefore we can drop it from  the dataset \nLead_score = Lead_score.drop('Prospect ID' , axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(Lead_score.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Checking the % of null values after all the computations***"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"round(100*(Lead_score.isnull().sum()/len(Lead_score.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(Lead_score['Do Not Email'].value_counts())\nprint(Lead_score['Do Not Call'].value_counts())\nprint(Lead_score['Search'].value_counts())\nprint(Lead_score['Through Recommendations'].value_counts())\nprint(Lead_score['A free copy of Mastering The Interview'].value_counts())\nprint(Lead_score['Newspaper Article'].value_counts())\nprint(Lead_score['X Education Forums'].value_counts())\nprint(Lead_score['Newspaper'].value_counts())\nprint(Lead_score['Digital Advertisement'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since these columns do not give away any specific information we can drop these columns .\nLead_score = Lead_score.drop(columns =['Do Not Email' , 'Do Not Call' , 'Search' ,'Through Recommendations' ,'A free copy of Mastering The Interview','Newspaper Article', 'X Education Forums', 'Newspaper', \n            'Digital Advertisement'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"Lead_score.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets drop rows with null values\nLead_score= Lead_score.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(100*(Lead_score.isnull().sum()/len(Lead_score.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummy variables for categorial variables\n\ncategorical_columns = ['Lead Origin' ,'Country','Lead Quality' ,'Lead Source','Last Activity' , 'Specialization' , 'What is your current occupation'\n                                       ,'City' ,'Last Notable Activity' ,'Asymmetrique Activity Index' , 'Asymmetrique Profile Index']\n\nfor x in categorical_columns:\n    cont = pd.get_dummies(Lead_score[x],prefix=x,drop_first=True)\n    Lead_score = pd.concat([Lead_score,cont],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Lead_score.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the original columns we created dummies for \nLead_score = Lead_score.drop(columns = ['Lead Origin' ,'Country','Lead Quality' ,'Lead Source','Last Activity' , 'Specialization' , 'What is your current occupation'\n                                       ,'City' ,'Last Notable Activity' ,'Asymmetrique Activity Index' , 'Asymmetrique Profile Index'] , axis =1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lead_score.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"Lead_score.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for outliers \noutlier_check = Lead_score[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']]\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"outlier_check.describe(percentiles=[.01,.1,.2,.25, .5, .75, .90, .95, .99])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = Lead_score['Page Views Per Visit'].quantile(0.25)\nQ3 = Lead_score['Page Views Per Visit'].quantile(0.75)\nIQR = Q3 - Q1\nLead_score=Lead_score.loc[(Lead_score['Page Views Per Visit'] >= Q1 - 1.5*IQR) & (Lead_score['Page Views Per Visit'] <= Q3 + 1.5*IQR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = Lead_score['TotalVisits'].quantile(0.25)\nQ3 = Lead_score['TotalVisits'].quantile(0.75)\nIQR = Q3 - Q1\nLead_score=Lead_score.loc[(Lead_score['TotalVisits'] >= Q1 - 1.5*IQR) & (Lead_score['TotalVisits'] <= Q3 + 1.5*IQR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = Lead_score['Total Time Spent on Website'].quantile(0.25)\nQ3 = Lead_score['Total Time Spent on Website'].quantile(0.75)\nIQR = Q3 - Q1\nLead_score=Lead_score.loc[(Lead_score['Total Time Spent on Website'] >= Q1 - 1.5*IQR) & (Lead_score['Total Time Spent on Website'] <= Q3 + 1.5*IQR)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Test-Train split*"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Lead_score.drop(['Lead Number','Converted'], axis=1)\n\nX.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = Lead_score['Converted']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling continuous variables\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"converted = (sum(Lead_score['Converted'])/len(Lead_score['Converted'].index))*100\nconverted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Almost 37% conversion rate ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Logistic regression model\nlogistics = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogistics.fit().summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature selection using RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  # running RFE with 20 variables as output\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20)           \nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assessing with statsModels"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[cols])\nlogm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there not a lot of multicollinearity present in our model , lets plot a heat map and check the correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15), dpi=80, facecolor='w', edgecolor='k', frameon='True')\n\ncor = X_train[cols].corr()\nsns.heatmap(cor, annot=True, cmap=\"YlGnBu\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets drop the variables with high VIF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = cols.drop('Lead Quality_Not Sure' , 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[cols])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = cols.drop('Lead Source_Olark Chat' , 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[cols])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = cols.drop('Last Notable Activity_Modified' , 1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[cols])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = cols.drop('Lead Quality_Might be',1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[cols])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train[cols].columns\nvif['VIF'] = [variance_inflation_factor(X_train[cols].values, i) for i in range(X_train[cols].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15), dpi=80, facecolor='w', edgecolor='k', frameon='True')\n\ncor = X_train[cols].corr()\nsns.heatmap(cor, annot=True, cmap=\"YlGnBu\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This shows now we have very less multi-collinearity compared to earlier Heatmap","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating new column 'predicted' with 1 if Conversion_Prob > 0.5 else 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy \nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sensitivity\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specificity\nTN / float(TN+FP)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP/ float(TN+FP))\n\n# positive predictive value \nprint (TP / float(TP+FP))\n\n# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting ROC Curve                                                                                                       \nAn ROC curve demonstrates several things:                                                                                      \n• It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).                                                                                                          \n• The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.       \n• The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr,tpr, thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate area under the curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def roc_auc(fpr,tpr):\n    AreaUnderCurve = 0.\n    for i in range(len(fpr)-1):\n        AreaUnderCurve += (fpr[i+1]-fpr[i]) * (tpr[i+1]+tpr[i])\n    AreaUnderCurve *= 0.5\n    return AreaUnderCurve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc(fpr,tpr)\nauc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An AUC can be classified as follows,\n\n0.90 - 1.00 = excellent                                                                                   \n0.80 - 0.90 = good                                                                   \n0.70 - 0.80 = fair                                                                                    \n0.60 - 0.70 = poor                                                                                            \n0.50 - 0.60 = fail                                                                                                   \nSince we got a value of 0.89, our model seems to be doing **GOOD** on the test dataset.\n"},{"metadata":{},"cell_type":"markdown","source":"Finding optimal cutoff"},{"metadata":{},"cell_type":"markdown","source":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try and create columns with different probability cutoffs and figure the optimal cutoff \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers: \n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating Accuracy, Sensitivity and Specificity for various probability cutoffs"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Accuracy, Sensitivity and Specificity for various probability cutoffs"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (12,8))\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'],figsize=(10,6))\nplt.xticks(np.arange(0, 1, step=0.05), size = 12)\nplt.show()\nfig.savefig('threshold.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**0.425** looks like an optimum cutoff point from the above plot.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.425 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\nconfusion1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion1[1,1] # true positive \nTN = confusion1[0,0] # true negatives\nFP = confusion1[0,1] # false positives\nFN = confusion1[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sensitivity\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating false postive rate - predicting churn when customer does not have churned\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision and Recall"},{"metadata":{},"cell_type":"markdown","source":"Precision = TP / TP + FP"},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = confusion1[1,1]/(confusion1[0,1]+confusion1[1,1])\nprecision","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall = TP / TP + FN"},{"metadata":{"trusted":true},"cell_type":"code","source":"recall = confusion1[1,1]/(confusion1[1,0]+confusion1[1,1])\nrecall","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using SKlearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision and Recall Tradeoff"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"y_train_pred_final.Converted, y_train_pred_final.final_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4), dpi=100, facecolor='w', edgecolor='k', frameon='True')\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.xticks(np.arange(0, 1, step=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimal threshold value comes to 0.42 (which is similar to the previous observation)"},{"metadata":{},"cell_type":"markdown","source":"**Calculate F1 force**"},{"metadata":{"trusted":true},"cell_type":"code","source":"F1 = 2*(precision*recall)/(precision+recall)\nF1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions on Test data"},{"metadata":{},"cell_type":"markdown","source":"Transfering data to test dataset using scaler function "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[cols]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding the constant \nX_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making predictions on the test data set\ny_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting into an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_df['LeadID'] = y_test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final= y_pred_final.rename(columns={ 0 : 'Conversion_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final = y_pred_final.reindex(['LeadID','Converted','Conversion_Prob'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.42 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the overall accuracy.\naccuracy_score=metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)\naccuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_test = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nprint(confusion_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion_test[1,1] # true positive \nTN = confusion_test[0,0] # true negatives\nFP = confusion_test[0,1] # false positives\nFN = confusion_test[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sensitivity of our logistic regression model\nTP / float(TP+FN)\n# specificity\nTN / float(TN+FP)\n# Calculate false postive rate - predicting converion when customer does not have converted\nprint(FP/ float(TN+FP))\n# Positive predictive value \nprint (TP / float(TP+FP))\n# Negative predictive value\nprint (TN / float(TN+ FN))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Precision\nconfusion_test[1,1]/(confusion_test[0,1]+confusion_test[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recall\nconfusion_test[1,1]/(confusion_test[1,0]+confusion_test[1,1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"y_pred_final.Converted, y_pred_final.final_predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p, r, thresholds = precision_recall_curve(y_pred_final.Converted, y_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_pred_final.Converted, y_pred_final.Conversion_Prob, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc(y_pred_final.Converted, y_pred_final.Conversion_Prob)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_test_pred = y_test_pred * 100\ny_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting the coefficients of the selected features from our final model excluding the intercept"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:.2f}'.format\nnew_params = res.params[1:]\nnew_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\nfeature_importance = new_params\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nfeature_importance","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Sorting the feature variables based on their relative coefficient values\nsorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')\nsorted_idx","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot showing the feature variables based on their relative coefficient values\n# Plotting the scree plot\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure(figsize=(10,6))\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center', color = 'tab:green',alpha=0.8)\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X_train[cols].columns)[sorted_idx], fontsize=12)\nfeatax.set_xlabel('Relative Feature Importance', fontsize=14)\n\nplt.tight_layout()   \nplt.show()\nfig.savefig('Question1.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting top 4 features\npd.DataFrame(feature_importance).reset_index().sort_values(by=0,ascending=False).head(4)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}