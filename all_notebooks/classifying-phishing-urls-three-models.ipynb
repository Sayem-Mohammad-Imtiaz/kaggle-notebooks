{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport sys\nimport os\n!pip install tldextract -q\nimport tldextract\nimport warnings\nimport regex as re\nimport eli5\nfrom typing import *\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn import svm\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom urllib.parse import urlparse\nfrom nltk.tokenize import RegexpTokenizer\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data = pd.read_csv('/kaggle/input/phishing-site-urls/phishing_site_urls.csv')\n#pd.set_option('display.max_colwidth', -1)\nurl_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook I go over the preprocessing of URLs and compare three different binary classification models for determining if a URL is legitimate or used in a phishing attempt. I also look at what features are most highly weighted in the most accurate model. As this is the first notebook I have created, I would greatly appreciate any feedback you may have.\n\nA thank you to [Matthew Franglen](https://www.kaggle.com/matthewfranglen) for improving the quality of my code!"},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data = url_data.rename(columns={\"URL\": \"url\", \"Label\": \"label\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To extract useful features from the URL, we first will break it down into useful parts using the urlparse method. This returns a dictionary of values of the url which will be added as columns in the dataframe soon."},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_url(url: str) -> Optional[Dict[str, str]]:\n    try:\n        no_scheme = not url.startswith('https://') and not url.startswith('http://')\n        if no_scheme:\n            parsed_url = urlparse(f\"http://{url}\")\n            return {\n                \"scheme\": None, # not established a value for this\n                \"netloc\": parsed_url.netloc,\n                \"path\": parsed_url.path,\n                \"params\": parsed_url.params,\n                \"query\": parsed_url.query,\n                \"fragment\": parsed_url.fragment,\n            }\n        else:\n            parsed_url = urlparse(url)\n            return {\n                \"scheme\": parsed_url.scheme,\n                \"netloc\": parsed_url.netloc,\n                \"path\": parsed_url.path,\n                \"params\": parsed_url.params,\n                \"query\": parsed_url.query,\n                \"fragment\": parsed_url.fragment,\n            }\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data[\"parsed_url\"] = url_data.url.apply(parse_url)\nurl_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data = pd.concat([\n    url_data.drop(['parsed_url'], axis=1),\n    url_data['parsed_url'].apply(pd.Series)\n], axis=1)\nurl_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An initial problem faced was that urlparse would not parse some unreadable URLs. The following removes any column where the netloc has no value (ie when the parse_url method returns None)."},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data = url_data[~url_data.netloc.isnull()]\nurl_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first meaningful bit of data to extract is the length of the URL."},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data[\"length\"] = url_data.url.str.len()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The TLD is then extracted using a python library, and if no TLD is present simply add 'None'."},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data[\"tld\"] = url_data.netloc.apply(lambda nl: tldextract.extract(nl).suffix)\nurl_data['tld'] = url_data['tld'].replace('','None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next is a regex to determine if the URL is an IP address."},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data[\"is_ip\"] = url_data.netloc.str.fullmatch(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next few sections relate to certain punctuation in the URL which may be an indicator one way or another that a URL is malicious. My reasoning behind this is that typosquatted domains (which are almost always malicious) may contain this punctation to appear similar to a legitimate domain. There may also be more of each in the path of the URL for a legitimate URL as blogs often use underscores in a URL. "},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data['domain_hyphens'] = url_data.netloc.str.count('-')\nurl_data['domain_underscores'] = url_data.netloc.str.count('_')\nurl_data['path_hyphens'] = url_data.path.str.count('-')\nurl_data['path_underscores'] = url_data.path.str.count('_')\nurl_data['slashes'] = url_data.path.str.count('/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Full stops in the path could indicate that theres an attempt to fool a user into thinking a domain is legit. For example, attacker.com/paypal.com may be used to trick a user. Full stops may also be a sign of files in the URL such as shell.exe"},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data['full_stops'] = url_data.path.str.count('.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to the previous datapoint, getting the full stops in a subdomain will count how many subdomains are present. Lots may be another visual trick such as paypal.com.attacker.com/"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_num_subdomains(netloc: str) -> int:\n    subdomain = tldextract.extract(netloc).subdomain \n    if subdomain == \"\":\n        return 0\n    return subdomain.count('.') + 1\n\nurl_data['num_subdomains'] = url_data['netloc'].apply(lambda net: get_num_subdomains(net))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As previous notebooks have shown, the lexical features of the URL will be important. In this instance, I have decided to separate the tokens from the path and the domain itself. My thinking here is that the same word in a path and domain may have very different meanings. By this i mean if you see 'paypal' in a URL path, it may be a malicious URL which is trying to seem legitimate, but 'paypal' in the domain may be more legitimate. "},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'[A-Za-z]+')\ndef tokenize_domain(netloc: str) -> str:\n    split_domain = tldextract.extract(netloc)\n    no_tld = str(split_domain.subdomain +'.'+ split_domain.domain)\n    return \" \".join(map(str,tokenizer.tokenize(no_tld)))\n         \nurl_data['domain_tokens'] = url_data['netloc'].apply(lambda net: tokenize_domain(net))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data['path_tokens'] = url_data['path'].apply(lambda path: \" \".join(map(str,tokenizer.tokenize(path))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The labels are now extracted and the URL column removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"url_data_y = url_data['label']\nurl_data.drop('label', axis=1, inplace=True)\nurl_data.drop('url', axis=1, inplace=True)\nurl_data.drop('scheme', axis=1, inplace=True)\nurl_data.drop('netloc', axis=1, inplace=True)\nurl_data.drop('path', axis=1, inplace=True)\nurl_data.drop('params', axis=1, inplace=True)\nurl_data.drop('query', axis=1, inplace=True)\nurl_data.drop('fragment', axis=1, inplace=True)\nurl_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"When using pipelines and vectorizers, you need a converter to feed the vectorizer every word of that column. It cannot add the values one row at a time and so a converter class must be created."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Converter(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_frame):\n        return data_frame.values.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(url_data, url_data_y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The numeric features need their own pipeline to scale the data, MinMaxScaler was used as MultinomialNB needs no negative values to work."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = ['length', 'domain_hyphens', 'domain_underscores', 'path_hyphens', 'path_underscores', 'slashes', 'full_stops', 'num_subdomains']\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', MinMaxScaler())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only categorical feature is TLD and OneHot encoding will be used for this. Interestingly there is no difference between using this or converting and using the TfidfVectorizer. However, using OneHot encoding makes the TLD obvious in the feature importance section."},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['tld', 'is_ip']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CountVectorizer and TfidfVectorizer produce very similar results, but with the best performing model (spoiler its SVC) Tfidf slightly improved the score."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_features = ['domain_tokens','path_tokens']\nvectorizer_transformer = Pipeline(steps=[\n    ('con', Converter()),\n    ('tf', TfidfVectorizer())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to link all the transformers together in a ColumnTransformer, and create a pipeline for each classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n        ('domvec', vectorizer_transformer, ['domain_tokens']),\n        ('pathvec', vectorizer_transformer, ['path_tokens'])\n    ])\n\nsvc_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LinearSVC())])\n\nlog_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\nnb_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', MultinomialNB())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The models can then be fit on the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf.fit(X_train, y_train)\nlog_clf.fit(X_train, y_train)\nnb_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{},"cell_type":"markdown","source":"To display the results I created a method that prints the score, classification report and creates a heat map for each model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def results(name: str, model: BaseEstimator) -> None:\n    preds = model.predict(X_test)\n\n    print(name + \" score: %.3f\" % model.score(X_test, y_test))\n    print(classification_report(y_test, preds))\n    labels = ['Good', 'Bad']\n\n    conf_matrix = confusion_matrix(y_test, preds)\n\n    font = {'family' : 'normal',\n            'size'   : 14}\n\n    plt.rc('font', **font)\n    plt.figure(figsize= (10,6))\n    sns.heatmap(conf_matrix, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\", cmap='Blues')\n    plt.title(\"Confusion Matrix for \" + name)\n    plt.ylabel('True Class')\n    plt.xlabel('Predicted Class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results(\"SVC\" , svc_clf)\nresults(\"Logistic Regression\" , log_clf)\nresults(\"Naive Bayes\" , nb_clf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see the SVC performs best out of the three followed by MultinomialNB. While Logistic Regression performs the worst, we can see it produces less false negatives than Naive Bayes. \n\nAlso if numerical features are removed, logistic regression performs better. I wouldn't know why this would be the case and would be interested to hear some ideas for it."},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"Finally, to see what features are most strongly weighted to the SVC classifier I use eli5 to show this. It is worth noting that weights may be high for rarer features and should be taken with a grain of salt."},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features))\ndomvect_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['domvec'].named_steps['tf'].get_feature_names())\npathvect_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['pathvec'].named_steps['tf'].get_feature_names())\nnumeric_features_list = list(numeric_features)\nnumeric_features_list.extend(onehot_columns)\nnumeric_features_list.extend(domvect_columns)\nnumeric_features_list.extend(pathvect_columns)\neli5.explain_weights(svc_clf.named_steps['classifier'], top=20, feature_names=numeric_features_list)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}