{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div align='center'><font size=\"5\" color='#353B47'>The data scientist bagpack</font></div>\n<div align='center'><font size=\"4\" color=\"#353B47\">A good preparation for case study interviews</font></div>\n<br>\n<div align='center'><img src=\"https://s27389.pcdn.co/wp-content/uploads/shutterstock_112621262-1000x440.jpg\"></div>\n<br>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"<div align=\"justify\">Through this notebook, you will find the good practises to have for a case study that you may likely have during a long and endless interview process. For a first role as data scientist, I was given this case study to do within 4h.</div>"},{"metadata":{},"cell_type":"markdown","source":"\n\n<font color=\"red\" size=\"4\">ADVICE</font>\n> If you want to play with the interactive functions allowing you to choose the parameters in a mini interface, this notebook must be forked."},{"metadata":{},"cell_type":"markdown","source":"# Let's dig\n\n<div align=\"justify\">The whole study consists in determining whether an applicant is going to be hired according to his caracteristics. This is a classification problem.</div>"},{"metadata":{},"cell_type":"markdown","source":"## Data description\n\n- date: date of the application\n- age: age of the candidate\n- diplome: highest qualification diploma (bac, licence, master, doctorat)\n- specialite: minor of the diploma (geologie, forage, detective, archeologie,...)\n- salaire: asked salary\n- dispo: oui : directly available, non : not directly available\n- sexe: female (F) or male (M)\n- exp: years of relevant experience\n- cheveux: hair color (chatain, brun, blond, roux)\n- note: grade (out of 100) for gold digging exam\n- embauche: Has the candidate been hired ? (0 : no, 1 : yes)"},{"metadata":{},"cell_type":"markdown","source":"## <div id=\"summary\">Table of contents</div>\n\n**<font size=\"2\"><a href=\"#chap1\">1. Import libraries and data</a></font>**\n**<br><font size=\"2\"><a href=\"#chap2\">2. Handling Missing Values</a></font>**\n**<br><font size=\"2\"><a href=\"#chap3\">3. EDA</a></font>**\n**<br><font size=\"2\"><a href=\"#chap4\">4. Preparing the data for modelling</a></font>**\n**<br><font size=\"2\"><a href=\"#chap5\">5. Random Forest</a></font>**\n**<br><font size=\"2\"><a href=\"#chap6\">6. Feature Importance</a></font>**\n**<br><font size=\"2\"><a href=\"#chap7\">7. Amelioration</a></font>**"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap1\">1. Import libraries and data</div>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# import sys\n# print(sys.version)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align=\"justify\">The test was coded under python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]</div>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%matplotlib inline\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nfrom pprint import pprint\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\n\n# Modelling\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import dataframe\ndata = pd.read_csv('../input/applicants-for-a-gold-digger-position/data.csv').drop(['Unnamed: 0'], axis = 1)\n\n# Display five first rows of the dataframe \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"categorical_feature = list(data.dtypes[data.dtypes == 'object'].index)\nnumerical_feature = list(data.describe().columns)\n\ndef information(dataframe):\n    \n    '''\n    Print the number of observations, features and those which are numerical or categorical\n    '''\n    \n    print(f'the dataframe contains {dataframe.shape[0]} observations and {dataframe.shape[1]} columns\\n')\n    print(f\"the dataframe contains:\\n   -   {len(numerical_feature)} numeric features:\\n   '--->   {numerical_feature}\\n\\n\\\n   -   {len(categorical_feature)} categorical features:\\n   '--->   {categorical_feature}\")\n    \n\ninformation(data)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# For further information\n#pandas_profiling.ProfileReport(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Export in html file\n#profile = pandas_profiling.ProfileReport(data)\n#profile.to_file(\"OrpheeProfiling.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap2\"> 2. Handling Missing Values</div>"},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>In this case study, I did not observe many missing values. I therefore decided to delete the lines containing at least one missing value. Nevertheless, be careful to always check that the data are usable.</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are too few observations with NAs, which is not significant for the study\ndata = data.dropna().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap3\">3. EDA</div>"},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>Exploratory analysis is an essential step in a case study. It allows you to quickly see if there are any outliers in the data and also to get an overview of the data you are manipulating. It is from this exploratory analysis that we will be able to confirm the variables to be used for our model.</div>"},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>3.1 Label proportion</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_label_proportion(df):\n    # Plot\n    fig = go.Figure([go.Bar(x=df.embauche.value_counts().index, y=df.embauche.value_counts().values)])\n\n    fig.update_layout(title=\"Countplot showing proportion of hired candidates\",\n                      xaxis_title=\"Embauche (hired)\",\n                      yaxis_title=\"Count\")\n\n    fig.show()\n\nplot_label_proportion(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size='3'>Do best performers on the exercice stand more chance ?</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's consider those who have the top 20% grade students\ntop_20perc_grades = data[data['note'] > 0.8*100 ].reset_index(drop=True)\nplot_label_proportion(top_20perc_grades)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking for outliers: check the min and max of note\nprint(min(data['note']),max(data['note']),\"\\n\")\n\n# Get index of highest note\nprint(np.argmax(data['note']),\"\\n\")\n\n# Check the data\nprint(data.loc[18992,:],\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>It turns out that the maximum grade is above 100. +100 points score could be considered as bonus. I chose to consider it is a tipo error. In the end, whatever your choices are as long as you are able to justify them.</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering\ndata = data[data['note']<=100].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>Surprisingly, there are more great students who are not hired. As a consequence, we can say that what makes them hired doesn't rely in their academic skills. Let's dig deepper...</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"diplome_20perc_grades = top_20perc_grades.groupby(['diplome'])['embauche'].count()\ndiplome_20perc_grades","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot\nfig = go.Figure(data=[go.Pie(labels=diplome_20perc_grades.index, values=diplome_20perc_grades.values, textinfo='label+percent',\n                             insidetextorientation='radial'\n                            )])\n\nfig.update_traces(hole=.3, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Type of diploma distribution\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salaries_asked_d = top_20perc_grades[top_20perc_grades['diplome']=='doctorat']['salaire']\nsalaries_asked_b = top_20perc_grades[top_20perc_grades['diplome']=='bac']['salaire']\nsalaries_asked_l = top_20perc_grades[top_20perc_grades['diplome']=='licence']['salaire']\nsalaries_asked_m = top_20perc_grades[top_20perc_grades['diplome']=='master']['salaire']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Graph 1\nx1 = salaries_asked_d.values\nx2 = data[data['diplome']=='doctorat']['salaire'].values\nx3 = data['salaire'].values\n\nhist_data = [x1,x2,x3]\ngroup_labels = ['doctorant_top_20%','doctorant','all']\n\nfig1 = ff.create_distplot(hist_data, \n                          group_labels, \n                          show_hist=False, \n                          show_rug=False)\n\nfig1.update_layout(\n    title_text=\"PhD distribution\")\n\nfig1.show()\n\n\n# Graph 2\nx1 = salaries_asked_b.values\nx2 = data[data['diplome']=='bac']['salaire'].values\nx3 = data['salaire'].values\n\nhist_data = [x1,x2,x3]\ngroup_labels = ['bac_top_20%','bac','all']\n\nfig2 = ff.create_distplot(hist_data, \n                          group_labels, \n                          show_hist=False, \n                          show_rug=False)\n\nfig2.update_layout(\n    title_text=\"Baccalauréat degree distribution\")\n\nfig2.show()\n\n# Graph 3\nx1 = salaries_asked_l.values\nx2 = data[data['diplome']=='licence']['salaire'].values\nx3 = data['salaire'].values\n\nhist_data = [x1,x2,x3]\ngroup_labels = ['licence_top_20%','licence','all']\n\nfig3 = ff.create_distplot(hist_data, \n                          group_labels, \n                          show_hist=False,\n                          show_rug=False)\n\nfig3.update_layout(\n    title_text=\"License degree distribution\")\n\nfig3.show()\n\n\n# Graph 4\nx1 = salaries_asked_m.values\nx2 = data[data['diplome']=='master']['salaire'].values\nx3 = data['salaire'].values\n\nhist_data = [x1,x2,x3]\ngroup_labels = ['master_top_20%','master','all']\n\nfig4 = ff.create_distplot(hist_data, \n                          group_labels, \n                          show_hist=False, \n                          show_rug=False)\n\nfig4.update_layout(\n    title_text=\"Master's degree distribution\")\n\nfig4.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>Doctorants fits well with top 20% doctorants regarding salaries expectations. Nevertheless, doctorant are not asking higher salaries compared to global distribution.</div>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Graph 5\nx1 = data[data['diplome']=='doctorat']['exp'].values\nx2 = data['exp'].values\n\nhist_data = [x1,x2]\ngroup_labels = ['master_top_20%','master']\n\nfig5 = ff.create_distplot(hist_data, \n                          group_labels, \n                          show_hist=False, \n                          show_rug=False)\n\nfig5.update_layout(\n    title_text=\"Master's degree distribution\")\n\nfig5.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>... NOT because of the fact they have less experience.</div>"},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>3.2 For further information, Date feature is a gold mine too</font>"},{"metadata":{},"cell_type":"markdown","source":"#### <font color='orange' size='2'>*Generating the TS*</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set date feature as a datetime format\ndata['date'] = pd.to_datetime(data['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if one date can contain many applications\nprint(data.shape[0])\nprint(len(data['date'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>It turns out that yes, so I don't generate TS with that feature as it is not a bijective function of observations. I need to groupby first.</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groupby data to count how many applications per day\napplication_per_day = data.groupby([\"date\"])['embauche'].count()\napplication_per_day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = list(application_per_day.index)\napplication_per_day = np.transpose(list(application_per_day))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check that the range of days corresponds to the length of the index (no missing days)\nprint(max(index) - min(index))\nprint(len(index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.date_range(start=min(index), freq='D',periods=1825)\n\ndata_obs_per_day = pd.DataFrame({'index':index, 'nb_app_day':application_per_day})\n\ndata_obs_per_day.set_index(X, inplace=True)\ndata_obs_per_day = data_obs_per_day['nb_app_day']\n\n# Compute the range of data_observations to calibrate the plot\nprint(max(data_obs_per_day))\nprint(min(data_obs_per_day))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def zoom(dataframe, startDate, endDate):\n    \n    ''' \n    Plot the time series in a specific interval\n    dataframe (dataframe): need timestamp as index\n    startDate (string): a string with same date format than index\n    endDate (string): a string with same date format than index\n    feature (list of string): list of features to select\n    '''\n    \n    ts_to_plot = dataframe[(dataframe.index>=startDate) & (dataframe.index<endDate)]\n\n    mean_ts_to_plot = np.mean(ts_to_plot)\n    std = np.std(ts_to_plot)\n    print(f'mean: {mean_ts_to_plot}, std: {std}')\n    \n    ts_to_plot = pd.DataFrame(ts_to_plot)\n    ts_to_plot['date'] = ts_to_plot.index\n    ts_to_plot = ts_to_plot.reset_index(drop=True)\n    \n    return px.line(ts_to_plot, x='date', y='nb_app_day', range_x=[startDate, endDate])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font color='orange' size='2'>*Plotting the TS*</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = zoom(data_obs_per_day, '2010', '2011')\nfig.show()\n\nfig = zoom(data_obs_per_day, '2011', '2012')\nfig.show()\n\nfig = zoom(data_obs_per_day, '2012', '2013')\nfig.show()\n\nfig = zoom(data_obs_per_day, '2013', '2014')\nfig.show()\n\nfig = zoom(data_obs_per_day, '2014', '2015')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>No saisonality observed, mean and std for each year are really close.</div>"},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>3.3 Levels of each categorical feature</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def levels_cat_features(dataframe, indexes):\n    levels = [dataframe.loc[:,i].unique() for i in indexes]\n    return(levels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"levels_cat_features(data, list(data.dtypes[data.dtypes == 'object'].index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>3.4 Statistical dependancies</font>"},{"metadata":{},"cell_type":"markdown","source":"#### <font color='orange' size='2'>*3.4.1 Speciality x sex*</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# No missing values for accurateness of proportions\n\n@interact\ndef class_proportion(speciality = ['geologie', 'forage', 'detective', 'archeologie'] , \n                     gender = ['','M','F'], \n                     hired = ['', True]):\n    \n    if hired == True:\n        data_embauche = data[data['embauche']==1]\n        temp = round(data_embauche[data_embauche['specialite']==speciality].shape[0] / data.shape[0], 3)\n        temp_m = round(data_embauche[(data_embauche['specialite']==speciality) &\\\n                        (data_embauche['sexe']=='M')].shape[0] / data[data['specialite']==speciality].shape[0], 3)\n        temp_f = round(data_embauche[(data_embauche['specialite']==speciality) &\\\n                        (data_embauche['sexe']=='F')].shape[0] / data[data['specialite']==speciality].shape[0], 3)    \n\n        plt.figure(figsize=(14,7))\n\n        prop_m = round(data_embauche[data_embauche['sexe']=='M'].shape[0] / data.shape[0], 3)\n        prop_f = round(data_embauche[data_embauche['sexe']=='F'].shape[0] / data.shape[0], 3)\n        print(f'\\nProportion of hired men: {prop_m}')\n        print(f'Proportion of hired women: {prop_f}\\n\\n')\n\n        if gender == '':\n            print(f\"Proportion of hired candidates in {speciality}: {temp}\\n\\n\")\n            print(f\"Proportion of hired male in {speciality} in the selected speciality: {temp_m}\")\n            print(f\"Proportion of hired female in {speciality} in the selected speciality: {temp_f}\")\n\n            plt.subplot(1,2,1)\n            sns.countplot(x=\"specialite\", \n                          hue=\"sexe\", \n                          data=data_embauche, \n                          palette=\"muted\")\n            plt.title('Number of hired candidates per speciality')\n            plt.tight_layout(pad = 7)\n            plt.subplot(1,2,2)\n            sns.countplot(x=\"specialite\", \n                          hue=\"sexe\", \n                          data=data_embauche[data_embauche['specialite']==speciality], \n                          palette=\"muted\")\n            plt.title('Number of hired candidates for a chosen speciality')\n            \n        else:\n            print(f\"Proportion of hired candidates in {speciality}: {temp}\")\n            sns.countplot(x=\"specialite\", \n                          hue=\"sexe\", \n                          data=data_embauche[data_embauche['sexe']==gender], \n                          palette=\"muted\")\n            plt.title('Number of hired candidates per speciality for a selected gender')\n            \n    else:\n        temp = round(data[data['specialite']==speciality].shape[0] / data.shape[0], 2)\n        temp_m = round(data[(data['specialite']==speciality) &\\\n                        (data['sexe']=='M')].shape[0] / data[data['specialite']==speciality].shape[0], 2)\n        temp_f = round(data[(data['specialite']==speciality) &\\\n                        (data['sexe']=='F')].shape[0] / data[data['specialite']==speciality].shape[0], 2)    \n\n        plt.figure(figsize=(14,7))\n\n        prop_m = round(data[data['sexe']=='M'].shape[0] / data.shape[0], 2)\n        prop_f = round(1 - prop_m, 2)\n        print(f'\\nProportion of men: {prop_m}')\n        print(f'Proportion of women: {prop_f}\\n\\n')\n\n        if gender == '':\n            print(f\"Proportion of candidates in {speciality}: {temp}\\n\\n\")\n            print(f\"Proportion of male in {speciality} in the selected speciality: {temp_m}\")\n            print(f\"Proportion of female in {speciality} in the selected speciality: {temp_f}\")\n\n            plt.subplot(1,2,1)\n            sns.countplot(x=\"specialite\", \n                          hue=\"sexe\", \n                          data=data, \n                          palette=\"muted\")\n            plt.title('Number of candidates per speciality')\n            plt.tight_layout(pad = 7)\n            plt.subplot(1,2,2)\n            sns.countplot(x=\"specialite\", \n                          hue=\"sexe\", \n                          data=data[data['specialite']==speciality], \n                          palette=\"muted\")\n            plt.title('Number of candidates for a selected speciality')\n            \n        else:\n            print(f\"Proportion of {speciality}: {temp}\")\n            sns.countplot(x=\"specialite\", \n                          hue=\"sexe\", \n                          data=data[data['sexe']==gender], \n                          palette=\"muted\")\n            plt.title('Number of candidates per speciality for a selected gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>The countplot shows that women are more represented in archeologie and detective specialities whereas men are more represented in geologie and forage specialities. The most represented speciality is **Geologie**, which contains more than 50% of all observations. It is also interesting to see that women are equally represented in each field of study.</div>"},{"metadata":{},"cell_type":"markdown","source":"#### <font color='orange' size='2'>*3.4.2 Hair x salary*</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\n\nsns.set(style=\"ticks\", palette=\"pastel\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"cheveux\", y=\"salaire\",\n            hue=\"sexe\", palette=[\"m\", \"g\"],\n            data=data)\nsns.despine(offset=10, trim=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\nsns.despine(left=True)\n\nroux = data.loc[data['cheveux']=='roux', 'salaire']\nblond = data.loc[data['cheveux']=='blond', 'salaire']\nbrun = data.loc[data['cheveux']=='brun', 'salaire']\nchatain = data.loc[data['cheveux']=='chatain', 'salaire']\n\n# Plot a filled kernel density estimate\nsns.distplot(roux, \n             hist=False, \n             color=\"orange\", \n             kde_kws={\"shade\": True}, \n             ax=axes[0, 0])\n\nsns.distplot(blond, \n             hist=False, \n             color=\"yellow\", \n             kde_kws={\"shade\": True}, \n             ax=axes[0, 1])\n\nsns.distplot(brun, \n             hist=False, \n             color=\"brown\", \n             kde_kws={\"shade\": True}, \n             ax=axes[1, 0])\n\nsns.distplot(chatain, \n             hist=False, \n             color=\"goldenrod\", \n             kde_kws={\"shade\": True}, \n             ax=axes[1, 1])\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>Hair colour has absolutely no correlation with salary, each level follows same distribution. Men have a better average salary for each level of hair compared to women and that average salary is almost the same for each level, among men and women.</div>"},{"metadata":{},"cell_type":"markdown","source":"#### <font color='orange' size='2'>*Experience x grade*</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"@interact\ndef correlation(numerical_feature_1 = numerical_feature, \n                numerical_feature_2 = numerical_feature):\n    \n    temp = round(data[numerical_feature_1].corr(data[numerical_feature_2]),4)\n    print(f'Correlation between {numerical_feature_1} and {numerical_feature_2}: {temp}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr = data.corr()\n\n# generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype = np.bool)\n# return the indices for the upper triangle of an (n,m) array\nmask[np.triu_indices_from(mask)] = True\n\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(11,7))\nplt.title(\"Correlation matrix\")\nsns.heatmap(corr, \n            mask=mask, \n            cmap=sns.diverging_palette(220,10, as_cmap=True),\n            square=True, \n            vmax = 1, \n            center = 0, \n            linewidths = .5, \n            cbar_kws = {\"shrink\": .5})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>There is no correlation between the experience and the grade obtained. It means that there is no linear combination can link A to B. The heatmap doesn't show much correlation either.</div>"},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap4\"> 4. Preparing the data for modelling</div>"},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>4.1 One Hot Encoding</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert each categorical feature in number of levels -1 boolean features\ndata_rdy = pd.get_dummies(data, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr = data_rdy.corr()\n# generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype = np.bool)\n# return the indices for the upper triangle of an (n,m) array\nmask[np.triu_indices_from(mask)] = True\n\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(11,7))\nplt.title(\"Correlation matrix\")\nsns.heatmap(corr, mask=mask, cmap=sns.diverging_palette(220,10, as_cmap=True),\n            square=True, vmax = 1, center = 0, linewidths = .5, cbar_kws = {\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>4.2 Split into train and test</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_rdy.drop(['embauche','date'], axis = 1)\ny = data_rdy.embauche\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 12345)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap5\"> 5. Random Forest</div>"},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>I would like to use an ensemble method, RF which I believe fits great for that problem. The execution time is usually quite fast, way faster than XGBoost and more accurate than CART. Plus they can handle unbalanced data, what is precisely our case. They also handle with missing data. The only problem with RF is that it generates many independant trees with different conditions at each nodes and with a wise majority vote, gives the prediction. It is difficult to interpret compared to a logistic regression.</div>"},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>5.1 Random gridsearch</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate model\nrf = RandomForestClassifier(random_state=42)\n\n# Look at parameters used by random forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier(random_state=42)\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 10 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 10, \n                               cv = 3, \n                               verbose=2, \n                               random_state=42, \n                               n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train)\nrf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict_proba(test_features)\n    \n    probs = predictions[:,1]\n    # calculate AUC\n    auc = roc_auc_score(test_labels, probs)\n    print('AUC: %.3f' % auc)\n\n    # calculate roc curve\n    fpr, tpr, thresholds = roc_curve(test_labels, probs)\n    # plot no skill\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    plt.plot(fpr, tpr, marker='.')\n    # show the plot\n    plt.show()\n    return(auc, probs)\n\nbase_model = RandomForestClassifier(n_estimators = 10, \n                                    random_state = 42)\n\nbase_model.fit(X_train, \n               y_train)\n\nbase_accuracy = evaluate(base_model, \n                         X_test, \n                         y_test)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>AUC is one of the most important evaluation metrics for checking any classification model’s performance. It represents degree or measure of separability. It tells how much model is capable of distinguishing between classes.the Higher AUC, the better the model is at predicting</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>Now there are 87% that model will be able to distinguish between positive class and negative class.</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy[0] - base_accuracy) / base_accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>5.2 Confusion matrix</font>"},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = evaluate(best_random, X_test, y_test)\ny_predict = y_predict[1]\ny_predict = [round(i) for i in y_predict]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = confusion_matrix(y_test, y_predict)\n\ndef display_results(confusion_matrix):\n    precision = round((confusion_matrix[0][0] + confusion_matrix[1][1]) / (confusion_matrix[0][0] + confusion_matrix[1][1] + confusion_matrix[0][1] + confusion_matrix[1][0]), 2)\n    recall = round(confusion_matrix[0][0] / (confusion_matrix[0][0] + confusion_matrix[1][0]), 2)\n    f1_score = round(2 * precision * recall / (precision + recall), 2)\n\n    print(f'Precision : {precision}')\n    print(f'Recall    : {recall}')\n    print(f'F1 Score  : {f1_score}')\n    \ndisplay_results(confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Precision : Of those predicted positive, how many of them are actual positive.\n\n- Recall : How many of the Actual Positives our model capture through labeling it as Positive (True Positive).\n\n- F1 Score : A balance between Precision and Recall."},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap6\"> 6. Feature importance</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(best_random.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(data.columns, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Set the style\nplt.style.use('fivethirtyeight')\n\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n\n# Tick labels for x axis\nplt.xticks(x_values, data.columns, rotation='vertical')\n\n# Axis labels and title\nplt.ylabel('Importance')\nplt.xlabel('Variable')\nplt.title('Variable Importances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='justify'>As expected, hair color has no influence in the model</div>"},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n---"},{"metadata":{},"cell_type":"markdown","source":"## <div id=\"chap7\"> 7. AMELIORATION</div>"},{"metadata":{},"cell_type":"markdown","source":"### <font color='blue' size='3'>7.1 Gridsearch with CV</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 200]\n}\n# Create a based model\nrf = RandomForestClassifier(random_state=42)\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy[0] - base_accuracy) / base_accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n<br>\n<div align='justify'><font color=\"#353B47\" size=\"4\">Thank you for taking the time to read this notebook. I hope that I was able to answer your questions or your curiosity and that it was quite understandable. <u>any constructive comments are welcome</u>. They help me progress and motivate me to share better quality content. I am above all a passionate person who tries to advance my knowledge but also that of others. If you liked it, feel free to <u>upvote and share my work.</u> </font></div>\n<br>\n<div align='center'><font color=\"#353B47\" size=\"3\">Thank you and may passion guide you.</font></div>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}