{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nRANDOM_STATE = 42\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        filepath = os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a DataFrame out of the dataset\n\ndataset = pd.read_csv(filepath)\nprint(dataset.head())\nprint(dataset.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing some ML stuff\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.datasets import make_classification","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def get_train_test(dataset, mask_col=None):\n        \n    # splitting into train and test dataset\n\n    X, y = dataset.iloc[:, :-1], dataset.iloc[:, -1]\n\n    # X, y = make_classification(n_samples=100, n_features=15, n_classes=3,\n    #                            n_informative=4, n_redundant=1, n_repeated=2,\n    #                            random_state=1)\n\n    # X, y = pd.DataFrame(X), pd.Series(y).values.ravel()\n    # X['Gender'] = X['Gender'].apply(lambda x: 0 if x == 'Male' else 1)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n    \n    if mask_col:\n        X_train, X_test = X_train[mask_col], X_test[mask_col]\n    \n    \n\n    print(\"Train Feature: {0}\\nTest Feature: {1}\\nTrain Target: {2}\\nTest Target: {3}\\n\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n    \n    # bringing all of the features into a similar scale\n\n    scaler = preprocessing.StandardScaler()\n    scaler.fit_transform(X_train)\n    scaler.transform(X_test)\n\n    print(X_test.head())\n    \n    # imputing missing or invalid values\n\n    my_imputer = SimpleImputer()\n    X_train = my_imputer.fit_transform(X_train)\n    X_test = my_imputer.transform(X_test)\n    \n    return X_train, X_test, y_train, y_test, X, y\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test, X, y = get_train_test(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_pred, y_test):\n    \n    acc = round(accuracy_score(y_pred, y_test)*100, 2)\n    f1 = round(f1_score(y_pred, y_test)*100, 2)\n    prec = round(precision_score(y_pred, y_test)*100, 2)\n    rec = round(recall_score(y_pred, y_test)*100, 2)\n    try:    \n        roc = round(roc_auc_score(y_pred, y_test)*100, 2)\n    except ValueError:\n        roc = 'NA'\n    \n    print(\"[INFO]: Accuracy: {0}\".format(acc))\n    print(\"[INFO]: F1 Score: {0}\".format(f1))\n    print(\"[INFO]: Specificity: {0}\".format(prec))\n    print(\"[INFO]: Sensitivity: {0}\".format(rec))\n    print(\"[INFO]: Area Under ROC Curve: {0}\".format(roc))\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# running the models\n\nfor model in [LogisticRegression(random_state=RANDOM_STATE), RandomForestClassifier(random_state=RANDOM_STATE), MLPClassifier(random_state=RANDOM_STATE), SVC(random_state=RANDOM_STATE)]:\n    \n    print(\"[INFO]: Fitting\", str(model), \"...\")\n    \n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    evaluate(y_pred, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyswarms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying only PSO for feature selection\n\nimport pyswarms as ps\n\n# Define objective function\n\ndef f_per_particle(m, alpha):\n    \n    \"\"\"Computes for the objective function per particle\n\n    Inputs\n    ------\n    m : numpy.ndarray\n        Binary mask that can be obtained from BinaryPSO, will\n        be used to mask features.\n    alpha: float (default is 0.5)\n        Constant weight for trading-off classifier performance\n        and number of features\n\n    Returns\n    -------\n    numpy.ndarray\n        Computed objective function\n    \"\"\"\n    total_features = X_train.shape[1]\n    \n    # Get the subset of the features from the binary mask\n    if np.count_nonzero(m) == 0:\n        X_train_subset = X_train\n        X_test_subset = X_test\n    else:\n        X_train_subset = X_train[:,m==1]\n        X_test_subset = X_test[:,m==1]\n        \n    # Perform classification and store performance in P\n    \n    model = SVC(random_state=RANDOM_STATE)\n    \n    model.fit(X_train_subset, y_train)\n    y_pred = model.predict(X_test_subset)\n    \n    P = f1_score(y_pred, y_test)\n    \n    # Compute for the objective function\n    j = (alpha * (1.0 - P)\n        + (1.0 - alpha) * (1 - (X_train_subset.shape[1] / total_features)))\n\n    return j","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f(x, alpha=0.88):\n    \n    \"\"\"Higher-level method to do classification in the\n    whole swarm.\n\n    Inputs\n    ------\n    x: numpy.ndarray of shape (n_particles, dimensions)\n        The swarm that will perform the search\n\n    Returns\n    -------\n    numpy.ndarray of shape (n_particles, )\n        The computed loss for each particle\n        \n    \"\"\"\n    n_particles = x.shape[0]\n    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]\n    \n    return np.array(j)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Initialize swarm, arbitrary\n\noptions = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n\n# Call instance of PSO\n\noptimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=X_train.shape[1], options=options)\n\n# Perform optimization\n\ncost, pos_justpso = optimizer.optimize(f, iters=1000, verbose=2)\n\n\noptimizer.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test, X, y= get_train_test(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using filter methods before PSO\n\n# ====== Chi2 + PSO ========\n\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, f_regression\n\nkbest_chi2 = SelectKBest(chi2, k=9)\nX_train = kbest_chi2.fit_transform(X_train, y_train)\nX_test = kbest_chi2.transform(X_test)\n\nmask_chi2 = kbest_chi2.get_support()\nfeatures_chi2 = X.columns[mask_chi2]\nprint(\"Chi2: \", features_chi2)\n\n# Initialize swarm, arbitrary\n\noptions = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n\n# Call instance of PSO\n\noptimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=X_train.shape[1], options=options)\n\n# Perform optimization\n\ncost, pos_chi2 = optimizer.optimize(f, iters=1000, verbose=2)\n\noptimizer.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test, X, y = get_train_test(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====== Information Gain + PSO ========\n\nkbest_info_gain = SelectKBest(mutual_info_classif, k=9)\nX_train = kbest_info_gain.fit_transform(X_train, y_train)\nX_test = kbest_info_gain.transform(X_test)\n\nmask_info_gain = kbest_info_gain.get_support()\nfeatures_info_gain = X.columns[mask_info_gain]\nprint(\"Information Gain: \", features_info_gain)\n\n# Initialize swarm, arbitrary\n\noptions = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n\n# Call instance of PSO\n\noptimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=X_train.shape[1], options=options)\n\n# Perform optimization\n\ncost, pos_info_gain = optimizer.optimize(f, iters=1000, verbose=2)\n\noptimizer.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test, X, y = get_train_test(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====== F-score + PSO ========\n\nkbest_f_reg = SelectKBest(f_regression, k=9)\nprint(X_train.shape, X_test.shape)\nX_train = kbest_f_reg.fit_transform(X_train, y_train)\nX_test = kbest_f_reg.transform(X_test)\n\nmask_f_reg = kbest_f_reg.get_support()\nfeatures_f_reg = X.columns[mask_f_reg]\nprint(\"F-Regression: \", features_f_reg)\n\n# Initialize swarm, arbitrary\n\noptions = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}\n\n# Call instance of PSO\n\noptimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=X_train.shape[1], options=options)\n\n# Perform optimization\n\ncost, pos_f_reg = optimizer.optimize(f, iters=1000, verbose=2)\n\noptimizer.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESULTS ==============\n\ncolumns_just_pso = list()\nmaster_filter_pso = set()\n\nchi2 = set()\ninfo_gain = set()\nf_reg = set()\n\nprint(\"Columns selected with just PSO:\\n\")\n\nfor x, y in zip(X.columns, pos_justpso):\n    \n    if y == 1:\n        print(x)\n        columns_just_pso.append(x)\n        \nprint(\"\\nColumns selected with chi2+PSO:\\n\")\n\nfor x, y in zip(X.columns, pos_chi2):\n    \n    if y == 1:\n        print(x)\n        chi2.add(x)\n        \nprint(\"\\nColumns selected with infogain+PSO:\\n\")\n\nfor x, y in zip(X.columns, pos_info_gain):\n    \n    if y == 1:\n        print(x)\n        info_gain.add(x)\n\nprint(\"\\nColumns selected with f-reg+PSO:\\n\")\n\nfor x, y in zip(X.columns, pos_f_reg):\n    \n    if y == 1:\n        print(x)\n        f_reg.add(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making the master filter+pso feature list\n\nfor feature in chi2.intersection(info_gain).intersection(f_reg):\n    master_filter_pso.add(feature)\n    \nfor feature in chi2.intersection(info_gain):\n    master_filter_pso.add(feature)\n    \nfor feature in chi2.intersection(f_reg):\n    master_filter_pso.add(feature)\n\nfor feature in info_gain.intersection(f_reg):\n    master_filter_pso.add(feature)\n    \nprint(master_filter_pso)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing just PSO with filter + PSO\n\n# Just PSO\n\nX_train, X_test, y_train, y_test, X, y = get_train_test(dataset, columns_just_pso)\n\n# running the models\n\nfor model in [LogisticRegression(random_state=RANDOM_STATE), RandomForestClassifier(random_state=RANDOM_STATE), MLPClassifier(random_state=RANDOM_STATE), SVC(random_state=RANDOM_STATE)]:\n    \n    print(\"[INFO]: Fitting\", str(model), \"...\")\n    \n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    evaluate(y_pred, y_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filter +  PSO\n\nX_train, X_test, y_train, y_test, X, y = get_train_test(dataset, list(master_filter_pso))\n\n# running the models\n\nfor model in [LogisticRegression(random_state=RANDOM_STATE), RandomForestClassifier(random_state=RANDOM_STATE), MLPClassifier(random_state=RANDOM_STATE), SVC(random_state=RANDOM_STATE)]:\n    \n    print(\"[INFO]: Fitting\", str(model), \"...\")\n    \n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    evaluate(y_pred, y_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}