{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Problem Statement:** Given an itemâ€™s review comment, predict the rating\nWe'll use Womens Clothing E-Commerce Reviews as our dataset, where the rating takes integer values from 1 to 5 (1 being worst and 5 being best).\n\nReference: https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from collections import Counter\nimport string\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport spacy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\nreviews = pd.read_csv(\"/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\")\nprint(reviews.shape)\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill empty cells, and combine the 'Title' and 'Review Text' into a new column 'review'\nreviews['Title'] = reviews['Title'].fillna('')\nreviews['Review Text'] = reviews['Review Text'].fillna('')\nreviews['review'] = reviews['Title'] + ' ' + reviews['Review Text']\n\n# Keep only relevant columns and calculate sentence lengths\nreviews = reviews[['review', 'Rating']]\nreviews.columns = ['review', 'rating']\nreviews['review_length'] = reviews['review'].apply(lambda x: len(x.split()))\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean sentence length: \", np.mean(reviews['review_length']))\nprint(\"Ratings distribution: \", Counter(reviews['rating']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change ratings to 0-numbering\nzero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\nreviews['rating'] = reviews['rating'].apply(lambda x: zero_numbering[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenization\n# ( Tokenization is the task of splitting a text into meaningful segments, called tokens. See https://spacy.io/usage/linguistic-features#tokenization )\ntok = spacy.load('en')\ndef tokenize (text):\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # Remove punctuation and numbers\n    nopunct = regex.sub(\" \", text.lower())\n    return [token.text for token in tok.tokenizer(nopunct)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count number of occurences of each word\ncounts = Counter()\nfor index, row in reviews.iterrows():\n    counts.update(tokenize(row['review']))\n    \n# Delete infrequent words\nprint(\"num_words before:\",len(counts.keys()))\nfor word in list(counts):\n    if counts[word] < 2:\n        del counts[word]\nprint(\"num_words after:\",len(counts.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)\n    \ndef encode_sentence(text, vocab2index, N=70):\n    tokenized = tokenize(text)\n    encoded = np.zeros(N, dtype=int)\n    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n    length = min(N, len(enc1))\n    encoded[:length] = enc1[:length]\n    return encoded, length\n\nreviews['encoded'] = reviews['review'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PyTorch Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ReviewsDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.x[idx][0].astype(np.int32)), self.y[idx], self.x[idx][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(reviews['encoded'])\ny = list(reviews['rating'])\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)\n\ntrain_ds = ReviewsDataset(x_train, y_train)\nvalid_ds = ReviewsDataset(x_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PyTorch Training Loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, y, l in train_dl:\n            x = x.long()\n            y = y.long()\n            y_pred = model(x, l)\n            optimizer.zero_grad()\n            loss = F.cross_entropy(y_pred, y)\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n\ndef validation_metrics (model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    sum_rmse = 0.0\n    for x, y, l in valid_dl:\n        x = x.long()\n        y = y.long()\n        y_hat = model(x, l)\n        loss = F.cross_entropy(y_hat, y)\n        pred = torch.max(y_hat, 1)[1]\n        correct += (pred == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n    return sum_loss/total, correct/total, sum_rmse/total","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Model\n\n### 1. LSTM with fixed input size"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 5000\nvocab_size = len(words)\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(valid_ds, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM_fixed_len(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 5)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])\n    \nmodel_fixed =  LSTM_fixed_len(vocab_size, 50, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model_fixed, epochs=30, lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. LSTM with variable input size:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM_variable_input(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(0.3)\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 5)\n        \n    def forward(self, x, s):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n        out_pack, (ht, ct) = self.lstm(x_pack)\n        out = self.linear(ht[-1])\n        return out\n    \nmodel = LSTM_variable_input(vocab_size, 50, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model, epochs=30, lr=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. LSTM with fixed input size and fixed pre-trained Glove word-vectors:\nhttps://nlp.stanford.edu/projects/glove/"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove_vectors(glove_file=\"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\"):\n    \"\"\"Load the glove word vectors\"\"\"\n    word_vectors = {}\n    with open(glove_file) as f:\n        for line in f:\n            split = line.split()\n            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n    return word_vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n    \"\"\" Creates embedding matrix from word vectors\"\"\"\n    vocab_size = len(word_counts) + 2\n    vocab_to_idx = {}\n    vocab = [\"\", \"UNK\"]\n    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n    vocab_to_idx[\"UNK\"] = 1\n    i = 2\n    for word in word_counts:\n        if word in word_vecs:\n            W[i] = word_vecs[word]\n        else:\n            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n        vocab_to_idx[word] = i\n        vocab.append(word)\n        i += 1   \n    return W, np.array(vocab), vocab_to_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vecs = load_glove_vectors()\npretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM_glove_vecs(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n        self.embeddings.weight.requires_grad = False ## freeze embeddings\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 5)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])\n    \nmodel = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model, epochs=30, lr=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bonus: Predict ratings using regression instead of classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_regr(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, y, l in train_dl:\n            x = x.long()\n            y = y.float()\n            y_pred = model(x, l)\n            optimizer.zero_grad()\n            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss = validation_metrics_regr(model, val_dl)\n        if i % 5 == 1:\n            print(\"train mse %.3f val rmse %.3f\" % (sum_loss/total, val_loss))\n\ndef validation_metrics_regr (model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, y, l in valid_dl:\n        x = x.long()\n        y = y.float()\n        y_hat = model(x, l)\n        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM_regr(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])\n    \nmodel =  LSTM_regr(vocab_size, 50, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model_regr(model, epochs=30, lr=0.05)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}