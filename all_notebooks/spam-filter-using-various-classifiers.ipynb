{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport regex as re\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/spam-filter/emails.csv')      # reading data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()        # checking data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking null/missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking counts of spams and non-spams\ndata['spam'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Punctutaion\ndef remove_punctuation(text):\n    no_punct=\"\".join([words for words in text if words not in string.punctuation])\n    return no_punct\ndata[\"text\"] = data['text'].apply(lambda x: remove_punctuation(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Stopwords\nstopword = set(stopwords.words('english'))\nstopword.add('Subject')\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stopword])\ndata['text'] = data['text'].apply(lambda x: remove_stopwords(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing\n'''Tokenization is the process of breaking text into smaller pieces called tokens. \nThese smaller pieces can be sentences, words, or sub-words.'''\ndef tokenize(text):\n    split=re.split(\"\\W+\",text) \n    return split\ndata['text']=data['text'].apply(lambda x: tokenize(x.lower()))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatizing\n'''Lemmatizing is the process of reducing a word to its root form.'''\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text])\n\ndata['text'] = data[\"text\"].apply(lambda text: lemmatize_words(text))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data according to spam and non-spam \nspam = \" \".join(data[data['spam'] == 1]['text'].tolist())\nnon_spam = \" \".join(data[data['spam'] == 0]['text'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding most repeated words in the data\ndef return_top_words(text,words = 10):\n    allWords = nltk.tokenize.word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')\n    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in allWords if w not in stopwords)    \n    mostCommontuples= allWordExceptStopDist.most_common(words)\n    mostCommon = [tupl[0] for tupl in mostCommontuples]\n    return mostCommon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_spam = return_top_words(spam,10)\ntop_10_non_spam = return_top_words(non_spam,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(top_10_spam)\nprint(top_10_non_spam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in data.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(spam) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in data.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(non_spam) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['text']\ny = data['spam']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF (Term Frequency - Inverse Document Frequency)\n'''This is a technique to quantify a word in documents, we generally compute a weight to each word\nwhich signifies the importance of the word in the document and corpus. \nThis method is a widely used technique in Information Retrieval and Text Mining.'''\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X)\nX_ct  = vectorizer.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data\nX_train,X_test,y_train,y_test = train_test_split(X_ct,y,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **KNN Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train,y_train)\ny_pred1 = knn_classifier.predict(X_test)\nprint(\"accuracy score is :\",accuracy_score(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy Score for KNN Classifier is 97%"},{"metadata":{},"cell_type":"markdown","source":"### **Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb= MultinomialNB()\nnb.fit(X_train,y_train)\ny_pred2 = nb.predict(X_test)\nprint(\"accuracy score is: \",accuracy_score(y_test,y_pred2))\nprint(classification_report(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy score for Naive Bayes Classifier is 89%"},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred3 = rf.predict(X_test)\nprint(\"accuracy score is: \",accuracy_score(y_test,y_pred3))\nprint(classification_report(y_test,y_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy Score for Random Forest is 97%"},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg = XGBClassifier()\nxg.fit(X_train, y_train)\ny_pred4 = xg.predict(X_test)\nprint(\"accuracy score is: \",accuracy_score(y_test,y_pred4))\nprint(classification_report(y_test,y_pred4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy Score for XGBoost is 98%"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}