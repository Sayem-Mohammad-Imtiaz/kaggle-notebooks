{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wine Quality Prediction\n\n<img src=\"https://user-images.githubusercontent.com/53637541/84410794-b8aae480-ac2b-11ea-9020-8e0e843e866c.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement\n\nWine is a beverage made from fermented grape and other fruit juices with lower amount of alcohol content.Quality of wine is graded based on the taste of wine and vintage. This process is time taking, costly and not efficient.\n\nIn industries, understanding the demands of wine safety testing can be a complex task for the laboratory with numerous analytes and residues to monitor. But, our applicationâ€™s prediction, provide ideal solutions for the analysis of wine, which will make this whole process efficient and cheaper with less human interaction.\n\nWe will create a machine learning model to predict the wine quality. We will analyse the quality of wine through different parameters like fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulphur dioxide, total sulphur dioxide, density, pH, sulphates, alcohol and quality.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Choosing the right tools","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the dataset\ndf = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic summary of the data\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic Statistics information about the data\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Processing & EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since all the values are numeric, we will check the correlation\nfig = plt.figure(figsize=(10,5))\nsns.heatmap(round(df.corr(),2),cmap = \"viridis\", annot = True)\nplt.title(\"Correlation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- __Citric acid__ and __density__ are more __positively__ correlated with __fixed acidity__.\n- __pH__ is __negatively__ correlated with __fixed acidity__.\n- __Citric acid__ is also __negatively__ correlated with __volatile acidity__.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA\nfig = plt.figure(figsize = (10,5))\ngs = fig.add_gridspec(1,2)\nax1 = fig.add_subplot(gs[0,0])\nax1 = sns.scatterplot(data = df, x = \"citric acid\", y = \"fixed acidity\", hue = \"quality\", palette = \"rainbow\", legend = \"full\")\nax2 = fig.add_subplot(gs[0,1])\nax2 = sns.scatterplot(data = df, x = \"density\", y = \"fixed acidity\", hue = \"quality\", palette = \"rainbow\", legend = \"full\")\nax1.set_title(\"Citric Acid vs Fixed Acidity\")\nax2.set_title(\"Density vs Fixed Acidity\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.quality, kde = True, color = \"y\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Observation__\n\n- Data is more densely populated at quality 5 and 6.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20,10))\ndf1 = df[df[\"quality\"].isin([6,7,8])]\nsns.lmplot(\"citric acid\",\"fixed acidity\", df1, hue = \"quality\", col = \"quality\")\n#plt.suptitle(\"Scatter plot with regression lines on different axes\", fontsize = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Observation__\n\n- From the above figures, __quality with 8__ has a best fitting line compared to other two. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Splitting the dataset into train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset into train and test\nfeatures = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides',\n            'free sulfur dioxide', 'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol']\ntarget = ['quality']\n\nX = sc.fit_transform(df[features])\ny = sc.fit_transform(df[target])\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.20, random_state = 1)\n\nprint(\"Training Dataset for features:\", X_train.shape)\nprint(\"Training Dataset for target:\", y_train.shape)\nprint(\"Testing Dataset for features:\", X_test.shape)\nprint(\"Testing Dataset for target:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets use Linear Regression\nfrom sklearn.linear_model import LinearRegression\nlr_model = LinearRegression()\nlr_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the Intercept\nprint('Intercept:',lr_model.intercept_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_X_train = pd.DataFrame(data = X_train)\nprint(type(df_X_train))\ndf_X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Changing columns names\ndf_X_train.columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\ndf_X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking Coefficients\npd.DataFrame((lr_model.coef_).T, index = df_X_train.columns, columns = [\"Coefficients\"]).sort_values(by = \"Coefficients\", ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction for training & testing set\ny_pred_train = lr_model.predict(X_train)\ny_pred_test = lr_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Evaluation\nfrom sklearn import metrics\n#MAE\nMAE_train = metrics.mean_absolute_error(y_train,y_pred_train)\nMAE_test = metrics.mean_absolute_error(y_test,y_pred_test)\n\n#MSE\nMSE_train = metrics.mean_squared_error(y_train,y_pred_train)\nMSE_test = metrics.mean_squared_error(y_test,y_pred_test)\n\n#RMSE\nRMSE_train = np.sqrt(MSE_train)\nRMSE_test = np.sqrt(MSE_test)\n\n#R-Squared\nR2_train = metrics.r2_score(y_train,y_pred_train)\nR2_test = metrics.r2_score(y_test,y_pred_test)\n\nprint(\"RMSE for train dataset is:\", RMSE_train)\nprint(\"RMSE for test dataset is:\", RMSE_test)\nprint(\"R-Squared value for train dataset is:\", R2_train)\nprint(\"R-Squared value for test dataset is:\", R2_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets change the target variable into a classification problem (Good(1) vs Bad(0))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The quality is bad if it falls between 2.5 to 6 and the quality is good if it falls between 6 to 8.5\nbins = (2.5,6,8.5)\ngroups_name = [\"Bad\",\"Good\"]\ndf[\"quality_new\"] = pd.cut(df[\"quality\"], bins = bins, labels = groups_name)\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_quality = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"quality_new\"].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"quality_new\"] = label_quality.fit_transform(df[\"quality_new\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets use Logistic Regression\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides',\n            'free sulfur dioxide', 'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol']\ntarget = ['quality_new']\nX = df[features]\ny = df[target]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.20, random_state = 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Standard scaling to get optimized result\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()\nlog_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the testing set\ny_pred_log = log_model.predict(X_test)\ny_pred_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy Score is:\", accuracy_score(y_test,y_pred_log))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nc_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_log))\nc_matrix.index = [\"Actual Bad Quality\", \"Actual Good Quality\"]\nc_matrix.columns = [\"Predicted Bad Quality\", \"Predicted Good Quality\"]\nprint(c_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stochastic Gradient Decent Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_model = SGDClassifier()\nsgd_model.fit(X_train,y_train)\ny_pred_sgd = sgd_model.predict(X_test)\nprint(\"Accuracy Score is:\", accuracy_score(y_test,y_pred_sgd))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_sgd))\nc_matrix.index = [\"Actual Bad Quality\", \"Actual Good Quality\"]\nc_matrix.columns = [\"Predicted Bad Quality\", \"Predicted Good Quality\"]\nprint(c_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets use Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier(random_state = 0)\ndt_model.fit(X_train,y_train)\ny_pred_dt = dt_model.predict(X_test)\nprint(\"Accuracy Score is:\", accuracy_score(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_dt))\nc_matrix.index = [\"Actual Bad Quality\", \"Actual Good Quality\"]\nc_matrix.columns = [\"Predicted Bad Quality\", \"Predicted Good Quality\"]\nprint(c_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets use Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators = 300)\nrf_model.fit(X_train,y_train)\ny_pred_rf = rf_model.predict(X_test)\nprint(\"Accuracy Score is:\", accuracy_score(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_rf))\nc_matrix.index = [\"Actual Bad Quality\", \"Actual Good Quality\"]\nc_matrix.columns = [\"Predicted Bad Quality\", \"Predicted Good Quality\"]\nprint(c_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## HyperParameter Tuning for Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#HyperParameter Tuning\n\nfrom sklearn.model_selection import RandomizedSearchCV\nparameters_rf = {\n    'n_estimators' : [300,700],\n    'criterion' : ['gini','entropy'],\n    'min_samples_split' : range(6,11),\n    'min_samples_leaf' : range(1,5),\n    'max_features' : ['sqrt','log2',5],\n    'bootstrap' : [True, False]\n    }\ntuned_rf_model = RandomizedSearchCV(rf_model, param_distributions = parameters_rf, n_iter = 100, n_jobs = -1)\ntuned_rf_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_tuned_rf_model = tuned_rf_model.predict(X_test)\nprint(\"Accuracy Score is:\", accuracy_score(y_test,y_pred_tuned_rf_model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_rf_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_tuned_rf_model))\nc_matrix.index = [\"Actual Bad Quality\", \"Actual Good Quality\"]\nc_matrix.columns = [\"Predicted Bad Quality\", \"Predicted Good Quality\"]\nprint(c_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- __HyperParameter tuned Random Forest model__ gives us an accuracy of __93%__ in predicting the quality of Wine.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}