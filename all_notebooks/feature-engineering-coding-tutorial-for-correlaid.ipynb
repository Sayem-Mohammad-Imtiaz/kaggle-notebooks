{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Source: https://www.kaggle.com/learn/feature-engineering\n\n# Welcome to Feature Engineering!\n\nFeature engineering specifically includes how to:\n- determine which features are the most important with *mutual information*\n- invent new features in several real-world problem domains\n- encode high-cardinality categoricals with a *target encoding*\n","metadata":{}},{"cell_type":"markdown","source":"## Goal\n\nThe goal of feature engineering is simply to make your data better suited to the problem at hand.\n\nYou might perform feature engineering to:\n- improve a model's predictive performance\n- reduce computational or data needs\n- improve interpretability of the results","metadata":{}},{"cell_type":"markdown","source":"## Example - Linear model with squared feature is more powerful\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/5D1z24N.png\" width=300, alt=\"A scatterplot of Length along the x-axis and Price along the y-axis, the points increasing in a curve, with a poorly-fitting line superimposed.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>A linear model fits poorly with only Length as feature.\n</center></figcaption>\n</figure>\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/BLRsYOK.png\" width=600, alt=\"Left: Area now on the x-axis. The points increasing in a linear shape, with a well-fitting line superimposed. Right: Length on the x-axis now. The points increase in a curve as before, and a well-fitting curve is superimposed.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center><strong>Left:</strong> The fit to Area is much better. <strong>Right:</strong> Which makes the fit to Length better as well.\n</center></figcaption>\n</figure>","metadata":{}},{"cell_type":"markdown","source":"## Example - Concrete Formulations #\n\nTo illustrate these ideas we'll see how adding a few synthetic features to a dataset can improve the predictive performance of a random forest model.\n\nThe [*Concrete*](https://www.kaggle.com/sinamhd9/concrete-comprehensive-strength) dataset contains a variety of concrete formulations and the resulting product's *compressive strength*, which is a measure of how much load that kind of concrete can bear. The task for this dataset is to predict a concrete's compressive strength given its formulation.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndf = pd.read_csv(\"../input/fe-course-data/concrete.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see here the various ingredients going into each variety of concrete. We'll see in a moment how adding some additional synthetic features derived from these can help a model to learn important relationships among them.\n\nWe'll first establish a baseline by training the model on the un-augmented dataset. This will help us determine whether our new features are actually useful.\n\nEstablishing baselines like this is good practice at the start of the feature engineering process. A baseline score can help you decide whether your new features are worth keeping, or whether you should discard them and possibly try something else.","metadata":{}},{"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"CompressiveStrength\")\n\n# Train and score baseline model\nbaseline = RandomForestRegressor(criterion=\"mae\", random_state=0)\nbaseline_score = cross_val_score(\n    baseline, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nbaseline_score = -1 * baseline_score.mean()\n\nprint(f\"MAE Baseline Score: {baseline_score:.4}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you ever cook at home, you might know that the *ratio* of ingredients in a recipe is usually a better predictor of how the recipe turns out than their absolute amounts. We might reason then that ratios of the features above would be a good predictor of `CompressiveStrength`.\n\nThe cell below adds three new ratio features to the dataset.","metadata":{}},{"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"CompressiveStrength\")\n\n# Create synthetic features\nX[\"FCRatio\"] = X[\"FineAggregate\"] / X[\"CoarseAggregate\"]\nX[\"AggCmtRatio\"] = (X[\"CoarseAggregate\"] + X[\"FineAggregate\"]) / X[\"Cement\"]\nX[\"WtrCmtRatio\"] = X[\"Water\"] / X[\"Cement\"]\n\n# Train and score model on dataset with additional ratio features\nmodel = RandomForestRegressor(criterion=\"mae\", random_state=0)\nscore = cross_val_score(\n    model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nscore = -1 * score.mean()\n\nprint(f\"MAE Score with Ratio Features: {score:.4}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And sure enough, performance improved! This is evidence that these new ratio features exposed important information to the model that it wasn't detecting before.","metadata":{}},{"cell_type":"markdown","source":"# Feature creation\n\nOnce you've identified a set of features with some potential, it's time to start developing them. In this lesson, you'll learn a number of common transformations you can do entirely in Pandas.\n\nWe'll use four datasets in this lesson having a range of feature types: [*US Traffic Accidents*](https://www.kaggle.com/sobhanmoosavi/us-accidents), [*1985 Automobiles*](https://www.kaggle.com/toramky/automobile-dataset), [*Concrete Formulations*](https://www.kaggle.com/sinamhd9/concrete-comprehensive-strength), and [*Customer Lifetime Value*](https://www.kaggle.com/pankajjsh06/ibm-watson-marketing-customer-value-data). The following hidden cell loads them up.","metadata":{}},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\naccidents = pd.read_csv(\"../input/fe-course-data/accidents.csv\")\nautos = pd.read_csv(\"../input/fe-course-data/autos.csv\")\nconcrete = pd.read_csv(\"../input/fe-course-data/concrete.csv\")\ncustomer = pd.read_csv(\"../input/fe-course-data/customer.csv\")","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n<strong>Tips on Discovering New Features</strong>\n<ul>\n<li>Understand the features. Refer to your dataset's <em>data documentation</em>, if available.\n<li>Research the problem domain to acquire <strong>domain knowledge</strong>. If your problem is predicting house prices, do some research on real-estate for instance. Wikipedia can be a good starting point, but books and <a href=\"https://scholar.google.com/\">journal articles</a> will often have the best information.\n<li>Study previous work. <a href=\"https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions\">Solution write-ups</a> from past Kaggle competitions are a great resource.\n<li>Use data visualization. Visualization can reveal pathologies in the distribution of a feature or complicated relationships that could be simplified. Be sure to visualize your dataset as you work through the feature engineering process.\n<ul>\n</blockquote>\n\n# Mathematical Transforms #\n\nRelationships among numerical features are often expressed through mathematical formulas, which you'll frequently come across as part of your domain research. In Pandas, you can apply arithmetic operations to columns just as if they were ordinary numbers.\n\n### Ratio\nIn the *Automobile* dataset are features describing a car's engine. Research yields a variety of formulas for creating potentially useful new features. The \"stroke ratio\", for instance, is a measure of how efficient an engine is versus how performant:","metadata":{}},{"cell_type":"code","source":"autos[\"stroke_ratio\"] = autos.stroke / autos.bore\n\nautos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*bore: the diameter of a cylinder in a piston engine or a steam locomotive\\\nstroke: height of a cylinder*\n\n### Formula\nThe more complicated a combination is, the more difficult it will be for a model to learn, like this formula for an engine's \"displacement\" (overall cylinder volume), a measure of its power:","metadata":{}},{"cell_type":"code","source":"autos[\"displacement\"] = (\n    np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logarithm\nData visualization can suggest transformations, often a \"reshaping\" of a feature through powers or logarithms. The distribution of `WindSpeed` in *US Accidents* is highly skewed, for instance. In this case the logarithm is effective at normalizing it:","metadata":{}},{"cell_type":"code","source":"# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\naccidents[\"LogWindSpeed\"] = accidents.WindSpeed.apply(np.log1p)\n\n# Plot a comparison\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\nsns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])\nsns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Counts #\n\nFeatures describing the presence or absence of something often come in sets, the set of risk factors for a disease, say. You can aggregate such features by creating a **count**.\n\n### 1/0 or True/False counted (per row)\nThese features will be *binary* (`1` for Present, `0` for Absent) or *boolean* (`True` or `False`). In Python, booleans can be added up just as if they were integers.\n\nIn *Traffic Accidents* are several features indicating whether some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the `sum` method:","metadata":{}},{"cell_type":"code","source":"roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n    \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n    \"TrafficCalming\", \"TrafficSignal\"]\n\naccidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1) # axis=1: sum over rows, not columns\n\naccidents[roadway_features + [\"RoadwayFeatures\"]].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Count features over a threshold\nYou could also use a dataframe's built-in methods to *create* boolean values. In the *Concrete* dataset are the amounts of components in a concrete formulation. Many formulations lack one or more components (that is, the component has a value of 0). This will count how many components are in a formulation with the dataframe's built-in greater-than `gt` method:","metadata":{}},{"cell_type":"code","source":"components = [ \"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\",\n               \"Superplasticizer\", \"CoarseAggregate\", \"FineAggregate\"]\n\nconcrete[\"Components\"] = concrete[components].gt(0).sum(axis=1)\n\nconcrete[components + [\"Components\"]].sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building-Up and Breaking-Down Features #\n\nOften you'll have complex strings that can usefully be broken into simpler pieces. Some common examples:\n- ID numbers: `'123-45-6789'`\n- Phone numbers: `'(999) 555-0123'`\n- Street addresses: `'8241 Kaggle Ln., Goose City, NV'`\n- Internet addresses: `'http://www.kaggle.com`\n- Product codes: `'0 36000 29145 2'`\n- Dates and times: `'Mon Sep 30 07:06:05 2013'`\n\nFeatures like these will often have some kind of structure that you can make use of. US phone numbers, for instance, have an area code (the `'(999)'` part) that tells you the location of the caller. As always, some research can pay off here.\n\n### String methods such as `split`\nThe `str` accessor lets you apply string methods like `split` directly to columns. The *Customer Lifetime Value* dataset contains features describing customers of an insurance company. From the `Policy` feature, we could separate the `Type` from the `Level` of coverage:","metadata":{}},{"cell_type":"code","source":"customer[[\"Type\", \"Level\"]] = (  # Create two new features\n    customer[\"Policy\"]           # from the Policy feature\n    .str                         # through the string accessor\n    .split(\" \", expand=True)     # by splitting on \" \"\n                                 # and expanding the result into separate columns\n)\n\ncustomer[[\"Policy\", \"Type\", \"Level\"]].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To extract information from more complex patterns as in phone no. `'(999) 555-0123'`, regular expressions (regex) are helpful.\n\n### Join string features\nYou could also join simple features into a composed feature if you had reason to believe there was some interaction in the combination:","metadata":{}},{"cell_type":"code","source":"autos[\"make_and_style\"] = autos[\"make\"] + \"_\" + autos[\"body_style\"]\nautos[[\"make\", \"body_style\", \"make_and_style\"]].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n<strong>Elsewhere on Kaggle Learn</strong><br>\nThere are a few other kinds of data we haven't talked about here that are especially rich in information. Fortunately, we've got you covered!\n<ul>\n<li> For <strong>dates and times</strong>, see <a href=\"https://www.kaggle.com/alexisbcook/parsing-dates\">Parsing Dates</a> from our Data Cleaning course.\n<li> For <strong>latitudes and longitudes</strong>, see our <a href=\"https://www.kaggle.com/learn/geospatial-analysis\">Geospatial Analysis</a> course.\n<li> For <strong>text</strong>, try <a href=\"https://www.kaggle.com/learn/natural-language-processing\">Natural Language Processing</a>.\n</ul>\n</blockquote>","metadata":{}},{"cell_type":"markdown","source":"# Group Transforms #\n\nFinally we have **Group transforms**, which aggregate information across multiple rows grouped by some category. With a group transform you can create features like: \"the average income of a person's state of residence,\" or \"the proportion of movies released on a weekday, by genre.\" If you had discovered a category interaction, a group transform over that categry could be something good to investigate.\n\n### Encode categories as attributes of each category\n\nUsing an aggregation function, a group transform combines two features: a categorical feature that provides the grouping and another feature whose values you wish to aggregate. For an \"average income by state\", you would choose `State` for the grouping feature, `mean` for the aggregation function, and `Income` for the aggregated feature. To compute this in Pandas, we use the `groupby` and `transform` methods:","metadata":{}},{"cell_type":"code","source":"customer.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer[\"AverageIncome\"] = (\n    customer.groupby(\"State\")  # for each state\n    [\"Income\"]                 # select the income\n    .transform(\"mean\")         # and compute its mean\n)\n\ncustomer[[\"Customer\", \"State\", \"Income\", \"AverageIncome\"]].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `mean` function is a built-in dataframe method, which means we can pass it as a string to `transform`. Other handy methods include `max`, `min`, `median`, `var`, `std`, and `count`. Here's how you could calculate the frequency with which each state occurs in the dataset:\n","metadata":{}},{"cell_type":"code","source":"customer[\"StateFreq\"] = (\n    customer.groupby(\"State\")\n    [\"State\"]\n    .transform(\"count\")            # number of customers in the state\n    / customer.Customer.count()    # number of customers in the dataset (pick any non-empty column)\n)                                  # The ratio is the frequency of the state in the dataset.\n\ncustomer[[\"Customer\", \"State\", \"StateFreq\"]].head(10)","metadata":{"lines_to_next_cell":0,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You could use a transform like this to create a \"frequency encoding\" for a categorical feature.\n\n### Bonus: train-test split merging\n\nIf you're using training and validation splits, to preserve their independence, it's best to create a grouped feature using only the training set and then join it to the validation set. We can use the validation set's `merge` method after creating a unique set of values with `drop_duplicates` on the training set:","metadata":{}},{"cell_type":"code","source":"# Create splits\ndf_train = customer.sample(frac=0.5)\ndf_valid = customer.drop(df_train.index)\n\n# Create the average claim amount by coverage type, on the training set\ndf_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")\n\n# Merge the values into the validation set\ndf_valid = df_valid.merge(\n    df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(),\n    on=\"Coverage\",\n    how=\"left\",\n)\n\ndf_valid[[\"Coverage\", \"AverageClaim\"]].head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features for model types\n\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n<strong>Tips on Creating Features</strong><br>\nIt's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:\n<ul>\n<li> Linear models learn <b>sums and differences</b> naturally, but can't learn anything more complex.\n<li> <b>Ratios</b> seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n<li> <b>Linear models and neural nets</b> generally do better with <b>normalized features</b>. Neural nets especially need features scaled to values not too far from 0. <b>Tree-based models</b> (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n<li> Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n<li> <b>Counts</b> are especially helpful for <b>tree models</b>, since these models don't have a natural way of aggregating information across many features at once.\n</ul>\n</blockquote>","metadata":{}},{"cell_type":"markdown","source":"# Good luck and have fun!\n\nSource:\n- https://www.kaggle.com/learn/feature-engineering\n\nExercise:\n- [**Combine and transform features**](https://www.kaggle.com/kernels/fork/14393912) from *Ames* and improve your model's performance.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/221677) to chat with other Learners.*","metadata":{}}]}