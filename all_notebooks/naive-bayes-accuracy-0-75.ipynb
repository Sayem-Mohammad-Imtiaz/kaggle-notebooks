{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pipeline:\n<br>- удаление пропущенных значений\n<br>- label encoding без Title\n<br>- проверка распределения признаков\n<br>- выделение X, y\n<br>- балансировка классов\n<br>- нормализация Х для кросс-валидации\n<br>- разделение на train-/test-части\n<br>- нормализация train/test по mean и std от train\n<br>- кросс-валидация для определения параметров обеспечивающих наибольший f1-score(если задача классификации) или наименьший r2-score(если регрессия)\n<br>- обучение модели с оптимальными параметрами ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Загрузка данных","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/dataisbeautiful/r_dataisbeautiful_posts.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Работа с признаками","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Удаление пропущенных значений","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# проверим пропущенные значения\nfig, ax = plt.subplots(figsize=(14,10))\nsns.heatmap(df.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# удалим колонки с пропущенными значениями\n#df.drop(columns=['id', 'author_flair_text', 'removed_by', 'created_utc', 'full_link'], inplace=True)\ndf.drop(columns=['id', 'author_flair_text', 'removed_by', 'total_awards_received', 'awarders', 'created_utc', 'full_link'], inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверим пропущенные значения еще раз\nfig, ax = plt.subplots(figsize=(14,10))\nsns.heatmap(df.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace = True)\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label encoding без Title","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Подключаем класс для предобработки категориальных признаках\nfrom sklearn import preprocessing\n\n# Напишем функцию, которая принимает на вход DataFrame, кодирует числовыми значениями категориальные признаки\n# и возвращает обновленный DataFrame и сами кодировщики.\ndef number_encode_features(init_df):\n    result = init_df.copy() # копируем нашу исходную таблицу\n    encoders = {}\n    for column in result.columns:\n        if result.dtypes[column] == np.object: # np.object -- строковый тип / если тип столбца - строка, то нужно его закодировать\n            encoders[column] = preprocessing.LabelEncoder() # для колонки column создаем кодировщик\n            result[column] = encoders[column].fit_transform(result[column]) # применяем кодировщик к столбцу и перезаписываем столбец\n    return result, encoders\n\n# кодируем все, что можно кроме колонки \"title\"\nencoded_data, encoders = number_encode_features(df.drop(columns='title')) # Теперь encoded data содержит закодированные кат. признаки \nencoded_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Проверка распределения признаков, поиск коррелирующих","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на распределение признаков\nnon_obj_cols = []\nfor column in encoded_data.columns:\n        if df.dtypes[column] != np.bool:\n            non_obj_cols.append(column)\n\n            \nfig = plt.figure(figsize=(16,8))\ncols = 3\n\nrows = np.ceil(float(encoded_data[non_obj_cols].shape[1]) / cols)\nfor i, column in enumerate(encoded_data[non_obj_cols].columns):\n    ax = fig.add_subplot(rows, cols, i + 1)\n    ax.set_title(column)\n    encoded_data[non_obj_cols][column].hist(axes=ax)\n    plt.xticks(rotation=\"vertical\")\nplt.subplots_adjust(hspace=0.7, wspace=0.2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# проверим ко\nplt.subplots(figsize=(12, 10))\nsns.heatmap(encoded_data.corr(), square = True, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.groupby(['over_18'])['over_18'].count())\nprint(f\"Доля значений \\'over_18\\'==True : {round(len(df[df.over_18==True])/len(df), 4)}\")\nlabels = (df['over_18'].unique())\ny_pos = np.arange(len(labels))\namount = df.groupby(['over_18'])['over_18'].count().tolist()\nplt.bar(y_pos, amount)[1].set_color('orange')\nplt.xticks(y_pos, labels)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### X, y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(encoded_data.drop(['over_18'], axis=1))\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(encoded_data['over_18'].astype(int))\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import scale\n# X_scaled = scale(np.array(X, dtype='float'), with_std=True, with_mean=True)\n# X_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Балансировка классов","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.argwhere вернет индексы тех элементов массива y (целевой переменной), где значение 0\nnot_over_18_ids = np.argwhere(y == 0).flatten()\nprint('Всего не 18+', len(not_over_18_ids))\nnot_over_18_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Перемешаем массив с выбранным random state (чтоб в дальнейшем у нас совпадали выборки) выберем в нем \"лишние\" id (delta). \n<br>delta = not_over_18 - over_18.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\nnot_over_18_ids = shuffle(not_over_18_ids, random_state = 42)\n# найдем \"лишних\", для этого обрежем найденные id на кол-во over_18 (внутри len)\nnot_over_18_ids = not_over_18_ids[len(np.argwhere(y == 1).flatten()):]\nprint(len(not_over_18_ids))\n# отображаем кол-во и сами id, которые мы должны выкинуть\nnot_over_18_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Проверим, сбалансированны ли классы","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# 182948(всего нулей) - 182005(нулей после обрезки на количество единиц) = 943(осталось нулей, должно быть равно кол-ву единиц)\nlen(np.argwhere(y == 0).flatten()) - len(not_over_18_ids) == len(np.argwhere(y == 1).flatten())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Теперь можно выкинуть \"лишние\" id из X и y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# из X и y выкинем избыточные нули (в количестве 182005)\n# np.delete принимает массив, индексы, которые выбросить и по какой оси выкидывать\nX = np.delete(X, not_over_18_ids, 0)\n#X_scaled = np.delete(X_scaled, not_over_18_ids, 0)\ny = np.delete(y, not_over_18_ids, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pd.Series(y).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Нормализация неразбитого набора для CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Нормализуем набор весь набор для кросс-валидации\nfrom sklearn.preprocessing import scale\nX_scaled = scale(np.array(X, dtype='float'), with_std=True, with_mean=True)\nX_scaled","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#a = df.loc[df.index.difference(df.iloc[not_over_18_ids].index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Разделение набора на train/test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# разбиваем отбалансированные, но ненормализованные\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Нормализация train- и test-частей","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = X_train.mean(axis=0)\nstd = X_train.std(axis=0)\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Алгоритмы","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\ngrid = {'n_neighbors': np.array(np.linspace(30, 50, 20), dtype='int')}\ngs = GridSearchCV(knn, grid)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = gs.best_params_['n_neighbors'], n_jobs=-1)\n\nknn.fit(X_train, y_train)\npreds = knn.predict(X_test)\nknn_res = metrics.classification_report(y_test, preds)\nprint(knn_res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alg = SVC()\n\ngrid = {'C': np.array(np.linspace(0.1, 5, 10), dtype='float'),\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n        }\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(C=gs.best_params_['C'], kernel = gs.best_params_['kernel'])\n\nsvm.fit(X_train, y_train)\npreds = svm.predict(X_test)\nsvm_res = metrics.classification_report(y_test, preds)\nprint(svm_res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Логистическая","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alg = LogisticRegression()\n\ngrid = {'penalty': ['l1', 'l2'],\n        'C': np.array(np.logspace(-3, 2, num = 10), dtype='float'),\n        }\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(penalty=gs.best_params_['penalty'], C = gs.best_params_['C'])\n\nlogreg.fit(X_train, y_train)\npreds = logreg.predict(X_test)\nlogreg_res = metrics.classification_report(y_test, preds)\nprint(logreg_res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SGD","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sgd = SGDRegressor()\n\ngrid = {'penalty': ['l1', 'l2'],\n        'alpha': [1e-4, 1e-5, 1e-6, 1e-7]}\n\ngs = GridSearchCV(sgd, grid, verbose = 2, scoring = 'r2')\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGDRegressor(alpha = gs.best_params_['alpha'], penalty = gs.best_params_['penalty'])\nsgd.fit(X_train, y_train)\npreds = sgd.predict(X_test)\nsgd_res = metrics.r2_score(y_test, preds)\nprint('R2 sgd (sklearn): ', sgd_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_test - preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GBR","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor()\n\ngrid = {'max_depth': [3, 4, 5],\n        'min_samples_split': [2, 3, 4, 5]}\n\ngs = GridSearchCV(gbr, grid, verbose = 2)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(max_depth = gs.best_params_['max_depth'], min_samples_split = gs.best_params_['min_samples_split'])\ngbr.fit(X_train, y_train)\npreds = gbr.predict(X_test)\ngbr_res = metrics.r2_score(y_test, preds)\nprint('R2 gbr: ', gbr_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_test - preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_test)\nplt.hist(preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data+tfidf","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Загрузка, удаление пустых колонок/строк","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/dataisbeautiful/r_dataisbeautiful_posts.csv')\ndf.drop(columns=['id', 'author_flair_text', 'removed_by', 'total_awards_received', 'awarders', 'created_utc', 'full_link'], inplace=True)\ndf.dropna(inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Предобработка колонки title","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n# реализуем предобработку\ndef preprocess(doc):\n    # к нижнему регистру\n    doc = doc.lower()\n    # убираем пунктуацию, пробелы, прочее\n    for p in string.punctuation + string.whitespace:\n        doc = doc.replace(p, ' ')\n    # убираем лишние пробелы, объединяем обратно\n    doc = doc.strip()\n    doc = ' '.join([w for w in doc.split(' ') if w != ''])\n    return doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  применим к этим столбцам нашу функцию понижения текста\nfor colname in df.select_dtypes(include= np.object).columns:\n    df[colname] = df[colname].map(preprocess)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label encoding за исключением title","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"encoded_data, encoders = number_encode_features(df.drop(columns='title')) # Теперь encoded data содержит закодированные кат. признаки \nencoded_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### X, y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(encoded_data['over_18'].astype(int))\nX = np.array(encoded_data.drop(['over_18'], axis=1))\n\nfrom sklearn.preprocessing import scale\nX_scaled = scale(np.array(X, dtype='float'), with_std=True, with_mean=True)\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF для колонки title и её присоединение к подготовленному набору данных","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# импортируем tfidf преобразование\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# инициализировали алгоритм\nvectorizer = TfidfVectorizer(max_features = 100)\n# преобразовали его в матрицу tfidf как в примере на картинке выше\nX_np = vectorizer.fit_transform(df['title'].values)\n# отобразили его размерность\nprint(X_np.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel df, encoded_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# т.к. сам тип матрицы из scipy - преобразуем в tfidf\nX_np = X_np.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# отобразим произвольные слова\n#print(vectorizer.get_feature_names()[13000:13010])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# добавление TF-IDF к X\nX = np.append(X, X_np, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Балансировка классов","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.delete(X, not_over_18_ids, 0)\ny = np.delete(y, not_over_18_ids, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(y).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Нормализация неразбитого набора для CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled = scale(np.array(X, dtype='float'), with_std=True, with_mean=True)\nX_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Разделение на train/test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# разбиваем отбалансированные, но ненормализованные\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = X[:int(len(X)*0.8)]\n# y_train = y[:int(len(X)*0.8)]\n\n# X_test = X[int(len(X)*0.8):]\n# y_test = y[int(len(X)*0.8):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Нормализация для train/test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = X_train.mean(axis=0)\nstd = X_train.std(axis=0)\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\ngrid = {'n_neighbors': np.array(np.linspace(30, 50, 20), dtype='int')}\ngs = GridSearchCV(knn, grid)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = gs.best_params_['n_neighbors'], n_jobs=-1)\n\nknn.fit(X_train, y_train)\npreds = knn.predict(X_test)\nknn_tfidf = metrics.classification_report(y_test, preds)\nprint(knn_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alg = SVC()\n\ngrid = {'C': np.array(np.linspace(0.1, 5, 10), dtype='float'),\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n        }\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(C=gs.best_params_['C'], kernel = gs.best_params_['kernel'])\n\nsvm.fit(X_train, y_train)\npreds = svm.predict(X_test)\nsvm_tfidf = metrics.classification_report(y_test, preds)\nprint(svm_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Логистическая","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alg = LogisticRegression()\n\ngrid = {'penalty': ['l1', 'l2'],\n        'C': np.array(np.logspace(-3, 2, num = 10), dtype='float'),\n        }\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(penalty=gs.best_params_['penalty'], C = gs.best_params_['C'])\n\nlogreg.fit(X_train, y_train)\npreds = logreg.predict(X_test)\nlogreg_tfidf = metrics.classification_report(y_test, preds)\nprint(logreg_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SGD","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sgd = SGDRegressor()\n\ngrid = {'penalty': ['l1', 'l2'],\n        'alpha': [1e-4, 1e-5, 1e-6, 1e-7]}\n\ngs = GridSearchCV(sgd, grid, verbose = 2, scoring = 'r2')\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGDRegressor(alpha = gs.best_params_['alpha'], penalty = gs.best_params_['penalty'])\nsgd.fit(X_train, y_train)\npreds = sgd.predict(X_test)\nsgd_tfidf = metrics.r2_score(y_test, preds)\nprint('R2 sgd (sklearn): ', sgd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_test - preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GBR","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor()\n\ngrid = {'max_depth': [3, 4, 5],\n        'min_samples_split': [2, 3, 4, 5]}\n\ngs = GridSearchCV(gbr, grid, verbose = 2)\ngs.fit(X_scaled, y)\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(max_depth = gs.best_params_['max_depth'], min_samples_split = gs.best_params_['min_samples_split'])\ngbr.fit(X_train, y_train)\npreds = gbr.predict(X_test)\ngbr_tfidf = metrics.r2_score(y_test, preds)\nprint('R2 gb: ', gbr_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_test - preds)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.hist(y_test)\nplt.hist(preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Результаты","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('KNN\\n', knn_res, knn_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SVM\\n', svm_res, svm_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LogReg\\n', logreg_res, logreg_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SGD\\n', sgd_res, sgd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GBR\\n', gbr_res, gbr_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Вывод","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Видим, что:\n<br>1) добавление в каждое наблюдение-строку TF-IDF-признаков для наиболее часто встречаемых по колонке <b>title</b> слов, из его предложения, увеличивает точность алгоритмов классификации, но уменьшает для регрессии\n<br>2) cамый хороший результат, из примененных алгоритмов, у классификации с помощью <b>логистической регрессии</b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes + stop words + ngrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.naive_bayes import ComplementNB, MultinomialNB, BernoulliNB\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/dataisbeautiful/r_dataisbeautiful_posts.csv')\ndf.drop(columns=['id', 'author_flair_text', 'removed_by', 'total_awards_received', 'awarders', 'created_utc', 'full_link'], inplace=True)\ndf.dropna(inplace = True)\ndf['over_18'] = df['over_18'].astype(int)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes - нелинейный алгоритм. Будем прогнозировать <b>over_18</b> по колонке <b>title</b> без:\n<br>- LabelEncoding\n<br>- нормализации\n<br>- балансировки классов (т.к. используем ComplementNB)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download(\"stopwords\")\n\nfrom nltk.corpus import stopwords\nstopWords = stopwords.words('english')\n\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n\n# ngram_range=(1, 2) - это сами слова (unigrams) и пары слов(bigrams)\nvectorizer = TfidfVectorizer(stop_words=stopWords + list(ENGLISH_STOP_WORDS), ngram_range=(1, 2))\nvectorizer = vectorizer.fit(df['title'])\n\nX_train_vectors = vectorizer.transform(df_train['title'])\nX_test_vectors = vectorizer.transform(df_test['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# массив значений разреженной матрицы 65-й строки в CSR-формате \nnum = 65\nX_train_vectors[num].data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# исходный title 65-й строки\ndf_train['title'].iloc[65]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Выведем слова и пары слов, составляющие title 65-й строки, в порядке увеличения их меры TF-IDF:\nvectorizer.inverse_transform(X_train_vectors[num])[0][np.argsort(X_train_vectors[num].data)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Алгоритм","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nalg = ComplementNB()\n\ngrid = {'alpha': np.array(np.linspace(0, 6, 30), dtype='float'),}\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1, scoring = 'f1')\ngs.fit(X_train_vectors, df_train['over_18'])\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Функция отрисовки графиков\ndef grid_plot(x, y, x_label, title, y_label='f1'):\n    # определили размер графика\n    plt.figure(figsize=(12, 6))\n    # добавили сетку на фон\n    plt.grid(True)\n    # построили по х - число соседей, по y - точность\n    plt.plot(x, y, 'go-')\n    # добавили подписи осей и название графика\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_plot(grid['alpha'], gs.cv_results_['mean_test_score'], 'alpha', 'ComplementNB')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"clf = ComplementNB(alpha = gs.best_params_['alpha'])\n\nclf.fit(X_train_vectors, df_train['over_18'])\npredicts = clf.predict(X_test_vectors)\ncompnb_tfidf_ngr_stpwds = classification_report(df_test['over_18'], predicts)\nprint(compnb_tfidf_ngr_stpwds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Вывод: разбалансированность набора данных значительно ухудшает результаты модели, не смотря на применение ComplementNB. Нужна балансировка.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Балансировка","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/dataisbeautiful/r_dataisbeautiful_posts.csv')\ndf.drop(columns=['id', 'author_flair_text', 'removed_by', 'total_awards_received', 'awarders', 'created_utc', 'full_link'], inplace=True)\ndf.dropna(inplace = True)\ndf['over_18'] = df['over_18'].astype(int)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# np.argwhere вернет индексы тех элементов массива y (целевой переменной), где значение 0\nnot_over_18_ids = np.argwhere(np.array(df['over_18']) == 0).flatten()\nprint('Всего не 18+', len(not_over_18_ids))\nnot_over_18_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Перемешаем массив с выбранным random state (чтоб в дальнейшем у нас совпадали выборки) выберем в нем \"лишние\" id (delta). \n<br>delta = not_over_18 - over_18.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\nnot_over_18_ids = shuffle(not_over_18_ids, random_state = 42)\n# найдем \"лишних\", для этого обрежем найденные id на кол-во over_18 (внутри len)\nnot_over_18_ids = not_over_18_ids[len(np.argwhere(np.array(df['over_18']) == 1).flatten()):]\nprint(len(not_over_18_ids))\n# отображаем кол-во и сами id, которые мы должны выкинуть\nnot_over_18_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Теперь можно выкинуть \"лишние\" id из X и y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# из X и y выкинем избыточные нули\ndf = df.loc[df.index.difference(df.iloc[not_over_18_ids].index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(df['over_18']).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Обучим алгоритм на отбалансированных данных","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = train_test_split(df, test_size=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = vectorizer.fit(df['title'])\n\nX_train_vectors = vectorizer.transform(df_train['title'])\nX_test_vectors = vectorizer.transform(df_test['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nalg = ComplementNB()\n\ngrid = {'alpha': np.array(np.linspace(0, 6, 30), dtype='float'),}\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1, scoring = 'f1')\ngs.fit(X_train_vectors, df_train['over_18'])\ngs.best_params_, gs.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_plot(grid['alpha'], gs.cv_results_['mean_test_score'], 'alpha', 'ComplementNB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = ComplementNB(alpha = gs.best_params_['alpha'])\n\nclf.fit(X_train_vectors, df_train['over_18'])\npredicts = clf.predict(X_test_vectors)\ncompnb_tfidf_ngr_stpwds = classification_report(df_test['over_18'], predicts)\nprint(compnb_tfidf_ngr_stpwds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LogReg\\n', logreg_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Вывод","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Complement Naive Bayes</b>, для данной задачи, показал результат лучше, чем <b>лог.регрессия</b> (лучший по результатам из линейных алгоритмов).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}