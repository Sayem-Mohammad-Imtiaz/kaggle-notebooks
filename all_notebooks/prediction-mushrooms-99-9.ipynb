{"cells":[{"metadata":{},"cell_type":"markdown","source":"### INTRODUCTION\n\n#### Predicting the mushrooms mainly through logistic regression\n#### Mushroom class was classified into two categories  poisonous(0) and edible(1)\n#### Steps taken in preprocessing includes Data cleaning,etc\n#### All our variables in this dataset are categorical\n\n### SIDE NOTE\n#### You can leave your question about any unclear part in the comment section\n#### Any correction will be highly welcomed"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOADING THE DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/mushroom-classification/mushrooms.csv'\n\ndf = pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DEALING WITH MISSING VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This dataset is clean it does not have any missing value"},{"metadata":{},"cell_type":"markdown","source":"### DUMMY VARIABLES"},{"metadata":{},"cell_type":"markdown","source":"#### Our target column 'class' contains two unique values 'e' and 'p' which we will map to '1' and '0' respectively\n#### note 'e' stands for edible while 'p' for  poisonous"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing e with 1\n#Replacing  p with 0\ndf['class'] = df['class'].map({'e':1, 'p':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Note that all our independent varibles are categorical variables\n#Using attribute get_dummies to convert conver our category variables into dummy indicator\ndf_dummies = pd.get_dummies(df, drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOGISTIC REGRESSION\n#### Here we will create, fit and train our model in addition to that we will try to predict using the already trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Declaring our target variable as y\n#Declaring our independent variables as x\nx = df_dummies.drop('class', axis = 1)\ny = df_dummies['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting the model\nreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting our dataset into train and test datasets \nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We train the model with x_train and y_train\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting with our already trained model using x_test\ny_hat = reg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mesuring the accuracy of our model\nacc = metrics.accuracy_score(y_hat, y_test)\nacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The intercept for our regression\nreg.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Coefficient for all our variables\nreg.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CONFUSION MATRIX"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_hat, y_test)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Format for easier understanding\ncm_df = pd.DataFrame(cm)\ncm_df.columns = ['Predicted 0','Predicted 1']\ncm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\ncm_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our model predicted '0' correctly 755 times while NEVER predicting '0' incorrectly \n#### Also it predicted  '1'  correctly 869 times while predicting '1' incorrectly  ONCE"},{"metadata":{},"cell_type":"markdown","source":"### OTHER MODELS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier # for K nearest neighbours\nfrom sklearn import svm #for Support Vector Machine (SVM) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny1 = dt.predict(x_test)\nacc1 = metrics.accuracy_score(y1, y_test)\nacc1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kk = KNeighborsClassifier()\nkk.fit(x_train,y_train)\ny2 = kk.predict(x_test)\nacc2 = metrics.accuracy_score(y2, y_test)\nacc2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sv = svm.SVC()\nsv.fit(x_train,y_train)\ny3 = sv.predict(x_test)\nacc3 = metrics.accuracy_score(y3, y_test)\nacc3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### After comparison with some other model we see that Decision tree and KNeigbors both gave us 100% accuracy but our model was close enough with ~99.9% accuracy "},{"metadata":{},"cell_type":"markdown","source":"###  CONCLUSION\n#### Let's try to make a table and interpret what weight(BIAS) and odds means"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_rows = 999\nresult = pd.DataFrame(data = x.columns, columns = ['Features'])\nresult['BIAS'] = np.transpose(reg.coef_)\nresult['odds'] = np.exp(np.transpose(reg.coef_))\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#T be able to identify our refernce model\ndf['cap-shape'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using feature cap-shape as an example cap-shape_b is our baseline model i.e all cap-shape feature will be referenced to it and note that it is the only one that doesn't appear in the table above"},{"metadata":{},"cell_type":"markdown","source":"#### From the dataset description which can be found on kaggle 'b' represent bell while 's' represent sunken\n#### To interpret cap-shape_s in terms of odds we can say that  cap-shape_s  is almost twice more likely to cause a change in our target varable than cap-shape_b(our baseline model)"},{"metadata":{},"cell_type":"markdown","source":"#### If you find this notebook useful don't forget to upvote. #Happycoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}