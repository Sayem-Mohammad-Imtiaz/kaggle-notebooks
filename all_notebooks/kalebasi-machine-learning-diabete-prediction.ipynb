{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\n* In this kernel, we will investigate Indian diabete data and try to apply logistic regression.\n\n<br>Content:\n1. [Import Libraries](#1)\n1. [Reading Data](#2)\n1. [Normalization](#4)\n1. [Train/Test Split](#5)\n1. [Parameter initialize and sigmoid function](#6)\n1. [Updating Parameters](#7)\n1. [Prediction](#8)\n1. [Logistic Regression](#9)\n1. [Test the Model](#10)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom IPython.display import Image\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# Reading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# =============================================================================\n# Means of Columns\n\n* Pregnanices: Number of times pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skin fold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U/ml)\n* BMI: Body mass index (weight in kg/(height in m)^2)\n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0\n\n# ============================================================================="},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = data.drop([\"Outcome\"],axis=1)\ny = data.Outcome.values\n\n# we seperate the result( Outcome column ) and other variables from each other.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# Normalization"},{"metadata":{},"cell_type":"markdown","source":"* Features which have too much numeric value can be dominated less numeric value features\n* so, we normalize our data to predict most truth machine learning model.\n* Normalizing is compress the data values between 0-1 as proportionally."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n\n# we will do matrice product, for matrice product first matrix's column and second matrix's row must be same so we will transpose our train/test data\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### now, features are row and values are column"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n# Parameter Initialize and Sigmoid Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_weights_and_bias(sizeofcolumn):\n    w = np.full((sizeofcolumn,1),0.01)\n    b = 0.00\n    return w,b\n\n# w = weights, b = bias ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    #sigmoid function returns y_head value\n    y_head = (1 / ( 1 + np.exp(-z))) # its formula of sigmoid func.\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # we must use weights and bias for training model\n    # we must change w and b for appropriate shape to matrice product\n    \n    z = np.dot(w.T,x_train) + b\n    \n    y_head = sigmoid(z)\n    \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) #thats formula for our wrong predictions\n    cost = (np.sum(loss))/x_train.shape[1] # thats average of loss \n    # forward propagation is completed\n    \n    # backward propagation\n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T))) / x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train) / x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n    #this func loop 1 times but we want to update our data as we learn new datas.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# Updating Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w,b,x_train,y_train,learning_rate,loopnumber):\n    \n    cost_list = []\n    cost_list2  =[]\n    index = []\n    \n    # updating(learning) parameters is loopnumber times\n    for i in range(1,loopnumber):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        \n        # updating \n        w = w - learning_rate*gradients[\"derivative_weight\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        \n        # we may want information about progress\n        if( i % 10 == 0):\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after {} times loop: {}\".format(i,cost))\n        \n    # showing progress as visual is important\n    parameters = {\"weights\" : w,\"bias\" : b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Loop\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w,b,x_test):\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n            \n    # if z is bigger than 0.5, our prediction is sign one (y_head = 1)\n    # if z is smaller than 0.5, our prediction is sign zero ( y_head = 0)\n    \n    for i in range(1,x_test.shape[1]):\n        if (z[0,i] <= 0.5):\n            y_prediction[0,i] == 0\n        else:\n            y_prediction[0,i] == 1\n            \n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,loopnumber):\n    # initialize\n    sizeofcolumn = x_train.shape[0]\n    w,b = fill_weights_and_bias(sizeofcolumn)\n    \n    # forward and backward propagation\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,loopnumber)\n    \n    y_prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"], x_test)\n    # y_prediction_test our y values for test data now we will comparise each other\n    \n    #print test erros\n    print(\"Test accuracy is: {}\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100 ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n# Testing our Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 1, loopnumber=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}