{"nbformat_minor":1,"nbformat":4,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"name":"python36","display_name":"Python 3.6","language":"python"},"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","file_extension":".py","name":"python","version":"3.6.3","pygments_lexer":"ipython3"}},"cells":[{"source":" #                             *Analysis in Airbnb dataset in Lisbon*","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"source":"__Project for fall semester 2017 on \"Problem Solving Environments and Applications in Data Science\"__\n\n\n**Authors:**  \n    Christinakis Ioannis, \n    Foufikos Evangelos,\n    Ntontis Christos\n    ","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"source":"## Contents","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"source":"1. Preproccesing Dataset\n2. Plot dataset's variables \n\n 1. find avg Price according vs room's type\n 2. find avg Price according vs neighborhood and how many booked there\n 3. plot reviews vs prices\n 4. plot reviews vs overall satisfaction\n 5. plot overall satisfaction vs price\n \n \n3. Use Linear Regression with and without PCA to predict prices with one hot encoding for caterigal variables\n4. Use Logistic Regression with and without PCA to predict overall satisfaction with one hot encoding for categorical variables\n5. Repeat step 3 and 4 with label encoding\n    ","metadata":{"slideshow":{"slide_type":"subslide"}},"cell_type":"markdown"},{"outputs":[],"source":"# Import needed libraries\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn import cross_validation\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import Counter","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"\ndata= pd.read_csv('../input/airbnb_lisbon_1480_2017-07-27.csv', header=0)\n# replace NaN values with 0\ndata.fillna(0, inplace = True)\n\n#Extract prices from the table\nprice = data['price']\nprices = []\n\n#convert prices from strings to float\nfor p in price:\n    p = float(p)\n    prices.append(p)\n    \ndata['price'] = prices\n\n#drop data that dont contain information\ndata = data[data.bedrooms > 0]\ndata = data[data.price  > 0]\ndata = data[data.accommodates  > 0]\ndata = data[data.price < 700]","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"data.head()","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"}}},{"source":"# Categorize different listings based on room_type","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"roomType = data.groupby('room_type').room_id.count()\nroomType = roomType.reset_index()\nroomType = roomType.rename(columns = {'id':'number_Of_Listings'})\nroomType","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"subslide"}}},{"outputs":[],"source":"matplotlib.style.use('ggplot')\n\nroom = data.room_type\nr = Counter(room)\n\nroom = pd.DataFrame.from_dict(r, orient = 'index').sort_values(by = 0)\nroom.columns = ['room_type']\n\n\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"room.plot.pie(y = 'room_type', \n                 colormap = 'Blues_r', \n                 figsize = (10,10), \n                 fontsize = 20, autopct = '%.2f',\n                 legend = False,\n                 title = 'Room Type Distribution')","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"}}},{"source":"# Average prices for each type of listing","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"avgPrice = data.groupby('room_type').price.mean()\navgPrice = avgPrice.reset_index()\navgPrice = avgPrice.rename(columns = {'price':'average_Price'})","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"\navgPrice\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"outputs":[],"source":"average_price = sum(data.price) / float(len(data.price))\n# standard deviation to compare \nstd = np.std(data.price)\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"print(\"Overall Average Price:\", average_price)\nprint (\"standard deviation: \" + str(std))","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"source":"# Average prices for each type of listing according to neighborhoods","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"#neighborhood frequency\nneighborhood = Counter(data['neighborhood'])\n\n\nneighborhood_prices = data[['neighborhood', 'price']]\nneighborhood_prices.columns = ['neighborhood', 'price']\n\nneighborhood_prices = neighborhood_prices[neighborhood_prices['neighborhood'].isin(neighborhood)]\n\n# group by neighbourhood and find the mean price for each of them\nneighborhood_prices_group = neighborhood_prices.groupby('neighborhood')\nneighborhood_prices = neighborhood_prices_group['price'].agg(np.mean)\n\nneighborhood_prices = neighborhood_prices.reset_index()\nneighborhood_prices['number of listings'] = neighborhood.values()\n\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"neighborhood_prices","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"outputs":[],"source":"nh_df = pd.DataFrame.from_dict(neighborhood, orient = 'index').sort_values(by = 0)\nnh_df.plot(kind = 'bar', color = 'LightBlue', figsize = (15,8), title = 'SF Neighborhood Frequency', legend = False)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"subslide"}}},{"outputs":[],"source":"price_review = data[['reviews', 'price']].sort_values(by = 'price')\n\nprice_review.plot(x = 'price', y = 'reviews', style = 'o', figsize =(12,8), legend = False, title = 'Reviews based on Price')\nplt.xlim(-20, 750)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"subslide"}}},{"outputs":[],"source":"\noverall_satisfaction_review = data[['reviews', 'overall_satisfaction']].sort_values(by = 'overall_satisfaction')\n\noverall_satisfaction_review.plot(x = 'overall_satisfaction', y = 'reviews', style = 'o', figsize =(12,8), legend = False,\n                  title = 'Reviews based on Overall_satisfaction')\nplt.xlim(-1, 6)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"subslide"}}},{"outputs":[],"source":"overall_satisfaction_price = data[['price', 'overall_satisfaction']].sort_values(by = 'price')\n\noverall_satisfaction_price.plot(x = 'price', y = 'overall_satisfaction', style = 'o', figsize =(12,8), legend = False,\n                  title = 'Overall_satisfaction based on Price')","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"subslide"}}},{"source":"# Data preprocessing ","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"new_data = data[['price',\n           'room_type',\n           'accommodates',\n           #'bathrooms',\n           #'bedrooms',\n           'reviews',\n           'neighborhood',\n           'overall_satisfaction']]\n\n\nlb_nh = LabelEncoder()\nlb_rt = LabelEncoder()\n\n#one hot encoding\noh_neighborhood = pd.get_dummies(new_data.neighborhood).astype(int)\noh_room_type = pd.get_dummies(new_data.room_type).astype(int)\n\n#label encoding \nle_neighborhood = lb_nh.fit_transform(new_data[\"neighborhood\"])\nle_room_type = lb_rt.fit_transform(new_data['room_type'])\n\n# drop the original columns and replace them with indicator columns\nnew_data = new_data.drop(['room_type','neighborhood'], axis = 1)\nle_data = pd.DataFrame(new_data)\n\nle_neighborhood = pd.DataFrame(le_neighborhood)\nle_room_type = pd.DataFrame(le_room_type)\nle_data = pd.concat((new_data, le_room_type, le_neighborhood), axis = 1)\nle_data.columns = ['price',\n           'accommodates',\n           #'bathrooms',\n           #'bedrooms',\n           'reviews',\n           'overall_satisfaction',\n           'room_type',\n           'neighborhood']\n\nnew_data = pd.concat((new_data, oh_room_type, oh_neighborhood), axis = 1)\nle_data = le_data.dropna(axis=0, how='any')\nnew_data = new_data[:le_data.shape[0]]\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"new_data.head() #ONE-HOT Encoded Data","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"outputs":[],"source":"le_data.head() #LABEL Encoded Data","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"source":"# PCA  using ONE HOT encoding","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"#split the data and set price as target variable\ny = new_data['price']\nX = new_data.drop(['price'],axis=1)\n\n#standarize the dataset\nX_std = StandardScaler().fit_transform(X)\n\n# call PCA \npca = PCA(n_components = 30)\npca.fit(X_std)\nprint('Components:\\n ', pca.components_)\nprint('Explained Variance Ratio:\\n ', pca.explained_variance_ratio_)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"#plot explained variance \nplt.bar(range(pca.explained_variance_ratio_.shape[0]), pca.explained_variance_ratio_, alpha = 0.5, \n        align = 'center', label = 'individual explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.ylim(0, 0.2)\nplt.legend(loc = 'best')\nplt.tight_layout()","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"outputs":[],"source":"s1 = sum(pca.explained_variance_ratio_[:17])\ns2 = sum(pca.explained_variance_ratio_[17:])","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"# dimensionality reduction, keeping only\n# 17 principal component\npca = PCA(n_components = 17)\nX_pca = pca.fit_transform(X_std)\n# inverse transform to obtain the projected data\nX_new = pca.inverse_transform(X_pca)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"print(\"Percentage of information on the components that we keep:\",s1,\"\\nPercentage of information of the components that we discard:\",s2)\nprint(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", X_pca.shape)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"subslide"}}},{"source":"###  Linear Regression with/without PCA (one hot encoding)","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"import time\nsplit_data = new_data.drop(['price'], axis = 1)\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(split_data,\n                                                y, \n                                                test_size=0.3,\n                                                train_size = 0.7,\n                                                random_state=13)\npipe1 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('pca', PCA(n_components = 17)),\n    ('linear', linear_model.LinearRegression())\n])\npipe1.fit(X_train, y_train)\ny_pred1 = pipe1.predict(X_test)\n\nlinear_reg_error1 = metrics.median_absolute_error(y_test, y_pred1) \n\n# pipeline without PCA\npipe2 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('linear', linear_model.LinearRegression())\n])\nstart = time.time()\npipe2.fit(X_train, y_train)\ny_pred2 = pipe2.predict(X_test)\n\nlinear_reg_error2 = metrics.median_absolute_error(y_test, y_pred2) \n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"print (\"Linear Regression's price deviation with PCA: \" + str(linear_reg_error1))\nprint (\"Linear Regression's price deviation without PCA: \" + str(linear_reg_error2))","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"outputs":[],"source":"from sklearn.metrics import r2_score\nprint(\"R-squared Error with PCA:\",r2_score(y_test,y_pred1))\nprint(\"R-squared Error without PCA:\",r2_score(y_test,y_pred2))\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"source":"###  Logistic Regression with/without PCA (one hot)","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"y = round(new_data['overall_satisfaction'])\nsplit_data = new_data.drop(['overall_satisfaction'], axis = 1)\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(split_data,\n                                                y, \n                                                test_size=0.3,\n                                                train_size = 0.7,\n                                                random_state=13)\npipe1 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('pca', PCA(n_components = 17)),\n    ('logistic', linear_model.LogisticRegression(C = 1e5))\n])\nstart = time.time()\n\npipe1.fit(X_train, y_train)\ny_pred1 = pipe1.predict(X_test)\nend = time.time()\nprint(end - start)\n\n# pipeline without PCA\npipe2 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('logistic', linear_model.LogisticRegression(C = 1e5))\n])\nstart = time.time()\n\npipe2.fit(X_train, y_train)\ny_pred2 = pipe2.predict(X_test)\nend = time.time()\nprint(end - start)\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"\nprint('Logistic Regression Accuracy with PCA:',sum(y_test == y_pred1),\" / \", sum(y_test==y_test) ,'=' , sum(y_test == y_pred1)/sum(y_test==y_test)*100 , \"%\\n\")\nprint('Logistic Regression Accuracy without PCA:',sum(y_test == y_pred2),\" / \", sum(y_test==y_test) ,'=' , sum(y_test == y_pred2)/sum(y_test==y_test)*100 , \"%\\n\")\n","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"source":"# PCA using Label encoding","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"y = le_data['price']\nX = le_data.drop(['price'],axis=1)\n#standarize the dataset\nX_std = StandardScaler().fit_transform(X)\n\n# call PCA specifying we only want the\n\npca = PCA(n_components =5)\npca.fit(X_std)\n\n# important information\nprint('Components:\\n ', pca.components_)\nprint('Explained Variance Ratio:\\n ', pca.explained_variance_ratio_)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"s1 = sum(pca.explained_variance_ratio_[:3])\ns2 = sum(pca.explained_variance_ratio_[3:])","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"plt.bar(range(pca.explained_variance_ratio_.shape[0]), pca.explained_variance_ratio_, alpha = 0.5, \n        align = 'center', label = 'individual explained variance')\n\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.ylim(0, 1)\nplt.legend(loc = 'best')\nplt.tight_layout()","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"outputs":[],"source":"\n# dimensionality reduction, keeping only\n# the first principal component\npca = PCA(n_components = 3)\nX_pca = pca.fit_transform(X_std)\n\n# inverse transform to obtain the projected data\n# and compare with the original\nX_new = pca.inverse_transform(X_pca)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"print(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", X_pca.shape)\nprint(\"Percentage of information on the components that we keep:\",s1,\"\\nPercentage of information of the components that we discard:\",s2)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"subslide"}}},{"source":"###  Linear Regression with/without PCA (label encoding)","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"import time\n\nstart = time.time()\nsplit_data = le_data.drop(['price'], axis = 1)\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(split_data,\n                                                y, \n                                                test_size=0.3,\n                                                train_size = 0.7,\n                                                random_state=13)\npipe1 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('pca', PCA(n_components = 4)),\n    ('linear', linear_model.LinearRegression())\n])\npipe1.fit(X_train, y_train)\ny_pred1 = pipe1.predict(X_test)\n\nlinear_reg_error1 = metrics.median_absolute_error(y_test, pipe1.predict(X_test)) \n\n# pipeline without PCA\n\npipe2 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('linear', linear_model.LinearRegression())\n])\npipe2.fit(X_train, y_train)\ny_pred2 = pipe2.predict(X_test)\n\nlinear_reg_error2 = metrics.median_absolute_error(y_test, pipe2.predict(X_test)) ","execution_count":null,"cell_type":"code","metadata":{"scrolled":true,"slideshow":{"slide_type":"skip"},"collapsed":true}},{"outputs":[],"source":"print (\"Linear Regression deviation with PCA: \" + str(linear_reg_error1))\nprint (\"Linear Regression deviation without PCA: \" + str(linear_reg_error2))","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"outputs":[],"source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint(\"R-squared Error with PCA:\",r2_score(y_test,y_pred1))\nprint(\"R-squared Error without PCA:\",r2_score(y_test,y_pred2))","execution_count":null,"cell_type":"code","metadata":{"scrolled":true,"slideshow":{"slide_type":"fragment"}}},{"source":"###  Logistic Regression with/without PCA (label encoding)","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"outputs":[],"source":"y = round(le_data['overall_satisfaction'])\nsplit_data = le_data.drop(['overall_satisfaction'], axis = 1)\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(split_data,\n                                                y, \n                                                test_size=0.3,\n                                                train_size = 0.7,\n                                                random_state=13)\npipe1 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('pca', PCA(n_components = 4)),\n    ('logistic', linear_model.LogisticRegression(C = 1e5))\n])\n\nstart = time.time()\npipe1.fit(X_train, y_train)\ny_pred1 = pipe1.predict(X_test)\nend = time.time()\nprint(end - start)\n\n# pipeline without PCA\npipe2 = Pipeline([\n    ('standardize', StandardScaler()),\n    ('logistic', linear_model.LogisticRegression(C = 1e5))\n])\nstart = time.time()\n\npipe2.fit(X_train, y_train)\ny_pred2 = pipe2.predict(X_test)\nend = time.time()\nprint(end - start)","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"skip"}}},{"outputs":[],"source":"print('Logistic Regression Accuracy with PCA:',sum(y_test == y_pred1),\" / \", sum(y_test==y_test) ,'=' , sum(y_test == y_pred1)/sum(y_test==y_test)*100 , \"%\\n\")\nprint('Logistic Regression Accuracy without PCA:',sum(y_test == y_pred2),\" / \", sum(y_test==y_test) ,'=' , sum(y_test == y_pred2)/sum(y_test==y_test)*100 , \"%\\n\")","execution_count":null,"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"}}},{"source":"\n# Conclusion\nIn this data analysis project, I analyzed some of the most popular trends given the Airbnb's data on listings in Lisbon.\n\nSummarized findings:","metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown"},{"source":"| Method  |      Accuracy (%)    | Dimensions | Execution Time |\n|----------|:-------------:|:-------------:|-------------:\n| Logistic Regression (ONE HOT) |  72.41 | (11998, 33) | 0.74 |\n| Logistic Regression with PCA (ONE HOT)  |    50.3   | (11998, 17) | 0.23 |\n| Logistic Regression (LABEL) |  72.61 | (11998, 8) | 0.16 |\n| Logistic Regression with PCA (LABEL)  |    65.1   | (11998, 4) | 0.12 |","metadata":{"slideshow":{"slide_type":"subslide"},"collapsed":true},"cell_type":"markdown"},{"source":"| Method  |      Deviation (in Euros)    | R-squared Error\n|----------|:-------------:|-------------:|\n| Linear Regression (ONE HOT) |  20.54 | 0.32 |\n    | Linear Regression with PCA (ONE HOT)  |   22.6   | 0.20 |\n| Linear Regression (LABEL) |  19.7 | 0.32 |\n| Linear Regression with PCA (LABEL)  |    20.4 | 0.30 |\n","metadata":{"slideshow":{"slide_type":"subslide"},"collapsed":true},"cell_type":"markdown"}]}