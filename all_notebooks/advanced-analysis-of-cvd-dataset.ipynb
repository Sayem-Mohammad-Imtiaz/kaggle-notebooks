{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Before move to this advanced analysis part please refer my descriptive analysis notebook to get an idea about the dataset.\n\n**Link:** https://www.kaggle.com/migdev/descriptive-analysis-of-cardiovascular-dataset","metadata":{}},{"cell_type":"markdown","source":"BMI is a good indicator of healthiness of humans. Therefore, BMI was calculated by using weight and height and then weight and height were deleted from the dataset. Now we move to build a predictive model to predict presence of CVD. Fist we move to machine learning techniques and then we move to classical statistical methods. ","metadata":{}},{"cell_type":"code","source":"#import necessary libraries \nimport pandas as pd\nimport numpy as np\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold,cross_val_predict,GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/all-cleaned-and-partitioned-datasets/cardio_final.csv')\n#only predictors\nx=df.drop(['cardio'],axis='columns')\n\n#only response\ny= df['cardio']\n\n# split dataset into train (70%) and test (30%)\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree model\nfrom sklearn.tree import DecisionTreeClassifier\n#Using max_depth, criterion will suffice for DT Models, rest all will remain constant \n#hyperparamter tuning\nparameters = {'max_depth' : (3,5,7,9,10,15,20,25)\n              , 'criterion' : ('gini', 'entropy')\n              , 'max_features' : ('auto', 'sqrt', 'log2')\n              , 'min_samples_split' : (2,4,6)\n             }\nDT_grid  = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions = parameters, cv = 3, verbose = True, random_state=42)\nDT_grid.fit(x_train,y_train)\n#this gives you the best parameters for decision tree model\nDT_grid.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Re Build Model with Best Estimators\nDT_Model = DecisionTreeClassifier(criterion='entropy',max_depth=7, max_features='auto',\n                                  min_samples_leaf=1, min_samples_split=4,random_state=42)\n\nDT_Model.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train and test accuracies\nprint (f'Train Accuracy - : {DT_Model.score(x_train,y_train):.3f}')\nprint (f'Test Accuracy - : {DT_Model.score(x_test,y_test):.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix\ny_pred=DT_Model.predict(x_test)\ncm1=confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10,7))\nsn.heatmap(cm1,annot=True,fmt='d')\nplt.xlabel('predicted')\nplt.ylabel('Truth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random Forest model\n#Hyperparameter tuning\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, cv = 3, verbose=2, random_state=42)\n# Fit the random search model\nrf_random.fit(x_train, y_train)\n\n#best parameters\nrf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model fitting\nregr = RandomForestClassifier(bootstrap=True , max_depth=10 ,max_features='sqrt',min_samples_leaf=4 ,\n                              min_samples_split=10 ,n_estimators=300,random_state=42)\nregr.fit(x_train,y_train)\n\n#Accuracies for train and test sets\nprint (f'Train Accuracy - : {regr.score(x_train,y_train):.3f}')\nprint (f'Test Accuracy - : {regr.score(x_test,y_test):.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix\ny_pred=regr.predict(x_test)\ncm2=confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10,7))\nsn.heatmap(cm2,annot=True,fmt='d')\nplt.xlabel('predicted')\nplt.ylabel('Truth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}