{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Práctica 2: Aprendizaje y selección de modelos de clasificación\n\n### Minería de Datos: Curso académico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jiménez\n* José Antonio Gámez Martín\n\n### Realizado por:\n\n* Antonio Beltrán Navarro\n* Ramón Jesús Martínez Sánchez\n\n\\* Adaptado de las prácticas de Jacinto Arias Martínez y Enrique González Rodrigo"},{"metadata":{},"cell_type":"markdown","source":"En esta práctica estudiaremos los modelos más utilizados en `scikit-learn` para conocer los distintos hiperparámetros que los configuran y estudiar los clasificadores resultantes. Además, veremos métodos de selección de modelos orientados a obtener una configuración óptima de hiperparámetros."},{"metadata":{},"cell_type":"markdown","source":"# Breast Cancer Wisconsin"},{"metadata":{},"cell_type":"markdown","source":"# 1. Preliminares"},{"metadata":{},"cell_type":"markdown","source":"Lo primero de todo es cargar las librerías para que estén disponibles posteriormente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, Normalizer\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\n# Local application\nimport utilidad_grupor as utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fijaremos tambíen una semilla aleatoria para que los experimentos sean reproducibles:"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 27912","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# 2. Carga de datos"},{"metadata":{},"cell_type":"markdown","source":"Comenzamos cargando el conjunto de datos `wisconsin`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/breast-cancer-wisconsin-data/data.csv\"\n\nindex = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath, index, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este conjunto de datos está compuesto por 30 variables predictoras que en realidad se agrupan en 10 ya que estas se ven divididas entre `_mean, _se y _worst`, una variable `Unnamed` que será tratada posteriormente y la variable clase `diagnosis` que nos indicará si el tumor es **maligno** (m) o **benigno** (b).\n\nLas variables predictoras, por tanto, son:\n* `radius, perimeter, area, compactness`: relacionadas con el tamaño del tumor\n* `symmetry`: se refiere a la simetría en la forma del tumor\n* `smoothness`: variación en la longitud de los tamaños\n* `concavity, concave_points`: relacionados con la concavidad del tumor\n* `texture`: referente a la textura del tumor (en escala de grises)\n* `fractal_dimension`"},{"metadata":{},"cell_type":"markdown","source":"Una vez hemos cargado el conjunto de datos, mostraremos 5 registros aleatorios mediante la función `sample` para comprobar que el proceso ha sido realizado correctamente"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar, tenemos una columna, la última, cuyo nombre es `Unnamed` y todo el contenido de sus filas `NaN`. Esto es debido a que en la declaración de las columnas en el archivo `csv` hay una `,` sobrante al final de la línea, lo que hace que se cree una columna sin nombres y con valores inexistentes.\n\nEsto significa que antes de continuar trabajando con nuestro conjunto de datos, debemos borrar esa columna."},{"metadata":{"trusted":true},"cell_type":"code","source":"del data['Unnamed: 32']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos que se ha borrado correctamente haciendo uso del método `sample` de nuevo:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.diagnosis.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Con el propósito de usar métricas como el `recall` en el apartado de selección de modelos, es necesario convertir nuestra variable clase que resulta ser categórica a una con valores numéricos.\n\nPara ello elegiremos que la clase **M** (malign) pasará a tomar un valor de 1, y la clase **B** (benign) pasará a tomar un valor de 0.\n\nMas adelante veremos que dicho cambio se ha realizado correctamente."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A continuación, separaremos en dos subconjuntos nuestro conjunto de datos inicial, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). "},{"metadata":{"trusted":true},"cell_type":"code","source":"(X, y) = utils.divide_dataset(data, target=\"diagnosis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobaremos que se hayan separado correctamente:"},{"metadata":{},"cell_type":"markdown","source":"Empezamos mostrando las variables predictoras:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y a continuación la variable clase:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Antes de comenzar el análisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **70%**\n* Conjunto de prueba: **30%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validación han sido obtenidos de una manera correcta."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguidamente, lo que haremos será unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento.\nHaremos los mismo para `X_test` e `y_test`, juntando así el conjunto de datos de test."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = utils.join_dataset(X_train, y_train)\ntest = utils.join_dataset(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nuevamente, vamos a asegurarnos de que el conjunto de datos se ha dividido correctamente. Para ello visualizaremos el conjunto de datos de entrenamiento, observando que la variable clase también aparece al final del conjunto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez tenemos bien definidos nuestros conjuntos de entrenamiento y prueba, podemos aplicar el **preprocesamiento de datos** definido en la práctica anterior."},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Preprocesamiento de datos"},{"metadata":{},"cell_type":"markdown","source":"### 2.1.1. Eliminación de variables"},{"metadata":{},"cell_type":"markdown","source":"Para realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline será el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.\n\nEliminaremos las siguientes columnas (variables):\n* Aquellas relacionadas con los `concave points`: `concavity_mean, compactness_mean, concavity_se, compactness_se, concavity_worst y compactness_worst`.\n* Aquellas relacionadas con `radius`, es decir: `area_mean, perimeter_mean, area_se, perimeter_se, area_worst y perimeter_worst`.\n\nPorque son variables que dependen directamente de las 2 que vamos a dejar en nuestro conjunto de datos: `radius y concave points`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"concavity_mean\", \"compactness_mean\", \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\", \"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \"area_worst\", \"perimeter_worst\"])], remainder=\"passthrough\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2. Normalización"},{"metadata":{},"cell_type":"markdown","source":"Será necesario **normalizar** el conjunto de datos a la hora de definir los modelos a emplear, de forma que todas las variables predictoras tengan el mismo peso.\n\nPara ello definiremos una variable `normalizacion` que haga uso de la función `Normalizer()`, la cual normaliza individualmente los datos a una norma unitaria, justo lo que buscamos."},{"metadata":{"trusted":true},"cell_type":"code","source":"normalizacion = Normalizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez podamos añadir al **Pipeline** el preprocesamiento de datos obtenido, podemos pasar a tratar los modelos de clasificación para nuestro conjunto de datos."},{"metadata":{},"cell_type":"markdown","source":"# 3. Modelos de clasificación supervisada"},{"metadata":{},"cell_type":"markdown","source":"Es el momento de definir los diferentes modelos que utilizaremos en esta práctica. Cada uno de estos modelos contará con el preprocesamiento de datos definido previamente haciendo uso del **Pipeline**.\n\nComentaremos los parámetros de cada modelo más importantes para nuestro problema y que utilizaremos posteriormente mediante un proceso de validación cruzada."},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Vecinos más cercanos"},{"metadata":{},"cell_type":"markdown","source":"Este algoritmo es considerado perozoso puesto que computa los parámetros necesarios para la clasificación durante inferencia. La clasificación de este modelo consistirá en asignar a la instancia de entrada la clase mayoritaria de los vecinos más cercanos.\n\nEncontramos 2 principales parámetros a la hora de definir nuestro modelo de vecinos mas cercanos, el número de vecinos `n_neighbours` que se dejará por defecto en un valor moderado como puede ser 5, y los pesos `weights`, donde consideraremos que todos los vecinos tienen la misma importancia, y por tanto, el mismo peso. Dejaremos los valores de esta variable por defecto.\n\nPor último, al crear el **Pipeline** de este algoritmo, deberemos pasarle las variables normalizadas como hemos definido previamente en el preprocesamiento de datos, de esta manera escalaremos las variables predictoras en proporciones similares que serán tratadas por los pesos."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = 5\nweights = 'distance'\n\nk_neighbors_model = make_pipeline(\n        preproc,\n        normalizacion,\n        KNeighborsClassifier(n_neighbors, weights=weights)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Árboles de decisión"},{"metadata":{},"cell_type":"markdown","source":"Los algoritmos basados en inducción de árboles de decisión representan los modelos mediante un conjunto de reglas.\n\nLos hiperparámetros que a nuestro parecer resultan más interesantes para este modelo serán `max_depth`, que tomará un valor de 3 y se referirá a la profundidad del arbol (evitaremos sobreajuste con este valor), y `min_samples_split`, que tomará un valor de 20, lo que significa que las hojas deberán tener un mínimo de 20 instancias para evitar, de nuevo, un posible sobreajuste.\n\nEs necesario comentar que posteriormente, en el apartado de evaluación de modelos, estudiaremos de mejor forma estos hiperparámetros."},{"metadata":{},"cell_type":"markdown","source":"Vamos a configurar nuestro árbol de decisión con lo mencionado anteriormente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_model = make_pipeline(\n        preproc,\n        DecisionTreeClassifier(random_state=seed, \n                               max_depth=3,\n                               criterion='entropy',\n                               ccp_alpha=0.1,\n                               min_samples_leaf=25)\n) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4. Adaptative Boosting (*AdaBoost*)"},{"metadata":{},"cell_type":"markdown","source":"Este algoritmo propone un entrenamiento de una serie de clasificadores de manera iterativa, de modo que cada nuevo clasificador se enfoque en los datos que fueron erróneamente clasificados por su predecesor, de esta forma el algoritmo se adapta y logra obtener mejores resultados.\n\nLos hiperparámetros más relevantes a tener en cuenta serán `base_estimator`, que nos indica el estimador que utilizará el ensemble (en este caso un Árbol de decisión), el número de estimadores `n_estimators`, o la tasa de aprendizaje que nos ayudará a controlar la contribución de cada estimador `learning_rate`.\n\nEs necesario comentar que posteriormente, en el apartado de evaluación de modelos, estudiaremos de mejor forma estos hiperparámetros."},{"metadata":{},"cell_type":"markdown","source":"\nVamos a configurar un modelo AdaBoost sencillo que utilice los hiperparámetros por defecto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboost_model = AdaBoostClassifier(random_state=seed)\n\nadaboost_model = make_pipeline(\n        preproc,\n        AdaBoostClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=1))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5. Bootstrap Aggregating (*Bagging*)"},{"metadata":{},"cell_type":"markdown","source":"Bagging es un ensemble cuya estrategia es utilizar una función de aprendizaje y obtener modelos diversos entre sí para reducir el error obtenido mediante varianza. Mediante un voto de estos modelos se obtendrá el ensemble.\n\nLos hiperparámetros más relevantes a tener en cuenta serán `base_estimator`, que nos indica el estimador que utilizará el ensemble (en este caso un Árbol de decisión) o el número de estimadores `n_estimators`. Para un muestreo con reemplazo, emplearemos el parámetro `bootstrap_features=True`.defecto, `random_state=None`).\n\nEs necesario comentar que posteriormente, en el apartado de evaluación de modelos, estudiaremos de mejor forma estos hiperparámetros."},{"metadata":{},"cell_type":"markdown","source":"\nVamos a configurar un ensemble tipo Bagging utilizando los hiperparámetros por defecto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging_model = make_pipeline(\n        preproc,\n        BaggingClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=None))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.6. Random Forests"},{"metadata":{},"cell_type":"markdown","source":"Un Random Forest es un conjunto (ensemble) de árboles de decisión combinados con bagging. Al usar bagging, lo que en realidad está pasando, es que distintos árboles ven distintas porciones de los datos. Ningún árbol ve todos los datos de entrenamiento. Esto hace que cada árbol se entrene con distintas muestras de datos para un mismo problema. De esta forma, al combinar sus resultados, unos errores se compensan con otros y tenemos una predicción que generaliza mejor."},{"metadata":{},"cell_type":"markdown","source":"Vamos a configurar un estimador tipo Random Forests usando los hiperparámetros por defecto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model = make_pipeline(\n        preproc,\n        RandomForestClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.7. Gradient Tree Boosting (*Gradient Boosting*)"},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting es una generalización de los algoritmos de Boosting con la capacidad de optimizar cualquier tipo de función pérdida, generando un conjunto de estimadores de forma secuencial. No todos estos estimadores tendrán la misma importancia, puesto que en cada iteración solo se tomarán en cuenta a los modelos anteriores."},{"metadata":{},"cell_type":"markdown","source":"\n\nVamos a configurar este estimador usando los hiperparámetros por defecto:"},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosting_model = make_pipeline(\n        preproc,\n        GradientBoostingClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.8. Histogram-Based Gradient Boosting (*Histogram Gradient Boosting*)"},{"metadata":{},"cell_type":"markdown","source":"El algoritmo Histogram Gradient Boosting es una optimización de *Gradient Boosting* que discretiza el conjunto de datos de entrada con el fin de poder trabajar con un mayor número de instancias en un tiempo razonable.\n\nAl contrario que en otros modelos, Histogram Gradient Boosting realiza su propia discretización, por lo que no tendremos que proporcionársela mediante el **Pipeline**."},{"metadata":{},"cell_type":"markdown","source":"Vamos a usar los hiperparámetros por defecto para configurar este estimador:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_gradient_boosting_model = make_pipeline(\n        preproc,\n        HistGradientBoostingClassifier(random_state=seed)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Evaluación de modelos"},{"metadata":{},"cell_type":"markdown","source":"A la hora de evaluar los distintos modelos, utilizaremos una técnica conocida como **validación cruzada**. En esta, se separa el conjunto de datos en `k` particiones y se repite `k` veces el proceso de aprendizaje y validación, pero utilizando cada vez una combinación única de `k-1` muestras para entrenar y la restante para validar. De este modo, obtendremos unos valores fiables de sesgo empleando solamente el conjunto de entrenamiento.\n\nEn este caso vamos a evaluar nuestros clasificadores y el tipo de validación cruzada a utilizar usando una `5*10-cv` estratificada:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Árbol de decisión"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = decision_tree_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Vecinos más cercanos"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = k_neighbors_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Adaptative Boosting (*AdaBoost*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = adaboost_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4. Bootstrap Aggregating (*Bagging*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = bagging_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.5. Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = random_forest_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.6. Gradient Tree Boosting (Gradient Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = gradient_boosting_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = hist_gradient_boosting_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Selección de modelos"},{"metadata":{},"cell_type":"markdown","source":"Ahora que somos capaces de evaluar correctamente los clasificadores, es importante decidir una estrategia que nos permita encontrar una configuración óptima de los hiperparámetros. Para ello haremos uso de una búsqueda **Grid** para explorar las posibles combinaciones de hiperparámetros y obtener el mejor modelo."},{"metadata":{},"cell_type":"markdown","source":"Definimos las 2 métricas que usaremos para seleccionar nuestros modelos, como son el `accuracy` y el `recall`. A la hora de elegir los mejores modelos, seleccionaremos aquellos que logren un mejor resultado en recall, el ratio de verdaderos positivos y falsos negativos obtenido mediante $\\frac {tp}{(tp+fn)}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = [\"accuracy\",\"recall\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Además de usar estas dos métricas, emplearemos el método refit que nos servirá para elegir cuál es el mejor modelo una vez acabada la búsqueda **Grid** y volver a entrenar con él tratando de mejorar, en este caso, su **recall**."},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Árbol de decisión"},{"metadata":{},"cell_type":"markdown","source":"En esta selección de modelos trataremos de optimizar los parámetros que consideramos son más relevantes en árboles de decisión.\n\nLos hiperparámetros elegidos para ser optimizados han sido:\n* `max_depth`: Profundidades del árbol que se estudiarán para evitar un posible sobreajuste y garantizar un buen resultado.\n* `ccp_alpha`: Parámetros de complejidad usado para una posibilidad de poda posterior con el mínimo coste computacional\n* `criterion`: Función para medir la calidad de las particiones del árbol. Usaremos `entropy` para la ganancia de información y el índice `gini`.\n* `min_samples_leaf`: Mínimo número de instancias para poder tener en cuenta un nodo hoja, lo cual afecta al suavizado del modelo que sea elegido. "},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = decision_tree_model\n\nmax_depth = [3, 4, 5, 6, 7]\nccp_alpha = [0, 0.05, 0.1, 0.2]\ncriterion = ['entropy', 'gini']\nmin_samples_leaf = [5, 10, 15, 20, 25]\n\ndecision_tree_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        scoring=scoring, refit=\"recall\",\n                                        decisiontreeclassifier__max_depth=max_depth,\n                                        decisiontreeclassifier__criterion=criterion,\n                                        decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                        decisiontreeclassifier__min_samples_leaf=min_samples_leaf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez se ha realizado la búsqueda **Grid**, vemos que el modelo que nos proporciona mejores resultados cuenta con un `ccp_alpha` de 0, lo que nos indica que una posible post poda no ayudaría a mejorar el modelo. Además vemos que el criterio elegido es el de entropía que trata de maximizar la ganancia de información, una profundidad del árbol de 3, y un número mínimo de ejemplos por nodo hoja de 10."},{"metadata":{},"cell_type":"markdown","source":"## 5.2. Vecinos más cercanos"},{"metadata":{},"cell_type":"markdown","source":"En esta selección de modelos trataremos de optimizar los parámetros que consideramos son más relevantes en el algoritmo de vecinos más cercanos.\n\nLos hiperparámetros elegidos para ser optimizados han sido:\n* `weights`: Función de peso de los vecinos, que pueden ser uniformes o por distancias.\n* `n_neighbors`: Número de vecinos más cercanos para elegir la clase del que se estudia. Un gran número de vecinos podría llevar al modelo a parecerse a un `ZeroR`, y un pequeño número de vecinos podría causarnos un gran sobreajuste. Es por ello que este hiperparámetro es el más importante a la hora de seleccionar nuestro modelo de vecinos más cercanos."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = k_neighbors_model\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3, 5, 7, 9, 11, 13, 15]\n\n\nk_neighbors_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La estrategia seleccionada utiliza una distribución de pesos uniforme, y un número de vecinos muy pequeño: 3. Es interesante que éste sea el número de vecinos, puesto que es el menor de los proporcionados y hemos comentado que un pequeño número de vecinos podría causarnos sobreajuste."},{"metadata":{},"cell_type":"markdown","source":"## 5.3. Adaptative Boosting (AdaBoost)"},{"metadata":{},"cell_type":"markdown","source":"En esta selección de modelos trataremos de optimizar los parámetros que consideramos son más relevantes en AdaBoost.\n\nLos hiperparámetros elegidos para ser optimizados han sido:\n* `criterion`: Función para medir la calidad de las particiones del árbol. Usaremos `entropy` para la ganancia de información y el índice `gini`.\n* `learning_rate`: Nos indica la contribución de los estimadores basada en el valor de esta variable, por defecto a 1.\n* `min_samples_split`: Mínimo número de instancias requeridas para dividir un nodo interno, este hiperparámetro será usado por los árboles que conforman nuestro modelo de AdaBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = adaboost_model\n\ncriterion = [\"gini\", \"entropy\"]\nlearning_rate = [0.25, 0.5, 0.75, 1]\nmin_samples_split = [2, 5, 10, 20, 30]\n\nadaboost_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                        adaboostclassifier__base_estimator__min_samples_split=min_samples_split,\n                                        adaboostclassifier__base_estimator__criterion=criterion,\n                                        adaboostclassifier__learning_rate=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En cuanto al criterio seleccionado, vemos que en este caso el índice gini ha conseguido darnos mejores resultados en nuestro modelo junto a una tasa de aprendizaje (learning rate) que mantiene su valor por defecto de 1.\n\nConociendo el algoritmo AdaBoost sabemos que éste trabaja con clasificadores 1R, los cuales reciben una inmensa cantidad de instancias, no obstante es interesante remarcar que el hiperparámetro `min_samples_split` para los nodos internos ha resultado no ser relevante para el problema."},{"metadata":{},"cell_type":"markdown","source":"## 5.4. Bootstrap Aggregating (Bagging)"},{"metadata":{},"cell_type":"markdown","source":"En esta selección de modelos trataremos de optimizar los parámetros que consideramos son más relevantes en Bagging.\n\nLos hiperparámetros elegidos para ser optimizados han sido:\n* `max_samples`: Número de instancias del conjunto de entrenamiento necesarias para entrenar cada estimador.\n* `max_features`: Número de variables predictoras de nuestro conjunto de entrenamiento necesarias para entrenar cada estimador.\n\nAmbas variables toman por defecto el valor 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = bagging_model\n\nmax_samples = [0.25, 0.5, 0.75, 1]\nmax_features = [0.25, 0.5, 0.75, 1]\n\nbagging_clf = utils.optimize_params(estimator,\n                                     X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                     baggingclassifier__max_samples=max_samples,\n                                     baggingclassifier__max_features=max_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De esta manera, podremos apreciar los valores de los hiperparámetros que optimizan las métricas del modelo seleccionado, en este caso `max_features = 0.75` y `max_samples = 0.5`"},{"metadata":{},"cell_type":"markdown","source":"## 5.5. Random Forests"},{"metadata":{},"cell_type":"markdown","source":"Como hemos comentado previamente, este algoritmo emplea árboles distintos con mucho sobreajuste, por lo que no resultaría de importancia controlar los hiperparámetros de éstos árboles como hicimos en AdaBoost. Nos limitaremos a controlar los siguientes hiperparámetros referentes a los Random Forest:\n* `max_samples`: Número de instancias del conjunto de entrenamiento necesarias para entrenar cada estimador (árbol).\n* `max_features`: Número máximo de variables para considerar al buscar la mejor división al entrenar los árboles. Al utilizar `sqrt` estamos indicando que `max_features=sqrt(n_features)`. Lo mismo para `log2`, tal que `max_features=log2(n_features)`\n* `criterion`: Función para medir la calidad de las particiones del árbol. Usaremos `entropy` para la ganancia de información y el índice `gini`."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = random_forest_model\n\nmax_samples = [0.25, 0.5, 0.75]\nmax_features = ['sqrt', 'log2']\ncriterion = [\"gini\", \"entropy\"]\n\nrandom_forest_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                               randomforestclassifier__max_samples=max_samples,\n                                               randomforestclassifier__criterion=criterion,\n                                               randomforestclassifier__max_features=max_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El modelo seleccionado utilizará un 50% de la instancias para entrenar los árboles, junto con un número máximo de variables guiado por `sqrt`, y haciendo uso de un criterio basado en el índice `gini`, garantizando así un modelo con un recall y una accuracy adecuados para nuestro problema."},{"metadata":{},"cell_type":"markdown","source":"## 5.6. Gradient Tree Boosting (Gradient Boosting)"},{"metadata":{},"cell_type":"markdown","source":"En esta selección de modelos trataremos de optimizar los parámetros que consideramos son más relevantes en Gradient Boosting.\n\nLos hiperparámetros elegidos para ser optimizados han sido:\n* `learning_rate`: Tasa de aprendizaje usada como factor multiplicativo para los nodos hoja. Toma un valor por defecto de 0.1.\n* `max_depth`: Profundidad máxima de cada árbol (desde la raíz al nodo más profundo). Este parámetro no se comprueba por defecto, pero puede resultar de interés estudiarlo.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = gradient_boosting_model\n\nlearning_rate = [0.025, 0.05, 0.1]\nmax_depth = [1, 3, 5, 7]\n\n\ngradient_boosting_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                               gradientboostingclassifier__learning_rate = learning_rate,\n                                               gradientboostingclassifier__max_depth=max_depth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La tasa de aprendizaje (learning rate), se ha dejado a 0.025, mientras que la profundidad del árbol parece no ser demasiado importante. Puesto que toma un valor de 3, podemos extraer que árboles más pequeños trabajan mejor con nuestro algoritmos que árboles que puedan tener una mayor profundidad y por tanto un mayor sobreajuste con nuestro conjunto de entrenamiento."},{"metadata":{},"cell_type":"markdown","source":"## 5.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)\n\n"},{"metadata":{},"cell_type":"markdown","source":"En esta selección de modelos trataremos de optimizar los parámetros que consideramos son más relevantes en Histogram Gradient Boosting.\n\nLos hiperparámetros elegidos para ser optimizados han sido:\n* `min_samples_leaf`: Mínimo número de instancias para poder tener en cuenta un nodo hoja, lo cual afecta al suavizado del modelo que sea elegido. \n* `learning_rate`: Tasa de aprendizaje usada como factor multiplicativo para los nodos hoja. Toma un valor por defecto de 0.1.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = hist_gradient_boosting_model\n\nmin_samples_leaf = [10, 20, 30, 40]\nlearning_rate = [0.05, 0.1, 0.15]\n\nhist_gradient_boosting_clf = utils.optimize_params(estimator,\n                                                  X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                                  histgradientboostingclassifier__learning_rate=learning_rate,\n                                                  histgradientboostingclassifier__min_samples_leaf=min_samples_leaf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las conclusiones que podemos obtener son que la tasa de aprendizaje es un factor relevante a la hora de seleccionar un modelo, y que un árbol con un número de instancias en cada nodo hoja nos proporciona mejores resultados."},{"metadata":{},"cell_type":"markdown","source":"# 5. Construcción y validación del modelo final"},{"metadata":{},"cell_type":"markdown","source":"Una vez que en el apartado anterior hemos obtenido mediante la búsqueda **grid** todos los modelos, podemos emplearlos contra el conjunto de test que reservamos al principio del documento para evaluar cuál nos proporciona mejores resultados en las métricas elegidas."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimators(estimators, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusiones"},{"metadata":{},"cell_type":"markdown","source":"De acuerdo con estos resultados, podemos concluir que los **árboles de decisión** y los ensembles como **Random Forest, Gradient Boosting** e **Histogram Gradient Boosting** son los clasificadores que nos ofrecen un mejor rendimiento en términos de **recall** en nuestro conjunto de datos `wisconsin`.\n\nNo obstante, pese a centrarnos en el **accuracy** y sobre todo en el **recall**, es importante destacar y tener en cuenta otros factores como el tiempo de aprendizaje y de inferencia.\n\nSi bien los ensembles mencionados anteriormente mejoran ligeramente el recall y la precisión cuando los enfrentamos contra el conjunto de test, los **árboles de decisión** tienen un tiempo de aprendizaje e inferencia mucho menor que el resto de modelos de ensembles estudiados.\n\nPor ello, antes de decantarnos por un modelo u otro para nuestro problema, debemos tener en cuenta si el aumento de coste computacional necesario para emplear un modelo más complejo compensa la ligera mejora que este supone frente a modelos más sencillos.\n\nEn el caso del problema que estudiamos, al ser una mejoría tan liviana, no nos resulta necesario aumentar el tiempo de aprendizaje e inferencia de ensembles como Random Forest, Gradient Boosting e Histogram Gradient Boosting, por lo que el modelo seleccionado finalmente han sido los **Árboles de Decisión**."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Pima Diabetes"},{"metadata":{},"cell_type":"markdown","source":"# 1. Preliminares\n\nPrimero cargaremos toda la funcionalidad que necesitaremos:\n* Los distintos algoritmos que aprenderemos (`AdaBoostClassifier`, \n`BaggingClassifier`, \n`GradientBoostingClassifier`, \n`HistGradientBoostingClassifier`, \n`RandomForestClassifier`, \n`KNeighborsClassifier` y \n`DecisionTreeClassifier`)\n\n* Funcionalidad relacionada con la selección y evaluación de modelos (`RepeatedStratifiedKFold` y \n`train_test_split`)"},{"metadata":{},"cell_type":"markdown","source":"Lo siguiente que debemos hacer es fijar una semilla con el objetivo de que todo lo que hagamos sea reproducible."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 27912","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora cargaremos el conjunto de datos, que es el conjunto [Pima Indian Diabetes](https://www.kaggle.com/uciml/pima-indians-diabetes-database). Con el objetivo de evaluar los modelos que obtendremos finalmente, reservaremos un $30\\%$ de los ejemplos para realizar una validación *holdout*."},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/pima-indians-diabetes-database/diabetes.csv\"\n\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, None, target)\n\ntrain, test = train_test_split(data, train_size=0.7, stratify=data[target], random_state=random_state)\n\ntrain_X, train_y = utils.divide_dataset(train, target)\ntest_X, test_y = utils.divide_dataset(test, target)\n\ntrain.sample(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Preprocesamiento de datos"},{"metadata":{},"cell_type":"markdown","source":"Como averiguamos en la práctica anterior, existen multitud de valores perdidos (codificados como $0$), que debemos tratar. Para ello, imputaremos por la media en las variables `Glucose`, `BloodPressure` y `BMI`; y eliminaremos las variables `Insulin` y `SkinThickness` que tienen un porcentaje demasiado alto de valores perdidos como para ser significativas.\n\nYa que dicho proceso no depende del algoritmo que vayamos a usar, lo aplicaremos directamente sobre el conjunto de entrenamiento a priori una sola vez, con el objetivo de ahorrar tiempo entrenando."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_imp = SimpleImputer(missing_values=0)\nremove_columns = ColumnTransformer([('a', 'drop', ['Insulin', 'SkinThickness']),\n                                    ('b', mean_imp, ['Glucose', 'BloodPressure', 'BMI'])],\n                                    remainder='passthrough')\nremove_columns.fit_transform(train_X)\nremove_columns.fit_transform(test_X)\n\ntrain_X.sample(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Selección de modelos\n\nEn esta práctica, aplicaremos distintos algoritmos de aprendizaje de clasificadores con el objetivo de encontrar el que mejor funciona para el problema en cuestión (lograr predecir que un paciente tiene diabetes). Estos son:\n\n* Vecinos más cercanos\n\n* Árbol de decisión\n\n* AdaBoost\n\n* Bagging\n\n* Random Forests\n\n* Gradient Boosting\n\n* Histogram Gradient Boosting\n\nPor cada algoritmo, buscaremos los hiperparámetros que mejor funcionen mediante el algoritmo de búsqueda en Grid. La evaluación de los modelos obtenidos se hará mediante una $10 \\times 5$ validación cruzada, usando como métrica de rendimiento el *recall*, al tratarse de un problema desbalanceado."},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Vecinos más cercanos"},{"metadata":{},"cell_type":"markdown","source":"Para este algoritmo variaremos dos de los hiperparámetros principales:\n\n* `n_neighbors`: la cantidad de ejemplos que se consideran para clasificar. Lo variaremos de $1$ a $15$\n\n* `weights`: el peso de cada ejemplo. Probaremos con pesos uniformes o proporcionales a la inversa de la distancia."},{"metadata":{"trusted":true},"cell_type":"code","source":"k_neighbors_classifier = KNeighborsClassifier()\nn_neighbors = np.arange(1, 16)\nweights = [\"uniform\", \"distance\"]\n\nk_neighbors_clf = utils.optimize_params(k_neighbors_classifier, train_X, train_y, cv=cv, scoring=\"recall\", weights=weights, n_neighbors=n_neighbors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar, se obtiene un mejor resultado con un valor de `n_neighbors` alto (11) y con una distribución de `weights` uniforme."},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Árbol de decisión"},{"metadata":{},"cell_type":"markdown","source":"Para este algoritmo probaremos distintas opciones para los hiperparámetros:\n\n* `criterion`: el criterio para hacer las particiones (`gini` o `entropy`).\n\n* `max_depth`: la profundidad máxima del árbol (sin limitar o de $2$ a $10$).\n\n* `min_samples_leaf`: el número mínimo de ejemplos por hoja. Probaremos a partir de $5$ para evitar sobreajuste.\n\n* `ccp_alpha`: parámetro para controlar la pospoda del árbol."},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_classifier = DecisionTreeClassifier(random_state=random_state)\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [None, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmin_samples_leaf = [5, 6, 7, 8, 9]\nccp_alpha = [0.0, 0.1, 0.2, 0.3, 0.4]\n\ndecision_tree_clf = utils.optimize_params(decision_tree_classifier,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          criterion=criterion,\n                                          max_depth=max_depth,\n                                          min_samples_leaf=min_samples_leaf,\n                                          ccp_alpha=ccp_alpha)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que existen diversas configuraciones de parámetros que nos dan la misma puntuación máxima. En este caso tomaremos la opción de no limitar la profundidad del árbol excepto por obligar a que las hojas tengan al menos $5$ ejemplos para evitar sobreajustar totalmente."},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Adaptative Boosting (AdaBoost)"},{"metadata":{},"cell_type":"markdown","source":"Para AdaBoost, deberemos decidir los parámetros del propio algoritmo así como los del clasificador usado como base, que será un árbol de decisión de poca profundidad. Estos serán:\n\n* `learning_rate`: la tasa de aprendizaje.\n\n* `n_estimators`: probaremos con $50$ y $75$ clasificadores.\n\n* Y para el árbol de decisión:\n    * `criterion`: si usar `gini` o `entropia` para tomar la decisión de particionar.\n\n    * `max_depth`: la profundidad máxima del árbol, que deberá ser pequeña.\n\n    * `ccp_alpha`: el parámetro de penalización para la pospoda."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nadaboost_classifier = AdaBoostClassifier(random_state=random_state, base_estimator=base_estimator)\n\nlearning_rate = [0.95, 1.0]\nn_estimators = [50, 75]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(adaboost_classifier,\n                                     train_X, train_y, cv,\n                                     scoring=\"recall\",\n                                     learning_rate=learning_rate,\n                                     n_estimators=n_estimators,\n                                     base_estimator__criterion=criterion,\n                                     base_estimator__max_depth=max_depth,\n                                     base_estimator__ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que los mejores resultados se dan cuando se aplica pospoda, usando como criterio `entropy`. También observamos que el aumento del número de clasificadores de $50$ a $75$ no tiene demasiado impacto, por lo que entre estas dos opciones elegiremos la de menor complejidad.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 3.4. Bootstrap Aggregating (Bagging)"},{"metadata":{},"cell_type":"markdown","source":"Para el caso del *bagging* aprovecharemos los hiperparámetros por defecto (queremos muestreo con reemplazo y la totalidad de las características), y solo modificaremos el número de clasificadores (`n_estimators`) y el criterio del árbol de decisión usado como base (`criterion`), que esta vez interesa que sea profundo por lo que mantendremos los parámetros relacionados con la prepoda (`max_depth`, `min_samples_leaf`, etc) o pospoda (`ccp_alpha`) en su valor por defecto.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nbagging_classifier = BaggingClassifier(random_state=random_state, base_estimator=base_estimator)\n\nn_estimators = [100, 250, 500]\ncriterion = [\"gini\", \"entropy\"]\n\nbagging_clf = utils.optimize_params(bagging_classifier,\n                                    train_X, train_y, cv,\n                                    scoring=\"recall\",\n                                    n_estimators=n_estimators,\n                                    base_estimator__criterion=criterion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En este caso parece claro que cuantos más clasificadores se usen, mejor resultado se alcanza, aunque la mejora entre emplear $250$ o $500$ es casi inapreciable por lo que puede no merecer la pena. Una vez más, usando entropía conseguimos mejores resultados. También es interesante observar que se ha conseguido un $100\\%$ clasificando los datos de entrenamiento, lo cual indica un gran sobreajuste a estos."},{"metadata":{},"cell_type":"markdown","source":"## 3.5. Random Forests"},{"metadata":{},"cell_type":"markdown","source":"En este algoritmo al igual que en el resto basados en árboles probaremos cuál de los criterios para medir la calidad de una partición es mejor. Además, al tomar para cada árbol un subconjunto aleatorio de características, deberemos elegir cuántas tomar. El muestreo nuevamente será con reemplazo. Por lo tanto los hiperparámetros que variaremos serán:\n\n* `criterion`: `gini` o `entropy`.\n\n* `max_features`: $\\sqrt{\\mathit{n\\_features}}$ o $log_2(\\mathit{n\\_features})$.\n\n* `n_estimators`: el número de clasificadores base."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_classifier = RandomForestClassifier(random_state=random_state)\n\nn_estimators = [100, 150]\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf = utils.optimize_params(random_forest_classifier,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          n_estimators=n_estimators,\n                                          criterion=criterion,\n                                          max_features=max_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los resultados indican que a mayor número de árboles mejor resultado, como era de esperar. También podemos observar que esta vez el criterio `gini` se impone a la entropía (`entropy`). En cuanto al número de características seleccionadas, debido al bajo número de características totales con los que contamos en este conjunto de datos, ambos métodos son en esencia idénticos y la ligera ventaja que encontramos que tiene la raíz cuadrada es seguramente debido al azar. Por último, observamos que se ha sobreajustado a los datos de entrenamiento, consiguiendo un $100\\%$ para estos."},{"metadata":{},"cell_type":"markdown","source":"## 3.6. Gradient Tree Boosting (Gradient Boosting)"},{"metadata":{},"cell_type":"markdown","source":"Para este algoritmo, tendremos en cuenta al igual que con el resto de *ensembles* el número de clasificadores y la tasa de aprendizaje. Para los árboles usados como base intentaremos ajustar la profundidad máxima y el parámetro de complejidad para la poda. Al estar hablando ahora de árboles de regresión en lugar de clasificación usaremos como criterio el error cuadrado medio con y sin la mejora de Friedman. Los parámetros del algoritmo que variaremos, por tanto, son:\n\n* `criterion`\n* `n_estimators`\n* `learning_rate`\n* `max_depth`\n* `ccp_alpha`"},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosting_classifier = GradientBoostingClassifier(random_state=random_state)\n\nlearning_rate = [0.01, 0.05, 0.1]\nn_estimators = [100, 200]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf = utils.optimize_params(gradient_boosting_classifier,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          learning_rate=learning_rate,\n                                          n_estimators=n_estimators,\n                                          criterion=criterion,\n                                          max_depth=max_depth,\n                                          ccp_alpha=ccp_alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Si nos fijamos en los mejores modelos, encontramos que la tasa de aprendizaje preferible de entre las probadas es $0.05$. Además un mayor número de árboles mejora los resultados como viene siendo habitual. Ambos criterios de calidad de las particiones probados no presentan una diferencia significativa en los resultados. Los árboles que mejor han funcionado han sido los de profundidad $3$ sin pospoda (`ccp_alpha` $=0$)."},{"metadata":{},"cell_type":"markdown","source":"## 3.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)\n\n"},{"metadata":{},"cell_type":"markdown","source":"Este último algoritmo se diferencia del anterior en que se reduce el número de umbrales que se prueban para hacer las particiones mediante el uso de histogramas, lo que lo hace bastante más rápido. En esta ocasión estudiaremos los parámetros:\n\n* `learning_rate`: la tasa de aprendizaje. En esta ocasión es viable usar valores más bajos debido al menor tiempo de entrenamiento con respecto a *Gradient Boosting*.\n\n* `max_leaf_nodes`: esta vez usaremos el máximo número de nodos hoja: otra forma de controlar el tamaño de los árboles en lugar de por su profundidad máxima."},{"metadata":{"trusted":true},"cell_type":"code","source":"histogram_gradient_boosting_classifier = HistGradientBoostingClassifier(random_state=random_state)\n\nlearning_rate = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes = [15, 31, 65, 127]\n\nhist_gradient_boosting_clf = utils.optimize_params(histogram_gradient_boosting_classifier,\n                                                   train_X, train_y, cv,\n                                                   scoring=\"recall\",\n                                                   learning_rate=learning_rate,\n                                                   max_leaf_nodes=max_leaf_nodes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar, los mejores resultados se han obtenido con distinto número máximo de hojas de los árboles, por lo que podemos deducir que este hiperparámetro no es el más importante. Se usará por tanto el modelo de menor complejidad. Sin embargo, la tasa de aprendizaje que ha dado mejores resultados ha sido $0.03$."},{"metadata":{},"cell_type":"markdown","source":"# 4. Construcción y validación del modelo final"},{"metadata":{},"cell_type":"markdown","source":"Por último, compararemos los resultados de cada modelo con los hiperparámetros aprendidos y entrenados con la totalidad del conjunto de entrenamiento, contra el conjunto de test que habíamos reservado al principio."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.evaluate_estimators(estimators, test_X, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusiones"},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar, en términos de *recall*, los modelos que proporcionan mejores resultados para este problema son el árbol de decisión y el AdaBoost; y el peor el Vecinos más cercanos. Si nos fijamos en otras métricas como el tiempo necesario para entrenamiento e inferencia, el árbol de decisión vuelve a destacar.\n\nAnte estos resultados, debemos priorizar el **árbol de decisión** al conseguir un buen resultado y ser más simple."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Mushroom Classification"},{"metadata":{},"cell_type":"markdown","source":"Vamos a estudiar el siguiente kernel [Mushroom Classification & why it's easy to 100%ac](https://www.kaggle.com/arevel/mushroom-classification-why-it-s-easy-to-100-ac) realizando las modificaciones que consideramos necesarias:\n* Traducción de la libreta.\n* Uso de un script de utilidades con el código necesario para plotear gráficas y generar los modelos.\n* Arreglo de **Data Leaks** encontrados."},{"metadata":{},"cell_type":"markdown","source":"# 1. Preliminares"},{"metadata":{},"cell_type":"markdown","source":"Lo primero de todo será cargar las librerías para que estén disponibles posteriormente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n#Preprocesamiento de datos\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n#Creación de modelos y búsqueda de hiperparámetros\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n#Validación y visualización de métricas\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, precision_score, recall_score, auc\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\nimport utilidad_grupor as utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Librerías usadas** \n\n* [Numpy](https://numpy.org/): Estudiar y trabajar con los datos\n* [Pandas](https://pandas.pydata.org/): Trabajr con el conjunto de datos\n* [Sklearn](https://scikit-learn.org/stable/): Crear y evaluar los modelos\n* [Seaborn](https://seaborn.pydata.org/): Visualizar gráficas con los datos\n* [Matplotlib](https://matplotlib.org/): Visualizar gráficas con los datos\n"},{"metadata":{},"cell_type":"markdown","source":"# 2. Carga de datos"},{"metadata":{},"cell_type":"markdown","source":"En esta libreta trabajaremos con distintos tipos de modelos que evaluaremos posteriormente para tratar de clasificar las setas en dos clases:\n* Comestibles (edible) clasificadas como `e`.\n* Venenosas (poisonous) clasificadas como `p`."},{"metadata":{},"cell_type":"markdown","source":"Fijamos también una semilla para que los experimentos sean reproducibles"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 27912","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargamos el conjunto de datos y visualizamos sus elementos"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mushroom-classification/mushrooms.csv')\ndataset = df.values\ndf.sample(5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mostramos el tamaño del conjunto de datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos si hay valores nulos"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nuestra variable clase, como vemos, se llama `class`. Lo que haremos será separar esta variable de las variables predictoras"},{"metadata":{"trusted":true},"cell_type":"code","source":"names = list(df.columns)\nx = df[names[1:]]\ny = df['class']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la libreta en la cual nos hemos basado, se pasaba directamente a realizar un análisis exploratorio de los datos teniendo en cuenta la totalidad del conjunto de datos.\n\nEsto significa que exploraba con el mismo conjunto de datos con los que luego iba a validar el modelo, causando así un **Data Leak**. Para solucionar este problema, hemos decidido dividir primero nuestro conjunto de datos en dos subconjuntos de entrenamiento y test.\n\nDe esta manera, trabajaremos en el análisis exploratorio con este conjunto de entrenamiento, y el de test permanecerá sin usarse hasta el apartado de selección de modelos."},{"metadata":{},"cell_type":"markdown","source":"Por tanto, antes de comenzar el análisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **80%**\n* Conjunto de prueba: **20%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validación han sido obtenidos de una manera correcta."},{"metadata":{"trusted":true},"cell_type":"code","source":"#añadimos la seed al random state para evitar sesgo y que los experimentos sean reproducibles\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seguidamente, lo que haremos será unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento. Haremos los mismo para `X_test` e `y_test`, juntando así el conjunto de datos de test."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = utils.join_dataset(x_train, y_train)\ntest = utils.join_dataset(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Análisis exploratorio de los datos"},{"metadata":{},"cell_type":"markdown","source":"El análisis exploratorio de datos es un paso fundamental a la hora de comprender los datos con los que vamos a trabajar.\n\nEl objetivo de este análisis es explorar, describir y visualizar la naturaleza de los datos recogidos mediante la aplicación de técnicas simples de resumen de datos y métodos gráficos, para observar las posibles relaciones entre las variables de nuestro conjunto de datos."},{"metadata":{},"cell_type":"markdown","source":"Comenzemos mostrando la distribución de la clase"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Arreglados Data Leaks\n\ncolors = ('#EF8787','#9CF29C')\npalette = sns.set_palette(sns.color_palette(colors))\n\nutils.plot_class_distribution(train, 'class', [\"Venenoso\", \"Comestible\"], colors, 'Mushroom Class Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar, el problema está cerca de ser balanceado, pero encontramos algún ejemplo más de setas venenosas que comestibles"},{"metadata":{},"cell_type":"markdown","source":"Veamos ahora una serie de diagramas de barras que nos relacionarán la distribución de cada variable predictora con sus posibles valores junto a la variable clase, y el número de ejemplos de las mismas"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Arreglados Data Leaks\n\nutils.plot_feature_distribution(train, 'class', train.columns, palette)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos apreciar así, como distintas variables predictoras nos darán mas información que otras. Podemos ver también que solo hay un tipo de **`veil-type`** en esa variable, por lo que en el posterior **preprocesamiento de datos** que llevemos a cabo, eliminaremos esa variable"},{"metadata":{},"cell_type":"markdown","source":"Como las variables de nuestro conjunto de datos son categóricas, llevaremos a cabo un mapa de calor para observar la relación entre estas variables.\n\nAntes, deberemos crear un `LabelEncoder` que transforme nuestras variables con un valor entre $0$ y el $ nclases - 1 $."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Arreglados Data Leaks\n\nlabelencoder=LabelEncoder()\ntrain_enc = train.copy()\nfor column in train.columns:\n    train_enc[column] = labelencoder.fit_transform(train[column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nsns.heatmap(train_enc.corr(),linewidths=.1,annot=True, cmap=\"magma\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De esta manera, podremos ver las variables que se encuentran más correladas unas con otras. A simple vista, encontramos diversas parejas de variables que están altamente relacionadas unas con otras.\n\n* `veil-color y gill-attachment` con una correlación del 89%\n* `ring-type y bruises` con una correlación del 69%\n* `ring-type y gill-color` con una correlación del 63%\n* `spore-print-color y gill-size` con una correlación del 62%\n\nDe este estudio extraemos que las variables `bruises y gill-color` dependen ambas de `ring-type`, y consideramos que podrían ser eliminadas. Pese a que en el procesamiento no se haya llevado a cabo dicha acción, la implementaremos.\n\nComo hemos mencionado anteriormente, la variable `veil-type` será eliminada puesto que no aporta ninguna información relevante."},{"metadata":{},"cell_type":"markdown","source":"# 4. Preprocesamiento de los datos"},{"metadata":{},"cell_type":"markdown","source":"En esta etapa limpiaremos y organizaremos los datos de manera adecuada para entrenar a nuestro modelo basándonos en las observación que hemos realizado en el análisis exploratorio de datos previo. Por ello, en este conjunto de datos nos centraremos en la selección de variables adecuadas para conseguir reducir el número de estas."},{"metadata":{},"cell_type":"markdown","source":"Para realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline será el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos."},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Eliminacion de variables"},{"metadata":{},"cell_type":"markdown","source":"Como hemos comentado anteriormente, será recomendable eliminar la variable que no aporta información `veil-type` y las dos variables que van relacionadas con `ring-type`, como son `bruises y gill-color`."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Añadimos pipeline para preprocesamiento \n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"bruises\",\"gill-color\",\"veil-type\"])], remainder=\"passthrough\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preproc.fit_transform(x_train)\npreproc.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Codificación de los datos"},{"metadata":{},"cell_type":"markdown","source":"Como nuestro conjunto de datos está formado por variables categóricas, para trabajar con ellos debemos codificarlos. A la hora de realizar este proceso, debemos tener en cuenta que para algunos modelos, codificar los valores directamente como números pueden crear sesgo hacia las variables con mayor valor.\n\nPara evitar eso, haremos uso de `OneHotEncoder`"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\nohe_x_train = OneHotEncoder(drop='first').fit(x_train)\nohe_x_train = ohe_x_train.transform(x_train).toarray()\n\naux = y_train.values.reshape(-1, 1)\nohe_y_train = OneHotEncoder(drop='first').fit(aux)\nohe_y_train = ohe_y_train.transform(aux).toarray()\nohe_y_train = ohe_y_train.flatten()\n\n#Test\nohe_x = OneHotEncoder(drop='first').fit(x_test)\nohe_x = ohe_x.transform(x_test).toarray()\n\naux = y_test.values.reshape(-1, 1)\nohe_y = OneHotEncoder(drop='first').fit(aux)\nohe_y = ohe_y.transform(aux).toarray()\nohe_y = ohe_y.flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No es necesario normalizar los datos en este caso, puesto que nuestras variables son categóricas"},{"metadata":{},"cell_type":"markdown","source":"Pasemos ahora a la selección de modelos, donde emplearemos para cada uno el **Pipeline** definido."},{"metadata":{},"cell_type":"markdown","source":"# 5. Selección de modelos"},{"metadata":{},"cell_type":"markdown","source":"Hay muchos modelos diferentes y muchas variaciones diferentes de cada modelo, para elegir los que tendrán un mejor rendimiento, debemos considerar cuál se ajustará mejor a nuestro conjunto de datos.\n\nNuestro conjunto de datos está equilibrado, la entrada y la salida son categóricas y tenemos alrededor de 8.000 instancias. Teniendo esto en cuenta usaremos los siguientes modelos:\n\n* Regresión Logística\n* Naive Bayes\n* Random Forest\n* KNN"},{"metadata":{},"cell_type":"markdown","source":"En primer lugar, definimos una función para mostrar los resultados obtenido por cada uno de los modelos."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = ['LogisticRegression','NaiveBayes','RandomForest','KNearestNeighbors']\n\nscores = [None] * len(models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Regresión Logística"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nlr = LogisticRegression()\nlr.fit(ohe_x_train, ohe_y_train)\ny_pred = lr.predict(ohe_x)\naccuracy = lr.score(ohe_x, ohe_y)\n\nutils.show_results(models, scores, ohe_x, ohe_y, lr, y_pred,\"LogisticRegression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemos obtenido un 100% de precisión, por lo que podríamos estar teniendo un problema de sobreajuste. Para evaluar mejor los modelos de ahora en adelante, usaremos validación cruzada. Al hacerlo, es probable que obtengamos peores resultados, pero estos serán más fiables."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_list = cross_val_score(lr,ohe_x,ohe_y, cv=10)\nscore = np.mean(score_list)\nprint (score)\n\n# we swap the score obtained before with the cross_val_score\nscores[0] = score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2. Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(ohe_x_train, ohe_y_train)\npreds= nb.predict(ohe_x)\nutils.show_results(models, scores, ohe_x, ohe_y, nb, preds,\"NaiveBayes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_list = cross_val_score(nb,ohe_x,ohe_y, cv=5)\nprint(score)\nscore = np.mean(score_list)\n# we swap the score obtained before with the cross_val_score\nscores[1] = score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nuevamente podemos ver cómo después de usar la validación cruzada, nuestra precisión ha disminuido esta vez hasta casi un 85%."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar cómo la precisión del 99% obtenida previamente no era fiable."},{"metadata":{},"cell_type":"markdown","source":"### Búsqueda Grid\nAhora separamos los datos en carpetas para la búsqueda **Grid**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_split = TimeSeriesSplit(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state=1)\nrf_params = {\n    'model__n_estimators': list(range(25,251,25)),\n    'model__max_features': list(np.arange(0.1,0.36,0.05))\n}\nrf_pipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('model', rf)\n])\ngridsearch_rf = GridSearchCV(estimator=rf_pipe,\n                          param_grid = rf_params,\n                          cv = cv_split,\n                         )\ngridsearch_rf.fit(ohe_x_train, ohe_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_best_model = gridsearch_rf.best_estimator_\npreds = rf_best_model.predict(ohe_x)\nutils.show_results(models, scores, ohe_x, ohe_y, rf_best_model, preds,'RandomForest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4. KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn_params = {\n    'n_neighbors': list(range(4,10)),\n    'weights': ['uniform','distance']\n}\n\ngridsearch_knn = GridSearchCV(knn,\n                          param_grid = knn_params,\n                          cv = cv_split,\n                         )\ngridsearch_knn.fit(ohe_x_train, ohe_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_best_model = gridsearch_knn.best_estimator_\npreds = knn_best_model.predict(ohe_x)\nutils.show_results(models, scores, ohe_x, ohe_y, knn_best_model, preds,'KNearestNeighbors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Después de esta búsqueda **grid** podemos pensar que independientemente de la búsqueda de hiperparámetros y el ajuste de las variables el modelo es propenso a obtener precisiones muy altas."},{"metadata":{},"cell_type":"markdown","source":"Veamos, para terminar, un resumen de la precisión obtenida por cada uno de los modelos que hemos estudiado para resolver nuestro problema."},{"metadata":{"trusted":true},"cell_type":"code","source":"utils.plot_accuracy(models, scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos apreciar, RandomForest y KNN son los que nos proporcionan una mayor precisión cuando los enfrentamos contra el conjunto de test que reservamos previamente."},{"metadata":{},"cell_type":"markdown","source":"# 6. ¿Precisión del 100%?"},{"metadata":{},"cell_type":"markdown","source":"Hemos visto cómo todos los modelos probados pueden obtener altas precisiones y lo poco que varían las precisiones entre los diferentes parámetros (ilustrado con el ejemplo KNN).\n\nPara comprender finalmente lo fácil que podemos lograr altas precisiones, trazaremos varias curvas ROC para determinados subgrupos de características aleatorias."},{"metadata":{"trusted":true},"cell_type":"code","source":"palette = sns.set_palette(sns.color_palette('Set1')) #just to define de plot palette\n\nfor i in range(0,5):\n    random.seed(i)\n    randlist = list(names[x] for x in random.sample(range(0,21),k=5))\n    rand_df = train[randlist]\n    rand_df = pd.get_dummies(rand_df)\n\n    utils.plot_roc(rand_df, ohe_y_train)\n    \nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\nplt.gcf().set_size_inches(15,10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusiones"},{"metadata":{},"cell_type":"markdown","source":"Tanto KNN como RandomForests nos darán una precisión casi perfecta. Incluso si las características del conjunto de datos permiten tener fácilmente altas precisiones.\n\nSiempre es importante procesar correctamente los datos y ajustar los modelos correctamente, entendiendo lo que está haciendo el programa en lugar de solo enfocarse en obtener mejores métricas.\n\nAl hacer esto, sabremos si nuestros scores son correctos o si estamos haciendo algo mal."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}