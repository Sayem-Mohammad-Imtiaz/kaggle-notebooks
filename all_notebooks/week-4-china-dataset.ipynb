{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting familiar with the data\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"houses = pd.read_csv('/kaggle/input/lianjia/new.csv',  encoding= 'unicode_escape', low_memory=False)\nhouses.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houses.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Got info about every column"},{"metadata":{},"cell_type":"markdown","source":"I noticed that for the statistics it's better to represent constructionTime as age "},{"metadata":{"trusted":true},"cell_type":"code","source":"houses['constructionTime'] = houses['constructionTime'].astype('str') \ncondition = (houses.constructionTime == '0') | (houses.constructionTime == 'Î´Öª')\ncolumn_name = 'constructionTime'\nhouses.loc[condition, column_name] = '2018'\n\ncondition = (houses.constructionTime == '1')\ncolumn_name = 'constructionTime'\nhouses.loc[condition, column_name] = '2017'\nhouses['year'] = pd.to_datetime(houses['constructionTime'])\nhouses['houseAge'] = (pd.datetime.today() - houses['year']).dt.days*0.00273973","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houses.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Got statistical information about every column and added some new, because the dataset was a encoded differentely\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhouses.hist(bins=50, figsize=(20,15))\n# save_fig(\"attribute_histogram_plots\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Got graphical representation of all the numerical values in the dataset."},{"metadata":{},"cell_type":"markdown","source":"From here I can see that I have some tail-heavy histograms that I will need to handle somehow to make the better predictions"},{"metadata":{},"cell_type":"markdown","source":"# Making a dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(houses, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the data \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"houses = train_set.copy()\nhouses.plot(kind=\"scatter\", x=\"Lng\", y=\"Lat\", alpha=0.3,\n    s=houses[\"communityAverage\"]/10000, label=\"Community average\", figsize=(10,7),\n    c=\"price\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking out if there is any correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = houses.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix[\"price\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can see that community average is highly correlated with price. Also total price and such parameters as condition of the building, subway availability have some correlation to price. On the opposite the square space of the house has negative correlation which means that the more price per square meter the less square space is there."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"price\", \"communityAverage\", \"DOM\",\n              \"renovationCondition\"]\nscatter_matrix(houses[attributes], figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get more correlations I used pandas plotting to see the graphs based on the columns that are the most corralated to price. I got interesting results. For example it seems to be strong correlation between the time the house is on the market(DOM) and its price - the more time it's on market the less it costs. Also the statement the more people(communityAverage) the higher the price per square meter is confirmed on these graphs."},{"metadata":{},"cell_type":"markdown","source":"# Preparing data for the algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nhouses_labels = train_set[\"price\"].copy()\nimputer = SimpleImputer(strategy=\"median\")\nhouses_num = houses.drop([\"url\", 'id', 'tradeTime', 'livingRoom', 'drawingRoom', 'bathRoom', 'floor', 'constructionTime', 'year'], axis=1)\nimputer.fit(houses_num)\ntest = imputer.transform(houses_num)\nhouses_trained = pd.DataFrame(test, columns=houses_num.columns,\n                          index=houses_num.index)\nhouses_trained","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I \"cleaned\" the dataset numerical values using imputer that replaces them with the average value"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhouses_categories = houses[[\"floor\"]]\nhousing_cat_encoded = ordinal_encoder.fit_transform(houses_categories)\nfrom sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(houses_categories)\nhousing_cat_1hot\nhousing_cat_1hot.toarray()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here I noticed that the non-numerical categories are to hard to be transformed, so I just dropped them down"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nprice_ix, communityAverage_ix = 6, 18\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_pricePerCommunityAverage=True): # no *args or **kargs\n        self.add_pricePerCommunityAverage = add_pricePerCommunityAverage\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        if (X[:, communityAverage_ix] == 0).any():\n            pricePerCommunityAverage = X[:, price_ix] / 0.001\n        else:\n            pricePerCommunityAverage = X[:, price_ix] / X[:, communityAverage_ix]\n        if self.add_pricePerCommunityAverage:\n            return np.c_[X, pricePerCommunityAverage]\n\nattr_adder = CombinedAttributesAdder(add_pricePerCommunityAverage=False)\nhouses_extra_attribs = attr_adder.transform(houses.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I added a function for adding a new extra atribute \"pricePerCommunityAverage\""},{"metadata":{},"cell_type":"markdown","source":"# Adding pipelines\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhouses_num_tr = num_pipeline.fit_transform(houses_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding pipeline for numerical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(houses_num)\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n    ])\n\nhouses_prepared = full_pipeline.fit_transform(houses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have no non-numerical values so I just followed the book"},{"metadata":{},"cell_type":"markdown","source":"# Selecting and training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(houses_prepared, houses_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the Linear Regression Model with the prepared data"},{"metadata":{"trusted":true},"cell_type":"code","source":"some_data = houses.iloc[:10]\nsome_labels = houses_labels.iloc[:10]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Labels:\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction are really close to the labels, which either mean that model is overfitting the data or it basically is trained well"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nhouses_predictions = lin_reg.predict(houses_prepared)\nlin_mse = mean_squared_error(houses_labels, houses_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying to find out the error root-square-mean deviation. The deviation is pretty low "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(houses_prepared, houses_labels)\nhousing_predictions = tree_reg.predict(houses_prepared)\ntree_mse = mean_squared_error(houses_labels, houses_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I trained a different model(DecisionTreeRegressor) to see what differences there will be with the previous one. This is a powerful model, capable of finding complex nonlinear relationships in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, houses_prepared, houses_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\ndisplay_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, houses_prepared, houses_labels,\n                              scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I applied \"cross validation\" for DecisionTreeRegressor and LinearRegression. Linear regressin showed better scores. Cross validation means that the dataset is shuffled and splitted into K groups. For each of them, the left groups are taken as a training data set and then fit and evaluated on the test set. The scores that were computed after passing all K number of the test shows the quality of the algorithm"},{"metadata":{},"cell_type":"markdown","source":"# Fine tuning the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nparam_grid = [\n    {'n_estimators': [10, 20], 'max_features': [1, 2]},\n  ]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=2,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\n\ngrid_search.fit(houses_prepared, houses_labels)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the system on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = lin_reg\n\nX_test = test_set\ny_test = test_set[\"price\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}