{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -f utils.py\n!wget -nv https://github.com/minesh1291/stackoverflow/raw/master/machine_learning/utils.py\n!pip install -q kneebow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler \nfrom sklearn.cluster import DBSCAN \nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport keras\nfrom keras import layers\n\nimport utils\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1291","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"satellite = pd.read_csv(\"/kaggle/input/statlog-landsat-satellite/satellite.mat.csv\")\nprint(satellite.shape)\nfeatures = [c for c in satellite.columns if c.startswith(\"V\")]\nsatellite.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with out anomaly\nidx = satellite[\"Y\"]==0\n\nfig, ax = plt.subplots(figsize=(24,5))\npd.DataFrame(satellite.loc[idx].iloc[:1400,1:-1].reset_index(drop=True)).plot(ax=ax, legend=False)\n\nplt.show()\nfig, ax = plt.subplots(figsize=(24,3))\nsatellite.loc[idx].loc[:1400,\"Y\"].reset_index(drop=True).plot(ax=ax)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24,5))\npd.DataFrame(satellite.iloc[-1400:,1:-1]).plot(ax=ax, legend=False)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(24,3))\nsatellite.loc[-1400:,\"Y\"].plot(ax=ax)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_org = satellite.iloc[:,1:-1]\ntrain_y = satellite[\"Y\"]\n\n# train_org = satimage.iloc[:,1:-1]\n# train_y = satimage[\"Y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocessing\n\nscaler = RobustScaler()\n\ntrain_org_scaled = scaler.fit_transform(train_org)\n\nfig, ax = plt.subplots(figsize=(24,4))\npd.DataFrame(train_org.values[-400:,1:5]).plot(ax=ax, legend=False)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(24,4))\npd.DataFrame(train_org_scaled[-400:,1:5]).plot(ax=ax, legend=False)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(24,2))\nsatellite[\"Y\"].iloc[-400:].plot(ax=ax)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_org = pd.DataFrame(train_org_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_resid(x, period=9):\n    return seasonal_decompose(x, model='additive', period=period).resid.fillna(0)\n\ntrain_resid = train_org.apply(get_resid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TIME_STEPS = 150\n\n# Generated training sequences for use in the model.\ndef create_sequences(values, time_steps=TIME_STEPS):\n    output = []\n    for i in range(len(values) - time_steps):\n        output.append(values[i : (i + time_steps)])\n    return np.stack(output)\n\nx_train = create_sequences(train_org)\nprint(\"Training input shape: \", x_train.shape) # last <time_steps> n rows are omitted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_org = train_org.iloc[:-TIME_STEPS]\ntrain_y = satellite[\"Y\"].iloc[:-TIME_STEPS]\n\ntrain_org.shape, train_y.shape, train_y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2 = keras.regularizers.L2(1e-3)\n\ndef get_AE():\n    model = keras.models.Sequential([\n        layers.Input(shape=(TIME_STEPS, 36)),\n        layers.BatchNormalization(),\n        layers.Conv1D(filters=64, kernel_size=15, padding='same', data_format='channels_last',\n            dilation_rate=1, activation=\"linear\", kernel_regularizer=l2),\n        layers.BatchNormalization(),\n        layers.Dropout(0.25),\n        layers.Bidirectional(keras.layers.LSTM(32, activation=\"tanh\", kernel_regularizer=l2)),\n        layers.RepeatVector(TIME_STEPS),\n        layers.Bidirectional(layers.LSTM(32, return_sequences=True, activation=\"tanh\", kernel_regularizer=l2)),\n        layers.BatchNormalization(),\n        layers.Dropout(0.25),\n        layers.Conv1D(filters=64, kernel_size=15, padding='same', data_format='channels_last',\n            dilation_rate=1, activation=\"linear\", kernel_regularizer=l2),\n        layers.BatchNormalization(),\n        layers.Dropout(0.25),\n        layers.TimeDistributed(layers.Dense(36, activation='linear', kernel_regularizer=l2))\n        \n    ])\n    adam = keras.optimizers.Adam(lr=1e-3, decay=1e-11)\n    model.compile(optimizer=adam, loss=\"mse\")\n    return model\n\nmodel = get_AE()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(patience=4, min_delta=1e-5, verbose=1, restore_best_weights=True)\n# cp = keras.callbacks.ModelCheckpoint(filepath=\"best_model.ckp\", verbose=1, save_best_only=True)\ncallbacks=[es] #, cp]\n\nnp.random.seed(SEED)\nmodel = get_AE()\nmodel.fit(x=x_train, y=x_train, batch_size=512//2, validation_split=0.2, epochs=20, \n          callbacks=callbacks\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save(\"best_model.ckp\")\n\n# model = keras.models.load_model(\"best_model.ckp\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_x_train = model.predict(x_train[[1,201,401]])\n\nfeat_idx = 2\n\nplt.plot(x_train[0,:,feat_idx])\nplt.plot(pred_x_train[0,:,feat_idx])\nplt.show()\nplt.plot(x_train[201,:,feat_idx])\nplt.plot(pred_x_train[1,:,feat_idx])\nplt.show()\nplt.plot(x_train[401,:,feat_idx])\nplt.plot(pred_x_train[2,:,feat_idx])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_recons = model.predict(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def make_encoder(ae_model):\n#     encoder = keras.models.Sequential(\n#         ae_model.layers[:5]\n#     )\n#     return encoder\n\n# encoder = make_encoder(model)\n\n# latent_out = encoder.predict(x_train.astype(\"float32\"))\n# latent_out.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = np.concatenate([satellite.iloc[:-TIME_STEPS,1:-1],latent_out], axis=1)\n\nfeats_dict= {\n    \"train_diff\": train_org - x_train_recons[:,0,:],\n    \n#     \"train_resid_w15\": train_org.apply(lambda x: get_resid(x, period=15)),\n#     \"train_resid_w12\": train_org.apply(lambda x: get_resid(x, period=12)),\n    \"train_resid_w9\": train_org.apply(lambda x: get_resid(x, period=9)),\n    \"train_resid_w6\": train_org.apply(lambda x: get_resid(x, period=6)),\n    \"train_resid_w3\": train_org.apply(lambda x: get_resid(x, period=3)),\n    \n    \"roll_diff_6\": train_org - train_org.rolling(6, center=True).median().fillna(0),\n    \"roll_diff_9\": train_org - train_org.rolling(9, center=True).median().fillna(0),\n    \"roll_diff_12\": train_org - train_org.rolling(12, center=True).median().fillna(0),\n#     \"roll_diff_15\": train_org - train_org.rolling(15).median().fillna(0),\n    \n    \"roll_median_6\":  train_org.rolling(6, center=True).median().fillna(0),\n    \"roll_median_9\":  train_org.rolling(9, center=True).median().fillna(0),\n    \"roll_median_12\":  train_org.rolling(12, center=True).median().fillna(0),\n#     \"roll_median_15\":  train_org.rolling(15, center=True).median().fillna(0),\n    \n    \"roll_std_15\":  train_org.rolling(15, center=True).std().fillna(0),\n    \"roll_diff_lq_15\":  train_org - train_org.rolling(15, center=True).quantile(0.15).fillna(0),\n    \"roll_diff_uq_15\":  train_org - train_org.rolling(15, center=True).quantile(0.85).fillna(0),\n}\n\ndata_ls = [train_org]\nfor k,v in feats_dict.items():\n    data_ls.append(v)\n\nX_train = np.concatenate(data_ls, axis=1)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loc, eps = utils.get_eps(X_train)\nprint(\"using eps:\", eps)\n\ndbscan = DBSCAN(eps, n_jobs=-1)\npred_y_out = dbscan.fit_predict(X_train)\nfreq = np.unique(pred_y_out, return_counts=True)\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(X_train[:600]);\n# plt.show();\n# plt.plot(train_y[:600]);\n# plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conta = freq[1][0]/freq[1].sum()\nconta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isof = IsolationForest(n_estimators=100, max_features=0.7, contamination=2/6, random_state=SEED, max_samples=\"auto\")\npred_y_out = isof.fit_predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from statsmodels.tsa.seasonal import seasonal_decompose\n\n# n_steps = 2300\n\n# series = pd.DataFrame(x_train[:n_steps, 0, feat_idx])\n# result = seasonal_decompose(series, model='additive', period=9)\n\n# fig, ax = plt.subplots(5,1,figsize=(24,8), sharex=True)\n# series.plot(ax=ax[0])\n# result.trend.plot(ax=ax[1])\n# result.seasonal.plot(ax=ax[2])\n# result.resid.plot(style=\".\",ax=ax[3])\n# ax[4].plot(train_y[:n_steps])\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = 1000\n\nfig, ax = plt.subplots(figsize=(24,4))\nplt.plot(x_train[:n_steps, 0, feat_idx])\nplt.plot(x_train_recons[:n_steps, 0, feat_idx])\nplt.legend([\"x-feat-i\", \"recons\"])\nplt.show()\n\nfig, ax = plt.subplots(figsize=(24,2))\nplt.plot(train_y[:n_steps])\nplt.legend([\"true_y\"])\nplt.show()\n\nfig, ax = plt.subplots(figsize=(24,2))\nplt.plot(pred_y_out[:n_steps]*-1)\nplt.legend([\"pred_y\"])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {-1:1,1:0}\n\ndef map_values(x):\n    if x in d:\n        return d[x]\n    else:\n        return 0\n\nvectorize = np.vectorize(lambda x: map_values(x))\n\ntrue_y = train_y.astype(\"int8\")\npred_y = vectorize(pred_y_out)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, ax = plt.subplots(2,1,figsize=(24,2))\n\n# n_steps = 1600\n\n# pd.Series(true_y)[:n_steps].plot(ax=ax[0])\n# pd.Series(pred_y).rolling(9, center=True).median().fillna(0)[:n_steps].plot(ax=ax[1])\n\n# plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncr = classification_report(true_y ,pred_y)\n\nprint(cr)\nprint(f\"AUC: {roc_auc_score(true_y, pred_y):0.4f}\")\nconfusion_matrix(true_y, pred_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}