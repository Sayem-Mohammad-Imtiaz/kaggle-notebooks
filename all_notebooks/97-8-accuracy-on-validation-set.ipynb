{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/mobile-price-classification/train.csv')\ntest = pd.read_csv('../input/mobile-price-classification/test.csv',index_col='id')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape, train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there are no null values in any of the sets but there can be 0s in some columns which can actually be missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"px_height, sc_w has minimum value of 0, which is of course missing values, let's mark them as nan to make things easier"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['px_height'] == 0,'px_height'] = np.nan\ntrain.loc[train['sc_w'] == 0,'sc_w'] = np.nan\n\ntest.loc[test['px_height'] == 0,'px_height'] = np.nan\ntest.loc[test['sc_w'] == 0,'sc_w'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts = train['price_range'].value_counts()\nplt.pie(value_counts.values, labels = value_counts.index, autopct='%1.1f%%', startangle=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset is balanced, which makes things easier, let's look at distribution of features with regard to class"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(5, 4,figsize=(19,19))\nfor i, col in enumerate(train.iloc[:,:-1]):\n    sns.histplot(x=col, hue = 'price_range', data=train,ax =ax[i//4][i%4], multiple = 'stack')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like other the main characteristics of a mobile phone is its ram, distribution of price range says so. Other key features which makes visible difference are battery_power, px_width, px_height. Let's look at correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nmat = train.corr()\nsns.heatmap(mat, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" + Correlation between ram and price_range is too high as expected. \n + Screen Height and Screen Width correlation is understandable as phones are created at similar screen ratios. Same thing goes for pixel height and pixel width.\n + There is a little correlation between price range and battery power, pixel height, pixel width.\n + Contrary to my assumptions clock speed and internal memory has no correlation with price range\n + Correlation between 3G and 4G is also high\n + One last thing, primary camera and front camera has similar correlation. \n + The other columns have too little or no correlation between them. I will train model with and without them and compare results.\n \n Let's fill missing values with median"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['px_height'].fillna(train['px_height'].median(), inplace=True)\ntest['px_height'].fillna(test['px_height'].median(), inplace=True)\n\ntrain['sc_w'].fillna(train['sc_w'].median(), inplace=True)\ntest['sc_w'].fillna(test['sc_w'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extracting \"main\" columns for further use"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_cols = ['battery_power', 'px_height','px_width', 'ram']\ntrain_main = train[main_cols+['price_range']]\ntest_main = test[main_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Front camera and primary camera having 0 megapixels means phone doesn't have camera at that side, we can create two extra features from that."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_pc'] = (train['pc'] != 0).astype(np.int8)\ntest['has_pc'] = (test['pc'] != 0).astype(np.int8)\n\ntrain['has_fc'] = (train['fc'] != 0).astype(np.int8)\ntest['has_fc'] = (test['fc'] != 0).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Screen area and pixel count (`px_width * px_height`) can also be useful"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['px_count'] = train['px_width']*train['px_height']\ntest['px_count'] = test['px_width']*test['px_height']\n\ntrain['sc_area'] = train['sc_w']*train['sc_h']\ntest['sc_area'] = test['sc_w']*test['sc_h']\n\ntrain['dp'] = train['px_count']/train['sc_area'] \ntest['dp'] = test['px_count']/test['sc_area']\n\ntrain['sc_ratio'] = train['sc_h']/train['sc_w']\ntest['sc_ratio'] = test['sc_h']/test['sc_w']\n\ntrain['px_ratio'] = train['px_height']/train['px_width']\ntest['px_ratio'] = test['px_height']/test['px_width']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nmat = train[['px_ratio','sc_area','sc_ratio', 'px_count','has_pc','has_fc','dp','price_range']].corr()\nsns.heatmap(mat, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like new features won't do anything too important, but let's keep them where they are"},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full = train.drop(['price_range'], axis = 1)\ny_full = train['price_range']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are categorical variables in data but they can have only 2 values (True, False), so there is no need for one-hot encoding. Linear models work better with scaled values, so we will scale them. But we have to be careful not to scale categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi','has_pc', 'has_fc']\nnum_cols =  list(X_full.drop(cat_cols, axis = 1).columns)\n\n\nscaler = StandardScaler()\nX_full[num_cols] = scaler.fit_transform(X_full[num_cols])\ntest[num_cols] = scaler.transform(test[num_cols])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since it's classification task, we should split data evenly, so we pass y_full as stratify"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_full, y_full,stratify=y_full, random_state=42,test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [LogisticRegression(),RandomForestClassifier(),DecisionTreeClassifier(), KNeighborsClassifier()]\nscores = []\nmodel_names = []\nfor model in models:\n    scores.append(np.mean(cross_val_score(model, X_train, y_train, n_jobs=3, verbose=2, cv=5)))\n    model_names.append(model.__class__.__name__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ng=sns.barplot(model_names, scores)\nax=g\nfor p in ax.patches:\n             ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n                 ha='center', va='center', fontsize=11,  xytext=(0, 10),\n                 textcoords='offset points')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's search for best parameters for Logistic Regression and Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']    ,\n            'penalty':['none'],\n            },\n          { 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n           'penalty':['l1'],\n              'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]},\n          {'solver':['liblinear'],\n           'penalty':['l1','l2'],\n            'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]}\n         ]\nlog_reg_cv = GridSearchCV(LogisticRegression(max_iter=10000), param_grid=params, cv = 5, verbose = 2, n_jobs = -1).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_model = log_reg_cv.best_estimator_\nlog_reg_model.score(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':np.arange(100,2001,100),\n         'max_depth':np.arange(3,31,2)}\nrfc_cv = GridSearchCV(RandomForestClassifier(), param_grid = params, cv =5, verbose = 2, n_jobs = -1).fit(X_train, y_train)\nrfc_cv.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier didn't show any significant improvement, but Logistic Regression gets 97.8% accuracy on the validation set which I think is pretty good. Let's look at confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_model = log_reg_cv.best_estimator_\ny_pred = log_reg_model.predict(X_val)\nmat = confusion_matrix(y_val, y_pred)\nsns.heatmap(mat, annot=True,fmt='1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It misclassified only 13 mobile phones wrong, I think it is good enough to predict test dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_model.fit(X_full, y_full)\ntest_pred = log_reg_model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':test.index,\n                          'class':test_pred})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main columns"},{"metadata":{},"cell_type":"markdown","source":"I wrote that I will check the main (columns with high correlation with target) features. Let's do that"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_main.shape, test_main.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will repeat some steps from before"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_main['px_count'] = train_main['px_width']*train_main['px_height']\ntest_main['px_count'] = test_main['px_width']*test_main['px_height']\ntrain_main['px_ratio'] = train_main['px_height']/train_main['px_width']\ntest_main['px_ratio'] = test_main['px_height']/test_main['px_width']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no categorical variable's in this \"new\" dataset, so we can scale dataset easily"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_main_full = train_main.drop(['price_range'],axis = 1)\ny_main_full = train_main['price_range']\n\nscaler = StandardScaler()\nX_main_full = scaler.fit_transform(X_main_full)\ntest_main = scaler.fit_transform(test_main)\n\nX_main_train, X_main_val, y_main_train, y_main_val = train_test_split(X_main_full, y_main_full,stratify=y_main_full, random_state=42,test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [LogisticRegression(),RandomForestClassifier(),DecisionTreeClassifier(), KNeighborsClassifier()]\nscores = []\nmodel_names = []\nfor model in models:\n    scores.append(np.mean(cross_val_score(model, X_main_train, y_main_train, n_jobs=3, verbose=2, cv=5)))\n    model_names.append(model.__class__.__name__)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\ng=sns.barplot(model_names, scores)\nax=g\nfor p in ax.patches:\n             ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n                 ha='center', va='center', fontsize=11,  xytext=(0, 10),\n                 textcoords='offset points')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There has been great improvement in KNeighborsClassifier. Let's fine-tune it. It would be better if I used knife method, but now I just want to decide if KNeighborsClassifier can come near to LogisticRegression."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_neighbors':range(1,20)}\nknn_cv = GridSearchCV(KNeighborsClassifier(),param_grid = params, cv =5, verbose = 2, n_jobs = -1).fit(X_main_train, y_main_train)\nprint(\"Best score:\",knn_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like not"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']    ,\n            'penalty':['none'],\n            },\n          { 'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n           'penalty':['l1'],\n              'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]},\n          {'solver':['liblinear'],\n           'penalty':['l1','l2'],\n            'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]}\n         ]\nlog_reg_cv = GridSearchCV(LogisticRegression(max_iter=10000), param_grid=params, cv = 5, verbose = 2, n_jobs = -1).fit(X_main_train, y_main_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_main_model = log_reg_cv.best_estimator_\nlog_reg_main_model.score(X_main_val, y_main_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':np.arange(100,301,100),\n         'max_depth':np.arange(7,31,2),\n          \"criterion\": [\"gini\", \"entropy\"]}\nrfc_cv = GridSearchCV(RandomForestClassifier(), param_grid = params, cv =5, verbose = 2, n_jobs = -1).fit(X_main_train, y_main_train)\nrfc_cv.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression outperformed Random Forest Classifier again. With only 6 features we were able to get 96.8% which is only 1% less than accuracy of model trained with all features. Even though accuracy is less than previous model, we can train our models much faster now and if we had more samples in our train set, we could see the difference really easy. I will stop here and consider Logistic Regression best model for this task. If you think this notebook is useful, please upvote. If you have any suggestions, please do write them in comments. so that I can improve myself "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}