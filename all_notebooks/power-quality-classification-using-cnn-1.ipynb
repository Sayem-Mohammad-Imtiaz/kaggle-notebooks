{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Power Quality Classification using CNN","metadata":{}},{"cell_type":"markdown","source":"This notebook focusses on developing a Convolutional Neural Network which classifies a particular power signal into its respective power quality condition. The dataset used here contains signals which belong to one of the 5 classes(power quality condition). This means that each signal is characterized by 128 data points. Here the signals provided are in time domain.\n\nThe power quality condition with respect to the output class value is as follows: <br>\n1 - Normal<br>\n2 - 3rd harmonic wave<br>\n3 - 5th harmonic wave<br>\n4 - Voltage dip<br>\n5 - transient<br>","metadata":{}},{"cell_type":"code","source":"#importing the required libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom scipy.fft import fft,fftfreq\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading the dataset using pandas\ndata = pd.read_csv(\"../input/powerqualitydistributiondataset1/PowerQualityDistributionDataset1.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The dataset is already preprocessed\ndata.drop(data.columns[[0]],axis=1,inplace=True)\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_arr = data.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data transformation","metadata":{}},{"cell_type":"markdown","source":"The data transformation steps employed here are as follows:<br>\n\n1) Fourier Transform<br>\n2) Normalization\n","metadata":{}},{"cell_type":"code","source":"#In this segment we are plotting one wave from each class after applying fourier transformation \nyf = np.abs(fft(data_arr[0][0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, yf)\nplt.show()\nprint(\"class\",data_arr[0][128], \"Normal wave\")\n\nyf = np.abs(fft(data_arr[1][0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, yf)\nplt.show()\nprint(\"class\",data_arr[1][128], \"3rd harmonic wave\")\n\nyf = np.abs(fft(data_arr[3][0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, yf)\nplt.show()\nprint(\"class\",data_arr[3][128], \"5th harmonic wave\")\n\nyf = np.abs(fft(data_arr[6][0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, yf)\nplt.show()\nprint(\"class\",data_arr[6][128], \"Voltage dip\")\n\nyf = np.abs(fft(data_arr[8][0:128]))\nxf = fftfreq(128,1/128)\nplt.plot(xf, yf)\nplt.show()\nprint(\"class\",data_arr[8][128], \"Transient wave\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are splitting the dataset in the ratio of 60%,20%,20% (training set,validation set, test set)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.loc[:,data.columns != 'output'],data['output'],test_size=0.2)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are overwritting the dataframe with the waves which we obtained after doing fourier transformation\nx_train = x_train.to_numpy()\nx_test = x_test.to_numpy()\nx_val = x_val.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we are performing normalization\ntransform = StandardScaler()\nx_train_tr = transform.fit_transform(x_train)\nx_test_tr = transform.fit_transform(x_test)\nx_val_tr = transform.fit_transform(x_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model creation and training","metadata":{}},{"cell_type":"code","source":"# get_dummies function is used here to perform one hot encoding of the y_* numpy arrays\ny_train_hot = pd.get_dummies(y_train)\ny_test_hot = pd.get_dummies(y_test)\ny_val_hot = pd.get_dummies(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training\",x_train.shape)\nprint(y_train_hot.shape)\nprint(\"Validation\",x_val.shape)\nprint(y_val_hot.shape)\nprint(\"Test\",x_test.shape)\nprint(y_test_hot.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshaping the Data so that it could be used in 1D CNN\nx_train_re = x_train.reshape(x_train_tr.shape[0],x_train_tr.shape[1], 1)\nx_test_re = x_test.reshape(x_test_tr.shape[0],x_test_tr.shape[1], 1)\nx_val_re = x_val.reshape(x_val_tr.shape[0],x_val_tr.shape[1], 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_re.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing required modules for working with CNN\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\nfrom tensorflow.keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initializing required parameters for the model\nbatch_size = 64\nnum_classes = 5\nepochs = 20\ninput_shape=(x_train.shape[1], 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=(2)))\n\nmodel.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=(2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compiling the model\n\nlog_dir = \"logs1/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training the model\nhistory = model.fit(x_train_re, y_train_hot, batch_size=batch_size, epochs=epochs, validation_data=(x_val_re, y_val_hot), callbacks=[tensorboard_callback])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs1/fit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.metrics_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"code","source":"np.mean(history.history['accuracy']) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_acc = model.evaluate(x_test_re,y_test_hot)\nprint(\"Test accuracy is {}\".format(pred_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array = confusion_matrix(y_test_hot.to_numpy().argmax(axis=1), model.predict(x_test_re).argmax(axis=1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_cm = pd.DataFrame(array, index = [i for i in [\"Type-1\",\"Type-2\",\"Type-3\",\"Type-4\",\"Type-5\"]],\n                  columns = [i for i in [\"Type-1\",\"Type-2\",\"Type-3\",\"Type-4\",\"Type-5\"]])\nplt.figure(figsize = (13,9))\nsn.heatmap(to_cm, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}