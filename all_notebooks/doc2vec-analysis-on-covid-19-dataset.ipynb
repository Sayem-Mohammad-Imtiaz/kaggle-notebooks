{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Our Team\n\nThis submission is the result of a collaboration between the following:\n\n* Catherine Doyle ([Catherine Doyle](https://www.kaggle.com/cddata))\n* John Doyle ([JohnDoyle](https://www.kaggle.com/johndoyle))\n* Keith Finnerty ([KeithFinnerty](https://www.kaggle.com/ketihf))\n* Piyush Rumao ([PIYUSH RUMAO](https://www.kaggle.com/piyushrumao))\n\nWe are all members of the Data Insights team in Deutsche Bank. Sitting within the Chief Data Office, the Data Insights team is the global Centre of Excellence for data science and data analytics within Deutsche Bank. Mostly based in Dublin, Ireland with some team members sitting in London and other locations, the Data Insights team comprises approximately forty people and includes data scientists, data analysts, data engineers, data architects, data visualization experts, and programme managers. We have expertise in areas such as advanced analytics, artificial intelligence, machine learning, natural language processing, visualization, dashboards, and software development.  We engage with teams across all areas and functions of the Bank, partnering with them to design and deliver analytics solutions that leverage the large amount of available data in order to create value for the Bank and our clients. Engagements vary between time-boxed Proofs of Concept and longer-term projects, and are conducted using the Scaled Agile Framework (SAFe).  As a Centre of Excellence, we also work to uplift data science and analytics across the Bank, for example by fostering Communities of Practice on different topics and rolling out governance support and resources around best practices for model development and analytics delivery.\n\nWe decided to collaborate on this Kaggle challenge and pool our knowledge and skills with the aim of making a positive impact in the on-going struggle against COVID-19.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Key Highlights:\n\nThis notebook is an extension to previous work done on [word embedding](https://www.kaggle.com/piyushrumao/word-embedding-analysis-on-covid-19-dataset) and will be using that as reference to explore detailed findings to discover more relevant research papers that would answer the questions which are presented as Tasks in this competition. Inorder to do that we will be going through following steps:\n\n1. Pre-process data and generate combined dataframe from it\n\n2. Train Doc2Vec models (currently done externally and added in notebook)\n\n3. Use vectors of Doc2Vec model (all input training data) to train a Unsupervised K-Nearest Neighbour model\n\n4. Create list of tasks (questions that we need answers for)\n\n5. Pass this list to Doc2Vec model first to generate vectors from it and then use this vectors to find its nearest neighbours from the trained KNN model.\n\n6. The result will be most relevant documents for our given question.\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport re\nimport errno\nimport json\nimport pickle\nimport glob\nimport multiprocessing\nfrom time import time  # To time our operations\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nnp.random.seed(2018)\n\nfrom gensim import corpora, models\nfrom gensim.models import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec\nfrom gensim.models.phrases import Phrases, Phraser\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.neighbors import NearestNeighbors\n\n\nimport pyLDAvis\nfrom pyLDAvis import sklearn as sklearn_lda\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('starting execution')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning and Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(txt):\n    \"\"\"\n    Basic string loading code.\n\n    :param txt:\n    :return:\n    \"\"\"\n    txt = re.sub(r'.\\n+', '. ', txt)  # replace multiple newlines with period\n    txt = re.sub(r'\\n+', '', txt)  # replace multiple newlines with period\n    txt = re.sub(r'\\[\\d+\\]', ' ', txt)  # remove reference numbers\n    txt = re.sub(' +', ' ', txt)\n    txt = re.sub(',', ' ', txt)\n    txt = re.sub(r'\\([^()]*\\)', '', txt)\n    txt = re.sub(r'https?:\\S+\\sdoi', '', txt)\n    txt = re.sub(r'biorxiv', '', txt)\n    txt = re.sub(r'preprint', '', txt)\n    txt = re.sub(r':', ' ', txt)\n    return txt.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class document():\n    def __init__(self, file_path):\n        if file_path:\n            with open(file_path) as file:\n                data = json.load(file)\n                self.paper_id = data['paper_id']\n                self.title = data['metadata']['title']\n                self.abstract_tripples = {}\n                self.text_tripples = {}\n                self.key_phrases = \"\"\n                self.abstract = \"\"\n                self.text = \"\"\n                self.entities = {}\n                if 'abstract' in data:\n                    for section in data['abstract']:\n                        self.abstract = self.abstract + \"\\n\" + section[\"text\"]\n\n                for section in data['body_text']:\n                    self.text = self.text + \"\\n\" + section['text']\n\n    def clean_text(self):\n        self.abstract = clean(self.abstract)\n        self.text = clean(self.text)\n        self.title =clean(self.title)\n        final_data_dict = self.combine_data()\n        return final_data_dict\n\n    def combine_data(self):\n        self.data = {'paper_id': self.paper_id,\n                     'title': self.title,\n                     'abstract': self.abstract,\n                     'text': self.text,\n                     'abstract_tripples': self.abstract_tripples,\n                     'text_tripples': self.text_tripples,\n                     'key_phrases': self.key_phrases,\n                     'entities': self.entities}\n        return self.data\n\n    def extract_data(self):\n\n        self.paper_id = self.data['paper_id']\n        self.title = self.data['title']\n        self.abstract = self.data['abstract']\n        self.text = self.data['text']\n        self.abstract_tripples = self.data['abstract_tripples']\n        self.text_tripples = self.data['text_tripples']\n        self.key_phrases = self.data['key_phrases']\n        self.entities = self.data['entities']\n\n    def save(self, dir):\n        self.combine_data()\n\n        if not os.path.exists(os.path.dirname(dir)):\n            try:\n                os.makedirs(os.path.dirname(dir))\n            except OSError as exc:  # Guard against race condition\n                if exc.errno != errno.EEXIST:\n                    raise\n\n        with open(dir, 'w') as json_file:\n            json_file.write(json.dumps(self.data))\n\n    def load_saved_data(self, dir):\n        with open(dir) as json_file:\n            self.data = json.load(json_file)\n        self.extract_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"desired_dirs=['/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset',\n             '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv',\n             '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset',\n             '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license']\n\nexp_dir = ['/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv']\n\nnoncomm_use_subset=[]\nbiorxiv_medrxiv = []\ncomm_use_subset = []\ncustom_license = []\n\nfor individual_dirs in desired_dirs:\n    files_list = []\n    data = []\n    print('############## Directory Working on #################')\n    print(individual_dirs)\n    print(individual_dirs.split('/')[-1])\n    for dirname,_, filenames in os.walk(individual_dirs):\n        #print(dirname)\n        for filename in filenames:\n            #print(os.path.join(dirname, filename))\n            files_list.append(os.path.join(dirname, filename))\n    print(len(files_list))\n    #print(files_list)\n    i=0\n    for individual_file in files_list:\n        try:\n            pub = document(individual_file)\n            data_dict = pub.clean_text()\n            #print(data_dict)\n            data.append(data_dict)\n            i+=1\n        except:\n            pass\n    print('Now writing back data')\n    print('files processed===>'+str(i))\n    with open(individual_dirs.split('/')[-1]+'.pickle', \"wb\") as f:\n                pickle.dump(data,f)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_dataframe = pd.DataFrame()\nmodels = glob.glob('/kaggle/working/' + \"*.pickle\")\n#models = glob.glob('/kaggle/working/biorxiv_medrxiv.pickle')\n# print('models via glob===>'+str(models))\nfor individual_model in models:\n    print(individual_model)\n    # open a file, where you stored the pickled data\n    file = open(individual_model, 'rb')\n\n    # dump information to that file\n    data = pickle.load(file)\n\n    # close the file\n    file.close()\n\n    print('Showing the pickled data:')\n    my_df = pd.DataFrame(data)\n    print(my_df.shape)\n    my_df = my_df.replace(r'^\\s*$', np.nan, regex=True)\n    total_dataframe = total_dataframe.append(my_df, ignore_index=True)\n    print(my_df.isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total data in dataframe')\ntotal_dataframe = total_dataframe.drop(['abstract_tripples', 'text_tripples','entities','key_phrases'], axis=1)\nprint(total_dataframe.shape)\ntotal_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_dataframe.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_dataframe.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_df = pd.DataFrame()\ntf_df['merged_text'] = total_dataframe['title'].astype(str) +  total_dataframe['abstract'].astype(str) +  total_dataframe['text'].astype(str)\ntf_df['paper_id'] = total_dataframe['paper_id']\nprint(tf_df.shape)\ntf_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Doc2Vec Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_corpus(df, column, tokens_only=False):\n    \"\"\"\n    Arguments\n    ---------\n        df: pd.DataFrame\n        column: str \n            text column name\n        tokens_only: bool\n            wether to add tags or not\n    \"\"\"\n    for i, line in enumerate(df[column]):\n\n        tokens = gensim.parsing.preprocess_string(line)\n        if tokens_only:\n            yield tokens\n        else:\n            # For training data, add tags\n            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n            \n\"\"\"\ntrain_corpus = (list(read_corpus(tf_df, 'merged_text')))\n# using distributed memory model\nmodel = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=60, min_count=5, epochs=20, seed=42, workers=6)\nmodel.build_vocab(train_corpus)\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n\nmodel.save('dm_doc2vec.model')\n\"\"\"\nmodel_loaded_dm = Doc2Vec.load('/kaggle/input/doc2vec-models/dm_doc2vec.model')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tasks that needs answers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_doc_vector(doc):\n    tokens = gensim.parsing.preprocess_string(doc)\n    vector = model_loaded_dm.infer_vector(tokens)\n    return vector\n\n\n\nTask_1 = \"What is known about transmission, incubation, and environmental stability?\"\nTask_2 = \"What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\"\nTask_3 = \"What do we know about virus genetics, origin, and evolution?\"\nTask_4 = \"What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?\"\nTask_5 = \"What do we know about the effectiveness of non-pharmaceutical interventions?\"\nTask_6 = \"What do we know about diagnostics and surveillance?\"\nTask_7 = \"What has been published about medical care?\"\nTask_8 = \"What has been published concerning ethical considerations for research?\"\nTask_9 = \"What has been published about information sharing and inter-sectoral collaboration?\" \nlist_of_tasks = [Task_1,Task_2,Task_3,Task_4,Task_5,Task_6,Task_7,Task_8,Task_9]\n\nabstract_vectors = model_loaded_dm.docvecs.vectors_docs\narray_of_tasks = [get_doc_vector(task) for task in list_of_tasks]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_df['abstract_vector'] = [vec for vec in abstract_vectors]\ntf_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_array = tf_df['abstract_vector'].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Unsupervised Nearest Neighbours on vectors generated from Doc2Vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ball_tree = NearestNeighbors(algorithm='ball_tree', leaf_size=20).fit(train_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distances, indices = ball_tree.kneighbors(array_of_tasks, n_neighbors=3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(total_dataframe.shape)\nprint(tf_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# View Findings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, info in enumerate(list_of_tasks):\n    print(\"=\"*80, f\"\\n\\nTask = {info[:100]}\\n\", )\n    df =  total_dataframe.iloc[indices[i]]\n    text = df['text']\n    titles = df['title']\n    dist = distances[i]\n    for l in range(len(dist)):\n        try:\n            print(f\" Text index = {indices[i][l]} \\n Distance = {distances[i][l]} \\n Title: {titles.iloc[l]} \\n Text extract: {text.iloc[l][:200]}\\n\\n\")\n        except Exception as e:\n            print('Error Occured==>'+str(e))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}