{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport numpy as np\nimport PIL.Image\n\nfrom os import listdir\n\nfrom numpy import array\nfrom numpy import argmax\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16 ,preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom pickle import dump, load\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n\nfrom nltk.translate.bleu_score import corpus_bleu","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"CSV_PATH=\"../input/flickr-image-dataset/flickr30k_images/results.csv\"\nresults = pd.read_csv(CSV_PATH,error_bad_lines=False,sep=\"|\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.columns = [i.strip() for i in results.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(results[['image_name']],columns=['image_name'])\ndf.to_csv('flickr30kimages.txt',sep=' ',header=None,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(results[['image_name','comment']],columns=['image_name','comment'])\ndf['image_name'] = df['image_name'].apply(lambda x: x[:-4])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('flickr30kdescribtion.txt',sep=' ',header=None,index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# File loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_file(filename):\n    file = open(filename, 'r')\n    text = file.readlines()\n    file.close()\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Describtion preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_clean_descriptions(filename, photos):\n    \n    file = load_file(filename)\n    \n    descriptions = dict()\n    for line in file:\n       \n        words = line.split()\n        \n        image_id, image_description = words[0], words[1:]\n       \n        if image_id in photos:\n            \n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            \n           \n            \n            desc = 'startseq ' + ' '.join(image_description[1:-1]) + ' endseq'\n            descriptions[image_id].append(desc)\n    return descriptions\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Images names handler"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_photo_identifiers(filename):\n    \n    file = load_file(filename)\n    \n    photos = list()\n    \n    for line in file:\n        if len(line) < 1:\n            continue\n        \n        identifier = line.split('.')[0]\n        \n        photos.append(identifier)\n        \n    return set(photos)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = './flickr30kimages.txt'\ntrain = load_photo_identifiers(filename) # Set of train images names\nprint('Dataset: ', len(train))\ntrain_descriptions = load_clean_descriptions('./flickr30kdescribtion.txt', train)\nprint('Descriptions: train=', len(train_descriptions))\ntokenizer = create_tokenizer(train_descriptions)\ndump(tokenizer, open('tokenizer30k.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features_resnet(directory):\n    model = VGG16()\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    #print(model.summary())\n    features = dict()\n    l = listdir(directory)\n    l.remove(\"flickr30k_images\")\n    l.remove(\"results.csv\")\n    for name in l:\n        filename = os.path.join(directory  , name)\n        image = load_img(filename, target_size=(224, 224))\n        image = img_to_array(image)\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        image = preprocess_input(image)/ 255.0\n        feature = model.predict(image, verbose=2)\n        image_id = name.split('.')[0]\n        features[image_id] = feature\n    return features\nfeatures = extract_features_resnet(\"../input/flickr-image-dataset/flickr30k_images/flickr30k_images/\")\n\nprint('Extracted Features: ', len(features))\n\ndump(features, open('features30k.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decoder Layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath='./',\n    monitor='accuracy',\n    mode='auto',\n    save_freq=\"epoch\")\ndef define_model_resnet(vocab_size, max_length):\n    \n    # feature extractor model\n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n\n    # sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n\n    # decoder model\n    decoder1 = tf.keras.layers.add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    \n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n    \n    #print(model.summary())\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import islice\ndef data_generator(descriptions, photos, tokenizer, max_length):\n    while True:\n        for key, description_list in descriptions.items():\n            photo = photos[key][0]\n            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, photo)\n            yield  [input_image,input_sequence],output_word\n\ndef create_sequences(tokenizer, max_length, desc_list, photo):\n    X1, X2, y = list(), list(), list()\n    for desc in desc_list:\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        for i in range(1, len(seq)):\n            in_seq, out_seq = seq[:i], seq[i]\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n            # store\n            X1.append(photo)\n            X2.append(in_seq)\n            y.append(out_seq)\n    return array(X1), array(X2), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_photo_features(filename, photos):\n    \n    all_features = load(open(filename, 'rb'))\n    \n    features = {k: all_features[k] for k in photos}\n    \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = load_photo_features('features30k_resnet.pkl', train)\nvocab_size = len(tokenizer.word_index) + 1\nlines = to_lines(train_descriptions)\nmax_length = max(len(d.split()) for d in lines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = define_model_resnet(vocab_size, max_length)\nepochs = 20\nsteps = len(train_descriptions)\ngenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\nfor i in range(epochs):\n    model.fit(generator, epochs=1,steps_per_epoch=steps,verbose=1,callbacks = [model_checkpoint_callback])\n    #model.save('decoder_resnet_30k_' + str(i) + '.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encoder_resnet(filename):\n    model = VGG16(weights=\"imagenet\")\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    image = load_img(filename, target_size=(224, 224))\n    image = img_to_array(image)\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    image = preprocess_input(image)\n    feature = model.predict(image, verbose=0)\n    return feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = argmax(yhat)\n        word = word_for_id(yhat, tokenizer)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = load_model('decoder_vgg.h5')\n# path = './WIN_20210109_20_48_40_Pro.jpg'\n# photo = encoder_resnet(path)\n# photo.shape\n# description = generate_desc(model, tokenizer, photo, max_length)\n# print(description)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n#     actual, predicted = list(), list()\n#     for key, desc_list in descriptions.items():\n#         prediction = generate_desc(model, tokenizer, photos[key], max_length)\n#         actual_desc = [d.split() for d in desc_list]\n#         actual.append(actual_desc)\n#         predicted.append(prediction.split())\n\n#     print('BLEU-1: ', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n#     print('BLEU-2: ', corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n#     print('BLEU-3: ', corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n#     print('BLEU-4: ', corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filename = 'decoder_vgg.h5'\n# model = load_model(filename)\n# evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}