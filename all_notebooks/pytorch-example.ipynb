{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport itertools\nimport warnings\nfrom joblib import dump\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats, signal\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nwarnings.filterwarnings(\"ignore\")\nrandom_state = 42\nactivities = [\"hair\", \"listen\", \"sidepump\", \"dab\", \"wipetable\", \"gun\", \"elbowkick\", \"pointhigh\", \"logout\"]\nn_labels = len(activities)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs = []\n\ndef load_data(activity, label, trials, subject):\n    for trial in trials:\n        df = pd.read_csv(os.path.join(\"/kaggle/input/human-activity-recognition-dancing/\" + subject, activity+str(trial)+\".csv\"), sep=\",\", index_col=0)\n        df = df[40:-40]\n        df[\"activity\"] = [label for _ in range(len(df))]\n        df[\"activityName\"] = [activity for _ in range(len(df))]\n        df[\"trial\"] = [trial for _ in range(len(df))]\n        df[\"subject\"] = [subject for _ in range(len(df))]\n        dfs.append(df)\n\nids = [1, 2]\nuser = \"subject1\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject2\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject3\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject4\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject5\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject6\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject7\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\n\ndf = pd.concat(dfs)\nprint(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_distribution_of_data(df, title=\"\"):\n    activity, counts = np.unique(df.activityName, return_counts=True)\n    index = np.arange(len(activity))\n    plt.figure(figsize=(12, 5))\n    plt.bar(index, counts, align = 'center', alpha=0.5, color='lightblue')\n    plt.suptitle(title, fontsize=16)\n    plt.xticks(index, activity, rotation=90, fontsize=8)\n    plt.xlim([-.5, 8.5])\n    plt.ylabel('frequency', fontsize=10)\n    for i, v in enumerate(counts):\n        plt.text(i, v, str(v), color='grey', fontweight='bold')\n    plt.show()\n\nplot_distribution_of_data(df, title='Class Distribution Of Dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_distribution_of_data_per_user(df, title=\"\"):\n    plt.figure(figsize=(16,8))\n    plt.title(title, fontsize=20)\n    sns.countplot(x='subject',hue='activityName', data=df)\n    plt.show()\n\nplot_distribution_of_data_per_user(df, title='Data Collected Per User')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_timeseries(features, num_time_steps, num_features):\n    data = np.reshape(features, newshape=(num_time_steps, num_features))\n    return data.T\n\nnum_time_steps = 60\nX = list()\ny = list()\ndf_len = len(df)\nfor idx in range(0, df_len, num_time_steps//2):\n    window_df = df[idx:idx+num_time_steps]\n    labels = window_df[\"activity\"].unique()\n    trials = window_df[\"trial\"].unique()\n    subjects = window_df[\"subject\"].unique()\n    if len(labels) != 1 or len(trials) != 1 or len(subjects) != 1 or len(window_df) < num_time_steps:\n        continue\n    assert len(labels) == 1 and len(window_df) == num_time_steps\n    features = window_df.drop(columns=[\"activity\", \"activityName\", \"subject\", \"trial\"]).values\n    features = convert_to_timeseries(features, num_time_steps=num_time_steps, num_features=9)\n    X.append(features)\n    y.append(labels)\n\nX = np.array(X)\ny = np.array(y)\n\nX.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix'):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(15,8))\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        cmap='Reds'\n        print(\"Normalized Confusion Matrix\")\n    else:\n        cmap='Greens'\n        print('Confusion Matrix Without Normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            上次验证集损失值改善后等待几个epoch\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            如果是True，为每个验证集损失值改善打印一条信息\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            监测数量的最小变化，以符合改进的要求\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''\n        Saves model when validation loss decrease.\n        验证损失减少时保存模型。\n        '''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model, 'finish_model.pkl')                 # 这里会存储迄今最优的模型\n        self.val_loss_min = val_loss\n\ndef plot_avg_loss_per_epoch(avg_train_losses, avg_valid_losses):\n    # visualize the loss as the network trained\n    fig = plt.figure(figsize=(10,8))\n    plt.plot(range(1,len(avg_train_losses)+1), avg_train_losses, label='Training Loss')\n    plt.plot(range(1,len(avg_valid_losses)+1), avg_valid_losses,label='Validation Loss')\n\n    # find position of lowest validation loss\n    minposs = avg_train_losses.index(min(avg_train_losses))+1 \n    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    plt.ylim(0, 2) # consistent scale\n    plt.xlim(0, len(avg_train_losses)+1) # consistent scale\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\ndef plot_accuracy_per_epoch(train_accuracies, valid_accuracies):\n    # visualize the loss as the network trained\n    fig = plt.figure(figsize=(10,8))\n    plt.plot(range(1,len(train_accuracies)+1), train_accuracies, label='Train Accuracy')\n    plt.plot(range(1,len(valid_accuracies)+1), valid_accuracies,label='Valid Accuracy')\n\n    plt.xlabel('epochs')\n    plt.ylabel('accuracy')\n    plt.ylim(0, 1) # consistent scale\n    plt.xlim(0, len(train_accuracies)+1) # consistent scale\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DLTrainer:\n    def __init__(self, model, trainloader, validloader, testloader, criterion, optimizer, early_stopping, n_epochs, patience, is_time_series=False):\n        self.model = model\n        self.trainloader = trainloader\n        self.validloader = validloader\n        self.testloader = testloader\n        \n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.early_stopping = early_stopping\n        self.n_epochs = n_epochs\n        self.patience = patience\n        self.is_time_series = is_time_series\n\n        self.avg_train_losses = []\n        self.avg_valid_losses = [] \n        self.train_accuracies = []\n        self.valid_accuracies = []\n\n    def train(self):\n        self.model.cuda()\n        for epoch in range(self.n_epochs):  # loop over the dataset multiple times\n            train_correct, train_total, train_loss = self.train_epoch()\n            valid_correct, valid_total, valid_loss = self.validate_epoch()\n\n            # calculate average loss over an epoch\n            train_accuracy = train_correct / train_total\n            valid_accuracy = valid_correct / valid_total\n\n            self.avg_train_losses.append(train_loss)\n            self.avg_valid_losses.append(valid_loss)\n            self.train_accuracies.append(train_accuracy)\n            self.valid_accuracies.append(valid_accuracy)\n\n            log_message = f'[{str(epoch)}/{str(self.n_epochs)}] train_loss: {train_loss:.5f} valid_loss: {valid_loss:.5f} train_accuracy: {train_accuracy:.5f} valid_accuracy: {valid_accuracy:.5f}'\n            print(log_message)\n\n            self.early_stopping(valid_loss, self.model)\n            \n            if self.early_stopping.early_stop:\n                break\n\n    def train_epoch(self):\n        correct = 0\n        total = 0\n        losses = []\n\n        self.model.train()\n        for data in self.trainloader:\n            inputs, labels = data\n            inputs = inputs.to(\"cuda\")\n            labels = labels.to(\"cuda\")\n\n            self.optimizer.zero_grad()\n\n            outputs = self.model(inputs.float())\n\n            loss = self.criterion(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n\n            losses.append(loss.item())\n\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        return correct, total, np.mean(losses)\n\n    def validate_epoch(self):\n        correct = 0\n        total = 0\n        losses = []\n\n        self.model.eval()\n        for data in validloader:\n            inputs, labels = data\n            inputs = inputs.to(\"cuda\")\n            labels = labels.to(\"cuda\")\n\n            outputs = self.model(inputs.float())\n            loss = self.criterion(outputs, labels)\n            losses.append(loss.item())\n\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        return correct, total, np.mean(losses)\n\n    def evaluate(self, dataloader):\n        correct = 0\n        total = 0\n        actual = []\n        expected = []\n\n        with torch.no_grad():\n            for data in dataloader:\n                inputs, labels = data\n                inputs = inputs.to(\"cuda\")\n                labels = labels.to(\"cuda\")\n                outputs = self.model(inputs.float())\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                expected += labels.cpu()\n                actual += predicted.cpu()\n\n        return correct/total, (expected, actual)\n\n    def show_evaluation(self):\n        return self.avg_train_losses, self.avg_valid_losses, self.train_accuracies, self.valid_accuracies\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_mean(data):\n    return np.mean(data)\n\ndef compute_variance(data):\n    return np.var(data)\n\ndef compute_median_absolute_deviation(data):\n    return stats.median_absolute_deviation(data)\n\ndef compute_root_mean_square(data):\n    def compose(*fs):\n        def wrapped(x):\n            for f in fs[::-1]:\n                x = f(x)\n            return x\n        return wrapped\n    rms = compose(np.sqrt, np.mean, np.square)\n    return rms(data)\n\ndef compute_interquartile_range(data):\n    return stats.iqr(data)\n\ndef compute_percentile_75(data):\n    return np.percentile(data, 75)\n\ndef compute_kurtosis(data):\n    return stats.kurtosis(data)\n\ndef compute_min_max(data):\n    return np.max(data) - np.min(data)\n\ndef compute_signal_magnitude_area(data):\n    return np.sum(data) / len(data)\n\ndef compute_zero_crossing_rate(data):\n    return ((data[:-1] * data[1:]) < 0).sum()\n\ndef compute_spectral_centroid(data):\n    spectrum = np.abs(np.fft.rfft(data))\n    normalized_spectrum = spectrum / np.sum(spectrum)  # like a probability mass function\n    normalized_frequencies = np.linspace(0, 1, len(spectrum))\n    spectral_centroid = np.sum(normalized_frequencies * normalized_spectrum)\n    return spectral_centroid\n\ndef compute_spectral_entropy(data):\n    freqs, power_density = signal.welch(data)\n    return stats.entropy(power_density)\n\ndef compute_spectral_energy(data):\n    freqs, power_density = signal.welch(data)\n    return np.sum(np.square(power_density))\n\ndef compute_principle_frequency(data):\n    freqs, power_density = signal.welch(data)\n    return freqs[np.argmax(np.square(power_density))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = []\nfor i in [\"yaw\", \"pitch\", \"row\", \"gyro_x\", \"gyro_y\", \"gyro_z\", \"acc_x\", \"acc_y\", \"acc_z\"]:\n        for j in [\"_mean\", \"_var\", \"_mad\", \"_rms\", \"_iqr\", \"_per75\", \"_kurtosis\", \"_min_max\", \"_sma\", \"_zcr\", \"_sc\", \"_entropy\", \"_energy\", \"_pfreq\"]:\n            feature_names.append(i+j)\n\nn_features = len(feature_names)\n\ndef rename_raw_data_headers(X, columns):\n    X = pd.DataFrame(X)\n    X.columns = columns\n    return X\n\ndef extract_raw_data_features_per_row(f_n):\n    f1_mean = compute_mean(f_n)\n    f1_var = compute_variance(f_n)\n    f1_mad = compute_median_absolute_deviation(f_n)\n    f1_rms = compute_root_mean_square(f_n)\n    f1_iqr = compute_interquartile_range(f_n)\n    f1_per75 = compute_percentile_75(f_n)\n    f1_kurtosis = compute_kurtosis(f_n)\n    f1_min_max = compute_min_max(f_n)\n    f1_sma = compute_signal_magnitude_area(f_n)\n    f1_zcr = compute_zero_crossing_rate(f_n)\n    f1_sc = compute_spectral_centroid(f_n)\n    f1_entropy = compute_spectral_entropy(f_n)\n    f1_energy = compute_spectral_energy(f_n)\n    f1_pfreq = compute_principle_frequency(f_n)\n    return f1_mean, f1_var, f1_mad, f1_rms, f1_iqr, f1_per75, f1_kurtosis, f1_min_max, f1_sma, f1_zcr, f1_sc, f1_entropy, f1_energy, f1_pfreq\n\ndef extract_raw_data_features(X):\n    new_features = np.ones((X.shape[0], n_features))\n    rows = X.shape[0]\n    cols = X.shape[1]\n\n    for row in range(rows):\n        features = []\n        for col in range(cols):\n            f_n = X[row][col]\n            feature = extract_raw_data_features_per_row(f_n)\n            features.extend(feature)\n        new_features[row] = np.array(features)\n\n    return new_features\n\n\ndef prepare_raw_data_dataframe(X, y, random_state=42):\n    X = extract_raw_data_features(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n    X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=random_state)\n\n    return X_train, X_valid, X_test, y_train, y_valid, y_test\n\nX_train, X_valid, X_test, y_train, y_valid, y_test = prepare_raw_data_dataframe(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\n\ndef scale_data(data, is_train=False):\n    if is_train:\n        data = scaler.fit_transform(data)\n    else:\n        data = scaler.transform(data)\n    return data\n\nX_train = scale_data(X_train, is_train=True)\nX_valid = scale_data(X_valid)\nX_test = scale_data(X_test)\nX_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.DataFrame(X_train, columns=feature_names)\ndf2[\"activityName\"] = y_train\ndf2[\"activityName\"] = df2[\"activityName\"].replace([0,1,2,3,4,5,6,7,8], activities)\ndf2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exploratory_features = [\"acc_z_min_max\", \"pitch_pfreq\", \"acc_z_kurtosis\"]\n\ndef plot_boxplot_activity_distribution(data, y):\n    sns.boxplot(x='activityName', y=y, data=data)\n    plt.axhline(y=0.08, xmin=0.1, xmax=0.9,c='m',dashes=(5,3))\n    plt.title(feature, fontsize=15)\n    plt.xticks(rotation = 40)\n    plt.show()\n\nfor feature in exploratory_features:\n    plot_boxplot_activity_distribution(df2, feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perplexities = [2,5,10,20,50]\n\ndef plot_tsne(X_data, y_data, perplexity, n_iter=1000, img_name_prefix='t-sne'):\n    # perform t-sne\n    X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n    \n    # prepare the data for seaborn         \n    df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n    \n    # draw the plot in appropriate place in the grid\n    sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\\\n                palette=\"Set1\")\n    plt.title(\"perplexity : {} and max_iter : {}\".format(perplexity, n_iter))\n    img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n    plt.show()\n\nfor perplexity in perplexities:\n    X_pre_tsne = df2.drop(columns=['activityName'], axis=1)\n    y_pre_tsne = df2['activityName']\n    plot_tsne(X_data=X_pre_tsne, y_data=y_pre_tsne, perplexity=perplexity)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DNNDataset(object):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __getitem__(self, idx):\n        data = self.X[idx]\n        target = self.y[idx][0] \n        return data, target\n\n    def __len__(self):\n        return len(self.X)\n\ndef prepare_dnn_dataloader(X_train, X_valid, X_test, y_train, y_valid, y_test):\n    traindataset = DNNDataset(X_train, y_train)\n\n    trainloader = torch.utils.data.DataLoader(\n        traindataset, \n        batch_size=100, \n        shuffle=True, \n        num_workers=4,\n    )\n\n    validdataset = DNNDataset(X_valid, y_valid)\n\n    validloader = torch.utils.data.DataLoader(\n        validdataset, \n        batch_size=100, \n        shuffle=True, \n        num_workers=4,\n    )\n\n    testdataset = DNNDataset(X_test, y_test)\n\n    testloader = torch.utils.data.DataLoader(\n        testdataset, \n        batch_size=100, \n        shuffle=True, \n        num_workers=4,\n    )\n\n    return trainloader, validloader, testloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DNN(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(126, 64)\n        self.dp1 = nn.Dropout(0.1)\n\n        self.fc2 = nn.Linear(64, 16)\n        self.dp2 = nn.Dropout(0.1)\n        \n        self.fc3 = nn.Linear(16,n_labels)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.dp1(x)\n\n        x = self.fc2(x)\n        x = self.dp2(x)\n\n        x = self.fc3(x)\n        return x\n\ntrainloader, validloader, testloader = prepare_dnn_dataloader(X_train, X_valid, X_test, y_train, y_valid, y_test)\ninputs, labels = next(iter(trainloader))\nmodel = DNN()\noutputs = model(inputs.float())\nlabels[:1], outputs[:1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patience = 10\nn_epochs = 1000\n\nmodel = DNN()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\nearly_stopping = EarlyStopping(patience=patience)\ntrainloader, validloader, testloader = prepare_dnn_dataloader(X_train, X_valid, X_test, y_train, y_valid, y_test)\n\ntrainer = DLTrainer(model, trainloader, validloader, testloader, criterion, optimizer, early_stopping, n_epochs, patience)\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_valid_accuracy, dnn_valid_prediction = trainer.evaluate(validloader)\ndnn_test_accuracy, dnn_test_prediction = trainer.evaluate(testloader)\nprint(dnn_valid_accuracy, dnn_test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(*dnn_test_prediction)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(15,8))\nplot_confusion_matrix(cnf_matrix, classes=activities, title='DNN Testing Confusion Matrix Without Normalization')\n\n# Plot normalized confusion matrix\nplt.figure(figsize=(15,8))\nplot_confusion_matrix(cnf_matrix, classes=activities, normalize=True, title='DNN Testing Normalized Confusion Matrix')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_train_losses, avg_valid_losses, train_accuracies, valid_accuracies = trainer.show_evaluation()\nplot_avg_loss_per_epoch(avg_train_losses, avg_valid_losses)\nplot_accuracy_per_epoch(train_accuracies, valid_accuracies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"dnn_model.pth\")\ndump(scaler, 'dnn_std_scaler.bin', compress=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs = []\n\nids = [3,]\nuser = \"subject1\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject2\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject3\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject4\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject5\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject6\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\nuser = \"subject7\"\nload_data(\"hair\", 0, ids, user)\nload_data(\"listen\", 1, ids, user)\nload_data(\"sidepump\", 2, ids, user)\nload_data(\"dab\", 3, ids, user)\nload_data(\"wipetable\", 4, ids, user)\nload_data(\"gun\", 5, ids, user)\nload_data(\"elbowkick\", 6, ids, user)\nload_data(\"pointhigh\", 7, ids, user)\nload_data(\"logout\", 8, ids, user)\n\n\ndf = pd.concat(dfs)\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = list()\ny = list()\ndf_len = len(df)\nfor idx in range(0, df_len, num_time_steps//2):\n    window_df = df[idx:idx+num_time_steps]\n    labels = window_df[\"activity\"].unique()\n    trials = window_df[\"trial\"].unique()\n    subjects = window_df[\"subject\"].unique()\n    if len(labels) != 1 or len(trials) != 1 or len(subjects) != 1 or len(window_df) < num_time_steps:\n        continue\n    assert len(labels) == 1 and len(window_df) == num_time_steps\n    features = window_df.drop(columns=[\"activity\", \"activityName\", \"subject\", \"trial\"]).values\n    features = convert_to_timeseries(features, num_time_steps=num_time_steps, num_features=9)\n    X.append(features)\n    y.append(labels)\n\nX = np.array(X)\ny = np.array(y)\nX = extract_raw_data_features(X)\nX = scale_data(X)\nX.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = DNNDataset(X, y)\n\ndataloader = torch.utils.data.DataLoader(\n    dataset, \n    batch_size=1, \n    shuffle=True, \n    num_workers=4,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn_test_accuracy, dnn_test_prediction = trainer.evaluate(dataloader)\ndnn_test_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(*dnn_test_prediction)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(15,8))\nplot_confusion_matrix(cnf_matrix, classes=activities, title='DNN Testing Confusion Matrix Without Normalization')\n\n# Plot normalized confusion matrix\nplt.figure(figsize=(15,8))\nplot_confusion_matrix(cnf_matrix, classes=activities, normalize=True, title='DNN Testing Normalized Confusion Matrix')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}