{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pylab as plot\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom matplotlib import style\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier,XGBRFRegressor,XGBRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score as cvs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nhouse_price = pd.read_csv('/kaggle/input/boston-house-prices/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\nhouse_price.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_price.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_price.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpreting Data Description"},{"metadata":{},"cell_type":"markdown","source":"1. There is no missing values\n2. Variable 'ZN' is 0 for 25th and 50th percentile that will result in skweed data\n3. Also for variable 'CHAS' it's 0 for 25th, 50th and 75th percentile that will also show us that data is highly skweed."},{"metadata":{},"cell_type":"markdown","source":"## Checking  outliers in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting boxplots to see if there are any outliers in our data (considering data betwen 25th and 75th percentile as non outlier)\nfig, ax = plt.subplots(ncols=7, nrows=2, figsize=(15, 5))\nax = ax.flatten()\nindex = 0\nfor i in house_price.columns:\n    sns.boxplot(y=i, data=house_price, ax=ax[index])\n    index +=1\nplt.tight_layout(pad=0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns CRIM, ZN, RM, DIS, PTRATIO, B, LSTAT and MEDV have outliers."},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(house_price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. we can infer from the histogram that the ‘MEDV’ variable seems to be normally distributed but contain several outliers.\n2. We can spot a linear relationship between ‘RM’ and House prices ‘MEDV’"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Establishing correlation in data\n\nfrom matplotlib import style\n#creating a correlation matrix\n\nstyle.use(\"classic\")\nsns.heatmap(house_price.corr(),annot=True,cmap='inferno')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To fit a regression model, the features of interest are the ones with a high correlation with the target variable ‘MEDV’"},{"metadata":{},"cell_type":"markdown","source":"##  Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=house_price.drop(\"MEDV\",axis=1)\ny=house_price[\"MEDV\"]\nx.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train_std, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Assessing feature importance with random forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot\nforest = RandomForestRegressor(n_estimators=500, random_state = 0)\nforest.fit(X_train,y_train)\nimportances = forest.feature_importances_\nfeat_labels = house_price.columns[1:]\nindices = np.argsort(importances)[::-1]\n\n# plot feature importance\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing variables 'ZN' and 'CHAS' form data\nhouse_price = house_price.drop(['CHAS', 'CHAS', 'TAX', 'INDUS', 'NOX'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"house_price.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train a model"},{"metadata":{},"cell_type":"markdown","source":"## LinearRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting our model to train and test\nlm = LinearRegression()\nmodel = lm.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = lm.predict(X_test)\nY_compare_linear= pd.DataFrame({\"Actual\": y_test, \"Predict\": pred_y})\nY_compare_linear.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,pred_y))\nprint(mean_absolute_error(y_test,pred_y))\nprint(mean_squared_error(y_test,pred_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb=XGBRegressor(n_estimators=1000)\nxgb.fit(X_train,y_train)\nkfold=KFold(n_splits=5)\nres=cross_val_score(model,X_train,y_train,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yp=xgb.predict(X_test)\nY_compare_XGBReg= pd.DataFrame({\"Actual\": y_test, \"Predict\": yp})\nY_compare_XGBReg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,yp))\nprint(mean_absolute_error(y_test,yp))\nprint(mean_squared_error(y_test,yp))\nprint(r2_score(y_test,yp))\nprint(mean_absolute_error(y_test,yp))\nprint(mean_squared_error(y_test,yp))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbour"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsRegressor(n_neighbors=13)\nknn.fit(X_train,y_train)\nY_pred = knn.predict(X_test)\nY_compare_knn = pd.DataFrame({'Actual': y_test, 'Predicted': Y_pred})\nY_compare_knn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,Y_pred))\nprint(mean_absolute_error(y_test,Y_pred))\nprint(mean_squared_error(y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forrest Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train,y_train)\nY_pred = rf.predict(X_test)\nY_compare_randomforrest = pd.DataFrame({'Actual': y_test, 'Predicted': Y_pred})\nY_compare_randomforrest.head() #displaying the comparision btween actual and predicted values of MEDV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,Y_pred))\nprint(mean_absolute_error(y_test,Y_pred))\nprint(mean_squared_error(y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation "},{"metadata":{},"cell_type":"markdown","source":"Plotting compariasion of actual and predicted values of MEDV that we got using different machine learning models"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(25, 4))\nax = ax.flatten()\nY_compare_linear.head(10).plot(kind='bar', title='Linear Regression', grid=True, ax=ax[0])\nY_compare_XGBReg.head(10).plot(kind='bar', title='XGBRegressor', grid=True, ax=ax[1])\nY_compare_randomforrest.head(10).plot(kind='bar', title='Random Forrest Regression', grid=True, ax=ax[2])\nY_compare_knn.head(10).plot(kind='bar', title='KNN Regression', grid=True, ax=ax[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scores (R squared) of different machine learning models using K-fold cross validation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('According to R squared scorring method we got below scores for out machine learning models:')\nmodelNames = ['Linear Regression', 'XGBRegressor', 'Random Forrest', 'K-Nearest Neighbour']\nmodelRegressors = [lm, xgb, rf, knn]\nmodels = pd.DataFrame({'modelNames' : modelNames, 'modelRegressors' : modelRegressors})\ncounter=0\nscore=[]\nfor i in models['modelRegressors']:\n    accuracy = cvs(i, X_train, y_train, scoring='r2', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n    counter+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Model Name' : modelNames,'Score' : score}).sort_values(by='Score', ascending=True).plot(x=0, y=1, kind='bar', figsize=(15,5), title='Comparison of R2 scores of differnt models', )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can summarise that Random Forrest (r2 =  0.87) machine learning model gives the best score"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}