{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. BART: Denoising Autoencoder for Pretraining Sequence-to-Sequence Models [Multi-Class Classifier]:\n### BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pre- training schemes. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive di-alogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. In the below task we utilize this pre-trained model for Zero-shot Classification.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## **Libraries/Dependencies**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import all the required libraries\n# Use Kaggle's pre-tuned notebooks to get the optimal versions of all the dependencies\n\nimport nltk\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\n# nltk.download('stopwords')\nfrom string import punctuation\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import all the required libraries\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dataframe for final sentiment classification result\ndef createDataFrame(labels, confidence, tweet):\n    labels = pd.DataFrame({'Labels': labels})\n    confidence = pd.DataFrame({'Confidence Scores': confidence})\n    column_values = ['Labels', 'Confidence']\n    sentiment_scores = pd.concat([labels,confidence], ignore_index=False, axis=1)\n    print(\"\\n--------------------------------------------------------------------------------------\")\n    print(f\"\\n Entered input sentence: {tweet}\")\n    print(\"\\n Sentiment of the tweet (Probability Distribution): \")\n    print(sentiment_scores.to_string(index=False))\n    #print(\"--------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_bart(tweet):\n    labels = []\n    confidence = []\n    \n    # Possible Sentiment Categories\n    candidate_labels = [\"happy\", \"sad\", \"warn\", \"angry\", \"sorrow\", \"alert\", \"neutral\"]\n    #candidate_labels = [\"OYC\", \"DTC\", \"NCF\", \"KNY\", \"DCF\"]\n    \n    # Send the labels and tweets to the classifier pipeline\n    result = classifier(tweet, candidate_labels)\n    \n    # Extract the labels from results dictionary\n    labels.append(result[\"labels\"])\n    labels = [item for sublist in labels for item in sublist] # Flatten the list of lists into list\n    \n    # Extract the labels from results dictionary\n    confidence.append(result[\"scores\"])\n    confidence = [(str(float(item)*100))[:6]+\" %\" for sublist in confidence for item in sublist] # Flatten the list of lists into list\n\n    createDataFrame(labels,confidence, tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Driver program\nprint(\"Neural Sentiment Analysis of COVID-19 Tweets with BART\")\nprint(\"\\n------Available Options------\")\nprint(\"1. Inference on Sample Tweets\")\nprint(\"2. Enter Custom Tweets/Sentences\")\nprint(\"3. Exit\")\nprint(\"\\nPlease select an option from the above:\")\n\n\nsample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\nsentiment_bart(sample_1)\n    \nsample_2 = 'I am happy that my family members are safe in this tough times'\nsentiment_bart(sample_2)\n\n\"\"\"\nwhile(True):\n    choice = int(input())\n\n    if choice == 1:\n        sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n        sentiment_bart(sample_1)\n    \n        sample_2 = 'I am happy that my family members are safe in this tough times'\n        sentiment_bart(sample_2)\n    \n    elif choice == 2:\n        print(\"\\nPlease enter a sentence/tweet:\")\n        user_input = input()\n        sentiment_bart(user_input)\n    \n    elif choice == 3:\n        print(\"\\nExiting...\")\n        break\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Deep Long Short Term Memory Networks [Binary Classifier]:\n### Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data."},{"metadata":{},"cell_type":"markdown","source":"## **Load Dataset & Initialize GPU**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the transfer learning tweet dataset\nsentiment_df = pd.read_csv('../input/twitterdata/finalSentimentdata2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if NVIDIA Graphics Card and CUDA is available\ngpu_available = torch.cuda.is_available\n\nif gpu_available:\n    print('Parallely Processing using CUDA')\nelse:\n    print('No CUDA Detected')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Pre-processing & Inference Module Definitions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-process the text and perform Stemming, Lemmatization and Stop-word removal\ndef text_preprocessing(text):\n    remove_punctuation = [ch for ch in text if ch not in punctuation]\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_text\n\n\n# Pad blank topken to keep the length of tweets consistent - mandatory to normalize and train the model\ndef pad_features(reviews_int, seq_length):\n    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n    for i, row in enumerate(reviews_int):\n        if len(row)!=0:\n            features[i, -len(row):] = np.array(row)[:seq_length]\n    return features\n\n# Convert the sentences into stream of tokens\ndef tokenize(tweet):\n    test_ints = []\n    test_ints.append([vocab_to_int[word] for word in tweet])\n    return test_ints\n\n# Predict the sentiment of the tweet - performs binary classification using the model inference\ndef sentiment(net, test_tweet, seq_length=50):\n    print(\"\\n--------------------------------------------------------------------------------------\")\n    print(f\"\\n Original input sentence: {test_tweet}\")\n    test_tweet = text_preprocessing(test_tweet)\n    tokenized_tweet = tokenize(test_tweet)\n    \n    print(f\"\\n Pre-processed input sentence: {test_tweet}\")\n    #print(f\"\\nSentence converted into tokens:\\n{tokenized_tweet}\")\n    \n    padded_tweet = pad_features(tokenized_tweet, 50)\n    feature_tensor = torch.from_numpy(padded_tweet)\n    batch_size = feature_tensor.size(0)\n    \n    if gpu_available:\n        feature_tensor = feature_tensor.cuda()\n    \n    h = net.init_hidden(batch_size)\n    output, h = net(feature_tensor, h)\n    \n    predicted_sentiment = torch.round(output.squeeze())\n    \n    if predicted_sentiment == 1:\n        print(\"\\n Sentiment: Positive\")\n        \n    else:\n        print(\"\\n Sentiment: Negative\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code block to invoke Pre-processing, Padding and Tokenization operations on the tweet\n\nsentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(text_preprocessing)\n\nreviews_split = []\nfor i, j in sentiment_df.iterrows():\n    reviews_split.append(j['text'])\n\nwords = []\nfor review in reviews_split:\n    for word in review:\n        words.append(word)\n\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n\nencoded_reviews = []\nfor review in reviews_split:\n    encoded_reviews.append([vocab_to_int[word] for word in review])\n\nlabels_to_int = []\nfor i, j in sentiment_df.iterrows():\n    if j['sentiment']=='joy':\n        labels_to_int.append(1)\n    else:\n        labels_to_int.append(0)\n\nreviews_len = Counter([len(x) for x in encoded_reviews])\nnon_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\nencoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\nencoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])\n\nseq_length = 50\npadded_features= pad_features(encoded_reviews, seq_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Dataset and Dataloaders for Train, Test and Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the dataset into Train (80%), Validation (10%) & Test (10%)\nbatch_size = 1\nsplit_frac = 0.8\nsplit_idx = int(len(padded_features)*split_frac)\n\ntraining_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\ntraining_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\n# Transform the data into a Tensor datastructure\ntrain_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n\n# Prepare the dataloader for Train, Test and Validation\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **LSTM Model Architecture**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embedding Dimension of Tokens\nembedding_dim = 400\n\n# Embedding Dimension of Hidden Layers\nhidden_dim = 256\n\n# Output of the model is binary (either Positive or Negative)\noutput_size = 1\n\n# Number of hidden LSTM cells\nn_layers = 2\nvocab_size = len(vocab_to_int)+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Structure of the Neural Network\nclass LSTM(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n        super(LSTM, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        weights = next(self.parameters()).data\n        if gpu_available:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n        return hidden\n\nnet = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters required for training of the network\n\n# Learning Rate\nlr = 0.001\n\n# Loss Function - Binary Cross Entropy\ncriterion = nn.BCELoss()\n\n# Gradient Descent based Optimizer - ADAM (Adaptive LR)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n# Number of epochs to train the model\nepochs = 8\ncount = 0\n\n# Step size\nprint_every = 200\nclip = 5 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Model Training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the Neural Network\n# Off-load the model to CUDA\nif gpu_available:\n    net.cuda()\n\nnet.train()\nfor e in range(epochs):\n    h = net.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        count += 1\n        \n        if gpu_available:\n            inputs, labels = inputs.cuda(), labels.cuda()\n        h = tuple([each.data for each in h])\n        \n        net.zero_grad()\n        outputs, h = net(inputs, h)\n        loss = criterion(outputs.squeeze(), labels.float())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm(net.parameters(), clip)\n        optimizer.step()\n        \n        if count % print_every == 0:\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            \n            for inputs, labels in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n                \n                if gpu_available:\n                    inputs, labels = inputs.cuda(), labels.cuda()\n                    \n            outputs, val_h = net(inputs, val_h)\n            val_loss = criterion(outputs.squeeze(), labels.float())\n            val_losses.append(val_loss.item())\n        \n            net.train()\n            print(f\"Epoch: {e+1}/{epochs}.....\",f\"Step: {count}.....\",\"Train Loss: {:.6f}......\".format(loss.item()),\"Validation Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Model Testing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the Neural Network\ntest_losses = []\nnum_correct = 0\n\nh = net.init_hidden(batch_size)\nnet.eval()\n\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    \n    if gpu_available:\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    outputs, h = net(inputs, h)\n    test_loss = criterion(outputs.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    pred = torch.round(outputs.squeeze())\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\ntest_acc = num_correct/len(test_loader.dataset)\n\nprint(\"Average Test Loss: {:.4f}\".format(np.mean(test_losses)))\nprint(\"Average Test Accuracy: {:.4f}\".format(test_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Main Program**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Driver program\nprint(\"Neural Sentiment Analysis of COVID-19 Tweets with Deep LSTM\")\nprint(\"\\n------Available Options------\")\nprint(\"1. Inference on Sample Tweets\")\nprint(\"2. Enter Custom Tweets/Sentences\")\nprint(\"3. Exit\")\nprint(\"\\nPlease select an option from the above:\")\n\nsample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\nsentiment(net, sample_1)\n    \nsample_2 = 'I am happy that my family members are safe in this tough times'\nsentiment(net, sample_2)\n\n\"\"\"\nwhile(True):\n    choice = int(input())\n\n    if choice == 1:\n        sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n        sentiment(net, sample_1)\n    \n        sample_2 = 'I am happy that my family members are safe in this tough times'\n        sentiment(net, sample_2)\n    \n    elif choice == 2:\n        print(\"\\nPlease enter a sentence/tweet:\")\n        user_input = input()\n        sentiment(net, user_input)\n    \n    elif choice == 3:\n        print(\"\\nExiting...\")\n        break\n        \n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}