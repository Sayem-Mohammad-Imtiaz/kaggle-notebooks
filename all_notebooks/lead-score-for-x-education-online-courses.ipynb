{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Lead Score for X-Education Online Courses\n\n#### Problem Statement\n\nThe dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted.\n\nBuild a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted"},{"metadata":{},"cell_type":"markdown","source":"### Importing and understanding Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Pandas and NumPy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib.pyplot import xticks\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing  datasets\nleads_orig_data = pd.read_csv('../input/leads-dataset/Leads.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's see the head of our dataset\npd.set_option(\"display.max_columns\", 500)\nleads_orig_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_orig_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_orig_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"### 1.a. Derived variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_data = leads_orig_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Yes to 1 and NO to 0\nleads_data['Do Not Email'] = leads_data['Do Not Email'].map({'Yes': 1, 'No': 0})\nleads_data['Do Not Call'] = leads_data['Do Not Call'].map({'Yes': 1, 'No': 0})\n\nleads_data['Search'] = leads_data['Search'].map({'Yes': 1, 'No': 0})\nleads_data['Magazine'] = leads_data['Magazine'].map({'Yes': 1, 'No': 0})\nleads_data['Newspaper Article'] = leads_data['Newspaper Article'].map({'Yes': 1, 'No': 0})\nleads_data['X Education Forums'] = leads_data['X Education Forums'].map({'Yes': 1, 'No': 0})\nleads_data['Newspaper'] = leads_data['Newspaper'].map({'Yes': 1, 'No': 0})\nleads_data['Digital Advertisement'] = leads_data['Digital Advertisement'].map({'Yes': 1, 'No': 0})\nleads_data['Through Recommendations'] = leads_data['Through Recommendations'].map({'Yes': 1, 'No': 0})\nleads_data['Receive More Updates About Our Courses'] = leads_data['Receive More Updates About Our Courses'].map({'Yes': 1, 'No': 0})\n\nleads_data['Update me on Supply Chain Content'] = leads_data['Update me on Supply Chain Content'].map({'Yes': 1, 'No': 0})\nleads_data['Get updates on DM Content'] = leads_data['Get updates on DM Content'].map({'Yes': 1, 'No': 0})\nleads_data['I agree to pay the amount through cheque'] = leads_data['I agree to pay the amount through cheque'].map({'Yes': 1, 'No': 0})\nleads_data['A free copy of Mastering The Interview'] = leads_data['A free copy of Mastering The Interview'].map({'Yes': 1, 'No': 0})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deriving Asymmetrique Activity Index to numerical\n\nleads_data['Asymmetrique Activity Index']=leads_data['Asymmetrique Activity Index'].str.split('.',n = 1, expand = True)[0].astype(float)\nleads_data['Asymmetrique Profile Index']=leads_data['Asymmetrique Profile Index'].str.split('.',n = 1, expand = True)[0].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing the case of all column values to lower case\nfor col in leads_data.columns:\n    leads_data[col] = leads_data[col].apply(lambda s: s.lower() if type(s)==str else s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# divide all features to numerical and categorical for creating dummies\nleads_data_col=list(leads_data.columns)\n\n#'Lead Number'and 'Prospect ID' can be dropped from the list of features\nleads_data_col.remove('Lead Number')\nleads_data_col.remove('Prospect ID')\n\n# divide all features to numerical and categorical\nleads_data_col_num= [x for x in leads_data_col if leads_data[x].dtype in ['float64','int64']]\nleads_data_col_cat= [x for x in leads_data_col if leads_data[x].dtype=='object']\n\n#'Lead Number'and 'Prospect ID' can be dropped from the list of features\nprint(leads_data_col_num)\nprint(leads_data_col_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"### 1.b. Checking for Missing Values and Inputing Them"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Checking the missing values in Numerical Columns\nleads_data_num_nulls=leads_data[leads_data_col_num].isnull().any()\nleads_data_num_nulls_cols = list(leads_data_num_nulls[leads_data_num_nulls.values==True].index)\nleads_data_num_nulls_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Asymmetrique columns have values 1, 2, 3 which is score assigned to each customer based on their activity and their profile. A blank can be replaced by 0 score.\nSimilarily Page Visits nulls can be imputed with 0."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"leads_data[leads_data_num_nulls_cols]=leads_data[leads_data_num_nulls_cols].fillna(0)\nleads_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the missing values in Categorical Columns\nleads_data_cat_nulls=leads_data[leads_data_col_cat].isnull().any()\nleads_data_cat_nulls_cols = list(leads_data_cat_nulls[leads_data_cat_nulls.values==True].index)\nleads_data_cat_nulls_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Lead Source' impute 'unknown'\nleads_data['Lead Source']=leads_data['Lead Source'].fillna('unknown')\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'Lead Source', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Last Activity' impute 'unknown'\nleads_data['Last Activity']=leads_data['Last Activity'].fillna('unknown')\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'Last Activity', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Country' impute mode 'india'\nleads_data['Country']=leads_data['Country'].fillna('india')\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'Country', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Specialization' impute 'unknown' as if leads has not selected means they don't have one\nleads_data['Specialization']=leads_data['Specialization'].fillna('unknown')\n\n# 'Specialization' also impute 'select' with unknown' as they are as good as null\nleads_data['Specialization']=leads_data['Specialization'].apply(lambda x : 'unknown' if x=='select' else x)\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'Specialization', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'How did you hear about X Education' impute 'other' as if leads has not selected means it is not in options\nleads_data['How did you hear about X Education']=leads_data['How did you hear about X Education'].fillna('other')\n\n# 'How did you hear about X Education' also impute 'select' with unknown' as they are as good as null\nleads_data['How did you hear about X Education']=leads_data['How did you hear about X Education'].apply(lambda x : 'other' if x=='select' else x)\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'How did you hear about X Education', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'What is your current occupation' impute 'other' as if leads has not selected means it is not in options\nleads_data['What is your current occupation']=leads_data['What is your current occupation'].fillna('other')\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'What is your current occupation', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'What matters most to you in choosing a course' impute 'other' as if leads has not selected means it is not in options\nleads_data['What matters most to you in choosing a course']=leads_data['What matters most to you in choosing a course'].fillna('other')\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'What matters most to you in choosing a course', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# 'Tags' impute 'unknown' as if nothing is tagged to leads\nleads_data['Tags']=leads_data['Tags'].fillna('unknown')\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'Tags', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lead Quality impute 'not sure' as if nothing is selected for leads \nleads_data['Lead Quality']=leads_data['Lead Quality'].fillna('not sure')\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'Lead Quality', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Lead Profile' impute 'other leads' as if nothing is selected for leads \nleads_data['Lead Profile']=leads_data['Lead Profile'].fillna('other leads')\n\n# 'Lead Profile' also impute 'select' with 'other leads' as they are as good as null\nleads_data['Lead Profile']=leads_data['Lead Profile'].apply(lambda x : 'other leads' if x=='select' else x)\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'Lead Profile', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'City' impute 'other cities' as leads does not select city available in the list\nleads_data['City']=leads_data['City'].fillna('other cities')\n\n# 'City' also impute 'select' with 'other cities' as they are as good as null\nleads_data['City']=leads_data['City'].apply(lambda x : 'other cities' if x=='select' else x)\n\nfig, axs = plt.subplots(figsize = (15,5))\nsns.countplot(x = 'City', hue = 'Converted', data = leads_data)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking missing values if any after removing the missing values\nleads_data.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we don't have any missing values"},{"metadata":{},"cell_type":"markdown","source":"### 1.c. Checking for ouliers in numerical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\nleads_data[leads_data_col_num].describe(percentiles=[.25,.5,.75,.90,.95,.99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'TotalVisits' , 'Total Time Spent on Website', 'Page Views Per Visit' needs to be visualized to see for outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# box plot above variable\nplt.figure(figsize=(20, 50))\nj=0\nfor i in ['TotalVisits' , 'Total Time Spent on Website', 'Page Views Per Visit']:\n    j=j+1\n    plt.subplot(5,3,j)\n    sns.boxplot(data=leads_data, y=i, x='Converted')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'TotalVisits' > 20 can be considered to be an outlier\n'Page Views Per Visit' >200 can also be considered to be an outlier\n'Total Time Spent on Website' does not have outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_data=leads_data[leads_data['TotalVisits'] < 20]\nleads_data=leads_data[leads_data['Page Views Per Visit'] < 200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Create Dummy Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dummy variable for 'Lead Origin' and dropping the first one.\ncont = pd.get_dummies(leads_data['Lead Origin'],prefix='Lead Origin',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Lead Source' and dropping the first one.\ncont = pd.get_dummies(leads_data['Lead Source'],prefix='Lead Source',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Last Activity' and dropping the first one.\ncont = pd.get_dummies(leads_data['Last Activity'],prefix='Last Activity',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Country' and dropping the first one.\ncont = pd.get_dummies(leads_data['Country'],prefix='Country',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Specialization' and dropping the first one.\ncont = pd.get_dummies(leads_data['Specialization'],prefix='Specialization',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'How did you hear about X Education and dropping the first one.\ncont = pd.get_dummies(leads_data['How did you hear about X Education'],prefix='How did you hear about X Education',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'What is your current occupation' and dropping the first one.\ncont = pd.get_dummies(leads_data['What is your current occupation'],prefix='What is your current occupation',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'What matters most to you in choosing a course' and dropping the first one.\ncont = pd.get_dummies(leads_data['What matters most to you in choosing a course'],prefix='What matters most to you in choosing a course',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Tags' and dropping the first one.\ncont = pd.get_dummies(leads_data['Tags'],prefix='Tags',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Lead Quality' and dropping the first one.\ncont = pd.get_dummies(leads_data['Lead Quality'],prefix='Lead Quality',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Lead Profile' and dropping the first one.\ncont = pd.get_dummies(leads_data['Lead Profile'],prefix='Lead Profile',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'City' and dropping the first one.\ncont = pd.get_dummies(leads_data['City'],prefix='City',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# Creating a dummy variable for 'Last Notable Activity' and dropping the first one.\ncont = pd.get_dummies(leads_data['Last Notable Activity'],prefix='Last Notable Activity',drop_first=True)\n#Adding the results to the master dataframe\nleads_data = pd.concat([leads_data,cont],axis=1)\n\n# We have created dummies for the below variables, so we can drop them\nleads_data = leads_data.drop(leads_data_col_cat, 1)\n\nleads_data .head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.Feature Standardisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nnumvars = ['TotalVisits' , 'Total Time Spent on Website', 'Page Views Per Visit','Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score']\n\nleads_data[numvars] = scaler.fit_transform(leads_data[numvars])\nleads_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the Lead Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = (sum(leads_data['Converted'])/len(leads_data['Converted'].index))*100\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Lead score is 38%."},{"metadata":{},"cell_type":"markdown","source":"## 4. Model Building"},{"metadata":{},"cell_type":"markdown","source":"### 4.a Splitting Data into Training and Test Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make Lead Number as index before split\nleads_data= leads_data.set_index('Lead Number')\n\n# test train split\nfrom sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = leads_data.drop(['Converted','Prospect ID'],axis=1)\n\n# Putting response variable to y\ny = leads_data['Converted']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if all values is same for dummy columns after split\nuniques= X_train.loc[:,X_train.nunique()==1]\n\n# remove those columns\ncol_drop = uniques.columns\nfor col in col_drop:\n    X_train = X_train.drop([col], axis = 1)\n    X_test = X_test.drop([col], axis = 1)\n    \nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.b.Running First Training Model"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.c. Feature Selection by RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As there are 170 features manual selection of features is not possible. Hence running RFE with 15 variables as output\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)   \nrfe = rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_col = X_train.columns[rfe.support_]\nrfe_result = pd.DataFrame(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))\nrfe_result = rfe_result.rename(columns={ 0 : 'Feature',1 : 'Selection', 2 : 'Ranking'})\nrfe_20 = rfe_result[rfe_result['Selection']==True]\nrfe_20.sort_values(by='Ranking')\nrfe_20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.d. Correlation matrix of RFE variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(X_train[rfe_col].corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping highly correlated features which are of no significance\n\n1. 'Asymmetrique Activity Score'\n2. 'What matters most to you in choosing a course_other'"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_col = rfe_col.drop(['What matters most to you in choosing a course_other', 'Asymmetrique Activity Score'])\nX_test2 = X_test[rfe_col]\nX_train2 = X_train[rfe_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.e. Re-Running the model dropping highly correlated variables"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"logm2 = sm.GLM(y_train,(sm.add_constant(X_train2)), family = sm.families.Binomial())\nlogm2.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop 'Tags_wrong number given' due to high p value\nrfe_col = rfe_col.drop(['Tags_wrong number given'])\nX_test3 = X_test[rfe_col]\nX_train3 = X_train[rfe_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.f Re-Running the Model dropping one more variable"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"logm3 = sm.GLM(y_train,(sm.add_constant(X_train3)), family = sm.families.Binomial())\nmodel =logm3.fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the p values are less than 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# see the VIF \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['Features'] = X_train3.columns\nvif['VIF'] = [variance_inflation_factor(X_train3.values, i) for i in range(X_train3.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The VIFs are low , so there isn't any multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get p values and columns selected in data frame and sort by p values to get top 3 variables\npvalues_df= pd.DataFrame(model.pvalues)\npvalues_df = pvalues_df.rename(columns={ 0 : 'pvalues'}) \npvalues_df = pvalues_df.sort_values(by='pvalues')\npvalues_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lower the p-value , higher the column has impact on the Conversion Probability.\n\n### 3 most significant columns 'Tags_will revert after reading the email', 'Last Activity_sms sent' , 'Total Time Spent on Website'."},{"metadata":{},"cell_type":"markdown","source":"### 4.h Running the final model on final selected variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's run the model using the selected variables\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogsk = LogisticRegression(C=1e9)\nlogsk.fit(X_train3, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5 Get predicted values by the model on the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logsk.predict_proba(X_train3)\n\n# Converting pred_probs_test to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_pred)\ny_pred_1 = y_pred_df.iloc[:,[1]]\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_train_df = pd.DataFrame(y_train)\ny_train_df['Lead Number']=y_train_df.index\ny_train_df.reset_index(drop=True, inplace=True)\ny_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_train_df,y_pred_1],axis=1)\n# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 1 : 'Lead_Score'})\n# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['Lead Number','Converted','Lead_Score'], axis=1)\n# Let's see the head of y_pred_final\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking cut-off =0.8"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if lead_score>0.8 else 0\ny_pred_final['predicted'] = y_pred_final.Lead_Score.map( lambda x: 1 if x > 0.8 else 0)\n# Let's see the head\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create columns with different probability cutoffs\nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_pred_final[i]= y_pred_final.Lead_Score.map(lambda x: 1 if x > i else 0)\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot accuracy sensitivity and specificity for various probabilities.\nax = cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nax.vlines(x=0.32, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0.32 is the optimum point to take it as a cutoff probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"# overwrite the previous prediction using new cut-off\ny_pred_final['predicted'] = y_pred_final.Lead_Score.map( lambda x: 1 if x > 0.32 else 0)\n# Let's see the head\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix( y_pred_final.Converted, y_pred_final.predicted )\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#false postive rate - predicting Conversion when customer does not have Converted\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_pred_final.Converted, y_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#draw_roc(y_pred_final.Converted, y_pred_final.predicted)\n\"{:2.2f}\".format(metrics.roc_auc_score(y_pred_final.Converted, y_pred_final.Lead_Score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\n\nprint(precision_score(y_pred_final.Converted , y_pred_final.predicted))\nprint(recall_score(y_pred_final.Converted, y_pred_final.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_pred_final.Converted, y_pred_final.Lead_Score)\n\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.vlines(x=0.37, ymax=1, ymin=0, colors=\"b\", linestyles=\"--\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thus Final cut0ff is chosen to be 0.37"},{"metadata":{},"cell_type":"markdown","source":"## 7. Making Predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logsk.predict_proba(X_test3)\n\n# Converting pred_probs_test to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_pred)\ny_pred_1 = y_pred_df.iloc[:,[1]]\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\ny_test_df['Lead Number']=y_test_df.index\ny_test_df.reset_index(drop=True, inplace=True)\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_test_final = pd.concat([y_test_df,y_pred_1],axis=1)\n# Renaming the column \ny_pred_test_final= y_pred_test_final.rename(columns={ 1 : 'Lead_Score'})\n# Rearranging the columns\ny_pred_test_final = y_pred_test_final.reindex(['Lead Number','Converted','Lead_Score'], axis=1)\n# Let's see the head of y_pred_final\ny_pred_test_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking cut-off =0.37"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if lead_score>0.8 else 0\ny_pred_test_final['predicted'] = y_pred_test_final.Lead_Score.map( lambda x: 1 if x > 0.37 else 0)\n# Let's see the head\ny_pred_test_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix( y_pred_test_final.Converted, y_pred_test_final.predicted )\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# overall accuracy.\nmetrics.accuracy_score(y_pred_test_final.Converted, y_pred_test_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"draw_roc(y_pred_test_final.Converted, y_pred_test_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#draw_roc(y_pred_final.Converted, y_pred_final.predicted)\n\"{:2.2f}\".format(metrics.roc_auc_score(y_pred_test_final.Converted, y_pred_test_final.Lead_Score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assign lead Score and predicted Conversion to the leads"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"a = y_pred_test_final[['Lead Number','Lead_Score', 'predicted']]\nb = y_pred_final[['Lead Number','Lead_Score', 'predicted']]\ny_predicted = pd.concat([a,b], axis=0)\ny_predicted['Lead_Score']= y_predicted['Lead_Score'].apply(lambda x : round(x*100,2))\ny_predicted.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leads_scored_data=leads_orig_data.merge(y_predicted, how='inner', on='Lead Number')\nleads_scored_data = leads_scored_data.sort_values(by='Lead_Score', ascending=False)\nleads_scored_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identify Hot Leads with Lead_conversion_rate more than 80%"},{"metadata":{"trusted":true},"cell_type":"code","source":"hot_leads = leads_scored_data[leads_scored_data['Lead_Score']>=18]\nhot_leads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = (sum(hot_leads['Converted'])/len(hot_leads['Converted'].index))*100\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns used for prediction\nrfe_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Order the hot leads based on 3 most significant columns ''Tags_will revert after reading the email', 'Last Activity_sms sent' , 'Total Time Spent on Website' to identify the leads to focused most"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  hot_leads against columns 'Lead Source', 'Last Activity', 'Tags', 'Lead_Quality', 'Last Notable Activity'\n\nhot_leads_ordered0 = hot_leads[(hot_leads['Tags'] =='will revert after reading the email')\n                              &(hot_leads['Last Activity']== 'sms sent')]\n\nhot_leads_ordered1 = hot_leads[(hot_leads['Last Notable Activity']!='modified') |\n                               (hot_leads['Lead Quality']!='worst') |\n                               ((hot_leads['Tags']!='invalid number') |\n                                (hot_leads['Tags']!='ringing') |\n                                (hot_leads['Tags']!='switched off'))]\n\nprint(hot_leads_ordered0.shape)\nprint(hot_leads_ordered1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * hot_leads_ordered0 can be contacted first in case of limited resources.\n> * hot_leads_ordered1 can be contacted first in case of good number of resources."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}