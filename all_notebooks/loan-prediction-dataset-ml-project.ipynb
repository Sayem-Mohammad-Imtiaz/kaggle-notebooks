{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n<table>\n  <tr><td>\n    <img src=\"https://pas-wordpress-media.s3.us-east-1.amazonaws.com/content/uploads/2015/12/loan-e1450497559334.jpg\"\n         width=\"400\" height=\"600\">\n      <tr><td align=\"center\">\n  </td></tr>\n  </td></tr>\n</table>\n\nIn finance, a loan is the lending of money by one or more individuals, organizations, or other entities to other individuals, organizations etc. The recipient (i.e., the borrower) incurs a debt and is usually liable to pay interest on that debt until it is repaid as well as to repay the principal amount borrowed. ([wikipedia](https://en.wikipedia.org/wiki/Loan))\n\n### **The major aim of this notebook is to predict which of the customers will have their loan approved.**\n\n![](https://i.pinimg.com/originals/41/b0/08/41b008395e8e7f888666688915750d1f.gif)\n\n# Data Id ðŸ“‹\n\nThis dataset is named [Loan Prediction Dataset](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset) data set. The dataset contains a set of **613** records under **13 attributes**:\n\n![](http://miro.medium.com/max/795/1*cAd_tqzgCWtCVMjEasWmpQ.png)\n\n## The main objective for this dataset:\nUsing machine learning techniques to predict loan payments.\n\n### target value: `Loan_Status`\n\n# Libraries ðŸ“•ðŸ“—ðŸ“˜"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os #paths to file\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport warnings# warning filter\n\n\n#ploting libraries\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#relevant ML libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n#ML models\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)\n\n#warning hadle\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# File path ðŸ“‚"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#list all files under the input directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#path for the training set\ntr_path = \"/kaggle/input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv\"\n#path for the testing set\nte_path = \"/kaggle/input/loan-prediction-problem-dataset/test_Y3wMUE5_7gLdaTN.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing and Data Analysis ðŸ’»\n\n## First look at the data:\n\nTraining set:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# read in csv file as a DataFrame\ntr_df = pd.read_csv(tr_path)\n# explore the first 5 rows\ntr_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing set:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# read in csv file as a DataFrame\nte_df = pd.read_csv(te_path)\n# explore the first 5 rows\nte_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Size of each data set:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f\"training set (row, col): {tr_df.shape}\\n\\ntesting set (row, col): {te_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now the focus is shifted for the preprocessing of the training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#column information\ntr_df.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#summary statistics\ntr_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the Id column is not needed, let's drop it for both test and train datasets\ntr_df.drop('Loan_ID',axis=1,inplace=True)\nte_df.drop('Loan_ID',axis=1,inplace=True)\n#checking the new shapes\nprint(f\"training set (row, col): {tr_df.shape}\\n\\ntesting set (row, col): {te_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values ðŸš«\nAs you can see we have some missing data, let's have a look how many we have for each column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing values in decsending order\ntr_df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each value will be replaced by the most frequent value (mode).\n\nE.G. `Credit_History` has 50 null values and has 2 unique values `1.0` (475 times) or `0.0` (89 times) therefore each null value will be replaced by the mode `1.0` so now it will show in our data 525 times. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#filling the missing data\nprint(\"Before filling missing values\\n\\n\",\"#\"*50,\"\\n\")\nnull_cols = ['Credit_History', 'Self_Employed', 'LoanAmount','Dependents', 'Loan_Amount_Term', 'Gender', 'Married']\n\n\nfor col in null_cols:\n    print(f\"{col}:\\n{tr_df[col].value_counts()}\\n\",\"-\"*50)\n    tr_df[col] = tr_df[col].fillna(\n    tr_df[col].dropna().mode().values[0] )   \n\n    \ntr_df.isnull().sum().sort_values(ascending=False)\nprint(\"After filling missing values\\n\\n\",\"#\"*50,\"\\n\")\nfor col in null_cols:\n    print(f\"\\n{col}:\\n{tr_df[col].value_counts()}\\n\",\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data visalization ðŸ“Š"},{"metadata":{},"cell_type":"markdown","source":"Firstly we need to split our data to categorical and numerical data,\n\n\nusing the `.select_dtypes('dtype').columns.to_list()` combination."},{"metadata":{},"cell_type":"markdown","source":"## Loan status distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of all the columns.columns\n#Cols = tr_df.tolist()\n#list of all the numeric columns\nnum = tr_df.select_dtypes('number').columns.to_list()\n#list of all the categoric columns\ncat = tr_df.select_dtypes('object').columns.to_list()\n\n#numeric df\nloan_num =  tr_df[num]\n#categoric df\nloan_cat = tr_df[cat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tr_df[cat[-1]].value_counts())\n#tr_df[cat[-1]].hist(grid = False)\n\n#print(i)\ntotal = float(len(tr_df[cat[-1]]))\nplt.figure(figsize=(8,10))\nsns.set(style=\"whitegrid\")\nax = sns.countplot(tr_df[cat[-1]])\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,height + 3,'{:1.2f}'.format(height/total),ha=\"center\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot our data\n\nNumeric:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in loan_num:\n    plt.hist(loan_num[i])\n    plt.title(i)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical (split by Loan status):"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cat[:-1]: \n    plt.figure(figsize=(15,10))\n    plt.subplot(2,3,1)\n    sns.countplot(x=i ,hue='Loan_Status', data=tr_df ,palette='plasma')\n    plt.xlabel(i, fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding data to numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting categorical values to numbers\n\nto_numeric = {'Male': 1, 'Female': 2,\n'Yes': 1, 'No': 2,\n'Graduate': 1, 'Not Graduate': 2,\n'Urban': 3, 'Semiurban': 2,'Rural': 1,\n'Y': 1, 'N': 0,\n'3+': 3}\n\n# adding the new numeric values from the to_numeric variable to both datasets\ntr_df = tr_df.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)\nte_df = te_df.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)\n\n# convertind the Dependents column\nDependents_ = pd.to_numeric(tr_df.Dependents)\nDependents__ = pd.to_numeric(te_df.Dependents)\n\n# dropping the previous Dependents column\ntr_df.drop(['Dependents'], axis = 1, inplace = True)\nte_df.drop(['Dependents'], axis = 1, inplace = True)\n\n# concatination of the new Dependents column with both datasets\ntr_df = pd.concat([tr_df, Dependents_], axis = 1)\nte_df = pd.concat([te_df, Dependents__], axis = 1)\n\n# checking the our manipulated dataset for validation\nprint(f\"training set (row, col): {tr_df.shape}\\n\\ntesting set (row, col): {te_df.shape}\\n\")\nprint(tr_df.info(), \"\\n\\n\", te_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation matrix "},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix\nsns.heatmap(tr_df.corr() ,cmap='cubehelix_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation table for a more detailed analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation table\ncorr = tr_df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that `Credit_History` has the highest correlation with `Loan_Status` (a positive correlation of `0.54`).\nTherefore our target value is highly dependant on this column."},{"metadata":{},"cell_type":"markdown","source":"# Machine learning models\n\nFirst of all we will divide our dataset into two variables `X` as the features we defined earlier and `y` as the `Loan_Status` the target value we want to predict.\n\n## Models we will use:\n\n* **Decision Tree** \n* **Random Forest**\n* **XGBoost**\n* **Logistic Regression**\n\n## The Process of Modeling the Data:\n\n1. Importing the model\n\n2. Fitting the model\n\n3. Predicting Loan Status\n\n4. Classification report by Loan Status\n\n5. Overall accuracy\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = tr_df['Loan_Status']\nX = tr_df.drop('Loan_Status', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree\n\n![](https://i.pinimg.com/originals/eb/08/05/eb0805eb6e34bf3eac5ab4666bbcc167.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"DT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\n\ny_predict = DT.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nDT_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(DT_SC*100,2)}% Accurate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https://miro.medium.com/max/900/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  </td></tr>\n  </td></tr>\n</table>\n\nYou can see each predition and true value side by side by the csv created in the output directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"Decision_Tree=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nDecision_Tree.to_csv(\"Dection Tree.csv\")     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest\n\n![](https://miro.medium.com/max/1280/1*9kACduxnce_JdTrftM_bsA.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"RF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n\ny_predict = RF.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nRF_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(RF_SC*100,2)}% Accurate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https://miro.medium.com/max/900/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  </td></tr>\n  </td></tr>\n</table>\n\nYou can see each predition and true value side by side by the csv created in the output directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"Random_Forest=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nRandom_Forest.to_csv(\"Random Forest.csv\")     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost\n\n![](https://f-origin.hypotheses.org/wp-content/blogs.dir/253/files/2015/06/boosting-algo-3.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBClassifier()\nXGB.fit(X_train, y_train)\n\ny_predict = XGB.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nXGB_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(XGB_SC*100,2)}% Accurate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https://miro.medium.com/max/900/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  </td></tr>\n  </td></tr>\n</table>\n\nYou can see each predition and true value side by side by the csv created in the output directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBoost=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nXGBoost.to_csv(\"XGBoost.csv\")     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression\nNow, I will explore the Logistic Regression model.\n\n<table>\n  <tr><td>\n    <img src=\"https://files.realpython.com/media/log-reg-2.e88a21607ba3.png\"\n          width=\"500\" height=\"400\">\n      <tr><td align=\"center\">\n  </td></tr>\n  </td></tr>\n</table>"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(X_train, y_train)\n\ny_predict = LR.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nLR_SC = accuracy_score(y_predict,y_test)\nprint('accuracy is',accuracy_score(y_predict,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Logistic_Regression=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nLogistic_Regression.to_csv(\"Logistic Regression.csv\")     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https://miro.medium.com/max/900/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  </td></tr>\n  </td></tr>\n</table>\n\nYou can see each predition and true value side by side by the csv created in the output directory."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n1. `Credit_History` is a very important variable  because of its high correlation with `Loan_Status` therefor showind high Dependancy for the latter.\n2. The Logistic Regression algorithm is the most accurate: **approximately 83%**."},{"metadata":{"trusted":true},"cell_type":"code","source":"score = [DT_SC,RF_SC,XGB_SC,LR_SC]\nModels = pd.DataFrame({\n    'n_neighbors': [\"Decision Tree\",\"Random Forest\",\"XGBoost\", \"Logistic Regression\"],\n    'Score': score})\nModels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If you found this notebook interesting please upvote!\n\n![](https://i.pinimg.com/originals/e2/11/cc/e211ccb9e3a579ba8a4a8e25d68b4897.gif)\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}