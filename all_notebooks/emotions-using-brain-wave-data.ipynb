{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"train_df=pd.read_csv('emotions.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"train_df.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Data contains variables: fft(Fast fourier Tranform), correlate, entropy, logm, eigen, covmat, min_q, max_q\n# moments, mean, stddev\n# All are numerical type variable (float)\n#output is Label : Postive/Negative/Neutral\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#class distribution from column label (Output is label)\nplt.figure(figsize=(12,5))\nsns.countplot(x=train_df.label, color='red')\nplt.title('Brain Wave Data', fontsize=14)\nplt.xlabel('Class Label', fontsize=14)\nplt.ylabel('Class count', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Null data\ntrain_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"label_df=train_df['label']\ntrain_df.drop('label', axis=1, inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using cross validation (10 fold in this case)\n#Pipeline based approach\n#No of dimensions are high. Hence we will start with random forest classifier which works well on high-dimension data\n#Since its probablity based classifier, no pre-processing stages like scaling or noise removal are required\n#not affected by scale factors","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n\nmodel_randomForest=Pipeline(steps=[('random_forest', RandomForestClassifier())])\nscores=cross_val_score(model_randomForest, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Random Forest = ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Accuracy is good and total time taken is short (4.34 secs)\n"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n\nmodel_logisticRegression=Pipeline(steps=[('scalar', StandardScaler()),\n                                         ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores=cross_val_score(model_logisticRegression, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Logistic Regression= ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy less than Random Forest Classifier and time taken is higher"},{"metadata":{},"cell_type":"markdown","source":"# PCA "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\nscaled_df=scaler.fit_transform(train_df)\npca=PCA(n_components=20)\npca_vectors=pca.fit_transform(scaled_df)\nfor index, var in enumerate(pca.explained_variance_ratio_):\n    print(\"Explained variance ratio by Principal Component \", (index+1) ,\" : \" , var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using mathematical mapping 2549 variables mapped to 20 variables\n#Of 2549 variables, 10 are of most importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(25,8))\nsns.scatterplot(x=pca_vectors[:,0], y=pca_vectors[:,1],\n               hue=label_df)\nplt.title('PC V/s Class', fontsize=14)\nplt.xlabel('PC 1', fontsize=14)\nplt.ylabel('PC 2', fontsize=14)\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# it can be seen that if we use Logistic regression the first classifier will seperate NEUTRAL class from other two\n# and the second classifier will seperate NEGATIVE and POSITIVE\n# Applying Logistic regression model on 2 main PCs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nmodel_lg_pca=Pipeline(steps=[('scaler', StandardScaler()),\n                            ('pca', PCA(n_components=2)),\n                            ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga',max_iter=200 ))])\nscores=cross_val_score(model_lg_pca, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Logistic Regression :', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy reduced but time improved sigificantly for Logistic Regression model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Taking 10 PCs and running the model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nmodel_lg_pca_10=Pipeline(steps=[('Scaler', StandardScaler()),\n                               ('pca', PCA(n_components=10)),\n                               ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\n\nscores=cross_val_score(model_lg_pca_10, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Logistic Regressionwith 10 PCs :', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Improved Accuracy of 86% compared to 2 PC cases with marginal increase in time taken"},{"metadata":{},"cell_type":"markdown","source":"# Artifical Neural Network Classifier (ANN)"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\nmodel_mlp=Pipeline(steps=[('scaler', StandardScaler()),\n                         ('mlp_classifier', MLPClassifier(hidden_layer_sizes=(1275, 637)))])\nscores=cross_val_score(model_mlp, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for ANN Classifier: ', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy is good (97%) but time taken increases"},{"metadata":{"trusted":false},"cell_type":"code","source":"# General convention is to start with 50% of the data size for the first hidden layer \n# and 50% of previous size in subsequent layer \n# Number of hidden layers can be taken as a hyper-parameter and can be used to tune for better accuracy\n# Hidden layers in this ccase is 2\n# Or number of hidden neurons =  average of the input and output layers summed together.\n# The upper bound on the number of hidden neurons that won't result in over-fitting is: ùëÅ‚Ñé=ùëÅùë†/(ùõº‚àó(ùëÅùëñ+ùëÅùëú))\n# ùëÅùëñ= number of input neurons.\n# ùëÅùëú= number of output neurons.\n# ùëÅùë†= number of samples in training data set.\n# Œ±= an arbitrary scaling factor usually 2-10.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Support Vector Machines Classifier"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nmodel_SVM=Pipeline(steps=[('Scaler', StandardScaler()),\n                         ('svm', LinearSVC())])\nscores=cross_val_score(model_SVM, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Linear SVM :', scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy little less than Random Forest Classifier but more time efficient than ANN"},{"metadata":{},"cell_type":"markdown","source":"# Extreme Gradient Boosting Classifier (XGBoost)"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport xgboost as xgb\n\nmodel_xgb=Pipeline(steps=\n                   [('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])\nscores=cross_val_score(model_xgb, train_df, label_df, cv=10, scoring='accuracy')\nprint('Accuracy for Extreme Gradient Boosting is :', scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy is maximum for XGBoost but time taken is quite high.\nhigh running time due to internal ensemble model structure"},{"metadata":{"trusted":false},"cell_type":"code","source":"# xgboost performs well in GPU Machines\n# os has been imported due to dead kernel problem\n# CONCLUSIONS\n# 1. For Accuracy XGBoost is most favourable\n# 2. Random FOrest is a perfect choice if \"time taken\" is also considered\n# 3. Simple classifiers like Logistic regression can give better accuracy with poper feauture engineering\n# 4. other classifiers don't need much feauture engineering effort","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}