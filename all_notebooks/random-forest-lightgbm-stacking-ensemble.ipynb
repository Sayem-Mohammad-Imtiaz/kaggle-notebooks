{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hello everyone.\nIn this notebook we will analyze a sales dataset. It includes these practices:\n\n* Brief analysis to understand the data\n* Null value filling with similar rows\n* Categoric feature handling\n* Quick feature engineering\n* Modelling with Random Forest, LightGBM Regressor and stacking + ensembling methods\n\nLet's begin.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 15\npd.options.display.max_rows = 70","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/bigmart-sales-data/Train.csv')\ntest = pd.read_csv('../input/bigmart-sales-data/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data types\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#null check\ntrain.info()\ntrain.isnull().sum()/len(train)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pd.unique(train[\"Item_Identifier\"])) #1559 different item\nlen(pd.unique(train[\"Outlet_Identifier\"])) #10 outlets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### OUTLET SALES ANALYSIS ###\ntrain.groupby([\"Outlet_Identifier\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Among the outlets, average sales vary. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby([\"Outlet_Identifier\"])[\"Item_Weight\"].agg(\"mean\").sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Item weight is a low-valued column. So its variation is numerically low. But we have null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby([\"Outlet_Establishment_Year\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values(ascending=False)\ntrain.groupby([\"Outlet_Establishment_Year\"])[\"Item_Outlet_Sales\"].agg(\"count\").sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average sell amount is almost random but older outlets seems to sell higher number of products.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby([\"Outlet_Size\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values(ascending=False)\ntrain.groupby([\"Outlet_Size\"])[\"Item_Outlet_Sales\"].agg(\"count\").sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1) Medium and high sized outlets sell with higher prices.\n2) Medium and small sized outlets sell more products.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now lets check each outlets most sold items.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby([\"Outlet_Identifier\",\"Item_Type\"])[\"Item_Outlet_Sales\"].agg(\"count\").sort_values().groupby(\"Outlet_Identifier\").tail(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like \"Snack foods\" + \"Fruits and Vegetables\" are the most selling products across outlets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby([\"Outlet_Identifier\",\"Item_Type\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values().groupby(\"Outlet_Identifier\").tail(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And seafood and breakfast are sold to higher prices.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### ITEM ANALYSIS ###\nplot_df = train.groupby([\"Item_Type\"])[\"Item_Weight\"].agg('mean').sort_values(ascending=False).reset_index()\nticks = np.arange(0,len(plot_df))\nlabels = plot_df[\"Item_Type\"]\n\nplt.figure(figsize=(8,8))\nplot_df.plot(kind='bar')\nplt.xticks(ticks,labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby([\"Item_Type\",\"Item_Fat_Content\"])[\"Item_Fat_Content\"].agg(\"count\").sort_values().groupby(\"Item_Type\").tail(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most products in the dataframe are low-fat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df = train.groupby([\"Item_Type\"])[\"Item_Visibility\"].agg('mean').sort_values(ascending=False).reset_index()\nticks = np.arange(0,len(plot_df))\nlabels = plot_df[\"Item_Type\"]\n\nplt.figure(figsize=(8,8))\nplot_df.plot(kind='bar')\nplt.xticks(ticks,labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Breakfast, seafood and dairy are more visible.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"These analysis can go much further but I will move to modelling part since this dataset is quite simple. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train,test],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(df[df[\"Outlet_Size\"].isnull()][\"Outlet_Identifier\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dont have size values of these outlets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(df[df[\"Item_Weight\"].isnull()][\"Item_Identifier\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we dont have item weight of several items","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are going to fill these missing values with similar rows. We will use \"Outlet_Type\" for estimating missing Outlet_Size, and \"Item_Type\",\"Item_Fat_Content\" features to estimate item_weight. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#filling outlet size with similar rows wrt outlet type\ndf[\"Outlet_Size\"] = df.groupby([\"Outlet_Type\"])[\"Outlet_Size\"].transform(lambda x: x.fillna(x.mode()[0]))\n\n\n#filling item weight with similar rows wrt Item_Type and Item_Fat_Content\ndf[\"Item_Weight\"] = df.groupby([\"Item_Type\",\"Item_Fat_Content\"])[\"Item_Weight\"].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to encode Outlet_Size variable since it contains ordinal information. High>Medium>Small ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding ordinal column outlet size\ndf['Outlet_Size']=df['Outlet_Size'].replace({'Small':1,\n                                             'Medium':2,\n                                             'High':3})\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And Item_Fat_Content has mispelled indices. Lets fix them too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(df['Item_Fat_Content'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correcting mispelled column\ndf['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'LF': 'lf',\n                                                         'Low Fat':'lf',\n                                                         'low fat':'lf',\n                                                         'reg':'reg',\n                                                         'Regular':'reg'\n                                                        })","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will drop identifiers since they are only ID's. We have the necessary info for the prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items = df[\"Item_Identifier\"]\ndf = df.drop(columns=\"Item_Identifier\")\ndf = df.drop(columns=\"Outlet_Identifier\")\ndf[\"Outlet_Year\"] = 2020- df[\"Outlet_Establishment_Year\"]\ndf = df.drop(columns=\"Outlet_Establishment_Year\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets use dummy variables for categorical variables:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Categorical value handling\ndf.columns[df.dtypes=='object']\ndf = pd.get_dummies(df,columns = df.columns[df.dtypes=='object'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And apply min-max scaling because we will create some new variables by doing linear aggregations. If we don't do scaling before the aggregations, larger valued features will absorb the smaller ones.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns[df.columns!=\"Item_Outlet_Sales\"]:\n    df[col] = (df[col]-df[col].min()) / (df[col].max()-df[col].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_MRP_X_Visi']=df['Item_MRP'] * df['Item_Visibility']\n#df['Item_MRP_+_Visi']=df['Item_MRP'] + df['Item_Visibility']\ndf['Item_MRP_X_Weight']=df['Item_MRP'] * df['Item_Weight']\n#df['Item_MRP_+_Weight']=df['Item_MRP'] + df['Item_Weight']\n#df['Fat_Con_+_Weight']=df['Item_Fat_Content']+df['Item_Weight']\n#df['Fat_Con_X_Weight']=df['Item_Fat_Content']*df['Item_Weight']\ndf['Total_Points']=df['Item_MRP']*df['Item_Visibility']*df['Item_Weight']\ndf['MrpPerUnit']=df['Item_MRP']/(df['Outlet_Size']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[0:len(train)]\ntest = df[len(train):]\ntest.drop(columns=\"Item_Outlet_Sales\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nX = train.drop(columns=\"Item_Outlet_Sales\")\ny = train[\"Item_Outlet_Sales\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will skip the paratemer optimisation to run this kernel faster but I keep them in comment lines. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### RANDOM FOREST ###\n\"\"\"\nrfr = RandomForestRegressor()\n\nrf_grid = {'n_estimators' : [100,200,500,800,1000,1200],\n           'max_depth' : [3,5,7,10,15,25,40,None],\n           'min_samples_split':[2,4,6,10],\n           'min_samples_leaf':[2,4,6,8]   \n           }\n\nsearch = RandomizedSearchCV(rfr,rf_grid,scoring='neg_mean_squared_error',cv=3, verbose=2, n_jobs=6, n_iter = 50)\nsearch.fit(X,y)\n\n\nprint(search.best_params_)\nprint(search.best_estimator_)\nprint(search.cv_results_)\nprint(search.best_score_)\n\"\"\"\n\nrfr_best = RandomForestRegressor(n_estimators=1200, max_depth=5,min_samples_split=2,min_samples_leaf=2)\nrfr_best.fit(X_train,y_train)\npred = rfr_best.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### LGBM REGRESSOR ###\n\"\"\"\nlgbmr = LGBMRegressor()\nlgb_grid = {\n    'n_estimators': [100, 200, 400, 500],\n    'colsample_bytree': [0.9, 1.0],\n    'max_depth': [5,10,15,20,25,35,None],\n    'num_leaves': [20, 30, 50, 100],\n    'reg_alpha': [1.0, 1.1, 1.2, 1.3],\n    'reg_lambda': [1.0, 1.1, 1.2, 1.3],\n    'min_split_gain': [0.2, 0.3, 0.4],\n    'subsample': [0.8, 0.9, 1.0],\n    'learning_rate': [0.05, 0.1]\n}\n\nsearch = RandomizedSearchCV(lgbmr,lgb_grid,scoring='neg_mean_squared_error',cv=3, verbose=2, n_jobs=6, n_iter = 100)\nsearch.fit(X,y)\n\nprint(search.best_params_)\nprint(search.best_estimator_)\nprint(search.cv_results_)\nprint(search.best_score_)\n\"\"\"\n\n#Default parameters giving almost exact result as the grid search gives. So I used default ones.\nlgb_best = LGBMRegressor()\nlgb_best.fit(X_train,y_train)\npred2 = lgb_best.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacking technique will have a smoothing effect on out first-phase predictions. It will look at RF and LGBMRegressor results, and try to smooth them thorugh the correct values. Don't forget that we use default parameters of estimators in StackingRegressor.\n\nYou can gather more info on stacking here: https://mlfromscratch.com/model-stacking-explained/#/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### STACKING ###\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nlgbm = LGBMRegressor()\nrf = RandomForestRegressor()\n\nstack = StackingCVRegressor(regressors=(rf, lgbm, xgb),\n                            meta_regressor=xgb, cv=3,\n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,\n                            random_state=42)\n\nstack.fit(X_train, y_train)\n\nX_test.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33']\nstack_pred = stack.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, stack_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, we will combine 3 predictions. In ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### ENSEMBLE ###\npred_df = pd.DataFrame({'pred':pred, 'pred2':pred2, 'stack': stack_pred, 'target': y_test})\n\nfinal_pred = pred*0.6 + pred2*0.2 + stack_pred*0.2\n\nprint(np.sqrt(mean_squared_error(y_test, final_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can achieve better scores with different coefficients in ensemble. This is an example although it increased our rmse just a liiiiitle bit :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### SUBMISSION ###\npred1_test = rfr_best.predict(test)\npred2_test = lgb_best.predict(test)\ntest.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33']\nstack_test = stack.predict(test)\nfinal_pred_test = pred1_test*0.6 + pred2_test*0.2 + stack_test*0.2\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}