{"cells":[{"metadata":{"_uuid":"b96ff499-cee2-4004-aa61-e08861fa9871","_cell_guid":"440c906b-d7f8-4a19-95b2-447b4745123e","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Libraries:"},{"metadata":{},"cell_type":"markdown","source":"## For Pre-Processing:"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go # interactive plotting library\nimport plotly.express as px # interactive plotting library\nfrom plotly.subplots import make_subplots\n!pip install RapidPlot  # Library that I created. Only Contains 1 classs with 4 functions till now ;)\nimport RapidPlot\nfrom IPython.display import display\n!pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\nimport ppscore as pps # importing ppscore\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For Handling Imbalance:"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install imblearn\nfrom scipy.stats import describe\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.metrics import classification_report_imbalanced\nfrom imblearn.pipeline import Pipeline as imb_Pipeline\nfrom imblearn.base import SamplerMixin","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For Model-Selection"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Models:\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n!pip install catboost\nfrom catboost import CatBoostClassifier\n\n# Metrics\n\nfrom sklearn.metrics import accuracy_score, roc_curve, f1_score, precision_score, recall_score, confusion_matrix\n\n\n# Model Selection:\n\nfrom sklearn.pipeline import Pipeline as sk_Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Library for plotting confusion matrix\nfrom mlxtend.plotting import plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading the dataset: "},{"metadata":{"_uuid":"94bfa6a8-d351-45df-83e5-d2382f7c8a21","_cell_guid":"2df1e14d-141c-4b77-84f8-9227cdc027d1","trusted":true},"cell_type":"code","source":"ci_df = pd.read_csv(\"../input/caravan-insurance-challenge/caravan-insurance-challenge.csv\")\nci_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec61946a-36fc-44dd-841f-62c8d6516be8","_cell_guid":"3a146dd3-a45f-4b97-86be-f2583af3057b","trusted":true},"cell_type":"markdown","source":"#### Extracting Train and Test indices:"},{"metadata":{"_uuid":"820a9011-c454-4526-9abc-74d97fe008c5","_cell_guid":"90648391-e5a3-4a98-ba7e-64e07a998dfa","trusted":true},"cell_type":"code","source":"train_index = ci_df.ORIGIN == 'train'\ntest_index = ci_df.ORIGIN == 'test'\nci_df.drop(columns=['ORIGIN'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing:"},{"metadata":{"_uuid":"f5f93fa4-6b8c-4b92-a0b9-e3eba2d9575d","_cell_guid":"42c3382a-63d3-4f6c-99fe-095d2d12a0e8","trusted":true},"cell_type":"markdown","source":"### Checking for null-values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"(ci_df.isnull() == True).sum().sum()    # Checking total null-values in the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No null values"},{"metadata":{},"cell_type":"markdown","source":"### Checking for Imbalance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ci_df.CARAVAN.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\npca = PCA(n_components=2)\nci_df_red = pca.fit_transform(scaler.fit_transform(ci_df.drop(columns=('CARAVAN'))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(x=ci_df_red[:, 0], y=ci_df_red[:, 1], color=ci_df.CARAVAN)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yupp, We were are correct. The data we have is highly skewed\n\n#### We will handle this Imbalance using before feeding to the model"},{"metadata":{},"cell_type":"markdown","source":"## Performing Feature-Selection:\n\nRemember to not use **PCA** for Feature-Selection.\nLook at this blog for more info: https://towardsdatascience.com/pca-is-not-feature-selection-3344fb764ae6"},{"metadata":{},"cell_type":"markdown","source":"### Checking Correlations:"},{"metadata":{},"cell_type":"markdown","source":"We can perform feature selection using two methods:\n    1. Pearson's Correlation\n    2. PPscore (Predictive Power Score)\n    \n***Limitations of Pearson's corr:***\n<br>\nIt is not usefull for non-linear data (eg: y = $x^2$), this is taken care of by PPscore\n<br><br>\n***Limitations of PPscore:***\n<br>\nCalculating PPscore for a large set of features will be take a lot longer than Pearson's corr.<br>\nAlso, PPscore doesn't tell you about the type of relation (eg: directly proportional, inversely proportional), Hence, for linear relations, using Pearson's corr is a lot more useful\n\nFor more info check:<br>\nhttps://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598\n\nGo through the entire blog, to understand when and when not to use PPscore"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for execution time uncomment the below line\n#%timeit ci_df_corr = ci_df.corr()\n\n# and comment this line\nci_df_corr = ci_df.corr()\n\nfig = px.imshow(ci_df_corr)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can clearly see two divides in the heatmap: upper-left & lower-right<br>\n#### Let's zoom in\n\nKeep the result of _timeit_ in mind, below we will also see for PPscore."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(20, 7), ncols=2)\n\nsns.heatmap(ci_df_corr.iloc[:43, :43], cmap='YlGnBu', ax=axs[0])\nsns.heatmap(ci_df_corr.iloc[43:-1, 43:-1], ax=axs[1])\n\naxs[0].set_title('Upper Left')\naxs[1].set_title('Lower Right')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in _\"Lower Right\"_ Heatmap, that feature with names starting with 'A' and 'P' are very much similar.<br> (eg: $r$ = 0.9647738 for PBYSTAND & ABYSTAND, so we can obtain PBYSTAND from ABYSTAND or vice-versa)<br> Hence, having both present in the dataset is useless.\n\nLet's remove the one's starting with 'A' (psst, my name start's with 'P' :P)"},{"metadata":{"trusted":true},"cell_type":"code","source":"needed_columns = ci_df_corr.columns[~pd.Series(ci_df_corr.columns).str.startswith('A')]\n\nneeded_corr = ci_df_corr.loc[needed_columns, needed_columns]\n\nfig = px.imshow(needed_corr)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_with_target = needed_corr.iloc[-1, :-1]\nprint(corr_with_target)\ncorr_with_target.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_feature_vector = corr_with_target[np.abs(corr_with_target) >=0.05]\nprint(pearson_feature_vector.shape)\npearson_feature_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Using Pearson's correlation, we got the above feature as important. Now, let's check using PPscore "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### PPscore:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for execution time uncomment the below line\n#%timeit ci_df_pps = pps.matrix(ci_df)\n\n# and comment this line\nci_df_pps = pps.matrix(ci_df)\n\nci_df_pps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Pearson's corr is approx 700x faster than pps, hence, for very fat datasets (large number of features) take care in using PPscore"},{"metadata":{"trusted":true},"cell_type":"code","source":"pps_val_matrix = pd.DataFrame(np.array(ci_df_pps.ppscore).reshape(len(ci_df.columns), len(ci_df.columns)),\n                              index=ci_df.columns,\n                              columns=ci_df.columns)\npps_val_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.imshow(pps_val_matrix)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Important: Unlike Correlation matrix, PPscore matrix are not symmetric, the main reason for Invention of PPscore is to handle asymmetric nature of real-world data"},{"metadata":{},"cell_type":"markdown","source":"Doing same for columns starting with name 'A' and 'P', like we in Pearson's Corr.\n\nFrom this we can already see that PPscore is not looking good for our target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"needed_pps = pps_val_matrix.loc[needed_columns, needed_columns]\n\nfig = px.imshow(needed_pps)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pps_with_target = pps_val_matrix.iloc[-1, :-1]\npps_with_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pps_with_target.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pps_feature_vector = pps_with_target[pps_with_target != 0]\nprint(pps_feature_vector.shape)\npps_feature_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now that we have selected the feature using both the methods - Pearson's corr and PPscore, we will feed these feature vectors along with a vector containing all the feature into GridSearchCV. This is because many of the times it can happen, that certain features will provide unnecessary noise to the model instead of contributing towards better prediction. Now, next step is Imbalance Handling."},{"metadata":{},"cell_type":"markdown","source":"# Handling Imbalance:"},{"metadata":{},"cell_type":"markdown","source":"Before handling imbalance, let's first create train and test sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting random state\nrnd_state = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = ci_df.drop(columns='CARAVAN')\ny = ci_df.CARAVAN\n\nX_train = X[train_index]\ny_train = y[train_index]\nX_test = X[test_index]\ny_test = y[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, data in zip(('X_train', 'y_train', 'X_test', 'y_test'), (X_train, y_train, X_test, y_test)):\n    print(name)\n    print(f'Shape: {data.shape}')\n    if len(data.shape) == 1:\n        print('Balance Stats:')\n        print(data.value_counts())\n        print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Instances:\n\nros = RandomOverSampler(random_state=rnd_state)\nrus = RandomUnderSampler(random_state=rnd_state)\nsmote_sampler = SMOTE(random_state=rnd_state, n_jobs=-1)\nadasyn_sampler = ADASYN(random_state=rnd_state, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handling Imbalance:\n\nX_under,y_under = rus.fit_sample(X_train,y_train)\nX_over,y_over = ros.fit_sample(X_train,y_train)\nX_smote,y_smote = smote_sampler.fit_sample(X_train,y_train)\nX_adasyn, y_adasyn = adasyn_sampler.fit_sample(X_train, y_train)\n\n# In the paper on smote it is sugested to first undersample, then use smote, let's do that\nX_under_smote, y_under_smote = smote_sampler.fit_sample(X_under, y_under)\nX_under_adasyn, y_under_adasyn = adasyn_sampler.fit_sample(X_under, y_under)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_index = ['Unbalanced','Undersample','Oversample','SMOTE', 'ADASYN', 'Undersample then SMOTE', 'Undersample then ADASYN']\n\nimbalance_handling_stats = pd.DataFrame([*map(lambda x:describe(x)._asdict(),[y_train,y_under,y_over,y_smote, y_adasyn, y_under_smote, y_under_adasyn])],index=data_index)\nimbalance_handling_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be creating a pipeline and feeding the different Oversampling Handlers to the Grid Search"},{"metadata":{},"cell_type":"markdown","source":"# Model Selection:"},{"metadata":{},"cell_type":"markdown","source":"#### We will also add a transformer which will manipulate the feature, according to Pearson's corr Feature vector, PPscore feature vector or complete feature vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature_vector=None):\n        self.feature_vector = feature_vector\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        return X[self.feature_vector]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd_state=3\n\nmodel_list = [LogisticRegression(random_state=rnd_state),\n              SGDClassifier(random_state=rnd_state),\n              SVC(random_state=rnd_state),\n              KNeighborsClassifier(),\n              GaussianNB(),\n              DecisionTreeClassifier(random_state=rnd_state),\n              RandomForestClassifier(random_state=rnd_state),\n              GradientBoostingClassifier(random_state=rnd_state),\n              \n              # The below three models are extremely powerful but require extensive hyperparameter tuning\n              # Hence, they might not perform well here\n              XGBClassifier(random_state=rnd_state), \n              LGBMClassifier(random_state=rnd_state),\n              CatBoostClassifier(random_state=rnd_state),\n             ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Main_Pipeline = imb_Pipeline([\n    ('feature_handler', FeatureTransformer(list(pearson_feature_vector.index))),\n    ('over', SMOTE()),\n    ('under', RandomUnderSampler()),\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=0.99)),\n    ('model', LogisticRegression()),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"params_grid = [\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [LogisticRegression()], \n        'model__C': np.random.uniform(0.5, 2, 10),\n        'model__max_iter': np.random.randint(1500, 2000, 2),\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [SGDClassifier()],\n        'model__alpha': np.random.uniform(0, 1, 10),\n        'model__max_iter': np.random.randint(1500, 2000, 2),\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [SVC()],\n        'model__C': np.random.uniform(0, 2, 10),\n        'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [KNeighborsClassifier()],\n        'model__n_neighbors': np.random.randint(2, 10, 7),\n        'model__weights': ['uniform', 'distance'],\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [GaussianNB()],\n    },\n \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [DecisionTreeClassifier()],\n        'criterion': ['gini', 'entropy'],\n        'model__max_iter': np.random.randint(1500, 2000, 2),\n        'model__max_depth': np.r_[np.random.randint(5, 15, 7), None],\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [RandomForestClassifier()],\n        'model__n_estimators': np.random.randint(90, 200, 10),\n        'model__criterion': ['gini', 'entropy'],\n        'model__max_depth': np.r_[np.random.randint(5, 15, 7), None],\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [GradientBoostingClassifier()],\n        'model__loss': ['deviance', 'exponential'],\n        'model__n_estimators': np.random.randint(90, 200, 10),\n        'model__learning_rate': np.random.uniform(0.00001, 1, 5),     # Remember, There is trade-off between learning_rate and n_estimators\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [XGBClassifier()],\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [LGBMClassifier()],\n    },\n    \n    {\n        'over': [ros, smote_sampler, adasyn_sampler],\n        'feature_handler__feature_vector': list(map(list, [pearson_feature_vector.index, pps_feature_vector.index])),\n        'pca__n_components': [1],\n        'model': [CatBoostClassifier()],\n    }\n]\"\"\"\n\n\n# param_grid for fast execution\nparams_grid = [{\n    \n    'over': [ros, smote_sampler, adasyn_sampler],\n    'feature_handler__feature_vector': list(map(list, [X.columns, pearson_feature_vector.index, pps_feature_vector.index])),\n    'model': model_list,  \n}]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"main_grid_f1 = GridSearchCV(Main_Pipeline, params_grid, scoring='f1', cv=2, verbose=2)\nmain_grid_f1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ques: What if n_components=1 in PCA?<br>\nAns: All the models except SVC(kernel='precomputed') will work perfectly fine. This is because 'precomputed' kernel requires a square matrix.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_grid_f1.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_grid_results = pd.DataFrame(main_grid_f1.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_grid_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = main_grid_f1.predict(X_test)\nprint(f1_score(y_test, y_pred))\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confu_matr = confusion_matrix(y_test, y_pred)\n\nplot_confusion_matrix(conf_mat=confu_matr);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"main_grid_acc_s = GridSearchCV(Main_Pipeline, params_grid, scoring='accuracy', cv=2, verbose=2)\nmain_grid_acc_s.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_grid_acc_s.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_grid_result = pd.DataFrame(main_grid_acc_s.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = main_grid_acc_s.predict(X_test)\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confu_matr = confusion_matrix(y_test, y_pred)\n\nplot_confusion_matrix(conf_mat=confu_matr);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The F1_score for Model selected using 'accuracy' as metric is abysmal. This is because the dataset is Severely Imbalanced, which causes poor sampling, even if use SMOTE and ADASYN."},{"metadata":{},"cell_type":"markdown","source":"### THANK YOU!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}