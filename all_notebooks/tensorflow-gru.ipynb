{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:38.120284Z","iopub.execute_input":"2021-07-07T03:22:38.120703Z","iopub.status.idle":"2021-07-07T03:22:38.129095Z","shell.execute_reply.started":"2021-07-07T03:22:38.120667Z","shell.execute_reply":"2021-07-07T03:22:38.128129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/english-to-hindi-parallel-dataset/newdata.csv')\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:38.130834Z","iopub.execute_input":"2021-07-07T03:22:38.131399Z","iopub.status.idle":"2021-07-07T03:22:38.767799Z","shell.execute_reply.started":"2021-07-07T03:22:38.131362Z","shell.execute_reply":"2021-07-07T03:22:38.766824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english = data.iloc[:,1].values\nhindi = data.iloc[:,2].values\n\nprint(english)\nprint(hindi)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:38.769551Z","iopub.execute_input":"2021-07-07T03:22:38.770066Z","iopub.status.idle":"2021-07-07T03:22:38.80222Z","shell.execute_reply.started":"2021-07-07T03:22:38.770027Z","shell.execute_reply":"2021-07-07T03:22:38.801358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport re\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:38.803795Z","iopub.execute_input":"2021-07-07T03:22:38.804152Z","iopub.status.idle":"2021-07-07T03:22:38.812767Z","shell.execute_reply.started":"2021-07-07T03:22:38.804118Z","shell.execute_reply":"2021-07-07T03:22:38.811973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_data(line):\n    line = str(line).lower().strip()\n    line = re.sub(r\"([?.!,多|])\", r\" \\1 \", line) # create the space between words and [?.!,多] these signs\n    line = re.sub(r'[\" \"]+', \" \", line) # remove the extra space between the words\n    #line = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", line) # allow only alphabets and [?.!,多] these symbols or remove the digits.\n    line = line.strip()\n    line = '<start> ' + line + ' <end>' \n    \n    return line","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:38.814077Z","iopub.execute_input":"2021-07-07T03:22:38.814451Z","iopub.status.idle":"2021-07-07T03:22:38.822061Z","shell.execute_reply.started":"2021-07-07T03:22:38.814415Z","shell.execute_reply":"2021-07-07T03:22:38.821236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hindi = [preprocessing_data(sent) for sent in hindi]\nenglish = [preprocessing_data(sent) for sent in english]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:38.823402Z","iopub.execute_input":"2021-07-07T03:22:38.823793Z","iopub.status.idle":"2021-07-07T03:22:45.26579Z","shell.execute_reply.started":"2021-07-07T03:22:38.823757Z","shell.execute_reply":"2021-07-07T03:22:45.264959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(hindi[0])\nenglish[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.266964Z","iopub.execute_input":"2021-07-07T03:22:45.267281Z","iopub.status.idle":"2021-07-07T03:22:45.273746Z","shell.execute_reply.started":"2021-07-07T03:22:45.267246Z","shell.execute_reply":"2021-07-07T03:22:45.272961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_to_vec(inputs):\n    tokenizer = Tokenizer(filters='') # this tokenizer will filters nothing.\n    tokenizer.fit_on_texts(inputs)\n\n    tensor = tokenizer.texts_to_sequences(inputs)\n    tensor = pad_sequences(tensor, padding='post')\n\n    return tensor, tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.276432Z","iopub.execute_input":"2021-07-07T03:22:45.27692Z","iopub.status.idle":"2021-07-07T03:22:45.283362Z","shell.execute_reply.started":"2021-07-07T03:22:45.276883Z","shell.execute_reply":"2021-07-07T03:22:45.282381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng_tensor, eng_token = word_to_vec(english[0:10])\nhindi_tensor, hindi_token = word_to_vec(hindi[0:10])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.285293Z","iopub.execute_input":"2021-07-07T03:22:45.285823Z","iopub.status.idle":"2021-07-07T03:22:45.301822Z","shell.execute_reply.started":"2021-07-07T03:22:45.285787Z","shell.execute_reply":"2021-07-07T03:22:45.30057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(eng_tensor[0])\nhindi_tensor[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.306367Z","iopub.execute_input":"2021-07-07T03:22:45.308723Z","iopub.status.idle":"2021-07-07T03:22:45.322768Z","shell.execute_reply.started":"2021-07-07T03:22:45.308687Z","shell.execute_reply":"2021-07-07T03:22:45.321621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_eng, max_hindi = len(eng_tensor[0,:]), len(hindi_tensor[0,:])\n\nprint(max_eng, max_hindi)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.325148Z","iopub.execute_input":"2021-07-07T03:22:45.325439Z","iopub.status.idle":"2021-07-07T03:22:45.34212Z","shell.execute_reply.started":"2021-07-07T03:22:45.325409Z","shell.execute_reply":"2021-07-07T03:22:45.340126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"buffer_size = len(eng_tensor)\nbatch_size = 64\nsteps_per_epoch = len(eng_tensor)//batch_size\nembedding_dim = 256\nunits = 1024\nvocab_train = len(eng_token.word_index)+1\nvocab_label = len(hindi_token.word_index)+1\n\n#dataset = tf.data.Dataset.from_tensor_slices(([eng_tensor, hindi_tensor], hindi_tensor[:,1:])).shuffle(buffer_size)\n#dataset = dataset.batch(batch_size, drop_remainder=True)\n\n# hindi_tensor[:, :1] remove <start> token.\nencoder_input, decoder_input, decoder_output = eng_tensor, hindi_tensor[:, :], hindi_tensor[:, 1:] \nprint(encoder_input.shape)\nprint(decoder_input.shape)\nprint(decoder_output.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.343977Z","iopub.execute_input":"2021-07-07T03:22:45.344753Z","iopub.status.idle":"2021-07-07T03:22:45.382763Z","shell.execute_reply.started":"2021-07-07T03:22:45.344715Z","shell.execute_reply":"2021-07-07T03:22:45.380507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder_input[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.387639Z","iopub.execute_input":"2021-07-07T03:22:45.389063Z","iopub.status.idle":"2021-07-07T03:22:45.397035Z","shell.execute_reply.started":"2021-07-07T03:22:45.389023Z","shell.execute_reply":"2021-07-07T03:22:45.396145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_input = np.zeros((10, max_hindi-1)).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.398382Z","iopub.execute_input":"2021-07-07T03:22:45.398885Z","iopub.status.idle":"2021-07-07T03:22:45.416088Z","shell.execute_reply.started":"2021-07-07T03:22:45.398836Z","shell.execute_reply":"2021-07-07T03:22:45.414444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(decoder_input[:,0])):\n    t = np.where(decoder_input[i, :] == 2)\n    dec_input[i, :] = np.delete(decoder_input[i, :], t, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.422125Z","iopub.execute_input":"2021-07-07T03:22:45.423109Z","iopub.status.idle":"2021-07-07T03:22:45.434367Z","shell.execute_reply.started":"2021-07-07T03:22:45.42307Z","shell.execute_reply":"2021-07-07T03:22:45.433394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(decoder_output[0])\nprint(dec_input[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.438438Z","iopub.execute_input":"2021-07-07T03:22:45.43872Z","iopub.status.idle":"2021-07-07T03:22:45.448326Z","shell.execute_reply.started":"2021-07-07T03:22:45.43869Z","shell.execute_reply":"2021-07-07T03:22:45.447233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(decoder_output.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.452083Z","iopub.execute_input":"2021-07-07T03:22:45.452667Z","iopub.status.idle":"2021-07-07T03:22:45.456976Z","shell.execute_reply.started":"2021-07-07T03:22:45.452619Z","shell.execute_reply":"2021-07-07T03:22:45.456148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoder \nencoder = tf.keras.Input(shape=(max_eng, ))\nenc_embd = tf.keras.layers.Embedding(vocab_train, embedding_dim)(encoder)\nencoder_gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, kernel_regularizer=tf.keras.regularizers.L2(0.001))\noutput_e, hidden_e = encoder_gru(enc_embd)\n\nprint(output_e.shape, hidden_e.shape)\n\n# decoder\ndecoder = tf.keras.Input(shape=(max_hindi-1, ))\ndec_embd = tf.keras.layers.Embedding(vocab_label, embedding_dim)(decoder)\ndecoder_gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, kernel_regularizer=tf.keras.regularizers.L2(0.001))\noutput_d, hidden_d = decoder_gru(dec_embd, initial_state = hidden_e)\nfinal_output = tf.keras.layers.Dense(vocab_label, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(0.001))\noutput_f = final_output(output_d)\nprint(output_f.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:45.458749Z","iopub.execute_input":"2021-07-07T03:22:45.460706Z","iopub.status.idle":"2021-07-07T03:22:47.946563Z","shell.execute_reply.started":"2021-07-07T03:22:45.460668Z","shell.execute_reply":"2021-07-07T03:22:47.945679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Model([encoder, decoder], output_f)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:47.947803Z","iopub.execute_input":"2021-07-07T03:22:47.948325Z","iopub.status.idle":"2021-07-07T03:22:47.956076Z","shell.execute_reply.started":"2021-07-07T03:22:47.948284Z","shell.execute_reply":"2021-07-07T03:22:47.955258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:47.957915Z","iopub.execute_input":"2021-07-07T03:22:47.958196Z","iopub.status.idle":"2021-07-07T03:22:47.970929Z","shell.execute_reply.started":"2021-07-07T03:22:47.958158Z","shell.execute_reply":"2021-07-07T03:22:47.969301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n\ndef loss_function(real, pred):\n    mask_local = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_fn = loss_object(real, pred)\n\n    mask = tf.cast(mask_local, dtype=loss_fn.dtype)\n    loss_fn *= mask\n    return tf.reduce_mean(loss_fn)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:47.972152Z","iopub.execute_input":"2021-07-07T03:22:47.972496Z","iopub.status.idle":"2021-07-07T03:22:47.977992Z","shell.execute_reply.started":"2021-07-07T03:22:47.972461Z","shell.execute_reply":"2021-07-07T03:22:47.976966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=loss_function, optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:22:47.979456Z","iopub.execute_input":"2021-07-07T03:22:47.979811Z","iopub.status.idle":"2021-07-07T03:22:47.992892Z","shell.execute_reply.started":"2021-07-07T03:22:47.979778Z","shell.execute_reply":"2021-07-07T03:22:47.992127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([encoder_input, dec_input], decoder_output, epochs=10, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:15.8503Z","iopub.execute_input":"2021-07-07T03:29:15.850618Z","iopub.status.idle":"2021-07-07T03:29:16.184611Z","shell.execute_reply.started":"2021-07-07T03:29:15.850588Z","shell.execute_reply":"2021-07-07T03:29:16.183891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = model.predict([encoder_input[0].reshape((-1, max_eng)), dec_input[0].reshape((-1, max_hindi-1))])\nprint(a.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:29:20.948331Z","iopub.execute_input":"2021-07-07T03:29:20.948647Z","iopub.status.idle":"2021-07-07T03:29:20.996667Z","shell.execute_reply.started":"2021-07-07T03:29:20.948617Z","shell.execute_reply":"2021-07-07T03:29:20.995646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(english[0])\nprint(hindi[0])\n\nfor i in range(len(a[0][:])):\n    print(hindi_token.index_word[np.argmax(a[0][i])])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:33:27.398447Z","iopub.execute_input":"2021-07-07T03:33:27.398767Z","iopub.status.idle":"2021-07-07T03:33:27.427394Z","shell.execute_reply.started":"2021-07-07T03:33:27.398735Z","shell.execute_reply":"2021-07-07T03:33:27.425679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# customize for prediction\nencoder_model = tf.keras.Model(encoder, hidden_e) \n\nstate = tf.keras.Input(shape=(None, ))\noutput, hidden_dest = decoder_gru(dec_embd, initial_state = state)\n#output_result = final_output(output)\n\ndecoder_model = tf.keras.Model([decoder, state], [hidden_dest, output])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:26:40.671497Z","iopub.execute_input":"2021-07-07T03:26:40.67182Z","iopub.status.idle":"2021-07-07T03:26:40.833783Z","shell.execute_reply.started":"2021-07-07T03:26:40.671791Z","shell.execute_reply":"2021-07-07T03:26:40.833013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove start end token\ndef Expand(sentence):\n    return sentence.split(\"<start>\")[-1].split(\"<end>\")[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:26:43.209108Z","iopub.execute_input":"2021-07-07T03:26:43.209451Z","iopub.status.idle":"2021-07-07T03:26:43.213554Z","shell.execute_reply.started":"2021-07-07T03:26:43.209422Z","shell.execute_reply":"2021-07-07T03:26:43.212627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data(line):\n    line = str(line).lower().strip()\n    line = re.sub(r\"([?.!,多|])\", r\" \\1 \", line) # create the space between words and [?.!,多] these signs\n    line = re.sub(r'[\" \"]+', \" \", line) # remove the extra space between the words\n    #line = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", line) # allow only alphabets and [?.!,多] these symbols or remove the digits.\n    line = line.strip()\n    line = '<start> ' + line \n    \n    return line","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:30:23.007111Z","iopub.execute_input":"2021-07-07T03:30:23.007433Z","iopub.status.idle":"2021-07-07T03:30:23.012744Z","shell.execute_reply.started":"2021-07-07T03:30:23.007404Z","shell.execute_reply":"2021-07-07T03:30:23.011913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction function\ndef call(inp):\n    inp = data(inp) # process the data\n    \n    whole = []\n    for i in inp.split(' '):\n        whole.append(eng_token.word_index[i])\n    \n    inp = pad_sequences([whole], maxlen=max_eng, padding='post') # set data in training format\n    print(inp.shape)\n    state = encoder_model.predict(inp) # initialize the initial state for decoder\n    decoder_input = tf.expand_dims([hindi_token.word_index['<start>']], 0) # initial input of decoder\n    \n    ans = ''\n    \n    for i in range(1, max_eng):\n        state, output = decoder_model([decoder_input, state])\n        pred = final_output(output)\n    \n        ans += hindi_token.index_word[np.argmax(pred[0][0])] + ' '\n\n        if hindi_token.index_word[np.argmax(pred[0][0])] == '<end>':\n            return Expand(ans)\n        \n        decoder_input = tf.expand_dims([np.argmax(pred[0][0])], 0) # input for next word prediction \n         \n    return Expand(ans)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:30:25.959765Z","iopub.execute_input":"2021-07-07T03:30:25.960123Z","iopub.status.idle":"2021-07-07T03:30:25.968061Z","shell.execute_reply.started":"2021-07-07T03:30:25.960092Z","shell.execute_reply":"2021-07-07T03:30:25.967067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(english[5])\ncall(english[5])","metadata":{"execution":{"iopub.status.busy":"2021-07-07T03:31:11.328982Z","iopub.execute_input":"2021-07-07T03:31:11.329419Z","iopub.status.idle":"2021-07-07T03:31:11.475581Z","shell.execute_reply.started":"2021-07-07T03:31:11.329378Z","shell.execute_reply":"2021-07-07T03:31:11.473376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}