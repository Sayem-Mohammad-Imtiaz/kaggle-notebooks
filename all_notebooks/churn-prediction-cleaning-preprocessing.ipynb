{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer Churn Prediction :\n\nIn this project we'll see how to perform a data preprocessing and data prediction in intermediate level.\nSo, in this dataset we have train on the dataset which has multiple numerical and categorical feaatures and predict over the data.\n\n\n![](https://miro.medium.com/max/750/1*8_Md5Ns2OKeW9F8XRRCMKg.jpeg)\n\n\n## UPVOTE if you like this notebook :)\nYou can see my other works in [sagnik1511](https://kaggle.com/sagnik1511/notebooks) or in [github](github.com/sagnik1511)"},{"metadata":{},"cell_type":"markdown","source":"## Libraries :"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Gathering & Primary Visualization:\n\nAt first we have to read the data .\nThen we'll overview on the data and prepare a process how to edit the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/churn-risk-rate-hackerearth-ml/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv('../input/churn-risk-rate-hackerearth-ml/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to check the number of output/target values has."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['churn_risk_score'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is visible that there are 6 target values and we have to classify them.\n\nIt is the final **objective**."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing :\n\nAfter visualizing the data we have planned some moves to process the data. \n\nIn this process we have seen that both the train and test data leakage.\n\nSo , basically we are going to take the following moves to prepare trainable and predictable data."},{"metadata":{},"cell_type":"markdown","source":"#### Filling leakages :\n\nAs the data have lekagaes we have to fill those.\nand in this process we have taken two techniques. --\n\n|Data Types|Will be filled with|\n|---|---|\n|Categorical data|a string named 'None'|\n|Numerical Data|Mean of the present values|\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in test_df.columns:\n    if train_df[col].isnull().sum()!=0 or test_df[col].isnull().sum()!=0:\n        if train_df[col].dtype=='int64':\n            value=int(np.mean(train_df[col]))\n            train_df[col].fillna(value,inplace=True)\n            test_df[col].fillna(value,inplace=True)\n        elif train_df[col].dtype=='float64':\n            value=np.mean(train_df[col])\n            train_df[col].fillna(value,inplace=True)\n            test_df[col].fillna(value,inplace=True)\n        else:\n            train_df[col].fillna('None',inplace=True)\n            test_df[col].fillna('None',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping Unnecessary Fearures:\n\nwe have seen that there are some name and id features, when we are going to predict over the data , we can definitely tell these features only helps to label the dta , but it won't be helpful when it is time for prediction so we are going to drop those features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cl_train_df=train_df.drop(labels=['customer_id','Name','security_no','referral_id','feedback'],axis=1)\ncl_test_df=test_df.drop(labels=['customer_id','Name','security_no','referral_id','feedback'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cl_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Change the time series datas :\n\nIn this part we are going to numerate and create seperate columns for each value."},{"metadata":{"trusted":true},"cell_type":"code","source":"type(cl_train_df['joining_date'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the type of the data isn't datettime series , we have to do it manually."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_dates(data):\n    df=data\n    day=[]\n    month=[]\n    year=[]\n    for i in range(len(data)):\n        year.append(int(data['joining_date'][i][:4]))\n        month.append(int(data['joining_date'][i][5:7]))\n        day.append(int(data['joining_date'][i][8:10]))\n    df['day']=day\n    df['month']=month\n    df['year']=year\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1=add_dates(cl_train_df)\ntest_1=add_dates(cl_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.drop('joining_date',1,inplace=True)\ntest_1.drop('joining_date',1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.head()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_time(data):\n    df=data\n    hour=[]\n    mint=[]\n    second=[]\n    for i in range(len(data)):\n        hour.append(int(data['last_visit_time'][i][:2]))\n        mint.append(int(data['last_visit_time'][i][3:5]))\n        second.append(int(data['last_visit_time'][i][6:8]))\n        \n    data['minute']=mint\n    data['hour']=hour\n    data['sec']=second\n    data.drop('last_visit_time',1,inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=add_time(train_1)\ntest=add_time(test_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Encoding :\n\nNow we have check if any categorical feature has more than 20 unique values , then we will omit that cause too much variety in dtaa will simply make the dataset more complex to predict correctly."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in test_1.columns:\n    if train_1[col].dtype=='object':\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in test.columns:\n    if train[col].dtype=='object':\n        if train[col].nunique() >20:\n            train.drop(col,1,inplace=True)\n            test.drop(col,1,inplace=True)\n        else:\n            k=0\n            for val in train[col].value_counts().index:\n                train[col].replace(val,k,inplace=True)\n                test[col].replace(val,k,inplace=True)\n                k+=1\n            \n            \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test splitting :"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=train.drop('churn_risk_score',1)\ny_train=train['churn_risk_score']\nX_test=test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape,y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### P.S. we can change the feedback column too.Which I have missed previously.Following kernels will be using NLP to encode those features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paragraph=[]\nfor line in train_df['feedback']:\n    paragraph.append(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paragraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordnet=WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"corpus=[]\n\nfor i in range(len(paragraph)):\n    review=re.sub('[^a-zA-Z]',' ',paragraph[i])\n    review=review.lower()\n    review=review.split()\n    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n    review=' '.join(review)\n    corpus.append(review)\n    \ncorpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx=pd.DataFrame(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx.columns=['name']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx['name'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feedback=xx['name'].unique()\nfeedback","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(9):\n    xx.replace(feedback[i],i,inplace=True)\nxx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame({'1':xx['name'],'2':train_df['feedback']})\ndf1.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['feedback']=xx['name']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paragraph=[]\nfor line in test_df['feedback']:\n    paragraph.append(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"corpus=[]\n\nfor i in range(len(paragraph)):\n    review=re.sub('[^a-zA-Z]',' ',paragraph[i])\n    review=review.lower()\n    review=review.split()\n    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n    review=' '.join(review)\n    corpus.append(review)\n    \ncorpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx=pd.DataFrame({'name':corpus})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(9):\n    xx.replace(feedback[i],i,inplace=True)\nxx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.DataFrame({'1':xx['name'],'2':test_df['feedback']})\ndf1.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['feedback']=xx['name']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basically what we did here is lemmatize every sentences and then encoded them."},{"metadata":{},"cell_type":"markdown","source":"# Data Prediction :\n\nNow we are goinf to predict the dataset using two type of classifiers.\n\n\n1. RandomForestClassifier    ( from sklearn.ensemble )\n2. CatBoostClassifier        ( from catboost )"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1=pd.concat([X_train,train_df['churn_risk_score']],axis=1)\ntrain_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_submission(test,model,file_name):\n    y_pred=model.predict(X_test)\n    y_pred=y_pred.reshape(y_pred.shape[0])\n    y_pred=y_pred.tolist()\n    subs=pd.DataFrame({'customer_id':test_df['customer_id'],'churn_risk_score':y_pred})\n    subs.to_csv('file_name',index=False)\n    \n    \n    \ndef train_model(epochs,df,model):\n    for i in range(epochs):\n        print('Epoch ',i+1,' initiated................................')\n        func=model\n        df=train_1.sample(frac=0.8)\n        x_train=df.drop('churn_risk_score',1)\n        y_train=df['churn_risk_score']\n        func.fit(x_train,y_train)\n        print('Accuracy over this random data : ',func.score(x_train,y_train) )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier as cbtc\nfrom sklearn.ensemble import RandomForestClassifier as rfc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have used catboost over the train and test data.Lets see what accuracy we cam achieve."},{"metadata":{},"cell_type":"markdown","source":"After checking for hours we have taken selected features in our dataset , so that it may predict the best solution."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=X_train.drop(labels=['joined_through_referral','year','month' ,  'minute', 'sec',],axis=1)\ndf2=X_test.drop(labels=['joined_through_referral','year', 'month', 'minute', 'sec',],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=cbtc(verbose=0)\nmodel.fit(df1,y_train)\nprint(model.score(df1,y_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel=cbtc()\nmodel.fit(df1,y_train)\nprint(model.score(df1,y_train))\ny_pred=model.predict(df2)\ny_pred=y_pred.reshape(y_pred.shape[0])\ny_pred=y_pred.tolist()\nsubs=pd.DataFrame({'customer_id':test_df['customer_id'][:10],'churn_risk_score':y_pred[:10]})\nsubs.to_csv('submission_catboost.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to do this on RandomForestClassifier .Lets see how much accuracy we can achieve."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=rfc(random_state=0,n_jobs=2,n_estimators=700,verbose=2)\nmodel.fit(df1,y_train)\nprint(model.score(df1,y_train))\ny_pred=model.predict(df2)\nsubs=pd.DataFrame({'customer_id':test_df['customer_id'][:10],'churn_risk_score':y_pred[:10]})\nsubs.to_csv('submission_rfc.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HURRAH !!!!!\n\nWe've completed the whole project.\n\nIf you like this do not forget to upvote .\n\nand if you have any query or feedback , do comment.\n\n# Thank You for visiting :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}