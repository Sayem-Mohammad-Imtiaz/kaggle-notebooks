{"cells":[{"metadata":{"_uuid":"09bb57b7921c032e947465ae4b3ce0582c10a419"},"cell_type":"markdown","source":"# Data manipulation, transformation and junction. \n\nA startup just started operating in São Paulo. The startup name is [Yellow](https://www.yellow.app/?gclid=Cj0KCQjwlqLdBRCKARIsAPxTGaXuc2q-r_SnOuWBHji4ZmSEQ6gqHbGbSFgyyfGwqAJy07vK2la1VbkaAstTEALw_wc). This company offers to his users the possibility to use bikes spread all over the city. All you need to do is donwload their app, put some credit and start using. \n\n![yellow bike](https://portalbr.akamaized.net/brasil/uploads/2018/08/03141321/yellow-capa.jpg 'One of the bikes you can use in São Paulo')\n\nThis little yellow ones are really increasing and the people are using a lot ! So, with this in mind i started to question my self if there was any data similar to this. Thanks to Kaggle the answer is YES, there are a lot of data about bike sharing out there ! \n\nAfter a quick research a decided to test myself in the quest about some answers about this bussines model. They do got success in another places ? Who uses this ? Why and when ? \n\nTo answer this questions i started my exploration with data about bike sharing in Seattle - USA. This data is divided in three sets, as described below: \n\n* trip dataset: It have the information abou the trips. Data like, trip duration, user gender, user type, trip date and more;\n* station dataset: It have data about the localization of the bike stations spread in seattle;\n* weather: Data about climate\n\nWe can found data since 2014. \n\nTo make a good EDA on this data, i think we should join the most relevant data in only one dataset. Thats my chalenge !\n\nSo i hope you come with me in this journey, with my fellow Pandas and other librarys that well help us all over the way ! \n\nI hope you enjoy the trip and if you like what you see, please, leave you comment and your Upvote would be more than welcome ! \n\nLets do this ! "},{"metadata":{"trusted":true,"_uuid":"e3a6e8d4acdad75b0eca8fb556c6d78bb1ed2467"},"cell_type":"code","source":"# importing the librarys\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom dateutil.parser import parse # Helps to format strins into date\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b5d8418e8c37add26196af1cdec778cac885f89"},"cell_type":"code","source":"# importing each dataset\n\ntrip = pd.read_csv('../input/trip.csv', error_bad_lines=False)\n\n\nstation = pd.read_csv('../input/station.csv',error_bad_lines=False)\n\n\nweather = pd.read_csv('../input/weather.csv',error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ebaeb6105fcbce52672013dbbecd718eeea9d32"},"cell_type":"code","source":"trip.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4bc8a0e900db21bcfe5552f1e3ae631227bcfc7"},"cell_type":"code","source":"trip.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"942d52a8d1c4a77fd73fde220c534564c49e8e13"},"cell_type":"code","source":"station.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f711bb1b33d505de3d3fd1fdda185c58c58ac62"},"cell_type":"code","source":"station.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"778fc6ac04f6c5109e1930fc251998be50510f77"},"cell_type":"code","source":"weather.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78a7ae3f704c2bb960114817d868747761e78215"},"cell_type":"code","source":"weather.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be039e8ed320675081a72b71e18820a81e933ace"},"cell_type":"markdown","source":"****Lets explore this datasets. I will start with the trips dataset, but before i start the exploration, there are some things i should do first:****\n\n* Transform the time columns in time series (I chalenging myself, thats the first time i do this)\n* Join the lat e long in the trip dataset, this way i can do some geoanalysis"},{"metadata":{"_uuid":"83557935d99bcddd2a1880adf318de5f787bca22"},"cell_type":"markdown","source":"# Data Manipulation on Trips Dataset"},{"metadata":{"trusted":true,"_uuid":"ff2caed4748bb4be584ac096f4d48a0fa87b9d98"},"cell_type":"code","source":"trip.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb27809b1f513a0e9296b5b628a2de5a835c8fe8"},"cell_type":"code","source":"# lets drop the id columns that that are unnecessary\n\ntrip.drop(['trip_id','bikeid'], axis = 1,   inplace = True) # This command drop off the columns we pass as argument, the axis=1 condition,\n#makes the drop on columns, the inplace = True, save the alteration on the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"842410e255728c164f15f34f1eec04e33f65de76"},"cell_type":"code","source":"trip.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd66eaf5894ffa67432e334cc9ca079086f20e88"},"cell_type":"code","source":"# Tranforming the starttime column\n\ndata = trip['starttime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbd2abde74c1fa70b344ddb537cbf41b29425feb"},"cell_type":"code","source":"dta = list(trip['starttime']) # transforms each element of the startime column into a string\ndta = pd.to_datetime(dta)  # In the format of string, each element of the list is transformed into date by the pandas\ntrip['starttime'] = dta # Saving column changes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"869659874b16c5536be87cf14d7954dbd50c32a1"},"cell_type":"code","source":"# Checking if everything went right\ntrip.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0906272f042e2ddaef7bfa74fb15d0f9a2de218"},"cell_type":"code","source":"trip.starttime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40801009079f53c0907c25c5cb881692f66da7b8"},"cell_type":"code","source":"trip.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fd658d36ad6eeeb84d8ec7ca60384333b6a1650"},"cell_type":"code","source":"# in fact we just need the starttime column, since we already have the variable duration of the trip\n\ntrip.drop(columns='stoptime', axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"779a6e32e8d800da1ec30b705a471657cf5a095a"},"cell_type":"code","source":"trip.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a9d20e36c25fa3ff07d1ae51a6d764a6fabd996"},"cell_type":"markdown","source":"### Now, it would be a good idea to find out the age of the users once we have their date of birth."},{"metadata":{"trusted":true,"_uuid":"7a7812fa3e21b088eced787c7061de49ac019bef"},"cell_type":"code","source":"trip.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fb70e9276922a0144e6fad9d9450b84865ce799"},"cell_type":"code","source":"trip.birthyear.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1862e14747c5cc00d6f81834f08dab260df7c1f0"},"cell_type":"code","source":"# Filling in the missing values with values between 1969 and 1989 (which is the range in which most of the data is).\ntrip.birthyear.fillna(value = np.random.randint(1969,1989), inplace=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b308a1ec18ae7d6d6a65cb6bfe312b04785bbdd"},"cell_type":"code","source":"trip.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41b7a0ac4460cafd684ecc4ebf9b6d9e55a2be80"},"cell_type":"code","source":"trip.birthyear.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16f88c4271863decbe5f5a95e961f8b43caf64f7"},"cell_type":"code","source":"year = trip.starttime\n\ndef age(year):\n    '''This function extracts the year from each element of the starttime column' '''\n    age = []\n    for i in year.index:  # get each element in the index of the variable 'year'\n        a = str(year[i])  # 'i' represents each element of the index of the variable 'year', so each time 'for' identifies a\n        # number in the index it plays within the variable 'a' that selects an item from the variable 'year' \n        b = a.split('-')[0] # variable 'b', stores the result of the .split () method applied on variable 'a', in\n        # Then I extract the first element of the result from .split (), which is the year\n        c = pd.to_numeric(b)  # converts the string year, to number\n        \n\n        age.append(c.astype(int)) # stores 'c' in the 'age' list, created at the beginning of the function\n    return age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3c54935dcec7b5c0d5712e001169481fecad2db"},"cell_type":"code","source":"# usando a função e armazenando o resultado em uma variável\naged = age(year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"729b0a19b22b33a09a34046b4319d4cb518ea510"},"cell_type":"code","source":"trip['age'] = aged - trip.birthyear\ntrip['age'] = trip['age'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0747260bcdf86b1c5858d8592e23942435fc4230"},"cell_type":"code","source":"trip.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"139a2ac979303d514eb39f49584af002a36a0ee0"},"cell_type":"code","source":"trip.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8bab68a01d164fdf7b5961a71d2887760551c8b"},"cell_type":"code","source":"# Populating missing values from the gender column\ntrip.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e6a2f83a37ef44e0c238eeb1fae87f2df584067"},"cell_type":"code","source":"trip.gender.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"371026a22bfee43e51237bb9be0ccad6773c35f8"},"cell_type":"code","source":"# Using the fillna method with the 'ffill' parameter to populate the null values ​​with the next valid observation of the dataset\ngender = trip.gender.fillna(method='ffill')\ntrip.gender = gender\ntrip.gender.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"59a57db8810f19fde1f019581e66e9d6c413d2e6"},"cell_type":"code","source":"trip.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"bad88e50fd4f7e92218c81901b52966268bb37d9"},"cell_type":"code","source":"station.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9db2a56c28f21dd6bc00115b856f22a81ba8a6f3"},"cell_type":"markdown","source":"### Merging data!\n\nThe pandas merge shank, serves to gather data from different bases. Unlike concat mode, this method \"joins\" the data side by side. Think of it as the 'procv' function of Excel. To merge, it is necessary that both dataframes have at least one column with the same name."},{"metadata":{"trusted":true,"_uuid":"68278e219c3aace93afa3dffd7190d5be95d9009"},"cell_type":"code","source":"# creating the 'from_station_id' and 'to_station_id' columns in the dataset station\n\nstation['from_station_id'] = station.station_id\nstation['to_station_id'] = station.station_id\nstation.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"905faea5fc362cae1c36af4e99d0e087d16b2510"},"cell_type":"code","source":"# creating another dataset, only with the 'from_station_id' column and the location data\nfrom_station = station[['lat', 'long','from_station_id']]\nfrom_station.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f9d8f3d59ec2e7aaf8912a18bbaa290b3d4ef06"},"cell_type":"code","source":"# Including the latitude and longitude of the start stations in a new dataset: trip2\n\ntrip2 = pd.merge(trip,from_station, on='from_station_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5288655fbeed60773d4a64be9499d6de33686e7"},"cell_type":"code","source":"trip2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f8d809455eb3be8957b1dd432d4208557d4d185"},"cell_type":"code","source":"# identifying the new columns as the data of the place of departure\ntrip2.columns = ['starttime', 'tripduration', 'from_station_name',\n       'to_station_name', 'from_station_id', 'to_station_id', 'usertype',\n       'gender', 'birthyear', 'idade', 'from_lat', 'from_long']\ntrip2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f9f65975892fb366a9c030415e8c32359a23f88"},"cell_type":"code","source":"# creating another dataset, only with the column 'to_station_id'\nto_station = station[['lat', 'long','to_station_id']]\nto_station.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e34f7ba7ed389d68ba693cde4a7996ae2572a30a"},"cell_type":"code","source":"# Including the latitude and longitude of the start stations in a new dataset: trip3\n\ntrip3 = pd.merge(trip2,to_station, on='to_station_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a07b56477edfe677ee17cfe6fc4c4965013c6965"},"cell_type":"code","source":"trip3.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"075fc12e7935fead5420afa2a3b613c85a6a1da9"},"cell_type":"code","source":"# identifying the columns of the arrival data\ntrip3.columns = ['starttime', 'tripduration', 'from_station_name',\n       'to_station_name', 'from_station_id', 'to_station_id', 'usertype',\n       'gender', 'birthyear', 'idade', 'from_lat', 'from_long', 'to_lat', 'to_long']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4f2235bbf178df19797fc33d9aee0351f95893c"},"cell_type":"code","source":"trip3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e90dd2d46e37b860a891fef998e6e8efac7d7636"},"cell_type":"code","source":"trip3.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d297d85ae58dd67624933e73e5d12413adb77d9"},"cell_type":"markdown","source":"### For the dataset to be complete, we need to include the weather data.\n\nBefore that, I need to familiarize myself with the variables in the dataset 'weather'. But first let's take a look at where the bike stations are."},{"metadata":{"trusted":true,"_uuid":"f947820b39bf9a13f287c4cafeb6555c355bd26d"},"cell_type":"code","source":"# Folium is the library that allows plotting with maps, very simple to use\n\nimport folium","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"40bd22a751bede669f34493706037ca2b88de640"},"cell_type":"code","source":"station.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"bf23ac2bff2c13e7e96832a3857b5d06241bc65f"},"cell_type":"code","source":"mapa = folium.Map(location=[ 47.608013,  -122.335167], zoom_start=12) # Determining the seattle map using latitude and longitude data\nlat = station['lat'].values # taking the latitude values from the stations of the dataset station\nlong = station['long'].values # taking the values of longitude of the stations of the dataset station\n\nfor la, lo in zip(lat, long): # for each value in lat and long...\n    folium.Marker([la, lo]).add_to(mapa) # create a marker and place in the map variable (which in this case is the map of Seattle)\nmapa # Show the Map","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a42e4efd6162a41eed34f40da6439ec99efc1e8"},"cell_type":"markdown","source":"### And voi la ! \n\n### Theres is our map !"},{"metadata":{"trusted":true,"_uuid":"41c35bf1a92d9b99a80718eb36c925b35f3c83dd"},"cell_type":"code","source":"trip3.from_station_name.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04f90c1706ea733f796d6b9edaf3e731cf3c7467"},"cell_type":"markdown","source":"# Let's see the 10 most popular stations on the map\n"},{"metadata":{"trusted":true,"_uuid":"1aa636410b547b8663e9c1711f8205dbdb3a201b"},"cell_type":"code","source":"\nestacoes_mais_pop = pd.DataFrame(trip3.from_station_name.value_counts().head(10)) # Counting the 10 plus creating a new df to be able to pass\n# for the folium\nstation_2 = station[['name','lat', 'long' ]]\nstation_2.columns = ['from_station_name','lat', 'long']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac1decc4cebb1f6ad18f68ce8f894a63ad0e1e94"},"cell_type":"code","source":"estacoes_mais_pop = estacoes_mais_pop.reset_index() # resetting the index to adjust the name of the columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f41acc27bdd633b10173b48777006b2b2f08b38"},"cell_type":"code","source":"estacoes_mais_pop # note that the column with the station name is named 'index'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c7c1706522b00a7d3de0e9e7c60f2c45d044615"},"cell_type":"code","source":"estacoes_mais_pop.columns = ['from_station_name','contagem'] # Correcting the problem by simply renaming the columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae914786a06b9fc978b5ef65ad80fa94d5ed46a1"},"cell_type":"code","source":"estacoes_mais_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"820762081a4b2609253d0d7bf18328920b76d974"},"cell_type":"code","source":"estacoes_mais_pop = pd.merge(estacoes_mais_pop, station_2, on='from_station_name') # including location data (lat and long) using merge again","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddecf857a3883013a730cbc1d86e52a864a5a675"},"cell_type":"code","source":"estacoes_mais_pop","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ed3c70af9018722ab8fb436ca3dfda4663cef482"},"cell_type":"code","source":"mapa2 = folium.Map(location=[47.608013,  -122.335167], zoom_start=13) # Same process as above, but we need to create a new Map\n\nlat = estacoes_mais_pop['lat'] \nlong = estacoes_mais_pop['long'] \n\n# This time I wrote line by line because I wanted to include the name of the station on the map. I could not find a more practical way to do it,\n# for a while...\n\nfolium.Marker([47.614315, -122.354093],popup='Pier 69 / Alaskan Way & Clay St').add_to(mapa2)\nfolium.Marker([47.615330 ,-122.311752],popup='E Pine St & 16th Ave').add_to(mapa2)\nfolium.Marker([47.618418 ,-122.350964],popup='3rd Ave & Broad St ').add_to(mapa2)\nfolium.Marker([47.610185 ,-122.339641],popup='2nd Ave & Pine St').add_to(mapa2)\nfolium.Marker([47.613628 ,-122.337341],popup='Westlake Ave & 6th Ave').add_to(mapa2)\nfolium.Marker([47.622063 ,-122.321251],popup='E Harrison St & Broadway Ave E ').add_to(mapa2)\nfolium.Marker([47.615486 ,-122.318245],popup='Cal Anderson Park / 11th Ave & Pine St').add_to(mapa2)\nfolium.Marker([47.619859 ,-122.330304],popup='REI / Yale Ave N & John St ').add_to(mapa2)\nfolium.Marker([47.615829 ,-122.348564],popup='2nd Ave & Vine St').add_to(mapa2)\nfolium.Marker([47.620712 ,-122.312805],popup='15th Ave E & E Thomas St').add_to(mapa2)\n\nmapa2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9dac9dc0e9cecdaac919a20f1844613f595c45c"},"cell_type":"markdown","source":"### So far, so close \n\nSo lets see the location of the first three stations !"},{"metadata":{"_uuid":"d97c891cf9bc879479404c06bb7e807935ee1c90"},"cell_type":"markdown","source":"# 1st Pier 69\n\n![Pier 69 - Seattle](http://www.gonorthwest.com/Washington/seattle/Waterfront/images/DSC_2184.jpg)\n\n# 2nd E Pine St / 16th Ave\n\n![E Pine St / 16th Ave](https://t-ec.bstatic.com/images/hotel/max1024x768/539/53967962.jpg)\n\n# 3rd 3rd Ave & 16th Ave\n![3rd Ave & 16th Ave](https://cdn.downtownseattle.org/app/uploads/2017/10/Metro-on-3rd-high-angle-2-2.jpg)\n"},{"metadata":{"trusted":true,"_uuid":"66529011653a552c41bdbc4e9568e03a89ba5af1"},"cell_type":"code","source":"# Evaluating the weather dataset\nweather.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd54bd352640b10205ace83fe63f721ccf1ada52"},"cell_type":"code","source":"# Evaluating the dataset trip3, remembering that this dataset contains the location data\ntrip3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d1d4f9132c7b685a97ee55039c8d44df59f574"},"cell_type":"code","source":"data_str = list(trip3.starttime) # Creating a new date column, with the same date format as the weather dataset\n# this will allow you to add the weather data on the trip.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"798e4ae6d4b97e47d4d1e99d8e5d4737357219c2"},"cell_type":"code","source":"data_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44897ad199156163698b87edfae4f90cd10e2d71"},"cell_type":"code","source":"data_str = [datetime.strftime(x, '%Y-%m-%d') for x in data_str] # Formatting the column using datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87b381f9179b166ffd81607f5f7fdd766f58479f"},"cell_type":"code","source":"data_str[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ef2ebe0d3532d11f87dcba017806c06fcd1466b"},"cell_type":"code","source":"trip3['Date'] = data_str # Adding the column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47573b9664932bd1a35fc402eadd64bb2bf884a3"},"cell_type":"code","source":"type(weather.Date)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"49534f0e78e311394ccddde713ca5d42da244937"},"cell_type":"code","source":"trip3.head() # Confirming column","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"6a342c441fad206cf1345c75f694ec7ec767f89c"},"cell_type":"code","source":"trip3.Date.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ccc3dee8e450d915936e64ef3b44880e0f89196"},"cell_type":"code","source":"weather.Date.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55fe0a4103eaea5dfdbad2ebb7592c0179bc9507"},"cell_type":"code","source":"# using the same method used in the starttime column of the dataset trip this is necessary because the Date columns of trip3 and weather\n\n\ndt = list(weather['Date']) # transforms each element of the Date column into a string\ndt = pd.to_datetime(dt)  # In the string format, each list element is transformed into a date by the pandas\n\nweather['Date'] = dt # Saving the changes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4d3b8c227b52150805a315c382c1cc564c84e29"},"cell_type":"code","source":"weather.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1236939af9cb3e517c3c3db0a569197cd266c5c4"},"cell_type":"code","source":"trip3.Date = pd.to_datetime(trip3.Date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f82d507f21d940783038c5b45548a09b38095f3a"},"cell_type":"code","source":"trip4 = pd.merge(weather,trip3, on = 'Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdbe56b23033ba3a053fca1bcc9f6dde384b1e66"},"cell_type":"code","source":"trip4.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dbc2bddc102b27ad14f7f003f4ce3b908d8072f"},"cell_type":"code","source":"trip4.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1066d39f4a7bd95d1546d124175f59a5c976e9b0"},"cell_type":"markdown","source":"Now we have a huge dataset with 35 columns with data about use, weather and localization. But there are some null values. Lets work on it ! "},{"metadata":{"_uuid":"eb3c1a70a71d1f838df62e8e7fcb6f4ab2916bbc"},"cell_type":"markdown","source":"### Mean_Temperature_F"},{"metadata":{"trusted":true,"_uuid":"a6ed9501bd570a9eddfd76be81cfe63a58b8fa91"},"cell_type":"code","source":"# We have 110 null values in this colunm\ntrip4.Mean_Temperature_F.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fa44038de05dac4a33ac7b631cd7ec847e2c80d"},"cell_type":"code","source":"trip4.Mean_Temperature_F.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fe8a485184fa5d102bbc07b2bbc594787e3a67a"},"cell_type":"code","source":"# Let us fill in the missing data with the mean value, plus or minus the standard deviation\ntrip4.Mean_Temperature_F = trip4.Mean_Temperature_F.fillna(value = np.random.randint(48,68))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f96846b00600b35f1a06848b8272b3eed210c8cb"},"cell_type":"code","source":"trip4.Mean_Temperature_F.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ec8cba7c4c4ce5602dcb62c45a9af5e62ecf501"},"cell_type":"code","source":"trip4.Mean_Temperature_F.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6731f8c9e129fcbd9fde7f75b3484f7f29c1b3a8"},"cell_type":"code","source":"trip4.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ddf536c9e98ce63937e087c9684559efd8afa73"},"cell_type":"markdown","source":"### Max_Gust_Speed_MPH"},{"metadata":{"trusted":true,"_uuid":"0b6117589c97b94918f057c5b2f717fc83495544"},"cell_type":"code","source":"trip4.Max_Gust_Speed_MPH.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b45230cbc5a5d142de558efe54ca0614c5e604a1"},"cell_type":"markdown","source":"this column should be numeric, but has to many null values and i dont know what it means. sou, we gonna drop it."},{"metadata":{"trusted":true,"_uuid":"58e6876fef1eb0843470b95f5f01639ff56bc6a5"},"cell_type":"code","source":"trip4.drop(columns='Max_Gust_Speed_MPH', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f012db1ece42f305870f2b227cb519895b7f5718"},"cell_type":"code","source":"trip4.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51817bd5d08d8f0f1b7ad9d1a9f7a868c0ad3e14"},"cell_type":"markdown","source":"### Events\n\nI think this information is very important, but has too many NaN values, that must be days that dont ocurried any event"},{"metadata":{"trusted":true,"_uuid":"3a566aadf444b7067c94037249b14bedadf3ed70"},"cell_type":"code","source":"trip4.Events.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e63b671fdaa242565a9997c35089dd5e62210520"},"cell_type":"code","source":"trip4.Events.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a52d37d2a56ef89772dbf5e4fdce96436a078444"},"cell_type":"code","source":"events = trip4.Events","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4a32e79d8ba33915af3ae81932534052ccfd597f"},"cell_type":"code","source":"events.replace('Rain , Thunderstorm', 'Rain-Thunderstorm', inplace = True)\nevents.replace('Rain , Snow', 'Rain-Snow', inplace = True)\nevents.replace('Fog , Rain', 'Rain-Snow', inplace = True)\nevents.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739c971a32f5953d891174842d01fbcce7ab973c"},"cell_type":"code","source":"events.fillna(value='No-Event', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b699fee6497f238cf4cb4ce0ac74b263af14762"},"cell_type":"code","source":"events.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37f0472d5c8e6358f530480bfe4b78f954e9fe49"},"cell_type":"code","source":"events.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9d4529fd6131340a89c306271963fd2cd8ed5a1"},"cell_type":"code","source":"trip4.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26022816bfcb27a116e0afb56f0a892cb635a8bc"},"cell_type":"markdown","source":"Lets choose the columns we are interested in"},{"metadata":{"trusted":true,"_uuid":"18776794f2e41f2f9dd3bb35d510c0792cdde0e1"},"cell_type":"code","source":"columns_to_drop = ['Max_Temperature_F','Min_TemperatureF', 'Max_Dew_Point_F', 'MeanDew_Point_F', 'Min_Dewpoint_F',\n                   'Max_Humidity', 'Min_Humidity','Max_Sea_Level_Pressure_In', 'Min_Sea_Level_Pressure_In', 'Max_Visibility_Miles',\n                   'Min_Visibility_Miles', 'Max_Wind_Speed_MPH']                 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb5148a00d6c825a1d0e69debc6544a010e09abc"},"cell_type":"code","source":"# converting the trip duration from seconds to minutes\ntrip4.tripduration = trip4.tripduration / 60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26b7403f4f630e8d52bc1d11e2b6e1fc76546045"},"cell_type":"code","source":"trip5 = trip4.drop(columns= columns_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83eefbc059322cc5bc9dda49302c59d432c8a935"},"cell_type":"code","source":"trip5.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f31fe9b9128a52ba1a4b626667e5b020e0d417e7"},"cell_type":"code","source":"trip5.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c774fe07af52b79caa47dd59c2fe24d30765fbf"},"cell_type":"code","source":"trip5.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01ecf4762c07aa4dab0e851c4e40efe420339939"},"cell_type":"markdown","source":"### Next steps  \n\nMy goal is:\n * separete the date into new columns: Year, month, day and hour\n * from the hour column create a new feature that identifies the time of the day:Morning, afternoon and evening, so we can understand the time of day when people use the bike most\n * From the month column, extract the season of the year: Summer, spring, winter and autumn. \n * create days of the week\n \nI think this changes can give us a real understanding about the use patterns\n\n***The work continues. So, to finish these transformations, we will begin the exploratory analysis of the data!***\n\n\n\nThank you so much for coming here. Leave your comment and do not forget to follow the end of this work !!\n\nSee you later !"},{"metadata":{"trusted":true,"_uuid":"6fb4edd779270eba08f2d4918235aab177a6614b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}