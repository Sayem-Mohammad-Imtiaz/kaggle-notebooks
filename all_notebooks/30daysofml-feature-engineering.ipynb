{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing useful Libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfinal_scores_mean = []\nfinal_scores_std = []","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-29T09:06:11.371772Z","iopub.execute_input":"2021-08-29T09:06:11.372208Z","iopub.status.idle":"2021-08-29T09:06:11.37955Z","shell.execute_reply.started":"2021-08-29T09:06:11.372176Z","shell.execute_reply":"2021-08-29T09:06:11.378165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost and Ordinal Encoding","metadata":{}},{"cell_type":"code","source":"data_train = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndata_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in data_train.columns if c not in ('id', 'target', 'kfold')]\nobj_cols = [col for col in useful_features if 'cat' in col]\nX_test = data_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    X_train = data_train[data_train.kfold != fold].reset_index(drop = True)\n    X_valid = data_train[data_train.kfold == fold].reset_index(drop = True)\n    X_test = X_test.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    X_train[obj_cols] = ordinal_encoder.fit_transform(X_train[obj_cols])\n    X_valid[obj_cols] = ordinal_encoder.transform(X_valid[obj_cols])\n    X_test[obj_cols] = ordinal_encoder.transform(data_test[obj_cols])\n   \n    model = XGBRegressor(random_state = fold, tree_method= 'gpu_hist', gpu_id = 0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    predictions_valid = model.predict(X_valid)\n    test_predictions = model.predict(X_test)\n    final_predictions.append(test_predictions)\n    rmse = mean_squared_error(y_valid, predictions_valid, squared = False)\n    scores.append(rmse)\n    print(\"RMSE :\", fold, rmse)\n    \nprint(\"Final Scores\")\nfinal_scores_mean.append(np.mean(scores))\nfinal_scores_std.append(np.std(scores))\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:06:20.268106Z","iopub.execute_input":"2021-08-29T09:06:20.268505Z","iopub.status.idle":"2021-08-29T09:06:41.13887Z","shell.execute_reply.started":"2021-08-29T09:06:20.268473Z","shell.execute_reply":"2021-08-29T09:06:41.137586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standardization (Using Scaler Method)","metadata":{}},{"cell_type":"code","source":"#Standardization\ndata_train = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndata_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in data_train.columns if c not in ('id', 'target', 'kfold')]\nobj_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\nX_test = data_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    X_train = data_train[data_train.kfold != fold].reset_index(drop = True)\n    X_valid = data_train[data_train.kfold == fold].reset_index(drop = True)\n    X_test = X_test.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    X_train[obj_cols] = ordinal_encoder.fit_transform(X_train[obj_cols])\n    X_valid[obj_cols] = ordinal_encoder.transform(X_valid[obj_cols])\n    X_test[obj_cols] = ordinal_encoder.transform(data_test[obj_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(data_test[numerical_cols])\n    \n    model = XGBRegressor(random_state = fold, tree_method= 'gpu_hist', gpu_id = 0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    predictions_valid = model.predict(X_valid)\n    test_predictions = model.predict(X_test)\n    final_predictions.append(test_predictions)\n    rmse = mean_squared_error(y_valid, predictions_valid, squared = False)\n    scores.append(rmse)\n    print(\"RMSE :\", fold, rmse)\n    \nprint(\"Final Scores\")\nfinal_scores_mean.append(np.mean(scores))\nfinal_scores_std.append(np.std(scores))\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:13:25.722913Z","iopub.execute_input":"2021-08-29T09:13:25.72329Z","iopub.status.idle":"2021-08-29T09:13:47.595262Z","shell.execute_reply.started":"2021-08-29T09:13:25.723258Z","shell.execute_reply":"2021-08-29T09:13:47.594154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Normalizer instead of StandardScaler","metadata":{}},{"cell_type":"code","source":"#Using Normalizer instead of StandardScaler\ndata_train = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndata_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in data_train.columns if c not in ('id', 'target', 'kfold')]\nobj_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\nX_test = data_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    X_train = data_train[data_train.kfold != fold].reset_index(drop = True)\n    X_valid = data_train[data_train.kfold == fold].reset_index(drop = True)\n    X_test = X_test.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    X_train[obj_cols] = ordinal_encoder.fit_transform(X_train[obj_cols])\n    X_valid[obj_cols] = ordinal_encoder.transform(X_valid[obj_cols])\n    X_test[obj_cols] = ordinal_encoder.transform(data_test[obj_cols])\n    \n    normalizer = preprocessing.Normalizer()\n    X_train[numerical_cols] = normalizer.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = normalizer.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = normalizer.transform(data_test[numerical_cols])\n    \n    model = XGBRegressor(random_state = fold, tree_method= 'gpu_hist', gpu_id = 0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    predictions_valid = model.predict(X_valid)\n    test_predictions = model.predict(X_test)\n    final_predictions.append(test_predictions)\n    rmse = mean_squared_error(y_valid, predictions_valid, squared = False)\n    scores.append(rmse)\n    print(\"RMSE :\", fold, rmse)\n    \nprint(\"Final Scores\")\nfinal_scores_mean.append(np.mean(scores))\nfinal_scores_std.append(np.std(scores))\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:15:07.442547Z","iopub.execute_input":"2021-08-29T09:15:07.442931Z","iopub.status.idle":"2021-08-29T09:15:29.541752Z","shell.execute_reply.started":"2021-08-29T09:15:07.442899Z","shell.execute_reply":"2021-08-29T09:15:29.5406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Log Tranformation","metadata":{}},{"cell_type":"code","source":"#Log Tranformation\ndata_train = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndata_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in data_train.columns if c not in ('id', 'target', 'kfold')]\nobj_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndata_test = data_test[useful_features]\n\nfor col in numerical_cols:\n    data_train[col] = np.log1p(data_train[col])\n    data_test[col] = np.log1p(data_test[col])\n   \nfinal_predictions =[]\nscores = []\nfor fold in range(5):\n    X_train = data_train[data_train.kfold != fold].reset_index(drop = True)\n    X_valid = data_train[data_train.kfold == fold].reset_index(drop = True)\n    X_test = data_test.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n  \n    ordinal_encoder = OrdinalEncoder()\n    X_train[obj_cols] = ordinal_encoder.fit_transform(X_train[obj_cols])\n    X_valid[obj_cols] = ordinal_encoder.transform(X_valid[obj_cols])\n    X_test[obj_cols] = ordinal_encoder.transform(X_test[obj_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n    \n    model = XGBRegressor(random_state = fold, tree_method= 'gpu_hist', gpu_id = 0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    predictions_valid = model.predict(X_valid)\n    test_predictions = model.predict(X_test)\n    final_predictions.append(test_predictions)\n    rmse = mean_squared_error(y_valid, predictions_valid, squared = False)\n    scores.append(rmse)\n    print(\"RMSE :\", fold, rmse)\n    \nprint(\"Final Scores\")\nfinal_scores_mean.append(np.mean(scores))\nfinal_scores_std.append(np.std(scores))\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:17:51.891911Z","iopub.execute_input":"2021-08-29T09:17:51.892315Z","iopub.status.idle":"2021-08-29T09:18:16.317789Z","shell.execute_reply.started":"2021-08-29T09:17:51.892268Z","shell.execute_reply":"2021-08-29T09:18:16.314705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Polynomial features","metadata":{}},{"cell_type":"code","source":"#Polynomial features\ndata_train = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndata_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in data_train.columns if c not in ('id', 'target', 'kfold')]\nobj_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndata_test = data_test[useful_features]\n\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\ntrain_poly = poly.fit_transform(data_train[numerical_cols])\ntest_poly = poly.fit_transform(data_test[numerical_cols])\n\ntrain_poly_columns = [f\"poly_{i}\" for i in range(train_poly.shape[1])]\ntest_poly_columns = [f\"poly_{i}\" for i in range(test_poly.shape[1])]\n\ndf_poly_train = pd.DataFrame(train_poly, columns = train_poly_columns)\ndf_poly_test = pd.DataFrame(test_poly, columns = test_poly_columns )\n\ndata_train = pd.concat([data_train, df_poly_train], axis =1)\ndata_test = pd.concat([data_test, df_poly_test], axis = 1)\n\nuseful_features = [c for c in data_train.columns if c not in ('id', 'target', 'kfold')]\nobj_cols = [col for col in useful_features if 'cat' in col]\ndata_test = data_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    X_train = data_train[data_train.kfold != fold].reset_index(drop = True)\n    X_valid = data_train[data_train.kfold == fold].reset_index(drop = True)\n    X_test = data_test.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    X_train[obj_cols] = ordinal_encoder.fit_transform(X_train[obj_cols])\n    X_valid[obj_cols] = ordinal_encoder.transform(X_valid[obj_cols])\n    X_test[obj_cols] = ordinal_encoder.transform(X_test[obj_cols])\n    \n    model = XGBRegressor(random_state = fold, tree_method= 'gpu_hist', gpu_id = 0, predictor='gpu_predictor')\n    model.fit(X_train, y_train)\n    predictions_valid = model.predict(X_valid)\n    test_predictions = model.predict(X_test)\n    final_predictions.append(test_predictions)\n    rmse = mean_squared_error(y_valid, predictions_valid, squared = False)\n    scores.append(rmse)\n    print(\"RMSE: \", fold, rmse)\nprint(\"Final Scores\")\nfinal_scores_mean.append(np.mean(scores))\nfinal_scores_std.append(np.std(scores))\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:19:31.123166Z","iopub.execute_input":"2021-08-29T09:19:31.123579Z","iopub.status.idle":"2021-08-29T09:20:09.361023Z","shell.execute_reply.started":"2021-08-29T09:19:31.123548Z","shell.execute_reply":"2021-08-29T09:20:09.359906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using XGboost + OneHotEncoding and Pipleline","metadata":{}},{"cell_type":"code","source":"#Using XGboost + OneHotEncoding and Pipleline\ndata_train = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndata_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in data_train.columns if c not in ('id', 'target', 'kfold')]\nobj_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndata_test = data_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    X_train = data_train[data_train.kfold != fold].reset_index(drop = True)\n    X_valid = data_train[data_train.kfold == fold].reset_index(drop = True)\n    X_test = data_test.copy()\n    \n    y_train = X_train.target\n    y_valid = X_valid.target\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n    \n    numerical_transformer = SimpleImputer(strategy='constant')\n    \n    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                                          ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n    \n    preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),\n                                               ('cat', categorical_transformer, obj_cols)\n                                              ])\n    \n    model_xgboost = XGBRegressor(random_state = fold, tree_method= 'gpu_hist', gpu_id = 0, predictor='gpu_predictor')\n    pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model_xgboost)\n                     ])\n    pipeline_xgb.fit(X_train, y_train)\n    predictions_valid = pipeline_xgb.predict(X_valid)\n    test_predictions = pipeline_xgb.predict(X_test)\n    final_predictions.append(test_predictions)\n    rmse = mean_squared_error(y_valid, predictions_valid, squared = False)\n    scores.append(rmse)\n    print(\"RMSE: \", fold, rmse)\nprint(\"Final Scores\")\nfinal_scores_mean.append(np.mean(scores))\nfinal_scores_std.append(np.std(scores))\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:22:26.554522Z","iopub.execute_input":"2021-08-29T09:22:26.554962Z","iopub.status.idle":"2021-08-29T09:24:38.805968Z","shell.execute_reply.started":"2021-08-29T09:22:26.554914Z","shell.execute_reply":"2021-08-29T09:24:38.804583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scores = pd.DataFrame({'Mean Scores': final_scores_mean, 'Standard Deviation': final_scores_std})\nprint(df_scores)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:36:55.187984Z","iopub.execute_input":"2021-08-29T09:36:55.188459Z","iopub.status.idle":"2021-08-29T09:36:55.205145Z","shell.execute_reply.started":"2021-08-29T09:36:55.188421Z","shell.execute_reply":"2021-08-29T09:36:55.203664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best Score Possible after applying feature engineering methods:\")\nprint(min(final_scores_mean))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:40:08.787238Z","iopub.execute_input":"2021-08-29T09:40:08.787608Z","iopub.status.idle":"2021-08-29T09:40:08.799126Z","shell.execute_reply.started":"2021-08-29T09:40:08.787576Z","shell.execute_reply":"2021-08-29T09:40:08.797633Z"},"trusted":true},"execution_count":null,"outputs":[]}]}