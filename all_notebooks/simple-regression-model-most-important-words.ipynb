{"cells":[{"metadata":{"_uuid":"ce59518992291dafd8d921bb54986e2f06c0e646","_cell_guid":"6bb6d7b7-b34f-4f94-91ae-86ea0a1342ef"},"cell_type":"markdown","source":"# Regression on review scores"},{"metadata":{"_uuid":"01bf33e272d1978ae150dc14fe8b5ebbb3ea6e2e","_cell_guid":"7e2e4e29-afff-4ea6-954d-b5a070b675ca"},"cell_type":"markdown","source":"## Introduction"},{"metadata":{"_uuid":"0da58414c68e1e467b29d1848bdd71e4bce1736d","_cell_guid":"458366d0-0a38-4f69-ada2-000c7f57e9f5"},"cell_type":"markdown","source":"In this notebook, I will fit a simple regression model on the reviewer's scores. The purpose of the model is to try to find out the words/phases that are indicative to the review score of the guest. Can we do it? I am not sure right now, but we will know it later!"},{"metadata":{"_uuid":"6a569b796e7aa56d58d20416b91b679d3737587b","_cell_guid":"2f8c1d94-114d-4b7b-a9eb-fd1761d27268"},"cell_type":"markdown","source":"# Preparing data"},{"metadata":{"_uuid":"ae9877cc3542e9372084a830a19c794863dff9cb","_cell_guid":"68914ed8-f065-450c-8899-f56542aed51d"},"source":"import pandas as pd\ndf = pd.read_csv('../input/Hotel_Reviews.csv')\ndf.head()","cell_type":"code","outputs":[],"execution_count":1},{"metadata":{"_uuid":"2aae07a2b08876a6e40cd9e4fe3eee9320a49dea","_cell_guid":"f9432a78-412d-4a86-8070-4ea32d6dafbb"},"source":"print(df.shape)","cell_type":"code","outputs":[],"execution_count":2},{"metadata":{"_uuid":"5495b9f87b4c93c76797e9aeaf236007d1039cb5","_cell_guid":"71568870-2288-4f5e-b16e-ca730fa05215","collapsed":true},"source":"df['all_review'] = df.apply(lambda x:x['Positive_Review']+' '+x['Negative_Review'],axis=1)","cell_type":"code","outputs":[],"execution_count":3},{"metadata":{"_uuid":"a03b466e8ededc0106cf72bc64114dc82bb33ec1","_cell_guid":"d657b0cb-b0ed-4f14-9c22-a5d99d402a93"},"cell_type":"markdown","source":"The size of data is not quite small, and we want to execute the code quickily as we got a time limitation in kernel! So I decide to train a model on 20% of the data and valid the model on 80% of the data. The validation set (80%) will be splitted into three parts and we will compare the statistics of validation seperately. This is always my validation strategy when I the dataset is large or I do not have enough computation resources."},{"metadata":{"_uuid":"6edbd3a2e85aced1897d44dbabf74ebfecf10110","_cell_guid":"1fc3041b-4143-4913-be91-522a80491717"},"source":"from sklearn.model_selection import train_test_split\ntrain,test1 = train_test_split(df,test_size=0.8,random_state=42)\ntest1,test2 = train_test_split(test1,test_size=0.67,random_state=42)\ntest2,test3 = train_test_split(test2,test_size=0.5,random_state=42)\nprint(train.shape);print(test1.shape);print(test2.shape);print(test3.shape)","cell_type":"code","outputs":[],"execution_count":4},{"metadata":{"_uuid":"ea2e094263daf6b2a459cd44290990e28d4a4c9d","_cell_guid":"0368bcde-f0b5-4421-b9b8-6dd02e4cbecc"},"cell_type":"markdown","source":"I plan to train a TFIDF model on both train and test set, in order to provide the data for sklearn model."},{"metadata":{"_uuid":"12e4acc81a48f1e850a3a8f13fe1263ab22650af","_cell_guid":"8f6a256d-c627-448c-a4a9-9ece77283a8a","collapsed":true},"source":"from sklearn.feature_extraction.text import TfidfVectorizer\nt = TfidfVectorizer(max_features=10000)\ntrain_feats = t.fit_transform(train['all_review'])\ntest_feats1 = t.transform(test1['all_review'])\ntest_feats2 = t.transform(test2['all_review'])\ntest_feats3 = t.transform(test3['all_review'])","cell_type":"code","outputs":[],"execution_count":5},{"metadata":{"_uuid":"cb27f21e94553de2c95281c8ada1ee036d2dca66","_cell_guid":"84ba9e85-cb22-4c74-9bc5-fa4621e9ca87"},"cell_type":"markdown","source":"After this step, the feature preparation is done, and we can start to think on a classifier."},{"metadata":{"_uuid":"2c8a85cbab882133cea814280c2badada8cfb82a","_cell_guid":"bec6edf8-eda4-4ac1-b5cb-61cccb4452f8"},"cell_type":"markdown","source":"## Model Fitting"},{"metadata":{"_uuid":"2688777ed2cd94adb9fd01573bb19a689da5e5a7","_cell_guid":"a3aedcd4-836d-44d6-803b-7ec8313a4424"},"cell_type":"markdown","source":"In this part, we will try to fit a Gradient Boosting Regressor on the data set. The ultimate objective of this, is to have a decent model that can tell us the importance of different words. "},{"metadata":{"_uuid":"3859a595997ca072f9e7daec572d199b31df9399","_cell_guid":"8b375128-990c-4101-95ab-29895404437b","collapsed":true},"source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","cell_type":"code","outputs":[],"execution_count":6},{"metadata":{"_uuid":"5f462f1f73ca581be2cba112906f884ded5dd9af","_cell_guid":"bb7fa539-09e9-48ee-96f7-e190877575ba"},"source":"gbdt = GradientBoostingRegressor(max_depth=5,learning_rate=0.1,n_estimators=150) # Large iteration, fewer estimators\ngbdt.fit(train_feats,train['Reviewer_Score'])","cell_type":"code","outputs":[],"execution_count":7},{"metadata":{"_uuid":"ee39707e75368b4b7b6961d4ce4fb985dc9803a4","_cell_guid":"8853edd8-b660-4fb1-8ec0-8486e63dbac8","collapsed":true},"cell_type":"markdown","source":"This is a simple GBDT model without elegant parameter tuning. Let's evaluate the performance of the model."},{"metadata":{"collapsed":true},"source":"pred_inbag = gbdt.predict(train_feats)\npred_test1 = gbdt.predict(test_feats1)\npred_test2 = gbdt.predict(test_feats2)\npred_test3 = gbdt.predict(test_feats3)","cell_type":"code","outputs":[],"execution_count":8},{"metadata":{},"cell_type":"markdown","source":"Let's first compare the mean absolute error of the inbag data and three out bag data."},{"metadata":{"collapsed":true},"source":"MAEs = pd.DataFrame({'data':['in_bag','out_bag1','out_bag2','out_bag3'],'MAE':[mean_absolute_error(train['Reviewer_Score'],pred_inbag),mean_absolute_error(test1['Reviewer_Score'],pred_test1),mean_absolute_error(test2['Reviewer_Score'],pred_test2),mean_absolute_error(test3['Reviewer_Score'],pred_test3)]})","cell_type":"code","outputs":[],"execution_count":9},{"metadata":{},"source":"MAEs","cell_type":"code","outputs":[],"execution_count":10},{"metadata":{},"source":"from ggplot import *\np = ggplot(MAEs,aes(x='data',weight='MAE')) + geom_bar()+theme_bw()+ggtitle('Mean Absolute Error of GBDT models')\nprint(p)","cell_type":"code","outputs":[],"execution_count":13},{"metadata":{},"cell_type":"markdown","source":"How about Rooted-Mean-Squred-Error?"},{"metadata":{"collapsed":true},"source":"RMSEs = pd.DataFrame({'data':['in_bag','out_bag1','out_bag2','out_bag3'],'RMSE':[mean_squared_error(train['Reviewer_Score'],pred_inbag)**0.5,mean_squared_error(test1['Reviewer_Score'],pred_test1)**0.5,mean_squared_error(test2['Reviewer_Score'],pred_test2)**0.5,mean_squared_error(test3['Reviewer_Score'],pred_test3)**0.5]})","cell_type":"code","outputs":[],"execution_count":16},{"metadata":{},"source":"RMSEs","cell_type":"code","outputs":[],"execution_count":17},{"metadata":{},"source":"p = ggplot(RMSEs,aes(x='data',weight='RMSE')) + geom_bar()+theme_bw()+ggtitle('Rooted Mean Squared Error of GBDT models')\nprint(p)","cell_type":"code","outputs":[],"execution_count":18},{"metadata":{},"cell_type":"markdown","source":"We can see that: the difference between in-bag data and three out-bag data samples are quite close to each other. This doesn't mean model is good enough, but at least it is generalized very well. But also, the errors are within a tolerable range. Therefore, if we take the feature importance of this model, it will be indicative to the guest's review scores."},{"metadata":{},"cell_type":"markdown","source":"## Most important words"},{"metadata":{},"source":"words = t.get_feature_names()\nimportance = gbdt.feature_importances_\nimpordf = pd.DataFrame({'Word' : words,\n'Importance' : importance})\nimpordf = impordf.sort_values(['Importance', 'Word'], ascending=[0, 1])\n## Check the top 30 most important words\nimpordf.head(30)","cell_type":"code","outputs":[],"execution_count":20},{"metadata":{},"cell_type":"markdown","source":"Words with strong emotion implication (like not, rude.etc) gain higher score in feature importance table."},{"metadata":{"collapsed":true},"source":"impordf.to_csv('Most_important_words.csv',index=False)","cell_type":"code","outputs":[],"execution_count":21},{"metadata":{},"cell_type":"markdown","source":"## Concluding Remark"},{"metadata":{},"cell_type":"markdown","source":"With some really simple tricks, one can get a better model than this one. Following are some tips:\n* Perform stemming and remove stopwords in preprocessing\n* Set ngram range to (1,2) in TFIDF training\n* Smaller learning rate, large number of trees\n* Fune tune models and try other algorithm\n\nPlease have fun with this dataset and be creative! Thanks!"},{"metadata":{"collapsed":true},"source":"","cell_type":"code","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.1","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}