{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer Churn Prediction with Imbalanced Data"},{"metadata":{},"cell_type":"markdown","source":"In this project we will use traditional classifiers to predict customer churn. Our dataset is significantly imbalanced, with the 'No Churn' instances outnumbering the 'Churn' ones to the degree that it will influence our models negatively.\n\nWe will deal with this by upsampling the minority class, paying attention to the rate of false negatives as we train our models.\n\nWe start with some exploratory analysis, then data preprocessing and preparation, and finally, machine learning models and comparison of their performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although there don't seem to be any nulls, there are eleven rows where the TotalCharges is \" \"."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df[df['TotalCharges']==\" \"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are eleven rows where the TotalCharges is \" \". These clients have zero tenure, and haven't churned, therefore we presume they're new customers who haven't paid anything yet.\n\nWe register these rows as Total Charges = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TotalCharges']=df['TotalCharges'].replace(\" \",0).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentage of classes\nch=df[df['Churn']=='Yes']\nno_ch=df[df['Churn']=='No']\nprint('churn percentage-->',(ch.shape[0]/df.shape[0])*100)\nprint('no churn percentage-->',(no_ch.shape[0]/df.shape[0])*100)\n\ndf['Churn'].value_counts().plot(kind='pie', autopct='%1.1f%%');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an imbalanced dataset and this will probably pose problems to our machine learning. We will deal with this later."},{"metadata":{"trusted":true},"cell_type":"code","source":"data=df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Analysis"},{"metadata":{},"cell_type":"markdown","source":"Distribution of categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pie(features):\n    for feature in features:\n        plt.figure(figsize=(10,10))\n        plt.subplot(1,2,1)\n        data[data['Churn']=='Yes'][feature].value_counts().plot(kind='pie', autopct='%1.1f%%');\n        plt.title('Churn');\n        plt.subplot(1,2,2)\n        data[data['Churn']=='No'][feature].value_counts().plot(kind='pie', autopct='%1.1f%%');\n        plt.title('No Churn');\n\nfeatures=['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n       'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod']\npie(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of Continuous Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kde(feature):\n    plt.figure(figsize=(14,4))\n    plt.title('Distribution of {}'. format(feature))\n    sns.kdeplot(data[data['Churn']=='Yes'][feature], label='Churn');\n    sns.kdeplot(data[data['Churn']=='No'][feature], label='No Churn');\n\nkde('tenure')\nkde('MonthlyCharges')\nkde('TotalCharges')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def box(feature):\n    plt.figure(figsize=(4,4))\n    sns.boxplot(x='Churn', y=feature, data=data);\n    \nbox('tenure')\nbox('MonthlyCharges')    \nbox('TotalCharges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing and Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"Now we will prepare the dataset for model training, by one-hot encoding categoricals and removing unnecessary columns ('customerID'). For maximus control over the process we will do it in a structured, step-by-step way."},{"metadata":{"trusted":true},"cell_type":"code","source":"#step-by-step dummy encoding, \n#encoding one column at a time and deleting redundant columns\n\ndata.drop(columns=data.columns[0],inplace=True)\n\ndata['Male']=pd.get_dummies(data.iloc[:,0], drop_first=True)\ndata.drop(columns=data.columns[0],inplace=True)\n\ndata['Partner_yes']=pd.get_dummies(data.iloc[:,1],drop_first=True)\ndata.drop(columns=data.columns[1], inplace=True)\n\ndata['Dependent_yes']=pd.get_dummies(data.iloc[:,1],drop_first=True)\ndata.drop(columns=data.columns[1], inplace=True)\n\ndata['Phone_service_yes']=pd.get_dummies(data.iloc[:,2],drop_first=True)\ndata.drop(columns=data.columns[2], inplace=True)\n\ndata['multiple_lines_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,-1]\ndata.drop(columns=data.columns[2], inplace=True)\n\ninternet=pd.get_dummies(data.iloc[:,2],prefix='Internet')\ndata=pd.concat([data,internet],axis=1).drop(columns=['InternetService'])\n\ndata['online security_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='OnlineSecurity',inplace=True)\n\ndata['online backup_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='OnlineBackup',inplace=True)\n\ndata['device protection_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='DeviceProtection',inplace=True)\n\ndata['tech support_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='TechSupport',inplace=True)\n\ndata['streamingTV_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='StreamingTV',inplace=True)\n\ndata['streaming movies_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='StreamingMovies',inplace=True)\n\ncontract=pd.get_dummies(data.iloc[:,2],prefix='contract')\ndata=pd.concat([data,contract],axis=1).drop(columns=['Contract'])\n\ndata['paperless biling_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,1]\ndata.drop(columns='PaperlessBilling',inplace=True)\n\npaymethod=pd.get_dummies(data.iloc[:,2],prefix='paymethod')\ndata=pd.concat([data,paymethod],axis=1).drop(columns=['PaymentMethod'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separate data and labels\ny=data['Churn']\ndata.drop(columns='Churn', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#feature selection via random forest\nforest=RandomForestClassifier(n_estimators=600, max_depth=5, random_state=7)\nforest.fit(data,y)\nimp=forest.feature_importances_\n\n#store feature importances in new DataFrame\nfeature_importances=pd.DataFrame()\nfeature_importances['feature']=pd.Series(data.columns)\nfeature_importances['importance']=imp\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.barplot(x='importance', y='feature', \n            data=feature_importances.sort_values(by='importance',ascending=False));","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#keep most important columns and create final training dataset\ncols=feature_importances.sort_values(by='importance',ascending=False).iloc[:12,0].values\nx=data[cols].values\ndata[cols].head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode labels, and train_test_split\nenc=LabelEncoder()\ny=enc.fit_transform(y)\n\nx_tr,x_ts,y_tr,y_ts=train_test_split(x,y,stratify=y, random_state=77)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling data, only numerical columns\nfrom sklearn.preprocessing import StandardScaler\nnum_cols=[1,3,4]#numerical columns(tenure,total charges, monthly charges)\nsc=StandardScaler()\nx_tr[:,num_cols]=sc.fit_transform(x_tr[:,num_cols])\nx_ts[:,num_cols]=sc.transform(x_ts[:,num_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA visualization"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca=PCA(n_components=3)\nx_tr_pca=pca.fit_transform(x_tr)\n\nx_viz=pd.concat(objs=[pd.DataFrame(x_tr_pca),pd.Series(y_tr)],axis=1).values\n\nplt.figure(figsize=(10,10))\nax=plt.axes()\nxv=x_viz[:,0]\nyv=x_viz[:,1]\nzv=x_viz[:,2]\ncv=x_viz[:,3]\nax.scatter(xv, yv, c=cv, cmap='winter')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine learning"},{"metadata":{},"cell_type":"markdown","source":"We will start by deploying a few models on the raw, unbalanced data, and compare the results with a dummy classifier. If the results don't look good, we will upsample the minority class."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score,recall_score\n\n#DataFrame to store performance metrics for later comparison between models\nresults=pd.DataFrame([], columns=['model', 'parameters','accuracy','precision','recall','F1-score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"nb=GaussianNB()\nnb.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, nb.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, nb.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNN"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"knn=KNeighborsClassifier()\nknn.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, knn.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, knn.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"svm=SVC()\nsvm.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, svm.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, svm.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sgd=SGDClassifier()\nsgd.fit(x_tr, y_tr)\nprint('accuracy:',accuracy_score(y_ts, sgd.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, sgd.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lr=LogisticRegression()\nlr.fit(x_tr,y_tr)\nprint('accuracy\"',accuracy_score(y_ts, lr.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, lr.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tree=DecisionTreeClassifier(max_depth=4, random_state=3)\ntree.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, tree.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, tree.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier(n_estimators=100,max_depth=8,random_state=17)\nrf.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, rf.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, rf.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What would happen if instead of making a classification, we blindly assigned each sample to the majority class? How would the accuracy of this process compare with our models?\n\nLet's try it"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#dummy classifier\nfrom sklearn.dummy import DummyClassifier\n\ndum=DummyClassifier(strategy='most_frequent')\ndum.fit(x_tr,y_tr)\npred=dum.predict(x_ts)\nprint('dummy class:',format(np.unique(pred)))\nprint('dummy accuracy:',accuracy_score(y_ts,pred) )\nsns.heatmap(confusion_matrix(y_ts, dum.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#another way to show this, array with all zeros\naccuracy_score(y_ts,np.zeros(x_ts.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resulting accuracy is exactly equal to the proportion of the majority class in the dataset, which is 73.45%. Our models didn't score much higher than that, so they barely surpassed a dummy classification."},{"metadata":{},"cell_type":"markdown","source":"Before upsampling the minority class, let's consider some models with configurable class weights"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"svm=SVC(class_weight='balanced')\nsvm.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, svm.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, svm.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sgd=SGDClassifier(class_weight='balanced')\nsgd.fit(x_tr, y_tr)\nprint('accuracy:',accuracy_score(y_ts, sgd.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts,sgd.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(class_weight='balanced')\nlr.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, lr.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, lr.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"tree=DecisionTreeClassifier(max_depth=4, random_state=3,class_weight='balanced')\ntree.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, tree.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, tree.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier(n_estimators=100,max_depth=8,\n                          random_state=17,class_weight='balanced')\nrf.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, rf.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, rf.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that although the overall accuracy has somewhat dropped, the ratio of true versus false negatives has been improved in all models, in some cases significantly. The performance is still unacceptable, though, and before playing with the models' parameters, we will try oversampling the minority class."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\nfrom sklearn.dummy import DummyClassifier\n\nx_up, y_up=resample(x_tr[y_tr==1],y_tr[y_tr==1],replace=True,\n                        n_samples=x_tr[y_tr==0].shape[0],random_state=42)\nprint(x_tr[y_tr==1].shape)\nprint(x_up.shape)\n\nx_bal=np.vstack((x_tr[y_tr==0], x_up))\ny_bal=np.hstack((y_tr[y_tr==0],y_up))\n\n\ndum2=DummyClassifier(strategy='most_frequent')\ndum2.fit(x_bal,y_bal)\nprint('dummy accuracy on balanced dataset:',\n      accuracy_score(y_bal,dum2.predict(x_bal)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dummy accuracy has now been lowered to 50%, and any results our models have from now on will be considered 'real' results."},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"gb=GaussianNB()\ngb.fit(x_bal,y_bal)\ny_pred=gb.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Gaussian NB', 'default', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, gb.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNN"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"knn=KNeighborsClassifier()\nknn.fit(x_bal,y_bal)\ny_pred=knn.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['KNN', 'default', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, knn.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm=SVC()\nrng=[0.01, 0.1, 1.0, 10.0, 100.0]\nparams={'C':rng, 'gamma':rng}\ngs=GridSearchCV(estimator=svm,param_grid=params)\n\n\ngs.fit(x_bal,y_bal)\nbest_params=gs.best_params_\nbest_est=gs.best_estimator_\n\nprint('best params',best_params)\n\nbest_est.fit(x_bal,y_bal)\n\ny_pred=best_est.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['SVM', best_params, accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\n\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"svm=SVC()\nsvm.fit(x_bal,y_bal)\n\n\ny_pred=svm.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['SVM', 'default', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\n\nsns.heatmap(confusion_matrix(y_ts, svm.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD"},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd=SGDClassifier(random_state=3)\nparams={'loss':['log', 'modified_huber', 'squared_hinge'],\n       'penalty': ['l1','l2'],\n       'alpha':[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\ngs=GridSearchCV(estimator=sgd, param_grid=params)\ngs.fit(x_bal, y_bal)\nbest=gs.best_estimator_\nbest_params=gs.best_params_\n\nbest.fit(x_bal,y_bal)\n\ny_pred=best.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['SGC', best_params, accuracy,precision,recall,f1]],columns=list(results.columns)))\n\n\nprint('best parameters:',best_params)\n\nprint('best estimator accuracy:',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(random_state=3)\nparams={'penalty':['l2'],\n       'C':[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n       'solver': [ 'sag','saga', 'lbfgs']}\ngs=GridSearchCV(estimator=lr,param_grid=params)\ngs.fit(x_bal,y_bal)\nbest=gs.best_estimator_\nbest_params=gs.best_params_\nprint('best params:', best_params)\ny_pred=best.predict(x_ts)\n\nbest.fit(x_bal,y_bal)\n\ny_pred=best.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Logistic Regression', best_params, accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy\"',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"tree=DecisionTreeClassifier(max_depth=4, random_state=3)\ntree.fit(x_bal,y_bal)\n\ny_pred=tree.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Decision Tree', 'max_depth=4, rand_state=3', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy)\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier(n_estimators=1000,max_depth=10,random_state=17)\nrf.fit(x_bal,y_bal)\n\n\ny_pred=rf.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Random Forest', 'max_depth=10, rand_state=17', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\n\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ada Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree=DecisionTreeClassifier(criterion='entropy',random_state=1, max_depth=1)\nada=AdaBoostClassifier(base_estimator=tree,n_estimators=1000,learning_rate=0.1, random_state=5)\nada.fit(x_bal,y_bal)\n\ny_pred=ada.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['AdaBoost Tree', 'criterion=entropy, max_depth=1, rate=0.1, estimators=500',\n                                      accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy)\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosted Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"gradb=GradientBoostingClassifier(random_state=42)\ngradb.fit(x_bal,y_bal)\n\ny_pred=gradb.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['GradientBoosted Tree', 'default, random_state=42', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy)\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The models may seem to have dropped in performance, but previously the dummy classifier's accuracy was 73% and the models exceeded that by two percent or so, while now the dummy accuracy is 50% and the models supersede it by up to 28,5%, and with fewer false negatives."},{"metadata":{"trusted":true},"cell_type":"code","source":"results=results.reset_index().drop(columns='index')\nresults","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.catplot(y='model', x='accuracy', kind='bar', data=results.sort_values(by='accuracy',ascending=False), color='grey');\nplt.title('Model Accuracy');\nprint(' ')\nsns.catplot(y='model', x='F1-score', kind='bar', data=results.sort_values(by='F1-score',ascending=False), color='black');\nplt.title('model F1-score' );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above barplots is evident that Random Forest, Gradient Boosted Tree, and SVM give the best results, and should be use to predict customer churn."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}