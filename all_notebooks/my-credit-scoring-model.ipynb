{"cells":[{"metadata":{"_cell_guid":"5250962e-8d89-486e-a353-5406f9b5d8e9","_uuid":"1e7821ad80b7859f1ab6a0b44a636ed57759fd2b"},"cell_type":"markdown","source":"# Credit scoring model\n"},{"metadata":{"_cell_guid":"c3dad96c-cd6a-45c6-8f56-b0f2ed4ff281","_uuid":"75d589dad563b8e204c671dc2156bb3719373677"},"cell_type":"markdown","source":"### Introduction : Pratique, explication du code et résultats"},{"metadata":{"_cell_guid":"0262c904-8746-4233-8119-b9a0cec9b746","_uuid":"24d61285addf55e625f3e1fb4c171840adb525e0"},"cell_type":"markdown","source":"#### Objectif: créer un algorithme de notation du crédit qui prédit la probabilité qu'un demandeur de prêt soit en défaut de remboursement."},{"metadata":{"_cell_guid":"92fe9a80-e04a-44cd-83e1-a637d8ef370e","_uuid":"0b877b61b35f533f719def824c30ad0583f92c76","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\n# importing necessary libraries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Import des du fichiers dans deux dataFrames differents"},{"metadata":{"_cell_guid":"2d9c866b-b9de-4da3-a050-34607187d0a8","_uuid":"34799a08532b29ff15ef1fff66f2134a1f61591e","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/hmeq.csv\")\ndf_i = pd.read_csv(\"../input/hmeq.csv\")\n# lecture de l'entrée\n# En le stockant dans 2 dataframes, nous effectuerons nos opérations sur df En cas de besoin de table initiale sans aucun changement nous pouvons utiliser df_i","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"e2a3e872-e4f0-4fa7-8a17-111e06f22025","_uuid":"894c0777495622b8882b043c12bd7f1f1eab7a99","trusted":true},"cell_type":"code","source":"df.head()\n\n# Aperçu du jeu de donnéesv, les différentes colonnes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59b09832-2cc6-4088-bf22-28f0e444162b","_uuid":"2cb62c8a3c6b0fe3a15ba8aba6d5cf6a10f24fa2"},"cell_type":"markdown","source":"# # Comprendre les données"},{"metadata":{"_cell_guid":"7306478c-d2ee-4194-9adb-3488ae0f6ef1","_uuid":"cc8280608505d75ae00163141523a0a6dbb3940d","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03fb1e37-3711-4df3-b307-de3ecdac1353","_uuid":"40b43ea3844352865291d8af1a5a82e55989a471","trusted":true},"cell_type":"code","source":"df.info()\n\n# nombre d'entrées qui ne sont pas c'est à dire  non null ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b011a359-ec79-449e-b53e-b57ad67102e6","_uuid":"ee15c1170eae275a0bebd73d2a01072457b2fd29","trusted":true},"cell_type":"code","source":"df.describe()\n# Statistiques descriptives\n# Distribution des données\n# Il n'y a pas d'anomalies dans les données (respecter les maximums et les moyennes dans chaque cas)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"14ef58d2-6c5a-49ea-9556-7aa570b2fdad","_uuid":"fd478b03b804200a416bd3502c655ac195639609","trusted":true},"cell_type":"code","source":"df.columns\n# Colonnes de l'ensemble de données","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f7756abc-fefa-4474-866c-df869817e956","_uuid":"27eabd1708aba64fe12414d8bcd5deef30470378"},"cell_type":"markdown","source":"## Distributions de diverses variables"},{"metadata":{"_cell_guid":"49dde92a-f600-43b3-99cf-f02aac905689","_uuid":"7f7c90724a3e57dd4105520ef47de65e43368645","trusted":true},"cell_type":"code","source":"print(df[\"BAD\"].value_counts())\ndf[\"BAD\"].value_counts().plot(\"barh\")\n# distribution de la variable cible \"BAD\"\n# La classe cible est un peu déséquilibrée - les zéros sont d'environ 80% et les uns sont d'environ 20%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" distribution de la variable cible \"BAD\"\n# La classe cible est un peu déséquilibrée - les zéros sont d'environ 80% et les uns sont d'environ 20%"},{"metadata":{"_cell_guid":"c5627a89-053d-4082-83b6-302aac96aea4","_uuid":"b1ea298c1315424029509d8f85ca0452d3f256ab","trusted":true},"cell_type":"code","source":"print(df[\"REASON\"].value_counts())\n\n# Ceci est une fonctionnalité nominale, elle doit être modifiée de manière à pouvoir l'utiliser.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0db7a298-7357-4dc3-9abb-98b330cd9f2b","_uuid":"5e59cbab8100044beb6cda90efdbcb2d7fa2490e","trusted":true},"cell_type":"code","source":"print(df[\"JOB\"].value_counts())\n\n# Identique au cas ci-dessus, nous devons trouver un moyen de l'utiliser.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b936c067-465b-4228-bd62-c4f97a0cd805","_uuid":"768df95096939dde03de17887472876cd1fa0c97","trusted":true},"cell_type":"code","source":"df[\"LOAN\"].plot.hist(bins = 20,figsize=(15,7.5))\n\n# distribution de la variable de prêt\n# la densité entre 10000-30000 est élevée","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17fc307a-7077-4135-87b5-c306c9242f2f","_uuid":"415cfb6ff40b6907872ccdfbab6fc2e3df393438","trusted":true},"cell_type":"code","source":"df[\"DEBTINC\"].plot.hist(bins = 20,figsize=(15,5))\n \n# Highly populated around 25-50\n# We may cap off the end values if required.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution de la variable de prêt\n#### la densité entre 10000-30000 est élevée"},{"metadata":{"_cell_guid":"c02ade06-82be-408e-bf66-ae638dbd3eb2","_uuid":"1d277ca76a0fda460b709232911fc2bdf43dab7c","trusted":true},"cell_type":"code","source":"df[\"CLAGE\"].plot.hist(bins = 20,figsize=(15,7.5))\n\n# La densité est élevée autour de 100-300 # Nous pouvons plafonner les valeurs> = 600 pour obtenir de meilleurs résultats","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47cf3519-6222-47b1-a79c-8859cc649469","_uuid":"83bf7d2f8009138e4c88292c74f953786b4f3a18","trusted":false,"collapsed":true},"cell_type":"code","source":"df[\"CLNO\"].plot.hist(bins = 20,figsize=(15,5))\n\n# Cette distribution semble bonne et nous n'avons rien à modifier ici.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3d5cf36-5559-4935-8670-521234e8c57f","_uuid":"01d7d5e95f97199f051bb25aa376e7690a5c9efc","trusted":true},"cell_type":"code","source":"df[\"VALUE\"].plot.hist(bins = 80,figsize=(15,7.5))\n\n# La concentration est élevée autour de 80000-100000 # Il y a très moins de valeurs à la fin (> = 400000) qui sont un peu élevées par rapport à la moyenne. Nous pouvons les plafonner.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b4b4aff-09cd-4973-b4a9-64908f80cbee","_uuid":"4184e791dc9c2b79738caeb1de97537a9a9b6358","trusted":true},"cell_type":"code","source":"df[\"MORTDUE\"].plot.hist(bins = 40,figsize=(15,7.5))\n\n# La concentration est élevée autour de 40000-100000 # Les valeurs à la fin (> = 300000) peuvent être plafonnées.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"640703a9-bf59-49b5-952b-63fa9271cb07","_uuid":"3bdf96b470655adb6519811622c044a3da88fa3e","trusted":true},"cell_type":"code","source":"df[\"YOJ\"].plot.hist(bins = 40,figsize=(15,7.5))\n# C'est très biaisé. Il serait préférable de modifier cette variable pour diminuer l'asymétrie.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff0830b3-6518-49fc-8604-7017f5dd86f6","_uuid":"4de134a79012663247470776227ff82c70399cd3","trusted":true},"cell_type":"code","source":"df[\"DEROG\"].value_counts()\n\n# Des incidents dérogatoires n'ont été signalés que dans quelques cas. # Ainsi, créer une variable binaire avec des valeurs 1 pour au moins un incident désobligeant et 0 pour aucun rapport de ce type peut être utile.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1de12f81-0f42-401b-a6a6-9da98ef89082","_uuid":"373c8c9e48071418bffa7179ec6e65d93f6a957b","trusted":true},"cell_type":"code","source":"df[\"DELINQ\"].value_counts()\n\n# La plupart d'entre eux sont nuls. # Identique au cas ci-dessus, la création d'une variable binaire serait utile.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4152e509-3230-4ea3-b1b4-dc90a95ee71d","_uuid":"5d59237ba3eb5c946edf850eac9f37598adb34b3","trusted":true},"cell_type":"code","source":"df[\"NINQ\"].value_counts()\n# Distribué principalement entre les cinq premières valeurs","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edcfd441-f458-4b83-833d-6d2a0a3c2c0e","_uuid":"1c029e6aee41bbde2ebe65b0b78b35ed395a2117"},"cell_type":"markdown","source":"### Conclusions: - Les distributions sont correctes et il n'y a pas d'anomalies dans les données. <br> - DEBTINC a un nombre très élevé de données manquantes (sera traité dans la section suivante - Imputation des variables). <br> - La fonction YOJ est fortement biaisée et peut être modifiée pour réduire l'asymétrie. <br> - Caractéristiques nominales: JOB et REASON doivent être modifiés de manière à pouvoir les utiliser pour le modèle de régression logistique. <br> - DELINQ, DEROG peut être divisé en 2 classes pour créer de nouvelles variables binaires. <br> - VALUE, MORTDUE, CLAGE, DEBTINC peuvent être plafonnés à la fin, c'est-à-dire que les valeurs très élevées seront réglées sur une valeur inférieure sélectionnée. -------------------------------------------------- -------------------------------------------------- ---------------------------"},{"metadata":{"_cell_guid":"cf2bdeea-3e64-4ef1-9c50-2d113cca4159","_uuid":"e3c04699a58e036f518ef0f2f1b7ab06e4241d18"},"cell_type":"markdown","source":"# Imputation des variables d'entrée\n<br>"},{"metadata":{"_cell_guid":"cb3a019e-a772-49f1-b643-7404389bd9b9","_uuid":"9324154cd3639dd508bf07bd19d98d95b2a43b40","trusted":true},"cell_type":"code","source":"df.isnull().sum()\n\n# Nombre de cas avec Nan.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db63a170-fc07-4c4b-bace-0f695bf0d1d9","_uuid":"42a90c310b09cf665c7f8e78fa3d730cebaf714e"},"cell_type":"markdown","source":"#### Observations: - Sauf dans le cas de DEBTINC, dans tous les autres cas, seules quelques valeurs n'ont pas été rapportées - Pour imputer les valeurs manquantes, nous pouvons penser à quelques idées comme: - En cas de caractéristiques nominales, les remplacer par la majorité classe - Dans le cas de variables numériques comme DEROG et DELINQ, la plupart des cas sont 0. Nous pouvons les remplacer par la classe majoritaire. - Dans le cas d'autres entrées numériques, nous pouvons les remplacer par la médiane ou la moyenne sans en modifier le plus. Dans ce cahier, je vais les remplacer par la colonne respective."},{"metadata":{"_cell_guid":"2992da60-7992-4c8b-94c1-86497dec98af","_uuid":"45ad614eaac15cfad47fe539de7c2f57e1b61ad0","trusted":true},"cell_type":"code","source":"df[\"DEROG\"].fillna(value=0,inplace=True)\ndf[\"DELINQ\"].fillna(value=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26324159-605e-44fc-808a-529a88dc4335","_uuid":"6c96bc432c716f888a2dff18d3a5b10e21a3be9a","trusted":true},"cell_type":"code","source":"# Caractéristiques nominales # Remplacement par classe majoritaire # classe majoritaire en cas de variable JOB est Other # classe majoritaire en cas de REASON varibale est DebtCon\n\ndf[\"REASON\"].fillna(value = \"DebtCon\",inplace = True)\ndf[\"JOB\"].fillna(value = \"Other\",inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0464267c-404e-4fad-81a9-9758133b2ecf","_uuid":"7a141da0fc09f2b9058bf92db2920b7c6b609b6f","trusted":true},"cell_type":"code","source":"# Caractéristiques numériques # Remplacement en utilisant la moyenne de chaque classe\n\ndf.fillna(value=df.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9786e0f4-bab5-4e9b-b827-a5b89622d8fc","_uuid":"fde2d79626b44d001ce17a7cdac2a8faf713525b","trusted":true},"cell_type":"code","source":"df.isnull().sum()\n# Caractéristiques numériques\n# Remplacement en utilisant la moyenne de chaque classe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5cbb27b9-d940-4297-8ca9-e94e0a427d08","_uuid":"81fdd8360290e233ed4f43c94bd45048b441c1e2"},"cell_type":"markdown","source":"### Regard final sur les données après avoir rempli les valeurs manquantes"},{"metadata":{"_cell_guid":"6b9621a7-513f-42fa-83bd-04619fe1b831","scrolled":true,"_uuid":"e4f79c1eea50ca2a73cb8379af27bbd3839d2324","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ea93d14-285f-4c11-9e66-93ecf83934cd","_uuid":"8994a697b848e26025909e5ff3d82effe73575ae"},"cell_type":"markdown","source":"# Application des modèles sur les données après imputation\n- Application de la classification de base aux données après remplacement / imputation. Vérifions la performance en appliquant à la fois les algorithmes de régression logistique et d'arbre de décision.\n- Avant d'appliquer les algorithmes, les données sont divisées en ensembles d'apprentissage et de test dans le rapport 2: 1, soit 33% des données de test et 67% des données de train.\n- Et aussi en prenant toutes les colonnes sauf JOB, RAISON comme caractéristiques d'entrée (comme ce sont des caractéristiques nominales, elles doivent être transformées en d'autres variables pour être utilisables, ce qui est pris en compte dans la section suivante)."},{"metadata":{"_cell_guid":"ea11d909-3809-41d9-976d-c56b3bf07eda","_uuid":"cc0dd44832f644d6f8cca371d60ec76e4c48282e","trusted":true},"cell_type":"code","source":"# importer les modules requis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# suppression des fonctionnalités BAD, JOB, REASON de l'ensemble des fonctionnalités d'entrée\nx_basic = df.drop(columns=[\"BAD\",\"JOB\",\"REASON\"])\ny = df[\"BAD\"]\n\n# Fractionnement des données en ensembles de test et de train\nx_basic_tr,x_basic_te,y_tr,y_te = train_test_split(x_basic,y,test_size =.33,random_state=1)\nlogreg_basic = LogisticRegression()\n\n# Formation du modèle de régression logistique de base avec un ensemble de formation\nlogreg_basic.fit(x_basic_tr,y_tr)\n\n\nprint(\"intercept \")\nprint(logreg_basic.intercept_)\nprint(\"\")\nprint(\"coefficients \")\nprint(logreg_basic.coef_)\n\n# Prédire la sortie des cas de test à l'aide de l'algorithme créé ci-dessus\ny_pre = logreg_basic.predict(x_basic_te)\n\n# Validating the algorithm using various Performance metrics\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nprint(\"\")\na1 = accuracy_score(y_te,y_pre)\nf1 = f1_score(y_te, y_pre, average=\"macro\")\np1 = precision_score(y_te, y_pre, average=\"macro\")\nr1 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a1)\nprint(\"f1 score : \",f1)\nprint(\"precision score : \",p1)\nprint(\"recall score : \",r1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96aec278-9752-45f3-871d-1e353d648c5a","_uuid":"e95830fc807b428670c1a9f01a348b6485a34c06","trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    Cette fonction imprime et trace la matrice de confusion.\n    La normalisation peut être appliquée en définissant `normalize = True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Véritable étiquette')\n    plt.xlabel('étiquette prédite')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f9c8467-8096-4314-94d2-fa1c67b95482","_uuid":"969fdbb3a1ab5a52b3926e05341b3fc38f530864","trusted":true},"cell_type":"code","source":"# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Tracer une matrice de confusion non normalisée\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Matrice de confusion - Algorithme de régression logistique')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2fec8c4-6557-4713-8b93-989a4818e7b2","_uuid":"05020ed83965d68f851b49b88ee5dfa836cda1fd","trusted":true},"cell_type":"code","source":"# importer les modules requis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\ndectree_basic = DecisionTreeClassifier()\ndectree_basic.max_depth = 100\n# Formation du modèle d'arbre de décision de base avec un ensemble de formation\ndectree_basic.fit(x_basic_tr,y_tr)\n# Prédire la sortie des cas de test à l'aide de l'algorithme créé ci-dessus\ny_pre = dectree_basic.predict(x_basic_te)\n# Validation de l'algorithme à l'aide de diverses métriques de performances\n\na2 = accuracy_score(y_te,y_pre)\nf2 = f1_score(y_te, y_pre, average=\"macro\")\np2 = precision_score(y_te, y_pre, average=\"macro\")\nr2 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a2)\nprint(\"f1 score : \",f2)\nprint(\"precision score : \",p2)\nprint(\"recall score : \",r2)\n\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Confusion matrix,Decision Tree Algorithm')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edd08b3e-cf0e-4375-8f57-4b58ec0c0249","_uuid":"16e5e3fa4166071dcaddc566c3b6c9c01f71f93e"},"cell_type":"markdown","source":"#### Quelques discussions sur les mesures de performance:\n- Généralement, le test de précision peut être utilisé pour évaluer les algorithmes. Mais dans ce cas, le simple fait d'utiliser la CLASSE DE MAJORITÉ (0) pour prédire la sortie donnera une précision élevée (79,2%).\n<br>\n- Par conséquent, d'autres mesures de performance doivent être utilisées pour évaluer le modèle.\n    - Score F1: moyenne pondérée de rappel et de précision\n    - Rappel: (TP / TP + FN)\n    - Précision: (TP / TP + FP)\n      TP est vrai positif, FN est faux négatif, FP est faux positif\n<br>\n- Ici, nous voulons diminuer le nombre de faux négatifs, c'est-à-dire que nous prévoyons que le crédit sera remboursé mais qu'il s'agit en fait d'un fraudeur. Diminuer FN implique d'augmenter le rappel.Par conséquent, RECALL sera la mesure de performance parfaite pour évaluer ce modèle.\n<br>\n- La précision peut diminuer dans le processus pour augmenter le rappel, mais il est normal de prédire quelques faux positifs supplémentaires.\n##### Nous pouvons également REMPLACER les données (nous y reviendrons à la fin).\n\n#### Conclusions:\n\n- En utilisant la régression logistique bien que la précision soit bonne (79%), le modèle n'a pas bien fonctionné sur les autres mesures de performance.Recal est juste au-dessus de 0,5 et ce n'est pas bon.Cela peut être dû à un surajustement et nous allons essayer de le supprimer dans la section suivante.\n<br>\n- Étonnamment, l'algorithme de l'arbre de décision a très bien fonctionné par rapport à la régression logistique avec un RAPPEL d'environ 0,78 et une très bonne PRÉCISION, car ce modèle effectue implicitement une sélection de variables / de fonctionnalités en divisant les nœuds supérieurs en fonction des caractéristiques les plus importantes du la sélection des données et des fonctionnalités se fait automatiquement.\n<br>\n- Enfin ce que je veux dire c'est:\n    - Il y aura une bonne amélioration du modèle de régression logistique après la sélection des fonctionnalités.\n    - Les résultats resteront presque les mêmes dans le cas du modèle Arbre de décision, même après la sélection des fonctionnalités.\n<br>\n- Nous allons prouver l'hypothèse ci-dessus en créant des modèles avec des caractéristiques sélectionnées et les comparer aux modèles ci-dessus."},{"metadata":{"_cell_guid":"e6b54a62-55ef-419b-b17c-67151897fa37","_uuid":"c4a581741b6bc611b1b6480aa980f25b0fc893b6"},"cell_type":"markdown","source":"# Transformation des fonctionnalités "},{"metadata":{"_cell_guid":"ceb3eeca-2f36-458b-993f-0a912a458c14","_uuid":"8836ea917290833b5a736b0736b84b2c32971688"},"cell_type":"markdown","source":"- Avant la sélection des fonctionnalités, comme indiqué dans la section \"Distribution des différentes fonctionnalités\", nous devons transformer certaines variables afin d'améliorer la prévisibilité.\n- Nous avons transformé l'ensemble de données, pas seulement l'ensemble de formation."},{"metadata":{"_cell_guid":"36ab1024-abb9-4e80-b882-0ea74445eb0c","_uuid":"3d5da69101dd67bc44d59815b9b7d9080269472b","trusted":true},"cell_type":"code","source":"# Couronnement des fonctionnalités CLAGE (valeurs> = 600 à 600), VALUE (valeurs> = 400000 à 400000), MORTDUE (valeurs> = 300000 à 300000) et DEBTINC (valeurs> = 100 à 100)\ndf.loc[df[\"CLAGE\"]>=600,\"CLAGE\"] = 600\ndf.loc[df[\"VALUE\"]>=400000,\"VALUE\"] = 400000\ndf.loc[df[\"MORTDUE\"]>=300000,\"MORTDUE\"] = 300000\ndf.loc[df[\"DEBTINC\"]>=100,\"DEBTINC\"] = 100","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ad43938-bce6-418f-bc28-d09c3b2ded0a","scrolled":true,"_uuid":"5be8897ef20883ae16d3f301bfa3126e102051d7","trusted":true},"cell_type":"code","source":"# Création de nouvelles variables binaires B_DEROG, B_DELINQ à partir de DEROG, DELINQ\n\ndf[\"B_DEROG\"] = (df[\"DEROG\"]>=1)*1\ndf[\"B_DELINQ\"] = (df[\"DELINQ\"]>=1)*1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"694735d3-cee0-4299-9e1c-51b195408a78","_uuid":"b434748c72638d2f6150f00008ae0f62f36617fb","trusted":true,"collapsed":true},"cell_type":"code","source":"df[\"JOB\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2e6b4f84-82ae-4933-9e33-01261a788ecd","_uuid":"17bc57d6530e1eb8f15dfe767315a9bc46775b6c","trusted":true},"cell_type":"code","source":"# Nous devons convertir les caractéristiques nominales JOB et RAISON en une forme utilisable et les supprimer de la table de données\ndf[\"REASON_1\"] = (df[\"REASON\"] == \"HomeImp\")*1\ndf[\"REASON_2\"] = (df[\"REASON\"] != \"HomeImp\")*1\ndf[\"JOB_1\"] = (df[\"JOB\"]==\"Other\")*1\ndf[\"JOB_2\"] = (df[\"JOB\"]==\"Office\")*1\ndf[\"JOB_3\"] = (df[\"JOB\"]==\"Sales\")*1\ndf[\"JOB_4\"] = (df[\"JOB\"]==\"Mgr\")*1\ndf[\"JOB_5\"] = (df[\"JOB\"]==\"ProfExe\")*1\ndf[\"JOB_6\"] = (df[\"JOB\"]==\"Self\")*1\ndf.drop([\"JOB\",\"REASON\"],axis = 1,inplace = True)\n\n# L'affectation ci-dessus crée de nouvelles fonctionnalités pour chaque JOB et chaque RAISON","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2027323b-929c-4370-b617-acf1892e1957","_uuid":"30cc8baf6d5644499872551150cceb575224e795","trusted":false},"cell_type":"code","source":"# Nous devons diminuer l'asymétrie de la fonctionnalité YOU, pour cela nous pouvons appliquer le log de YOU mais comme certains d'entre eux sont 0, nous utiliserons log (constante YOJ)\ndf[\"YOJ\"] = df[\"YOJ\"].apply(lambda t : np.log(t+1))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c929b270-27aa-45db-a32d-e6914126dab1","scrolled":true,"_uuid":"43d0b79844374cddd4e1e5f3e78825e00391c508","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3708713a-521d-46c0-a3ea-df142074ff52","_uuid":"2f1ec7e61b694374dc0bca9eeffd3002864acc8e"},"cell_type":"markdown","source":"* # Sélection de fonctionnalité"},{"metadata":{"_cell_guid":"26dde5ef-ddcd-4200-9b94-7a858987e424","_uuid":"45b1450f0294b4d3cdba4c18092da98546d1306c"},"cell_type":"markdown","source":"- Au fur et à mesure que nous avons terminé la partie transformation, nous passons maintenant à la sélection des fonctionnalités. Nous allons maintenant découvrir les fonctionnalités d'importation les plus affectant la variable cible \"MAUVAIS\".\n- Nous utiliserons les éléments suivants à cette fin:\n    - Facteur de corrélation de Pearson Pearson\n    - test du chi carré\n    - f_régression\n    - f_classif"},{"metadata":{"_cell_guid":"b1c1b77d-b2cc-44bb-a7e8-c41630d38366","_uuid":"71111881a571b5f043ad44058cc35d24caf726c8"},"cell_type":"markdown","source":"### Utilisation du facteur de corrélation de Pearson pour la sélection des fonctionnalités"},{"metadata":{"_cell_guid":"f8f82cfb-4bfa-42aa-ae9c-99de79d332a6","scrolled":false,"_uuid":"f483c388ba8f80f39dcfd31cf96303f0462a92a1","trusted":true},"cell_type":"code","source":"# Recherche de corrélation entre toutes les fonctionnalités et la fonctionnalité cible \"BAD\"\n\ndf.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19895063-f4ae-469a-acde-2a4de0a575e5","_uuid":"bf131e30f1a12f158318b333f63f4181df51b732","trusted":true},"cell_type":"code","source":"# Rassembler les 2 ensembles de fonctionnalités avec une valeur de corrélation élevée, une avec 7 et l'autre avec 10 fonctionnalités\n\nfeat1=[\"DEROG\",\"DELINQ\",\"CLAGE\",\"NINQ\",\"DEBTINC\",\"YOJ\",\"LOAN\"]\n#feat2=[\"DEROG\",\"DELINQ\",\"CLAGE\",\"NINQ\",\"DEBTINC\",\"LOAN\",\"JOB_2\",\"YOJ\",\"JOB_3\",\"MORTDUE\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e29aa3a9-6d9a-4772-be86-6aa8713d230f","_uuid":"0981e04ce21d380238edc09f3c7c0a58b1a0e7a8"},"cell_type":"markdown","source":"## Maintenant que nous avons les fonctionnalités avec une forte corrélation avec la fonctionnalité BAD, nous allons exécuter les algorithmes de classification et les comparer"},{"metadata":{"_cell_guid":"2f8b04ef-d62a-4d98-8c44-dfad9749ed44","_uuid":"baf5967ace7b12333d8fc74cd87251e29d03a02f","trusted":true},"cell_type":"code","source":"# Régression logistique à l'aide de l'ensemble de fonctionnalités 1 ci-dessus\n\nx = df[feat1]\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = 0.33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre = logreg.predict(x_te)\na3 = accuracy_score(y_te,y_pre)\nf3 = f1_score(y_te, y_pre, average=\"macro\")\np3 = precision_score(y_te, y_pre, average=\"macro\")\nr3 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a3)\nprint(\"f1 score : \",f3)\nprint(\"precision score : \",p3)\nprint(\"recall score : \",r3)\n\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Tracer une matrice de confusion non normalisée\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Matrice de confusion - Algorithme de régression logistique avec Pearson corr_f')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61ec51fa-6a4a-4b8b-b85e-a5db149246f6","_uuid":"1205acdde8fffe7dad8a960d9b6110eb2eb79473","trusted":false,"collapsed":true},"cell_type":"code","source":"# Classificateur d'arbre de décision utilisant feat1\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na4 = accuracy_score(y_te,y_pre)\nf4 = f1_score(y_te, y_pre, average=\"macro\")\np4 = precision_score(y_te, y_pre, average=\"macro\")\nr4 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a4)\nprint(\"f1 score : \",f4)\nprint(\"precision score : \",p4)\nprint(\"recall score : \",r4)\nprint(\"\")\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Tracer une matrice de confusion non normalisée\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Matrice de confusion - Algorithme d\\'arbre de décision utilisant Pearson corr_f')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3590e63-71e2-4456-af83-3c9c94fa5f8e","_uuid":"60de7f2d1a75afff8390a7da1f5e7d4e29daf38a"},"cell_type":"markdown","source":"### Utilisation du test chi2 pour la sélection des fonctionnalités"},{"metadata":{"collapsed":true,"_cell_guid":"c34ea72c-7816-49b4-8ddc-13a814494447","_uuid":"8ed5c9e339b4163f8e94a833177ec6830e9772a7","trusted":false},"cell_type":"code","source":"# Trouver les 10 meilleures fonctionnalités à l'aide du test chi2\n\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\ndf_new = pd.DataFrame(SelectKBest(chi2, k=10).fit_transform(df.drop([\"BAD\"],axis = 1),df[\"BAD\"]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"132a03f7-29ae-4454-b468-8e7f0c2e8a24","_uuid":"30cbb0ea67c74734e5fcbe8497f744896242ca30","trusted":false,"collapsed":true},"cell_type":"code","source":"# dataframe contenant les fonctionnalités sélectionnées\n\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d2984b2-b8a0-4c7e-b9f2-0405307dee0e","_uuid":"0f94ca7e902533b8e8e74e1369b0184c39852a67","trusted":true,"collapsed":true},"cell_type":"code","source":"# Exécution de l'algorithme de régression logistique en utilisant les fonctionnalités sélectionnées à partir du test chi2\nx = df_new\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = .33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre = logreg.predict(x_te)\ny_pre = logreg.predict(x_te)\na5 = accuracy_score(y_te,y_pre)\nf5 = f1_score(y_te, y_pre, average=\"macro\")\np5 = precision_score(y_te, y_pre, average=\"macro\")\nr5 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a5)\nprint(\"f1 score : \",f5)\nprint(\"precision score : \",p5)\nprint(\"recall score : \",r5)\n\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Tracer une matrice de confusion non normalisée\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n title='Matrice de confusion - Algorithme de régression logistique avec test chi2')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32b94f00-091f-4266-9202-0e5cc54f524f","_uuid":"b2dd548c78c88d9511d519d673e00b8bfe1b03c2","trusted":true},"cell_type":"code","source":"# Classificateur d'arbre de décision utilisant les fonctionnalités du test chi2\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na6 = accuracy_score(y_te,y_pre)\nf6 = f1_score(y_te, y_pre, average=\"macro\")\np6 = precision_score(y_te, y_pre, average=\"macro\")\nr6 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a6)\nprint(\"f1 score : \",f6)\nprint(\"precision score : \",p6)\nprint(\"recall score : \",r6)\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Matrice de confusion - Algorithme d\\'arbre de décision utilisant le test chi2 pour la sélection des fonctionnalités')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6064ac33-5b3f-49de-bd25-4484c73c2ccf","scrolled":true,"_uuid":"929218b8f5d12c215bc6d390b47cb4c15f773d4e","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"792ba618-e433-4889-89e2-9350443cee25","_uuid":"3073b1747d980b062e5df6ac393e964e00df99e4"},"cell_type":"markdown","source":"## Utilisation de f_classif pour la sélection des fonctionnalités"},{"metadata":{"_cell_guid":"9758103e-8cb4-4853-a2ab-1dd4556b5bbc","_uuid":"d7ea238d04a21286a2eb276a0aec44a520b7b451","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_classif\n\ndf_new2 = pd.DataFrame(SelectKBest(f_classif, k=10).fit_transform(df.drop([\"BAD\"],axis=1),df[\"BAD\"]))\ndf_new2.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f69d2634-9d0b-4803-b97c-f327a607d799","_uuid":"6435bfb5d0f10d7ea3260918bd015959a3958a60","trusted":true,"collapsed":true},"cell_type":"code","source":"# Exécution de l'algorithme de régression logistique en utilisant les fonctionnalités sélectionnées dans le test f_classif\nx = df_new2\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = .33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre = logreg.predict(x_te)\na7 = accuracy_score(y_te,y_pre)\nf7 = f1_score(y_te, y_pre, average=\"macro\")\np7 = precision_score(y_te, y_pre, average=\"macro\")\nr7 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a7)\nprint(\"f1 score : \",f7)\nprint(\"precision score : \",p7)\nprint(\"recall score : \",r7)\n\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Tracer une matrice de confusion non normalisée\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\ntitle='Confusion matrix - Logistic Regression Algorithm with f_classif')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2697ec7-8e67-47b7-92c4-5ca589f2a031","_uuid":"1dc5f10361b9853ffa8c7c4da595c6c36361df30","trusted":true},"cell_type":"code","source":"# Decision Tree classifier using features from f_classif test\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na8 = accuracy_score(y_te,y_pre)\nf8 = f1_score(y_te, y_pre, average=\"macro\")\np8 = precision_score(y_te, y_pre, average=\"macro\")\nr8 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a8)\nprint(\"f1 score : \",f8)\nprint(\"precision score : \",p8)\nprint(\"recall score : \",r8)\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Tracer une matrice de confusion non normalisée\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Confusion matrix - Decision Tree Algorithm using f_classif feature selector')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa62c59c-fd07-49d3-96da-57a683557b5d","_uuid":"bbdfbe22b786b4bc55e93d73f4d9d466c7ce8bd5"},"cell_type":"markdown","source":" # Pour visualiser l'arbre de décision créé -"},{"metadata":{"_cell_guid":"6237aef7-1be7-4f1a-bdb7-93b54c9cfe61","scrolled":true,"_uuid":"93e539ac55e58add89d4ac2d28e3d434b4c86dd3","trusted":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz \ndot_dat = tree.export_graphviz(clf_tree, out_file=None) \ngraph = graphviz.Source(dot_dat) \ngraph","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ca2cbab-5e43-4ee2-8097-1f4dae322347","_uuid":"65da3474956c203308946fa3db35712cb2aba9d2"},"cell_type":"markdown","source":"Utilisation de f_regression pour la sélection des fonctionnalités"},{"metadata":{"_cell_guid":"523ffad4-e093-4eff-a40f-79441238f7d1","_uuid":"47aed7ca48dd5f8d87ac9d160f66515d0a144117","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression\n\ndf_new3 = pd.DataFrame(SelectKBest(f_regression, k=10).fit_transform(df.drop([\"BAD\"],axis=1),df[\"BAD\"]))\ndf_new3.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f123f91-b82c-40a2-b28f-c07f523f990b","_uuid":"2aefb2e4c7477d95c51a0c395c0b40b243e49c56","trusted":true,"collapsed":true},"cell_type":"code","source":"# Exécution de l'algorithme de régression logistique en utilisant les fonctionnalités sélectionnées dans le test de f_regression\n\nx = df_new3\ny = df[\"BAD\"]\nx_tr,x_te,y_tr,y_te = train_test_split(x,y,test_size = .33,random_state=1)\nlogreg = LogisticRegression()\nlogreg.fit(x_tr,y_tr)\ny_pre2 = logreg.predict(x_te)\na9 = accuracy_score(y_te,y_pre2)\nf9 = f1_score(y_te, y_pre2, average=\"macro\")\np9 = precision_score(y_te, y_pre2, average=\"macro\")\nr9 = recall_score(y_te, y_pre2, average=\"macro\")\nprint(\"accuracy score : \",a9)\nprint(\"f1 score : \",f9)\nprint(\"precision score : \",p9)\nprint(\"recall score : \",r9)\n\n# Calcul de la matrice de confusion pour l'algorithme ci-dessus\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"],\n                      title='Matrice de confusion - Algorithme de régression logistique avec f_regression')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"572efbf5-0f80-42b2-9d20-5a1cc42dffbb","_uuid":"4bcd6823b795890db20fe43f2cab4958fcd6b22b","trusted":true},"cell_type":"code","source":"# Classificateur d'arbre de décision utilisant les fonctionnalités du test f_regression\n\nclf_tree=DecisionTreeClassifier()\nclf_tree.max_depth = 100\nclf_tree.fit(x_tr,y_tr)\ny_pre = clf_tree.predict(x_te)\na10 = accuracy_score(y_te,y_pre)\nf10 = f1_score(y_te, y_pre, average=\"macro\")\np10= precision_score(y_te, y_pre, average=\"macro\")\nr10 = recall_score(y_te, y_pre, average=\"macro\")\nprint(\"accuracy score : \",a10)\nprint(\"f1 score : \",f10)\nprint(\"precision score : \",p10)\nprint(\"recall score : \",r10)\n\n# Computing Confusion matrix for the above algorithm\n\ncnf_matrix = confusion_matrix(y_te, y_pre)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], \n                      title='Confusion matrix - Decision Tree Algorithm using f_regression feature selector')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c803b868-09e7-444c-b17f-9989ffc276f7","_uuid":"1992f79d5068e067c56d0ce5c85476f653f9cc02"},"cell_type":"markdown","source":"# Comparaison de tous les modèles\n - Nous pouvons maintenant classer notre évaluation de tous les modèles pour choisir le meilleur pour notre problème."},{"metadata":{"collapsed":true,"_cell_guid":"530205f6-8b39-479f-b183-cee433663712","_uuid":"81f495d6791f56287bc451c684c6b99d11e21069","trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree','Logistic Regression', 'Decision Tree'],\n    'Feature Selection Method' : ['None','None','Pearson corr_fact','Pearson corr_fact','chi2 test','chi2 test','f_classif','f_classif','f_regression','f_regression'],\n    'Accuracy Score': [a1,a2,a3,a4,a5,a6,a7,a8,a9,a10],\n    'Recall Score' : [r1,r2,r3,r4,r5,r6,r7,r8,r9,r10],\n    'F1 Score' : [f1,f2,f3,f4,f5,f6,f7,f8,f9,f10],\n    'Precision Score' : [p1,p2,p3,p4,p5,p6,p7,p8,p9,p10]\n})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26da1ec0-0b25-4bae-a81c-0156182cfc72","scrolled":true,"_uuid":"16890129db679d5188d852d3001e8c21c6818dc0","trusted":false,"collapsed":true},"cell_type":"code","source":"models","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6260371-d002-4cf5-8e82-3602df0db678","_uuid":"6ac58d6dc804219e196b0509aa8ce3271b996b77","trusted":false,"collapsed":true},"cell_type":"code","source":"pd.pivot_table(models,index = [\"Feature Selection Method\",\"Model\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33606d7f-f296-4ba2-8f01-1f576245585a","_uuid":"4882655efcec6d4fbcd34190660c63ca2289d677"},"cell_type":"markdown","source":"# Discussion et idées:\n- La régression logistique a produit des résultats avec une bonne précision mais les performances globales ne sont pas très bonnes.\n<br>\n- L'arbre de décision a dominé la régression logistique dans tous les cas.\n<br>\n- Comme mentionné précédemment, les performances de l'arbre de décision sont restées presque les mêmes depuis le début car il sélectionne les fonctionnalités de manière héréditaire.Les performances de la régression logistique se sont également améliorées après le processus de sélection des fonctionnalités.\n<br>\n- Enfin, le modèle d'arbre de décision avec le sélecteur de fonctionnalités f_classf serait la meilleure méthode à utiliser car il a la valeur RECALL la plus élevée\n<br>\n- La profondeur maximale de l'arbre de décision est fixée à 100 dans tous les cas, donc le nombre de niveaux est de 101 dans tous les cas. Et puisque nous n'avons pas fixé le nombre minimum d'observations dans la feuille sera de 1 car il s'agit d'un problème de classification.\n<br>\n- Le seuil est défini par défaut sur 0,5 dans la régression logistique!\n<br>\n- Évidemment, la modification du seuil affecte les performances du modèle et cela peut être observé dans la section suivante.\n<br>\n- Cela peut être encore étendu en rééchantillonnant les données pour augmenter le score RECALL"},{"metadata":{"_cell_guid":"3ed99cb8-2258-4842-9e65-ec16ef95e0ed","_uuid":"ba88c91db0e53a81c627bb5df9c06cd39907d32d"},"cell_type":"markdown","source":"#### Using decision tree with f_classif feature selector would give the best results!!"},{"metadata":{"_cell_guid":"5f7c720c-202b-4247-815b-192b9e17385f","_uuid":"80fb323ead1214bf185a83fbe2075b56a07a8a2a"},"cell_type":"markdown","source":"# Modification du seuil et observation des performances:"},{"metadata":{"_cell_guid":"74f0a685-ac0e-4f56-91d3-3e73c4ccbe3b","_uuid":"ff77083edb731704047607e804a40f53969de468","trusted":false,"collapsed":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(x_tr,y_tr)\ny_pred_proba = lr.predict_proba(x_te)\n\nthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\nplt.figure(figsize=(10,10))\n\nj = 1\nfor i in thresholds:\n    y_test_predictions_high_recall = y_pred_proba[:,1] >= i\n    \n    plt.subplot(3,3,j)\n    j += 1\n    \n    # Compute confusion matrix\n    cnf_matrix = confusion_matrix(y_te,y_test_predictions_high_recall)\n    np.set_printoptions(precision=2)\n    rec1 = recall_score(y_te, y_test_predictions_high_recall)\n    acc= 1.0*(cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0]+cnf_matrix[1,0]+cnf_matrix[1,1]+cnf_matrix[0,1])\n    print(\"Recall metric in the testing dataset: \",rec1)\n    print(\"Accuracy score for the testing dataset: \",acc)\n    # Plot non-normalized confusion matrix\n    class_names = [0,1]\n    plot_confusion_matrix(cnf_matrix\n                          , classes=class_names\n                          , title='Threshold >= %s'%i)\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5bfd2b43-09d3-4343-9404-bb91ec6accf7","_uuid":"dd24805f2287c162416eb760b3eb13c602f3330a"},"cell_type":"markdown","source":"#### Conclusion :\n- Les valeurs de rappel et de précision varient en fonction du seuil sélectionné.\n- Sur la base de la précision requise et des valeurs de rappel, il faut décider et sélectionner un seuil.\n- Il est suggéré de passer au seuil par défaut qui est de 0,5 dans les cas généraux.\n<br>\n-------------------------------------------------- -------------------------------------------------- --------------------------"},{"metadata":{"_cell_guid":"7f3b0c9c-a7ba-4691-b096-8da08ed4307c","_uuid":"bdeea1ee674c643be280462a9e7ddd6b11447faf"},"cell_type":"markdown","source":"# Plus à ce sujet: Utilisation du RESAMPLING pour augmenter la valeur de rappel"},{"metadata":{"_cell_guid":"dff269cf-03d0-47c0-ae5a-419a1d71bf59","_uuid":"3d874c1c796ecfe8f7dc92d0d0194b25f82b85a5"},"cell_type":"markdown","source":"- Comme mentionné précédemment, nous pouvons utiliser le rééchantillonnage pour améliorer les performances des algorithmes d'apprentissage.\n- Dans cette méthode, nous allons diviser les données pour obtenir le rapport de classe cible 1: 1.\n- Il s'agit essentiellement d'une méthode qui traitera les données pour avoir un rapport d'environ 50-50.\n- Il existe 2 processus pour ce faire, sous-échantillonnage et sur-échantillonnage. Ici, nous allons utiliser UNDER-SAMPLING."},{"metadata":{"_cell_guid":"6c367245-0412-4bb2-8f34-525abf88becc","_uuid":"17cbc93260696a515c70daf1517b60a13bbce8f7","trusted":true},"cell_type":"code","source":"# obtenir la longueur et les indices de la classe minoritaire.\ndefault_len = len(df[df[\"BAD\"]==1])\ndefault_indices = np.array(df[df[\"BAD\"]==1].index)\n\n# sélectionner le même nombre d'éléments de la classe majoritaire au hasard.\ngood_indices = np.array(df[df[\"BAD\"]==0].index)\nrand_good_indices = np.random.choice(good_indices, default_len, replace = False)\nrand_good_indices = np.array(rand_good_indices)\n\n# combing the indices\ncombined_indices = np.concatenate([rand_good_indices,default_indices])\n\n# getting the corresponding dataset with above indices.\ncomb_df = df.iloc[combined_indices,:]\ncomb_y = comb_df[\"BAD\"]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9d007658-a9f3-4fa9-8d14-0db32158a52d","_uuid":"59518cf15e80002d77feff37a1db0cdebfed1f79","trusted":true},"cell_type":"code","source":"# en utilisant la méthode de sélection de fonctionnalités f_classif qui a produit de bons résultats dans les cas ci-dessus\n\nfrom sklearn.feature_selection import f_classif\n\ncomb_x = pd.DataFrame(SelectKBest(f_classif, k=10).fit_transform(comb_df.drop([\"BAD\"],axis=1),comb_df[\"BAD\"]))\ncomb_x.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"50b026b3-d3e3-4a7c-9b74-83f8f3e3c98e","_uuid":"d0614b4e5ee55687f489c9201530a2265ca37d82","trusted":false},"cell_type":"code","source":"# fractionnement des données en ensembles de données de train et de test\n\nx_trc,x_tec,y_trc,y_tec = train_test_split(comb_x,comb_y,test_size =.33,random_state=1000)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2d49e907-3a1a-4be0-a0b1-ffba33d8a344","_uuid":"946a7ece9d5086f4aa61a4c5a0910ffefb76198e","trusted":false},"cell_type":"code","source":"# utiliser les scores Kfold pour entraîner les données car très peu de données sont disponibles\n\nfrom sklearn.cross_validation import KFold, cross_val_score\n\nlr = LogisticRegression()\n\ndef printing_Kfold_scores(x_trc,y_trc):\n    fold = KFold(len(y_trc),4,shuffle=False) \n    for train,test in fold :  \n        x1 = x_trc.iloc[train,:]\n        y1 = y_trc.iloc[train]\n        x2 = x_trc.iloc[test,:]\n        y2 = y_trc.iloc[test]\n        lr.fit(x1,y1)\n        y_pred_undersample = lr.predict(x2)\n        recall_acc = recall_score(y2,y_pred_undersample)\n        print(recall_acc)  \n        \nprinting_Kfold_scores(x_trc,y_trc)\n\ny_predr = lr.predict(x_tec)\n\nprint(\"\")\nprint('Accuracy Score = ',accuracy_score(y_tec,y_predr))\nprint('F1 Score = ',f1_score(y_tec, y_predr, average=\"macro\"))\nprint('Precision Score = ',precision_score(y_tec, y_predr, average=\"macro\"))\nprint('Recall Score = ',recall_score(y_tec, y_predr, average=\"macro\"))\nprint(\"\")\ncnf_matrix = confusion_matrix(y_tec, y_predr)\nnp.set_printoptions(precision=2)\n\n# Tracer une matrice de confusion non normalisée\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], \n                      title='Confusion matrix - Logistic Regression Algorithm after Resampling the data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b2127440-bbca-4ff2-b15d-deee272ad9cd","_uuid":"e74e1806c1233a09ae43527d8b49bbbf675e8db0","trusted":true},"cell_type":"code","source":"lr = DecisionTreeClassifier()\n\ndef printing_Kfold_scores(x_trc,y_trc):\n    \n    print(\"# Tracer une matrice de confusion non normalisée\")\n    fold = KFold(len(y_trc),4,shuffle=False) \n    for train,test in fold :  \n        x1 = x_trc.iloc[train,:]\n        y1 = y_trc.iloc[train]\n        x2 = x_trc.iloc[test,:]\n        y2 = y_trc.iloc[test]\n        lr.fit(x1,y1)\n        y_pred_undersample = lr.predict(x2)\n        recall_acc = recall_score(y2,y_pred_undersample)\n        print(recall_acc)\n        \nprinting_Kfold_scores(x_trc,y_trc)\n\ny_predr = lr.predict(x_tec)\nprint(\"\")\nprint('Accuracy Score = ',accuracy_score(y_tec,y_predr))\nprint('F1 Score = ',f1_score(y_tec, y_predr, average=\"macro\"))\nprint('Precision Score = ',precision_score(y_tec, y_predr, average=\"macro\"))\nprint('Recall Score = ',recall_score(y_tec, y_predr, average=\"macro\"))\nprint(\"\")\n\ncnf_matrix = confusion_matrix(y_tec, y_predr)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], \n                      title='Confusion matrix - Decision Tree Algorithm after Resampling the data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c12b1aa-54d5-4ffe-88c8-dbcbb0d07ab4","_uuid":"12f0677d01deb2a67ad3af5198456a9e717b108a"},"cell_type":"markdown","source":"#### Conclusion :\n- Comme vous pouvez le voir, Recall a AUGMENTÉ d'environ 5 à 7% en cas de régression logistique en rééchantillonnant les données. C'est une énorme réussite !!\n<br>\n- En cas d'algorithme d'arbre de décision, le rappel est plus ou moins le même."},{"metadata":{"_cell_guid":"a168f0a6-2a40-406a-9562-091b96c128ab","_uuid":"79aefbf57a099da1a41e8cd87c7303a1117b94fd"},"cell_type":"markdown","source":""},{"metadata":{"collapsed":true,"_cell_guid":"4adb7b4b-cb3b-4119-906c-4505eb623613","_uuid":"da53192508e72b4e320167aad02c7f827dec4547","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}