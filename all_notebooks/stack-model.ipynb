{"cells":[{"metadata":{"_uuid":"87e49640-337c-4365-9c7c-7f317f3122e8","_cell_guid":"1162dae2-7626-44e3-a04b-3729cf06ad33","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures, LabelEncoder, RobustScaler, MinMaxScaler\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.linear_model import RidgeCV, Ridge, LassoCV, Lasso, LinearRegression, BayesianRidge \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n \n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d3ddb0d-3630-418d-95d0-b54c1db61839","_cell_guid":"ec8aac9f-8878-4d80-b1ab-fce59884cc9b","trusted":true},"cell_type":"markdown","source":" <font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">My idea is to keep this notebook as simple as I can, try some classic models and methods and have fun. So, let's start opening the file and delete the columns 'Loud Cover' and 'Apparent Temperature (C)' because they'll not be of much use, specially the Apparent Temperature</font> ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/szeged-weather/weatherHistory.csv')\ndf = df.drop(['Loud Cover', 'Apparent Temperature (C)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Next we are goint to make a simple heatmap with the correlation of the variables.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.zeros_like(df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(15,10))\n\nsns.heatmap(df.corr(), annot=True, mask=mask, linewidths=0.1, square=True, annot_kws={'size':8}, cmap=\"BuGn\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Temperature is correlate with 0.63 with Humidity. Interesting.. tough we're going to use the heatmap to solve the NaN values. But first make usable information of the 'Formatted Date' column</font>","execution_count":null},{"metadata":{"_uuid":"67f19441-0bad-4d18-ae5a-0bc91067ccb1","_cell_guid":"79d6646a-a66c-4887-b83e-fa647487ae42","trusted":true},"cell_type":"code","source":"df['Formatted Date'] = pd.to_datetime(df['Formatted Date'], utc=True)\n\ndf['day']  = df['Formatted Date'].dt.day\ndf['month']  = df['Formatted Date'].dt.month\n\ndf = df.drop('Formatted Date', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Let's take a look at the Precip Type, Summary and the Months</font>","execution_count":null},{"metadata":{"_uuid":"4cc6bae4-a0c4-41f6-949d-48e175374abf","_cell_guid":"f1f1cb97-b3b4-4334-804b-889c5f7447db","trusted":true},"cell_type":"code","source":"f, axes =  plt.subplots(1,2, figsize=(15,5))\n\nsns.boxplot(x='Precip Type', y='Temperature (C)', data=df, palette=\"Set2\", ax=axes[0])\n\nsns.boxplot(x='month', y='Temperature (C)', data=df, palette=\"Set2\", ax=axes[1])\nsns.despine(left=True, bottom=True)\n\nf, axe =  plt.subplots(1,1, figsize=(15,5))\n\nsns.boxplot(x='Summary', y='Temperature (C)', data=df, palette=\"Set2\", ax=axe)\nplt.setp(axe.get_xticklabels(), rotation=45, fontsize=7)\nsns.despine(left=True, bottom=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Rain and snow shows what we expected; the lower the temperature the more snow and rain. Months gives us the chance to watch the change in the seasons and the average temperatures of each one. Summary doesn't give us importante information and will make it difficult to encode, so let's drop it.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('Summary', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Let's check the skewness of the Temperature our future variable y</font>","execution_count":null},{"metadata":{"_uuid":"5ec8c9f9-6dc6-4375-8d7d-cba8ac02ae55","_cell_guid":"1f6a09f6-587b-4a2f-ad90-82e1a9583684","trusted":true},"cell_type":"code","source":"sns.distplot(df['Temperature (C)'], fit=norm)\n(mu, sigma) = norm.fit(df['Temperature (C)'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Temperature (C)')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df['Temperature (C)'], plot=plt)\nplt.show()\n\nprint(skew(df['Temperature (C)']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">No skew and pretty good for the models.</font>","execution_count":null},{"metadata":{"_uuid":"0c1c317f-f66a-4d1a-8a17-0db96c1bd9e0","_cell_guid":"f90f9118-e2e5-460d-835a-a6f6ba27bb5c","trusted":true},"cell_type":"code","source":"f, axes =  plt.subplots(1,2, figsize=(15,5))\n\nsns.regplot(x='Humidity', y='Temperature (C)', data=df, ax=axes[0], color='g')\nsns.regplot(x='Visibility (km)', y='Temperature (C)', data=df, ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Some outliders in the data, get rid of them.</font>","execution_count":null},{"metadata":{"_uuid":"743df57a-8954-4480-b87b-1580fcee25c7","_cell_guid":"db0c8cec-7ea7-4082-ad8d-c4a64613711d","trusted":true},"cell_type":"code","source":"df['Humidity'] = df['Humidity'].loc[(df['Humidity']>0.0) & (df['Temperature (C)']) > 0]\n\nsns.regplot(x='Humidity', y='Temperature (C)', data=df, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">And now what I promised, the NaN values!</font>","execution_count":null},{"metadata":{"_uuid":"fba0a116-58bf-4cef-972e-3604a718014f","_cell_guid":"8405442e-a783-409e-947c-eb25a3aeff85","trusted":true},"cell_type":"code","source":"df_nan = (df.isna().sum() / len(df)) * 100\ndf_nan = df_nan.drop(df_nan[df_nan == 0].index).sort_values(ascending=False)[:5]\ndf_nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">As I said in the heatmap we are going to use the correlation between Visibility-Humidity, and Visibility-Precip Type to fill the NaN values with the median and mode, I think this's pretty logic; days with less visibility means that the probability of rain or snow is high.</font>","execution_count":null},{"metadata":{"_uuid":"f9ff9f97-8daf-40a1-9c92-97474d8edbdb","_cell_guid":"4bcac84b-2225-488c-8e66-a80dcec78daf","trusted":true},"cell_type":"code","source":"df['Humidity'] = df.groupby('Visibility (km)')['Humidity'].transform(lambda x: x.fillna(x.median()))\ndf['Precip Type'] = df.groupby('Visibility (km)')['Precip Type'].transform(lambda x: x.fillna(x.mode()[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ce9abf7-d165-407f-859a-babb01db9002","_cell_guid":"6f1383c3-6141-47c4-9515-11fbddd8518e","trusted":true},"cell_type":"code","source":"df_nan = (df.isna().sum() / len(df)) * 100\ndf_nan = df_nan.drop(df_nan[df_nan == 0].index).sort_values(ascending=False)[:5]\ndf_nan","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34ea07e6-a8a3-419c-b0ed-f9973bf20032","_cell_guid":"f59500ed-a91f-46e7-a33f-be8723b10a8f","trusted":true},"cell_type":"code","source":"display(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Keep it easy for this case and use LabelEncoder fot the categorical variables</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [col for col in df.columns if df[col].dtypes == 'object']\nprint(categorical_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6a1965a-d492-4266-b6fb-fabbaaffec8f","_cell_guid":"679b54c7-9fe3-4d41-bf96-675afbcaafe9","trusted":true},"cell_type":"code","source":"label_enc = LabelEncoder()\n\nfor cols in categorical_cols:\n    df[cols] = label_enc.fit_transform(df[cols])\n\n\ndisplay(df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Start with the model!</font>","execution_count":null},{"metadata":{"_uuid":"239f6c24-d9ca-4e8c-8421-8ab75ae94db9","_cell_guid":"2eeefe5e-f866-465a-afe4-120af668938d","trusted":true},"cell_type":"code","source":"y = df['Temperature (C)'].values.reshape(-1, 1)\nX = df.drop('Temperature (C)', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">I going to create a funtion to score the mean squared error and a data frame to save the values</font>","execution_count":null},{"metadata":{"_uuid":"1750de81-fa2e-4100-a6cb-3380187206f4","_cell_guid":"db91295d-b7f9-4021-8b10-f8e379efef9e","trusted":true},"cell_type":"code","source":"n_folds = 5\n\ndef rmse_cv(model):\n    kf = KFold(n_folds, shuffle=True).get_n_splits(X)\n    rmse= np.sqrt(-cross_val_score(model, X, y.ravel(), scoring=\"neg_mean_squared_error\", cv = kf))\n    return rmse\n\n\n\nscores = pd.DataFrame({}, columns=[ 'Model', 'Score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">I said \"keep it easy\", I will do it also with the models. Let's try a Linear Regression.</font>","execution_count":null},{"metadata":{"_uuid":"e4c246b8-2819-4b0e-986c-a8c8ffb6c32f","_cell_guid":"580f43ee-fbd9-4835-a347-be693c721c8c","trusted":true},"cell_type":"code","source":"\nlr = make_pipeline(MinMaxScaler(), LinearRegression())\n\nlr_sc = rmse_cv(lr)\n\nprint('RMSE in Linear Regression:', lr_sc.mean())\n\nr = scores.shape[0] + 1 \n\nscores.loc[r] = ['Linear Regressor', lr_sc.mean()]\n\n#The score is king of rare and you can thiks it's too high for these values, but no problem it works just fine.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Try a Polynomial change of degree 3</font>","execution_count":null},{"metadata":{"_uuid":"e7e49cd5-fedc-42d6-ae1b-785d3adeaef6","_cell_guid":"3af0a203-0d51-4d2d-ae2b-46e2eabdd86d","trusted":true},"cell_type":"code","source":"\nlr_poly_3 = make_pipeline(MinMaxScaler(), PolynomialFeatures(3), LinearRegression())\n\nlr_poly_3_scor = rmse_cv(lr_poly_3)\n\n\nprint('RMSE in Linear Regression Poly 3:', lr_poly_3_scor.mean())\n\n\nr = scores.shape[0] + 1 \n\nscores.loc[r] = ['Linear Regressor Poly = 3', lr_poly_3_scor.mean()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This part is not so easy when it comes to decide the alpha for regularization models as Rige and Lasso.**","execution_count":null},{"metadata":{"_uuid":"bf8003a4-4ed1-46b2-a155-d94709f7959f","_cell_guid":"afbf48a6-f867-4a41-beaf-211fe3a2134f","trusted":true},"cell_type":"code","source":"#split the data outside the rmse_cv funtion to find the best alpha.\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n#make a Robust Scaler that fit's better for outliders\nscaler = RobustScaler()\n\n\n#make the change\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n#try some alphas with Ridge Cross Validation\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60], store_cv_values=True,\n               cv=None).fit(X_train, y_train)\n\n\n#Create a score of alpha and the alpah itself\nscore = ridge.score(X_train, y_train)\nalpha = ridge.alpha_\n\n\nprint('Best alpha: ', alpha)\n\nprint('Try again for more precision centered in: ', alpha)\n\n#Try with some changes the best alpha and check if improves\nridge_2 = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85,\n                          alpha * 0.9, alpha * .95, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.2,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], cv=None,\n                          store_cv_values=True).fit(X_train, y_train)\n\n\n\nscore_2 = ridge_2.score(X_train, y_train)\nalpha_2 = ridge_2.alpha_\n\n\nbest_alpha = alpha\n\nif score_2 > score:\n    best_alpha = alpha_2\n    \n    \n\nprint('Best alpha: ', best_alpha)\n\n\n#Finally the model!\nridge_f = make_pipeline(RobustScaler(), Ridge(alpha=best_alpha))\n \nridge_scor = rmse_cv(ridge_f)\n\n\nprint('Ridge RMSE: ', ridge_scor.mean())\n\n\nr = scores.shape[0] + 1 \n\nscores.loc[r] = ['Ridge', ridge_scor.mean()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All the same with Lasso Model. My idea is to explore regression models with regulatization and without it, finally try a Ramdom Forest Regressor.**","execution_count":null},{"metadata":{"_uuid":"535cfd1b-37e7-412f-91fc-26d4661e05e7","_cell_guid":"58bae0ff-c476-4da0-aa39-1cfb116e6aa2","trusted":true},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.001, 0.01, 0.03,0.06, 1], \n                max_iter = 50000, cv = 10).fit(X_train, y_train.ravel())\n\n\n\nalpha = lasso.alpha_\n\n\nprint('Best alpha: ', alpha)\nprint('Try again for more precision centered in: ', alpha)\n\nlasso_2 = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85,\n                       alpha * 0.9, alpha * .95, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.2,\n                        alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],  \n                  max_iter = 50000, cv = 10).fit(X_train, y_train.ravel())\n\n\nalpha_2 = lasso_2.alpha_\n\nbest_alpha = alpha\n\nif alpha_2 > alpha:\n    best_alpha = alpha_2\n    \n\n\n\n\nprint('Best alpha: ', best_alpha)\n\nlasso_f = make_pipeline(RobustScaler(), Lasso(alpha=best_alpha))\n\nlasso_scor = rmse_cv(lasso_f)\n\n\n\nprint('Lasso RMSE: ', lasso_scor.mean())\n\nr = scores.shape[0] + 1 \n\nscores.loc[r] = ['Lasso', lasso_scor.mean()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Try a basic Bayes just to check what happens.**","execution_count":null},{"metadata":{"_uuid":"e95eb0e2-7bc9-414b-83d8-e2d1991ff67e","_cell_guid":"73637a3c-9116-41cc-9690-238c40794ddf","trusted":true},"cell_type":"code","source":"bayes = make_pipeline(RobustScaler(), BayesianRidge())\n\nbayes_scor = rmse_cv(bayes)\n\nr = scores.shape[0] + 1 \n\n\nprint('Bayes RMSE: ', bayes_scor.mean())\nscores.loc[r] = ['reg', bayes_scor.mean()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Ajam! The classic Random Forest Regressor, the cheater of models jaja.</font>","execution_count":null},{"metadata":{"_uuid":"42b440be-fcec-43b5-9fdb-309b071c8d5d","_cell_guid":"fe995be6-a22c-4287-bca4-f27ade26b1cf","trusted":true},"cell_type":"code","source":"RFR = make_pipeline(MinMaxScaler(), RandomForestRegressor(n_estimators=350))\n\nrfr_scor = rmse_cv(RFR)\n\n\nprint('Random Forest Regressor: ', rfr_scor.mean())\n\n#As I expected the score is better!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = scores.shape[0] + 1 \nscores.loc[r] = ['Random forest reggresor', rfr_scor.mean()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c07b0af0-a0b2-486b-b6a6-9f1be612cdcb","_cell_guid":"7237819f-63ef-4540-b455-63e652281393","trusted":true},"cell_type":"code","source":"#Split the data.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">It will be boring if I just leave all the fun to a ensemble model. Don't forget about the regression models, they're our friends. So, in order to add fun, I will create a Stacked model, so make the Class and apply it!</font>","execution_count":null},{"metadata":{"_uuid":"efd8295f-4b05-4c10-a93a-ff558eaeaaf4","_cell_guid":"d97b1df0-7005-46ab-892e-c8c91719848a","trusted":true},"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">I will use the Random Forest and the Polyfeature of degree 3</font>","execution_count":null},{"metadata":{"_uuid":"e7fde8ad-7aad-4f31-8191-615494eed7d9","_cell_guid":"02546e12-f7b3-4ce5-afe5-357414627ae2","trusted":true},"cell_type":"code","source":"averaged_models = AveragingModels(models=(RFR, lr_poly_3))\n\naveraged_model_scor = rmse_cv(averaged_models)\n\nr = scores.shape[0] + 1 \n\nscores.loc[r] = ['Stacked RFR-LR', averaged_model_scor.mean()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Let's check all the models</font>","execution_count":null},{"metadata":{"_uuid":"da259b5d-09fd-4ba9-b096-56cec0e461c0","_cell_guid":"19847fb1-88de-4e3e-b3b6-92a4980d90a6","trusted":true},"cell_type":"code","source":"display(scores.sort_values(by='Score'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">Finally watch the predictions against the real values in the train and test sets.</font>","execution_count":null},{"metadata":{"_uuid":"72aa8af4-3899-4946-8ed1-12151c30b6a9","_cell_guid":"b992293c-0171-45d7-8a38-b4b9d47a6d62","trusted":true},"cell_type":"code","source":"averaged_models.fit(X_train, y_train.ravel())\n\npredictions = averaged_models.predict(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_sq_error = np.sqrt(mean_squared_error(predictions, y_test))\n\n\nprint(mean_sq_error)\nprint(predictions[:10].round(0))\nprint(y_test[:10].round(0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font style=\"color:slateblue; font-size:20px; font:Andale Mono;\">+/- 2 degrees is a good result for a quick model. Please comment for ideas!</font>\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}