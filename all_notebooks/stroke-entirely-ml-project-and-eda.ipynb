{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis and Machine Learning Classification on Stroke Prediction"},{"metadata":{},"cell_type":"markdown","source":"\nIn this notebook, I performed an EDA on the 'Stroke Prediction Dataset'. I visualized the data using the Seaborn and Plotly libraries. My comments on the graphics and results are below the code snippet.\nI applied Machine Learning algorithms to this dataset. I evaluated models based on their accuracy. In addition, I applied k-Fold Cross Validation and HyperParameter Optimization. Lastly, I determined best features for some algorithms. I hope this notebook will be useful to you."},{"metadata":{},"cell_type":"markdown","source":"## **I have been denied access to my account. That's why I'm sharing it again.**"},{"metadata":{},"cell_type":"markdown","source":"### If you have questions please ask them on the comment section.\n\n### I will be glad if you can give feedback."},{"metadata":{},"cell_type":"markdown","source":"Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n   1. [Univariate Variable Analysis](#4)\n      1. [Categorical Variables](#5)\n      1. [Numerical Variables](#6)\n1. [Basic Data Analysis](#7)\n   1. [Gender](#8)\n   1. [Ever Married](#9)\n   1. [Work Type](#10)\n   1. [Residence Type](#11)\n   1. [Smoking Status](#12)\n1. [Pandas Profiling](#13)\n1. [Correlation](#14)\n1. [Anomaly Detection](#15)\n1. [Missing Values](#16)\n   1. [bmi](#17)\n1. [Encoding](#18)\n   1. [Label Encoding](#19)\n   1. [One-Hot Encoding](#20)\n1. [Train-Test Split](#21)\n1. [Scores of Models](#22)\n1. [Evaluation of Models](#23)\n   1. [k-Fold Cross Validation](#24)\n   1. [Hyper-Parameter Optimization](#25)\n      1. [GridSearchCV](#26)\n      1. [RandomizedSearchCV](#27)\n   1. [Best Features Selection](#28)\n1. [Conclusion](#29)      "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> \n# Importing the Necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> \n# Read Datas & Explanation of Features & Information About Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pandas.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndataset.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I dropped 'id' column because it can cause unwanted correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(\"id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> \n## Variable Descriptions"},{"metadata":{},"cell_type":"markdown","source":"* id: unique identifier\n* gender: \"Male\", \"Female\" or \"Other\"\n* age: age of the patient\n* hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n* heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n* ever_married: \"No\" or \"Yes\"\n* work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n* Residence_type: \"Rural\" or \"Urban\"\n* avg_glucose_level: average glucose level in blood\n* bmi: body mass index\n* smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n* stroke: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\nSource: https://www.kaggle.com/fedesoriano/stroke-prediction-dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have null variables in 'bmi' column. We will handle them after."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> \n## Univariate Variable Analysis"},{"metadata":{},"cell_type":"markdown","source":"*** Categorical Variables:** 'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n\n*** Numerical Variables:** 'id', 'hypertension', 'heart_disease', 'stroke'"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> \n### Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\ncategorical_variables = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\nfor c in categorical_variables:\n    bar_plot(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.set_theme(style=\"darkgrid\")\nplt.subplot(2,3,1)\nsns.violinplot(x = 'ever_married', y = 'stroke', data = dataset)\nplt.subplot(2,3,2)\nsns.violinplot(x = 'ever_married', y = 'stroke', data = dataset)\nplt.subplot(2,3,3)\nsns.violinplot(x = 'ever_married', y = 'stroke', data = dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> \n### Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_int64 = (dataset.dtypes == \"int64\")\nnumerical_int64_list = list(numerical_int64[numerical_int64].index)\n\nprint(\"Categorical variables:\")\nprint(numerical_int64_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_variables = ['hypertension', 'heart_disease', 'stroke']\nfor n in numerical_variables:\n    plot_hist(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Histogram\".format(variable))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_variables = ['age', 'avg_glucose_level', 'bmi']\nfor n in numerical_variables:\n    plot_hist(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.histplot(dataset['age'], color = 'red', kde = True).set_title('Age Interval and Counts')\n\nplt.subplot(2,3,2)\nsns.histplot(dataset['bmi'], color = 'green', kde = True).set_title('BMI Interval and Counts')\n\nplt.subplot(2,3,3)\nsns.histplot(dataset['avg_glucose_level'], kde = True, color = 'blue').set_title('avg_glucose_level Interval and Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.histplot(dataset['hypertension'], color = 'red', kde = True).set_title('hypertension Interval and Counts')\n\nplt.subplot(2,3,2)\nsns.histplot(dataset['heart_disease'], color = 'green', kde = True).set_title('heart_disease Interval and Counts')\n\nplt.subplot(2,3,3)\nsns.histplot(dataset['stroke'], kde = True, color = 'blue').set_title('Stroke Interval and Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,3,1)\nsns.boxenplot(x=dataset['stroke'], y=dataset['age'],\n              color=\"b\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,2)\nsns.boxenplot(x=dataset['stroke'], y=dataset['bmi'],\n              color=\"b\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,3)\nsns.boxenplot(x=dataset['stroke'], y=dataset['avg_glucose_level'],\n              color=\"b\", \n              scale=\"linear\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,2,1)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"gender\", data=dataset, palette=\"PRGn\")\n\nplt.subplot(2,2,2)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"ever_married\", data=dataset, palette=\"PRGn\")\n\nplt.subplot(2,2,3)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"smoking_status\", data=dataset, palette=\"PRGn\")\n\nplt.subplot(2,2,4)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"work_type\", data=dataset, palette=\"PRGn\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> \n# Basic Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> \n## Gender - Stroke"},{"metadata":{},"cell_type":"markdown","source":"Average stroke rate by gender. Male's stroke rate is higher."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"gender\",\"stroke\"]].groupby([\"gender\"], as_index = False).mean().sort_values(by=\"stroke\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset\n\nlabels = dataset['gender'].value_counts().index\npie1 = dataset['gender'].value_counts().values\n# figure\nfig = {\n  \"data\": [\n    {\n      \"values\": pie1,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"\",\n      \"hoverinfo\":\"label+percent+name+value\",\n      \"hole\": .2,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Distribution of Genders\",\n        \"annotations\": [\n            { \"font\": { \"size\": 25},\n              \"showarrow\": True,\n              \"text\": \"Genders\",\n                \"x\": 1,\n                \"y\": 1,\n            },\n        ]\n    }\n}\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> \n## Ever Married - Stroke"},{"metadata":{},"cell_type":"markdown","source":"stroke rate based on married or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"ever_married\",\"stroke\"]].groupby([\"ever_married\"], as_index = False).mean().sort_values(by=\"stroke\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['ever_married'].value_counts().index\nsizes = dataset['ever_married'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'ever_married'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> \n## Work Type - Stroke"},{"metadata":{},"cell_type":"markdown","source":"stroke rate based on work_type"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"work_type\",\"stroke\"]].groupby([\"work_type\"], as_index = False).mean().sort_values(by=\"stroke\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"work_type\",\"stroke\"]].groupby([\"work_type\"], as_index = False).count().sort_values(by=\"stroke\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset\n\nlabels = dataset['work_type'].value_counts().index\npie1 = dataset['work_type'].value_counts().values\n# figure\nfig = {\n  \"data\": [\n    {\n      \"values\": pie1,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"\",\n      \"hoverinfo\":\"label+percent+name+value\",\n      \"hole\": .2,\n      \"type\": \"pie\",\n    },],\n  \"layout\": {\n        \"title\":\"Distribution of Work Type\",\n        \"annotations\": [\n            { \"font\": { \"size\": 25},\n              \"showarrow\": True,\n              \"text\": \"Work Type\",\n                \"x\": 1,\n                \"y\": 1,\n             \n            },\n        ]\n    }\n}\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a> \n## Residence_type - Stroke"},{"metadata":{},"cell_type":"markdown","source":"stroke rate based on residence type."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"Residence_type\",\"stroke\"]].groupby([\"Residence_type\"], as_index = False).mean().sort_values(by=\"stroke\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = dataset['Residence_type'].value_counts()\n\nplt.figure(figsize=(10,7))\nsns.barplot(x=counts.index, y=counts.values, palette=\"Set3\")\n\nplt.ylabel('Count')\nplt.xlabel('Residence_type', style = 'normal', size = 24)\n\nplt.xticks(rotation = 45, size = 12)\nplt.yticks(rotation = 45, size = 12)\n\nplt.title('Distribution of Residence_type',color = 'black',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> \n## smoking_status - Stroke"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"smoking_status\",\"stroke\"]].groupby([\"smoking_status\"], as_index = False).mean().sort_values(by=\"stroke\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset\n\nlabels = dataset['smoking_status'].value_counts().index\npie1 = dataset['smoking_status'].value_counts().values\n# figure\nfig = {\n  \"data\": [\n    {\n      \"values\": pie1,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"\",\n      \"hoverinfo\":\"label+percent+name+value\",\n      \"hole\": .2,\n      \"type\": \"pie\",\n    },],\n  \"layout\": {\n        \"title\":\"Distribution of Smoking Status\",\n        \"annotations\": [\n            { \"font\": { \"size\": 25},\n              \"showarrow\": True,\n              \"text\": \"Smoking Status\",\n                \"x\": 1,\n                \"y\": 1,\n             \n            },\n        ]\n    }\n}\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a> \n# Pandas Profiling"},{"metadata":{},"cell_type":"markdown","source":"Pandas profiling is a useful library that generates interactive reports about the data. With using this library, we can see types of data, distribution of data and various statistical information. This tool has many features for data preparing. Pandas Profiling includes graphics about specific feature and correlation maps too. You can see more details about this tool in the following url: https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a> \n# Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset, hue = 'stroke')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a> \n# Anomaly Detection"},{"metadata":{},"cell_type":"markdown","source":"Anomaly is one that differs / deviates significantly from other observations in the same sample. An anomaly detection pattern produces two different results. The first is a categorical tag for whether the observation is abnormal or not; the second is a score or trust value. Score carries more information than the label. Because it also tells us how abnormal the observation is. The tag just tells you if it's abnormal. While labeling is more common in supervised methods, the score is more common in unsupervised and semisupervised methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This code is retrieved from here: https://www.kaggle.com/kanncaa1/dataiteam-titanic-eda#Introduction\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.loc[detect_outliers(dataset,['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease', 'stroke'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop outliers\ndataset = dataset.drop(detect_outliers(dataset,['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease', 'stroke']),axis = 0).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a> \n# Missing Values"},{"metadata":{},"cell_type":"markdown","source":"We have 201 null values in total. bmi includes all. (After Anomaly Detection, it decreases to 192)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a> \n## bmi"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[dataset['bmi'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***How can we handle null values?***\n\n* I think the most obvious differences for BMI will be between gender."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.boxplot(column=\"bmi\",by = \"gender\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get different BMI averages for women and men, although not very large. I will assign the total BMI mean as there are very few examples for the Others gender."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean of BMI value for Females: \", np.mean(dataset[dataset['gender'] == 'Female']['bmi']))\nprint(\"Mean of BMI value for Males: \", np.mean(dataset[dataset['gender'] == 'Male']['bmi']))\nprint(\"Mean of BMI value: \", np.mean(dataset['bmi']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I filled null values with 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['bmi'] = dataset['bmi'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This loop assign these values to zero values accoring to the gender."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,5035):\n    if(dataset['bmi'][i] == 0):\n        if(dataset['gender'][i] == 'Male'):\n            dataset['bmi'][i] = 28.594683544303823\n        elif(dataset['gender'][i] == 'Female'):\n            dataset['bmi'][i] = 29.035926055109936\n        else:\n            dataset['bmi'][i] = 28.854652338161664\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[dataset['bmi'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a> \n# Encoding"},{"metadata":{},"cell_type":"markdown","source":"I will handle Categorical Values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique Values for Gender\", dataset['gender'].unique())\nprint(\"Unique Values for ever_married\", dataset['ever_married'].unique())\nprint(\"Unique Values for work_type\", dataset['work_type'].unique())\nprint(\"Unique Values for Residence_type\", dataset['Residence_type'].unique())\nprint(\"Unique Values for smoking_status\", dataset['smoking_status'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"19\"></a> \n## Label Encoding"},{"metadata":{},"cell_type":"markdown","source":"Label Encoding is an encoding technique for handling categorical variables. In this technique, each data is assigned a unique integer."},{"metadata":{"trusted":true},"cell_type":"code","source":"ever_married_mapping = {'No': 0, 'Yes': 1}\ndataset['ever_married'] = dataset['ever_married'].map(ever_married_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Residence_type_mapping = {'Rural': 0, 'Urban': 1}\ndataset['Residence_type'] = dataset['Residence_type'].map(Residence_type_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"20\"></a> \n## One-Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"One Hot Encoding is the binary representation of categorical variables. This process requires categorical values to be mapped to integer values first. Next, each integer value is represented as a binary vector with all values zero except the integer index marked with 1.\n\nOne Hot Encoding makes the representation of categorical data more expressive and easy. Many machine learning algorithms cannot work directly with categorical data, so categories must be converted to numbers. This operation is required for input and output variables that are categorical.\n\nIn this part, I converted categorical datas to the binary values. This operation increases the accuracy.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"onehotencoder = OneHotEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['gender'] = pd.Categorical(dataset['gender'])\ndatasetDummies_gender = pd.get_dummies(dataset['gender'], prefix = 'gender_encoded')\ndatasetDummies_gender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['work_type'] = pd.Categorical(dataset['work_type'])\ndatasetDummies_work_type = pd.get_dummies(dataset['work_type'], prefix = 'work_type_encoded')\ndatasetDummies_work_type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['smoking_status'] = pd.Categorical(dataset['smoking_status'])\ndatasetDummies_smoking_status = pd.get_dummies(dataset['smoking_status'], prefix = 'smoking_status_encoded')\ndatasetDummies_smoking_status","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(\"gender\", axis=1, inplace=True)\ndataset.drop(\"work_type\", axis=1, inplace=True)\ndataset.drop(\"smoking_status\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.concat([dataset, datasetDummies_gender], axis=1)\ndataset = pd.concat([dataset, datasetDummies_work_type], axis=1)\ndataset = pd.concat([dataset, datasetDummies_smoking_status], axis=1)\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we don't have categorical variables. Dataset is ready for Machine Leraning algorithms."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"21\"></a> \n# Train - Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['age',\n 'hypertension',\n 'heart_disease',\n 'ever_married',\n 'Residence_type',\n 'avg_glucose_level',\n 'bmi',\n 'gender_encoded_Female',\n 'gender_encoded_Male',\n 'gender_encoded_Other',\n 'work_type_encoded_Govt_job',\n 'work_type_encoded_Never_worked',\n 'work_type_encoded_Private',\n 'work_type_encoded_Self-employed',\n 'work_type_encoded_children',\n 'smoking_status_encoded_Unknown',\n 'smoking_status_encoded_formerly smoked',\n 'smoking_status_encoded_never smoked',\n 'smoking_status_encoded_smokes']\n\nlabel = ['stroke']\n\nX = dataset[features]\ny = dataset[label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardization is a method in which the mean value is 0 and the standard deviation is 1, and the distribution approaches the normal. The formula is as follows, we subtract the average value from the value we have, then divide it by the variance value."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"22\"></a> \n# Scores of Models"},{"metadata":{},"cell_type":"markdown","source":"These are the ML algorithms that will apply to dataset. Results will contain train-validation-test scores, confusion matrix, statistical information and classification reports for each algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n    'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  score = model.score(X_valid, y_valid)\n  #print(f'{m} validation score => {score*100}')\n    \n  print(f'{m}') \n  train_score = model.score(X_train, y_train)\n  print(f'Train score of trained model: {train_score*100}')\n  trainScores.append(train_score*100)\n\n  validation_score = model.score(X_valid, y_valid)\n  print(f'Validation score of trained model: {validation_score*100}')\n  validationScores.append(validation_score*100)\n\n  test_score = model.score(X_test, y_test)\n  print(f'Test score of trained model: {test_score*100}')\n  testScores.append(test_score*100)\n  print(\" \")\n    \n  y_predictions = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_predictions, y_test)\n\n  print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n  predictions = model.predict(X_test)\n  cm = confusion_matrix(predictions, y_test)\n\n  tn = conf_matrix[0,0]\n  fp = conf_matrix[0,1]\n  tp = conf_matrix[1,1]\n  fn = conf_matrix[1,0]\n  accuracy  = (tp + tn) / (tp + fp + tn + fn)\n  precision = tp / (tp + fp)\n  recall    = tp / (tp + fn)\n  f1score  = 2 * precision * recall / (precision + recall)\n  specificity = tn / (tn + fp)\n  print(f'Accuracy : {accuracy}')\n  print(f'Precision: {precision}')\n  print(f'Recall   : {recall}')\n  print(f'F1 score : {f1score}')\n  print(f'Specificity : {specificity}')\n  print(\"\") \n  print(f'Classification Report: \\n{classification_report(predictions, y_test)}\\n')\n  print(\"\")\n   \n  for m in range (1):\n    current = modelNames[m]\n    modelNames.remove(modelNames[m])\n\n  preds = model.predict(X_test)\n  confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\n  print(\"############################################################################\")\n  print(\"\")\n  print(\"\")\n  print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    print(f'Accuracy of {modelNames[i]} -----> {testScores[i]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"23\"></a> \n# Evaluation of Models"},{"metadata":{},"cell_type":"markdown","source":"I evaluated these models according to their accuracies. Best algorithm is KNN with 96.02%. So, I will make k-Fold Cross Validation and Hyper-Parameter Optimization for KNN algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNeighborsClassifier()\nknn_model.fit(X_train, y_train)\n\ntrain_score = knn_model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score}')\n\nvalidation_score = knn_model.score(X_valid, y_valid)\nprint(f'Validation score of trained model: {validation_score}')\n\ntest_score = knn_model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score}')\n\ny_predictions = knn_model.predict(X_test)\n\nconf_matrix = confusion_matrix(y_predictions, y_test)\n\n\nprint(f'Accuracy: {accuracy_score(y_predictions, y_test)*100}')\nprint()\nprint(f'Confussion matrix: \\n{conf_matrix}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nimport numpy as np\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmesh_size = .02\nmargin = 0.25\n\n# Load and split data\nX, y = make_moons(noise=0.3, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y.astype(str), test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\n# Create a mesh grid on which we will run our model\nx_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin\ny_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin\nxrange = np.arange(x_min, x_max, mesh_size)\nyrange = np.arange(y_min, y_max, mesh_size)\nxx, yy = np.meshgrid(xrange, yrange)\n\n# Create classifier, run predictions on grid\nclf = knn_model\nclf.fit(X, y)\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\n\n# Plot the figure\nfig = go.Figure(data=[\n    go.Contour(\n        x=xrange,\n        y=yrange,\n        z=Z,\n        colorscale='purp'\n    )\n])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"24\"></a> \n## k-Fold Cross Validation"},{"metadata":{},"cell_type":"markdown","source":"Cross Validation will enable us to see whether we are facing an overfitting problem and also to see the quality of our model. Thus, it will enable us to test the performance of our model before encountering high error rates in the test data set that we have not seen yet. It is a method that is frequently used because it is easy to apply."},{"metadata":{},"cell_type":"markdown","source":"***cv = 10 means k = 10 for KNN.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cross_val_score(knn_model, X = X_train, y = y_train, cv = 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = cross_val_score(knn_model, X = X_train, y = y_train, cv = 10)\nplt.title(\"Cross Validation Scores\")\nplt.xlabel(\"cv \")\nplt.ylabel(\"Scores\")\nplt.plot(g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = cross_val_score(estimator = knn_model, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy (mean):\", accuracies.mean()*100, \"%\")\nprint(\"std: \", accuracies.std()*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as: \n\nSource: https://en.wikipedia.org/wiki/Mean_absolute_error"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(knn_model, X_train, y_train, scoring = 'neg_mean_absolute_error', cv = 10)\nprint (\"MAE:\", scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"25\"></a> \n## Hyper-Parameter Optimization"},{"metadata":{},"cell_type":"markdown","source":"Unlike parameters, hyperparameters are not learned during training the model. They are determined by the data scientist before the modeling phase. For example, KNN algorithm, which is one of the non-parametric classification algorithms, makes classification by looking at the nearest k neighbors to the desired value. Here, the k number (n_neighbors:) and the distance metric (metric:) to be used are the hyperparameters that should be specified by the data scientist before the modeling, which increases the performance of the model.\n\n**Hyperparameter optimization** is the process of finding the most suitable hyperparameter combination according to the success metric specified for a machine learning algorithm.\n\nGiven that there are dozens of hyperparameters for a machine learning algorithm and dozens of values these hyperparameters can take, it's clear how difficult it will be to try all combinations one by one and pick the best combination. For this reason, different methods have been developed for hyperparameter optimization. GridSearcCV and RandomizedSearchCV are among these methods."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"26\"></a> \n### GridSearchCV"},{"metadata":{},"cell_type":"markdown","source":"For the hyperparameters and their values that are desired to be tested in the model, a separate model is established with all combinations and the most successful hyperparameter set is determined according to the specified metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'n_neighbors': [2,3,5,7,9,11,15,20],\n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n             'weights': ['uniform', 'distance'],\n             'metric': ['manhattan', 'euclidean', 'minkowski', 'cosine', 'jaccard', 'hamming']\n             }\n\ngcv = GridSearchCV(knn_model, parameters, cv=5, verbose = 1, n_jobs = -1).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'GridSearchView Best Score: {gcv.best_score_*100}')\nprint(f'GridSearchView Best Estimator: {gcv.best_estimator_}')\nprint(f'GridSearchView Best Params: {gcv.best_params_}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"27\"></a> \n### RandomizedSearchCV"},{"metadata":{},"cell_type":"markdown","source":"A set of hyperparameters is randomly selected and tested by cross-validation and the model set up. These steps continue until the specified calculation time limit or the number of iterations is reached."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nparams = {'n_neighbors': [2,3,5,7,9,11,15,20],\n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n             'weights': ['uniform', 'distance'],\n             'metric': ['manhattan', 'euclidean', 'minkowski', 'cosine', 'jaccard', 'hamming']\n             }\n\nrandomizedcv = RandomizedSearchCV(knn_model, params, n_iter=200, cv=5, scoring='accuracy', n_jobs=-1, verbose=2).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'RandomizedSearchCV Best Score: {randomizedcv.best_score_*100}')\nprint(f'RandomizedSearchCV Best Estimator: {randomizedcv.best_estimator_}')\nprint(f'RandomizedSearchCV Best Params: {randomizedcv.best_params_}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"28\"></a> \n## Best Features Selection"},{"metadata":{},"cell_type":"markdown","source":"Feature Importance and Best Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I wrote this part again because X y changed above, in the k-nn visualization. \nfeatures = ['age',\n 'hypertension',\n 'heart_disease',\n 'ever_married',\n 'Residence_type',\n 'avg_glucose_level',\n 'bmi',\n 'gender_encoded_Female',\n 'gender_encoded_Male',\n 'gender_encoded_Other',\n 'work_type_encoded_Govt_job',\n 'work_type_encoded_Never_worked',\n 'work_type_encoded_Private',\n 'work_type_encoded_Self-employed',\n 'work_type_encoded_children',\n 'smoking_status_encoded_Unknown',\n 'smoking_status_encoded_formerly smoked',\n 'smoking_status_encoded_never smoked',\n 'smoking_status_encoded_smokes']\n\nlabel = ['stroke']\n\nX = dataset[features]\ny = dataset[label]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')\n\nsc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")\n\n  imp_feature = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_})\n  plt.figure(figsize=(10,4))\n  plt.title(\"Feature Importance Graphic\")\n  plt.xlabel(\"importance \")\n  plt.ylabel(\"features\")\n  plt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'rgbkymc')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Source for Feature Importance Code:* https://www.kaggle.com/umutalpaydn/heart-disease-analysis-classification#Feature-Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"29\"></a> \n# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I examined Stroke Prediction Dataset. Firstly, I made Exploratory Data Analysis, Visualization, then I applied Machine Learning algorithms to this dataset. \n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, I will be glad to be informed :)\n\n* ***Thank you for your time.***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}