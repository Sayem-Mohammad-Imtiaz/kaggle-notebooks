{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nfrom scipy import stats","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/customer-analytics/Train.csv')\ndf = df.drop(['ID'],axis=1)\n\ndf.columns = ['ware_block','mode_ship','cust_call','cust_rating','product_cost','prior_purchase','product_impt','gender','discount','weight','not_ontime_delivery']\n\ndf_num = ['cust_call','cust_rating','product_cost','prior_purchase','discount','weight']\ndf_cat = ['ware_block','mode_ship','gender']\n\nheatmap_corr = df.corr()\nplt.figure(figsize = (10,8))\nsns.heatmap(heatmap_corr,annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Korelasi feature dengan variabel target**\n- Discount memiliki korelasi (positif) paling tinggi dengan not_ontime delivery dengan koefesien korelasi 0.4\n- Kedua berat barang atau weight memiliki korelasi (negatif) tertinggi kedua dengan koefesien korelasi -0.27\n- Ke empat variabel lain product cost (-0.074), customer care call (-0.067), priority purchase (-0.056) dan customer rating (0.013) memiliki korelasi yang cenderung rendah dibawah 0.1","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering\nBy **Syahrul Ilyasa**","metadata":{}},{"cell_type":"markdown","source":"### Data Outliers","metadata":{}},{"cell_type":"code","source":"#Data include outlier\n\nplt.figure(figsize = (12,4))\nfor i in range(0, len(df_num)):\n    plt.subplot(1, 6, i+1)\n    sns.boxplot(y = df[df_num[i]], orient='v')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Handling outliers\n\ndfx = df.copy()\ndfxx = df.copy()\n\nprint(f'Jumlah baris sebelum memfilter outlier: {len(dfxx)}')\n\nfil_ent = np.array([True] * len(dfxx))\nfor col in ['product_cost', 'discount','weight']:\n    Q1 = dfxx[col].quantile(0.25)\n    Q3 = dfxx[col].quantile(0.75)\n    IQR = Q3 - Q1\n    low_limit = Q1 - (IQR * 1.5)\n    high_limit = Q3 + (IQR * 1.5)\n\n    fil_ent = ((dfxx[col] >= low_limit) & (dfxx[col] <= high_limit)) & fil_ent\n    \ndfxx = dfxx[fil_ent].reset_index()\n\nprint('Jumlah baris setelah memfilter outlier', len(dfxx))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data setelah oulier dikeluarkan\nplt.figure(figsize = (12,4))\nfor i in range(0, len(df_num)):\n    plt.subplot(1, 6, i+1)\n    sns.boxplot(y = dfx[df_num[i]], orient='v')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalisasi Data","metadata":{}},{"cell_type":"code","source":"# Cek data sebelum di normalisasi\nplt.figure(figsize = (12,5))\nfor i in range(0, len(df_num)):\n    plt.subplot(2, 3, i+1)\n    sns.histplot(dfx[df_num[i]], kde=True)\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalisasi data\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\ndfx['product_cost_norm'] = MinMaxScaler().fit_transform(dfx['product_cost'].values.reshape(len(dfx), 1))\ndfx['discount_norm'] = MinMaxScaler().fit_transform(dfx['discount'].values.reshape(len(dfx), 1))\ndfx['weight_norm'] = MinMaxScaler().fit_transform(dfx['weight'].values.reshape(len(dfx), 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cek data setelah dinormalisasi\ndf_norm = ['product_cost_norm','discount_norm','weight_norm']\n\nplt.figure(figsize = (12,5))\nfor i in range(0, len(df_norm)):\n    plt.subplot(2, 4, i+1)\n    sns.histplot(dfx[df_norm[i]], kde=True)\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop variabel yang sudah di normalisasi\ndfx.drop(['product_cost','discount','weight'], axis=1, inplace=True)\n\ndfx.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#labelling pada product importance\ndef product_impt(x):\n    if 'low' in x['product_impt']:\n        product_impt = 1\n    elif 'medium' in x['product_impt']:\n        product_impt = 2\n    else:\n        product_impt = 3\n    return product_impt\n\ndfx['product_impt'] = df.apply(lambda x: product_impt(x), axis=1)\ndfx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Encoding","metadata":{}},{"cell_type":"code","source":"# Feature encoding\nfor cat in df_cat:\n    onehots = pd.get_dummies(dfx[cat], prefix=cat)\n    dfx = dfx.join(onehots)\ndfx.info()\n\n# Parameter n-1 feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop feature awal yang sudah masuk proses feature encoding\ndfx.drop(['ware_block','mode_ship','gender','gender_F'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bagian 1 - Decision Tree (Model+Turning)","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\n\ndef eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n    \n    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) # pos_label: label yang kita anggap positive\n    print(\"AUC: %.2f\" % auc(fpr, tpr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dfx.drop(columns=['not_ontime_delivery'])\ny = dfx['not_ontime_delivery'] # target / label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n# List of hyperparameter\nmax_depth = [int(x) for x in np.linspace(1, 20, num = 20)] # Maximum number of levels in tree\ncriterion = ['gini','entropy']\nsplitter = ['best','random']\nmin_samples_split = [int(x) for x in np.linspace(1, 110, num = 110)] # Minimum number of samples required to split a node\nmin_samples_leaf = [int(x) for x in np.linspace(1, 1100, num = 1100)] # Minimum number of samples required at each leaf node\nmax_features = ['auto','sqrt','log2'] # Number of features to consider at every split\n\nhyperparameters = dict(max_depth=max_depth,\n                       criterion=criterion,\n                       splitter=splitter,\n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                      )\n\n# Inisialisasi Model\ndt = DecisionTreeClassifier(random_state=42)\nmodel = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=42, scoring='recall')\nmodel.fit(X_train, y_train)\n\n# Predict & Evaluation\ny_pred = model.predict(X_test)#Check performa dari model\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best max_depth:', model.best_estimator_.get_params()['max_depth'])\nprint('Best Criterion:', model.best_estimator_.get_params()['criterion'])\nprint('Best Splitter:', model.best_estimator_.get_params()['splitter'])\nprint('Best min_samples_split:', model.best_estimator_.get_params()['min_samples_split'])\nprint('Best min_samples_leaf:', model.best_estimator_.get_params()['min_samples_leaf'])\nprint('Best max_features:', model.best_estimator_.get_params()['max_features'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(20, 10))\ntree.plot_tree(model.best_estimator_,\n               feature_names = X.columns.tolist(), \n               class_names=['0','1'],\n               filled = True, max_depth=5, fontsize=8)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figsize(10, 8)\nfeat_importances = pd.Series(model.best_estimator_.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\nax.invert_yaxis()\n\nplt.xlabel('score')\nplt.ylabel('feature')\nplt.title('feature importance score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bagian 2 - Decision Tree (Feature Selection+Turning)","metadata":{}},{"cell_type":"code","source":"X = dfx.drop(columns=['not_ontime_delivery','ware_block_C','mode_ship_Road','gender_M'])\ny = dfx['not_ontime_delivery'] # target / label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n# List of hyperparameter\nmax_depth = [int(x) for x in np.linspace(1, 20, num = 20)] # Maximum number of levels in tree\ncriterion = ['gini','entropy']\nsplitter = ['best','random']\nmin_samples_split = [int(x) for x in np.linspace(1, 90, num = 90)] # Minimum number of samples required to split a node\nmin_samples_leaf = [int(x) for x in np.linspace(1, 1100, num = 1100)] # Minimum number of samples required at each leaf node\nmax_features = ['auto','sqrt', 'log2'] # Number of features to consider at every split\n\nhyperparameters = dict(max_depth=max_depth,\n                       criterion=criterion,\n                       splitter=splitter,\n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                      )\n\n# Inisialisasi Model\ndt = DecisionTreeClassifier(random_state=42)\nmodel = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=42, scoring='recall')\nmodel.fit(X_train, y_train)\n\n# Predict & Evaluation\ny_pred = model.predict(X_test)#Check performa dari model\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best max_depth:', model.best_estimator_.get_params()['max_depth'])\nprint('Best Criterion:', model.best_estimator_.get_params()['criterion'])\nprint('Best Splitter:', model.best_estimator_.get_params()['splitter'])\nprint('Best min_samples_split:', model.best_estimator_.get_params()['min_samples_split'])\nprint('Best min_samples_leaf:', model.best_estimator_.get_params()['min_samples_leaf'])\nprint('Best max_features:', model.best_estimator_.get_params()['max_features'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(30, 10))\ntree.plot_tree(model.best_estimator_,\n               feature_names = X.columns.tolist(), \n               class_names=['0','1'],\n               filled = True, max_depth=5, fontsize=10)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figsize(10, 8)\nfeat_importances = pd.Series(model.best_estimator_.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\nax.invert_yaxis()\n\nplt.xlabel('score')\nplt.ylabel('feature')\nplt.title('feature importance score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bagian 3 - Decision Tree (Feature Selection 2 + Turning)","metadata":{}},{"cell_type":"code","source":"X = dfx.drop(columns=['not_ontime_delivery','ware_block_C','mode_ship_Road','gender_M','ware_block_D','ware_block_B','prior_purchase'])\ny = dfx['not_ontime_delivery'] # target / label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nimport numpy as np\n\n# List of hyperparameter\nmax_depth = [int(x) for x in np.linspace(1, 20, num = 20)] # Maximum number of levels in tree\ncriterion = ['gini','entropy']\nsplitter = ['best','random']\nmin_samples_split = [int(x) for x in np.linspace(1, 100, num = 100)] # Minimum number of samples required to split a node\nmin_samples_leaf = [int(x) for x in np.linspace(1, 1100, num = 1100)] # Minimum number of samples required at each leaf node\nmax_features = ['auto','sqrt', 'log2'] # Number of features to consider at every split\n\nhyperparameters = dict(max_depth=max_depth,\n                       criterion=criterion,\n                       splitter=splitter,\n                       min_samples_split=min_samples_split, \n                       min_samples_leaf=min_samples_leaf,\n                       max_features=max_features,\n                      )\n\n# Inisialisasi Model\ndt = DecisionTreeClassifier(random_state=42)\nmodel = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=42, scoring='recall')\nmodel.fit(X_train, y_train)\n\n# Predict & Evaluation\ny_pred = model.predict(X_test)#Check performa dari model\neval_classification(model, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best max_depth:', model.best_estimator_.get_params()['max_depth'])\nprint('Best Criterion:', model.best_estimator_.get_params()['criterion'])\nprint('Best Splitter:', model.best_estimator_.get_params()['splitter'])\nprint('Best min_samples_split:', model.best_estimator_.get_params()['min_samples_split'])\nprint('Best min_samples_leaf:', model.best_estimator_.get_params()['min_samples_leaf'])\nprint('Best max_features:', model.best_estimator_.get_params()['max_features'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train score: ' + str(model.score(X_train, y_train)))\nprint('Test score:' + str(model.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(30, 10))\ntree.plot_tree(model.best_estimator_,\n               feature_names = X.columns.tolist(), \n               class_names=['0','1'],\n               filled = True, max_depth=7, fontsize=10)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figsize(10, 8)\nfeat_importances = pd.Series(model.best_estimator_.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\nax.invert_yaxis()\n\nplt.xlabel('score')\nplt.ylabel('feature')\nplt.title('feature importance score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest - Bagian 1","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\n\ndef eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n    \n    fpr, tpr, thresholds = roc_curve(ytest, pred, pos_label=1) # pos_label: label yang kita anggap positive\n    print(\"AUC: %.2f\" % auc(fpr, tpr))\n\ndef show_feature_importance(model):\n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\n    ax.invert_yaxis()\n\n    plt.xlabel('score')\n    plt.ylabel('feature')\n    plt.title('feature importance score')\n\ndef show_best_hyperparameter(model, hyperparameters):\n    for key, value in hyperparameters.items() :\n        print('Best '+key+':', model.get_params()[key])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dfx.drop(columns=['not_ontime_delivery'])\ny = dfx['not_ontime_delivery'] # target / label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_test)\neval_classification(rf, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n#List Hyperparameters yang akan diuji\nhyperparameters = dict(\n                       n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)], # Jumlah subtree \n                       bootstrap = [True], # Apakah pakai bootstrapping atau tidak\n                       criterion = ['gini','entropy'],\n                       max_depth = [int(x) for x in np.linspace(10, 110, num = 11)],  # Maximum kedalaman tree\n                       min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)], # Jumlah minimum samples pada node agar boleh di split menjadi leaf baru\n                       min_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)], # Jumlah minimum samples pada leaf agar boleh terbentuk leaf baru\n                       max_features = ['auto', 'sqrt', 'log2'], # Jumlah feature yg dipertimbangkan pada masing-masing split\n                       n_jobs = [-1], # Core untuk parallel computation. -1 untuk menggunakan semua core\n                      )\n\n# Init\nrf = RandomForestClassifier(random_state=42)\nrf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=42, scoring='recall')\nrf_tuned.fit(X_train,y_train)\n\n# Predict & Evaluation\ny_pred = rf_tuned.predict(X_test)#Check performa dari model\neval_classification(rf_tuned, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_best_hyperparameter(rf_tuned.best_estimator_, hyperparameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBOOST","metadata":{}},{"cell_type":"code","source":"X = dfx.drop(columns=['not_ontime_delivery'])\ny = dfx['not_ontime_delivery'] # target / label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\nfrom xgboost import XGBClassifier\nxg = XGBClassifier(random_state=42)\nxg.fit(X_train, y_train)\n\ny_pred = xg.predict(X_test)\neval_classification(xg, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport numpy as np\n\n#Menjadikan ke dalam bentuk dictionary\nhyperparameters = {\n                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],\n                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],\n                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],\n\n                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],\n\n                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]\n                    }\n\n# Init\nxg = XGBClassifier(random_state=42)\nxg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=5, random_state=42, scoring='recall')\nxg_tuned.fit(X_train,y_train)\n\n# Predict & Evaluation\ny_pred = xg_tuned.predict(X_test)#Check performa dari model\neval_classification(xg_tuned, y_pred, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}