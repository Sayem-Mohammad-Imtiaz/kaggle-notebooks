{"cells":[{"metadata":{"_uuid":"b12492351befcacd31561f6bf98d96e3aab67294"},"cell_type":"markdown","source":"## Introduction\n\nThis document presents:\n * An initial analysis of customer churn of a given telecom company\n * Insights and possible actions the client can take to improve the scenario\n * Recommendations of further hypotheses and improvements over the model\n \n**Executive Summary**\nHere, we present some of the partial conclusions and what the business could do with these new information.\n\n*Tenure and longer contracts*\nThese variables have positive impacts in reducing churn. In charge with this informations, the selling and customer success departaments could push longer contract to clients. Each month that the client stays increases the chances of the client staying yet another month.\n\n*More comments on Tenure*\nThe churn is very high after one month, and we have two main hypotheses for that:\n * Our client (the telecom) does not make a good screening process to accecpt or not clients. This is a opportunitty to yet another project, risk modelling for new customers acceptance.\n * Our onboarding process is too bad (we may take too long to install the service in the customer's house, the product may be hard to use, etc)\n \n*Monthly Charges*\nCheaper payments have a good effect on churn. We could further investigate it to find out what is the effect in the life-time value when the price is decrease for a certain service plan. We could get new Monthly charges that would optimize life-time value of the client.\nThe second usage of this insight is more direct. If a customer wants to finish his contract with the telecom, offering the customer a discount for a certain time is a good practice. The changes of churn decreases and even when the discount is over, the chances of churn are smaller because of the increase in tenure.\n\n*Phone Service and InternetService*\nInternetService has a bad effect on Churn (see Lasso analysis), and PhoneService has a null effect (see the chi analysis). Our hypotheses are that the customers don't care much about the PhoneService and that our InternetService is bad.\nThe telecom could survey clients about the PhoneService and InternetService to test these hypotheses. If they turn out to be true, maybe reducing the offer of PhoneService to a niche group and adjustments to the InternetService could improve our profits.\n\n*The Model*\nNote: the model has not achieved the desired results and can not be used by the business as is. Improvements are commented in this document. In this section, we would like to expose what could be done with a good prediction model.\nOne of the main uses of the models would be to automate customer services.\nFor instance, since decreases in Montly charges improve churn, the clients with the highest probabilities of churn could receive automatic discounts or coupons. "},{"metadata":{"_uuid":"eb8538a1ad09dec978ac3db4040dbef7fde53002"},"cell_type":"markdown","source":"# First glance at the data"},{"metadata":{"_uuid":"3604906a2ddc238e8d5dad5d81412e7e1aedaa59"},"cell_type":"markdown","source":"The first step of our analysis is get a better notion of what we have on our hands.\nThis will lead us to the first insights of how to clean the data and which models might have a good result."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"a08e7274682500433954a866faadf2b0a575f71e"},"cell_type":"code","source":"import pandas as pd\nraw_data = pd.read_csv(\"../input/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nprint(raw_data.dtypes)\npd.set_option('display.max_columns', None)\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0690dcecae0bf92b5fa68db518455af9ac7bfe96"},"cell_type":"markdown","source":"We have then 20 features (customerID is only an index), 19 independent variables (or input variables) and one dependent, the one named 'Churn'.\n\nHere, I want to point out three things:\n* Most of the data is categorical, what suggests me that linear or logistics regression would not work well. These regressions fit better problems with continuous features.\n* There are some obvious relationships among the features. For instance, there is Phone Service feature and the Multiple Lines feature has a 'No phone service' category. Further on the analysis, this insight will help us to eliminate some variables of the analysis\n* And, we see that some numerical variables could be converted automatically to int or float types, what indicates absence of NaNs (Not the same as saying that the data was 100% correctly filled). The same cannot be said for TotalCharges. \n\nSo, moving on..."},{"metadata":{"_uuid":"f83d53d08ade4bad5c47256946728d66301c19b4"},"cell_type":"markdown","source":"# Data Quality"},{"metadata":{"_uuid":"5b07a6356ce22890f970c8947dca7c9d63fde4d5"},"cell_type":"markdown","source":"Here, we want to look into Exploratory Data Analyses. Our objective is to get a deeper understanding of the data, more specifically, we have the following tasks:\n* Find NaNs and what is their impact in the data\n* Plot some distributions and correlations to see which data to use and how to use the data when modelling\n\nSo, starting by Gender. In the gender feature, I would expect to find a similar distribution to the general population (55% for male and 45% for female)."},{"metadata":{"trusted":false,"_uuid":"65e97ef38bda9d0063a706511501689d64527eae"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ngender_values = raw_data['gender'].unique()\ngender_counts = [None] * len(gender_values)\ni = 0\nfor value in gender_values:\n    gender_counts[i] = raw_data['gender'].loc[raw_data.gender == value].count()\n    i = i + 1\n    \nplt.bar(gender_values, gender_counts / raw_data['gender'].count())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78cc07ba9c3e0ffb495d723b07713bdd6a5f1dce"},"cell_type":"markdown","source":"Seems right, and no NaN, very good."},{"metadata":{"trusted":false,"_uuid":"47b4d7ebea392c36f32bfecbaf5111c99fb6f384"},"cell_type":"code","source":"#let's get rid of code repetition before continuing\ndef plot_series_bar(series, series_name, ax):\n    values = series.unique()\n    counts = [None] * len(values)\n    i = 0\n    for value in values:\n        counts[i] = series.loc[series == value].count()\n        i = i + 1\n    \n    ax.bar(values, counts / series.count())\n    ax.set_title(series_name)\n    \n    \nfig, axes = plt.subplots(ncols=5, sharex=False, sharey=True, figsize=(15,5))\ni = 0\nfor column in ['SeniorCitizen', 'Partner', 'Dependents','PhoneService', 'MultipleLines']:\n    plot_series_bar(raw_data[column], column, axes[i])\n    i = i + 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1514c687d4437657d9078107ec5abea571869e7c"},"cell_type":"markdown","source":"All the data is complete what is very good.\n\nThe percentage of Senior Citizens is similar to the national population (around 15%) and here, the proportion to be a little higher than the national average is expected since we probably don't have underagers as clients.\n\nI thought we would not have all the data for partner or dependents, as it seems odd that a telecom company asks for that. So I guess that some NOs may actually be NaNs, but the quantity should be small since at least the proportion of partner is also close to the general population that is married (around 40%).\n\nContinuing with the plot..."},{"metadata":{"trusted":false,"_uuid":"4f3fb1aa3a032ba40821c5c2b8b4045ff19eff36"},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=7, sharex=False, sharey=True, figsize=(15,5))\ni = 0\nfor column in ['InternetService', 'OnlineSecurity', 'OnlineBackup','DeviceProtection', 'TechSupport', \n               'StreamingTV', 'StreamingMovies']:\n    plot_series_bar(raw_data[column], column, axes[i])\n    i = i + 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"34eb93fc12d81985175820908caa430b57bb0633"},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=4, sharex=False, sharey=True, figsize=(15,5))\ni = 0\nfor column in ['Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']:\n    plot_series_bar(raw_data[column], column, axes[i])\n    i = i + 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed4e16b20751905163dba43d34bb2d2d64e01ee1"},"cell_type":"markdown","source":"The \"No internet service\" or \"No phone service\" match the values accross all grpahs. A good point for the data quality.\n\nBelow, we continue to evaluate data quality, but we start to check the variables correlations. Mainly with Churn."},{"metadata":{"trusted":false,"_uuid":"92eaa9902ee1f2abb6164dd0d6b5b20c464f920a"},"cell_type":"code","source":"import numpy as np\nbinwidth = 1\ntenure_ser = raw_data['tenure']\nplt.hist([raw_data.loc[raw_data['Churn']=='Yes']['tenure'], raw_data.loc[raw_data['Churn']=='No']['tenure']],\n         bins=np.arange(min(tenure_ser), max(tenure_ser) + binwidth, binwidth), histtype='barstacked',\n        label=['Has Churned', 'Has not'])\nplt.legend(prop={'size': 10})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca3efff6709da645d450551d69a57b4967dee1d4"},"cell_type":"markdown","source":"We can conclude two things with this graph from tenure:\n * It seems that there is a strong inverse relationship between tenure and Churn\n * There is a lot of clients accumulated with a tenure of 72\n\nThe first one is kind of expected, the longer you stick as a cliente, the bigger the probability of you staying a month more. The second seems to indicate that 72 is actually a representation of clients with 72 or more months with the company.\n\nTo solve this distortion, we have two alternatives:\n * Create a dummy variable that indicates that the client has a tenure of 72, so the model can more easily figure out if it changes anything\n * To transform the variable in a categorical variable\n\nI would test both if I had the time, but here, we will implement the second alternative because it is more commonly applied and several models respond well for it."},{"metadata":{"trusted":false,"_uuid":"ee5fb5f1a066cb5108da8b709efb7812406af2f3"},"cell_type":"code","source":"import numpy as np\n\ndef plot_hist_with_churn(df, series_name, binwidth, ax):\n    ser = df[series_name]\n    ax.hist([df.loc[df['Churn']=='Yes'][series_name], df.loc[df['Churn']=='No'][series_name]],\n         bins=np.arange(min(ser), max(ser) + binwidth, binwidth), histtype='barstacked',\n        label=['Has Churned', 'Has not'])\n    ax.legend(prop={'size': 10})\n\nfig, ax = plt.subplots()\nplot_hist_with_churn(raw_data, 'MonthlyCharges', 1, ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b6f575adb215b47a68755ab0968844e582cf9b5"},"cell_type":"markdown","source":"As already exposed in the introduction. The graph above shows one of ours main insights, that cheaper plans have a positive impact in churn. There is a low percentage of churned clients in high paying plans too, probably very satisfied clients, with more time, I would try to find how these very satisfied clients, correlating it with the demographic and services features.\n\nFor the total charges plot, we need to first get rid of the NaN info"},{"metadata":{"trusted":false,"_uuid":"3bb3057b52c19126dfbb85ef1e4281c3fb72fcd0"},"cell_type":"code","source":"total_charges_ser = raw_data['TotalCharges']\ntotal_charges_ser = pd.to_numeric(total_charges_ser, errors='coerce')\nraw_data['TotalCharges'] = total_charges_ser\n#How many NaN do we have at TotalCharges?\nprint(raw_data.loc[total_charges_ser.isnull()==True].count()['customerID'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40720e7288d38fd2d9a683376d2070c8f9e6c661"},"cell_type":"markdown","source":"Eleven out of more than 7000 samples is pretty insignificant, we can get rid of these eleven rows if we decide that Total Charges is a important variable to use in the model. If Total Charges turns out to be not so important, we don't need to be concerned with that."},{"metadata":{"trusted":false,"_uuid":"6b17f440ef984efcdae2eebe2831c620df0182ff"},"cell_type":"code","source":"without_nan = raw_data.loc[total_charges_ser.isnull()==False].infer_objects()    \nprint(without_nan.TotalCharges.hist())\n\n#Create a boxplot\nwithout_nan.boxplot('TotalCharges', by='Churn')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67a5513f8dcf526e21b95247c8f424439675dff9"},"cell_type":"markdown","source":"So, people who have payed more through history tend to churn less (the main hypothesis is that they are longer with the telecom). There are though several outliers in the churned clients, maybe another signal that high paying plans are not good for churn."},{"metadata":{"trusted":false,"_uuid":"b0776a1b903bcc58accb5371b35ac79bc9f815a7"},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\ndef print_anova(series_name, data):\n    model = ols(series_name+' ~ Churn', data=data).fit()\n    anova_table = sm.stats.anova_lm(model, typ=2)\n    print(anova_table)\n    print('\\n')\n\nprint_anova('TotalCharges', without_nan) \nprint_anova('MonthlyCharges', without_nan)\nprint_anova('tenure', without_nan)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39cbde6f6008d2c306853a774b35795f377c5111"},"cell_type":"markdown","source":"Above, we presented anova as a measure for correlation. We can see by the p-value (last column) the probability of finding such distributions by random. Since they are vey low, we can safely conclude that these variables have effects on churn.\n\nBelow, we present a chi analysis. Again calculating the p-value to get a notion of correlation."},{"metadata":{"trusted":false,"_uuid":"9b69dfc2f3979dc8b1611227a33456b8cafaf83f"},"cell_type":"code","source":"import scipy.stats as stats\n\ndef print_cross_table(df, series_name):\n    tab = pd.crosstab(df.Churn, df[series_name], margins = True)\n    tab.columns = [\"Not \"+series_name, series_name, \"row_totals\"]\n    tab.index = [\"Has Churned\", \"Has not\", \"col_totals\"]\n    print(tab)\n    print(\"\\n\")\n    return tab\n    \ntab = print_cross_table(without_nan,'SeniorCitizen')\nprint(stats.chi2_contingency(observed= tab.iloc[0:2,0:2]))\nprint(\"\\n\")\ntab = print_cross_table(without_nan,'PhoneService')\nprint(stats.chi2_contingency(observed= tab.iloc[0:2,0:2]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d826968aafa7e69ca7bd85dc24dfebb4bc4608e9"},"cell_type":"markdown","source":"Senior has a good effect on churn. But Phone service seems to be indifferent (p-value of 0.35, bigger than 0.05 so we cannot reject the null hypothesis)."},{"metadata":{"_uuid":"c842d0c7d28e61a3bf8f9531ee0899da87b95b6a"},"cell_type":"markdown","source":"# Model"},{"metadata":{"_uuid":"8302df61b251b7f5cf50a14e7b724a16e9367f81"},"cell_type":"markdown","source":"First, the preprocessing.\nWe create the dummy variables for the categorical data.\nAnd, we also split the tenure and monthly charges into categories."},{"metadata":{"trusted":false,"_uuid":"ac5a146de25d4c9930d73d44ee91f34658d3dab8"},"cell_type":"code","source":"cat_data =  pd.DataFrame()\ncat_data['tenure'] = pd.cut(without_nan['tenure'], 6, labels=['1','2','3','4','5','6']) #cat of one year\ncat_data['MonthlyCharges'] = pd.cut(without_nan['MonthlyCharges'], 5, labels=['1','2','3','4','5'])\n\nmulti_categorical_columns = ['MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection',\n                       'TechSupport','StreamingTV','StreamingMovies','Contract','PaymentMethod']\n\nsimple_categorical_columns = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','PaperlessBilling','Churn']\n\npreproc_data =  pd.DataFrame()\n\nfor category in multi_categorical_columns:\n    dummies = pd.get_dummies(without_nan[category], drop_first=True)\n    try:\n        dummies = dummies.drop(columns=['No phone service'])\n    except:\n        pass\n    try:\n        dummies = dummies.drop(columns=['No internet service'])\n    except:\n        pass\n    \n    col_names = []\n    for col_name in dummies.columns:\n        col_names = col_names + [category + '-' + col_name]\n    dummies.columns = col_names\n    preproc_data = pd.concat([preproc_data, dummies], axis=1)\n    \n    \nfor category in ['tenure','MonthlyCharges']:\n    dummies = pd.get_dummies(cat_data[category], drop_first=True)\n    col_names = []\n    for col_name in dummies.columns:\n        col_names = col_names + [category + '-' + col_name]\n    dummies.columns = col_names\n    preproc_data = pd.concat([preproc_data, dummies], axis=1)\n\nfor category in simple_categorical_columns:\n    dummies = pd.get_dummies(without_nan[category], drop_first=True)\n    col_names = []\n    for col_name in dummies.columns:\n        col_names = col_names + [category + '-' + str(col_name)]\n    dummies.columns = col_names\n    preproc_data = pd.concat([preproc_data, dummies], axis=1) \n\npreproc_data['TotalCharges'] = without_nan['TotalCharges']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"16163264f6832afc4453f0763d5d93ffbdcf1717"},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(preproc_data)\nnorm_data = pd.DataFrame(data=scaler.transform(preproc_data), columns=preproc_data.columns)\n\nys = []\nxs = []\n\nindependent = norm_data.drop(columns=['Churn-Yes'])\ndependent = norm_data['Churn-Yes']\n\nfor alpha in range(1,20):\n    \n    lassoreg = Lasso(alpha=(alpha/1e2),normalize=False, max_iter=1e4)\n\n    lassoreg.fit(independent,dependent)\n    \n    ys = ys + [lassoreg.coef_.tolist()]\n    \n    xs = xs + [(alpha/1e2)]\n\nhandles = plt.plot(xs, ys)\nplt.legend(handles=handles, labels=independent.columns.tolist(), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"87e133593a6e21f0ca5147b063e45c0e9e39e095"},"cell_type":"code","source":"lassoreg = Lasso(alpha=0.05, normalize=False, max_iter=1e4)\nlassoreg.fit(independent,dependent)\ncoefs = np.abs(lassoreg.coef_.tolist())\nbest_ten = sorted(range(len(coefs)), key=lambda k: coefs[k], reverse=True)[0:10]\nbest_ten_feats = []\nfor i in best_ten:\n    print(independent.columns[i] + ' with coef: ' + str(lassoreg.coef_[i]))\n    best_ten_feats = best_ten_feats + [independent.columns[i]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8877a0b9f278ce8943e86945d4629a0a8d8b1276"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n#Get the best features, add back the missing categories of the best features and put independent variable\nmodel_data = preproc_data[best_ten_feats + ['PaymentMethod-Credit card (automatic)', 'PaymentMethod-Mailed check',\n                                           'tenure-2','tenure-4','tenure-5','tenure-6'] + ['Churn-Yes']]\n\nrf = RandomForestClassifier(n_estimators = 100, random_state = 12)\ntrain_features, test_features = train_test_split(model_data) #by default, 25% of the data is test data\n\nx_train = train_features.drop(columns=['Churn-Yes'])\ny_train = (train_features['Churn-Yes']>0.5)\n\nx_test = test_features.drop(columns=['Churn-Yes'])\ny_test = (test_features['Churn-Yes']>0.5)\n\nrf.fit(x_train, y_train)\n\n# Use the forest's predict method on the test data\npredictions = rf.predict(x_test)\n\ntrue_positives = ( (predictions==1) & (y_test==1) )\nfalse_negatives = ( (predictions==0) & (y_test==1) )\n\n# Calculate and display accuracy\nprint('Precision:', round(100*(true_positives.sum()/(predictions>0.5).sum()), 2), '%.')\nprint('Recall:', round(100*(true_positives.sum()/(y_test==True).sum()), 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"19448cf53b4b855941c170c1f7934cdd61218c9e"},"cell_type":"code","source":"import xgboost as xgb\n\n# specify parameters via map\nparam = {'max_depth':5, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\nnum_round = 10\n\nxgtrain = xgb.DMatrix(x_train.values, y_train.values)\nxgtest = xgb.DMatrix(x_test.values)\n\nbst = xgb.train(param, xgtrain, num_round)\n# make prediction\npredictions = bst.predict(xgtest)\n\ntrue_positives = ( (predictions>0.5) & (y_test==True) )\n\n# Calculate and display accuracy\nprint('Precision:', round(100*(true_positives.sum()/(predictions>0.5).sum()), 2), '%.')\nprint('Recall:', round(100*(1 - true_positives.sum()/(y_test==True).sum()), 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b098fd7d03cf278cf4d38e2a55221bdd1384ca2"},"cell_type":"markdown","source":"Trying a Neural Network "},{"metadata":{"trusted":false,"_uuid":"340651bd0d83316a21b8cbeffcfbf0482848ef09"},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\ntrain_data, test_data = train_test_split(preproc_data) #by default, 25% of the data is test data\n\nx_train = train_data.drop(columns=['Churn-Yes'])\ny_train = (train_data['Churn-Yes']>0.5)\n\nx_test = test_data.drop(columns=['Churn-Yes'])\ny_test = (test_data['Churn-Yes']>0.5)\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(x_train)\n\nx_train = scaler.transform(x_train)  \n# apply same transformation to test data\nx_test = scaler.transform(x_test)  \n\nclf = MLPClassifier(solver='adam', alpha=1e-7,\n                    hidden_layer_sizes=(10, 10, 2), random_state=1, max_iter=200000)\n\nclf.fit(x_train, y_train)\n\n# make prediction\npredictions = clf.predict(x_test)\n\ntrue_positives = ( (predictions>0.5) & (y_test==True) )\n\n# Calculate and display accuracy\nprint('Precision:', round(100*(true_positives.sum()/(predictions>0.5).sum()), 2), '%.')\nprint('Recall:', round(100*(1 - true_positives.sum()/(y_test==True).sum()), 2), '%.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}