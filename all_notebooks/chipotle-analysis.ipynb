{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Chipotle Saturation Analysis\n\nThis notebook presents a sample analysis of Chipotle restaurant saturation by county in the US.  It begins by pulling in geographic shape data on US counties from a COVID-19 data set, then brings in various columns of demographic data from another COVID-19 data set and a 2017 US census data set.  It then cleans and combines that data, and creates a linear regression model of the number of Chipotles by county.  By removing predictors that are not significant, a linear model with four variables is produced with a relatively high r-squared value.  Then, the full US county data is brought against the regression, residuals are computed, and a map generated to show potential areas for expansion and oversaturation.\n\n**Please note**: this is my first attempt at this type of data analysis using Python, Pandas, Folium, etc.  I'm 100% sure there are things that could be improved upon, and I welcome feedback.  Also, take note of ideas and conclusions near the end of the analysis.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The analysis begins by reading in various libraries that will be used throughout.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom shapely import wkt\nimport folium\nfrom folium import Choropleth\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn import linear_model\n\nimport statsmodels.regression.linear_model as sm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading in files, filling in Chipotle geometry.  Note that most of the information comes in as an object datatype, so there will be some later work to convert these types to float.\n\nThere were a few US counties that didn't have geometries listed.  These were in Alaska and didn't apper to be relevant to the analysis, so they were dropped.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"chipotle = gpd.read_file(\"../input/chipotle-locations/chipotle_stores.csv\")\ngeometry = [Point(xy) for xy in zip(chipotle['longitude'].astype('float'),chipotle['latitude'].astype('float'))]\nchipotle['geometry'] = geometry\nchipotle.head()\n\nus_states = gpd.read_file(\"../input/chipotle-locations/us-states.json\")\nus_counties = gpd.read_file('../input/enrichednytimescovid19/us_county_pop_and_shps.csv')\nus_counties = us_counties.loc[us_counties['county_geom'] != 'None']\n\ncensus2017 = gpd.read_file('../input/us-census-demographic-data/acs2017_county_data.csv')\n\ncovid2019 = gpd.read_file('../input/covid19-us-county-jhu-data-demographics/us_county.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The county geometry data has both a center point and a polygon.  I just want the polygon, so I'll set it to be the main geometry column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_counties['county_geom'] = us_counties['county_geom'].apply(wkt.loads)\nus_counties = gpd.GeoDataFrame(us_counties, geometry='county_geom')\ncensus2017","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I used geopandas spacial join feature to assign counties to each Chipotle store's latitude and longitude.  Later, I added the CRS.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"chipotle_enhanced = gpd.sjoin(chipotle,us_counties)\nchipotle_enhanced.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of my ideas was that population density might be a predictor.  However, in order to do this, I needed to calculate area and population density of each county, and add it to the chipotle_enhanced dataframe. I was able to validate a few sample counties against Wikipedia data for reasonability.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate population density as people / km**2\nus_counties['popdensity'] = us_counties['county_pop_2019_est'].astype(float) / (us_counties['county_geom'].area * 10000)\n\n#create a common key for reading in calculated density\nchipotle_enhanced['statecounty'] = chipotle_enhanced['state_right'] + chipotle_enhanced['county']\nus_counties['statecounty'] = us_counties['state'] + us_counties['county']\n\n#read in calculated density\nchipotle_enhanced = pd.merge(chipotle_enhanced, us_counties[['statecounty','popdensity']], on='statecounty',how='left')\n\n#create a common key in the us census data\ncensus2017['statecounty'] = census2017['State'] + census2017['County']\ncensus2017['statecounty'] = census2017['statecounty'].replace(' County','', regex=True)\n\n#create a common key in the us county covid data\ncovid2019['statecounty'] = covid2019['state'] + covid2019['county']\ncovid2019['statecounty'] = covid2019['statecounty'].replace(' County','',regex=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, I need to start bringing everything together.  First, I want to summarize a count of how many Chipotles are in each county that has one.  Then, I read in the various demographic data columns from the other datasets into a master dataframe.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a pivot table giving the number of stores by county\nchipotle_counts = pd.pivot_table(chipotle_enhanced, values=['address'], index=['statecounty'], aggfunc=lambda x: len(x.unique()))\n\n#trim some columns from the census data\n#dropping TotalPop because we have a 2019 estimate, which should be better than a 2017 estimate\ncensus2017_to_merge = census2017.drop(['State','County','CountyId','geometry','TotalPop'], axis=1)\n\n#read in other potentially useful fields for a regression\nchipotle_counts = pd.merge(chipotle_counts, us_counties[['statecounty','popdensity','county_pop_2019_est']], on='statecounty',how='left')\nchipotle_counts = pd.merge(chipotle_counts, census2017_to_merge, on='statecounty',how='left')\nchipotle_counts = pd.merge(chipotle_counts, covid2019[['median_age','statecounty']],on='statecounty',how='left')\n\n# #rename the \"address\" column to \"count\" because it annoys me\n# chipotle_counts.rename(columns={'address':'count'})\n\n#There are some missing values in a few fields.  Filling them in with the next entry, or failing that, zero\nchipotle_counts = chipotle_counts.fillna(method='bfill',axis=0).fillna(0)\n\n#convert values to floats so we don't have to keep doing it\n#chipotle_counts = chipotle_counts.loc[:, chipotle_counts.columns != 'statecounty'].astype('float64')\nchipotle_counts.loc[:, chipotle_counts.columns != 'statecounty'] = chipotle_counts.loc[:, chipotle_counts.columns != 'statecounty'].apply(pd.to_numeric)\n\n#display data to make sure things look about right\nchipotle_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try out a linear regression on these variables (except statecounty, which is a key field)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty'],axis=1)\ny = chipotle_counts.address\n\nlm = linear_model.LinearRegression()\nmodel=lm.fit(X,y)\nlm.score(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above confirms that a linear regression has a high predictive value.  Now I run several iterations to figure out which variables are statistically significant.  For that, sklearn isn't sufficient, because it doesn't give data on the p-values of each predictor - at least not in a way I can understand.  We need statsmodels OLS, which gives some statistics on the variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trim variables with high p-values (over 0.75) and try again.  Variables dropped are:\n\n* Asian\n* IncomeErr\n* PrivateWork\n* PublicWork\n* SelfEmployed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed'],axis=1)\n\nmod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying again, this time with variables that have p-values over 0.5.  New variables dropped:\n\n* popdensity\n* Native\n* FamilyWork\n\nSo much for my idea that population density might be a good predictor.  It also seems like the type of work isn't very important for the number of Chipotles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed','popdensity','Native','FamilyWork'],axis=1)\n\nmod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying again, this time trimming variables with p-value > 0.25. New variables dropped:\n\n* White\n* Black\n* Poverty\n* ChildPoverty\n* median_age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed','popdensity','Native','FamilyWork','White','Black','Poverty','ChildPoverty','median_age'],axis=1)\n\nmod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next round omits variables with p-values > 0.1.  New variables dropped:\n\n* Men\n* Hispanic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed','popdensity','Native','FamilyWork','White','Black','Poverty','ChildPoverty','median_age','Men','Hispanic'],axis=1)\n\nmod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed','popdensity','Native','FamilyWork','White','Black','Poverty','ChildPoverty','median_age','Men','Hispanic','Office','Construction','Production','Drive','Carpool','Transit','WorkAtHome','MeanCommute','Unemployment'],axis=1)\n\nmod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed','popdensity','Native','FamilyWork','White','Black','Poverty','ChildPoverty','median_age','Men','Hispanic','Office','Construction','Production','Drive','Carpool','Transit','WorkAtHome','MeanCommute','Unemployment','Income','IncomePerCap','Professional','Service','Walk','OtherTransp'],axis=1)\n\nmod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = chipotle_counts.drop(['address','statecounty','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed','popdensity','Native','FamilyWork','White','Black','Poverty','ChildPoverty','median_age','Men','Hispanic','Office','Construction','Production','Drive','Carpool','Transit','WorkAtHome','MeanCommute','Unemployment','Income','IncomePerCap','Professional','Service','Walk','OtherTransp','VotingAgeCitizen','IncomePerCapErr'],axis=1)\n\nmod = sm.OLS(y, X)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next few rounds will omit any variables with p-value > 0.05, until all variables fit that condition.  Final variables dropped:\n\n* Office\n* Construction\n* Production\n* Drive\n* Carpool\n* Transit\n* WorkAtHome\n* MeanCommute\n* Unemployment\n* Income\n* IncomePerCap\n* Professional\n* Service\n* Walk\n* OtherTransp\n* VotingAgeCitizen\n* IncomePerCapErr","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we know the significant variables are county_pop_2019_est, Women, Pacific, and Employed, we can go back to sklearn for easier prediction capacities. The next section retrains the model using sklearn, then does a set of predictions with all US county data (other than NaNs, which are dropped). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#retrain model\nX = chipotle_counts.loc[:,['Women','Pacific','Employed','county_pop_2019_est']]\ny = chipotle_counts.address\nlm = linear_model.LinearRegression()\nmodel=lm.fit(X,y)\n\n#develop new predictions with all counties\ncensus_xpred = census2017.drop(['geometry','CountyId','State','County','TotalPop','Asian','IncomeErr','PrivateWork','PublicWork','SelfEmployed','Native','FamilyWork','White','Black','Poverty','ChildPoverty','Men','Hispanic','Office','Construction','Production','Drive','Carpool','Transit','WorkAtHome','MeanCommute','Unemployment','Income','IncomePerCap','Professional','Service','Walk','OtherTransp','VotingAgeCitizen','IncomePerCapErr'],axis=1)\nxpred = pd.merge(census_xpred,us_counties[['statecounty','county_pop_2019_est']], on='statecounty',how='left')\nxpred = xpred.dropna()\ncensus2017_with_pred = xpred\nxpred = xpred.loc[:, xpred.columns != 'statecounty'].astype('float64')\ncensus2017_with_pred['ypred'] = model.predict(xpred)\ncensus2017_with_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This next section of code merges the prediction data back with county geometry for the final map.  I also create a \"residuals\" variable, which is the predicted number of Chipotles per county minus the actual number.  Typically residuals are expressed as \"actual minus expected\", not \"expected minus actual\", but I wanted areas that appear to be deficient in Chipotles to show up as positives, i.e., build this many Chipotles in this county.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging the prediction and actual counts with county geometry\nmodel_data = pd.merge(us_counties,census2017_with_pred, on='statecounty',how='left')\nmodel_data['statecounty'] = model_data['state'] + model_data['county']\nmodel_data = pd.merge(model_data, chipotle_counts[['address','statecounty']],on='statecounty',how='left')\nmodel_data.address.fillna(0, inplace=True)\nmodel_data.ypred.fillna(0, inplace=True)\n\n#Normally we'd subtract \"actual\" minus \"expected\" to determine residuals.  In this case, \n#it would be better to have \"expected\" minus \"actual\" to show how many restaurants should be built.\n\nmodel_data['residuals'] = model_data['ypred'] - model_data['address']\nmodel_data.crs = \"epsg:4326\"\nmodel_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to put a map together of those residuals.  First, let's figure out the min and max. to help set the color scale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum residual = ' + str(model_data.residuals.max()))\nprint('Minimum residual = ' + str(model_data.residuals.min()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The color scale was difficult for me to set.  Basically, anywhere that there's no need to build a Chipotle shows up in red, areas that are basically right are in orange, and the yellow and green areas appear to be able to support more Chipotles than are currently in the county.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"geo_data = model_data[['statecounty','county_geom']].set_index('statecounty')\nresidual_data = model_data[['statecounty','residuals']].set_index('statecounty')\n\nm_1 = folium.Map(location=[40,-100], tiles='openstreetmap', zoom_start=4)\n\nChoropleth(geo_data=geo_data.__geo_interface__, data=residual_data['residuals'], fill_color='RdYlGn', key_on='feature.id', legend_name='Chipotles to build per county', threshold_scale=[-50,-1,1,5,10,12]).add_to(m_1)\n\nm_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tabular view of top 10 counties needing more Chipotles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.sort_values('residuals',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tabular view of top 10 counties unable to support additional Chipotles.  This may include counties where there are currently no Chipotles and it's a really bad idea to build one there.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.sort_values('residuals',ascending=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Concluding Thoughts\n\nA few observations from the final dataset and map:\n\n* The final model has the equation: Number of Chipotles in County = 0.000005397 * 2019 County Population Estimate - 0.0000363 * Number of Women in County - 2.0156 * Percentage of Pacific Islanders in County + 0.00005169 * Number of employed people in county.  \n* Variables that I had suspected might be significant turned out not to be.  In particular, population density dropped out of the running for statistically significant variables rather quickly.\n* Areas that appear to have lower Chipotles than expected seem to cluster around a few large cities: Seattle, Salt Lake City, Houston, San Antonio, Oklahoma City, Memphis, Detroit, Buffalo, Maimi, and certain portions of the northeast megalopolis.  \n* Interestingly, New York appears to be over-saturated with Chipotles.\n* The model can produce a negative number of predicted Chipotles.  While practically impossible, I don't think there's any theoretical problem with that outcome: it simply means that it's a bad place to even try starting a Chipotle.\n\nA few other areas for future research:\n\n* The original data gave existing Chipotle locations, but some measure of each restuarant's profitability would have added another dimension to the analysis.\n* While this analysis gives a general idea of which counties appear to be able to support more or fewer Chipotles, it ignores more granular data.  There may be opportunities for additional Chipotles in a county that seems oversaturated if it is on the border with another that has fewer Chipotles than expected, for example.\n* Information on competing restaurant chains could be useful.  Some of the cities above may have other, regional franchises operating that limit Chipotle's market share in those domains.\n* My method of choosing significant variables may be sub-optimal.  I didn't know how to implement something like the Bayesian Information Criterion or Schwartz Information Criterion here.\n\nA few comments on my learning experience.  As I mentioned above, this notebook was my first attempt at this type of analysis.\n\n* I'm sure some of my methods are clunky, but it got me where I needed to go.\n* My color scale in Folium is not my first choice, but it seems like there are limited options in the Choropleth map.\n* I was unable to add a tooltip or pop-up marker to the Choropleth map in Folium.\n\nI hope this helps someone think about the task in a new way.  Feel free to use my notebook as a starting point for your own.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}