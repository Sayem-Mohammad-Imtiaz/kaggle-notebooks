{"cells":[{"metadata":{},"cell_type":"markdown","source":"# IBM Employee Attrition dataset"},{"metadata":{},"cell_type":"markdown","source":"The purpose of this dataset is to analyze factors leading to employee attrition."},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis"},{"metadata":{"slideshow":{"slide_type":""}},"cell_type":"markdown","source":"### 1.1. Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom ipywidgets import interact\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(r'../input/WA_Fn-UseC_-HR-Employee-Attrition.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspect some example entries in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there any null values among the entries? (couple of ways to do this)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there's no need to handle null values in the dataset"},{"metadata":{},"cell_type":"markdown","source":"### 1.3. Feature analysis"},{"metadata":{},"cell_type":"markdown","source":"Split dataframe to numerical and categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df = df.select_dtypes(include = 'object')\nnum_types = [t for t in df.dtypes.unique() if t not in cat_df.dtypes.unique()]\nnum_df = df.select_dtypes(include = num_types)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First let's print basic statistics about all the numerical features (like mean, std, percentiles etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We delete all the numerical features with 0 variance (all observations are the same) - they don't provide any useful information."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_labels = num_df.columns[num_df.std() == 0]\nnum_df.drop(columns = drop_labels, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also analyze all the numerical features that have discrete values from a very short range just like we analyze categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"potential_cat_df = num_df[num_df.columns[num_df.nunique() <= 5]].astype('str')\nreduced_num_df = num_df.drop(columns=potential_cat_df.columns)\next_cat_df = pd.concat([cat_df, potential_cat_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect statistics about categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as we did above, we delete Over18 column as every employee from the dataset is older than 18."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_labels = cat_df.columns[cat_df.nunique() == 1]\ncat_df.drop(columns = drop_labels, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution plots for numerical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_dist_plot(feature):\n    sns.distplot(df[feature])\ninteract(num_dist_plot, feature=reduced_num_df.columns);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another column that looks like it might not be necesarry for data analysis based on the distribution plot is EmployeeNumber. The meaning of this column is probably Employee ID."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_df[\"EmployeeNumber\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it should be for an ID number every employee has unique one, so this attribute is redundant."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_df.drop(columns = \"EmployeeNumber\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see correlation between the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = num_df.corr()\nmask = np.array(corr)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data = corr, mask = mask, square = True, annot = True, cbar = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features are predictably strongly correlated like: total working years with income and age, etc.\n\nBut some of them are unexpectedly not correlated, that is:\n* monthly, hourly, daily rates and monthly income - previously if we assumed these rates meant pay per time period (hour, ...) it would suggest strong correlation. Hard to guess the meaning of those features without prior knowledge about the dataset.\n* maybe less unexpected, job involvement and satisfaction is not related with salary."},{"metadata":{},"cell_type":"markdown","source":"Now we'll analyze the relation between the target variable and other attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.concat([cat_df[\"Attrition\"], reduced_num_df], axis=1)\ndef boxplot_numerical_target(feature):\n    sns.boxplot(x=\"Attrition\", y=feature, data=temp_df)\n\ninteract(boxplot_numerical_target, feature=reduced_num_df.columns);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the plots above we can see that most of the features have some impact on attrition, but there are some like: training times last year, monthly/hourly rate, that look like they have no influence on the target variable. Because of that we can drop them (and also daily rate, even though there's some dominance of people with low daily rate that experience attrition, but as it is probably connected with two other attributes it might just be a coincidence. Without prior knowledge about the dataset we can't really understand the meaning of this feature, which might be another reason to drop these columns)."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_df.drop(columns=[\"DailyRate\", \"HourlyRate\", \"MonthlyRate\", \"TrainingTimesLastYear\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count plots for categorical features with attrition and cross-tabulation of these two factors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def relation_to_attrition(feature):\n    grouped = ext_cat_df.groupby([feature, \"Attrition\"])[\"Attrition\"].count().unstack()\n    grouped.plot(kind=\"bar\", stacked=True)\n    xtab = pd.crosstab(columns=ext_cat_df.Attrition, index=ext_cat_df[feature], margins=True, normalize='index')\n    table = plt.table(cellText=np.round(xtab.values, 3), rowLabels=xtab.index,\n            colLabels=xtab.columns, loc='top', cellLoc='center')\n    table.auto_set_column_width(range(xtab.columns.size))\n    fig=plt.gcf()\n    fig.set_size_inches(8,6)\n\ninteract(relation_to_attrition, feature=ext_cat_df.columns.drop(\"Attrition\"));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the above figures we could propose couple of hypotheses (at least the more evident ones):\n1. More frequently traveling employees are more likely to experience attrition\n2. Employees working overtime are more likely to ...\n3. Women are less likely to ...\n4. Employees that are single are more likely to ...\n5. Stressed employees and those in a weaker mental condition (ones that are giving lowest scores in tests related with situation at work, outside of work, work-life balance) "},{"metadata":{},"cell_type":"markdown","source":"Features that are left will be used to train a model to predict employee attrition."},{"metadata":{"trusted":true},"cell_type":"code","source":"selected = pd.concat([cat_df, num_df], axis=1)\nselected.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Test hypothesis"},{"metadata":{},"cell_type":"markdown","source":"Let's check if men earn more (on similar positions).\n* The null hypothesis H0: women earn equal to men\n* Alternative hypothesis H1: women don't earn equal to men\n\nTo check if we can reject null hypothesis we can use two-tailed t-test. Scipy function used here to calculate this test statistic returns signed values, so if the null hypothesis is rejected we can determine which group earns more."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"hyp_df = selected[['Gender', 'JobRole', 'MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany']]\nhyp_df.info()\nfemale = hyp_df[hyp_df.Gender == 'Female']\nmale = hyp_df[hyp_df.Gender == 'Male']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we need to calculate the critical value for the test statistic based on level of significance and degrees of freedom."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom IPython.display import display\n\nalpha = 0.1\ndf = len(hyp_df.index) - 2\n# alpha/2, because it's a two-tailed test\ncrit_val = np.abs(stats.t.ppf(alpha/2, df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check our null hypothesis regardless of any other factor."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Gender', y='MonthlyIncome', data=hyp_df);\nt, p = stats.ttest_ind(female['MonthlyIncome'], male['MonthlyIncome'], equal_var=False)\nif np.abs(t) < crit_val:\n    display(f\"Can't reject null hypothesis (t_val : {t}, p_val : {p})\")\nelse:\n    display(f\"Hypothesis rejected (t_val : {t}, p_val : {p})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It would seem in general women earn around the same as men, but we could also check if there is some difference in wages for people at certain position."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"JobRole\", y=\"MonthlyIncome\", hue=\"Gender\", data=hyp_df);\nfig=plt.gcf()\nfig.set_size_inches(8, 8)\n\ndef test_for_position(position):\n    dof = len(hyp_df[hyp_df.JobRole == position].index) - 2\n    cv = np.abs(stats.t.ppf(alpha/2, dof))\n    t, p = stats.ttest_ind(female[female.JobRole == position]['MonthlyIncome'], male[male.JobRole == position]['MonthlyIncome'], equal_var=False)\n    if np.abs(t) < cv:\n        display(f\"Can't reject null hypothesis (t_val : {t}, p_val : {p})\")\n    else:\n        display(f\"Hypothesis rejected (t_val : {t}, p_val : {p})\")\n\ninteract(test_for_position, position=hyp_df[\"JobRole\"].unique());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all the job roles except one we can't reject the null hypothesis. Only women working as Research Directors are statistically proven to earn less than men on the same position. The difference in wages seems pretty evident when we look at the box plot above.\n\nLet's see if this difference in salaries can be explained by other factors that have strong correlation with monthly income, specifically total working years and years worked in this company."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\nfig.set_size_inches(12, 5)\nsns.boxplot(y=\"YearsAtCompany\", x=\"Gender\", data=hyp_df[hyp_df.JobRole == 'Research Director'], ax=ax[0]);\nsns.boxplot(y=\"TotalWorkingYears\", x=\"Gender\", data=hyp_df[hyp_df.JobRole == 'Research Director'], ax=ax[1]);\n\nhyp_df[hyp_df.JobRole == 'Research Director'].groupby('Gender')[['YearsAtCompany', 'TotalWorkingYears']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One factor that could explain the difference between the wages is the fact that men on this position have generally more years of experience.\n\nOn the other hand women Research Directors are generally working longer in this specific company, and this kind of commitment is usually rewarded by companies with higher salary.\n\nEven though monthly income is more strongly correlated with total working years (which could explain men's higher salaries), the factor that causes difference in wages may still lie outside of this dataset."},{"metadata":{},"cell_type":"markdown","source":"## 3. Prepare training and test datasets"},{"metadata":{},"cell_type":"markdown","source":"After exploratory analysis of the dataset we are left only with features that are potentially useful for predicting if the employee will experience attrition. First we have to encode the categorical features so they can be used to train a model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ncat_mask = selected.dtypes==object\ncat_cols = selected.columns[cat_mask].tolist()\n\nselected[cat_cols] = selected[cat_cols].apply(lambda col: le.fit_transform(col))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will split the dataset in two parts: training and testing (80-20 split)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# could be also done with sklearn.model_selection.train_test_split\nmask = np.random.rand(len(selected)) < 0.8\ntrain = selected[mask]\ntest = selected[~mask]\n\ny_train = train['Attrition']\nx_train = train.drop(columns='Attrition')\n\ny_test = test['Attrition']\nx_test = test.drop(columns='Attrition')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\noversampler=SMOTE(random_state=1234)\nx_train_smote,  y_train_smote = oversampler.fit_resample(x_train,y_train)\nx_train_smote = pd.DataFrame(data=x_train_smote, columns=x_train.columns)\ny_train_smote = pd.Series(data=y_train_smote)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will train a model for predicting employee attrition. The one used below is LightGBM implementation of gradient boosting with decision trees as a weak learner. We will also look for the optimal hyperparameters for the model using grid search which chooses the best performing model based on the results of cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n        'num_iterations' : [50, 200, 500, 1000],\n        'learning_rate' : [0.05, 0.1, 0.25],\n        'subsample': [0.2, 0.4, 0.6, 0.8],\n        'num_leaves': [4, 6, 10, 20, 50]\n        }\n\ngsearch_LGBM = GridSearchCV(estimator=lgb.LGBMClassifier(), param_grid=params,\n                            scoring='recall', n_jobs=-1, cv=5)\n\ngsearch_XGB  = GridSearchCV(estimator=xgb.XGBClassifier(), param_grid=params,\n                            scoring='recall', n_jobs=-1, cv=5)\n\n%time gsearch_LGBM.fit(x_train_smote, y_train_smote);\n%time gsearch_XGB.fit(x_train_smote, y_train_smote);\nprint(\"Training finished\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try the best performing model on our test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nimport json\n\npred_LGBM = gsearch_LGBM.predict(x_test) > 0.5\npred_XGB = gsearch_XGB.predict(x_test) > 0.5\npreds = {\"XGB\" : pred_XGB, \"LGBM\" : pred_LGBM}\nmetrics = {}\nfor k, v in preds.items():\n    metrics[k] = {'acc' : accuracy_score(y_test, v), 'prec' : precision_score(y_test, v),\n                  'rec' : recall_score(y_test, v),   'roc' : roc_auc_score(y_test, v)}\n\nprint(f'XGB params: {gsearch_XGB.best_params_}')\nprint(f'XGB score : {gsearch_XGB.best_score_}')\nprint('XGB scores: {}'.format(json.dumps(metrics['XGB'], indent=4)))\nprint(f'LGBM params : {gsearch_LGBM.best_params_}')\nprint(f'LGBM score : {gsearch_LGBM.best_score_}')\nprint('LGBM scores: {}'.format(json.dumps(metrics['LGBM'], indent=4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case we want a model optimized for recall, so that we will predict as much attrition as possible and we don't care that much about false positives. That way it may be possible to handle the cases of employees that are probable to leave the company, before they do it.\n\nThe best model (XGBoost) predicts around 50% of employees that will leave the company."},{"metadata":{},"cell_type":"markdown","source":"Below we can see the analysis of feature importance on the model prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(gsearch_LGBM.best_estimator_, figsize=(6, 6), title='LGBM');\nax = xgb.plot_importance(gsearch_XGB.best_estimator_, title='XGB');\nfig = ax.figure\nfig.set_size_inches(6, 6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we could see from the exploratory analysis the most important features include: monthly income, distance from home, working over time, job and environment satisfaction. Performance rating of an employee and department in which his currentyl working has almost no effect on the output of the model so we could have dropped these features before training our model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}