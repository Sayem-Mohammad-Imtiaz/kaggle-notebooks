{"cells":[{"metadata":{"_uuid":"94616290dc68b7b74f9da465f049c7e084ff8f99"},"cell_type":"markdown","source":"# Understanding decision trees"},{"metadata":{"_uuid":"c95cda437c866f56f02c556b45fe306448c26291"},"cell_type":"markdown","source":"This kernel was inspired by the kernel - [Do You Have Spinal Disease? Decision Tree in R](https://www.kaggle.com/petrkajzar/do-you-have-spinal-disease-decision-tree-in-r)"},{"metadata":{"_uuid":"67221b7cda261e5253bccfda13630e6d2c4a33b8"},"cell_type":"markdown","source":"## What are Decision Trees?\n>A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).\n\n*(Source: [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree#Overview))*\n\nIn simpler terms, a decision tree checks if an attribute or a set of attributes satisfy a condition and based on the result of the check, the subsequent checks are performed. The tree splits the data into different parts based these checks."},{"metadata":{"_uuid":"3cd88f5d55228e32a2b26d7ccca10cc67d1a84e9"},"cell_type":"markdown","source":"## What is achieved in this kernel?\nThe following are achieved in this dataset\n* Loading the data\n* Visualizing the data using a correlaton matrix and a pair plot\n* Building a Decision Tree Classifier\n* Determining the accuracy of the model using a confusion matrix\n* Viualizing the Decision tree as a flow chart"},{"metadata":{"_uuid":"b1405ba600ccac1fcf1eeb4b76f1d5be51aa69b4"},"cell_type":"markdown","source":"## Importing the necessary libraries"},{"metadata":{"trusted":true,"_uuid":"3e461e95a55db9379c25e94ec948e2d2319fbb11"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport tflearn.data_utils as du\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/column_3C_weka.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e6d0cebe859424c31e4a67c36b07719d4e95022"},"cell_type":"markdown","source":"The dataset used here is the [Biomechanical features of orthopedic patients](https://www.kaggle.com/uciml/biomechanical-features-of-orthopedic-patients)"},{"metadata":{"trusted":true,"_uuid":"e128551227c22cbb16b8522ca7109cf350216361"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e8a0424dbbdfa2a86897672407cd4a32c6b4500"},"cell_type":"markdown","source":"## What is correlation?\nCorrelation is a statistical term which in common usage refers to how close two variables are to having a linear relationship with each other. \n\nFor example, two variable which are linearly dependent (say, x and y which depend on each other as x = 2y) will have a higher correlation than two variables which are non-linearly dependent (say, u and v which depend on each other as u = v<sup>2</sup>)"},{"metadata":{"trusted":true,"_uuid":"0b19a5491d71a1b2b0925ce9131e70b5c8c74726"},"cell_type":"code","source":"# Calculating the correlation matrix\ncorr = data.corr()\n# Generating a heatmap\nsns.heatmap(corr,xticklabels=corr.columns, yticklabels=corr.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"129d4e02956085cfbb5693830fdbb59bf2b5341c"},"cell_type":"code","source":"sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbbfe28eb9189222f860c88486868d879efa44d1"},"cell_type":"markdown","source":"In the above two plots you can clearly see that the pairs of independent variables with a higher correlation have a more linear scatter plot than the independent variables having a relatively lesser correlation"},{"metadata":{"_uuid":"3ceabb0453e3c0d9b32b56044b6c090b016477f4"},"cell_type":"markdown","source":"## Splitting the dataset into independent (x) and dependent (y) variables"},{"metadata":{"trusted":true,"_uuid":"515667997ff5bb5707609de32500ba45d310655b"},"cell_type":"code","source":"x = data.iloc[:,:6].values\ny = data.iloc[:,6].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e667932ff39cc372084a97d9bf0420dd25cc1b4c"},"cell_type":"markdown","source":"## Splitting the dataset into train and test data\nThe train data to train the model and the test data to validate the model's performance"},{"metadata":{"trusted":true,"_uuid":"ced97d82ec0f84df83122482bb076ad7615aa4c7"},"cell_type":"code","source":"x_train , x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08df95a6a90f0298c808e9dff6a87111c84f3510"},"cell_type":"markdown","source":"## Scaling the independent variables\n[This](https://stackoverflow.com/questions/26225344/why-feature-scaling#26229427) question on stackoverflow has responses which gives a brief explanation on why scaling is necessary and how it can affect the model"},{"metadata":{"trusted":true,"_uuid":"636794fe5eccae47c37e1e8c19aa8726833c57c4"},"cell_type":"code","source":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3cce41716a6303d27d702c045d5bbad9938d1c3"},"cell_type":"markdown","source":"## Building the Decision tree\nThe criterion here is `entropy`. The criterion parameter detemines the function to measure the quality of a split. When the `entropy` is used as a criterion, each split tries to reduce the randomness in that part of the data.\n\nThe another parameter used is the `max_depth`. This determines how deep a tree can go. The affect of this parameter on the model will be discusses later in this notebook"},{"metadata":{"trusted":true,"_uuid":"c683b7384e312739fec16d4ce23457b4730728b3"},"cell_type":"code","source":"classifier = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4)\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d60c7305f198aee9728aaed9ac176faa8a4ced01"},"cell_type":"markdown","source":"## Making the prediction on the test data"},{"metadata":{"trusted":true,"_uuid":"d0ea2e1f49adcffa3dc697d0817609a937f070e5"},"cell_type":"code","source":"y_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c25660b1f69f819fdc3dbe3d96becd54500a8e5e"},"cell_type":"markdown","source":"## What is a confusion matrix?\n>A confusion matrix is a technique for summarizing the performance of a classification algorithm.\nClassification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset.\nCalculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making.\n\n*[Source](https://machinelearningmastery.com/confusion-matrix-machine-learning/)*"},{"metadata":{"trusted":true,"_uuid":"63e021c5a3a50cf97b733caa57ccf760ee2e52f1"},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48fed9307c414621a57ed27511f24c3164f20a88"},"cell_type":"code","source":"accuracy = sum(cm[i][i] for i in range(3)) / y_test.shape[0]\nprint(\"accuracy = \" + str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42aad6419bc8ccf1684ae034b40e2b7c2783b65e"},"cell_type":"markdown","source":"## Visualizing the Decision Tree"},{"metadata":{"trusted":true,"_uuid":"dd25e410df607e90ea3ff2953f2c5ac2e800dce7"},"cell_type":"code","source":"dot_data = StringIO()\n\nexport_graphviz(classifier, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71e21045288201feb5439289fdeba9b438a0e82c"},"cell_type":"markdown","source":"## Building a model without the `max_depth` parameter"},{"metadata":{"trusted":true,"_uuid":"7a99bf2be60694cc06a3b60f566a8ae0ee2fa959"},"cell_type":"code","source":"classifier2 = DecisionTreeClassifier(criterion = 'entropy')\nclassifier2.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab7fd12ac3fc8e4a4012d0f14e850feec92eb42f"},"cell_type":"code","source":"y_pred2 = classifier2.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d046794b158a929f59560638cf110a6b6b90eaba"},"cell_type":"code","source":"cm2 = confusion_matrix(y_test, y_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebf371e6aa5ac893433d87751771468386fa82a0"},"cell_type":"code","source":"accuracy2 = sum(cm2[i][i] for i in range(3)) / y_test.shape[0]\nprint(\"accuracy = \" + str(accuracy2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6b3eb6e7ee2f20073cc7c15dc0e2e389cdb7658"},"cell_type":"markdown","source":"## Visualizing the decision tree without the `max_depth` parameter"},{"metadata":{"trusted":true,"_uuid":"3ba89d94acd23ebcb0f652e551aeec5acf4be9e4"},"cell_type":"code","source":"dot_data = StringIO()\n\nexport_graphviz(classifier2, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c7cc73bdf0727285545cc0a4305d103b8607227"},"cell_type":"markdown","source":"Now, consider the leaf nodes (terminal nodes) of the tree with and without the `max_depth` paratemer. You will notice that the entropy all the terminal nodes are zero in the tree without the `max_depth` parameter and non zero in three with that parameter. This is because when the parameter is not mentioned, the split recursively takes place till the terminal node has an entropy of zero."},{"metadata":{"_uuid":"9e4ff8066e43d28a8712eda37f3cc36f67957a92"},"cell_type":"markdown","source":"To know more about the different paramteres of the `sklearn.tree.DecisionTreeClassifier`, click [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}