{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Eksploratory Data Analysis (EDA) \n**Pendekatan Untuk Menganalisis Set Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sebaran Data Film dan TV Show Di Netflix"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar, ax = plt.subplots(figsize = (12,12))\nplt.pie(data['type'].value_counts(), labels = data['type'].value_counts().index, autopct=\"%.1f%%\")\nplt.title('Sebaran Data Film dan Tv Show di Netflix', size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Banyaknya Rilis Dalam Tahun"},{"metadata":{"trusted":true},"cell_type":"code","source":"bar, ax = plt.subplots(figsize = (10,10))\nsns.barplot(x = data['release_year'].value_counts().index[:5], y = data['release_year'].value_counts()[:5])\nplt.xlabel('Tahun')\nplt.ylabel('Banyaknya')\nplt.title('Banyaknya Rilis Dalam Tahun')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_data = data[data['type'] == 'Movie']\ntv_show_data = data[data['type'] == 'TV Show']\n# bar,ax = plt.subplots(1,2,figsize=(10,10))\ntemp = data[['type', 'release_year']]\ntemp = temp.value_counts().to_frame()\ntemp.reset_index(level=[0,1], inplace=True)\ntemp = temp.rename(columns = {0:'count'})\ntemp = pd.concat([temp[temp['type'] == 'Movie'][:5], temp[temp['type']== 'TV Show'][:5]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ax, bar = plt.subplots(figsize = (10,10))\nsns.catplot(x = 'release_year', y = 'count', hue = 'type', data = temp, kind = 'point')\nplt.xlabel('Tahun Rilis')\nplt.ylabel('Banyaknya')\nplt.title('Banyaknya Rilis pertahun dalam film dan Tv Show', size=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Genre Paling Populer dalam 10 Tahun"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = list()\nclean_data = data.dropna()\nclean_data.reset_index(inplace=True)\nfor ind, element in clean_data.iterrows():\n    type_show = element['release_year']\n    for cast in str(element['listed_in']).split(','):\n        temp.append([type_show, cast])\ncast_data = pd.DataFrame(temp, columns= ['release_year', 'cast'])\ncast_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cast = cast_data.value_counts().to_frame()\ncast.reset_index(level=[0,1], inplace=True)\ncast = cast.rename(columns = {0:'count'})\n\n\nyears = [2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]\nyear_data = list()\nfor year in years:\n    temp1 = cast[cast['release_year'] == year].iloc[0,:]\n    temp2 = cast[cast['release_year'] == year].iloc[1,:]\n    year_data.append(list(temp1))\n    year_data.append(list(temp2))\n    \nyear = pd.DataFrame(year_data, columns=('years', 'genre', 'count'))\nbar, ax = plt.subplots(figsize=(10,10))\nsns.barplot(x = 'years', y ='count', hue='genre', data = year)\nplt.xlabel('Tahun')\nplt.ylabel('Banyaknya')\nplt.title('Genre Paling Populer dalam 10 Tahun', size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predisksi Rating\n**Menggunakan Algoritma Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data():\n    movies = pd.read_csv('../input/netflix-shows/netflix_titles.csv')\n    \n    # Replace NaN values with empty string\n    movies.dropna(inplace=True)\n    \n    # Uniformisation of labels with same signification\n    movies['rating'].replace(to_replace='PG-13', value='TV-PG', inplace=True)\n    movies['rating'].replace(to_replace='PG', value='TV-PG', inplace=True)\n    movies['rating'].replace(to_replace='TV-Y7-FV', value='TV-Y7', inplace=True)\n    movies['rating'].replace(to_replace='G', value='TV-G', inplace=True)\n    movies['rating'].replace(to_replace='NC-17', value='R', inplace=True)\n    # Drop rows that don't have any labels (NR = UR = Unrated)\n    movies.drop(movies[movies['rating']=='NR'].index, inplace=True)\n    movies.drop(movies[movies['rating']==''].index, inplace=True)\n    movies.drop(movies[movies['rating']=='UR'].index, inplace=True)\n    #Drop useless labels\n    movies.drop(movies[movies['rating']=='TV-G'].index, inplace=True)\n    movies.drop(movies[movies['rating']=='TV-Y'].index, inplace=True)\n    movies.drop(movies[movies['rating']=='TV-Y7'].index, inplace=True)\n\n\n    #Drop useless colmns\n    movies = movies[[\"type\", \"title\", \"description\", \"director\", \"cast\", 'rating', \"listed_in\", \"country\"]]\n    \n    return movies","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediksi Menggunakan Algoritma Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = get_data()\n\n#Plot label repartition\nfig = plt.figure(figsize=(8,6))\nmovies.groupby('rating').title.count().plot.bar(ylim=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert labels to int and create a new column\nmovies['category_id'] = movies['rating'].factorize()[0]\n#Get unique values\ncategory_id_df = movies[[\"rating\", \"category_id\"]].drop_duplicates().sort_values('category_id')\n#Dicts associating rating and its int value\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'rating']].values)\nlabels = movies.category_id\n\n#Group all the columns together\nmovies ['txt'] = movies['type'] + \" \" + movies['title'] + \" \" + movies['description'] + \" \" + movies['listed_in'] \nmovies['txt'] = movies['txt'] + \" \" + movies['director'] + \" \" + movies['cast'] + \" \" + movies[\"country\"]\nmovies = movies[['rating', 'txt']]\n\n#Cross val\nmskf = KFold(n_splits = 15, random_state=0, shuffle = True)\nacc = []\n\nfor train_index, test_index in mskf.split(movies['txt']) :\n    X_train, X_test = movies['txt'].iloc[train_index], movies['txt'].iloc[test_index]\n    y_train, y_test = movies['rating'].iloc[train_index], movies['rating'].iloc[test_index]\n    \n    #Preprocess\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(X_train)\n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n    #Fit\n    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n    \n    #score\n    acc.append(clf.score(count_vect.transform(X_test.values), y_test.values))\n    \nplot_confusion_matrix(clf, count_vect.transform(X_test), np.array(y_test.values))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy with Naive Bayes: \" + str(np.mean(acc) * 100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mencoba meningkatkan akurasi terutama mencoba memprediksi beberapa film 'R'. Untuk ini akan melihat korelasi antara kolom dan label menggunakan chi2 independency test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = get_data().drop(['rating'], axis = 1)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n\nfor rating, category_id in sorted(category_to_id.items()):\n    print(\"######### '{}':\".format(rating))\n    for col in movies.columns :\n        features = tfidf.fit_transform(movies[col]).toarray()\n        features_chi2 = chi2(features, labels == category_id)\n        mean = np.mean(features_chi2[0])\n        print(\"    '{}':\".format(col))\n        print(\"        {}\".format(mean*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = get_data()\n#Group all the columns together\nmovies ['txt'] = movies['director'] + \" \" + movies['listed_in'] + \" \" + movies['type'] + \" \" + movies['country'] + \" \" + movies['cast'] \nmovies = movies[['rating', 'txt']]\n\n\n#Cross val\nmskf = KFold(n_splits = 15, random_state=0, shuffle = True)\nacc = []\n\nfor train_index, test_index in mskf.split(movies['txt']) :\n    X_train, X_test = movies['txt'].iloc[train_index], movies['txt'].iloc[test_index]\n    y_train, y_test = movies['rating'].iloc[train_index], movies['rating'].iloc[test_index]\n    \n    #Preprocess\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(X_train)\n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n    \n    #Fit\n    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n    \n    #score\n    acc.append(clf.score(count_vect.transform(X_test.values), y_test.values))\n    \nplot_confusion_matrix(clf, count_vect.transform(X_test), np.array(y_test.values))\nplt.show()\n\nprint(\"Accuracy with Naive Bayes on the whole dataset: \" + str(clf.score(count_vect.transform(movies['txt'].values), movies['rating'].values) * 100) + \"%\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Konten Berdasarkan Rekomendasi Sistem\n**Menggunakan Cosine Similarity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize  \nimport nltk\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['director'] = data['director'].fillna('')\ndata['cast'] = data['cast'].fillna('')\ndata['text'] = data['title'] + ' '+data['director'] + ' '+ data['cast']+ ' ' +data['listed_in'] + ' '+data['description']\n\ndef preprocess(text):\n    text = re.sub('[^A-z]', ' ', text)\n    stop_words = set(stopwords.words('english'))  \n    word_tokens = word_tokenize(text)  \n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    \n    filtered_sentence = []  \n    for w in word_tokens:  \n        if w not in stop_words:  \n            filtered_sentence.append(lemmatizer.lemmatize(w))\n    filtered = ' '.join([x for x in filtered_sentence])\n    return filtered.lower().strip()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ntext_features = vectorizer.fit_transform(data['text'])\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity_matrix = cosine_similarity(text_features)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_recommendation(movie_name):\n        movie_index = data[data['title'] == movie_name].index\n        movie_similarity = similarity_matrix[movie_index]\n        movie_data = pd.DataFrame({'cosine_similarity':movie_similarity[0], 'index':np.arange(6234)})\n        movie_data = movie_data.sort_values(by = 'cosine_similarity', ascending = False)\n        topn=10\n        movie_ids = movie_data['index'][1:topn]\n        recommendation_movies = list()\n        for temp in movie_ids:\n            movie = data['title'][temp]\n            recommendation_movies.append(movie)\n        return  recommendation_movies\n        \nget_recommendation('Transformers: Robots in Disguise')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}