{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nHai, today are another episode of classification. This time we are going to deal with imbalance dataset using oversampling method from imblearn. As for algorithm we are going to use Random Forest, Logistic Regression, and SGD Classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Modules","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(rc={'figure.figsize':(13,13)})\nsns.set_style(\"whitegrid\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quick Look","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/income-classification/income_evaluation.csv', skipinitialspace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income By Age\nax = sns.boxplot(x=\"income\", y=\"age\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income By age and race\nax = sns.boxplot(x=\"income\", y=\"age\", hue=\"race\",\n                 data=df, palette=\"Set2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income by age, race and sex\ng = sns.catplot(x=\"income\", y=\"age\",\n                hue=\"sex\", col=\"race\",\n                col_wrap=3,\n                data=df, kind=\"box\",\n                palette=\"Set3\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income by age, native country and sex\ng = sns.catplot(x=\"income\", y=\"age\",\n                hue=\"sex\", col=\"native-country\",\n                col_wrap=3,\n                data=df, kind=\"box\",\n                palette=\"vlag\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income by age and hours per week\nax = sns.scatterplot(x=\"age\", y=\"hours-per-week\", hue=\"income\",\n                     data=df, palette='prism')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income by age, hours per week and occupation\ng = sns.relplot(x=\"age\", y=\"hours-per-week\",\n                 col=\"occupation\", col_wrap=3, hue=\"income\",\n                 kind=\"scatter\", data=df, palette='rocket')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income by age, hours per week and education\ng = sns.relplot(x=\"age\", y=\"hours-per-week\",\n                 col=\"education\", col_wrap=3, hue=\"income\",\n                 kind=\"scatter\", data=df, palette='seismic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income by age, hours per week and marital status\ng = sns.relplot(x=\"age\", y=\"hours-per-week\",\n                 col=\"marital-status\", col_wrap=3, hue=\"income\",\n                 kind=\"scatter\", data=df, palette='gist_heat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Income by age, hours per week and relationship\ng = sns.relplot(x=\"age\", y=\"hours-per-week\",\n                 col=\"relationship\", col_wrap=3, hue=\"income\",\n                 kind=\"scatter\", data=df, palette='Oranges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\nLet's change categorical feature to numerical.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf.income = labelencoder.fit_transform(df.income)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split Data\n\nX = df.drop('income', 1)\ny = df.income\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Oversampling\n\nJust as i explain before, today we are going to use oversampling method. There are Random Oversampler, SMOTE and ADASYN. So what is the difference, while the RandomOverSampler is over-sampling by duplicating some of the original samples of the minority class, SMOTE and ADASYN generate new samples in by interpolation. However, the samples used to interpolate/generate new synthetic samples differ. In fact, ADASYN focuses on generating samples next to the original samples which are wrongly classified using a k-Nearest Neighbors classifier while the basic implementation of SMOTE will not makeany distinction between easy and hard samples to be classified using the nearest neighbors rule. SMOTE might connect inliers and outliers while ADASYN might focus solely on outliers which, in both cases, might lead to a sub-optimal decision function\n\nFor more about Oversampling :\n\nhttps://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Oversample\n\ndf.income.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Oversample with Random Oversmpling\nfrom imblearn.over_sampling import RandomOverSampler\n\noversample = RandomOverSampler()\nXTRO, YTRO = oversample.fit_resample(X_train, y_train)\n\n# Check value distribution\nYTRO.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Oversample with SMOTE\nfrom imblearn.over_sampling import SMOTE\n\noversample = SMOTE()\nXTS, YTS = oversample.fit_resample(X_train, y_train)\n\n# Check value distribution\nYTS.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Oversample with ADASYN\nfrom imblearn.over_sampling import ADASYN\n\noversample = ADASYN()\nXTA, YTA = oversample.fit_resample(X_train, y_train)\n\n# Check value distribution\nYTA.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n\nThere is 3 algorithm i'm using today, there is Random Forest, Logistic regression, and SGD Classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Oversampling = ['Random OverSampler','SMOTE','ADASYN']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest\nRF = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 12)\nRFs = []\n\n### Random Oversampling\nRF.fit(XTRO, YTRO)\npred = RF.predict(X_test)\nactual = np.array(y_test)\nRF_RO = accuracy_score(actual, pred)\nRFs.append(RF_RO)\n\nprint('Random Oversampling')\nprint('Accuracy Score :', RF_RO)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### SMOTE\nRF.fit(XTS, YTS)\npred = RF.predict(X_test)\nactual = np.array(y_test)\nRF_S = accuracy_score(actual, pred)\nRFs.append(RF_S)\n\nprint('SMOTE')\nprint('Accuracy Score :', RF_S)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### ADASYN\nRF.fit(XTA, YTA)\npred = RF.predict(X_test)\nactual = np.array(y_test)\nRF_A = accuracy_score(actual, pred)\nRFs.append(RF_A)\n\nprint('ADASYN')\nprint('Accuracy Score :', RF_A)\nprint('Report : ')\nprint(classification_report(actual, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression\nLR = LogisticRegression()\nLRs = []\n\n### Random Oversampling\nLR.fit(XTRO, YTRO)\npred = LR.predict(X_test)\nactual = np.array(y_test)\nLR_RO = accuracy_score(actual, pred)\nLRs.append(LR_RO)\n\nprint('Random Oversampling')\nprint('Accuracy Score :', LR_RO)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### SMOTE\nLR.fit(XTS, YTS)\npred = LR.predict(X_test)\nactual = np.array(y_test)\nLR_S = accuracy_score(actual, pred)\nLRs.append(LR_S)\n\nprint('SMOTE')\nprint('Accuracy Score :', LR_S)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### ADASYN\nLR.fit(XTA, YTA)\npred = LR.predict(X_test)\nactual = np.array(y_test)\nLR_A = accuracy_score(actual, pred)\nLRs.append(LR_A)\n\nprint('ADASYN')\nprint('Accuracy Score :', LR_A)\nprint('Report : ')\nprint(classification_report(actual, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SGD Classifier\nSGD = SGDClassifier(loss='modified_huber')\nSGDs = []\n\n### Random Oversampling\nSGD.fit(XTRO, YTRO)\npred = SGD.predict(X_test)\nactual = np.array(y_test)\nSGD_RO = accuracy_score(actual, pred)\nSGDs.append(SGD_RO)\n\nprint('Random Oversampling')\nprint('Accuracy Score :', SGD_RO)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### SMOTE\nSGD.fit(XTS, YTS)\npred = SGD.predict(X_test)\nactual = np.array(y_test)\nSGD_S = accuracy_score(actual, pred)\nSGDs.append(SGD_S)\n\nprint('SMOTE')\nprint('Accuracy Score :', SGD_S)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### ADASYN\nSGD.fit(XTA, YTA)\npred = SGD.predict(X_test)\nactual = np.array(y_test)\nSGD_A = accuracy_score(actual, pred)\nSGDs.append(SGD_A)\n\nprint('ADASYN')\nprint('Accuracy Score :', SGD_A)\nprint('Report : ')\nprint(classification_report(actual, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame({'Random Forest': RFs,\n                       'Logistic Regression': LRs,'SGD Classifier': SGDs},\n                         index = Oversampling)\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\nFrom all this experiment, the best algorithm is Random Forest since its always come with a good accuracy no matter the oversampler method is. And for the oversample method, the best method is SMOTE.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result.plot(figsize=(12,8));","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}