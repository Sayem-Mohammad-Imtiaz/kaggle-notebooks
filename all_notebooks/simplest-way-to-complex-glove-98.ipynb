{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries and Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy import spatial\nfrom nltk.corpus import stopwords\n\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers import Input, Dense\nfrom keras.models import Sequential\n\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/atis-airlinetravelinformationsystem/atis_intents_train.csv\", header=None)\ntest = pd.read_csv(\"../input/atis-airlinetravelinformationsystem/atis_intents_test.csv\", header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"words = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor elem in iter(words):\n    count = count + 1\n    if count == 20:\n        break\n    print (elem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stopwords Corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (words)]))\ntest['text'] = test[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (words)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Digits Removal \\d+"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].str.replace('\\d+', '')\ntest['text'] = test['text'].str.replace('\\d+', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train['text']\nlabels = train[0]\ntest_text = test['text']\ntest_labels = test[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize and Padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntok = Tokenizer()\ntok.fit_on_texts(text)\nword_index = tok.word_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indexed each words as there are 631 chars, words are listed to 0-631"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_vocab_size = len(word_index) + 1\ninput_length = 25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_tokens = tok.texts_to_sequences(text)\ntest_data_tokens = tok.texts_to_sequences(test_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenized each word based off of word index"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_tokens[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = pad_sequences(train_data_tokens, input_length)\ntest_input = pad_sequences(test_data_tokens, input_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Padded each sentence (text) to the same size of 25"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Hot Encode with LabelEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_transformer = preprocessing.LabelEncoder()\nlabel_transformer.fit(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.externals import joblib\n# joblib.dump(label_transformer, 'atis-airlinetravelinformationsystem/label_encoder.pk1')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = label_transformer.transform(labels)\ntest_labels = label_transformer.transform(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = to_categorical(np.asarray(labels))\ntest_labels = to_categorical(np.asarray(test_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Validation Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train_input, labels, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Embeddings - Vector Representations"},{"metadata":{},"cell_type":"markdown","source":"Embedded Index saves the info from pretrained GloVe model which can be later used for word embedding in terms of its\napplication to our specific model. Our embedded matrix is first matrix of zeros, and then updated according to the \nour dataset-GloVe dataset comparison."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_dim = 300\nembedded_index = dict()\n\nwith open('../input/glove42b300dtxt/glove.42B.300d.txt', 'r', encoding='utf-8') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embedded_index[word] = vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove.close","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_matrix = np.zeros((max_vocab_size, embedded_dim))\nfor x, i in word_index.items():\n    vector = embedded_index.get(x)\n    if vector is not None:\n        embedded_matrix[i] = vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN for NLP task"},{"metadata":{},"cell_type":"markdown","source":"As words and their sequence are important for NLP solutions, pixels and their order are also essential and something valubale to keep in mind while training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_vocab_size, 300, input_length=input_length, weights=[embedded_matrix], trainable=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Conv1D(filters=32, kernel_size=8, activation='selu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='selu'))\nmodel.add(Dense(8, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc(y_true, y_pred):\n    return np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc(test_labels, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Thanks for reading it to the end. Credits to the OpenSourceCommunity"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}