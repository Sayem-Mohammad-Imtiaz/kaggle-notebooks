{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Spam or Ham\n#### \"Spam\"  is a irrelevant or unsolicited messages sent over the internet, typically to a large number of users, for the purposes of advertising, phishing, spreading malware, etc.\"\"Ham\" is e-mail or messages that is not Spam.\n\n#### Here we will be building a model which can detect spam messages using classification model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/smsspamcollection/train.csv')\ndf.columns=['id','label','message']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n# we have only one null value, so thats is not going to make any difference we can ignore that.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(['id'], axis=1)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our main issue with our data is that it is all in text format (strings).So,the classification algorithms for two or more features will not work.Here comes NLP which works on text data and covert it into machine understandable format."},{"metadata":{},"cell_type":"markdown","source":"# Text Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\nimport nltk\nimport re\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STOPWORDS\n#### Stopwords are the most common words in natural language which does not add much value for the purpose of analyzing text data and building NLP models.eg- “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc.\n\n#### In that case we should remove the stopwords present in the text data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n# Stemmer used to give the root words.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\nfor i in range(0, len(df)):\n    review = re.sub('[^a-zA-Z]', ' ',str(df['message'][i]))## replacing all with space other that characters or alphabets \n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]# stemming all the words which are not present in  stopwords \n    review = ' '.join(review)\n    corpus.append(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corpus)# list of stemmed words ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n### We can use methods-\n### 1. CountVectorizer\n### 2. TF-IDF(Term Frequency and Inverse Document Freqency)"},{"metadata":{},"cell_type":"markdown","source":"## CountVectorizer\n###  It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. This is helpful when we have multiple such texts, and we wish to convert each word in each text into vectors such that machine can process data easily. It is used to create Bag of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=2500)\nX1 = cv.fit_transform(corpus).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X1[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see it has contains \"1\" in some places.It will contain values only in integers."},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF\n\n### TF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. \n\n### One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n\n### Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a sentence, divided by the total number of words in that sentence; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the sentences in the corpus divided by the number of sentences containing that word.\n\n### TF: Term Frequency, which measures how frequently a term occurs in a sentence. Since every sentence is different in length, it is possible that a term would appear much more times in long sentences than shorter ones. Thus, the term frequency is often divided by the sentence length (aka. the total number of terms in the sentence) as a way of normalization:\n\n### TF(t) = (Number of repition of words in sentence) / (Total number of words in sentences).\n\n### IDF: Inverse Document Frequency, which measures how important a word is. While computing TF, all words are considered equally important. However it is known that certain words, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n\n### IDF(t) = log_e(Total number of sentences / Number of sentences containing words).\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer()\nX2 = cv.fit_transform(corpus).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X2[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It contains decimal values.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=pd.get_dummies(df['label'])\ny=y.iloc[:,1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# USING CountVectorizer\nfrom sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y, test_size = 0.20, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF\nfrom sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size = 0.20, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB #classification model\nspam_detect_model1 = MultinomialNB().fit(X_train1, y_train1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detect_model2 = MultinomialNB().fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"CountVectorizer\")\nprint('Training Accuracy :',spam_detect_model1.score(X_train1, y_train1))\nprint('Testing Accuracy :',spam_detect_model1.score(X_test1, y_test1))\n\nprint(\"TF-IDF\")\nprint('Training Accuracy :',spam_detect_model2.score(X_train2, y_train2))\nprint('Testing Accuracy :',spam_detect_model2.score(X_test2, y_test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that CountVectorizer gives better accuracy as compared to TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=spam_detect_model1.predict(X_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating confusing matrix\nfrom sklearn.metrics import confusion_matrix\ncon_mat = confusion_matrix(y_test1,y_pred)\n\n\nprint('\\nCONFUSION MATRIX')\nplt.figure(figsize= (6,4))\nsns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THANKS.\n####  upvote if you found it useful"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}