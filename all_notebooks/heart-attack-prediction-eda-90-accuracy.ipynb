{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nimport warnings\nwarnings.simplefilter(action='ignore', category = FutureWarning)\n\nsns.set_style(\"darkgrid\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:23.47493Z","iopub.execute_input":"2021-06-23T01:04:23.47552Z","iopub.status.idle":"2021-06-23T01:04:24.701853Z","shell.execute_reply.started":"2021-06-23T01:04:23.475397Z","shell.execute_reply":"2021-06-23T01:04:24.700381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:24.705098Z","iopub.execute_input":"2021-06-23T01:04:24.70555Z","iopub.status.idle":"2021-06-23T01:04:25.874203Z","shell.execute_reply.started":"2021-06-23T01:04:24.705512Z","shell.execute_reply":"2021-06-23T01:04:25.872669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. General Infos About Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:25.877858Z","iopub.execute_input":"2021-06-23T01:04:25.878209Z","iopub.status.idle":"2021-06-23T01:04:25.953476Z","shell.execute_reply.started":"2021-06-23T01:04:25.878179Z","shell.execute_reply":"2021-06-23T01:04:25.950756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.1 Features\n\n**1. age** - age in years\n\n**2. sex** - sex (1 = male; 0 = female)\n\n**3. cp** - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic)\n\n**4. trestbps** - resting blood pressure (in mm Hg on admission to the hospital)\n\n**5. chol** - serum cholestoral in mg/dl\n\n**6. fbs** - fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n\n**7. restecg** - resting electrocardiographic results (1 = normal; 2 = having ST-T wave abnormality; 0 = hypertrophy)\n\n**8. thalach** - maximum heart rate achieved\n\n**9. exang** - exercise induced angina (1 = yes; 0 = no)\n\n**10. oldpeak** - ST depression induced by exercise relative to rest\n\n**11. slope** - the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping)\n\n**12. ca** - number of major vessels (0-3) colored by flourosopy\n\n**13. thal** - 2 = normal; 1 = fixed defect; 3 = reversable defect\n\n**14. num** - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < diameter narrowing; Value 1 = > 50% diameter narrowing)\n\nThanks to https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/discussion/234843","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:25.955216Z","iopub.execute_input":"2021-06-23T01:04:25.955658Z","iopub.status.idle":"2021-06-23T01:04:25.982464Z","shell.execute_reply.started":"2021-06-23T01:04:25.95559Z","shell.execute_reply":"2021-06-23T01:04:25.981367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:25.985709Z","iopub.execute_input":"2021-06-23T01:04:25.986246Z","iopub.status.idle":"2021-06-23T01:04:26.05118Z","shell.execute_reply.started":"2021-06-23T01:04:25.986209Z","shell.execute_reply":"2021-06-23T01:04:26.049762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our categorical features had encoded with using label encoder or had been considered as ordinal feature. I will remap them with meanings and I will re-encode them with using one-hot encoder.","metadata":{}},{"cell_type":"code","source":"df[\"slp\"] = df[\"slp\"].map({0: \"downsloping\", 1: \"flat\", 2: \"upsloping\"})\ndf[\"thall\"] = df[\"thall\"].map({1: \"fixed_effect\", 2: \"normal\", 3: \"reversable_defect\", 0: \"else\"})","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:26.054035Z","iopub.execute_input":"2021-06-23T01:04:26.054472Z","iopub.status.idle":"2021-06-23T01:04:26.06934Z","shell.execute_reply.started":"2021-06-23T01:04:26.054433Z","shell.execute_reply":"2021-06-23T01:04:26.067582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Target Value","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (8, 8))\nax.pie(df.output.value_counts(), labels=[\"0\", \"1\"], autopct='%1.2f%%', startangle=180)\nax.set_title(\"Target\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:26.071757Z","iopub.execute_input":"2021-06-23T01:04:26.072345Z","iopub.status.idle":"2021-06-23T01:04:26.274451Z","shell.execute_reply.started":"2021-06-23T01:04:26.072302Z","shell.execute_reply":"2021-06-23T01:04:26.272884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target value's distribution is 54.5% - 46.5%. It is balanced. So, we don't have to use stratification techniques for cross validation and splitting the data, or we don't need to applying sampling to the data.","metadata":{}},{"cell_type":"code","source":"cat_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exng\", \"slp\", \"caa\", \"thall\"]\nnum_cols = [\"age\", \"trtbps\", \"chol\", \"thalachh\", \"oldpeak\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:26.276316Z","iopub.execute_input":"2021-06-23T01:04:26.276657Z","iopub.status.idle":"2021-06-23T01:04:26.281894Z","shell.execute_reply.started":"2021-06-23T01:04:26.276618Z","shell.execute_reply":"2021-06-23T01:04:26.280721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. EDA","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Categorical Features","metadata":{}},{"cell_type":"code","source":"def count_percentage(df, col, hue):\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\n    order = sorted(df[col].unique())\n    \n    sns.countplot(col, data = df, hue = hue, ax = ax1, order = order).set_title(\"Counts For Feature:\\n\" + col)\n\n    df_temp = df.groupby(col)[hue].value_counts(normalize = True).\\\n    rename(\"percentage\").\\\n    reset_index()\n\n    fig = sns.barplot(x = col, y = \"percentage\", hue = hue, data = df_temp, ax = ax2, order = order)\n    fig.set_ylim(0,1)\n    \n    fontsize = 14 if len(order) <= 10 else 10\n    for p in fig.patches:\n        txt = str(p.get_height().round(2)) + '%'\n        txt_x = p.get_x() \n        txt_y = p.get_height()\n        fig.text(txt_x + 0.125, txt_y + 0.02,txt, fontsize = fontsize)\n\n    ax2.set_title(\"Percentages For Feature: \\n\" + col)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:26.283824Z","iopub.execute_input":"2021-06-23T01:04:26.284638Z","iopub.status.idle":"2021-06-23T01:04:26.298426Z","shell.execute_reply.started":"2021-06-23T01:04:26.284567Z","shell.execute_reply":"2021-06-23T01:04:26.297083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cat_cols:\n    count_percentage(df, col, \"output\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:26.300225Z","iopub.execute_input":"2021-06-23T01:04:26.300968Z","iopub.status.idle":"2021-06-23T01:04:30.132798Z","shell.execute_reply.started":"2021-06-23T01:04:26.300918Z","shell.execute_reply":"2021-06-23T01:04:30.131725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.1 Takeaways - Categorical Features\n\n**Sex**: Really effective feature. Male's target value is 75%, female's target value is 45%\n\n**cp**: If a person doesn't have asymptomatic chest pain (encoded as 0), target value is at least 70%\n\n**fbs**: This feature looks like ineffective. Target's values are 55% and 51% for two option.\n\n**nestecg**: Target values are 46%, 63% and 25%. This variable could be useful\n\n**exang**: Really effective feature. 70% - 23%\n\n**slp**: 43, 35 and 75 percent. It could be also useful.\n\n**thall**: 50-33-78-24. It could be also useful.","metadata":{}},{"cell_type":"markdown","source":"# 4.2 Numerical Features\n## 4.2.1 Numerical vs Target","metadata":{}},{"cell_type":"code","source":"def feature_dist_clas(df, col, hue):\n    fig, axes = plt.subplots(1, 4, figsize = (25, 5))\n    order = sorted(df[hue].unique())\n\n    sns.histplot(x = col, hue = hue, data = df, ax = axes[0])\n    sns.kdeplot(x = col, hue = hue, data = df, fill = True, ax = axes[1])\n    sns.boxplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[2])\n    sns.violinplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[3])\n    \n    fig.suptitle(\"For Feature:  \" + col)\n    axes[0].set_title(\"Histogram For Feature \" + col)\n    axes[1].set_title(\"KDE Plot For Feature \" + col)   \n    axes[2].set_title(\"Boxplot For Feature \" + col)   \n    axes[3].set_title(\"Violinplot For Feature \" + col)   ","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:30.134197Z","iopub.execute_input":"2021-06-23T01:04:30.134517Z","iopub.status.idle":"2021-06-23T01:04:30.143686Z","shell.execute_reply.started":"2021-06-23T01:04:30.134484Z","shell.execute_reply":"2021-06-23T01:04:30.142829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:\n    feature_dist_clas(df, col, \"output\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:30.14485Z","iopub.execute_input":"2021-06-23T01:04:30.145146Z","iopub.status.idle":"2021-06-23T01:04:35.280037Z","shell.execute_reply.started":"2021-06-23T01:04:30.145119Z","shell.execute_reply":"2021-06-23T01:04:35.278996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_distribution(df, col):\n    \n    skewness = np.round(df[col].skew(), 3)\n    kurtosis = np.round(df[col].kurtosis(), 3)\n\n    fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\n    sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = \"orangered\")\n    sns.boxplot(data = df, y = col, ax = axes[1], color = \"orangered\")\n    stats.probplot(df[col], plot = axes[2])\n\n    axes[0].set_title(\"Distribution \\nSkewness: \" + str(skewness) + \"\\nKurtosis: \" + str(kurtosis))\n    axes[1].set_title(\"Boxplot\")\n    axes[2].set_title(\"Probability Plot\")\n    fig.suptitle(\"For Feature:  \" + col)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:35.281995Z","iopub.execute_input":"2021-06-23T01:04:35.282381Z","iopub.status.idle":"2021-06-23T01:04:35.292889Z","shell.execute_reply.started":"2021-06-23T01:04:35.282346Z","shell.execute_reply":"2021-06-23T01:04:35.291447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:\n    feature_distribution(df, col)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:35.294774Z","iopub.execute_input":"2021-06-23T01:04:35.295171Z","iopub.status.idle":"2021-06-23T01:04:38.537735Z","shell.execute_reply.started":"2021-06-23T01:04:35.295137Z","shell.execute_reply":"2021-06-23T01:04:38.535883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.1 Takeaways - Numerical Features\n\nAge, thalachh and oldpeak would be useful variables.\n\ntrtbps, col and oldpeak have a few outliers. thalachh has just an outlier.\n\nOur numerical variables generally have normal distribution except oldpeak. Also, outliers at chol feature are problem for this feature's normality.","metadata":{}},{"cell_type":"code","source":"def heatmap(df):\n    \n    fig, ax = plt.subplots(figsize = (15, 15))\n    \n    sns.heatmap(df.corr(), cmap = \"coolwarm\", annot = True, fmt = \".2f\", annot_kws = {\"fontsize\": 9},\n                vmin = -1, vmax = 1, square = True, linewidths = 0.8, cbar = False)\n    \nheatmap(df)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:38.539545Z","iopub.execute_input":"2021-06-23T01:04:38.539928Z","iopub.status.idle":"2021-06-23T01:04:39.386553Z","shell.execute_reply.started":"2021-06-23T01:04:38.539897Z","shell.execute_reply":"2021-06-23T01:04:39.385556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Preprocessing\n\nOne hot encoding for two feature,\nSplitting the data,\nDefining cross validations,\nScaling data with using Standard Scaler","metadata":{}},{"cell_type":"code","source":"encode_cols = [\"slp\", \"thall\"]\n\ndummies = pd.get_dummies(df[encode_cols], drop_first = True)\n\nfin = pd.concat([df, dummies], axis = 1).drop(encode_cols, axis = 1)\nfin","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:39.387978Z","iopub.execute_input":"2021-06-23T01:04:39.38853Z","iopub.status.idle":"2021-06-23T01:04:39.422737Z","shell.execute_reply.started":"2021-06-23T01:04:39.388488Z","shell.execute_reply":"2021-06-23T01:04:39.421709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 303 observations, it means we have a small data. 25-75 or 30-70 is ideal for train test proportion.","metadata":{}},{"cell_type":"code","source":"target = \"output\"\npredictors = [col for col in fin.columns if col != target]\n\nX_train, X_test, y_train, y_test = train_test_split(fin[predictors],\n                                                    fin[target],\n                                                    test_size = 0.25,\n                                                    random_state = 42)\n\ncv3 = KFold(n_splits = 3, shuffle = True, random_state = 42)\ncv5 = KFold(n_splits = 5, shuffle = True, random_state = 42)\ncv10 = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\ndef cv_model(model, X = X_train, y = y_train, cv = cv5):\n    return cross_val_score(model, X, y, scoring = \"accuracy\", cv = cv, n_jobs = -1).mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:39.424027Z","iopub.execute_input":"2021-06-23T01:04:39.424576Z","iopub.status.idle":"2021-06-23T01:04:39.437535Z","shell.execute_reply.started":"2021-06-23T01:04:39.424534Z","shell.execute_reply":"2021-06-23T01:04:39.436012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:   \n    scaler = StandardScaler()\n\n    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))\n    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:39.439258Z","iopub.execute_input":"2021-06-23T01:04:39.4397Z","iopub.status.idle":"2021-06-23T01:04:39.461715Z","shell.execute_reply.started":"2021-06-23T01:04:39.43966Z","shell.execute_reply":"2021-06-23T01:04:39.460276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I just remove the variable **fbs**. It is a categorical feature that has two possibility(55% - 51%). It has lowest correlation as we can from heatmap. ","metadata":{}},{"cell_type":"code","source":"X_train2 = X_train.drop(\"fbs\", axis = 1)\nX_test2 = X_test.drop(\"fbs\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:39.463268Z","iopub.execute_input":"2021-06-23T01:04:39.463639Z","iopub.status.idle":"2021-06-23T01:04:39.47234Z","shell.execute_reply.started":"2021-06-23T01:04:39.463569Z","shell.execute_reply":"2021-06-23T01:04:39.471007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Models\n\nI just use basic classification algorithms with their default parameters.\n\nI also use Voting Classifier to construct ensemble models with choosing a couple of them. I won't tune hyperparameters.","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression(random_state = 42)\nsvc = SVC(random_state=42, probability = True)\ngnb = GaussianNB()\nrfc = RandomForestClassifier(random_state = 42)\nknnc = KNeighborsClassifier(n_jobs = -1)\nlgbc = lgb.LGBMClassifier(random_state = 42, n_jobs = 1)\ndtc = DecisionTreeClassifier(random_state = 42)\nxgbc = xgb.XGBClassifier(random_state = 42, n_jobs = -1, use_label_encoder = False, eval_metric = \"logloss\")\n\nvc_logreg_svc_rfc_knn = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"rfc\", rfc), (\"knn\", knnc)],\n                                         voting = \"soft\")\nvc_logreg_svc_knn = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"knn\", knnc)],\n                                         voting = \"soft\")\nvc_logreg_svc = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc)],\n                                         voting = \"soft\")\nvc_all = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"gnb\", gnb), ( \"rfc\", rfc), (\"knn\", knnc),\n                           (\"lgb\", lgbc), (\"dtc\", dtc), (\"xgb\", xgbc)],\n                          voting = \"soft\")\n\ntrain_accuracy = {}\ntest_accuracy = {}\ncv_score3 = {}\ncv_score5 = {}\ncv_score10 = {}\n\nmodels = {\n    \"LogisticRegression\": logreg,\n    \"SupportVectorMachine\": svc,\n    \"GaussianNaiveBayes\": gnb,\n    \"RandomForest\": rfc,\n    \"KNN\": knnc,\n    \"LightGBM\": lgbc,\n    \"DecisionTree\": dtc,\n    \"XGBoost\": xgbc,\n    \"VotingClassifier (All Models)\": vc_all,\n    \"VotingClassifier (Logreg-SVC)\": vc_logreg_svc,\n    \"VotingClassifier (Logreg-SVC-KNN)\": vc_logreg_svc_knn,\n    \"VotingClassifier (Logreg-SVC-RFC-KNN)\": vc_logreg_svc_rfc_knn   \n}\n\nfor name, model in models.items():\n    model.fit(X_train2, y_train)\n    train_preds = model.predict(X_train2)\n    test_preds = model.predict(X_test2)\n    \n    train_accuracy[name] = accuracy_score(train_preds, y_train).round(4)\n    test_accuracy[name] = accuracy_score(test_preds, y_test).round(4)\n    cv_score3[name] = cv_model(model, X_train2, y_train, cv = cv3).round(4)\n    cv_score5[name] = cv_model(model, X_train2, y_train, cv = cv5).round(4)\n    cv_score10[name] = cv_model(model, X_train2, y_train, cv = cv10).round(4)\n    \nscores = pd.DataFrame([train_accuracy, test_accuracy, cv_score3, cv_score5, cv_score10], \n                      index = [\"TrainAccuracy\", \"TestAccuracy\", \"3FoldCVScore\", \"5FoldCVScore\", \"10FoldCVScore\"]).T","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:04:39.473665Z","iopub.execute_input":"2021-06-23T01:04:39.474077Z","iopub.status.idle":"2021-06-23T01:09:29.39064Z","shell.execute_reply.started":"2021-06-23T01:04:39.474048Z","shell.execute_reply":"2021-06-23T01:09:29.389138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores","metadata":{"execution":{"iopub.status.busy":"2021-06-23T01:09:29.392483Z","iopub.execute_input":"2021-06-23T01:09:29.392969Z","iopub.status.idle":"2021-06-23T01:09:29.413898Z","shell.execute_reply.started":"2021-06-23T01:09:29.392924Z","shell.execute_reply":"2021-06-23T01:09:29.412291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.1. Takeaways - Models\n\nIf we look at above table;\n\nLinear classifiers achieves better results i.e Logistic Regression, SVM\n\nTree based algorithms have overfitting problem since we don't tune hyperparameters.\n\nCross validation scores are unstable. Our cv scores are changing between 0.79 - 0.83 but our scores on testing data are 0.88 - 0.90\n\nFor individual models, Logistic Regression has best test accuracy with nearly 90%\n\nFor ensemble models, Voting Classifier with using Logistic Regression and SVM has best test accuracy with almost 91%","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}}]}