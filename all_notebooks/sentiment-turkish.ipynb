{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install JPype1","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:06.365585Z","iopub.execute_input":"2021-05-23T12:34:06.366051Z","iopub.status.idle":"2021-05-23T12:34:15.565797Z","shell.execute_reply.started":"2021-05-23T12:34:06.366003Z","shell.execute_reply":"2021-05-23T12:34:15.564521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras import regularizers\n\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.preprocessing import sequence\nfrom keras.models import model_from_json\nfrom keras.models import load_model\nfrom wordcloud import WordCloud, STOPWORDS\nimport pylab as pl\nimport jpype as jp\nfrom pathlib import Path\nimport os\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-23T12:36:40.466864Z","iopub.execute_input":"2021-05-23T12:36:40.467415Z","iopub.status.idle":"2021-05-23T12:36:40.474739Z","shell.execute_reply.started":"2021-05-23T12:36:40.467367Z","shell.execute_reply":"2021-05-23T12:36:40.474037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ZEMBEREK_PATH = r'../input/zemberek/zemberek-full.jar'\njp.startJVM(jp.getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:22.063403Z","iopub.execute_input":"2021-05-23T12:34:22.063955Z","iopub.status.idle":"2021-05-23T12:34:22.982468Z","shell.execute_reply.started":"2021-05-23T12:34:22.063905Z","shell.execute_reply":"2021-05-23T12:34:22.981675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TurkishMorphology = jp.JClass('zemberek.morphology.TurkishMorphology')\nTurkishSpellChecker = jp.JClass('zemberek.normalization.TurkishSpellChecker')\nTurkishSentenceNormalizer = jp.JClass('zemberek.normalization.TurkishSentenceNormalizer')\nPaths = jp.JClass('java.nio.file.Paths')\n# Get the path to the (baseline) lookup files\nlookupRoot = Paths.get(r'../input/zemberek2/normalization')\n# Get the path to the compressed bi-gram language model\nlmPath = Paths.get(r'../input/zemberek3/lm.2gram.slm')\nmorphology = TurkishMorphology.createWithDefaults()\n# Initialize the TurkishSentenceNormalizer class\n# Instantiate the morphology class with the default RootLexicon\nmorph = TurkishMorphology.createWithDefaults()\n\n# Instantiate the spell checker class using the morphology instance\nspell = TurkishSpellChecker(morph)\n\n#normalizer = TurkishSentenceNormalizer(morphology, lookupRoot, lmPath)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:22.983914Z","iopub.execute_input":"2021-05-23T12:34:22.98438Z","iopub.status.idle":"2021-05-23T12:34:27.716928Z","shell.execute_reply.started":"2021-05-23T12:34:22.98433Z","shell.execute_reply":"2021-05-23T12:34:27.715268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reverse(s): \n    if len(s) == 0: \n        return s \n    else: \n        return reverse(s[1:]) + s[0] \n\ndef checkOpennes(word):\n    vowels=['a','e','i','ı','o','ö','u','ü']\n    open_vowels=['e','i','ü','ö']\n    close_vowels=['a','ı','o','u']\n    for i in range(len(word)):\n        if reverse(word)[i] in vowels:\n            if reverse(word)[i] in open_vowels:\n                return True\n            else:\n                return False\n        else:\n            continue\ndef PresentCheck(word):\n        ei=['e','i']\n        aı=['a','ı']\n        üö=['ü','ö']\n        uo=['u','o']\n        for i in range(len(word)):\n            if reverse(word)[i] in ei:\n                return 'ei'\n            elif reverse(word)[i] in üö:\n                return 'üö'\n            elif reverse(word)[i] in aı:\n                return 'aı'\n            elif reverse(word)[i] in uo:\n                return 'uo'\n            else:\n                continue\n    \ndef StartCheck(word):\n        ei=['e','i']\n        aı=['a','ı']\n        üö=['ü','ö']\n        uo=['u','o']\n        for i in range(len(word)):\n            if (word)[i] in ei:\n                return 'ei'\n            elif (word)[i] in üö:\n                return 'üö'\n            elif (word)[i] in aı:\n                return 'aı'\n            elif (word)[i] in uo:\n                return 'uo'\n            else:\n                continue","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:27.718836Z","iopub.execute_input":"2021-05-23T12:34:27.719264Z","iopub.status.idle":"2021-05-23T12:34:27.73529Z","shell.execute_reply.started":"2021-05-23T12:34:27.719202Z","shell.execute_reply":"2021-05-23T12:34:27.734329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WORDS = dict()\n\nspell_folder = Path(\"../input/turkish-spell-check\")\n\ndef words(text): return re.findall(r'\\w+', text.lower())\nwith open(os.path.expanduser(Path(spell_folder/ \"big2.txt\")), \"r\", encoding = 'utf-8') as f:\n    for line in f:\n        splitted = line.split()\n        WORDS[splitted[0]] = int(splitted[1])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:27.736862Z","iopub.execute_input":"2021-05-23T12:34:27.737412Z","iopub.status.idle":"2021-05-23T12:34:29.901149Z","shell.execute_reply.started":"2021-05-23T12:34:27.737376Z","shell.execute_reply":"2021-05-23T12:34:29.900073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replaceall(s, n,a):\n    occurence = s.count(n)\n    alt = []\n    temp = s\n    for i in range(occurence):\n        temp2 = temp\n        for j in range(i,occurence):\n            temp2 = temp2.replace(n,a,1)\n            alt.append(temp2)\n        temp = temp.replace(n,\"!\",1)\n    for i in range(len(alt)):\n        alt[i] = alt[i].replace(\"!\",n)\n\n    return alt\n\ndef P(word, N=sum(WORDS.values())):\n    \"Probability of `word`.\"\n    if  word in WORDS.keys():\n        number = WORDS[word]\n    else:\n        number = 1\n    if number == 0:\n        number = 1\n    return number / N\n\ndef correction(word):\n      return max(candi(word), key=P)\n\ndef candi(word):\n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words):\n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcçdefgğhıijklmnoöprsştuüvyzw'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n \n    sp     = replaceall(word,'ı','i')\n    sp2     = replaceall(word,'u','ü')\n    sp3    = replaceall(word,'o','ö')\n    sp4     = replaceall(word,'g','ğ')\n    sp5     = replaceall(word,'c','ç')\n    sp6     = replaceall(word,'s','ş')\n    sp7     = replaceall(word,'i','ı')\n    sp8     = replaceall(word,'ö','o')\n    sp9     = replaceall(word,'ş','s')\n    sp10     = replaceall(word,'ğ','g')\n    sp11     = replaceall(word,'ç','c')\n    sp12     = replaceall(word,'ü','u')\n    specials=[]\n    specials.extend(sp)\n    specials.extend(sp2)\n    specials.extend(sp3)\n    specials.extend(sp4)\n    specials.extend(sp5)\n    specials.extend(sp6)\n    specials.extend(sp7)\n    specials.extend(sp8)\n    specials.extend(sp9)\n    specials.extend(sp10)\n    specials.extend(sp11)\n    specials.extend(sp12)\n    return set(deletes+transposes+replaces+inserts+specials)\n\ndef edits2(word):\n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\ndef print_diff(word, s):\n    if not word == s:\n        print(word + \" --> \" + s)\ncounter = 0\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:29.902599Z","iopub.execute_input":"2021-05-23T12:34:29.902907Z","iopub.status.idle":"2021-05-23T12:34:29.938071Z","shell.execute_reply.started":"2021-05-23T12:34:29.902876Z","shell.execute_reply":"2021-05-23T12:34:29.93688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"morphology = TurkishMorphology.createWithDefaults()\n\ndef lemmatizer(word,texts):\n        wordList=[]\n        wordList = re.sub(\"[^\\w]\", \" \",  texts).split()\n        if '�' in word:\n            return 'question'\n        pos=wordList.index(word)\n        if word==\"SMILEYPOSITIVE\":\n            return word\n        if word==\"SMILEYNEGATIVE\":\n            return word\n        sakin=''\n        word=correction(word)\n        if len(wordList)-pos>3 and pos>2:\n            for i, kelime in enumerate(wordList[pos-3:pos+4]):\n                sakin=sakin+correction(kelime)+' '\n        elif pos<=2 and len(wordList)-pos>5:\n            for i, kelime in enumerate(wordList[pos:pos+5]):\n                sakin=sakin+correction(kelime)+' '\n        elif pos<=2 and len(wordList)-pos<=5:\n            for i, kelime in enumerate(wordList[pos:len(wordList)]):\n                sakin=sakin+correction(kelime)+' '\n        elif len(wordList)-pos<1 and pos>3:\n            for i, kelime in enumerate(wordList[pos-3:len(wordList)]):\n                sakin=sakin+correction(kelime)+' '\n        elif len(wordList)<3:\n            for i, kelime in enumerate(wordList):\n                sakin=sakin+correction(kelime)+' '\n        else:\n             for i, kelime in enumerate(wordList):\n                sakin=sakin+correction(kelime)+' '\n        results = morphology.analyze(word)\n        lemma=[]\n        form=[]\n        l=[]\n        m=[]\n        for i, result in enumerate(results):\n            form.append(str(result.formatLong()))\n            lemma.append(result.getLemmas()[0])\n        if len(lemma)>1:\n                analysis = morphology.analyzeSentence(sakin)\n                results = morphology.disambiguate(sakin, analysis).bestAnalysis()\n                for i, result in enumerate(results):\n                        l.append(result.getLemmas()[0])\n                        m.append(result.formatLong())\n                for i in range(len(m)):\n                    for j in range(len(form)):\n                        if m[i]==form[j]:\n                            lema=lemma[j]\n                            if lema=='değil':\n                                return 'değil'\n                            if 'Neg' in form[j] or 'WithoutHavingDoneSo' in form[j] or 'Unable' in form[j]:\n                                if checkOpennes(word):\n                                    return lema+'me'\n                                else:\n                                    return lema+'ma'\n                            if 'Without' in form[j]:\n                                if PresentCheck(word)=='ei':\n                                    return lema+'siz'\n                                elif PresentCheck(word)=='aı':\n                                    return lema+'sız'\n                                elif PresentCheck(word)=='uo':\n                                    return lema+'suz'\n                                else:\n                                    return lema+'süz'\n                            if 'With' in form[j]:\n                                if PresentCheck(word)=='ei':\n                                    return lema+'li'\n                                elif PresentCheck(word)=='aı':\n                                    return lema+'lı'\n                                elif PresentCheck(word)=='uo':\n                                    return lema+'lu'\n                                else:\n                                    return lema+'lü'\n                            else:\n                                return lema\n                    else:\n                        continue\n        elif len(lemma)==1:\n            if lemma[0]=='değil':\n                return lemma[0]\n            if 'Neg' in form[0] or 'WithoutHavingDoneSo' in form[0] or 'Unable' in form[0]:\n                 if checkOpennes(word):\n                    return lemma[0]+'me'\n                 else:\n                    return lemma[0]+'ma'\n            elif 'Without' in form[0]:\n                if PresentCheck(word)=='ei':\n                    return lemma[0]+'siz'\n                elif PresentCheck(word)=='aı':\n                    return lemma[0]+'sız'\n                elif PresentCheck(word)=='uo':\n                    return lemma[0]+'suz'\n                else:\n                    return lemma[0]+'süz'\n            elif 'With' in form[0]:\n                if PresentCheck(word)=='ei':\n                    return lemma[0]+'li'\n                elif PresentCheck(word)=='aı':\n                    return lemma[0]+'lı'\n                elif PresentCheck(word)=='uo':\n                    return lemma[0]+'lu'\n                else:\n                    return lemma[0]+'lü'\n            else:\n                return lemma[0]\n        else:\n            return word","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:29.940836Z","iopub.execute_input":"2021-05-23T12:34:29.94155Z","iopub.status.idle":"2021-05-23T12:34:30.26516Z","shell.execute_reply.started":"2021-05-23T12:34:29.941512Z","shell.execute_reply":"2021-05-23T12:34:30.26391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from snowballstemmer import TurkishStemmer\n#turkStem=TurkishStemmer()\n#turkStem.stemWord(\"\") #ekmek\n#word = \"ses kalitesi ve ergonomisi rezalet sony olduğu için aldım ama de fiyatına çin replika ürün alsaydım çok çok daha iyiydi kesinlikle tavsiye etmiyorum\"\n#word = word.split()\n#word = [turkStem.stemWord(p) for p in word]\n#word = \" \".join(word)\n#print(word)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:30.267268Z","iopub.execute_input":"2021-05-23T12:34:30.267672Z","iopub.status.idle":"2021-05-23T12:34:30.275808Z","shell.execute_reply.started":"2021-05-23T12:34:30.267628Z","shell.execute_reply":"2021-05-23T12:34:30.274603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = pd.read_csv(\"../input/duygu-analizi-icin-urun-yorumlari/magaza_yorumlari_duygu_analizi.csv\", encoding=\"utf-16\")\nsentiment = {'Tarafsız': 0,'Olumsuz': -1, \"Olumlu\": 1}\ndataset_train.Durum = [sentiment[item] for item in dataset_train.Durum]\ndataset_train = dataset_train.dropna()\nprint(dataset_train.Görüş[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T09:27:05.589098Z","iopub.execute_input":"2021-05-22T09:27:05.589663Z","iopub.status.idle":"2021-05-22T09:27:06.33678Z","shell.execute_reply.started":"2021-05-22T09:27:05.589615Z","shell.execute_reply":"2021-05-22T09:27:06.336014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"description_list = []\nstop_words = stopwords.words('turkish')\nfor description in tqdm(dataset_train.Görüş):\n    #Regular Expression\n    description = re.sub(\"[^a-zA-ZığçşüöİĞÜÇÖŞ]\", \" \", description)\n    description = description.replace('I','ı')\n    description = description.replace('İ','i') \n    description = description.lower()\n    description = nltk.word_tokenize(description)\n    metin = \" \".join(description)\n    for i, word in enumerate(description):\n        description[i] = lemmatizer(word,metin)\n    description = [str(word) for word in description if not word in stopwords.words()]\n    description = \" \".join(description)\n    description_list.append(description)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T09:27:06.337736Z","iopub.execute_input":"2021-05-22T09:27:06.337974Z","iopub.status.idle":"2021-05-22T13:04:43.809212Z","shell.execute_reply.started":"2021-05-22T09:27:06.33795Z","shell.execute_reply":"2021-05-22T13:04:43.808164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = pd.read_csv(\"../input/lemmatizeson/lemmatize_son.csv\")\nX = dataset_train.text.values\ny = dataset_train.sentiment.values","metadata":{"execution":{"iopub.status.busy":"2021-05-22T13:04:43.810486Z","iopub.execute_input":"2021-05-22T13:04:43.810728Z","iopub.status.idle":"2021-05-22T13:04:43.814445Z","shell.execute_reply.started":"2021-05-22T13:04:43.810703Z","shell.execute_reply":"2021-05-22T13:04:43.813704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sıfırdan yapıldığında bura\nX = np.array(description_list)\ny = dataset_train.Durum.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\ny_train = pd.Series(y_train)\ny_test = pd.Series(y_test)\n\n#dico = {\"text\":X, \"sentiment\":y}\n#dfo = pd.DataFrame(data=dico)\n#dfo.to_csv (\"dataset_train.csv\", index = False, header=True)\n\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 3)\ny_test = to_categorical(y_test, num_classes = 3)\nprint(X_train)\nprint(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T13:25:31.123089Z","iopub.execute_input":"2021-05-22T13:25:31.123468Z","iopub.status.idle":"2021-05-22T13:25:31.232724Z","shell.execute_reply.started":"2021-05-22T13:25:31.123435Z","shell.execute_reply":"2021-05-22T13:25:31.231779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = pd.read_csv(\"../input/trke-lemmatized/dataset_train.csv\")\ndataset_train = dataset_train.dropna()\nprint(dataset_train[\"sentiment\"].value_counts())\nX = dataset_train.text.values\ny = dataset_train.sentiment.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\ny_train = pd.Series(y_train)\ny_test = pd.Series(y_test)\n\n#dico = {\"text\":X, \"sentiment\":y}\n#dfo = pd.DataFrame(data=dico)\n#dfo.to_csv (\"dataset_train.csv\", index = False, header=True)\n\n \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 3)\ny_test = to_categorical(y_test, num_classes = 3)\nprint(X_train)\nprint(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:30.280823Z","iopub.execute_input":"2021-05-23T12:34:30.281294Z","iopub.status.idle":"2021-05-23T12:34:31.166483Z","shell.execute_reply.started":"2021-05-23T12:34:30.281245Z","shell.execute_reply":"2021-05-23T12:34:31.165453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 5000\nembedding_dim = 16\nmax_length = 25\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:31.168071Z","iopub.execute_input":"2021-05-23T12:34:31.168724Z","iopub.status.idle":"2021-05-23T12:34:31.175133Z","shell.execute_reply.started":"2021-05-23T12:34:31.16868Z","shell.execute_reply":"2021-05-23T12:34:31.173719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\nprint(X_train)\nprint(type(X_train))\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\n\n# Train\nsequences_train = tokenizer.texts_to_sequences(X_train)\npadded_train = pad_sequences(sequences_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint(padded_train)\n\n# Test\nsequences_test = tokenizer.texts_to_sequences(X_test)\npadded_test = pad_sequences(sequences_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:31.176556Z","iopub.execute_input":"2021-05-23T12:34:31.176992Z","iopub.status.idle":"2021-05-23T12:34:31.842774Z","shell.execute_reply.started":"2021-05-23T12:34:31.176955Z","shell.execute_reply":"2021-05-23T12:34:31.841413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_padded = np.array(padded_train)\ntraining_label = np.array(y_train)\ntest_padded = np.array(padded_test)\ntest_label = np.array(y_test)\n\nprint(X[1312])\nprint(training_padded[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:31.844119Z","iopub.execute_input":"2021-05-23T12:34:31.844464Z","iopub.status.idle":"2021-05-23T12:34:31.852426Z","shell.execute_reply.started":"2021-05-23T12:34:31.844434Z","shell.execute_reply":"2021-05-23T12:34:31.851057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 16, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(20, activation='relu'),\n    tf.keras.layers.Dense(20, activation='relu'),\n    #tf.keras.layers.LSTM(15, dropout=0.5),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nnum_epochs = 25\nhistory = model.fit(training_padded, training_label, batch_size=16 ,epochs=num_epochs, validation_data=(test_padded, test_label), verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:34:32.635495Z","iopub.execute_input":"2021-05-23T12:34:32.635882Z","iopub.status.idle":"2021-05-23T12:34:53.958538Z","shell.execute_reply.started":"2021-05-23T12:34:32.63585Z","shell.execute_reply":"2021-05-23T12:34:53.957357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    mode5 = Sequential()\n    mode5.add(tf.keras.layers.Embedding(vocab_size, 40, input_length=max_length))\n    mode5.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n    mode5.add(tf.keras.layers.Dense(512, activation='relu'))\n    mode5.add(tf.keras.layers.Dropout(0.50))\n    mode5.add(tf.keras.layers.Dense(3, activation='softmax'))\n    mode5.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n    history = mode5.fit(training_padded, training_label, epochs=20,validation_data=(test_padded, test_label))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(15, activation='relu'),\n    tf.keras.layers.Dense(15, activation='relu'),\n    #tf.keras.layers.LSTM(15, dropout=0.5),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nnum_epochs = 25\nhistory = model.fit(training_padded, training_label, batch_size=32 ,epochs=num_epochs, validation_data=(test_padded, test_label), verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T13:04:45.958525Z","iopub.status.idle":"2021-05-22T13:04:45.959092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nsentence = [\"tek kelimeyle berbat hayal kırıklığına uğradım\"]#, \"game of thrones season finale showing this sunday night\"]\nsequences = tokenizer.texts_to_sequences(sentence)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint(sequences)\nraw_prediction = model.predict(padded)\nprint(raw_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:14:26.232709Z","iopub.execute_input":"2021-05-22T14:14:26.233052Z","iopub.status.idle":"2021-05-22T14:14:26.276223Z","shell.execute_reply.started":"2021-05-22T14:14:26.233018Z","shell.execute_reply":"2021-05-22T14:14:26.275299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nimport nltk\nimport re\nsentence = \"sizinle tanışmak çok hoş\"\ndescription = nltk.word_tokenize(sentence)\nmetin = \" \".join(description)\nfor i, word in enumerate(description):\n    description[i] = lemmatizer(word,metin)\ndescription = [str(word) for word in description if not word in stopwords.words()]\ndescription = \" \".join(description)\nsequences = tokenizer.texts_to_sequences([description])\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nraw_prediction = model.predict(padded)\nprint(raw_prediction[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T12:38:34.955707Z","iopub.execute_input":"2021-05-23T12:38:34.956097Z","iopub.status.idle":"2021-05-23T12:38:35.581014Z","shell.execute_reply.started":"2021-05-23T12:38:34.956065Z","shell.execute_reply":"2021-05-23T12:38:35.579714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}