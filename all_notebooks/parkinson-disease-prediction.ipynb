{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df = pd.read_csv('../input/parkinsons2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df.head().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Description of the columns:\nMDVP:Fo(Hz) - Average vocal fundamental frequency \n\nMDVP:Fhi(Hz) - Maximum vocal fundamental frequency \n\nMDVP:Flo(Hz) - Minimum vocal fundamental frequency \n\nMDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency \n\nMDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude \n\nNHR,HNR - Two measures of ratio of noise to tonal components in the voice \n\nRPDE,D2 - Two nonlinear dynamical complexity measures \n\nDFA - Signal fractal scaling exponent \n\nspread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation \n\nstatus - Health status of the subject (one) - Parkinson's, (zero) - healthy"},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that there are 195 rows and 23 columns in the given dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since column names are big it will be easy to do plots and calculations if the column names are small\nparkinson_df.columns = ['Fo','Fhi','Flo','Jitter(%)','Jitter(Abs)','RAP','PPQ','DDP','Shimmer','Shimmer(dB)','APQ3','APQ5','APQ','DDA','NHR','HNR','RPDE','DFA','spread1','spread2','D2','PPE','status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above information, we observe that all the given features are continuous except 'status'(since given in the description)."},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df[parkinson_df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above two cells,we observe that there are no missing values in the given dataset."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"parkinson_df.boxplot(figsize=(24,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above box plots, we observe that there are less outliers. So, the model will not be affected by the outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above table,we observe that 'Fhi'(Maximum vocal fundamental frequency) and 'NHR'(Measure of ratio of noise to tonal components in the voice) are having less correlation(-0.166136 and 0.189429 respectively) with respect to status . So, we can drop these two features based on their correlations."},{"metadata":{"trusted":true},"cell_type":"code","source":"parkinson_df['status'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most are having Parkinson disease. The ratio is almost 1:3 in favor of status 1. So, the model's ability to predict status 1 will be better than predicting status 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = parkinson_df.drop(['Fhi','NHR','status'],axis=1)\nY = parkinson_df['status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data into train and test in 70/30 ratio with random state as 2.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y1_predict = LR.predict(X_test)\nY1_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_acc = metrics.accuracy_score(Y_test,Y1_predict)\nprint(\"Accuracy of the model is {0:2f}\".format(Y_acc*100))\nY_cm=metrics.confusion_matrix(Y_test,Y1_predict)\nprint(Y_cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sensitivity\nTPR=Y_cm[1,1]/(Y_cm[1,0]+Y_cm[1,1])\nprint(\"Sensitivity of the model is {0:2f}\".format(TPR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Specificity\nTNR=Y_cm[0,0]/(Y_cm[0,0]+Y_cm[0,1])\nprint(\"Specificity of the model is {0:2f}\".format(TNR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_CR=metrics.classification_report(Y_test,Y1_predict)\nprint(Y_CR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So, for the above model, Accuracy is 81.35% , Sensitivity is 93.61% and Specificity is 33.33%"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(Y_test, Y1_predict)\nroc_auc = auc(fpr, tpr)\n\nprint(\"Area under the curve for the given model is {0:2f}\".format(roc_auc))\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Area under the curve is 0.6347 implies it is a good model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = parkinson_df.drop(['Fhi','NHR','status'],axis=1)\nY = parkinson_df['status']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# K-fold cross validation for the given model:\n#Since the dataset contains 197 rows, we are taking the number of splits as 3\nkf=KFold(n_splits=3,shuffle=True,random_state=2)\nacc=[]\nfor train,test in kf.split(X,Y):\n    M=LogisticRegression()\n    Xtrain,Xtest=X.iloc[train,:],X.iloc[test,:]\n    Ytrain,Ytest=Y[train],Y[test]\n    M.fit(Xtrain,Ytrain)\n    Y_predict=M.predict(Xtest)\n    acc.append(metrics.accuracy_score(Ytest,Y_predict))\n    print(metrics.confusion_matrix(Ytest,Y_predict))\n    print(metrics.classification_report(Ytest,Y_predict))\nprint(\"Cross-validated Score:{0:2f} \".format(np.mean(acc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So, for the above K-fold cross validation model, Precision for each fold is 0.82,0.89,0.88 respectively, Recall for each fold is 0.83,0.88,0.88 respectively and the overall Accuracy is 86.15%"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy for each fold\nacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Error\nerror=1-np.array(acc)\nerror","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variance Error of the model\nnp.var(error,ddof=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr, _ = roc_curve(Ytest, Y_predict)\nroc_auc = auc(fpr, tpr)\n\nprint(\"Area under the curve for the given model is {0:2f}\".format(roc_auc))\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Area under the curve is 0.8416 implies it is a very good model."},{"metadata":{},"cell_type":"markdown","source":"### By Comparing the above two models, we observe that by doing K-fold cross validation, accuracy has been improved from 81.35% to 86.15% and area under the curve has been improved from 0.6347 to 0.8416. So, we can conclude that K-fold cross validation model will be the better model for this dataset."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}