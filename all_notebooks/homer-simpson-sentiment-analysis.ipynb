{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# The Simpsons: Sentiment Analysis for Homer"},{"metadata":{},"cell_type":"markdown","source":"In this project we will try to determine if Homer's speaking line is positive, negative or neutral by using VADER. We will be also using NRC Emotion Lexicon to establish proportion of speech related to certain emotion, creating wordclouds and treemap for top 50 most frequent words used by Homer Simpson. "},{"metadata":{},"cell_type":"markdown","source":"![simpsons](https://wallpaperaccess.com/full/535134.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries"},{"metadata":{},"cell_type":"markdown","source":"Before we import libraries we will need to install vaderSentiment. For this purpose we will just run pip install ... command below."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"pip install vaderSentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import regex (regular expression operation):\nimport re\n\n# Importing numpy, pandas, matplotlib and seaborn:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Imports for plotly:\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n\n# To keep graph within the nobebook:\n%matplotlib inline\n\n# Libraries for NLP:\nimport nltk\nimport spacy\n\n\n# Imports for Wordcloud:\nfrom PIL import Image\nimport urllib\nfrom wordcloud import WordCloud, ImageColorGenerator\n\n# Scattertext\nimport scattertext as st\n\n# Import sentiment analyzer from vaderSentiment:\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# Import collections (Counter):\nimport collections\n\n# To hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore what's in the-simpsons-dataset folder:\nimport os\n\nprint(os.listdir('../input/the-simpsons-dataset/'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading Data and Initial Exploration"},{"metadata":{},"cell_type":"markdown","source":"We will be working with script lines, so let's load simpsons_script_lines.csv into a new dataframe df. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe df from the-simpsons-dataset/simpsons_script_lines.csv:\ndf = pd.read_csv('../input/the-simpsons-dataset/simpsons_script_lines.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show 5 random rows from df dataset:\ndisplay(df.sample(n=5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to describe variables:\ndef desc(df):\n    d = pd.DataFrame(df.dtypes,columns=['Data_Types'])\n    d = d.reset_index()\n    d['Columns'] = d['index']\n    d = d[['Columns','Data_Types']]\n    d['Missing'] = df.isnull().sum().values    \n    d['Uniques'] = df.nunique().values\n    return d\n\n# Apply desc to df:\ntab = ff.create_table(desc(df))\ntab.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Change data types, add columns for word_count and tens for tenth of episode:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change columns [3:12] to string:\ncolumns = list(df.columns)\n\ndf[columns[3:12]] = df[columns[3:12]].astype(str)\n\n# Word count for normalized_text:\nfrom nltk import word_tokenize\ndf['word_count'] = df['normalized_text'].apply(word_tokenize).apply(len)\n\ndf['tens'] = (df.episode_id//10+1)*10 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"#### How many episodes?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"episodes = list(df.episode_id.unique())\neposodes = episodes.sort()\n\nprint('# of Episodes:', len(episodes), '  Max. ', max(episodes), '  Min.', min(episodes)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 564 unique episodes in The Simpsons dataset. From episode 1 to episode 568 (this means that 4 episodes are missing)."},{"metadata":{},"cell_type":"markdown","source":"#### Unique Categories for Speaking lines"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print unique speaking line categories:\nprint(df.speaking_line.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's have a look at Frankenstein category of speaking line:\ndf[df.speaking_line == 'Guess what. I also play Frankenstein!']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map categories for speaking_line:\n\nline = {'true': 'True'\n        , 'false': 'False'\n        , 'True': 'True'\n        , 'False': 'False'\n        , 'Guess what. I also play Frankenstein!' : 'Frankenstein'\n       }\n\ndf['speaking_line'] = df.speaking_line.map(line)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Simpsons characters"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Compare number of unique character ids and names:\n\ncharacters = list(df.raw_character_text.unique())\nids = list(df.character_id.unique())\n\nprint('# of Characters in raw_character_text :', len(characters))\n\nprint('# of Characters in character_id :', len(ids)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From description table we can see that there are 6,766 unique characters in raw_character_text column, but looking at character_id we have 7,186 unique characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of all characters in raw_character_text column:\n\ncharacters = []\nfor person in df['raw_character_text']:\n    characters.append(person)\n        \n# List top 25 words according to frequency of speaking_lines:\n\nfrom collections import Counter\ncharacter = dict(Counter(characters).most_common())\ndel character['nan']\n\n# Create a dataframe df_char that holds character and # of speaking lines:\ndf_char = pd.DataFrame.from_dict(character, orient='index').reset_index()\ndf_char.columns = ['Character', 'Volume']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create wordcloud:\n\nprint('Wordcloud for Simpsons Characters (Speaking lines volume)')\n\n# Create WordCloud: \nwc = WordCloud(  width= 1500\n               , height=1000\n               , max_words=1000\n               , max_font_size = 300\n               #, mask = mask\n               , background_color='white'\n               , random_state = 42 \n              )\n\n# Generate word cloud\nwc.generate_from_frequencies(character)\n\n# Set size to image \nplt.figure(figsize=[20,10])\n\n# Show, swich off axis:\nplt.imshow(wc)\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now focus on Simpsons family only (Homer, Marge, Bart, Lisa). Maggie is mute and has only a very few speaking lines, so will exclude her from the simpsons dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create simpsons dataframe:\nsimpsons = df[df['raw_character_text'].isin(['Homer Simpson', 'Marge Simpson', 'Bart Simpson', 'Lisa Simpson'])]\nsimpsons = simpsons.reset_index(drop=True)\nsimpsons = simpsons.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required libraries:\nnlp = spacy.load(\"en_core_web_sm\")\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# Assign english stopwords to stopword:\nstopwords = list(STOP_WORDS)\nnew_stopwords = ['im', 'youre', 'youll', 'ill', 'ive', 'hes', 'thats', 'theres', 'id', 'cant', 'couldnt', 'shes']\n\n# Use a simple function to append elements of new_stopwords to stopwords:\nfor i in new_stopwords:\n    stopwords.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new column cleaned_text by using 'normalized_text' and removing stopwords:\nsimpsons['cleaned_text'] = simpsons.normalized_text.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create bar chart for class distribution:\n\ndf_info = simpsons[['raw_character_text']]\ndf_info['frequency'] = 0 \ndf_info = pd.DataFrame(df_info.groupby(['raw_character_text']).count()).reset_index()\n\ndata=go.Bar( x = df_info.raw_character_text\n           , y = df_info.frequency\n           ,  marker=dict( color=['#33C7FF', '#00e545',  '#FF33F3','#8E00B2'])\n           , text=df_info.frequency\n           , textposition='auto' \n           )\n\nlayout = go.Layout( title = 'Speaking lines by Simpsons'\n                  , xaxis = dict(title = 'Character')\n                  , yaxis = dict(title = '# of lines')\n                  )\n\nfig = go.Figure(data,layout)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot for #of Words per Episode:\n\ndf_chat = simpsons[['episode_id','raw_character_text', 'word_count']]\ndf_chat = pd.DataFrame(df_chat.groupby(['episode_id', 'raw_character_text'])['word_count'].sum()).reset_index()\ndf_chat.columns = ['Episode', 'CHARACTER', '# of words']\n\n\nfig = px.box(df_chat\n             , x='CHARACTER'\n             , y='# of words'\n             , points='all'\n             , color='CHARACTER'\n             , color_discrete_sequence=['#33C7FF', '#00e545',  '#FF33F3','#8E00B2']\n             , title='Distribution #of Words (by Episode)'\n             )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graphs above, we can see that Homer has the most speaking lines across available episodes."},{"metadata":{},"cell_type":"markdown","source":"## Homer Simpson - Sentiment Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"hs = simpsons[simpsons.raw_character_text == 'Homer Simpson'].reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below is an application of VADER on spoken_words columns. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n\nIt produces four sentiment metrics from word ratings: positive, neutral, negative and compound.\n\nFirst three represent the proportion of the text that falls into those categories. Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive).\n","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()\n\nsentiment = hs['spoken_words'].apply(analyzer.polarity_scores)\nsentiment_df = pd.DataFrame(sentiment.tolist())\n\n# Concatenate df and sentimet DataFrames together:\nhs = pd.concat([hs,sentiment_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).\n\nThe function below returns new columns with number count for each of the emotions.","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create function for extracting emotion into DataFrame:\n\n# Import required libraries:\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nimport tqdm\n\ndef text_emotion(df, column):\n    new_df = df.copy()\n    new_df[column] = new_df[column].astype(str)\n\n    url = 'https://raw.githubusercontent.com/sebastianruder/emotion_proposition_store/master/NRC-Emotion-Lexicon-v0.92/NRC_emotion_lexicon_list.txt'\n\n    emolex_df = pd.read_csv(url,  names=[\"word\", \"emotion\", \"association\"], sep='\\t')\n    emolex_words = emolex_df.pivot(index='word',\n                                   columns = 'emotion',\n                                   values = 'association').reset_index()\n\n    emotions = emolex_words.columns.drop('word') \n    \n    emo_df = pd.DataFrame(0, index = df.index, columns = emotions)\n\n    stemmer = SnowballStemmer('english')\n\n    for i, row in new_df.iterrows():\n        document = word_tokenize(new_df.loc[i][column])\n        for word in document:\n            word = stemmer.stem(word.lower())\n            emo_score = emolex_words[emolex_words.word == word]\n            if not emo_score.empty:\n                for emotion in list(emotions):\n                    emo_df.at[i, emotion] += emo_score[emotion]\n               \n    return pd.DataFrame(emo_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new DataFrame called data by applying text_emotion function on No_Stopwords column in df:\ndata = text_emotion(hs,'spoken_words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate df and data to create a homer DataFrame:\nhomer = pd.concat([hs, data], axis = 1)\nhomer.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### VADER"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Create vader_df with mean values per episode:\nvader_hom = homer[['episode_id','pos','neu','neg', 'compound']].copy()\n\n# Calculate mean per episode:\ndf_vader_hom = vader_hom.groupby('episode_id').mean()\n\n\n\n# Create a line Graph for VADER Sentiment:\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df_vader_hom.index,\n    y=df_vader_hom.pos,\n    name = 'Positive',\n    mode='lines',\n    line=dict(width=2, color='#00AAAA'),\n    stackgroup='one'\n))\n\nfig.add_trace(go.Scatter(\n    x=df_vader_hom.index,\n    y=df_vader_hom.neg,\n    name = 'Negative',\n    mode='lines',\n    line=dict(width=2, color='#FF0000'),\n    stackgroup='one'\n))\n\nfig.update_layout(\n    title = 'Homer: Positive vs. Negative',\n    yaxis_title='%',\n    xaxis_title='episode',\n    height = 650\n\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Compound Score per Episode:\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df_vader_hom.index,\n    y=df_vader_hom.compound,\n    name = 'Compound',\n    mode='lines',\n    line=dict(width=2, color='#00AAAA'),\n    stackgroup='one' # define stack group\n))\n\nfig.update_layout(\n    title = 'Homer - Compound Score Per Episode',\n    yaxis_title='score',\n    xaxis_title='episode',\n    height = 650\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the compund score: Homer's sentiment is very neutral, it means that the score is between 0.5 and -0.5. However, compound score is mostly in positive values. The highest score has Homer's sentiment for episode 536 (0.30), the lowest is for episode 130 (-0.19)."},{"metadata":{},"cell_type":"markdown","source":"### NRC Emotion Lexicon"},{"metadata":{"trusted":true},"cell_type":"code","source":"emotions_h = homer[['tens','anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust', 'word_count']].copy()\ndf_nrc_h = emotions_h.groupby('tens').sum()\n\n\nemotion = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n\nfor e in emotion:\n    df_nrc_h[e] = df_nrc_h[e] * 100 /df_nrc_h['word_count']\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# HOMER's Emotions:\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.anticipation,\n    name = 'Anticipation',\n    mode='lines',\n    line=dict(width=0.5, color='#32cebd'),\n    stackgroup='one' # define stack group\n))\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.joy,\n    name = 'Joy',\n    mode='lines',\n    line=dict(width=0.5, color='#4d2af0'),\n    stackgroup='one'\n))\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.trust,\n    name = 'Trust',\n    mode='lines',\n    line=dict(width=0.5, color='#120d6c'),\n    stackgroup='one'\n))\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.surprise,\n    name = 'Surprise',\n    mode='lines',\n    line=dict(width=0.5, color='#BB00BB'),\n    stackgroup='one'\n))\n\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.anger,\n    name = 'Anger',\n    mode='lines',\n    line=dict(width=0.5, color='#440411'),\n    stackgroup='one'\n))\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.disgust,\n    name = 'Disgust',\n    mode='lines',\n    line=dict(width=0.5, color='#f33828'),\n    stackgroup='one'\n))\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.fear,\n    name = 'Fear',\n    mode='lines',\n    line=dict(width=0.5, color='#ff7755'),\n    stackgroup='one'\n))\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.sadness,\n    name = 'Sadness',\n    mode='lines',\n    line=dict(width=0.5, color='#ff8100'),\n    stackgroup='one'\n))\n\nfig.update_layout(\n    title = 'Emotion Sentiment - (Grouped by 10 Episodes)',\n    yaxis_title='%',\n    xaxis_title='episode',\n    height = 650\n    #boxmode='group' # group together boxes of the different traces for each value of x\n)\n\n#fig.update_layout(yaxis_range=(0, 35))\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Create a line Graph for Positive vs Negative Emotion:\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.positive,\n    name = 'Positive',\n    mode='lines',\n    line=dict(width=2, color='#00AAAA'),\n    stackgroup='one'\n))\n\nfig.add_trace(go.Scatter(\n    x=df_nrc_h.index,\n    y=df_nrc_h.negative,\n    name = 'Negative',\n    mode='lines',\n    line=dict(width=2, color='#FF0000'),\n    stackgroup='one'\n))\n\nfig.update_layout(\n    title = 'NRC Lexicon: Positive vs. Negative Emotion',\n    yaxis_title='%',\n    xaxis_title='episode',\n    height = 650\n\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TreeMap for Top 50 Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of all words used in cleaned_text column:\n\nwords_content = []\nfor i in range(0, len(homer.cleaned_text)): \n    for word in str(homer['cleaned_text'][i]).split():\n        words_content.append(word)\n        \n# List top 10 words according to frequency of use:\nfrom collections import Counter\ncounts = dict(Counter(words_content).most_common())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rank = pd.DataFrame.from_dict(counts, orient='index').reset_index()\ndf_rank.columns = ['Word', 'Volume']\ndf_rank = df_rank[df_rank.Word != 'nan']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(df_rank.head(50), path=['Word'], values='Volume',title=\"TreeMap for Homer's top 50 Words\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh is the most frequent word in Homer's dictionary, followed by Marge, don't and like. :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}