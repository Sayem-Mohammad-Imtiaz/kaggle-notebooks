{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThis is the first notebook in a series, where I will create basic machine learning algorithms from scratch with explanations and additional materials.\n\nSo I myself will figure out the little things, and later, maybe it will help someone else \n\nIn gradient descent problems, I set myself the goal of using at a minimum ready-made mathematical solutions, such as numpy\n\nGradient descent notebooks series:\n1. Gradient descent for formulas like y = a * x + b (current)\n2. [Multivariate gradient descent](https://www.kaggle.com/konstantinsuspitsyn/multivariate-gradient-descent)\n3. More algorithms in progress\n\n**Current notebook task:**<br>\nCreate a number of functions that will allow you to come as close as possible to the actual distribution of data through a formula of the form $y=a*x+b$\n\n1. We will build synthetic data that obeys a well-known law, for example, $ y = 5 * x + 6 $, without outliers\n2. We will look through the theory of gradient descent\n3. Construct the function of gradient descent\n4. Check the function on real data of the dependence of weight on height ","metadata":{}},{"cell_type":"markdown","source":"## Creating synthetic data ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First, fill in x. From -10 to 10 using steps = 0.1\n# And from it we construct y, using the formula 5 * x + 6\n\n# X_true - we will be feeded into the function to get y_pred\n\nX_true = []\ny_true = []\n\n# range works only with integers. Multuply everything by 10, and devide by 10 when append\nfor i in range(-100, 100, 1):\n  X_true.append(i/10)\n  y_true.append(5*i/10+6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at data, that we know","metadata":{}},{"cell_type":"code","source":"X_true[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_true, y_true, color='#003f5c')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We solve the regression problem<br>\nWe know that linear regression will obey the law y = a * x + b\n\nLet's take random a and b from actual x and calculate a new y ","metadata":{}},{"cell_type":"code","source":"# I'll take a=1 и b=1\n\na = 1\nb = 1\n\ny_pred = []\nfor f in X_true:\n  y_pred.append(f*a + b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_true, y_true, color='#003f5c')\nplt.scatter(X_true, y_pred, color='#ff6361')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obviously, we did not hit the right spots. To understand how close we are in our prediction, it is necessary to create and use the cost function","metadata":{}},{"cell_type":"markdown","source":"## Theory\n\nThe standard optimization path looks like this: <br>\n\n\n1. Checking the difference between predicted data and real\n2. If we are not satisfied with the result, change the weights in front of $ x_i $ and return to item 1. If the result of the first point suits us, we start using the derived coefficients\n<br>\nNow, let's analyze the algorithm from a mathematical point of view.\n<br>\n\nThere are a lot of functions that describe the accuracy of models. But we need one where we can calculate the derivative. <br> I chose [MSE] (https://en.wikipedia.org/wiki/Mean_squared_error) <br> <br> $ MSE = \\frac {1} { n} \\sum_ {i = 1} ^ n (y_i - \\hat {y_i}) ^ 2 $ <br>\n$ y_i $ - y_true <br>\n$ \\hat {y_i} $ - y_pred, as well as $ \\hat {y_i} = a * x_i + b $, it means that, $ MSE = \\frac {1} {n} \\sum_ {i = 1} ^ n (y_i - (a * x_i + b)) ^ 2 $\n<br> We figured out how we will measure the quality of predictions <br> <br>\nLet's deal with changing the weights of $ x_i $ and the free term $ b $. <br> <br> Many years ago, specially trained scientists said that in order to minimize the cost function, we must move in the opposite direction of the gradient (the gradient is always directed towards the increase of the function). And move by some coefficient, it is called the learning rate. <br> <br>\nWhat you need to know to calculate the gradient and offset:\n\n\n1. How to find a derivative\n2. The derivative of the sum is equal to the sum of the derivatives\n3. Derivative of a complex function\n4. Gradient and partial derivatives\n<br> <br>\nWe are ready to calculate MSE partial derivatives for $ a $ and $ b $ <br> <br>\nThe partial derivative with respect to $ a $ is equal to $ \\frac {\\delta f} {\\delta a} = \\frac {1} {n} \\sum_ {i = 1} ^ n-2 * (y- (a * x_i + b )) * x_i $ <br> <br>\nThe partial derivative with respect to $ b $ is equal to $ \\frac {\\delta f} {\\delta b} = \\frac {1} {n} \\sum_ {i = 1} ^ n-2 * (y- (a * x_i + b )) $ ","metadata":{}},{"cell_type":"markdown","source":"## The code","metadata":{}},{"cell_type":"code","source":"# Creating MSE cost-function\ndef mse_function(y_true: list, y_pred: list) -> float:\n  '''\n  Function that calculates MSE\n\n  :param y_true: values of y, that we know from training data\n  :param y_pred: values of y, created by our model\n\n  :return mse: MSE value by formula\n  '''\n  # Number of values\n  n = len(y_true)\n  # Starting from 0\n  pre_mse = 0\n  for index, value in enumerate(y_true):\n    pre_mse += (value - y_pred[index])**2\n  mse = pre_mse/n\n  return mse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's count MSE. We could got lucky and guess a and b correctly from start","metadata":{}},{"cell_type":"code","source":"mse_function(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we did not. Let's make algorithm that will change a and b","metadata":{}},{"cell_type":"code","source":"# learning rate\nlr = 0.003\n# The maximum number of steps, not to wait forever,\n# if we don't get to the optimum \nmax_steps = 30000\n# Starting mse\nmse = 999\n# Starting coefficients\na=2\nb=-1\n# Steps counter\nstep = 0\n\n# Learning tracker\nmse_list = []\na_list = []\nb_list = []\ny_preds = []\n\ny_pred = []\n# Number of elements in real data\nn=len(y_true)\n\n# Model will work until our current step is less than max_steps, \n# or difference between current MSE and previous MSE is less than 1e-10\nwhile (step <= max_steps) and (mse >= 1e-10):\n  # Creating gradient start\n  grad_a=0\n  grad_b=0\n  # Calculating moving steps for weights (just like in theory)\n  for i, x in enumerate(X_true):\n    grad_a += -2*(y_true[i] - (a*x + b))* x\n    grad_b += -2*(y_true[i] - (a*x + b))\n  grad_a = grad_a/n\n  grad_b = grad_b/n\n  # Make a move, according to lr\n  a -= lr*grad_a\n  b -= lr*grad_b\n  # New forecast\n  y_pred = [a*x+b for x in X_true]\n  # Check MSE\n  mse = mse_function(y_true, y_pred)\n  # Writing results\n  mse_list.append(mse)\n  a_list.append(a)\n  b_list.append(b)\n  y_preds.append(y_pred)\n  step += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_x = []\nfor i, f in enumerate(mse_list):\n  steps_x.append(i+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.scatter(steps_x, mse_list, color='#ff6361')\nplt.title('Изменение MSE')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nalpha = 0.5\nfor j in range(0, len(y_preds), 20):\n  alpha = min(alpha + 0.5/(len(y_preds)/20),1)\n  ax.plot(X_true, y_preds[j], color='#ff6361', alpha=alpha)\nax.scatter(X_true, y_true, color='#003f5c')\nplt.title('Обучение')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Our final formula: {:.4f}*x+{:.4f}'.format(a, b))\nprint('MSE Loss: {:.4f}'.format(mse))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test is successful. Let's write a class","metadata":{}},{"cell_type":"code","source":"class GradientDescents:\n  \n  '''\n  Gradient Descents implementation from scratch\n  '''\n\n  def progress_tracker(self, step: int, cost_function: float) -> None:\n    '''\n    Printing current progress\n\n    :param step: current step\n    :param cost_function: Loss\n\n    '''\n    from IPython.display import clear_output\n    clear_output(wait=True)\n    print('Step: {}'.format(step))\n    print('Loss: {:.2f}'.format(cost_function))\n\n  def mse_function(self, y_true: list, y_pred: list) -> float:\n    '''\n    MSE calculation\n\n    :param y_true: y from data, that we know\n    :param y_pred: predicted ys\n\n    :return mse: MSE Loss\n    '''\n    # Number of ys\n    n = len(y_true)\n    # Starting from 0\n    pre_mse = 0\n    for index, value in enumerate(y_true):\n      pre_mse += (value - y_pred[index])**2\n    mse = pre_mse/n\n    return mse\n  \n  def gradient_descent(self, X_true, y_true, \\\n                       start_a=1.0, start_b=1.0, \\\n                       learning_rate=0.003, max_steps=30000, \\\n                       save_steps=0):\n    '''\n    Simple gradient descent for formulas like y=a*x+b\n\n    :param start_a: first a value in y=a*x+b\n    :param start_b: first b value in y=a*x+b\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps\n    :param save_steps: if 0, only result will be saved and returned\n                       if > 0, than every Ns' step will be saved\n   \n    :return return_dict: { \n\n            :return a: a value\n            :return b: b value\n            :return steps: total number of steps made\n            :return mse: MSE value\n            :return mse_list: lisr of MSE values if save_steps > 0\n            :return a_list: list of a values if save_steps > 0\n            :return b_list: list of b values if save_steps > 0\n    \n                        }\n    '''\n    # Initialize first step\n    step = 0\n    a = start_a\n    b = start_b\n    mse = 9999999\n    mse_prev = 0\n\n    # Let's make learning tracking\n    mse_list = []\n    a_list = []\n    b_list = []\n\n    # Predicted ys\n    y_pred = []\n    # Number of y elements in dataset\n    n=len(y_true)\n\n    # Initialize first gradients\n    grad_a=0\n    grad_b=0\n\n    # Model will work until our current step is less than max_steps, \n    # or difference between current MSE and previous MSE is less than 1e-10\n    # or MSE will be less than 1e-5\n    while (step <= max_steps) and (mse >= 1e-5) \\\n           and (abs(mse - mse_prev) >= 1e-5):\n      \n      mse_prev = mse\n      # Calculating moving steps for weights (just like in theory)\n      for i, x in enumerate(X_true):\n        grad_a += -2*(y_true[i] - (a*x + b))* x\n        grad_b += -2*(y_true[i] - (a*x + b))\n      grad_a = grad_a/n\n      grad_b = grad_b/n\n      # Make a move, according to lr (-= because we need oposite direction from gradient)\n      a -= learning_rate*grad_a\n      b -= learning_rate*grad_b\n      # New forecast\n      y_pred = [a*x+b for x in X_true]\n      # Check MSE loss\n      mse = self.mse_function(y_true, y_pred)\n\n      step += 1\n\n      # Writing progress\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          a_list.append(a)\n          b_list.append(b)\n      \n      self.progress_tracker(step-1, mse)\n\n    if save_steps > 0:\n      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1, \\\n            'mse_list': mse_list, 'a_list': a_list, 'b_list': b_list}\n    else:\n      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1}\n\n    return return_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have added weight-height data from https://www.kaggle.com/mustafaali96/weight-height?select=weight-height.csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_hw = pd.read_csv('../input/weight-height/weight-height.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_hw.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_true = df_hw['Height'].to_list()\ny_true = df_hw['Weight'].to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.scatter(X_true, y_true, color='#003f5c', label='Real data')\nax.plot(X_true, [f*6 + -256 for f in X_true], color='#ff6361', label='Starting regression')\nplt.title('Regression step #0')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad_test = GradientDescents()\ngd = grad_test.gradient_descent(X_true, y_true, \\\n                                start_a=6, start_b=-256, \\\n                                learning_rate=0.0002, max_steps=200000, \\\n                                save_steps=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.scatter(X_true, y_true, color='#003f5c', label='Real data')\nax.plot(X_true, [f*6 + -256 for f in X_true], color='#ff6361', \\\n        label='Starting regression')\nplt.plot(X_true, [f*gd['a'] + gd['b'] for f in X_true], color='#ffa600', \\\n         label='Regression after few steps')\nplt.title('Learning progress')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Final formula: {:.4f}*x+{:.4f}'.format(gd['a'], gd['b']))\nprint('MSE loss: {:.4f}'.format(gd['mse']))","metadata":{},"execution_count":null,"outputs":[]}]}