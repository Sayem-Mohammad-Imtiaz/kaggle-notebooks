{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import preprocessing \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\n\n\n\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"ID\"], axis=1, inplace=True)\ndf.dropna(axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Describe Dataset\",\"\\n\")\nprint(df.info())\nprint(df.head(10))\nprint(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical Variables**\n\nThat are SEX, MARRIAGE, EDUCATION, AGE and determine how our dataset is divided and if there are sparse classes which can cause overfit of our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"EDUCATION\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['EDUCATION']=np.where(df['EDUCATION'] == 5, 4, df['EDUCATION'])\ndf['EDUCATION']=np.where(df['EDUCATION'] == 6, 4, df['EDUCATION'])\ndf['EDUCATION']=np.where(df['EDUCATION'] == 0, 4, df['EDUCATION'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"EDUCATION\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"MARRIAGE\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MARRIAGE']=np.where(df['MARRIAGE'] == 0, 3, df['MARRIAGE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"MARRIAGE\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def age(x):\n    if x in range(21,41):\n        return 1\n    elif x in range(41,61):\n        return 2\n    elif x in range(61,80):\n        return 3\n\ndf['AGE']=df['AGE'].apply(age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of numerical features\nnumeric = [\n    'LIMIT_BAL',\n    'BILL_AMT1',\n    'BILL_AMT2',\n    'BILL_AMT3',\n    'BILL_AMT4',\n    'BILL_AMT5',\n    'BILL_AMT6',\n    'PAY_0',\n    'PAY_2',\n    'PAY_3',\n    'PAY_4',\n    'PAY_5',\n    'PAY_6',\n    'PAY_AMT1',\n    'PAY_AMT2',\n    'PAY_AMT3',\n    'PAY_AMT4',\n    'PAY_AMT5',\n    'PAY_AMT6',\n]\n\n# List of categorical features\ncategorical = ['SEX', 'EDUCATION', 'MARRIAGE','AGE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Continuous Variables Visualization\",\"\\n\")\ndf.hist(column=numeric,figsize=(16,16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Categorical Variables Visualization\",\"\\n\")\nfig, axes = plt.subplots(1, 4, figsize=(25, 5))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.7, hspace=0.3)\nfor i, ax in enumerate(axes.ravel()):\n    if i > 4:\n        ax.set_visible(False)\n        continue\n    sns.countplot(y = categorical[i], data=df, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Correlation Matrix\",\"\\n\")    \ncorrelation=df.corr(method=\"pearson\")\nplt.figure(figsize=(8,7))\nsns.heatmap(correlation,vmax=1, cmap=\"coolwarm\", cbar = True,  square = True, annot = False, fmt= '.1f',xticklabels= True, yticklabels= True,linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, :-1]\ny = df.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_confusion_matrix(y_test, prediction):\n\n  print(\"Confusion Matrix\",\"\\n\")\n  score = round(accuracy_score(y_test, prediction),3)\n  cm1 = cm(y_test, prediction)\n  sns.heatmap(cm1, annot=True, fmt=\".1f\", linewidths=.3, \n        square = True, cmap = 'PuBu')\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n  plt.title('Accuracy Score: {0}'.format(score), size = 12)\n  plt.show()\n  print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_roc_auc (y_test, y_pred,model):\n\n  fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n  roc_auc = metrics.auc(fpr, tpr)\n\n  plt.title(model + ' ROC')\n  plt.plot(fpr, tpr, 'b', label ='AUC= %0.5f' %roc_auc)\n  plt.legend(loc='lower right')\n  plt.plot([0,1],[0,1], 'r--')\n  plt.xlim([0,1])\n  plt.ylim([0,1])\n  plt.ylabel('True Positive Rate')\n  plt.xlabel('False Positive Rate')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Columns(BaseEstimator, TransformerMixin):\n    def __init__(self, names=None):\n        self.names = names\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X):\n        return X[self.names]\n\nfeatures = FeatureUnion([\n        ('numeric', make_pipeline(Columns(names=numeric),StandardScaler())),\n        ('categorical', make_pipeline(Columns(names=categorical),OneHotEncoder(sparse=False)))\n        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def modelcomparison(X_train, X_test, y_train,y_test):\n\n    tested_models = {\n    'LogisticRegression': LogisticRegression(solver='liblinear'),\n    'DecisionTree': DecisionTreeClassifier(),\n    'RandomForest': RandomForestClassifier(),\n    'AdaBoost': AdaBoostClassifier(),\n    'KNeighbors': KNeighborsClassifier(),\n    'XGB': XGBClassifier(),\n    'SVC': SVC(),\n    'MLPClassifier' : MLPClassifier(),\n    'GaussianNB' : GaussianNB(),\n    'GradientBoosting' :  GradientBoostingClassifier(random_state=0),\n    'LGBMClassifier'   : LGBMClassifier(random_state=5)\n    }\n\n    comparison = pd.DataFrame(columns=['Models', 'Accuracy',  'Precision', 'Recall', 'AreaUnderCurve','MeanSquaredError', 'RootMeanSquaredError', 'MeanAbsoluteError'])\n    cv_accuracy=[]\n\n    for model in tested_models:\n          pipe = Pipeline([\n           (\"features\", features),                          \n          ('model', tested_models[model])\n        ])\n          print(tested_models[model])\n          pipe.fit(X_train, y_train)\n          y_pred = pipe.predict(X_test)\n\n          fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n          roc_auc = metrics.auc(fpr, tpr)\n\n          comparison = comparison.append({'Models'          :   model, \n                                    'Accuracy'              :   round(accuracy_score(y_test, y_pred),5), \n                                    'Precision'             :   round(precision_score(y_test, y_pred, average='macro'),5),\n                                    'Recall'                :   round(recall_score(y_test, y_pred, average='macro'),5),\n                                    'AreaUnderCurve'        :   roc_auc,\n                                    'MeanSquaredError'      :   mean_squared_error(y_test,y_pred), \n                                    'RootMeanSquaredError'  :   np.sqrt(mean_squared_error(y_test,y_pred )),\n                                    'MeanAbsoluteError'     :   mean_absolute_error(y_test,y_pred)\n                                    }, ignore_index=True )\n    \n          print('======================')\n          print('Tested Model: ', model)\n          print('======================')\n          print(classification_report(y_test, y_pred))\n          print_confusion_matrix(y_test, y_pred)  \n          print_roc_auc(y_test, y_pred,model)\n  \n    return comparison\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ModelsResults=modelcomparison(X_train,X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Comparison of Data Set According to accuracy,precison and other metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"ordered_model_results=ModelsResults.sort_values(by=['Accuracy'], ascending=False)\nordered_model_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **K-FOLD CROSS VALIDATION**\n\nAs shown below, it is taken models with uppermost accuracy Then applied 10-fold validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ModelValidation(clf, k=10, displayscores=True):\n    '''\n       k: number of folds\n       m: model list\n    '''\n    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n    accarr = np.zeros(k)\n    precarr = np.zeros(k)\n    recarr = np.zeros(k)\n    index = 0\n    result = pd.DataFrame(columns=['index', 'acuracy',  'precision', 'recall',])\n\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        pre = precision_score(y_test, y_pred, average='macro')\n        rec = recall_score(y_test, y_pred, average='macro')\n        accarr[index] = acc\n        precarr[index] = pre\n        recarr[index] = rec\n        result = result.append({'index'       :   index, \n                                'acuracy'     :   acc, \n                                'precision'   :   pre, \n                                'recall'      :   rec\n                                }, ignore_index=True )\n        index += 1\n    # if displayscores:\n    #     print(accarr)\n    return accarr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def EvaluateModels(clflist=[], k=10):\n\n    KFoldComparison = pd.DataFrame(columns=['Models', 'Fold1',  'Fold2','Fold3','Fold4','Fold5', 'Fold6', 'Fold7','Fold8','Fold9','Fold10', 'AverageAccuracy'])\n\n    for clf in clflist:\n        acc = ModelValidation(clf, k)\n        #print(clf, acc[0])\n        KFoldComparison = KFoldComparison.append({'Models'          :   clf.__class__.__name__, \n                                                  'Fold1'           :   acc[0],\n                                                  'Fold2'           :   acc[1],\n                                                  'Fold3'           :   acc[2],\n                                                  'Fold4'           :   acc[3],\n                                                  'Fold5'           :   acc[4],\n                                                  'Fold6'           :   acc[5],\n                                                  'Fold7'           :   acc[6],\n                                                  'Fold8'           :   acc[7],\n                                                  'Fold9'           :   acc[8],\n                                                  'Fold10'          :   acc[9],\n                                                  'AverageAccuracy' :  np.average(acc)\n                                                  \n                                                }, ignore_index=True  )\n\n    return KFoldComparison","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**10 FOLD COMPARISON TABLE AND AVERAGE ACCURACY OF FOLDS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"KFoldComparisonTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **FEATURE SELECTION**\nIn this section, feature selection is applied to standardized data from df."},{"metadata":{"trusted":true},"cell_type":"code","source":"data=df.copy()\ndata_X = data.iloc[:, :-1]\ndata_y = data.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ndata_X[numeric] = scaler.fit_transform(data_X[numeric])\ndmmy_col1 =pd.get_dummies(data_X['SEX'],prefix ='SEX',dummy_na=False)\ndmmy_col2 =pd.get_dummies(data_X['EDUCATION'],prefix ='EDUCATION',dummy_na=False)\ndmmy_col3 =pd.get_dummies(data_X['MARRIAGE'],prefix ='MARRIAGE',dummy_na=False)\ndmmy_col4 =pd.get_dummies(data_X['AGE'],prefix ='AGE',dummy_na=False)\nstd_X = pd.concat([data_X[numeric],dmmy_col1,dmmy_col2,dmmy_col3,dmmy_col4],axis=1)\nstd_X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train&Test Data Splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X_train, data_X_test, data_y_train, data_y_test = train_test_split(std_X, data_y, test_size=.2, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bestFeatureSelection(clf,data_X,data_y):\n  NUM_FEATURES = 5\n  rfe_stand = RFE(clf, NUM_FEATURES)\n  fit_stand = rfe_stand.fit(std_X, data_y)\n  #print(\"St Model Num Features:\", fit_stand.n_features_)\n  #print(\"St Model Selected Features:\", fit_stand.support_)\n  print(\"Std Model Feature Ranking:\", fit_stand.ranking_)\n  score_stand = rfe_stand.score(std_X,data_y)\n  print(\"Standardized Model Score with selected features is: %f (%f)\" % (score_stand.mean(), score_stand.std()))\n  feature_names = np.array(std_X.columns)\n  print('Most important features (RFE): %s'% feature_names[rfe_stand.support_])\n  return feature_names[rfe_stand.support_], score_stand.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def modelsFeatureSelection(clflist, std_X, data_y):\n  \n    BestFeaturesOfModels = pd.DataFrame(columns=['Models', 'Feature1',  'Feature2','Feature3','Feature4','Feature5', 'ScorewithSelectedFeatures'])\n\n    for clf in clflist:\n        bst = bestFeatureSelection(clf,std_X, data_y)\n\n        BestFeaturesOfModels = BestFeaturesOfModels.append({'Models'                    :   clf.__class__.__name__, \n                                                  'Feature1'                            :   bst[0][0],\n                                                  'Feature2'                            :   bst[0][1],\n                                                  'Feature3'                            :   bst[0][2],\n                                                  'Feature4'                            :   bst[0][3],\n                                                  'Feature5'                            :   bst[0][4],\n                                                  'ScorewithSelectedFeatures'           :   bst[1]\n                                                  \n                                                }, ignore_index=True  )\n        \n    return BestFeaturesOfModels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\nGradientBoosting      = GradientBoostingClassifier(random_state=0)\nXGB                   = XGBClassifier()\nLGBMClassifier        = LGBMClassifier()\nLogisticRegression    = LogisticRegression(solver='liblinear')\n#SVC                   = SVC()\n#AdaBoost              = AdaBoostClassifier()\n#DecisionTree          = DecisionTreeClassifier(criterion = 'gini', max_depth=None)\n#RandomForest          = RandomForestClassifier()\n#KNeighbors            = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n#GaussianNB            = GaussianNB()\n\nclflist = []\n\nclflist.append(GradientBoosting)\nclflist.append(XGB)\nclflist.append(LGBMClassifier)\nclflist.append(LogisticRegression)\n# clflist.append(SVC)\n#clflist.append(AdaBoost)\n#clflist.append(DecisionTree)\n#clflist.append(RandomForest)\n#clflist.append(KNeighbors)\n#clflist.append(GaussianNB)\n\n\nFeatureSelectionResult=modelsFeatureSelection(clflist,std_X, data_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scores with selected most important 5 features\nComparing to first model accuracy scores, only LogisticRegression classifier made an improvement."},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureSelectionResult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL USING NEURAL NETWORK"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(100, input_shape =(None,data_X_train.shape[1])))\nmodel.add(Dense(50, activation=\"relu\"))\nmodel.add(Dense(25, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model.compile(optimizer=\"adam\", loss= \"mse\", metrics=[\"accuracy\"],)\nhistory=model.fit(data_X_train, data_y_train, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, host = plt.subplots()\nfig.subplots_adjust(right=1)\n\npar1 = host.twinx()\n\np1, = host.plot(history.history[\"loss\"], \"b-\", label=\"Loss\")\np2, = par1.plot(history.history[\"accuracy\"], \"r-\", label=\"Accuracy\")\nhost.set_xlabel(\"Epoch\")\nhost.set_ylabel(\"Loss\")\npar1.set_ylabel(\"Accuracy\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RESULTS\nAs a result, comparing neural network vs other classifiers , artificial neural network is the only one that can reach to maximum accuracy and can estimate the real probability of default. After neural network, GradientBoosting, XGB and LGBM classifiers follows by accuracy consecutively."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}