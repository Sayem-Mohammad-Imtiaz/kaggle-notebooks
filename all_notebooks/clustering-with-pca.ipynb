{"cells":[{"metadata":{"_uuid":"5c49733e09365f532cf489da1026f47fa9c8e723"},"cell_type":"markdown","source":"In this kernel, we are going to first find the principle components and then use them for clustering. Let's get started ! :) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Load data\ndf = pd.read_csv('../input/Mall_Customers.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8fe48a0449e5ec4308c639efa3a87b790d3de4d"},"cell_type":"code","source":"# Are there any null values?\ndf.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf5a6f317f061b15d56ecf310df553052a698b44"},"cell_type":"code","source":"# Convert gender to 1 and 0\ndf['Gender'].replace({'Male':0,'Female':1},inplace=True)\ndf.drop(['CustomerID'],axis=1,inplace=True) # We dont need Customer ID\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8036380a7441e018784c69cdd9d36c2457b2c3a"},"cell_type":"code","source":"df.shape # How many data points?","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Any features strongly correlated?\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nfor col in df.columns:\n    X = df.drop([col], axis=1)\n    y = df[col]\n    X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n    reg = DecisionTreeRegressor()\n    reg.fit(X_train,y_train)\n    print('Score for {} as dependent variable is {}'.format(col,reg.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4228bc7127d6cc0a24fe63d07e85fed628cc1d94"},"cell_type":"code","source":"# Visualize the feature correlation using scatter plot\n# Are there any outliers?\npd.scatter_matrix(df, figsize=(16,9), diagonal ='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ae3cf4534fdacd0ffa24794e523cbcd52b1edf8"},"cell_type":"code","source":"# Visualize the feature correlation using heatmap\nimport seaborn as sns\nsns.heatmap(df.corr(),xticklabels=df.columns,yticklabels=df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7432f71ba322b2d9dbee583e677b6913644cc1ed"},"cell_type":"code","source":"# Find the principal components!\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca.fit(df)\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42bd940610a1b96d79da9157fabf07dce7b80665"},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8535cb8c59ef2df641662ba2521c98ddf0e85b00"},"cell_type":"code","source":"dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\ncomponents = pd.DataFrame(pca.components_,columns=df.columns)\ncomponents.index = dimensions\n\nvariance = pd.DataFrame(pca.explained_variance_ratio_, columns=['Explained Variance'])\nvariance.index = dimensions\n\npd.concat([variance,components], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c67092b846042210b3b2730d4510198319296a9e","scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(16,9))\ncomponents.plot(kind='bar', ax=ax)\nax.set_xticklabels(dimensions)\nfor i,variance in enumerate(pca.explained_variance_ratio_):\n    ax.text(i,ax.get_ylim()[1]+0.05,'Explained variance {}'.format(np.round(variance,4)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f6471567b77b84a69e36113085a37c8fad17002"},"cell_type":"markdown","source":"As the variance explained by the first two principal components is ~ 0.9(0.45+0.44) we can say its good to use only the first two principal components. "},{"metadata":{"trusted":true,"_uuid":"87607f933e56c879a9e253cff5f2cee11e6ed430"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(df)\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42aec06aaa3fc0744918bb5453140a4dbb5bc74d","scrolled":true},"cell_type":"code","source":"transformed_data = pca.transform(df)\ntransformed_data = pd.DataFrame(transformed_data,columns=['Dimension 1','Dimension 2'])\ntransformed_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae728060445eeb79b807b4a5795b17bdb5e3a293"},"cell_type":"markdown","source":"**What is the ideal number of clusters?**"},{"metadata":{"trusted":true,"_uuid":"2e3da3efd3042521d14397f993639a1ccd450a0a"},"cell_type":"code","source":"# Use silhouette score to find the ideal number of clusters.\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nno_of_clusters= range(2,11)\nkmeans = [KMeans(n_clusters=i) for i in no_of_clusters]\nscore = [silhouette_score(transformed_data,kmeans[i].fit(transformed_data).predict(transformed_data),metric='euclidean') for i in range(len(kmeans))]\nplt.plot(no_of_clusters,score)\nplt.xlabel('No of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3545adeb5bf0d503a91eb47dd55aeaba73f8c7de"},"cell_type":"markdown","source":"The score starts decreasing after 5. So, we are going to use 5 clusters."},{"metadata":{"_uuid":"2fb49c3e744472c14d9e74e3496273e74c1d7992"},"cell_type":"markdown","source":"**K - Means Clustering**"},{"metadata":{"trusted":true,"_uuid":"94e80c65b09abb650ee07e4381d055688aa7040d"},"cell_type":"code","source":"kmeans = KMeans(n_clusters=5)\nkmeans.fit(transformed_data)\nkmeans.predict(transformed_data)\n\nplt.scatter(transformed_data.iloc[:,0],transformed_data.iloc[:,1],c=kmeans.labels_,cmap='rainbow')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8a0025b4f6eb8abe4b639f51c81658ecfcc3528"},"cell_type":"markdown","source":"**Gaussian Mixture**"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"46f24cd07092c184548afcbe92cc8d7e90906b39"},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=5)\ngmm.fit(transformed_data)\nlabels = gmm.predict(transformed_data)\n\nplt.scatter(transformed_data.iloc[:,0],transformed_data.iloc[:,1],c=labels,cmap='rainbow')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd2f649cda732f2bb21700a9d62b23b32674aff1"},"cell_type":"markdown","source":"This is a soft clustering method. So, each data point is associated with a probability value for each of the clusters. We can say the cluster with the highest probability owns the data point ! :)  "},{"metadata":{"trusted":true,"_uuid":"a4c85b01794ea121ba96e8fd31d08aff0e16ce25"},"cell_type":"code","source":"cluster_proba_df = pd.DataFrame(gmm.predict_proba(transformed_data), columns = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5'])\ncluster_proba_df['Belongs to'] = cluster_proba_df.idxmax(axis=1)\ncluster_proba_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}