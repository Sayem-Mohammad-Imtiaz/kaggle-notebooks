{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nfrom IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hi!!!"},{"metadata":{},"cell_type":"markdown","source":"**Hi, I am Sina Tavakoli, today I am trying to get a good example of large scale data. For this we need a fitness function and an algorithm!**\n\nI get my main idea for fitness function from The surface area below the chart."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(filename='../input/photo123/maxresdefault.jpg') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I think if we have two distributions, and the area below the diagram of these two distributions in very small intervals is Equal then this two distributions is equal**\n\nLet's go to implement the idea"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics.pairwise import pairwise_distances_argmin\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_=pd.read_csv(r'../input/banking-data/Datae2.csv')\n\ndf=df_.drop(['id','year','message_code','service','atm_status',\n       'atm_manufacturer','atm_location','atm_streetname',\n       'atm_street_number','atm_zipcode','weather_lat',\"atm_id\",\n       'weather_lon','weather_city_id','weather_city_name','weather_description'],axis=1)\n\n\ndf['rain_3h']=df['rain_3h'].fillna(0)\ndf.message_text=df.message_text.fillna(\"None\")\n\ndf.message_text = df.message_text.replace([\"Suspected malfunction, no cash dispensed\",\"Suspected malfunction, card retained\"],\"Suspected malfunction\")\ndf.message_text = df.message_text.replace([\"Timed-out taking card, card retained and no cash dispensed\",\"Timed-out taking money\"],\"Timed-out\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# fuzzy encoding!!"},{"metadata":{},"cell_type":"markdown","source":"I have doubts about whether this is useful and defensible, and I would like you to comment on this.:)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fuzzyday(x):\n    a=0\n    b=0\n    c=0\n\n    if x<4:\n        a=1\n    elif x<9:\n        Y=x-4\n        a = 1-Y*0.25\n    elif x>8:\n        a=0\n        \n    if x<4:\n        b=0\n    elif x<8:\n        T = x-4\n        b=T*0.25\n    elif x<25:\n        b=1\n    elif x<29:\n        T = x-25\n        b=1-T*0.25\n        T+=1\n        \n    if x<25:\n        c=0\n    elif x<29:\n        L=x-25\n        c=0.25*L\n    elif x>28:\n        c=1\n    \n    \n    return round(a*10),round(b*10),round(c*10)\n\ndef fuzzyhour(x):\n    a=0\n    b=0\n    c=0\n\n    if x<6:\n        a=1\n    elif x<11:\n        Y=x-5\n        a = 1-Y*0.2\n    elif x>10:\n        a=0\n        \n    if x<6:\n        b=0\n    elif x<11:\n        T = x-5\n        b=T*0.2\n    elif x<15:\n        b=1\n    elif x<21:\n        T = x-15\n        b=1-T*0.2\n        \n        \n    if x<15:\n        c=0\n    elif x<20:\n        L=x-15\n        c=0.20*L\n    elif x>20:\n        c=1\n    \n    \n    return round(a*10),round(b*10),round(c*10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data pre-processing & PCA\nWe use pca for continuous distribution that represents the actual distribution! (Our comparison will be easier in this case.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"1day\"]=1\ndf[\"2day\"]=2\ndf[\"3day\"]=3\ndf[\"1day\"],df[\"2day\"],df[\"3day\"] = zip(*df.day.apply(lambda x:fuzzyday(x)))\ndf.drop(\"day\",axis=1, inplace=True)\n\ndf[\"1hour\"]=1\ndf[\"2hour\"]=2\ndf[\"3hour\"]=3\ndf[\"1hour\"],df[\"2hour\"],df[\"3hour\"] = zip(*df.hour.apply(lambda x:fuzzyhour(x)))\ndf.drop(\"hour\",axis=1, inplace=True)\n\n\ndff = pd.get_dummies(df[[\"month\",\"weekday\",\"currency\",\"card_type\",'weather_main',\"message_text\"]],drop_first=True)\n\ndf = df.drop([\"month\",\"weekday\",\"currency\",\"card_type\",'weather_main',\"message_text\"],axis=1)\n\ndf = pd.merge(df,dff,left_index=True,right_index=True)\n\nMMS = MinMaxScaler()\nMMS.fit(df)\ndf_norm = MMS.transform(df)\ndf_norm = pd.DataFrame(df_norm, columns=df.columns)\n# df_norm.head()\n\npca_model = PCA(n_components=0.98)\npca_model.fit(df_norm)\ndf_pca = pca_model.transform(df_norm)\ndf_pca = pd.DataFrame(df_pca)\n\n\npca_ratio=pca_model.explained_variance_ratio_\npca_ratio","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We multiply each column by its **pca_ratio** to give more **importance** to maintaining the distribution of the PC0 than the other columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"p=0\ndf_pca2 = pd.DataFrame()\nfor j in df_pca.columns:\n    df_pca2[j] = df_pca[j]*pca_ratio[p]\n    p+=1\n    \ndf_pca2 = df_pca2*10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And the area below the relative distribution diagram of each feature is stored in the intervals of 0.02**"},{"metadata":{"trusted":true},"cell_type":"code","source":"distrToTal = []\n\nfor j in df_pca.columns:\n    distr = []\n    arange = []\n    pas = 0\n    sta = -0.95\n    for i in range(2000): \n        lenn =len(df_pca[j][df_pca[j]<sta])\n        \n        distr.append((lenn / 1250000) - pas)\n        pas = np.sum(distr)\n        \n        arange.append(round(sta,2))\n        sta +=0.02\n        \n        if sta>1.5:\n            break\n            \n    distrToTal.append(distr)\ndistrToTal[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitness"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fitness(df):\n    summy = []\n    for j in df.columns:\n        j = int(j)\n        sc=0\n        pas=0\n        count=0\n        summ=[]\n        for i in arange:\n            lenn =df[df[j]<i].shape[0]\n            pers = (lenn / df.shape[0]) - pas\n            pas += pers\n            summ.append(np.abs(pers - distrToTal[j][count]))\n            count+=1\n        summy.append(np.sum(summ) * pca_ratio[j])\n        \n    return (np.sum(summy)/2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mini batch K-means"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca_mi = df_pca2.copy()\nmbkm =MiniBatchKMeans(n_clusters=1700,max_iter=800, batch_size=50000 , verbose=1)\n\nmbkm.fit(df_pca_mi)\ndf_pca_mi[\"lable\"] = mbkm.predict(df_pca_mi)\n\n\n\nlist0 = []\nfor i in range(1900):\n    dff = df_pca_mi[df_pca_mi[\"lable\"] == i]\n    list1 = np.random.choice(dff.index ,size = int(np.round((len(dff)/10))),replace=False)\n    list0.append(list1)\n    \n    \n    \nhhh = []\nfor i in list0:\n    for j in i:\n        hhh.append(j)\n        \n        \n        \nprint(len(hhh))\n\n\ndf_sample = df_pca[df_pca.index.isin(hhh)]\n\nprint(\"Matching percentage:\",((1-fitness(df_sample))*100) , \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A strange algorithm I made myself:)))))))"},{"metadata":{"trusted":true},"cell_type":"code","source":"best=10\nfor i in tqdm(range(20)):\n    ind = []\n\n    for i in range(len(arange)-1):\n        data = df_pca[(df_pca[0]<arange[i+1]) & (df_pca[0]>arange[i])]\n        data = data.iloc[:,1:]\n        le = round(len(data)/10)\n\n        if le<500:\n            tt = np.random.choice(data.index,le,replace=False)\n            ind.append(tt)\n        else:\n            n_c = round(le/100)\n            mbkm =MiniBatchKMeans(n_clusters=n_c,max_iter=800, batch_size=round(le/4) , verbose=0)\n\n            mbkm.fit(data)\n            data[\"lable\"] = mbkm.predict(data)\n\n\n\n            list3 = []\n            for i in range(n_c):\n                dff = data[data[\"lable\"] == i]\n                list1 = np.random.choice(dff.index ,size = int(np.round((len(dff)/10))),replace=False)\n                list3.append(list1)\n\n            hhh = []\n            for i in list3:\n                for j in i:\n                    hhh.append(j)\n\n            ind.append(hhh)\n    fina = []\n    for i in ind:\n        for j in i:\n            fina.append(j)\n            \n    df_sample = df_pca[df_pca.index.isin(fina)] \n    fit = fitness(df_sample)\n\n    if fit<best:\n        best = fit\n        best_sample=df_sample\n        print(\"Matching percentage:\" ,(1-best)*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Apparently, this algorithm that I made gets a better answer\nIf you enjoyed this notebook, I will be happy to like it**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"my best sample is:\")\nbest_sample","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}