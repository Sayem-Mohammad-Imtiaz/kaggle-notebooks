{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Brazilian Houses to Rent\n\n\n\n### Dataset\n\n- Este dataset possui 6079 casas para alugar com 13 classes diferentes. This dataset contains 6079 houses to rent with 13 diferent features\n- Webcrawler de informações abertas de um site imobiliário. Webcrawler from open information from real state website. (https://www.quintoandar.com.br/)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Contents\n\n\n### 1) Import Data\n\n\n### 2) Editing Data\n\n\n### 3) Generate Metadata\n\n\n### 4) Fast DataPrep - Missing Treatment, Dummification and Label Encoding\n\n\n### 5) Training Models","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport io\nimport requests\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport gc\nimport random\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nimport lightgbm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1) Import Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/brasilian-houses-to-rent/houses_to_rent_v2.csv\")\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Editing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['city'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['animal'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['furniture'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['floor'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['city'] = dataset['city'].replace(['São Paulo', 'Porto Alegre', 'Rio de Janeiro', 'Campinas','Belo Horizonte'],['Sao_Paulo','Porto_Alegre','Rio_Janeiro','Campinas','Belo_Horizonte'])\ndataset['city'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['animal'] = dataset['animal'].replace(['acept', 'not acept'],['acept', 'not_acept'])\ndataset['animal'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['animal'] = dataset['animal'].replace(['furnished', 'not furnished'],['furnished', 'not_furnished'])\ndataset['animal'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['floor'] = dataset['floor'].replace(['7', '20', '6', '2', '1', '-', '4', '3', '10', '11', '24', '9',\n                                             '8', '17', '18', '5', '13', '15', '16', '14', '26', '12', '21',\n                                             '19', '22', '27', '23', '35', '25', '46', '28', '29', '301', '51','32'],\n                                            ['7', '20', '6', '2', '1', '0', '4', '3', '10', '11', '24', '9',\n                                             '8', '17', '18', '5', '13', '15', '16', '14', '26', '12', '21',\n                                             '19', '22', '27', '23', '35', '25', '46', '28', '29', '301', '51','32'])\ndataset['floor'] = dataset['floor'].astype(np.float64)\ndataset['floor'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['id'] = dataset.index*1\ndataset['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['target'] = dataset['total (R$)'].astype(np.float64)\ndataset = dataset.drop(columns=[\"total (R$)\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns = ['city', 'area', 'rooms', 'bathroom', 'parking_spaces', 'floor',\n                   'animal', 'furniture', 'hoa', 'rent_amount',\n                   'property_tax', 'fire_insurance', 'id', 'target']\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) Generate Metadata\n\nI use a function to generate metadata of dataset. The goal here is make DataPrep easier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Generate Metadata Function\n\ndef GenerateMetadata(train,var_id,targetname): \n    print('Running metadata...')\n    \n    for ids in var_id:\n        print('Renaming ---> ', ids,'to ---> ', 'ID_'+ids)\n        train = train.rename(columns={ids: 'ID_'+ids})\n   \n    train = train.rename(columns={targetname: 'target'})\n    # Verifying type of columns\n    t = []\n    for i in train.columns:\n            t.append(train[i].dtype)\n\n    n = []\n    for i in train.columns:\n            n.append(i)\n\n    aux_t = pd.DataFrame(data=t,columns=[\"Type\"])\n    aux_n = pd.DataFrame(data=n,columns=[\"Features\"])\n    df_tipovars = pd.merge(aux_n, aux_t, left_index=True, right_index=True) \n\n    data = []\n    for f in train.columns:\n        # Defining variable roles:\n        if f == 'target':\n            role = 'target'\n        elif f[0:3] == 'ID_':\n            role = 'id'\n        else:\n            role = 'input'\n\n        # Defining variable types: nominal, ordinal, binary ou interval\n        if f == 'target':\n            level = 'binary'\n        if train[f].dtype == 'object' or f == 'id': \n            level = 'nominal'\n        elif train[f].dtype in ['float','float64'] :\n            level = 'interval'\n        elif train[f].dtype in ['int','int64','int32'] :\n            level = 'ordinal'\n        else:\n            level = 'NA'\n\n        # Remove IDs\n        keep = True\n        if f[0:3] == 'ID_':\n            keep = False\n\n        #  Defining the type of input table variables\n        dtype = train[f].dtype\n\n        # Metadata list\n        f_dict = {\n            'Features': f,\n            'Role': role,\n            'Level': level,\n            'Keep': keep,\n            'Type': dtype\n        }\n        data.append(f_dict)\n\n    meta = pd.DataFrame(data, columns=['Features', 'Role', 'Level', 'Keep', 'Type'])\n\n    # Cardinality of columns\n    card = []\n\n    v = train.columns\n    for f in v:\n        dist_values = train[f].value_counts().shape[0]\n        f_dict = {\n                'Features': f,\n                'Cardinality': dist_values\n            }\n        card.append(f_dict)\n\n    card = pd.DataFrame(card, columns=['Features', 'Cardinality'])\n\n    metadata = pd.merge(meta, card, on='Features')\n    print('Metadada successfully completed')\n    return metadata, train ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_list = ['id']\ntargetname = 'target'\nmetadata, dataset_01 = GenerateMetadata(dataset,id_list,targetname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3) Fast DataPrep - Missing Treatment, Dummification and Label Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Convert numbers to \"float64\" and categorical to \"str\"\n\nnumeric_list = metadata[((metadata.Level  == 'ordinal')|(metadata.Level == 'interval')) & (metadata.Role == 'input')]\ncategory_list = metadata[(metadata.Level  == 'nominal') & (metadata.Role == 'input')]\n\nnumeric_list = list(numeric_list['Features'].values)\ncategory_list = list(category_list['Features'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_02 = dataset_01[numeric_list].astype(np.float64)\ndataset_03 = pd.merge(dataset_02, dataset_01[category_list].astype(np.str), left_index=True, right_index=True)\ndataset_03.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_03['ID_id'] = dataset_01['ID_id'].values\ndataset_03['target'] = dataset_01['target'].values\ndataset_03.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def DataPrep(metadados,input_df,var_id,targetname):\n    \n    print('Starting data preparation ...')\n    \n    #-------------- Handling missing of numeric columns -----------------\n    input_df.rename(columns={var_id: 'id', targetname: 'target'}, inplace=True)\n    df_00 = input_df\n    targetname = 'target'\n    print('Executing')\n    \n    #--------- Numeric Features --------------------\n    vars_numericas_df = metadados[((metadados.Level  == 'ordinal')|(metadados.Level == 'interval')) & (metadados.Role == 'input')]\n    lista_vars_numericas = list(vars_numericas_df['Features'])\n    df01 = df_00[lista_vars_numericas]\n    df01 = df01.fillna(df01[lista_vars_numericas].mean())\n    df01 = df01.round(4)\n    \n    print('Missings done')\n    \n    #-------------- Numeric Features - Standart Scaler -----------------\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    df01 = df01.astype(float)\n    df01[lista_vars_numericas] = scaler.fit_transform(df01[lista_vars_numericas])\n    \n    print('Normalization done')\n\n    #--------- Nominal Features - Low Cardinality --------------------\n    vars_char_baix_cardin_df = metadados[(metadados.Level  == 'nominal') & (metadados.Role == 'input') & (metadados.Cardinality <= 50)]\n    lista_char_baix_cardin_df = list(vars_char_baix_cardin_df['Features'])\n    \n    df_00[lista_char_baix_cardin_df].apply(lambda x: x.fillna(x.mode, inplace=True))\n    df02 = df_00[lista_char_baix_cardin_df]\n    \n    df03 = pd.get_dummies(df02,columns=lista_char_baix_cardin_df,drop_first=True,\n                          prefix=lista_char_baix_cardin_df,prefix_sep='_')\n    print('Dummifications done')    \n    \n    #--------- Nominal Features - High Cardinality --------------------\n    vars_char_alta_cardin_df = metadados[(metadados.Level  == 'nominal') & (metadados.Role == 'input') & (metadados.Cardinality > 50)]\n    lista_char_alta_cardin_df = list(vars_char_alta_cardin_df['Features'])\n    \n    df_00[lista_char_alta_cardin_df].apply(lambda x: x.fillna(x.mode, inplace=True)) \n    df04 = df_00[lista_char_alta_cardin_df]\n\n    def MultiLabelEncoder(columnlist,dataframe):\n        for i in columnlist:\n            labelencoder_X=LabelEncoder()\n            dataframe[i]=labelencoder_X.fit_transform(dataframe[i])\n\n    MultiLabelEncoder(lista_char_alta_cardin_df,df04)\n    print('Label Encodings done')\n    \n    #---------- Checking IDs -----------------------\n    vars_ids_df = metadados[(metadados.Role  == 'id')]\n    lista_ids = list(vars_ids_df['Features'])\n\n    df1_3 = pd.merge(df01, df03, left_index=True, right_index=True)\n    df1_3_4 = pd.merge(df1_3, df04, left_index=True, right_index=True)\n    \n    lista_vars_keep = lista_ids + [targetname]\n    \n    df_out = pd.merge(input_df[lista_vars_keep], df1_3_4, left_index=True, right_index=True)    \n    \n    print('Data Preparation Sucess')\n    \n    return df_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_04 = DataPrep(metadata, dataset_03,'id','target')\ndataset_04.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_04.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train/Test split\n\nX = dataset_04.drop(['target','ID_id'], axis=1)\ny = dataset_04[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5) Training Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgbm = lightgbm.LGBMRegressor(n_estimators = 300,\n                                    learning_rate = 0.05,\n                                    max_depth = 6,\n                                    num_leaves = 40,\n                                    random_state = 42)\n\nmodel_lgbm.fit(X_train, y_train)\n\ny_pred_train = model_lgbm.predict(X_train)\ny_pred_test = model_lgbm.predict(X_test)\n\nresidual_train = (y_train - y_pred_train).astype(\"float\")\nresidual_test = (y_test - y_pred_test).astype(\"float\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get Model Performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Set Performance \\n')\nprint('R-Squared: ', np.round(metrics.r2_score(y_train, y_pred_train, multioutput='variance_weighted'),2))\nprint('Mean Absolute Error: ', np.round(metrics.mean_absolute_error(y_train, y_pred_train),2))  \nprint('Mean Squared Error: ', np.round(metrics.mean_squared_error(y_train, y_pred_train),2))  \nprint('Root Mean Squared Error: ', np.round(np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test Set Performance \\n')\nprint('R-Squared: ', np.round(metrics.r2_score(y_test, y_pred_test, multioutput='variance_weighted'),2))\nprint('Mean Absolute Error: ', np.round(metrics.mean_absolute_error(y_test, y_pred_test),2))  \nprint('Mean Squared Error: ', np.round(metrics.mean_squared_error(y_test, y_pred_test),2))  \nprint('Root Mean Squared Error: ', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get Variable Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = lightgbm.plot_importance(model_lgbm, max_num_features=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}