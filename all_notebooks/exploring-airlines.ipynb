{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[],"source":"%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\ndf = pd.read_csv(\"../input/Tweets.csv\")\n\nprint (df.shape)\n#print (df.head())\nprint (df.info())\n\nplt.figure(1)\npd.Series(df[\"airline_sentiment\"]).value_counts().plot(kind = \"barh\" , title = \"airline_sentiment\")\nplt.figure(2)\npd.Series(df[\"airline\"]).value_counts().plot(kind = \"barh\" , title = \"airline\")\nplt.figure(3)\npd.Series(df[\"tweet_location\"]).value_counts().head(20).plot(kind = \"barh\" , title = \"tweet_location\")\nplt.figure(4)\npd.Series(df[\"retweet_count\"]).value_counts().plot(kind = \"barh\" , title = \"retweet\")\nplt.figure(5)\npd.Series(df[\"name\"]).value_counts().head(20).plot(kind = \"barh\" , title = \"name\")\nplt.figure(6)\npd.Series(df[\"user_timezone\"]).value_counts().head(20).plot(kind = \"barh\" , title = \"user_timezone\")\nplt.figure(9)\npd.Series(df[\"negativereason\"]).value_counts().plot(kind = \"barh\" , title = \"negativereason\")\n#time conversion\ndf[\"tweet_created\"] = df[\"tweet_created\"].apply(lambda x: pd.to_datetime(x))\ndf[\"hour\"] =  df[\"tweet_created\"].apply(lambda x: x.hour)\ndf[\"dayofweek\"] =  df[\"tweet_created\"].apply(lambda x: x.dayofweek) #monday = 0 Sunday = 6\nplt.figure(7)\ntmp = pd.Series(df[\"hour\"]).value_counts().sort_index().plot(kind = \"barh\" , title = \"hour\")\nplt.figure(8)\npd.Series(df[\"dayofweek\"]).value_counts().sort_index().plot(kind = \"barh\" , title = \"dayofweek\")\n\n\n#exploring correlation between features\npd.crosstab(index = df[\"airline\"] ,  columns = df[\"airline_sentiment\"] ).plot(kind = \"barh\")\n\n#the tweets themselves\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nstemmer = SnowballStemmer(\"english\")\nimport re\nfrom bs4 import BeautifulSoup  \n\n          \ndef cleanword(w):    \n    return re.sub('[^a-zA-Z,]' , ' ' , w)  \n\ndef cleantext(review):\n    review = BeautifulSoup(review ,\"lxml\").get_text()\n    review_words = cleanword(review.lower()).split()    \n    stop = stopwords.words('english')\n    stemmed_words = [stemmer.stem(w) for w in review_words if w not in stop]\n    return \" \".join(stemmed_words)\n\n\nprint (cleantext(\"GSW2015!Winners rasta! so tomatoes\"))\ndf[\"text\"]  = df[\"text\"].apply(cleantext)\n\n\n################# ML ##############################################3\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cross_validation import train_test_split \n \nX_clean = df[\"text\"] \n#X_train, X_test, y_train, y_test = train_test_split(X_clean, Y, test_size=0.33, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X_clean, df[\"airline_sentiment\"], test_size=0.33, random_state=42)\n\nvectorizer = TfidfVectorizer(max_df=0.5, max_features=2000, min_df=2, stop_words='english')\nvectorizer.fit(X_clean)                                 \nX_train = vectorizer.transform(X_train)\nX_test = vectorizer.transform(X_test)\n\n\n\n# select K best features\nfrom sklearn.feature_selection import SelectKBest\nword_list = vectorizer.get_feature_names()\nterm_doc_mat = vectorizer.fit_transform(X_clean)  \nselector = SelectKBest(k=10).fit(term_doc_mat, df[\"airline_sentiment\"])\ninformative_words_index = selector.get_support(indices=True)\nlabels = [word_list[i] for i in informative_words_index]\ndata = pd.DataFrame(term_doc_mat[:,informative_words_index].todense(), columns=labels)\ndata['airline_sentiment'] = df[\"airline_sentiment\"]\nprint (data.corr())\n#sns.heatmap(data.corr())\n\n\n#using the metrics package\nfrom sklearn.metrics import *\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \ndef printScores(y_test, y_pred, classif_name):    \n    print ( \"--------------  \"  + classif_name + \"  ------------------\" ) \n    print (\"recall : %0.2f\" %  recall_score(y_test, y_pred) )\n    print (\"precision : %0.2f\" %  precision_score(y_test, y_pred) )   \n    print (\"f1 : %0.2f\" %  f1_score(y_test, y_pred)  )\n    print (\"accuracy : %0.2f\" %  accuracy_score(y_test, y_pred)  )\n    print (\"---------------------------------------------------\" ) \n    \n#multinomial NB\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprintScores(y_test, y_pred, \"MultinomialNB\")\n\n#logreg \nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprintScores(y_test, y_pred, \"LogisticRegression\")\n\n#random forest\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 100)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprintScores(y_test, y_pred, \"RandomForestClassifier\")\n\n#knn \nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=7)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nprintScores(y_test, y_pred, \"KNeighborsClassifier\")\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}