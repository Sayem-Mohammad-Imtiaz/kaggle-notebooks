{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"! pip install trax","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport trax\nfrom trax import data\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The `trax` deep learning framework\n\nReference:\n- https://github.com/google/trax \n\n`trax` is coming out of the Google Brain team and is the latest iteration after almost a decade of work on TensorFlow, machine translation, and Tensor2Tensor. Being a new-comer in a somewhat crowded space (`keras`, `pytorch`, `thinc`), it has been able to learn from the mistakes or the best practices of those APIs.\nIn particular:\n- it is very concise\n- it runs on `TensorFlow` backend\n- it uses `Jax` to speed up tensor-based computation (instead of `numpy`)\n\n## The AG News dataset\n\nA great dataset to look into text classification. https://www.tensorflow.org/datasets/catalog/ag_news_subset\n\n- 0 is \"World News\"\n- 1 is \"Sports News\"\n- 2 is \"Business News\"\n- 3 is \"Science-Technology News\""},{"metadata":{},"cell_type":"markdown","source":"# Dataset\nI'm not using the Kaggle dataset here, but rather the TensorFlow dataset as it is more convenient with `trax`.\n`trax` needs generators of data. Each element is a tuple (input, target) or (input, target, weight) (usually weight is =1 because all examples have the same importance)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# you need to run this cell twice in Kaggle\n# Reference: https://www.tensorflow.org/datasets/catalog/ag_news_subset\ntrain_stream = data.TFDS('ag_news_subset', keys=('description', 'label'), train=True)()\neval_stream = data.TFDS('ag_news_subset', keys=('description', 'label'), train=False)()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(next(train_stream))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pipeline = data.Serial(\n    data.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n    data.Shuffle(),\n    data.FilterByLength(max_length=2048, length_keys=[0]),\n    data.BucketByLength(boundaries=[  32, 128, 512, 2048],\n                             batch_sizes=[512, 128,  32,    8, 1],\n                             length_keys=[0]),\n    data.AddLossWeights()\n)\ntrain_batches_stream = data_pipeline(train_stream)\neval_batches_stream = data_pipeline(eval_stream)\nexample_batch = next(train_batches_stream)\nprint(f'shapes = {[x.shape for x in example_batch]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\n`trax` is really concise, you can use the library of layers available."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tl.Serial(\n    tl.Embedding(vocab_size=8192, d_feature=50),\n    tl.Mean(axis=1),\n    tl.Dense(4),\n    tl.LogSoftmax()\n)\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\nFor training, there is the concep of a \"task\" which wraps the data, the optimiser, the metrics etc..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training task.\ntrain_task = training.TrainTask(\n    labeled_data=train_batches_stream,\n    loss_layer=tl.CrossEntropyLoss(),\n    optimizer=trax.optimizers.Adam(0.01),\n    n_steps_per_checkpoint=500,\n)\n\n# Evaluaton task.\neval_task = training.EvalTask(\n    labeled_data=eval_batches_stream,\n    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n    n_eval_batches=20  # For less variance in eval numbers.\n)\n\n# Training loop saves checkpoints to output_dir.\noutput_dir = os.path.expanduser('~/output-dir/')\n!rm -rf {output_dir}\ntraining_loop = training.Loop(model,\n                              train_task,\n                              eval_tasks=[eval_task],\n                              output_dir=output_dir)\n\n# Run 2000 steps (batches).\ntraining_loop.run(2000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Look at predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs, targets, weights = next(eval_batches_stream)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_input = inputs[0]\nexpected_class = targets[0]\nexample_input_str = trax.data.detokenize(example_input, vocab_file='en_8k.subword')\nprint(f'example input_str: {example_input_str}')\nsentiment_log_probs = model(example_input[None, :])  # Add batch dimension.\nprint(f'Model returned sentiment probabilities: {np.exp(sentiment_log_probs)}')\nprint(f'Expected class: {expected_class}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}