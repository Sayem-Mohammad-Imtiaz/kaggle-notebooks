{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AIRQUALITY DATASET EXPLORATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install miceforest\n{'name':'Álvaro Riobóo de Larriva',\n'start_date':'2021/03/09'}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#--< main modules >--#\nimport numpy as np \nimport pandas as pd \nimport scipy as sp\nimport statsmodels.api as sm\nimport sklearn\n\n#--< visualization >--#\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotnine as p9\n\n#--< others >--#\nfrom datetime import datetime\nimport time\nfrom IPython.display import display \nimport gc\n\nfor i in locals().copy():\n    try:\n        print(\"%s:\"%eval(i).__name__, eval(i).__version__)\n    except:\n        continue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### REFERENCE LINKS:\n- [UCI ML Airquality Dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality) : Descripción de los datos\n- [AirQualityUCI.csv](https://github.com/shrikumarp/airquality/blob/master/AirQualityUCI.csv) : Repositorio con los datos en CSV. Usaremos la versión raw para leer desde GitHub.\n\n\n*Columns:*\nIndex(['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)',\n       'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)',\n       'PT08.S5(O3)', 'T', 'RH', 'AH'],\n      dtype='object')\n      \n**Data Set Information:**\n\nThe dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. The device was located on the field in a significantly polluted area, at road level,within an Italian city. Data were recorded from March 2004 to February 2005 (one year)representing the longest freely available recordings of on field deployed air quality chemical sensor devices responses. Ground Truth hourly averaged concentrations for CO, Non Metanic Hydrocarbons, Benzene, Total Nitrogen Oxides (NOx) and Nitrogen Dioxide (NO2) and were provided by a co-located reference certified analyzer. Evidences of cross-sensitivities as well as both concept and sensor drifts are present as described in De Vito et al., Sens. And Act. B, Vol. 129,2,2008 (citation required) eventually affecting sensors concentration estimation capabilities. Missing values are tagged with -200 value.\nThis dataset can be used exclusively for research purposes. Commercial purposes are fully excluded.\n\n**Attribute Information:**\n\n0. Date (DD/MM/YYYY)\n1. Time (HH.MM.SS)\n\n2. True hourly averaged concentration CO in mg/m^3 (reference analyzer)\n3. PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)\n\n4. True hourly averaged overall Non Metanic HydroCarbons concentration in microg/m^3 (reference analyzer)\n> 5. True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)\n6. PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)\n\n7. True hourly averaged NOx concentration in ppb (reference analyzer)\n8. PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)\n\n9. True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)\n10. PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)\n\n> 11. PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)\n\n12. Temperature in Â°C\n13. Relative Humidity (%)\n14. AH Absolute Humidity "},{"metadata":{"trusted":true},"cell_type":"code","source":"###----------------< START of 'airquality_nb.ipynb'>---------------###","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 0. LOAD DATASET AND PARAMETERS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATASET LOADING\nairq = pd.read_csv(\"https://raw.githubusercontent.com/shrikumarp/airquality/master/AirQualityUCI.csv\",\n                  delimiter=\";\", decimal=\",\", \n                  usecols=range(15)) # dropna deletes empty lines, we need NAN the -200 values.\nairq = airq.dropna().replace(-200,np.nan) # 1 more fix after\n\nprint('Need to fix columns names a bit: %a\\n'%airq.columns)\nairq.columns = [c[0] for c in airq.columns.str.split(\"(\")]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA CLEANING & PARAMETERS\n##--<INDEX>--## # TIME_INDEX, drop rest of time-referers\ntry:\n    time_index = pd.Series([datetime.strptime(i, \"%d/%m/%Y%H.%M.%S\") for i in airq['Date'] + airq['Time']])\n    airq.insert(0, 'time_index', time_index)\n    airq = airq.drop(['Time','Date'], axis=1)\nexcept Exception as e:\n    print('Ya se ha realizado la operacion anteriormente\\nError:',e)\n\ndate_range = (airq['time_index'].min(), airq['time_index'].max())\nprint(date_range)\n\n##---<DTYPE COLS>---##\nnumeric_cols = airq.select_dtypes(np.float64).columns # all numericals were parsed as np.float64\ndate_cols = ['time_index']                           # time_index from dataset\nsensor_cols = airq.columns[airq.columns.str.startswith('PT08')]\nabundance_cols = ['CO','NMHC','NOx','NO2','C6H6']    # last 'C6H6' is not measured, they are put in order with sensor_cols\nairprops_cols = ['T','RH','AH']                      # air properties\nsensor_refs = [par for par in zip(sensor_cols,abundance_cols[:-1])]\n\n##---<PRINTS>---##\nfor i in airq.isna().sum():\n        print(\"NAN: %5i    p: %.2f \"%(i,i/len(airq)))\n        \nprint('\\nSum NAN values:%i'%airq.isna().sum().sum())\nprint('\\nNew column names:%a\\n'%airq.columns)\ndisplay(airq.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensor_refs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. EXPLORATORY DATA ANALYSIS (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPUTE MISSING DATA WITH MICEFOREST -> MICE AND RANDOM FOREST GUESSING.\nimport miceforest as mf\n\nairq_amp = mf.ampute_data(airq.reset_index()[numeric_cols], random_state=1994)\n\n# Create kernel\nkds = mf.KernelDataSet(\n  airq_amp,\n  save_all_iterations=True,\n  random_state=1994\n)\n\n# Run the MICE algorithm for 3 iterations\nkds.mice(3)\n\n# Return the completed kernel data\nimputed_data = kds.complete_data()\n\ngc.collect()\n\nprint('Before imputation:%a'%airq.isna().sum())\nprint('Total NAN values after imputation:', imputed_data.isna().sum().sum())\n\nairq[numeric_cols] = imputed_data\n\nairq.set_index('time_index', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display plain information, types and main stats.\ndescr = airq.describe()\nnunique_values = airq.apply( lambda x: x.nunique())\n\nairq.info()\nprint('number of unique values:\\n%a'%nunique_values)\ndisplay(descr)\n\nqlabels = ['min','25%', '50%', '75%', 'max']\nqvals = descr.loc[qlabels] # we keep quartile values from df description \nstatslabels = ['count','mean','std']\nstatsvals = descr.loc[statslabels] # also keep mean statistics\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CORRPLOT\nmask = np.zeros_like(airq.corr(), dtype=np.bool)\nmask[np.tril_indices_from(mask)] = True\n\nplt.figure(figsize=(20,5))\nsns.heatmap(airq.corr(), mask=np.triu(np.ones_like(airq.corr())), cmap=plt.cm.viridis, annot=True, fmt=\".2f\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver información interesante en nuestra matriz de correlación. Comentaremos la respuesta del sensor frente a la concentración que pretende medir.\n\n- PT08.S1(C0)/C0:     0.88 (BUENA)\n- PT08.S2(NMHC)/NMHC: 0.90 (BUENA)\n- PT08.S3(NOx)/NOx:  -0.66 (ANTICORR)\n- PT08.S4(NO2)/NO2:   0.19  (POBRE)\n- PT08.S5(O3): Buena correlación en torno a las demás concentraciones y sensores, no hay referencia de O3. No correlaciona con AH y poco con RH y T.\n- C6H6: Buena correlación en torno a las demás concentraciones y sensores, no hay sensor de C6H6. No correlaciona con las propiedades del aire (T,RH,AH).\n- Las propiedades del aire no son buenos predictores de las demás variables, ni siquiera se correlacionan entre sí."},{"metadata":{"trusted":true},"cell_type":"code","source":"# PAIRPLOT\nsns.pairplot(airq[sensor_cols])\nsns.pairplot(airq[abundance_cols])\nsns.pairplot(airq[airprops_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por un estudio similar [Fig 3., pag 1524](https://www.researchgate.net/publication/311459930_Summertime_ambient_ammonia_and_its_effects_on_ammonium_aerosol_in_urban_Beijing_China) realizado en China, vemos que han aproximado las curvas de concentración de gases a dos Lorentzianas, una para el aire limpio y otra para el contaminado, con buenos resultados. De hecho, se pueden ver dos picos propios de las dos distribuciones en la mayoría de gráficas de este tipo. Un ajuste de estas curvas se expera que dé como resultado los picos (~ medias de la distribución) en un aire limpio y otro contaminado.\n\n- En los gráficos de sensores, se aprecia una correlación positiva entre todos ellos, menos en el caso del S3, que anticorrelaciona con los demás.\n\n- En los gráficos de concentraciones de gases, se aprecia una correlación positiva y clara entre todos ellos.\n\n- En los gráficos de propiedades del aire, se aprecia una correlación positiva, aunque también una dependencia entre la humedad relativa y la absoluta."},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOx vs. S5(NOx)\nsns.lmplot(x='PT08.S3',y='NOx', scatter_kws={'color':'grey','alpha':.6}, data=airq,line_kws={'color':'green'})  # Vemos que efectivamente NOx y su sensor tienen correlación negativa.\nplt.title(\"NOx & sensor S5(NOx)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AH vs. RH\nnormalize = lambda x: (x-x.mean())/x.std()\n\nfig,axs = plt.subplots(1,3, figsize=(10,5))\nwith plt.style.context('seaborn'):\n    airq.plot(kind='scatter', x='RH', y='AH', ax=axs[0],\n             color=\"red\" )\n    axs[0].set_title('Scatter plot AH/RH')\n    \n    airq.plot(kind=\"hist\", y='AH', ax=axs[1], \n              bins=50, fill='black', density=True, grid=True, rwidth=0.5)\n    axs[1].set_xlabel('AH')\n    axs[1].set_title('Histogram of AH')\n    \n    airq.plot(kind=\"hist\", y='RH', ax=axs[2], \n              bins=50, fill='black', density=True, grid=True, rwidth=0.5, color=\"green\")\n    axs[2].set_xlabel('RH')\n    axs[2].set_title('Histogram of RH')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HISTOGRAMS\nfig, axs = plt.subplots(1,2, figsize=(10,5))\nairq.plot(kind=\"hist\", bins=30, density=True, alpha=0.7, ax=axs[0])\naxs[0].set_title('Histogram of numeric variables')\nairq.plot(kind=\"hist\", bins=30, density=True, alpha=0.7, stacked=True, ax=axs[1])\naxs[1].set_title('Histogram of numeric variables (stacked)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DENSITYPLOT of normalized dataset.\nfig ,ax = plt.subplots(figsize=(20,10))\nnormalize(airq).plot(kind=\"density\", ax=ax,\n                    style='-*', ms=0.6, cmap=plt.cm.Accent)\nplt.axvline(x=0,**{'marker':'^','color':'black'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"En el gráfico anterior normalizado, solemos ver mayormente poblaciones sesgadas a la izquierda de su media (centrada en 0). Vemos algunas distribuciones bimodales, pero sobre todo se pueden  observar exponenciales negativas, en teoría correspondientes a la referencia de las concentraciones de los gases que estamos midiendo. La ley que describe el comportamiento a grandes rasgos de la concentración de un gas es: $\\frac{dN}{dt} = -\\lambda N \\rightarrow N(t)=N_0 e^{-t/\\tau}$, que nos dice que la concentración disminuye proporcionalmente a la concentración en ese momento, por lo que esperamos que estas distribuciones sean exponenciales negativas en el tiempo ($\\tau$ es el tiempo de vida medio, por el cual la concentración disminuye en $1/e$ de su valor original). Podríamos más adelante ajustar a las generalizables funciones gamma."},{"metadata":{"trusted":true},"cell_type":"code","source":"# DENSITYPLOT dists of gases:\nfig, axs = plt.subplots(1,2, figsize=(20,5), sharey=True)\nnormalize(airq[abundance_cols]).plot(kind=\"density\", ax=axs[0])\naxs[0].axvline(x=0, color='black', marker=\"|\" ,linestyle=\"--\")\naxs[0].set_title(r'Distributions: Concentration of gas')\n\nnormalize(airq[sensor_cols]).plot(kind=\"density\", ax=axs[1])\naxs[1].axvline(x=0, color='black', marker=\"|\" ,linestyle=\"--\")\naxs[1].set_title(r'Distributions: Sensor measurements of gas')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver en los gráficos anteriores que la respuesta de los sensores a las abundancias las tendencias bimodales son inapreciables y en general se parece más a una única distribución normal. La referencia posee picos mucho más puntiagudos, mientras que los sensores parecen diseñados para tener una respuesta normal."},{"metadata":{},"cell_type":"markdown","source":"Veremos ahora las propiedades del aire (Temperature, Relative Humidty, Absolute Humidity). Investigamos ésto un poco más en ventanas temporales."},{"metadata":{"trusted":true},"cell_type":"code","source":"# AIR PROPERTIES\nrolling_freqs = ['24h', '7d','30d','120d','365d']\nairprops = airq[airprops_cols]\n\nwith plt.style.context('seaborn-notebook'): \n    \n    fig, axs = plt.subplots(1,len(rolling_freqs), figsize=(15,4))   \n    for i,freq in enumerate(rolling_freqs):\n        airprops.rolling(freq).mean().plot(ax=axs[i], grid=True)\n        axs[i].set_title(\"Air_props\\ win:%s\"%freq)\n        labels = axs[i].get_xticks()\n        plt.setp(axs[i].xaxis.get_majorticklabels(), rotation=90 )\n        \n    fig, axs = plt.subplots(1,len(rolling_freqs), figsize=(15,4))\n    for i,freq in enumerate(rolling_freqs): \n        normalize(airprops).rolling(freq).mean().plot(ax=axs[i], grid=True)\n        axs[i].set_title(\"N(0,1) Air_props\\ win:%s\"%freq)\n        plt.setp(axs[i].xaxis.get_majorticklabels(), rotation=90 )\n        \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Hemos escogido ventanas temporales para 1 dia,1 semana (7d), 1 mes (30d), 4 meses (120d), 1 año (365d). Se puede observar que nuestros datos son insuficientes para mostrar un ciclo estacional completo. Se estima que hay un ciclo de alrededor de 1 año, pero como tenemos una muestra temporal escasa y afectada gravemente por la tendencia positiva, no podemos concluirlo a simple vista. \n\nHabría que realizar un modelo de autoregresión y media móvil **(ARIMA)** para poder estimar la tendencia y estacionalidad de una manera más rigurosa. Se pueden valorar a simple vista 3 frecuencias naturales de nuestros datos:\n\n- ~ 1-2 semanas\n- ~ 1 mes\n- ~ 1 año\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"hour_fraction_crit_values = []\nfor crit_value in [50,100,150,200]:\n    hour_fraction_crit_values.append(airq['NO2'].where(airq['NO2'] >=crit_value).count()/len(airq))\nprint(hour_fraction_crit_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Éste dataset no tiene un target pre-establecido. Sin embargo, el S4 no correlaciona con la concentración **NO2** para el cual está diseñado. Siendo el dióxido de nitrógeno un gas peligroso para el ser humano y proveniente de vehículos, plantas eléctricas, emisión industrial, construcción, y en definitiva de combustibles fósiles, debemos intentar medir y predecir la concentración del gas y pretender que se sitúe en un rango razonable. \n    - (Bueno: 0-50, Moderado: 50-100, Peligroso para grupos sensibles: 100-150, Insano: 150-200). \n    - Mean: 111.4 , Max: 340\nSegún el output de la celda anterior, aprox. un ~21% del tiempo el nivel se sitúa por encima de la calificación de Peligroso, y un ~4% de éste en el nivel Insano. Ésto podría afectar gravemente a la población que viva o transite por éste lugar a lo largo del día. Por ello, **NO2 será nuestra variable objetivo en este estudio**, aunque también podría estudiarse la concentración de **NOx**, otro gas tóxico y peligroso."},{"metadata":{},"cell_type":"markdown","source":"## 2. FINDING CANDIDATE ALGORITHMS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA/TARGET definition\ndata = airq.reset_index().drop(['time_index','NO2'], axis=1)\ntarget = airq.reset_index().drop('time_index', axis=1)['NO2']\n\n# TRAIN/TEST subsets\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=1994)\n\n# SCALER\nrob = RobustScaler()\nX_train = rob.fit_transform(X_train)\nX_test = rob.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debemos estandarizar nuestro dataset, es decir, convertir nuestros datos a rangos y dispersión similares. Usaremos el RobustScaler(), puesto que nos ayudará a minimizar el impacto de los outliers sustrayendo la mediana y dividiendo por el rango intercuartil de nuestros datos.\n\nLa variable target será **NO2** como se tratará de justificar un poco más adelante.\n\nPara la búsqueda de algoritmos candidatos, usaremos los siguientes para el problema de regresión:\n\n- LinearRegression()\n- KNeighborsRegressor()\n- DecisionTreeRegressor()\n- RandomForestRegressor()\n- SVR()\n- XGBRegressor()\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We define a function to evaluate all models in a dictionary.\ndef train_all_models(models_dict, train_now=True):\n    if train_now:\n        for k,m in models_dict.items():\n            start_time = time.time()\n            print('MODEL:%s'%k)\n            m.fit(X_train, y_train)\n            print(\"(model_runtime= %.2f s)\\n\"%(time.time() - start_time))\n    else: \n        print(\"Not training for now\")\n    gc.collect()\n    return models_dict\n\ndef plot_predict_all_models(models_dict):\n    for k,m in models_dict.items():\n        y_pred = m.predict(X_test)\n        print('MODEL :%s\\n'%k, \n              'R^2= %.3f\\n'%r2_score(y_test, y_pred),\n              'MSE= %.3f\\n'%mean_squared_error(y_test, y_pred) , \n              'MAE= %.3f\\n'%mean_absolute_error(y_test, y_pred))\n        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n        axs[0].set_title(\"Predicted/Observed\")\n        axs[0].set_xlabel(\"Observed\") ; axs[0].set_ylabel(\"Predicted\")\n        axs[0].scatter(x=y_test, y=y_pred, color=\"g\", marker=\"o\", alpha=0.3)\n\n        axs[1].set_title(\"Error distribution\")\n        axs[1].set_xlabel(\"X\"); axs[1].set_ylabel(\"Y\")\n        axs[1].hist( y_pred-y_test, bins=30, rwidth=0.6, density=True)\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit\n# MODELS WITH NO PARAMETERS (VANILLA)\n\nLr = LinearRegression()\nKNr = KNeighborsRegressor()\nDTr = DecisionTreeRegressor()\nRFr = RandomForestRegressor()\nSVr = SVR()\nXGBr = XGBRegressor()\n\nmodels_vanilla = {'Lr':Lr,\n                 'KNr':KNr,\n                 'DTr':DTr,\n                 'RFr':RFr,\n                 'SVr':SVr,\n                 'XGBr':XGBr}\n\n\n\nmodels_vanilla = train_all_models(models_vanilla)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## EVALUATION VANILLA_MODELS\nplot_predict_all_models(models_vanilla)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Vemos que nuestros modelos 'vanilla' mejor parados han sido:\n- Random Forest: $R^2=0.886$\n- XGBoost: $R^2=0.869$\n- K-Nearest Neighbours: $R^2=0.859$"},{"metadata":{},"cell_type":"markdown","source":"## 3. EVALUATION AND IMPROVEMENTS FOR THOSE ALGORITHMS"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Para la optimización de nuestros hiperparámetros, usaremos \"RandomizedSearchCV\" de sklearn para quedarnos con la mejor combinación posible de hiperparámetros para cada algoritmo.\n\nExpondremos gráficos y los resultados principales de $R^2$ de nuestros modelos con los datos. \n\nGuardaremos los resultados de los modelos anteriores ('train_accuracy','test_accuracy') en un DataFrame y exportaremos a csv. Exportaremos también el mejor modelo con formato 'pickle'."},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit\n# MODELS AND KEYWORD PARAMETERS DEFINED: RANDOMIZEDSEARCHCV  # NOTE: THIS IS THE MOST TIME-DEMANDING\nLr_kw = {'fit_intercept':[True,False],\n        'normalize':[True,False],\n        'positive':[True,False]}\nLr_cv = RandomizedSearchCV(Lr, Lr_kw, \n                           cv=4, random_state=1994)\n\nKNr_kw = {'n_neighbors':np.arange(1,13),\n          'weights':['uniform','distance'],\n          'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n          'leaf_size':np.arange(12,30)}\nKNr_cv = RandomizedSearchCV(KNr, KNr_kw, \n                            cv=5, random_state=1994)\n\nDTr_kw = {\"splitter\" : [\"best\",\"random\"], \n        \"min_samples_leaf\" : np.arange(1,9), \n        \"max_features\" : [\"auto\",\"sqrt\",\"log2\", None], \n        \"max_depth\" : [3,4,5,None], \n        \"criterion\" : ['mse',\"mae\",\"poisson\"]}\nDTr_cv = RandomizedSearchCV(DTr, DTr_kw,\n                           cv=5, random_state=1994)\n          \nRFr_kw = {\"n_estimators\" : [50,75,100], \n          \"min_samples_leaf\" : np.arange(1,7, 2), \n          \"max_features\" : [\"auto\",\"sqrt\",\"log2\"], \n          \"max_depth\" : [4,5,6], \n          \"criterion\" : ['mse','mae']}\nRFr_cv = RandomizedSearchCV(RFr, RFr_kw,\n                           cv=4, random_state=1994)\n          \nSVr_kw = {\"kernel\" : [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n          \"degree\": [2, 3, 4],\n          \"gamma\" : [\"scale\",\"auto\"]\n         }\nSVr_cv = RandomizedSearchCV(SVr, SVr_kw,\n                           cv=5, random_state=1994)\n          \nXGBr_kw = {'n_estimators' : [100, 500, 1000],\n    'max_depth' : [2, 3, 5, 10],\n    'learning_rate' : [0.05, 0.1, 0.15],\n    'min_child_weight' : [1, 2, 3, 4],\n    'booster' : ['gbtree','gblinear'],\n    'base_score' : [0.25, 0.5, 0.75, 1]}\nXGBr_cv = RandomizedSearchCV(XGBr, XGBr_kw,\n                            cv=3, n_iter=5, \n                            scoring = 'neg_mean_absolute_error',\n                            n_jobs = -1,\n                            verbose = 5, \n                            return_train_score = True,\n                            random_state=1994)\n\n\nmodels_CV = {'Lr':Lr_cv,\n            'KNr':KNr_cv,\n            'DTr':DTr_cv,\n            'RFr':RFr_cv,\n            'SVr':SVr_cv,\n            'XGBr':XGBr_cv}\n\nmodels_CV = train_all_models(models_CV, train_now=True)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PRINT BEST_PARAMS FOUND BY RANDOM SEARCH\nfor k,v in models_CV.items():\n    print(\"Model:%s\"%k,\"\\nBest_parameters:%a\"%v.best_params_)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit\n### MODELS AFTER HYPERPARAMETERS TUNING: IMPROVED\nLr_imp = LinearRegression(**models_CV['Lr'].best_params_) \nKNr_imp = KNeighborsRegressor(**models_CV['KNr'].best_params_)\nDTr_imp = DecisionTreeRegressor(**models_CV['DTr'].best_params_)\nRFr_imp = RandomForestRegressor(**models_CV['RFr'].best_params_)\nSVr_imp = SVR(**models_CV['SVr'].best_params_)\nXGBr_imp = XGBRegressor(**models_CV['XGBr'].best_params_)\n\nmodels_improved = {\"Lr\" : Lr_imp,\n                  \"KNr\" : KNr_imp,\n                  \"DTr\" : DTr_imp,\n                  \"RFr\" : RFr_imp,\n                  \"SVr\" : SVr_imp,\n                  \"XGBr\" : XGBr_imp}\n\nmodels_improved = train_all_models(models_improved)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## EVALUATION CV_MODELS\nplot_predict_all_models(models_improved)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit\n### BAGGING ENSEMBLE TO ALL METHODS\nLr_bag = BaggingRegressor(LinearRegression(**models_CV['Lr'].best_params_)) \nKNr_bag = BaggingRegressor(KNeighborsRegressor(**models_CV['KNr'].best_params_))\nDTr_bag = BaggingRegressor(DecisionTreeRegressor(**models_CV['DTr'].best_params_))\nRFr_bag = BaggingRegressor(RandomForestRegressor(**models_CV['RFr'].best_params_))\nSVr_bag = BaggingRegressor(SVR(**models_CV['SVr'].best_params_))\nXGBr_bag = BaggingRegressor(XGBRegressor(**models_CV['XGBr'].best_params_))\n\n\nmodels_bag = {\"Lr\" : Lr_bag,\n             \"KNr\" : KNr_bag,\n             \"DTr\" : DTr_bag,\n             \"RFr\" : RFr_bag,\n             \"SVr\" : SVr_bag,\n             \"XGBr\" : XGBr_bag}\n\n# TRAIN\nmodels_bag = train_all_models(models_bag, train_now=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## EVALUATION BAG_MODELS\nplot_predict_all_models(models_bag)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit\n# MAIN RESULTS INTO DATAFRAMES\nalgorithms = ['Linear Regression',\n            'K-Nearest Neighbour Regressor',\n            'Decision Tree Regressor', \n            'Random Forest Regressor',\n            'Support Vector Machine Regressor',\n            'XGBoost']\n\nvanilla_results = pd.DataFrame( {\"Algorithm\": algorithms,\n                          \"Train_score\":[m.score(X_train,y_train) for m in models_vanilla.values()],\n                          \"Test_score\":[m.score(X_test,y_test) for m in models_vanilla.values()]\n                                }).sort_values('Test_score', ascending=False)\n\nimproved_results = pd.DataFrame({\"Algorithm\": algorithms,\n                          \"Train_score\":[m.score(X_train,y_train) for m in models_improved.values()],\n                          \"Test_score\":[m.score(X_test,y_test) for m in models_improved.values()]\n                                }).sort_values('Test_score', ascending=False)\n\nbag_results = pd.DataFrame({\"Algorithm\": algorithms,\n                          \"Train_score\":[m.score(X_train,y_train) for m in models_bag.values()],\n                          \"Test_score\":[m.score(X_test,y_test) for m in models_bag.values()]\n                               }).sort_values('Test_score', ascending=False)\n\ndisplay(\"VANILLA:\", vanilla_results)\ndisplay(\"IMPROVED:\", improved_results)\ndisplay(\"BAGGING:\", bag_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EXPORT CLEANED AND IMPUTED DATASET TO CSV (for production)\nairq.to_csv(\"airq.csv\", header=True)\n\n# EXPORTS TO CSV\nvanilla_results.to_csv(\"airq_vanilla_results.csv\")\nimproved_results.to_csv(\"airq_improved_results.csv\")\nbag_results.to_csv(\"airq_bagging_results.csv\")\n\n# EXPORT TO PICKLE\nimport pickle\nmodel_filename = 'airq_xgboost_bag.sav'\npickle.dump(models_bag['XGBr'], open(model_filename, 'wb'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FEATURE IMPORTANCES\nfeature_importances = pd.Series(models_improved['XGBr'].feature_importances_).sort_values(ascending=False)\nfeature_importances.index = feature_importances.index.map({ i:col for i,col in enumerate(data.columns)})\ndisplay(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver que el la referencia de 'NOx' resulta ser el mejor predictor, de lejos los sigue la humedad absoluta \"AH\", el gas \"CO\" y la humedad relativa (\"RH\"). El sensor de \"NHMC\" (PT08.S2) los sigue en importancia.\n\nComo adelantábamos por el mapa de correlación, el sensor asociado a \"NO2\" tiene pobre relevancia como predictor del mismo."},{"metadata":{},"cell_type":"markdown","source":"## <ins>*CONCLUSIONES*:</ins>\n\n- 1. ANÁLISIS EXPLORATORIO DE DATOS:\n\n    - La correlación de Pearson entre variables indica lo siguiente: S1(CO) y S2(NMCH) son buenos predictores lineales de sus concentraciones, S3(NOx) está anticorrelacionado con su variable, sin embargo, la transformación -np.log10(S3) llega a correlacionar 0.75 con NOx, S4 no correlaciona con NO2.\n    \n    - Los histogramas indican que las concentraciones siguen de forma más o menos diferenciada una distribución con dos jorobas, pudiéndose ajustar por dos lorentzianas diferentes, con un pico de aire limpio y otro de aire poluto. Los sensores y su respuesta promediada cada hora dan poca cuenta de este comportamiento, y se ajustan mejor en el caso de las concentraciones que no tienen picos muy altos y que pueden ajustarse a una distribución normal.\n    \n    - Las propiedades del aire (Temperatura, Humedad relativa y Humedad absoluta) no son buenos predictores de las demás variables, ni siquiera se correlacionan demasiado entre sí. La temperatura y humedad absoluta se correlacionan positivamente, y éstas negativamente con la humedad relativa.\n    \n    - Las propiedades del aire son muy cambiantes y siguen ciclos establecidos. Probablemente los ciclos de estacionalidad y tendencia relacionados con la serie temporal sean los que expliquen la variabilidad en las propiedades del aire. Por ejemplo, si estamos en verano la humedad absoluta, relativa y la temperatura serán consistentemente diferentes a las del invierno. Hemos distinguido visualmente 3 frecuencias para los datos (1-2 dias, 1-2 semanas, 1año), lo cual podría investigarse con modelos ARIMA.\n    \n- 2. BUSCANDO ALGORITMOS CANDIDATOS:\n\nÉste dataset no tiene un target pre-establecido. Sin embargo, el S4 no correlaciona con la concentración **NO2** para el cual está diseñado. Siendo el dióxido de nitrógeno un gas altamente peligroso para el ser humano y proveniente de vehículos, plantas eléctricas, emisión industrial, construcción, y en definitiva de combustibles fósiles, debemos intentar medir y predecir los valores de éste gas y que se sitúen en un rango razonable. \n    \n    - (Bueno: 0-50, Moderado: 50-100, Peligroso para grupos sensibles: 100-150, Insano: 150-200). \n    - Nuestros datos indican lo siguiente:      Mean=111.4 , Max=340\n\nPor tanto, hemos definido nuestro target como 'NO2', separando el resto de datos (de los cuales hemos quitado también el tiempo).\n\nHemos estandarizado nuestros datos con un escalador de mediana y rango intercuartil para minimizar el impacto de valores extremos. Luego, hemos separado nuestros datos en 'train/test' (sin validación cruzada para una mayor simplicidad), con una fracción de test del 30%.\n\nHemos usado los siguientes y famosos algoritmos de sklearn para problemas de regresión:\n\n    - LinearRegression(): Modelo más simple, ajusta los coeficientes por medio de minimizar diferencias cuadráticas.\n    - KNeighborsRegressor(): Modelo sencillo que, una vez se establece k (el número de predictores máximo), nos dará la importancia de predictores en torno a la variable de regresión.\n    - DecisionTreeRegressor(): Modelo sencillo que penaliza la distancia de nuestros datos en relación a valores de prueba , construye un árbol de decisión y permite realizar una regresión. Permite ver la importancia de predictores.\n    - RandomForestRegressor(): Modelo mejorado basado en árboles de decisión que permite promediar una muestra significativa de ellos, sus resultados, y ver la importancia de predictores.\n    - SVR(): Modelo de máquinas de soporte vectorial ('Support Vector Machines'), que busca hacer la mejor división de nuestros datos mediante hiperplanos que los separen.\n    - XGBRegressor(): Modelo complejo y óptimo ('eXtreme Gradient Boosting') basado en árboles de decisión, optimización del descenso del gradiente y una randomizazión de parámetros optimizada, entre otros.\n\nHemos usado 'RandomizedSearchCV' para buscar y quedarnos con los mejores hiperparámetros de nuestros algoritmos. \n\n**NOTA: Hemos evaluado el $R^2$ (score) del modelo y otras métricas, también hemos dibujado el gráfico de dispersión de la predicción respecto al objetivo, y el histograma del error asociado. Ésto lo hemos hecho para todos los modelos.**\n\n- 3. EVALUACIÓN Y MEJORAS PARA ÉSTOS ALGORITMOS:\n\nUna vez buscado los mejores parámetros para nuestros modelos, hemos evaluado nuestros modelos con éstos, lo que hemos llamado modelos \"improved\".\n\nLuego, hemos usado un método de ensamblado('BaggingRegressor') con los originales para mejorar nuestros modelos con parámetros optimizados, lo que hemos llamado modelos \"bagging\".\n\nLos algoritmos principales evaluados en test, tanto para los modelos \"improved\" como \"bagging\" nos dan como favoritos:\n    1. XGBoost\n    2. K-Nearest Neighbours\n    3. Random Forest\n\nSegún nuestro mejor modelo (XGBoost mejorado en hiperparámetros), que además nos permite estimar la importancia de los predictores en la variable objetivo, irían por orden:\n\nNOx        0.656528 \\\nAH         0.082007 \\\nCO         0.055414 \\\nRH         0.045607 \\\nPT08.S2    0.035066 \n\nLo cual quiere decir que prácticamente ninguno de nuestros sensores es útil para predecir la concentración de NO2."},{"metadata":{"trusted":true},"cell_type":"code","source":"###----------------< END of 'airquality_nb.ipynb'>---------------###","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}