{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Make numpy values easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/walmart-recruiting-store-sales-forecasting/\"\ndataset = pd.read_csv(path + \"train.csv.zip\", names=['Store','Dept','Date','weeklySales','isHoliday'],sep=',', header=0)\nfeatures = pd.read_csv(path + \"features.csv.zip\",sep=',', header=0,\n                       names=['Store','Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4',\n                              'MarkDown5','CPI','Unemployment','IsHoliday']).drop(columns=['IsHoliday'])\nstores = pd.read_csv(path + \"stores.csv\", names=['Store','Type','Size'],sep=',', header=0)\ndataset = dataset.merge(stores, how='left').merge(features, how='left')\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = dataset.groupby(['Store', 'Dept', 'Date'])['weeklySales'].sum().unstack()\nprint(sales.shape)\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.isna().sum().hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.isna().sum(axis=1).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize values\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata = scaler.fit_transform(sales).astype(np.float32)\nsales_scaled = pd.DataFrame(data=data, columns=sales.columns, index=sales.index)\nsales_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale it back\ndata_inv = scaler.inverse_transform(sales_scaled)\nsales_inv = pd.DataFrame(data=data_inv, columns=sales.columns, index=sales.index)\nsales_inv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the complete rows\nsales_complete = sales_scaled[sales_scaled.isna().sum(axis=1) == 0]\nsales_complete.isna().sum(axis=1).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no = sales_complete.shape[0] # No\ndim = sales_complete.shape[1]\nh_dim = dim\n\n# System parameters\nbatch_size = 128 # mb_size\nhint_rate = 0.9 # p_hint\nalpha = 10 # loss hyperparameter\ntrain_rate = 0.8\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generator\ndef make_generator_model(dim, h_dim):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(h_dim, input_shape=(dim*2,), activation='relu'))\n    model.add(tf.keras.layers.Dense(h_dim, activation='relu'))\n    model.add(tf.keras.layers.Dense(dim, activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discriminator\ndef make_discriminator(dim, h_dim):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(h_dim, input_shape=(dim*2,), activation='relu'))\n    model.add(tf.keras.layers.Dense(h_dim, activation='relu'))\n    model.add(tf.keras.layers.Dense(dim, activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use generator to create an instance\ngenerator = make_generator_model(dim, h_dim)\n\nind = 123\nX_mb_original = sales_complete.iloc[ind, :].to_numpy()\nM_mb = np.random.choice([0.0, 1.0], size=dim).reshape([1, -1]).astype(np.float32)\nZ_mb = np.random.rand(1, dim) * 0.01\nX_mb = X_mb_original * M_mb + Z_mb * (1 - M_mb)\ninput_gen = np.hstack([X_mb, M_mb])\n\nG_sample = generator(input_gen, training=False)\nprint(input_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_mb_original)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(M_mb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(G_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the discriminator to detect mask\ndef binary_sampler(p, rows, cols):\n    unif_random_matrix = np.random.uniform(0.0, 1.0, size=[rows, cols])\n    binary_random_matrix = (unif_random_matrix < p).astype(np.float32)\n    return binary_random_matrix\n\n# Sample hint vectors\nH_mb_temp = binary_sampler(hint_rate, 1, dim)\nH_mb = M_mb * H_mb_temp\ninput_disc = np.hstack([G_sample, H_mb])\n\ndiscriminator = make_discriminator(dim, h_dim)\nD_prob = discriminator(input_disc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(D_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GAIN loss\ndef G_loss(X, M, G_sample, D_prob):\n#     X = real_output[:dim]\n#     M = real_output[dim:]\n    G_loss_temp = -tf.reduce_mean((1-M) * tf.math.log(D_prob + 1e-8))\n    MSE_loss = tf.reduce_mean((M * X - M * G_sample)**2) / tf.reduce_mean(M)\n    total_loss = G_loss_temp + alpha * MSE_loss \n    return total_loss\n\ndef D_loss(M, D_prob):\n    D_loss_temp = -tf.reduce_mean(M * tf.math.log(D_prob + 1e-8) \\\n                                + (1-M) * tf.math.log(1. - D_prob + 1e-8))\n    total_loss = D_loss_temp\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G_loss_curr = G_loss(X_mb, M_mb, G_sample, D_prob)\nD_loss_curr = D_loss(M_mb, D_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(G_loss_curr, D_loss_curr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(X, M):\n    Z = tf.random.uniform([batch_size, dim])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        X = M * X + (1 - M) * Z\n        input_gen = tf.concat([X, M], axis=1)\n        G_sample = generator(input_gen, training=True)\n\n        # Combine with observed data\n        Hat_X = X * M + G_sample * (1 - M)\n        input_disc = tf.concat([Hat_X, M], axis=1)\n\n        D_prob = discriminator(input_disc, training=True)\n        \n        G_loss_curr = G_loss(X, M, G_sample, D_prob)\n        D_loss_curr = D_loss(M, D_prob)\n        print(\"Losses:\", G_loss_curr, D_loss_curr)\n        \n    gradients_gen = gen_tape.gradient(G_loss_curr, generator.trainable_variables)\n    gradients_disc = disc_tape.gradient(D_loss_curr, discriminator.trainable_variables)\n    \n    generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n    return G_loss_curr, D_loss_curr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(dataset, maskset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        print(\"Epoch:\", epoch)\n        \n        for X, M in zip(dataset, maskset):\n#             print(M)\n#             print(X * M)\n#             print(\"Shapes:\", X.shape, M.shape)\n#             G_loss_curr, D_loss_curr = train_step(X, M)\n#             print(\"Losses:\", G_loss_curr, D_loss_curr)\n            train_step(X, M)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_rate = 0.2\n\ndata_m = binary_sampler(1-miss_rate, no, dim)\nmiss_data_x = sales_complete.copy().to_numpy()\nmiss_data_x[data_m == 0] = 12.34 # np.nan will create error in X*M\ntrain_dataset = tf.data.Dataset.from_tensor_slices(miss_data_x).batch(batch_size, drop_remainder=True)\nmaskset = tf.data.Dataset.from_tensor_slices(data_m).batch(batch_size, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\n\ntrain(train_dataset, maskset, EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"complete_generation = generator(np.hstack([miss_data_x, data_m])).numpy()\n# Remove unnecessary predictions\n# complete_generation[data_m == 1] = 0\ncomplete_generation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_m.shape\ncomplete_generation.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}