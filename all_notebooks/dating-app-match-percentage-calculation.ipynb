{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\n\n#### A recently launched online dating site has assigned you the duty of playing Cupid and matching two lovebirds. As a Machine Learning expert, you are required to build a sophisticated model that predicts the match percentage between its users based on multiple attributes such as â€” their identifiers, preferences, interests, and the like.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing:","metadata":{}},{"cell_type":"markdown","source":"### Bios Analysis:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramAssocMeasures, BigramCollocationFinder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/hackerearth-love-in-the-time-of-screens/data.csv\")\ndata = data.drop(columns='username')\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef tokenize(text):\n    \"\"\"\n    First we will tokenize the bios,\n    then lemmatize them\n    \"\"\"\n    \n    # Create library of stopwords\n    stops = stopwords.words('english')\n    stops.extend([\"i'm\", 'i', \"i'd\", \"i've\", 'im', 'ive', 'like', 'also', 'would'\n                 \"i'll\", 'year', 'old', 'ago'])\n    # Lower case the text\n    text = text.lower()\n    \n    # Remove punctuations and useless characters\n    chars = ('.', ',', '!', \"_\", '1', '2',\n            '3','4','5','6','7','8','9','0')\n    for char in chars:\n        text = text.replace(char,\"\")\n    text = text.replace(\"-\", \" \")\n    text = text.replace(\"  \", \" \")\n    \n    #Split the text on spaces\n    text = text.split(\" \")\n    \n    # Lemmatizeing the words and removing stopwords\n    text = [lemmatizer.lemmatize(i) for i in text if i not in stops]\n    \n    return text\n\ndata['bio'] = data['bio'].apply(tokenize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['bio']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create set of all words\ntotal_vocab = set()\n\n# Add all words to the vocabulary\nfor bio in data['bio']:\n    total_vocab.update(bio)\n\n# Print total number of unique words\nprint(\"Number of unique words: \", len(total_vocab))\n\n# Add all the tokenized words to a list\nwords = []\nfor bio in data['bio']:\n    words.extend(bio)\n    \n# Determine frequency of each words\nword_freq = FreqDist(words)\nword_freq.most_common(100)\n\n# Plot most frequently used words\nplt.style.use('ggplot')\nplt.figure(figsize=[15,5])\n\nplt.bar(*zip(*word_freq.most_common(25)))\nplt.xticks(rotation=75)\nplt.title('Most Frequently Used Words in Bios')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiating the score of each bigram\nbigram_meas = BigramAssocMeasures()\n\n# Finding and ranking bigrams in each bio\nbio_finder = BigramCollocationFinder.from_words(words)\n\n# Find frequency scores of each bigram\nbio_score = bio_finder.score_ngrams(bigram_meas.raw_freq)\n\n# Create a list of bigrams\nbigram_list = list(map(lambda x: x[0][0]+' '+x[0][1], bio_score))\n\n# Create list of scores\nbigram_scores = list(map(lambda x: x[1],bio_score))\n\n# Combining score and bigrams\nbigrams = list(zip(bigram_list,bigram_scores))\n\n# Plot the bigram and frequency scores\nplt.style.use('bmh')\nplt.figure(figsize=(15,5))\n\nplt.bar(*zip(*bigrams[:25]))\nplt.xticks(rotation=75)\nplt.title('25 Most Common Bigrams')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Encoding:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nHere we process only those columns whose same value between \ntwo people will favour the match making. Like interest, hobbies, etc.\nSame values of Gender, orientation, etc are not favourable.\nSo we don't process them.\n\"\"\"\n\n# Consider only object type data\ndata_categorical = data.select_dtypes('object')\n\ndata_categorical = data_categorical.drop(columns=['user_id', 'sex', 'orientation', 'bio', 'location', 'location_preference'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create separate dataframe for encoded values\ndata_encoded = pd.DataFrame()\n\n\n# Create instance of labelEncoder class\nlabelencoder = LabelEncoder()\n\n# Encode by assigning numerical values to categories of all columns\nfor col in data_categorical:\n    data_encoded['{}'.format(col)] = labelencoder.fit_transform(data_categorical['{}'.format(col)])\n\ndata_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concatenation with remaining columns:","metadata":{}},{"cell_type":"code","source":"\"\"\"\nNow we concatenate the remaining numerical columns with the encoded dataframe\n\"\"\"\n\ndata_encoded = pd.concat([data['age'],data['height'],data['education_level'],data_encoded],\n                        axis=1)\ndata_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling of Encoded Columns:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the Scaler Class\n\nscaler = MinMaxScaler()\n\n# Scaling and replacing old values for all columns\n\ndata_encoded = pd.DataFrame(scaler.fit_transform(data_encoded),\n                            columns= data_encoded.columns,\n                            index= data_encoded.index)\n    \ndata_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding New Features:","metadata":{}},{"cell_type":"markdown","source":"### Bios Vectorization:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the vectorizer of choice\nvectorizer = CountVectorizer()\n\n# Bio column contains lists instead of strings. Convert them to string \ndef listtostring(lis):\n    str1 =\"\"\n    for word in lis:\n        str1 = str1 + \" \" + word\n    return str1[1:]\n\ndata['bio'] = data['bio'].apply(listtostring)\n    \n# Fit the vectorizer to bios\nbios_vect = vectorizer.fit_transform(data['bio'])\n\n# Create dataframe for vectorized bios\nbios_df = pd.DataFrame(bios_vect.toarray(), columns=vectorizer.get_feature_names())\n\n# Sort the columns in descending order of their sums.\nbios_df = bios_df.reindex(bios_df.sum().sort_values(ascending=False).index, axis=1)\nbios_df\n# Concatinate Encoded dataframe with vectorized bios\ndata_final = pd.concat([data_encoded,bios_df], axis=1)\n\ndata_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analysis:","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Instantiate PCA\npca = PCA()\n\n# Fit and transform the final dataframe\ndata_pca = pca.fit_transform(data_final)\n\n# Plot to determine how many features should the dataset be reduced to\n\"\"\"\nOur final dataframe has 2001 rows and 13523 columns. But the maximum number\nof principal components a dataframe can have is MIN(n_samples, n_features).\nSo we have considered only 2001 features of the 13523. But we have already\nsorted these features in descending order of their sums to get most weighted \nfeatures in the 2001 chosen ones.\n\"\"\"\nplt.style.use('bmh')\nplt.figure(figsize=(14,4))\nplt.plot(range(data_final.shape[0]), pca.explained_variance_ratio_.cumsum())\nplt.title(\"No. of Features accounting for % of Variance\")\n# We find exact number of features which account for at least 95% of variance\ntotal_variance = pca.explained_variance_ratio_.cumsum()\nn_for_95 = len(total_variance[total_variance>=.95])\nn_to_reach_95 = data_final.shape[0] - n_for_95\n\n# Print number of features required to retain 95% variance\nprint(\"Number of features: {}\\nTotal variance: {}\".format(n_to_reach_95,total_variance[n_to_reach_95]))\n\n# Reducing the dataset to number of features determined before\npca = PCA(n_components=n_to_reach_95)\n\n# Fit and transform the dataset to specified number of features and add to new dataframe\ndata_pca = pca.fit_transform(data_final)\n\n# Print the variance ratio after dataset is reduced\nprint(\"Achieved Variance ratio: {}\".format(pca.explained_variance_ratio_.cumsum()[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation:","metadata":{}},{"cell_type":"code","source":"# Generate correlation matrix for the data_final dataframe\ndata_corr = data_final.T.corr()\ndata_corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_corr.to_csv('corr.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the dataframe to eleminate any negative values generated by corr()\ndata_corr = pd.DataFrame(scaler.fit_transform(data_corr),\n                            columns= data_corr.columns,\n                            index= data_corr.index)\n    \ndata_corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Processing:","metadata":{}},{"cell_type":"code","source":"corr_arr = np.array(data_corr)\ncorr_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(corr_arr)):\n    for j in range(len(corr_arr)):\n        \n        # A person can't match with himself. So we asign zero to rows and columns having same user_id\n        if i==j:\n            corr_arr[i][j] = 0\n            \n        # When a person is straight\n        if data.iloc[i]['orientation']=='straight' and data.iloc[j]['orientation']=='straight':\n            # Straight person should not be matched with person of same sex. So assign zero to such instances\n            if data.iloc[i]['sex'] == data.iloc[j]['sex']:\n                corr_arr[i][j] = 0\n                corr_arr[j][i] = 0\n        \n        # When a person is gay\n        if data.iloc[i]['orientation']=='gay' and data.iloc[j]['orientation']=='gay':\n            # Gay person should not be matched with person of opposite sex. So assign zero to such instances\n            if data.iloc[i]['sex'] != data.iloc[j]['sex']:\n                corr_arr[i][j] = 0\n                corr_arr[j][i] = 0\n                \n        # When a person is bisexual\n        if data.iloc[i]['orientation']=='bisexual' and data.iloc[j]['orientation']=='bisexual':\n            # Bisexual person can match with any gender. So we don't do anything\n            pass\n        \n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_mat = pd.DataFrame(corr_arr, columns=data['user_id'].tolist(), index=data['user_id'].tolist())\nfinal_mat = final_mat.mul(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_mat.index.name = 'user_id'\nfinal_mat.columns.name = 'user_id'\nfinal_mat.to_csv('Final.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_mat","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}