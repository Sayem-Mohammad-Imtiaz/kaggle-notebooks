{"cells":[{"metadata":{},"cell_type":"markdown","source":"![center](https://wallpapercave.com/wp/wp5836619.jpg)\n\n# Hello and Welcome\n\nI want to introduce you to a simple comparative model analysis for lego classification. I never work with such a small data-set. It's important to understand the Convolution Neural Network nature. I know that every person that has been started to learn the NN concept has been faced with to sentence \" Neural Networks need tons of data\". Comtratyly its value to understanding why. I think a nice way to understand is comparing models with the same small dataset. In this work, you can see the result of 5 different network architecture on the problem space. Actually there 2 neural networks with either image-net initialized weights or not. Lastly, 1 custom and simple architecture (I called it JARJAR-Net ðŸ˜œ) yet includes some important concepts like Batchnormlization, Dropout, and Maxpooling. I want to discuss why networks need more data and which network has been best fitted for the problem space and of course why? I especially thank you, that who joins this conversation. I hope you will enjoy it."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\nfrom tensorflow.keras.applications import EfficientNetB0, DenseNet121\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Conv2D, MaxPool2D, BatchNormalization, Flatten, Dropout, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nIMG_SHAPE = (256,256,3)\nPATH = '/kaggle/input/lego-minifigures-classification/'\nNB_CLASS = 29","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(PATH + 'index.csv')\ndf = df.drop(['Unnamed: 0'], axis = 1)\n\ntrain_df = df[df['train-valid'] == 'train']\nvalidation_df = df[df['train-valid'] == 'valid']\n\ntrain_df['class_id'] = train_df['class_id'].astype(str)\nvalidation_df['class_id'] = validation_df['class_id'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = DenseNet121(\n                            input_shape=IMG_SHAPE, \n                            include_top=False, \n                            weights='imagenet'\n    \n                            )\n\nbase_model.trainable = True\n\nx = base_model.output\n\nglobal_average = GlobalAveragePooling2D()(x)\noutput_layer = Dense(NB_CLASS, activation = 'softmax')(global_average)\n\ndenseNet_withW= Model(inputs=base_model.input, outputs=output_layer)\n\n#denseNet_withW.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = DenseNet121(\n                            input_shape=IMG_SHAPE, \n                            include_top=False, \n                            weights= None\n    \n                            )\n\nbase_model.trainable = True\n\nx = base_model.output\n\nglobal_average = GlobalAveragePooling2D()(x)\noutput_layer = Dense(NB_CLASS, activation = 'softmax')(global_average)\n\ndenseNet_withoW = Model(inputs=base_model.input, outputs=output_layer)\n\n#denseNet_withoW.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I didn't freeze the layer for transfer learning. I just used imageNet weights for initialization.\n### Additionally I selected the B0 model for simplicity."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = EfficientNetB0(\n                            input_shape=IMG_SHAPE, \n                            include_top=False, \n                            weights= 'imagenet'\n    \n                            )\n\nbase_model.trainable = True\n\nx = base_model.output\n\nglobal_average = GlobalAveragePooling2D()(x)\noutput_layer = Dense(NB_CLASS, activation = 'softmax')(global_average)\n\neNet_withW = Model(inputs=base_model.input, outputs=output_layer)\n\n#eNet_withW.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = EfficientNetB0(\n                            input_shape=IMG_SHAPE, \n                            include_top=False, \n                            weights= None\n    \n                            )\n\nbase_model.trainable = True\n\nx = base_model.output\n\nglobal_average = GlobalAveragePooling2D()(x)\noutput_layer = Dense(NB_CLASS, activation = 'softmax')(global_average)\n\neNet_withoW = Model(inputs=base_model.input, outputs=output_layer)\n\n#eNet_withoW.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This is simple CNN architecture with ;\n    * 2-D max pooling for reducing parameter size and help to overcome overfitting problem\n    * Batch Normalization to avoid overfitting problem and gradient exploding and vanishing. (It's important for deep neural nets)\n    * Relu activation has been selected."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape = IMG_SHAPE)\n\n\nx = Conv2D(256, (3,3), activation = 'relu')(input_layer) \nx = MaxPool2D((2,2))(x)\n\n\nx = Conv2D(256, (3,3), activation = 'relu')(x) \nx = Conv2D(128, (3,3), activation = 'relu')(x)\nx = MaxPool2D((3,3))(x)\n\n\nx = Conv2D(128, (3,3), activation = 'relu')(x)\nx = Conv2D(128, (3,3), activation = 'relu')(x)\nx = Conv2D(64, (3,3), activation = 'relu')(x) \nx = MaxPool2D((2,2))(x)\n\n\nx = Conv2D(256, (3,3), activation = 'relu')(x) \nx = Dropout(0.5)(x)\nx = Conv2D(256, (3,3), activation = 'relu')(x)\nx = Conv2D(32, (3,3), activation = 'relu')(x) \nx = MaxPool2D((2,2))(x)\nx = BatchNormalization()(x)\n\nx = Flatten()(x) \nx = Dense(1024, activation = 'relu')(x) \n\noutput_layer = Dense(NB_CLASS, activation = 'softmax')(x)\n\njarjar_model = Model(inputs = input_layer, outputs = output_layer) \n\n#jarjar_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = ImageDataGenerator(\n                                     rescale=1./255,\n                                     rotation_range=45,\n                                     width_shift_range=0.25,\n                                     height_shift_range=0.25,\n                                     shear_range=0.25,\n                                     zoom_range=0.25,\n                                     horizontal_flip=True,\n                                     brightness_range=[0.5, 1.0], \n                                     \n\n                                    )\n\nvalidation_generator = ImageDataGenerator(rescale=1./255)\n\n\n\ntrain_set = train_generator.flow_from_dataframe(train_df, \n                                                PATH,\n                                                x_col = 'path',\n                                                y_col = 'class_id',\n                                                batch_size = BATCH_SIZE,\n                                                class_mode = 'categorical',\n                                                target_size = IMG_SHAPE[:2],\n                                                suffle=True\n                                               )\n\nvalidation_set = validation_generator.flow_from_dataframe(validation_df, \n                                                          PATH,\n                                                          x_col = 'path',\n                                                          y_col = 'class_id',\n                                                          batch_size = 1,\n                                                          class_mode = 'categorical',\n                                                          target_size = IMG_SHAPE[:2],\n                                                          shuffle = False\n                                               )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"denseNet_withW.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics='accuracy')\nhistory_denseNet_withW = denseNet_withW.fit_generator(train_set, \n                              steps_per_epoch=train_set.n//train_set.batch_size, \n                              validation_data = validation_set, \n                              epochs = 50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"denseNet_withoW.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics='accuracy')\nhistory_denseNet_withoW = denseNet_withoW.fit_generator(train_set, \n                              steps_per_epoch=train_set.n//train_set.batch_size, \n                              validation_data = validation_set, \n                              epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eNet_withoW.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics='accuracy')\nhistory_eNet_withoW = eNet_withoW.fit_generator(train_set, \n                              steps_per_epoch=train_set.n//train_set.batch_size, \n                              validation_data = validation_set, \n                              epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eNet_withW.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics='accuracy')\nhistory_eNet_withW = eNet_withW.fit_generator(train_set, \n                              steps_per_epoch=train_set.n//train_set.batch_size, \n                              validation_data = validation_set, \n                              epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jarjar_model.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics='accuracy')\nhistory_jarjar = jarjar_model.fit_generator(train_set, \n                              steps_per_epoch=train_set.n//train_set.batch_size, \n                              validation_data = validation_set, \n                              epochs = 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history_jarjar.history['accuracy'])\nplt.plot(history_jarjar.history['val_accuracy'])\nplt.title('JARJAR-NET ACCURACY PLOT')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.plot(history_denseNet_withW.history['accuracy'])\nplt.plot(history_denseNet_withW.history['val_accuracy'])\nplt.title('DENSE-NET WITH IMAGENET ACCURACY PLOT')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.plot(history_denseNet_withoW.history['accuracy'])\nplt.plot(history_denseNet_withoW.history['val_accuracy'])\nplt.title('DENSE-NET WITHOUT ACCURACY PLOT')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.plot(history_eNet_withW.history['accuracy'])\nplt.plot(history_eNet_withW.history['val_accuracy'])\nplt.title('EFFICIENT-NET WITH IMAGENET ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.plot(history_eNet_withoW.history['accuracy'])\nplt.plot(history_eNet_withoW.history['val_accuracy'])\nplt.title('EFFICIENT-NET WITHOUT IMAGENET ACCURACY PLOT')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nIt is interesting to see the result of the models. Maybe it will be emotional but as though every model has its own language and characteristics. When we look at the E-Net model, the model quickly fits the training data. It's good if your training and validation set is big and both of them come from similar distribution. Also, we can say that this is complex to understand tiny patterns in the data. We can say the same thing for denseNet. But for Jarjar Net its clear to see JarJar himself as a model. It learns slowly and has oscillation in the learning phase. But in the end, it gives the best result for the validation set. Also, graphs are proving that"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Future Work\n\nIt's interesting to see Siamese Network for these problems. A similarity-based network can perform better than direct classifier models."},{"metadata":{},"cell_type":"markdown","source":"![](https://64.media.tumblr.com/4a9f1e246f624e0d09c572516f6dab46/22b6215d00ec5863-df/s1280x1920/c4cbc1cee5d4b25c843c0e825e1233961fdebbac.gifv)"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}