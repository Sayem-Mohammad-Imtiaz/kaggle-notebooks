{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nprint('Process start time :', datetime.now())\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/craigslist-carstrucks-data/vehicles.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show all columns\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing region since we already have 'county'\ndf=df.drop(['region', 'region_url', 'vin','url','image_url','description','county'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert characters to numbers using label Encoding\ndf[['size','manufacturer', 'model', 'condition','cylinders', 'fuel', 'title_status', 'transmission','drive', 'type', 'paint_color', 'state']] = df[['size','manufacturer', 'model', 'condition','cylinders', 'fuel', 'title_status', 'transmission','drive','type', 'paint_color', 'state']].apply(le.fit_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling continous values - Scaling down ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"odometer\"] = np.sqrt(preprocessing.minmax_scale(df[\"odometer\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperate Features and Outcome\nX = df.drop('price',axis=1).values\ny = df.price.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=2)\nskf.get_n_splits(X, y)\n\nfor train_index, test_index in skf.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n# works for classification\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe to store accuracy scores of different algorithms\naccuracy_df = pd.DataFrame(columns=('r2', 'rmse'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport math\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Fit\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Decision Tree Baseline']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree with auto Hyper Parameter Tuning with Grid Search","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nscoring = metrics.make_scorer(metrics.mean_squared_error)\n\nparam_grid = {\n'criterion':['mse'] \n,'splitter':['best','random']\n,'max_depth':[4, 5, 6, 7, 8]\n,'min_samples_split':[0.8, 2]\n,'max_features':['auto','sqrt','log2']\n}\n\ng_cv = GridSearchCV(DecisionTreeRegressor(random_state=0),param_grid=param_grid,scoring=scoring, cv=5, refit=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"g_cv.fit(X_train, y_train)\ng_cv.best_params_\nresult = g_cv.cv_results_\n# print(result)\n\n# Predict\ny_pred = g_cv.best_estimator_.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Decision Tree HyperParam Tuning']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Fit\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Random Forest Baseline']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest with auto Hyper Parameter Tuning with Grid Search","execution_count":null},{"metadata":{},"cell_type":"raw","source":"from sklearn.model_selection import GridSearchCV\n\nscoring = metrics.make_scorer(metrics.mean_squared_error)\n\nparam_grid = {\n# 'n_estimators':[50,70,100,120,130] \n# 'max_features':['auto','sqrt','log2']\n#,'oob_score':[False, True] # whether to use out-of-bag samples to estimate the R^2 on unseen data.\n# ,'bootstrap':[False, True]\n# ,'random_state':[10, None]\n# ,'warm_start':[True, False]\n'max_depth':[4, 5, 6, 7, 8]\n# ,'min_samples_split':[0.8, 2, 3]\n}\n\ng_cv = GridSearchCV(RandomForestRegressor(random_state=0),param_grid=param_grid,scoring=scoring, cv=5, refit=True)","execution_count":null},{"metadata":{"scrolled":true},"cell_type":"raw","source":"g_cv.fit(X_train, y_train)\ng_cv.best_params_\nresult = g_cv.cv_results_\n# print(result)\n\n# Predict\ny_pred = g_cv.best_estimator_.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Random Forest HyperParam Tuning']))","execution_count":null},{"metadata":{},"cell_type":"raw","source":"accuracy_df","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\n# Fit\nmodel = GradientBoostingRegressor(random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Gradient Boosting Baseline']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{},"cell_type":"raw","source":"### factorize the categorical columns\ncats = ['manufacturer','model','condition','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','county','state']\nfor cat in cats:\n#     sorting_list=np.unique(sorted(df[cat],key=lambda x:(str.lower(x),x)))\n#     df[cat]=pd.Categorical(df[cat], sorting_list)\n    df=df.sort_values(cat)\n    df[cat]=pd.factorize(df[cat], sort=True)[0]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Fit\nmodel = XGBRegressor(random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['XGBoost Baseline']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost with Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nmodel = xgb.XGBRegressor(\n#     gamma=1,                 \n    learning_rate=0.05,\n#     max_depth=3,\n#     n_estimators=10000,                                                                    \n#     subsample=0.8,\n    random_state=34,\n    booster='gbtree',    \n    objective='reg:squarederror',\n    eval_metric='rmse'\n) \nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['XGBoost with Parameters']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MLP Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\n\nmlp = MLPRegressor()\nparam_grid = {\n#               'hidden_layer_sizes': [i for i in range(2,20)],\n              'activation': ['relu'],\n              'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [0.01],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n              'early_stopping': [True],\n              'warm_start': [False]\n}\nmodel = GridSearchCV(mlp, param_grid=param_grid, \n                   cv=10, verbose=True, pre_dispatch='2*n_jobs')\n\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['MLPRegressor with Parameter Tuning']))\naccuracy_df.sort_values('rmse')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the training data in to training and validation datasets for Model training\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n#         'max_depth': -1,\n#         'subsample': 0.8,\n#         'bagging_fraction' : 1,\n#         'max_bin' : 5000 ,\n#         'bagging_freq': 20,\n#         'colsample_bytree': 0.6,\n        'metric': 'rmse',\n#         'min_split_gain': 0.5,\n#         'min_child_weight': 1,\n#         'min_child_samples': 10,\n#         'scale_pos_weight':1,\n#         'zero_as_missing': False,\n#         'seed':0,        \n    }\nmodel = lgb.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=8000,\n                  verbose_eval=500, valid_sets=valid_set)\n\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['LightGBM with Parameters']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM with categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the training data in to training and validation datasets for Model training\n\nimport lightgbm as lgb1\nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n#Define categorical features, training and validation data\ncategorical_positions = []\ncat = ['manufacturer','model','condition','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','county','state']\nfor c, col in enumerate(df.columns):\n    for x in cat:\n        if col == x:\n            categorical_positions.append(c-1)\n\n\ntrain_set = lgb1.Dataset(Xtrain, label=Ztrain, categorical_feature=categorical_positions)\nvalid_set = lgb1.Dataset(Xval, label=Zval)\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n#         'max_depth': -1,\n#         'subsample': 0.8,\n#         'bagging_fraction' : 1,\n#         'max_bin' : 5000 ,\n#         'bagging_freq': 20,\n#         'colsample_bytree': 0.6,\n        'metric': 'rmse',\n#         'min_split_gain': 0.5,\n#         'min_child_weight': 1,\n#         'min_child_samples': 10,\n#         'scale_pos_weight':1,\n#         'zero_as_missing': False,\n#         'seed':0,        \n    }\nmodel = lgb1.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=8000,\n                  verbose_eval=500, valid_sets=valid_set)\n\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['LightGBM with Categories & Parameters']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no change in accuracy even after mentioning the categorical columns explicitly. This is because most of the columns in the dataset is categorical. LightGBM algorithm automatically selects the categorical columns if its not given.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## CatBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor, Pool\n    \nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain)\nvalid_set = lgb.Dataset(Xval, Zval)\n\nmodel = CatBoostRegressor()\n\nmodel.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], verbose=100, early_stopping_rounds=1000)\n\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['CatBoost Baseline']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CatBoost with Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor, Pool\n    \nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)\n\n\n        \nmodel = CatBoostRegressor(\n                          iterations=1000, \n                          depth=8, \n                          learning_rate=0.01, \n                          loss_function='RMSE', \n                          eval_metric='RMSE', \n                          use_best_model=True)\n\nmodel.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], verbose=100, early_stopping_rounds=1000)\n\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\n# accuracy_df = accuracy_df.drop('CatBoost Parameter Tuning')\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['CatBoost Parameter Tuning']))\naccuracy_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracies of Models sorted by RMSE Scores","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Plotting the RMSE Scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot\nplt.figure(figsize=[25,6])\nplt.tick_params(labelsize=14)\nplt.plot(accuracy_df.index, accuracy_df['rmse'], label = 'RMSE Scores')\nplt.legend()\nplt.title('RMSE Score comparison for 10 popular models for test dataset')\nplt.xlabel('Models')\nplt.ylabel('RMSE Scores')\nplt.xticks(accuracy_df.index, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Process start time :', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rerunning MLP Neural Network to save the model\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\nmlp = MLPRegressor()\nparam_grid = {\n#               'hidden_layer_sizes': [i for i in range(2,20)],\n              'activation': ['relu'],\n              'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [0.01],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n              'early_stopping': [True],\n              'warm_start': [False]\n}\nmodel = GridSearchCV(mlp, param_grid=param_grid, \n                   cv=10, pre_dispatch='2*n_jobs')\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the neural network model\nfrom joblib import dump, load\n\nfilename = 'mlp_neural_network_001.joblib'\nwith open(filename, 'wb') as file:  \n    dump(model, file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ny_pred = model.predict(X_test)\ndf1 = pd.DataFrame({\"y\":y_test,\"y_pred\":y_pred })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}