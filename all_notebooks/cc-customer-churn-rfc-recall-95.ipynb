{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Card Churn"},{"metadata":{},"cell_type":"markdown","source":"By Eric Wilson"},{"metadata":{},"cell_type":"markdown","source":"The individual on Kaggle who submitted this data set said they need to predict customer churn, and have managed to get 62% as the highest accuracy. It's ok to predict someone who will stay as one who will churn, but the most important task is making sure everyone who will churn is not marked as someone who will stay. Let's see what we can do..."},{"metadata":{},"cell_type":"markdown","source":"### Import libraries and data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import confusion_matrix,classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we need to turn attrition/churn into numeric values, followed by gender, education, income, marital status, and card catagory. We also need to get rid of the last two columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Attrition_Flag'].value_counts())\nprint(df['Gender'].value_counts())\nprint(df['Education_Level'].value_counts())\nprint(df['Marital_Status'].value_counts())\nprint(df['Income_Category'].value_counts())\nprint(df['Card_Category'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Attrition_Flag'].replace({'Existing Customer' : 0, 'Attrited Customer' : 1},inplace = True)\ndf['Gender'].replace({'F': 0, 'M': 1}, inplace = True)\ndf['Education_Level'].replace({'Unknown' : 0, 'Uneducated' : 1, 'High School' : 2, 'College' : 3, \n                               'Graduate' : 4, 'Post-Graduate' : 5, 'Doctorate' : 6}, inplace = True)\ndf['Marital_Status'].replace({'Unknown' : 0, 'Single' : 1, 'Divorced' : 2, 'Married' : 3}, inplace = True)\ndf['Income_Category'].replace({'Unknown' : 0, 'Less than $40K' : 1, '$40K - $60K' : 2, '$60K - $80K' : 3,\n                              '$80K - $120K' : 4, '$120K +' : 5}, inplace = True)\ndf['Card_Category'].replace({'Blue' : 0, 'Silver' : 1, 'Gold' : 2, 'Platinum' : 3}, inplace = True)\ndf.drop(df.columns[[0,21,22]].values,axis=1,inplace = True)\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have nothing but numbers. Let's start trying to build a model."},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{},"cell_type":"markdown","source":"Let's start by seeing what features correlate most with Attrition, and which correlate with one another, in order to have an idea of what features may be more useful than others in an attempt to avoid data overload and overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that the most correlated fields to churn are transaction counts, count change from Q4 to Q1, revolving balance, 12 month contact count, inactive months, utilization ratio,relationship count, and transaction amount. That being said, none of them share a particularly strong correlation, but essentially all of the demographic information (gender, income, education level, marital status, dependant count) lack any real correlation with churn."},{"metadata":{},"cell_type":"markdown","source":"### Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfm = df[['Attrition_Flag', 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n          'Contacts_Count_12_mon', 'Total_Revolving_Bal', 'Total_Trans_Amt', 'Total_Trans_Ct',\n          'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']]\ndfm.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = dfm[['Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon',\n        'Total_Revolving_Bal', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1',\n        'Avg_Utilization_Ratio']]\ny = dfm['Attrition_Flag']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100, max_depth=13, random_state=2)\nmodel.fit(x, y)\nrfvalue = model.predict(x_test)\n\nprint('Model Accuracy : ', accuracy_score(y_test, rfvalue) *  100)\nprint('Model Recall : ', recall_score(y_test, rfvalue) *  100)\nprint('Model Precision : ', precision_score(y_test, rfvalue) *  100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, rfvalue))\nprint(classification_report(y_test, rfvalue))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this model, we still have roughly two dozen false negatives - what we've been asked to avoid. That being said, we're still looking at pretty high accuracy, precision, recall, and f1 scores."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"By narrowing down the data used to the factors which have the highest correlation to attrition, we're left with a pretty accurate model. I've tried to optimize it with larger and smaller train / test splits and random states, but the combination used in this notebook seemed to be pretty optimal."},{"metadata":{},"cell_type":"markdown","source":"### Addendum"},{"metadata":{},"cell_type":"markdown","source":"I value feedback, tips, and criticism highly - I'm still fairly new to DS and ML, so if I make a mistake or error, I would greatly appreciate knowing so; the best way to learn is by doing, and it's better to fix an error before it becomes a habit.\n\nThank you for taking the time to read this notebook!"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}