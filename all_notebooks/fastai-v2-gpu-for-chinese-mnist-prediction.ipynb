{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\">fastai-v2/GPU for Chinese MNIST Prediction</font></center></h1>\n\n# <a id='0'>Table of Contents</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Preparing our Data</a>   \n- <a href='#3'>Preparing our DataBlock</a>   \n- <a href='#4'>Preparing our Learner</a> \n- <a href='#5'>Baseline Model</a> \n- <a href='#6'>Improving our Model</a> \n- <a href='#7'>Conclusions</a> "},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>Introduction</a> "},{"metadata":{},"cell_type":"markdown","source":"The Chinese MNIST dataset provides us with 15,000 images of Chinese numbers handwritten by 100 volunteers. Each participant provided 10 samples of the 15 Chinese characters for numbers. \n\nThe objective of this notebook is to demonstrate how to solve the Chinese MNIST classification task with 0.999 accuracy using: \n1. `fastai` version 2\n2. GPU acceleration\n3. Multilabel classification"},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>Preparing our Data</a>   "},{"metadata":{},"cell_type":"markdown","source":"### Install Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import fastai\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\nimport pandas as pd\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Files"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#imports files from kaggle\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After importing your files, you can create a `path` to the folder that contains your images. The `path` object contains a `.ls()` method that behaves similar to a Python `list` but has additional functionality. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#creates a path to the folder containing image files\npath = Path(\"../input/chinese-mnist/data/data\")\n\n#makes .ls() format easier to read  \nPath.BASE_PATH = path\n\n#checks image files using .ls() method\npath.ls()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a DataFrame"},{"metadata":{},"cell_type":"markdown","source":"The Chinese MNIST dataset also contains a CSV file that we can use to label our variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/chinese-mnist/chinese_mnist.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We currently don't have a column that we can use to reference our x variables/images. \n\nNotice that our image files have a similar structure. For example, `\"input_47_6_7.jpg\"` and `\"input_12_8_2.jpg\"` share various components. All of our image files begin and end similarly, and they all contain, in the same order, the number of the participant, the number of the sample, and the code of the Chinese character.\n\nWe can create a new column in Pandas and use our existing columns to concatenate our file names."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['fname'] = (\"input_\" + df['suite_id'].astype(str) \n               + \"_\" \n               + df['sample_id'].astype(str) \n               + \"_\" \n               + df['code'].astype(str) \n               + \".jpg\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>Preparing our DataBlock</a>  "},{"metadata":{},"cell_type":"markdown","source":"### Define Variables"},{"metadata":{},"cell_type":"markdown","source":"Now that we have our DataFrame, we can define our variables. We can use our new `fname` column for our x variables, but we will need to attach a path to each variable. We can also use our `value` column to label our y variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_x(r): return path/r['fname']\n\n#.astype() and .split() method were added to contain each label\ndef get_y(r): return r['value'].astype(str).split(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From DataFrames to DataLoaders"},{"metadata":{},"cell_type":"markdown","source":"Before we jump into creating a `DataLoaders` object, lets review some terminology. Note that the last two classes are specific to fastai and build ontop of PyTorch's `Dataset` and `DataLoader` classes.\n\n* `Dataset`: A collection that returns a tuple of your independent and dependent variable for a single item.\n* `DataLoader`: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables.\n* `Datasets`: An object that contains a training `Dataset` and a validation `Dataset`.\n* `DataLoaders`: An object that contains a training `DataLoader` and a validation `DataLoader`.\n<p><font size=\"1\">*From \"Deep Learning for Coders with Fastai and PyTorch\" - credit to fastai/Jeremy Howard/Sylvain Gugger</font></p>\n\nWe will need to compile a `DataBlock` to create our `DataLoaders` object:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creates a Datablock object\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                  splitter=RandomSplitter(seed=42),\n                  get_x=get_x,\n                  get_y=get_y,\n                  item_tfms = RandomResizedCrop(128, min_scale=0.35))\n\n# passes our dataframe into the dataloaders method of our DataBlock object\ndls = dblock.dataloaders(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets break this down: \n* `blocks` let us to pass `ImageBlock` and `MultiCategoryBlock.` Even though we converted our x variables into image paths, we still need a method to open our images and to transform them into tensors. `ImageBlock` does this. \n* `MultiCategoryBlock` allows us to have multiple labels for each item. More on one-hot-encoding later.\n* `splitter` splits our `DataFrame` into a training and validation set. Default split is 80/20.\n* `get_x` calls our `get_x` function to retreive our image paths. \n* `get_y` calls our `get_y` function to retreive our image labels.\n* `item_tfms` makes sure all of our images are the same scale and GPU compatible.\n* `dblock.dataloaders(df)` creates a `DataLoaders` object and passes in our `DataFrame`.\n\nLet's analyze our new `DataLoaders` object: "},{"metadata":{"trusted":true},"cell_type":"code","source":"#displays number of batches for our training and validation sets\nlen(dls.train), len(dls.valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#displays a batch with images and labels\ndls.show_batch(nrows=1, ncols=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#displays training Dataset\ndls.train_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#displays validation Dataset\ndls.valid_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that our `DataLoaders` object has split our `DataFrame` into a training `Dataset` of 12,000 and a validation `Dataset` of 3,000. Furthermore, our `DataLoaders` object transforms our `Dataset` objects into batches. \n\nAlso notice the structure of our `Dataset` objects:\n* Our x variables are images with a size of 64x64 pixels. \n* The lists of 0s and 1s contains our category labels and refers to ***one-hot-encoding***. Each category is considered independently and a 1 is granted if the category is present. Therefore, we can expect to see 15 digits for our 15 possible categories. \n* Although we don't expect to find multiple labels in our data, our multilabel classification approach allows our model to choose no label in the abscence of a prediction above our treshold. This is in contrast to a multicategory classification model with a softmax loss function which always predicts a category label even when there are no valid matches."},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>Preparing our Learner</a>   "},{"metadata":{},"cell_type":"markdown","source":"### Batch Testing the Model"},{"metadata":{},"cell_type":"markdown","source":"Before we test our `DalaLearner` object, lets generate predictions from a single batch."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#uses fastai's resnet18 model\nlearn = cnn_learner(dls, resnet18)\n\n#creates a batch from our train dataset\nx,y = to_cpu(dls.train.one_batch())\n\n#generates predictions from our batch\nbatch = learn.model(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzes batch\nbatch.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our batch size is 64 images, which is the default for fastai, and each image is generating predictions for 15 seperate categories. Lets analyze a single image:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can index into our batch to return predictions for a single image\nbatch[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we expected, our image is receiving predictions for each of our 15 categories. We will compare our predictions with our targets/labels to calculate a loss."},{"metadata":{},"cell_type":"markdown","source":"### Loss Function"},{"metadata":{},"cell_type":"markdown","source":"By default, fastai will apply a Binary Cross-Entropy loss function to multilabel classification problems. We can call `loss_func` on our learner object to see our loss function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.loss_func","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Because we have a one-hot-encoded dependent variable, we cannot use a cross entropy loss function. The softmax function that's used to transform predictions into comparative activations makes it impossible to do multilabel classification. Softmax tends to push one activation over the others and cannot identify multiple labels in one image.\n* Instead, a Binary Cross-Entropy loss function uses a sigmoid function to transform our predictions into activations between 0 and 1. Each prediction is then compared with our targets using a similar function to `mnist_loss`.\n* `BCEWithLogitsLoss` refers to both sigmoid and binary cross-entropy loss in a single function"},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining our own sigmoid function\ndef sigmoid(x): return 1/(1+torch.exp(-x))\n\n#defining our own BCELoss function  \ndef binary_cross_entropy(inputs, targets):\n    inputs = inputs.sigmoid()\n    return -torch.where(targets==1, inputs, 1-inputs).log().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we understand `BCEWithLogitsLoss`, lets compare our batch predictions with our targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"#creates a loss function\nloss_func = nn.BCEWithLogitsLoss()\n\n#passes our predictions and our labels into our loss function\nloss = loss_func(batch, y)\n\n#prints out or loss\nloss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice the `grad_fn` attribute. This tells us fastai is automatically keeping track of our gradients for us! Our gradients will be calculated from our loss and they will be used to update our parameters."},{"metadata":{},"cell_type":"markdown","source":"### Metrics for Accuracy"},{"metadata":{},"cell_type":"markdown","source":"We will also need to make sure our metric is compatible with our multilabel classification task. Because we could have more than one prediction on a single image, we need to pick a treshold to evaluate the accuracy of each prediction. The default treshold in fastai is 0.5, but Jeremy Howard suggests using 0.2."},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>Baseline Model</a>"},{"metadata":{},"cell_type":"markdown","source":"### Results"},{"metadata":{},"cell_type":"markdown","source":"Now that we have our `DataLoaders` object, lets create a `learner`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our baseline model was able to achieve 0.999 accuracy on our first attempt. Lets break down the `learner` object to see how we got our results:  \n* `cnn_learner` is a fastai class that allows us to build our model with a pretrained convolutional neural network. \n* `dls` is our `DataLoaders` object that contains our images in batches of training and validation sets.\n* `resnet18` tells fastai we want to use a pretrained `cnn` with 18 layers.\n* `metrics` calls our `accuracy_multi()` function which is needed for multilabel classification.\n* `fine_tune` is a fastai method that allows us to train our model and pass in the total number of epochs\n* `base_lr` is our learning rate which will be multiplied by our gradients to inform new activations. The default learning rate in fastai is 1e-3 and does not need to be specified inside of `fine_tune`. "},{"metadata":{},"cell_type":"markdown","source":"### Model Analysis"},{"metadata":{},"cell_type":"markdown","source":"Now that we have our results, we can plot our top losses with fastai's `ClassificationInterpretation` class. Notice that our `probabilities` category is a tensor of 15 predictions for each image. "},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(5, nrows=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of our 3000 images in our validation set, our model only mislabeled a few. Because we are using one-hot-encoding our results are saying actual 1s were predicted as 0s x times, and actual 0s were predicted as 1s y times. In any case, we can use the sum of x and y to determine the total number of mislabelled images."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='6'>Improving our Model</a>"},{"metadata":{},"cell_type":"markdown","source":"### Learning Rate Finder"},{"metadata":{},"cell_type":"markdown","source":"We can improve our model by finding our ideal learning rate. Fastai lets us do this with the `.lr_find()` method on our `learner` object:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there's not much activity between 1e-7 and 1e-3 (fastai's default learning rate), so lets test a learning rate of 1e-2:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(6, base_lr=1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our results improved! But there's still more we can do. "},{"metadata":{},"cell_type":"markdown","source":"### Unfreezing and Transfer Learning"},{"metadata":{},"cell_type":"markdown","source":"Because we are using transfer learning, we are replacing the final linear layer of our `cnn` with a new layer of random weights. We want to train a model in such a way that it is able to remember the useful ideas from the pretrained model so that it can adjust these weights as required for our specific task. We can do this by freezing pretrained layers and only updating the weights for our new linear layer. \n\n`fastai` lets us do this with the `fit_one_cycle` method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fit_one_cycle(3, base_lr=1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've gone through 3 epochs, we can unfreeze our pretrained layers using the fastai `.unfreeze()` method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After unfreezing our layers, we call the `.lr_find` method again since adding new layers results in a new learning rate:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have a steep decending slope like our previous plot because the model has already been trained. The goal is to pick a point before the sharp increase, not the maximum gradient. Given the flattened slope, we will train our model with our transfered weights for 6 more epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(6, 1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our accuracy has increased once more!"},{"metadata":{},"cell_type":"markdown","source":"# <a id='7'>Conclusions</a>"},{"metadata":{},"cell_type":"markdown","source":"We were able to create a baseline model with .999+ accuracy using fastai's library, gpu acceleration, and multilabel classification. We were further able to improve our model by freezing our pretrained weights and by finding the ideal learning rates for multiple steps in our model. There is still some fine-tuning we can do, but this should be enough to get others started with fastai and multilabel classification!\n\nThank you to Jeremy Howard."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}