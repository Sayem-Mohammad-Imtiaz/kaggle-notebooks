{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Prediction of data analyst job application probability"},{"metadata":{},"cell_type":"markdown","source":"Based on the size,sector,type of ownership and revenue, the application probability is predicted."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib \nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/data-analyst-jobs/DataAnalyst.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Wrangling"},{"metadata":{},"cell_type":"markdown","source":"There are NaN records in most of the columns. But they are provided as '-1'. These records are updated as Nan to ease the cleaning process."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Easy Apply'].replace(to_replace='-1',value='False',inplace=True)\ndf.replace(to_replace='-1',value=np.nan,inplace=True)\ndf['Revenue'].replace(to_replace='Unknown / Non-Applicable',value=np.nan,inplace=True)\ndf['Founded'].replace(to_replace=-1,value=np.nan,inplace=True)\ndf = df.dropna()\ndf = df.drop(columns='Unnamed: 0',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.reset_index()\ndf = df.drop(columns='index',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Few of the columns like Size, type of ownership,Revenue have categorised values. Hence, Label encoding has been incorporated to turn them into independent features that can be used for the classification model,"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncol = df[['Size','Type of ownership','Sector','Revenue','Easy Apply']]\ndf1 = col.apply(lambda x: le.fit_transform(x))\ndf1['Rating'] = df['Rating']\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon conversion, there is an imbalance in the samples categorised under 'Easy Apply' column as shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x=df1['Easy Apply'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Over sampling the Minority class via SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"import imblearn\nfrom imblearn.over_sampling import SMOTE\n\nx = df1[['Size','Type of ownership','Sector','Revenue','Rating']]\ny = df1['Easy Apply']\nsamples = SMOTE()\nX,Y = samples.fit_resample(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame(X)\ndf2['Easy Apply'] = Y\ndf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Samples are now balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=df2['Easy Apply'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Development"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = df2[['Size','Type of ownership','Sector','Revenue','Rating']]\ny = df2['Easy Apply']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross validation - accuracy and Learning curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve,KFold,cross_val_predict,cross_val_score\nfold = KFold(shuffle=True)\ndef cv_accuracy(estimator,train_size=np.linspace(0.1,1.0,5)):\n                        \n                        model = cross_val_predict(estimator,x_train,y_train,cv=fold)\n                        accuracy1 = cross_val_score(estimator,x_train,y_train,cv=fold)\n                        print(accuracy1.mean())\n                        train_sizes,train_scores,test_scores = learning_curve(estimator,x_train,y_train,train_sizes=train_size,cv=fold)\n                        train_scores_mean = np.mean(train_scores,axis=1)\n                        train_scores_std = np.std(train_scores,axis=1)\n                        test_scores_mean = np.mean(test_scores,axis=1)\n                        test_scores_std = np.std(test_scores,axis=1)\n                        plt.plot(train_sizes,train_scores_mean,'o-',color='r',label='Training samples')\n                        plt.plot(train_sizes,test_scores_mean,'o-',color='g',label='Test samples')\n                        plt.xlabel('Training sizes')\n                        plt.ylabel('Error')\n                        plt.title('Learning curve')\n                        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression"},{"metadata":{},"cell_type":"markdown","source":"Since the target variable is a feature of probablity, the first option would to choose Logistic regression due to its effective probability feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\ncv_accuracy(LogisticRegression())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the cross validation results, the accuracy is around 80%. However, the learning curve indicates a high bias meaning the dataset is undersampled. Hence, Logistic regresion model is not considered."},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ncv_accuracy(DecisionTreeClassifier())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is 93% and the learning curve shows a decent gap between the training samples and cross-validation samples. "},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\ncv_accuracy(RandomForestClassifier())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Among the three models tested in CV, random forest classifier provides highest accuracy of 94%. The learning curve shows low bias and variance."},{"metadata":{},"cell_type":"markdown","source":"## Hyper Parameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nrc = RandomForestClassifier()\nparm = {'n_estimators':[100,200,300,500],'max_depth':[1,3,5,7]}\ngrid = GridSearchCV(estimator=rc,param_grid=parm,cv=fold)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy1 = cross_val_score(RandomForestClassifier(max_depth=7, n_estimators=300),x_train,y_train,cv=fold)\nprint(accuracy1.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the Grid search result, the best parameters are maximum depth '7' and the estimator number is 300. Hence, the same is applied to the test samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"rc = RandomForestClassifier(max_depth=7,n_estimators=300)\nrc = rc.fit(x_train,y_train)\nyhat = rc.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score,plot_confusion_matrix\nprint('Classification Report')\nprint(classification_report(y_test,yhat))\nprint('Accuracy',accuracy_score(y_test,yhat))\nplot_confusion_matrix(rc,x_test,y_test,cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the overall evaluation metrics, the model fits for 96"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_output = x_test\ndf_output['Easy Apply'] = yhat\ndf_output","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}