{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"B03jUPAsox5y"},"cell_type":"markdown","source":"# **Imports**"},{"metadata":{"id":"bdt0_6gD0Opt","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\n# No warnings about setting value on copy of slice\npd.options.mode.chained_assignment = None\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['font.size'] = 24\n\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Preprocessing, model selection & metrics import\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import f1_score\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"id":"uII8xLvGIldW","trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/richters-predictor-modeling-earthquake-damage'","execution_count":null,"outputs":[]},{"metadata":{"id":"l6PbNaEGrfpV"},"cell_type":"markdown","source":"# **Data Cleaning & Formatting**\n## **Load data**"},{"metadata":{"id":"rEScvhEr047O","trusted":true},"cell_type":"code","source":"train_values = pd.read_csv(os.path.join(root_path, 'train_values.csv'))\ntrain_labels = pd.read_csv(os.path.join(root_path, 'train_labels.csv'))\ntest_values = pd.read_csv(os.path.join(root_path, 'test_values.csv'))","execution_count":null,"outputs":[]},{"metadata":{"id":"_z4lcCSz1UMP","outputId":"e352287e-c6fb-4533-c869-596cc7f1bea9","trusted":true},"cell_type":"code","source":"print(f'Train shape: {train_values.shape}')\nprint(f'Test shape: {test_values.shape}')","execution_count":null,"outputs":[]},{"metadata":{"id":"uZLNtteVu_9s","trusted":true},"cell_type":"code","source":"#display to 5 rows\ntrain_values.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"qrKnYSTWusW-"},"cell_type":"markdown","source":"The data contains details about buildings/constructions damaged due to the earthquake occured in a region.\n\nFor that we are provided with geographical details, how many floor does the building had, age of construction, height, land surface, materials used for building the construction etc.\n\nHere we have supervised data, the label/target is damage level indicated as value 1, 2, & 3.\n\nThis comes under *multiclass* or *multinomial* classification where we need to predict one class from more than two classes."},{"metadata":{"id":"URAncThuxM8Z"},"cell_type":"markdown","source":"## Data types & missing values"},{"metadata":{"id":"5BGqulVZ1p-2","outputId":"242312d2-2c04-4bf4-ce3a-cc61f6e3ae3d","trusted":true},"cell_type":"code","source":"train_values.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"gxSH9VaRxaWF"},"cell_type":"markdown","source":"Most of the columns are numeric, 8 columns are categorical features which need to converted to numeric later to train model"},{"metadata":{"id":"_O5x5btM2A4-","outputId":"1b4853c8-0888-4f4f-90f3-0e79824820db","trusted":true},"cell_type":"code","source":"train_values.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"YbViwHk8yJ91"},"cell_type":"markdown","source":"## Missing values\n\nData is clean without missing values or nan, so we can proceed further for EDA."},{"metadata":{"id":"RPQFrYdt5eNu","trusted":true},"cell_type":"code","source":"#Merge the lable/target column with features\ntrain = pd.merge(train_values, train_labels, on='building_id')","execution_count":null,"outputs":[]},{"metadata":{"id":"WhZQcpd92CX7"},"cell_type":"markdown","source":"# **Exploratory Data Analysis**\n\nTo start data analysis, we begin with target variable here we have ordinal values so let us do count plot for this"},{"metadata":{"id":"28FfY8BT2QAF","outputId":"897ab937-f130-45f3-9820-b9620e8494db","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.style.use('fivethirtyeight')\nsns.countplot(train_labels['damage_grade'])\nplt.xlabel('Values'); plt.ylabel('counts'); \nplt.title('Damage grade')","execution_count":null,"outputs":[]},{"metadata":{"id":"7c_tK6IkO22j","outputId":"fe6e33f8-d62f-4656-c92d-da801d2c21e2","trusted":false},"cell_type":"code","source":"percent = list(train['damage_grade'].value_counts()/len(train['damage_grade'])*100)\nlabel = list(train['damage_grade'].value_counts().index)\nexplode = (0.1,0,0)\nfig,ax1 = plt.subplots()\nax1.pie(percent, explode=explode, labels=label, autopct='%1.1f%%', shadow=True)\nax1.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"tqETPkG_AM7I"},"cell_type":"markdown","source":"Damage category 2 & 3 are higher than 1"},{"metadata":{"id":"d0uILxSjIi6p"},"cell_type":"markdown","source":"### Density plot on continuous value features"},{"metadata":{"id":"LXHCDMbjAKvj","outputId":"a60b7830-45e0-47d1-ddbd-a53f805630de","trusted":true},"cell_type":"code","source":"#continuous values\ncont_values = ['geo_level_1_id','geo_level_2_id','geo_level_3_id','age','area_percentage','height_percentage']\n\ndef densityPlot(cont_values):\n  fig = plt.figure(figsize=(18,16))\n  plt.style.use('fivethirtyeight')\n  for i,txt in enumerate(cont_values):\n    ax = fig.add_subplot(3,2,i+1)\n    sns.kdeplot(train.loc[train['damage_grade'] == 1, txt], ax=ax, label='damage_grade==1')\n    sns.kdeplot(train.loc[train['damage_grade'] == 2, txt], ax=ax, label='damage_grade==2')\n    sns.kdeplot(train.loc[train['damage_grade'] == 3, txt], ax=ax, label='damage_grade==3')\n  plt.show()\ndensityPlot(cont_values)","execution_count":null,"outputs":[]},{"metadata":{"id":"9NPruvoaEJW_"},"cell_type":"markdown","source":"From plotting continuous feature values using kdeplot (kernel density plot), we can see there is few changes in pattern on columns `geo_level_1_id`,`geo_level_2_id` which will help us for better prediction"},{"metadata":{"id":"uRcWQ1g4Jtm7"},"cell_type":"markdown","source":"### Count plot on binary features"},{"metadata":{"id":"5H2qfMQ5J290","outputId":"d7ccf0e3-9ea1-4214-bcc6-0df2ed76f910","trusted":false},"cell_type":"code","source":"#Binary columns are selected using the column name having word 'has'\nbin_cols = train.columns[train.columns.str.startswith('has')]\n\ndef countPlot(bin_cols):\n  plt.rcParams['font.size'] = 18\n  plt.style.use('fivethirtyeight')\n  fig = plt.figure(figsize=(20,37))\n  for i,txt in enumerate(bin_cols):\n    ax = fig.add_subplot(8,3,i+1)\n    sns.countplot(x=train[txt], ax=ax, hue=train['damage_grade'])\n  plt.show()\ncountPlot(bin_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"XvOIxpf5wyoj"},"cell_type":"markdown","source":"1. All binary columns have more zero than 1, except `has_superstructure_cement_mortar_stone`\n2. In some columns there seems having only single value like all 0's, need to check."},{"metadata":{"id":"mRtsJHpPKXSy"},"cell_type":"markdown","source":"### Count plot on categorical columns"},{"metadata":{"id":"cqFyLpd9Kffq","outputId":"f71553a6-8684-4023-8e35-3e3f3dcd7c0c","trusted":false},"cell_type":"code","source":"cat_cols = train.select_dtypes(include=np.object).columns\n\ndef catPlot(cat_cols):\n  plt.rcParams['font.size'] = 18\n  plt.style.use('fivethirtyeight')\n  fig = plt.figure(figsize=(18,15))\n  for i,txt in enumerate(cat_cols):\n    ax = fig.add_subplot(3,3,i+1)\n    sns.countplot(x=train[txt], ax=ax, hue=train['damage_grade'])\n  plt.show()\ncatPlot(cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"sqc-93WbPxHL"},"cell_type":"markdown","source":"### Feature engineering\n\nAs a first step we need to concat both train & test data to do feature engineering on both the data."},{"metadata":{"id":"ffjj1ZxeY5DM","outputId":"5074eb60-106b-46f3-8dca-d41fba3a290d","trusted":false},"cell_type":"code","source":"df_full = pd.concat([train, test_values], axis=0).reset_index(drop=True)\ndf_full.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"LkGdI31eQcWq"},"cell_type":"markdown","source":"Function to merge new columns generated through feature engineering"},{"metadata":{"id":"aetjav8bOdbJ","trusted":false},"cell_type":"code","source":"def merge_by_concat(df1, df2, merge_on):\n  merged_gf = df1[merge_on]\n  merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n  new_columns = [col for col in list(merged_gf) if col not in merge_on]\n  df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n  return df1","execution_count":null,"outputs":[]},{"metadata":{"id":"HSmsYLIbQz2j"},"cell_type":"markdown","source":"Now let us create new columns by grouping *geographical* columns.\n1. Creating mean `age`,`height_percentage`,`area_percentage` from grouping geographic level columns `geo_level_1_id`,`geo_level_2_id`,`geo_level_3_id`"},{"metadata":{"id":"LB1fSXp6RjCH","trusted":false},"cell_type":"code","source":"geo_age_mean = df_full.groupby(['geo_level_1_id','geo_level_2_id','geo_level_3_id'])['age'].agg(['mean']).reset_index().rename(columns={'mean':'geo_grp_age_mean'})\ngeo_height_mean = df_full.groupby(['geo_level_1_id','geo_level_2_id','geo_level_3_id'])['height_percentage'].agg(['mean']).reset_index().rename(columns={'mean':'geo_grp_height_mean'})\ngeo_area_mean = df_full.groupby(['geo_level_1_id','geo_level_2_id','geo_level_3_id'])['area_percentage'].agg(['mean']).reset_index().rename(columns={'mean':'geo_grp_area_mean'})","execution_count":null,"outputs":[]},{"metadata":{"id":"X7eOEYNTTCCj"},"cell_type":"markdown","source":"Now let us create new columns by grouping different types of *roof* columns"},{"metadata":{"id":"lYDIwz47SL9k"},"cell_type":"markdown","source":"2. Creating mean `age`,`height_percentage`,`area_percentage` from grouping roof & foundation columns `foundation_type`,`roof_type`,`ground_floor_type`,`other_floor_type'"},{"metadata":{"id":"uQ6s_wNnTm7B","trusted":false},"cell_type":"code","source":"type_age_mean = df_full.groupby(['foundation_type','roof_type','ground_floor_type','other_floor_type'])['age'].agg(['mean']).reset_index().rename(columns={'mean':'type_grp_age_mean'})\ntype_height_mean = df_full.groupby(['foundation_type','roof_type','ground_floor_type','other_floor_type'])['height_percentage'].agg(['mean']).reset_index().rename(columns={'mean':'type_grp_height_mean'})\ntype_area_mean = df_full.groupby(['foundation_type','roof_type','ground_floor_type','other_floor_type'])['area_percentage'].agg(['mean']).reset_index().rename(columns={'mean':'type_grp_area_mean'})","execution_count":null,"outputs":[]},{"metadata":{"id":"dJMVOghCUSRX","trusted":false},"cell_type":"code","source":"#Merge the newly created columns\ndf_full = merge_by_concat(df_full, geo_age_mean, ['geo_level_1_id','geo_level_2_id','geo_level_3_id'])\ndf_full = merge_by_concat(df_full, geo_height_mean, ['geo_level_1_id','geo_level_2_id','geo_level_3_id'])\ndf_full = merge_by_concat(df_full, geo_area_mean, ['geo_level_1_id','geo_level_2_id','geo_level_3_id'])\n\ndf_full = merge_by_concat(df_full, type_age_mean, ['foundation_type','roof_type','ground_floor_type','other_floor_type'])\ndf_full = merge_by_concat(df_full, type_height_mean, ['foundation_type','roof_type','ground_floor_type','other_floor_type'])\ndf_full = merge_by_concat(df_full, type_area_mean, ['foundation_type','roof_type','ground_floor_type','other_floor_type'])","execution_count":null,"outputs":[]},{"metadata":{"id":"3MPrxkxzyRML"},"cell_type":"markdown","source":"Let us check whether binary columns having only single values, because they will not help for model creation"},{"metadata":{"id":"8zUP7NGVju-y","outputId":"964d04f9-7fb5-4b03-fef4-6631d100da59","trusted":false},"cell_type":"code","source":"colname=[]\nval=[]\nbin_cols = df_full.columns[df_full.columns.str.contains('has')]\nfor bcol in bin_cols:\n  colname.append(bcol)\n  val.append(df_full[bcol].value_counts().sort_index().values)\npd.DataFrame(val, index=colname)","execution_count":null,"outputs":[]},{"metadata":{"id":"00SRdlIBzRlq"},"cell_type":"markdown","source":"Our assumption is wrong there is no column with single values, all binary column having both 0 & 1."},{"metadata":{"id":"wWVNMDg1UFXs"},"cell_type":"markdown","source":"Now let us create new column with frequency encoding technique for categorical columns as a process in feature engineering"},{"metadata":{"id":"B_JzSGSd7hYC","trusted":false},"cell_type":"code","source":"freq_cols = ['land_surface_condition','foundation_type','roof_type',\n             'ground_floor_type','other_floor_type','position','plan_configuration',\n             'legal_ownership_status']\n\ndef frequency_encode(cols, df, self_encoding=False):\n  for c in cols:\n    fq_dict = df[c].value_counts().to_dict()\n    if self_encoding:\n      df[c] = df[c].map(fq_dict)\n    else:\n      df[c+'fq_enc'] = df[c].map(fq_dict)\n  return df\ndf_full_freq = frequency_encode(freq_cols, df_full, self_encoding=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"9ommfHIWUxXz"},"cell_type":"markdown","source":"We have finished feature engineering, as a next step we may split the train & test data for further process"},{"metadata":{"id":"GPhScvkffseJ","trusted":false},"cell_type":"code","source":"train_new = df_full_freq[:train_values.shape[0]]\ntest_new = df_full_freq[train_values.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"id":"X92VaHBnVQsz"},"cell_type":"markdown","source":"As a final step we need to check whether columns in our training data have any collinearity between them"},{"metadata":{"id":"iXv6G54Ti-9K","outputId":"77c77b30-87fa-4e51-853d-8ed328b01388","trusted":false},"cell_type":"code","source":"corr_matrix = train_new.drop(columns=['building_id','damage_grade'], axis=1).corr()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\nprint('Number of columns having collinearity with other columns other than target: ', len(to_drop))","execution_count":null,"outputs":[]},{"metadata":{"id":"ZO_TZR7x0XyC"},"cell_type":"markdown","source":"## Scaling data\n\nWe will scale all columns to avoid unwanted weightage for columns based on different range of numeric values like binary columns will have only 0 & 1, but geographic column having numeric values > 4 digits. "},{"metadata":{"id":"MTmNjCF5vwU0","trusted":false},"cell_type":"code","source":"scaler = StandardScaler()\n\nfeatures = train_new.drop(columns=['building_id','damage_grade'], axis=1).columns\n\nX = train_new.drop(columns=['building_id','damage_grade'], axis=1)\ntest_new = test_new.drop(columns=['building_id','damage_grade'], axis=1)\n\ny = train_new['damage_grade']\n\nsc = scaler.fit(X)\ntemp_train_X = sc.transform(X)\ntest_scaled = sc.transform(test_new)\n\nX = pd.DataFrame(temp_train_X, columns=features)\ntest = pd.DataFrame(test_scaled, columns=features)","execution_count":null,"outputs":[]},{"metadata":{"id":"TMW35WqQ-HyZ"},"cell_type":"markdown","source":"# Model creation\n\n### Function to check f1 score of model"},{"metadata":{"id":"Zr39aXP85cWs","trusted":false},"cell_type":"code","source":"def check_model_f1_score(model):\n  model.fit(X_train, y_train)\n  test_y_pred = model.predict(X_test)\n  return f1_score(y_test, test_y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"id":"3Jni5Dpt5H8E"},"cell_type":"markdown","source":"Spliting train data as 75% as train & remaining as test data for validating our model."},{"metadata":{"id":"otR-IDPmw0vN","trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"lWnMefRI8jqs"},"cell_type":"markdown","source":"Let us create a base prediction, by guessing damage grade value.\nHere we may do value_counts and select the grade with most value counts as the guessing damage grade prediction value.\nUsing that value let us check base prediction to check our trained model score need to cross more than the base model."},{"metadata":{"id":"Hq8kh3Qb6s3c","outputId":"35310ba3-54f8-477f-f885-82982472c385","trusted":false},"cell_type":"code","source":"grade_guess = y_test.value_counts().index[0]\nbase_prediction = np.full(np.shape(y_test), grade_guess)\nprint('F1 score of base prediction with guess value: ',f1_score(y_test, base_prediction, average='micro'))","execution_count":null,"outputs":[]},{"metadata":{"id":"5JzKLufRxKwF","outputId":"4f21a1fa-e94f-4610-ded3-a1e132b6dcc9","trusted":false},"cell_type":"code","source":"lr = LogisticRegression(multi_class='multinomial')\nlr_score = check_model_f1_score(lr)\nprint('Basic LOGISTIC REGRESSION model with default params: ', lr_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"c2M0AeCI9tLB"},"cell_type":"markdown","source":"Out initial logistic model have crossed the base model score, so we can proceed further to try different types of model like ensemble for better score."},{"metadata":{"id":"ihKeTya6JCm-","outputId":"cb6659c7-1c94-4853-9f19-bec54ae5b90c","trusted":false},"cell_type":"code","source":"dc = DecisionTreeClassifier()\ndc_score = check_model_f1_score(dc)\nprint('DECISION CLASSIFIER model with default params: ', dc_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"ydZ7wldSyk80","outputId":"f0c9c37b-2999-4f35-8025-0098bc69fcdc","trusted":false},"cell_type":"code","source":"rc = RandomForestClassifier()\nrc_score = check_model_f1_score(rc)\nprint('Basic RANDOM FOREST model with default params: ', rc_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"pgHtEEULKQGW","outputId":"f55a3bb3-de48-4de1-8404-a3b1d295f498","trusted":false},"cell_type":"code","source":"knc = KNeighborsClassifier()\nknc_score = check_model_f1_score(knc)\nprint('KNEIGHBORS model with default params: ', knc_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"ziYyrHnB-yh8","outputId":"4d580f99-73ef-4b39-d284-5f084f7fbc0c","trusted":false},"cell_type":"code","source":"xg = xgb.XGBClassifier()\nxg_score = check_model_f1_score(xg)\nprint('XGB model with default params: ', xg_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"MezQBR7L_Agt","outputId":"5501e85c-ef92-44ce-ab4c-1e8f48e55da5","trusted":false},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(8,6))\nmodel_comparison = pd.DataFrame({'model':['Logistic classifier','Decision Tree classifier',\n                       'Random forest classifier', 'kneighbors classifier',\n                       'xgb classifier'], 'f1_score':[lr_score,dc_score,rc_score,\n                                                      knc_score,xg_score]})\n\n# Horizontal bar chart of test mae\nmodel_comparison.sort_values('f1_score', ascending = False).plot(x = 'model', y = 'f1_score', kind = 'barh',\n                                                           color = 'red', edgecolor = 'black')\n\n# Plot formatting\nplt.ylabel(''); plt.yticks(size = 14); plt.xlabel('F1 score'); plt.xticks(size = 14)\nplt.title('Model Comparison on Test F1 score', size = 20);","execution_count":null,"outputs":[]},{"metadata":{"id":"Xi--K3qQR_DA"},"cell_type":"markdown","source":"On seeing the model comparison plot, it is clear that *Random foreset classsifier* out performs than other models, so let us proceed to fine tune hyper parameter of the model further for analysis."},{"metadata":{"id":"PmtK5PZdYN9L"},"cell_type":"markdown","source":"Fine tuned params for Random forest not improved, so let us use other model for prediction."},{"metadata":{"id":"VPiLER44l7gD","trusted":false},"cell_type":"code","source":"clf = xgb.XGBClassifier(\n    n_estimators=2000,\n    objective='multi:softmax',\n    num_class=3,\n    max_depth=12, \n    learning_rate=0.02, \n    subsample=0.8,\n    colsample_bytree=0.4, \n    missing=-1, \n    eval_metric='mlogloss',\n    nthread=4,\n    tree_method='hist'     \n    )","execution_count":null,"outputs":[]},{"metadata":{"id":"hCLvY_E3oSoZ","outputId":"83abed76-2dc8-4ae5-d75d-156e9f015674","trusted":false},"cell_type":"code","source":"clf.fit(X_train, y_train, eval_set=[(X_test,y_test)],verbose=50, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"id":"JFyXwQpAybBQ","outputId":"577dc94e-75be-4edb-dd20-5c5e49b24db1","trusted":false},"cell_type":"code","source":"xgb_score = check_model_f1_score(clf)\nprint('XGB score with tuned params: ', xgb_score)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}