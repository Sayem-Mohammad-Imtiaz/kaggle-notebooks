{"cells":[{"metadata":{"_uuid":"ce8f87da9f5cd240c05e94030e3a9556d0ca8b5b","_cell_guid":"dbdafe14-224a-43a5-9943-f537e1a80e8d"},"cell_type":"markdown","source":"# Introduction\nIn this notebook I'm going to try and predict the type(s) of Pokemon based on other features, such as their ability, weight, height and stats. Originally I'd wanted to do a multi-class, multi-label model, but I was having trouble getting that to work. Instead, I decided to settle for building the Type 1 & Type 2 predictions separately.\n\nI found a dataset on Kaggle to start off with, but soon realised it was missing useful features like Egg group(s), and contained errors due to how the data scraping was done.\n\nI found a much more complete set of data online, which I used to enhance my model:  https://github.com/veekun/pokedex/tree/master/pokedex/data/csv\n\nSome of the problems in the original data were related to Pokemon with multiple forms, like Alolan forms. Since multiple heights & weights were listed, they'd been extracted as NAN, so needed correcting. These Pokemon also had problems related to their typings, with normal & Alolan types mixed up. For simplicity, I reverted everything to its normal form.\n\nAdditionally, Genderless Pokemon had NAN values for their male percentage, since it wasn't listed in terms of a male/female ratio. To handle this, I changed the values to 0, and made a new Genderless feature.\n\nFinally, the original dataset did not cover the handful of new Pokemon from Ultra Sun / Ultra Moon, beyond Magearna. For simplicity, I deleted these entries from veekun's data, rather than trying to fill in the missing entries. I might lose a tiny bit of predictive power, but there are still 801 other Pokemon left.\n\nThroughout, I used an XGBoost model to make my predictions. I started by looking at predicting Type 1 on a .9/.1 train/test split, before fitting to the entire Pokedex. Then I tried predicting each generation based on the other 6. Finally, I repeated this, but instead looking at Type 2. I had originally expected that stat distributions would be important features, but I instead found that some surprising results.\n\nI hope you all find this interesting! I'm still learning Python, so apologies for instances of bad code practice (I'm sure there are many!), and would be happy to know better or more elegant solutions."},{"metadata":{"_uuid":"87ae1ff7821d0b5cd9caf3472a299add23418199","_cell_guid":"6dd3f768-1d7b-46fb-affa-9e6b676f88c2"},"cell_type":"markdown","source":"# Preparations\nFirstly, I loaded a bunch of packages,  in case I wanted to run a variety of different models. Since I just settled on using XGBoost, some of them never got used."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeClassifierCV,RidgeClassifier,LogisticRegression,LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import metrics\nimport seaborn as sns\nprint(os.listdir(\"../input\"))\nfrom sklearn import tree\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Any results you write to the current directory are saved as output.\n","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"628921a535d4e9485ece5293951a10eea375b259","_cell_guid":"727b809b-1c60-4c69-8707-53a0c20f75ac"},"cell_type":"markdown","source":"# Loading and Modifying Data\n\nI started by loading the data from veekun, and the data that I found on Kaggle. The former of which was much more thorough, and already contained multiple categorical features encoded as numbers, such as abilities or shape. Both sets required some degree of checking, cleaning and reorganizing. With veekun's data this mostly meant pulling out the relevant information I wanted, and reformatting it. For example, Ability data was split over multiple rows if the Pokemon had more than one ability.\n\nThe data I found on Kaggle mostly needed cleaning, because of how they'd scraped information from Serebii (a famous Pokemon website). For example, how Alolan Pokemon were handled lead to errors.\n\nSo I wasn't working with too many features, I decided to leave all categorical data that I used in numerical form, rather than using one-hot encoding. 3 Ability slots with the potential for hundreds of abilities would have made those features too unwieldy.\n\nFrom veekun's data I decided that Egg groups, Abilities, and miscellaneous information like Colour ID would be useful to include. Egg group for example should help finding Water and Flying Pokemon, whilst many abilities are directly tied to a Pokemon's type. Since veekun's data included Pokemon above Magearna, and in some cases alternative forms or Mega Evolutions, I had to trim off the ends of the data in order to fit with my original set. Where necessary, I also renamed columns to make merging and understanding easier.\n\nveekun's species data had a lot of overlap with the Kaggle user's set, but with some useful extra columns. Namely, color_id, shape_id and habitat_id. For example, lots of Fire Pokemon are Red, so that might be useful in the models. I extracted these and added them into my dataset.\n\nFinally, veekun's data had the abilities encoded numerically, and labelled based on their slot. As a note, all Pokemon can have up to 3 abilities, with the 3rd being a special Hidden ability, usually only available under certain conditions / promotions. As with the egg data, I converted the duplicate rows to additional columns to get Ability 1-3.\n\nNotable problems in the original data are mostly due to certain entries having odd formatting. For example, I had to correct Minior's capture rate, due to two listings. There were similar problems with Alolan Pokemon, or others with multiple forms. If more than one height / weight was listed, the scraping returned NAN, and if the Alolan form had extra types compared to the original versions, this was incorrectly pulled from the pages. I corrected all of these to make the data more useable.\n\nNext, the original data contained lots of features directly indicative of a Pokemon's type, such as their effectiveness against other types, so those had to be dropped.\n\nFinally, I opted to just use all of the numerical data, in the hope it would be enough to make reasonable models."},{"metadata":{"_uuid":"fc8aa7faac714ab53dbcdbd28cb5b84fb725b405","_kg_hide-output":false,"_kg_hide-input":true,"_cell_guid":"0a6f6d63-d5e2-408b-b8ce-2e615594825d","trusted":false,"collapsed":true},"cell_type":"code","source":"#Read data\npath = '../input/'\negg_df=pd.read_csv(path+\"egg-group-data-for-pokemon/pokemon_egg_groups.csv\")\nspecies_df=pd.read_csv(path+\"pokemon-species/pokemon_species.csv\")\nabilities_df=pd.read_csv(path+\"abilities/pokemon_abilities.csv\")\n\n#Split duplicates off & combine back\negg2_df=pd.DataFrame.copy(egg_df)\negg2_df=egg_df.loc[egg_df['species_id'].duplicated(), :]\negg_df.drop_duplicates('species_id',inplace=True)\nmerged = egg_df.merge(egg2_df,on=\"species_id\",how='outer')\nmerged.fillna(0,inplace=True)\n\n#Rename columns to simpler form.\nmerged.rename(index=str,columns={\"egg_group_id_x\":\"egg_group_1\"},inplace=True)\nmerged.rename(index=str,columns={\"egg_group_id_y\":\"egg_group_2\"},inplace=True)\n\n#Drop last 6 columns\nmerged.drop(merged.tail(6).index,inplace=True)\n\n#Rename\nmerged.rename(index=str,columns={\"species_id\":\"pokedex_number\"},inplace=True)\n\n#Make a new smaller dataframe\nspecies_trim_df=pd.DataFrame()\nspecies_trim_df[\"pokedex_number\"]=species_df['id']\nspecies_trim_df[\"color_id\"]=species_df['color_id']\nspecies_trim_df[\"shape_id\"]=species_df['shape_id']\nspecies_trim_df[\"habitat_id\"]=species_df['habitat_id']\nspecies_trim_df.drop(species_trim_df.tail(6).index,inplace=True)\n\n#Trim all below Magearna off\nabilities_df = abilities_df[abilities_df.pokemon_id < 802]\n\n#Make 3 new columns\nabilities_df[\"Ability1\"]=0\nabilities_df[\"Ability2\"]=0\nabilities_df[\"Ability3\"]=0\n\n#Assign values to the 3 columns based on the ability slot (1-3)\nabilities_df[\"Ability1\"] = abilities_df.ability_id.where(abilities_df.slot == 1,0)\nabilities_df[\"Ability2\"] = abilities_df.ability_id.where(abilities_df.slot == 2,0)\nabilities_df[\"Ability3\"] = abilities_df.ability_id.where(abilities_df.slot == 3,0)\n\n#Split duplicates off into new dataframes \n#3 abilities on some means it needs to be split twice\n#I'm sure there's an easier way to do this\nabilities_df2=pd.DataFrame.copy(abilities_df)\nabilities_df2=abilities_df.loc[abilities_df['pokemon_id'].duplicated(), :]\nabilities_df.drop_duplicates('pokemon_id',inplace=True)\nabilities_df3=pd.DataFrame.copy(abilities_df2)\nabilities_df3=abilities_df2.loc[abilities_df2['pokemon_id'].duplicated(), :]\nabilities_df2.drop_duplicates('pokemon_id',inplace=True)\n\n#Drop extra columns\nabilities_df.drop(['ability_id','is_hidden','slot'],axis=1,inplace=True)\nabilities_df2.drop(['ability_id','is_hidden','slot'],axis=1,inplace=True)\nabilities_df3.drop(['ability_id','is_hidden','slot'],axis=1,inplace=True)\n\n#Combine everything back\nabilities_df=abilities_df.set_index('pokemon_id').add(abilities_df2.set_index('pokemon_id'),fill_value=0).reset_index()\nabilities_df=abilities_df.set_index('pokemon_id').add(abilities_df3.set_index('pokemon_id'),fill_value=0).reset_index()\n\n#Rename pokemon_id to pokedex number to allow for merging.\nabilities_df.rename(index=str,columns={\"pokemon_id\":\"pokedex_number\"},inplace=True)\n\n#Read Kaggle data\npath = '../input/'\npokemon_df=pd.read_csv(path+\"pokemon/pokemon.csv\")\n\nName_df=pd.DataFrame()\nName_df[\"name\"]=pokemon_df[\"name\"].copy()\n\n#Fix Minior's capture rate\npokemon_df.capture_rate.iloc[773]=30\n\n#Change the type\npokemon_df['capture_rate']=pokemon_df['capture_rate'].astype(str).astype(int)\n\n#Merge all my data.\npokemon_df=pokemon_df.merge(merged,on=\"pokedex_number\",how='outer')\npokemon_df=pokemon_df.merge(species_trim_df,on=\"pokedex_number\",how='outer')\npokemon_df=pokemon_df.merge(abilities_df,on=\"pokedex_number\",how='outer')\n\n#Remove against columns\npokemon_df.drop(list(pokemon_df.filter(regex = 'against')), axis = 1, inplace = True)\n#Correct the spelling error\npokemon_df.rename(index=str,columns={\"classfication\":\"classification\"},inplace=True)\n\n#Change nan to 'none'\npokemon_df.type2.replace(np.NaN, 'none', inplace=True)\n\n#Drop Pokedex number for now\npokemon_df.drop(\"pokedex_number\",axis=1,inplace=True)\npokemon_df.drop(\"generation\",axis=1,inplace=True)\n\n#First find the NAs.\nindex_height = pokemon_df['height_m'].index[pokemon_df['height_m'].apply(np.isnan)]\nindex_weight = pokemon_df['weight_kg'].index[pokemon_df['weight_kg'].apply(np.isnan)]\nindex_male   = pokemon_df['percentage_male'].index[pokemon_df['percentage_male'].apply(np.isnan)]\n\n#Manually replace the missing heights & weights using the Kanto version etc\npokemon_df.height_m.iloc[18]=0.3\npokemon_df.height_m.iloc[19]=0.7\npokemon_df.height_m.iloc[25]=0.8\npokemon_df.height_m.iloc[26]=0.6\npokemon_df.height_m.iloc[27]=1.0\npokemon_df.height_m.iloc[36]=0.6\npokemon_df.height_m.iloc[37]=1.1\npokemon_df.height_m.iloc[49]=0.2\npokemon_df.height_m.iloc[50]=0.7\npokemon_df.height_m.iloc[51]=0.4\npokemon_df.height_m.iloc[52]=1.0\npokemon_df.height_m.iloc[73]=0.4\npokemon_df.height_m.iloc[74]=1.0\npokemon_df.height_m.iloc[75]=1.4\npokemon_df.height_m.iloc[87]=0.9\npokemon_df.height_m.iloc[88]=1.2\npokemon_df.height_m.iloc[102]=2.0\npokemon_df.height_m.iloc[104]=1.0\npokemon_df.height_m.iloc[719]=0.5\npokemon_df.height_m.iloc[744]=0.8\n\npokemon_df.weight_kg.iloc[18]=3.5\npokemon_df.weight_kg.iloc[19]=18.5\npokemon_df.weight_kg.iloc[25]=30.0\npokemon_df.weight_kg.iloc[26]=12.0\npokemon_df.weight_kg.iloc[27]=29.5\npokemon_df.weight_kg.iloc[36]=9.9\npokemon_df.weight_kg.iloc[37]=19.9\npokemon_df.weight_kg.iloc[49]=0.8\npokemon_df.weight_kg.iloc[50]=33.3\npokemon_df.weight_kg.iloc[51]=4.2\npokemon_df.weight_kg.iloc[52]=32.0\npokemon_df.weight_kg.iloc[73]=20.0\npokemon_df.weight_kg.iloc[74]=105.0\npokemon_df.weight_kg.iloc[75]=300.0\npokemon_df.weight_kg.iloc[87]=30.0\npokemon_df.weight_kg.iloc[88]=30.0\npokemon_df.weight_kg.iloc[102]=120.0\npokemon_df.weight_kg.iloc[104]=45.0\npokemon_df.weight_kg.iloc[719]=9.0\npokemon_df.weight_kg.iloc[744]=25.0\n\n#Create a Genderless column to separate them from the all-female cases.\npokemon_df[\"Genderless\"]=0\npokemon_df[\"Genderless\"].loc[list(index_male)]=1\n\n#Replace all the NANs with zeros in the % male\npokemon_df.percentage_male.replace(np.NaN, 0, inplace=True)\n\n#Check the typings of the pokemon with Alolan forms & fix\n#I'm sure this can be done much more elegantly\npokemon_df.type2.iloc[18]='none'\npokemon_df.type2.iloc[19]='none'\npokemon_df.type2.iloc[25]='none'\npokemon_df.type2.iloc[26]='none'\npokemon_df.type2.iloc[27]='none'\npokemon_df.type2.iloc[36]='none'\npokemon_df.type2.iloc[37]='none'\npokemon_df.type2.iloc[49]='none'\npokemon_df.type2.iloc[50]='none'\npokemon_df.type2.iloc[51]='none'\npokemon_df.type2.iloc[52]='none'\npokemon_df.type2.iloc[87]='none'\npokemon_df.type2.iloc[88]='none'\npokemon_df.type2.iloc[104]='none'\n\n#Lets start with just the numerical data for now.\nnum_features=pokemon_df.select_dtypes(include=np.number)\nnum_features=num_features.columns\n\nprint(\"The Type models will be built using the following features\")\nprint(list(num_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e6f5547a3959e32d4ed90d62e33c4b549c8a8d7","_cell_guid":"119de3ce-f59c-405f-b5da-fb987d679ac5"},"cell_type":"markdown","source":"These include, all 6 of a Pokemon's stats, and their combined total, how many steps it takes for an egg of that type to hatch, how happy the Pokemon is when you catch it, how hard the Pokemon is to catch, how much experience they need to reach level 100, their height and weight, their male ratio and if they are genderless, if they are legendary or not, their egg groups and abilities, their primary color, their body shape and where they live."},{"metadata":{"_uuid":"b3e50a399da577be77a7c5573593b31ceae9a342","_cell_guid":"00d08fff-3746-4196-9380-ecfc17b6f704"},"cell_type":"markdown","source":"# Features & Targets\n\nWith my numerical features now decided, I created new dataframes that would contain just the features and the targets, in the latter case Type 1 and Type 2 of the Pokemon."},{"metadata":{"_uuid":"df42166920f8dd2bd66a1ebde2e53032aeaaccf0","collapsed":true,"_kg_hide-input":true,"_cell_guid":"f490eecb-52e1-485b-b47f-8d62773adf9f","trusted":false},"cell_type":"code","source":"features=pd.DataFrame()\ntargets=pd.DataFrame()\ntargets2=pd.DataFrame()\nfeatures[num_features]=pokemon_df[num_features]\ntargets[\"type1\"]=pokemon_df[\"type1\"]\ntargets=np.ravel(targets)\ntargets2[\"type2\"]=pokemon_df[\"type2\"]\ntargets2=np.ravel(targets2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb665bfb61231454319b472ddaa93ce563043002","_cell_guid":"dd77a9ea-10fb-44c2-a97c-995289fd4daa"},"cell_type":"markdown","source":"# XGBoost for Type 1 with a 0.9/0.1 train/test split\n\nFor my very first model, I did a 0.9/0.1 training / test split on the full Pokedex, and measured the accuracy of an XGBoost model at guessing Type 1 of the Pokemon in the Test set.\n\nBefore going further, it's worth setting some benchmarks for prediction. There are 18 types, so random guessing would have about a 5% accuracy. The most common type is Water, at about 14%, so another strategy would be to just guess Water all the time. Anything better than that is an improvement."},{"metadata":{"_uuid":"722399f0f4284314444cf964fa145f9729585fbf","collapsed":true,"_kg_hide-input":true,"_cell_guid":"d9e00ad1-6f6d-4ab0-96e5-d0c15887b815","trusted":false},"cell_type":"code","source":"#Train Test Split\ntrain_features,test_features,train_targets,test_targets = train_test_split(features,targets,test_size=0.1,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb8a988eaf9b199cd78b39421b7004424d75566d","_cell_guid":"c6c588ca-fed4-445d-8634-60b3edc0ded8"},"cell_type":"markdown","source":"I started by fitting a model based on all the features, and trying to optimise the XGBoost parameters. For my dataset I found that most of these had no effect, or a detrimental effect on my results. The most significant improvement I found was setting the maximum depth to 5.\n\nIt soon became clear that it was relatively easy to reach 100% accuracy on the training set, with only a couple of features. However, this would only get about 40% accuracy on the test set, suggesting underfitting. On the other hand, if all the features were used\n\nMost of these barely affected the results, but I found that using a maximum depth of 5 would improve my results, so continued to use this value throughout.\n\nSince it's very likely that using all the features would lead to overfitting on the training data, I used forwards and backwards searches for feature selection, to improve my Test accuracy. I found that it was very easy to get 100% accuracy on the training set, with only a few features, but that this would only lead to about 40% accuracy on the Test set. At the other end, using all the features would overfit to the training data.\n\nTo find a balance, I did feature selection to find the best test accuracy."},{"metadata":{"_uuid":"d2fbbdf4be9836163634640308776d6c74d3c483","collapsed":true,"_kg_hide-input":true,"_cell_guid":"c1d5d892-1e44-4a28-b348-8ca8c5f9d3e5","trusted":false},"cell_type":"code","source":"use_feat=list(['egg_group_1','Ability1','egg_group_2','base_egg_steps','color_id',\n              'Ability3','base_happiness'])\n#82.72 % accuracy\n#Best sub-set of features on the training set.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"079fdac0c177a93dd4406eb6b43c5c2d98e449ae","collapsed":true,"_kg_hide-input":true,"_cell_guid":"5c19d7fc-2afb-45b0-b78a-f88248383eb6","trusted":false},"cell_type":"code","source":"#XGB parameters\nmodel_xgb=xgb.XGBClassifier(learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n #objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27,\n reg_alpha=0,\n reg_lambda=1,\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf543b31fd47f44e000a6e175ec3ef3205a7dafa","_kg_hide-input":true,"_cell_guid":"fa893b33-7862-4cb1-93a0-39aa64f506d2","trusted":false,"collapsed":true},"cell_type":"code","source":"model_xgb.fit(train_features[use_feat], train_targets)\ntrain_pred=model_xgb.predict(train_features[use_feat])\ntest_pred = model_xgb.predict(test_features[use_feat])\n\n# evaluate predictions\ntrain_accuracy = accuracy_score(train_targets, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(test_targets, test_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nsns.set(font_scale=0.8)\nxgb.plot_importance(model_xgb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba7285cec2eece5ef9e957bf4e5311aa260a8517","_cell_guid":"009c9c07-90ad-4c2c-979e-bbf138a5de7c"},"cell_type":"markdown","source":"After non-exhaustive searching, my best result was 82.72% test accuracy (with 99.03 % train accuracy), using a model with 7 features, which will all likely need an explanation for non-experts.\n\nThe features are: Egg group 1, Egg Group 2, Ability 1,  Ability 3, base egg steps, color id and base happiness.\n\nIt's obvious how some of these features are related to a Pokemon's types, but not others.\n\nEgg Groups 1 & 2 indicate which other Pokemon a specific species is capable of breeding with. For example, there are several different Water egg groups, which primarily include Water Pokemon. Similarly for groups like Bug, Fairy & Flying. Others like Field group are less obvious, but could be used to exclude non terrestrial Pokemon.\n\nAbility 1 & 3 are the special abilities a Pokemon can have, with 1 being their normal ability, and 3 being their Hidden ability, usually obtained under special conditions. It's no surprise that these work well, because fire Pokemon often have abilities that power up fire moves and so on. So even though the algorithm doesn't know an ability improve the attack power of fire Pokemon, it can see that it's always on fire Pokemon. Some other abilities are found on a variety of different Pokemon, meaning extra information is needed to improve the predicitons.\n\nColor ID is associated with the main colour of the Pokemon. It doesn't cover every single colour (for example I don't think there's one for orange), but will give a slight indication of the Pokemon's appearance. Many times, a Pokemon's colour scheme is fairly simplistic, with fire normally red, grass green and water blue. Some Pokemon will however completely throw this off, like Scizor, who is a Bug/Steel that is bright red. This could be useful in combination with other features.\n\nBase egg steps is a measure of how many steps it would take to hatch an egg of that type. Some Pokemon are very quick, others very slow. At first, you might wonder how on earth this is related to a Pokemon's type, since the rate at which eggs hatch will usually be all over the place for any given type. However, there are certain types, like Dragons, which tend to have slower hatching eggs, so might help separate them out from other types.\n\nThe oddest inclusion is the base happiness, which I didn't even realised varied before I started this project. In Pokemon, happiness is a measure of how much the Pokemon likes you, and can power up certain moves, or trigger certain evolutions. Base happiness is how happy they are when you catch them. The default value is 70, but certain types can be higher (Fairies) or lower (Dark), and some special Pokemon can even have 0."},{"metadata":{"_uuid":"2e3cf973f31f665f7b6c83fe70101dc14aaad36c","_kg_hide-input":true,"_cell_guid":"c20522c3-2c25-4b37-9786-2d802c6ada36","trusted":false,"collapsed":true},"cell_type":"code","source":"# Output a plot of the confusion matrix.\nlabels =list(set(test_targets))\ncm = metrics.confusion_matrix(test_targets, test_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84b12c33bd493fedd995a3c734f14878366f2101","_cell_guid":"c78c6abc-60a0-4cde-8ab7-fa9f466b8d02"},"cell_type":"markdown","source":"Another way to check how the model performs is to look at the confusion matrix for the test data. It's clear that most types get a perfect match, but others are less successful. The worst of which is Flying, which always gets misclassified as Dragon. This is potentially because many flying Pokemon have flying as their 2nd type, with the 1st type being relatively rare.\n\nOther notable problems are seen for Ground and Psychic, which are misclassified as several other types.\n\nA few of the mistakes are not surprising, given the similarities between the types, and the fact that they are usually associated with each other. For example, the ice/water and steel/rock confusion."},{"metadata":{"_uuid":"08222559ffcec1a0ed9aaf6d9ca1cb61346265d1","_cell_guid":"0f351d1a-5a66-47df-8731-9883e8b14026"},"cell_type":"markdown","source":"# XGBoost on the Full Pokedex\n\nBefore going any further, I decided it would be a good idea to explore fitting to the entire Pokedex, without worrying about making any new predictions.  As mentioned above it became clear that a good model for the training data could be built using very few features."},{"metadata":{"_uuid":"4eee20f28d5f30f09597f8e0d2cece12e12e4786","collapsed":true,"_kg_hide-input":true,"_cell_guid":"c9db63e4-ec89-4c7e-88e3-4963d9b49808","trusted":false},"cell_type":"code","source":"#Best 3 feature: for the final feature Defense /  HP / Sp Defense / Speed @ 100%\n#use_feat = list(['Ability1','weight_kg','defense'])\n\n#What about if I exclude abilities, since I feel these are too closely linked to Type?\nuse_feat = list(['weight_kg','base_total','defense'])\n# Special attack also works","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b661be62eda46626b2c8c6d49db821c64a0c09c3","_kg_hide-input":true,"_cell_guid":"67d07e21-2e71-429d-91f9-34470ac42522","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.set(font_scale=0.8)\nmodel_xgb.fit(features[use_feat], targets)\ny_pred = model_xgb.predict(features[use_feat])\n# evaluate predictions\naccuracy = accuracy_score(targets, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nxgb.plot_importance(model_xgb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6c238b0a715363297993f4811f5b0745c58c5e9","_cell_guid":"f66637b7-56cd-43e7-af2c-a1e8f407098c"},"cell_type":"markdown","source":"I managed to find two subsets of features, which could both reach 100% accuracy with only 3 features. \n\nIn the first case, Ability 1, Weight and then one out of either Defense, HP, Special Defense or Speed was all you needed. In fact, a 2 feature model could already reach over 99% accuracy, with the choice of several stats helping with the last 1 or 2 classifications.\n\nIn the second case, Weight, total base stats, and one out of Defense or Special attack could also get 100% accuracy.\n\nAs mentioned above, it's not surprising that Ability 1 is a good predictor, and it makes intuitive sense that their stats would be important in some way. For example, most electric Pokemon are faster than most rock or steel Pokemon.\n\nWhat surprised me most was how useful the weight proved to be. I knew that, for example, Ghosts are usually lighter than Rock or Steel types, but did not think there was a significant difference between other types, like Fire and Water."},{"metadata":{"_uuid":"947b3ebdb2ea348ce68a4e146e305b319c2acdef","_cell_guid":"f34b4a70-a396-40ae-b882-1fbb189cee80"},"cell_type":"markdown","source":"# XGBoost for each of the 7 Generations of Pokemon\n\nMy next step was to make Type 1 predictions for all of the Pokemon within a certain generation, based on a model trained on the other 6 generations.\nI wanted to see which generations were easier or harder to predict compared to the others, and thus make a statement on which generations were more normal or unusual.\n\nIt quickly became apparent that the subset of features that gave the highest accuracy could vary from generation to generation, meaning that it is difficult to select a subset that will perform well for all 7 generations.\n\nIf  I used all of the features, I was able to get accuracies of  61.59, 62.00, 48.90, 52.34, 48.08, 38.89 and 42.5 % on generations 1 to 7 respectively. From this alone, it's clear that generations 1 & 2 can be modelled more accurately, and that later generations are harder to model, possibly due to more diverse typings and designs.\n\nWith these as the baseline, I then performed feature selection for all 7 generations, to find the best accuracy. Here I report the best feature selections I was able to find, but cannot guarantee they are the absolute minimum.  Since my XGBoost parameters include some degree of subsampling, I had not realised at the time that feature order could sometimes matter. I have not properly explored this effect in this data, due to the large time commitment it would involve."},{"metadata":{"_uuid":"662a4fdf25e34cab82da84319294827ea117bff9","collapsed":true,"_kg_hide-input":true,"_cell_guid":"5037acb5-95a6-4bb9-9eb2-d27e1648de37","trusted":false},"cell_type":"code","source":"#Split features & targets into each generation.\nGen1_features=features[0:151]\nGen2_features=features[151:251]\nGen3_features=features[251:386]\nGen4_features=features[386:493]\nGen5_features=features[493:649]\nGen6_features=features[649:721]\nGen7_features=features[721:801]\nGen1_targets=targets[0:151]\nGen2_targets=targets[151:251]\nGen3_targets=targets[251:386]\nGen4_targets=targets[386:493]\nGen5_targets=targets[493:649]\nGen6_targets=targets[649:721]\nGen7_targets=targets[721:801]\nGen1_targets=np.ravel(Gen1_targets)\nGen2_targets=np.ravel(Gen2_targets)\nGen3_targets=np.ravel(Gen3_targets)\nGen4_targets=np.ravel(Gen4_targets)\nGen5_targets=np.ravel(Gen5_targets)\nGen6_targets=np.ravel(Gen6_targets)\nGen7_targets=np.ravel(Gen7_targets)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1779adc5e1b80a5a6edf1d538bf2012cd2276c23","collapsed":true,"_kg_hide-input":true,"_cell_guid":"f1f831d4-222e-4ea2-b1b2-486db673faac","trusted":false},"cell_type":"code","source":"#Recombine 6 of them, in 7 different ways, to make my different training sets\n#Ordering of the features & targets should be the same!\n#But doesn't have to be necessarily in numerical order\nGens_not1_features=pd.concat([Gen2_features,Gen3_features,Gen4_features,Gen5_features,Gen6_features,Gen7_features],axis=0)\nGens_not2_features=pd.concat([Gen1_features,Gen3_features,Gen4_features,Gen5_features,Gen6_features,Gen7_features],axis=0)\nGens_not3_features=pd.concat([Gen2_features,Gen1_features,Gen4_features,Gen5_features,Gen6_features,Gen7_features],axis=0)\nGens_not4_features=pd.concat([Gen2_features,Gen3_features,Gen1_features,Gen5_features,Gen6_features,Gen7_features],axis=0)\nGens_not5_features=pd.concat([Gen2_features,Gen3_features,Gen4_features,Gen1_features,Gen6_features,Gen7_features],axis=0)\nGens_not6_features=pd.concat([Gen2_features,Gen3_features,Gen4_features,Gen5_features,Gen1_features,Gen7_features],axis=0)\nGens_not7_features=pd.concat([Gen2_features,Gen3_features,Gen4_features,Gen5_features,Gen6_features,Gen1_features],axis=0)\nGens_not1_targets=np.concatenate((Gen2_targets,Gen3_targets,Gen4_targets,Gen5_targets,Gen6_targets,Gen7_targets),axis=0)\nGens_not2_targets=np.concatenate((Gen1_targets,Gen3_targets,Gen4_targets,Gen5_targets,Gen6_targets,Gen7_targets),axis=0)\nGens_not3_targets=np.concatenate((Gen2_targets,Gen1_targets,Gen4_targets,Gen5_targets,Gen6_targets,Gen7_targets),axis=0)\nGens_not4_targets=np.concatenate((Gen2_targets,Gen3_targets,Gen1_targets,Gen5_targets,Gen6_targets,Gen7_targets),axis=0)\nGens_not5_targets=np.concatenate((Gen2_targets,Gen3_targets,Gen4_targets,Gen1_targets,Gen6_targets,Gen7_targets),axis=0)\nGens_not6_targets=np.concatenate((Gen2_targets,Gen3_targets,Gen4_targets,Gen5_targets,Gen1_targets,Gen7_targets),axis=0)\nGens_not7_targets=np.concatenate((Gen2_targets,Gen3_targets,Gen4_targets,Gen5_targets,Gen6_targets,Gen1_targets),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-output":true,"_uuid":"8b9614b890b290c0a01af2f5faf84a816d6c8381","collapsed":true,"_kg_hide-input":true,"_cell_guid":"aa395a5e-609e-4e16-8629-29dce27c16be","trusted":false},"cell_type":"code","source":"#Iterate forwards\n#use_feat=list(['attack', 'base_egg_steps', 'base_happiness', 'base_total',\n#       'capture_rate', 'defense', 'experience_growth', 'height_m', 'hp',\n#       'percentage_male', 'sp_attack', 'sp_defense', 'speed', 'weight_kg',\n#       'is_legendary', 'egg_group_1', 'egg_group_2', 'color_id', 'shape_id',\n#      'habitat_id', 'Ability1', 'Ability2', 'Ability3', 'Genderless'])\n#use_feat=list(['attack',\n#       'capture_rate', 'experience_growth',\n#         'speed', 'weight_kg',\n#         'shape_id','sp_attack',\n#      'habitat_id', 'Ability1',  'Ability3'])\n#for i in use_feat:\n#    feats=['height_m', 'percentage_male','Genderless', 'hp','defense','base_total','base_egg_steps','is_legendary','egg_group_1','egg_group_2','sp_defense',\n#           'base_happiness', 'Ability2','color_id']\n#    feats.insert(0,i),\n#    print(\"Adding\")\n#    print(i)\n#    model_xgb.fit(Gens_not5_features[feats], Gens_not5_targets)\n#    test_pred = model_xgb.predict(Gen5_features[feats])\n#    test_accuracy = accuracy_score(Gen5_targets, test_pred)\n#    print(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a703b9f457364884bc258cd507b920f1619f0aa5","collapsed":true,"_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"00dbd55d-ee36-4bbd-b989-97a80b25feae","trusted":false},"cell_type":"code","source":"#Iterate backwards\n#use_feat=list(['attack', 'base_egg_steps', 'base_happiness', 'base_total',\n#       'capture_rate', 'defense', 'experience_growth', 'height_m', 'hp',\n#       'percentage_male', 'sp_attack', 'sp_defense', 'speed', 'weight_kg',\n#       'is_legendary', 'egg_group_1', 'egg_group_2', 'color_id', 'shape_id',\n#      'habitat_id', 'Ability1', 'Ability2', 'Ability3', 'Genderless'])\n#\n#use_feat=list([ 'base_egg_steps',\n#       'capture_rate', \n#         \n#       'egg_group_1',\n#      ])\n#\n#for i in use_feat:\n#    feats=use_feat.copy()\n#    print (\"Remove\")\n#    print(i)\n#    feats.remove(i)\n#    model_xgb.fit(Gens_not7_features[feats], Gens_not7_targets)\n#    test_pred = model_xgb.predict(Gen7_features[feats])\n#    test_accuracy = accuracy_score(Gen7_targets, test_pred)\n#    print(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f992484c7937b16f9dcbfc1e15a5e0c81efbd071","_cell_guid":"6b5e6cea-f50b-4c13-9084-1cc18f6d6a08"},"cell_type":"markdown","source":"# Generation 1\nThe best accuracy I could get for Generation 1 was 70.20 %, which I got by dropping the male percentage and speed from the full set of features. This results in a fairly complicated model. By comparison, I could get ~66% with only 5 features."},{"metadata":{"_uuid":"33234622dce9c0159210c226eb08266f7e4d417c","_kg_hide-input":true,"_cell_guid":"281322bc-3c42-4116-b819-e414714abdfd","trusted":false,"collapsed":true},"cell_type":"code","source":"#Generation 1 model\nuse_feat=list(['attack', 'base_egg_steps', 'base_happiness', 'base_total',\n       'capture_rate', 'defense', 'experience_growth', 'height_m', 'hp',\n        'sp_attack', 'sp_defense',  'weight_kg',\n       'is_legendary', 'egg_group_1', 'egg_group_2', 'color_id', 'shape_id',\n      'habitat_id', 'Ability1', 'Ability2', 'Ability3', 'Genderless'])\nsns.set(font_scale=0.8)\nmodel_xgb.fit(Gens_not1_features[use_feat], Gens_not1_targets)\nGen1_T1_pred = model_xgb.predict(Gen1_features[use_feat])\n\n# evaluate predictions\ntest_accuracy = accuracy_score(Gen1_targets, Gen1_T1_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen1_targets))\ncm = metrics.confusion_matrix(Gen1_targets, Gen1_T1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fcffb4edefa10f4a23659af446ba91beec13b03","_cell_guid":"f0d9c802-cb25-47df-91de-41b8756959d4"},"cell_type":"markdown","source":"You can see that Grass and Bug are predicted with 100% accuracy, and Normal and Water are also fairly accurate.\n\nThe worst performing by far are Dragons and Ghost, which get misclassified as Water and Psychic. This is possibly not surprising, because there are only 3 of each in Generation 1.\n\nOther types appear to be commonly misclassified as Normal, Water and Psychic.\n\nSome of the mistakes sort of make sense, like the water/ice, fairy/normal and fighting / ground problems, since the types are sort of similar. Others like fire/electricity make no sense to me.\n\nSomething that will be interesting to see later is whether some of these misclassifications were actually the Pokemon's 2nd type."},{"metadata":{"_uuid":"a86d9fb043a86fa23cc7a9c0f1217abdbdbaf294","_cell_guid":"9d126eff-6537-4ef3-966b-576f31dd9e47"},"cell_type":"markdown","source":"# Generation 2\nFor Generation 2, I was able to get a test accuracy of 74% by reducing the features to 12:\nbase_happiness,  capture_rate, defense, experience_growth, weight_kg, egg_group_1, egg_group_2, color_id, Ability2, Ability3"},{"metadata":{"_uuid":"23acc65ae62d3436996f4e03ce6acc25f5547493","collapsed":true,"_kg_hide-input":true,"_cell_guid":"e1d8230c-b360-4d0a-a65f-724dc4649526","trusted":false},"cell_type":"code","source":"#Generation 2 model\nuse_feat=list(['base_happiness',\n       'capture_rate', 'defense', 'experience_growth',\n        'weight_kg',\n        'egg_group_1', 'egg_group_2', 'color_id',\n        'Ability2', 'Ability3'])\nsns.set(font_scale=0.8)\nmodel_xgb.fit(Gens_not2_features[use_feat], Gens_not2_targets)\nGen2_T1_pred = model_xgb.predict(Gen2_features[use_feat])\n\n# evaluate predictions\ntest_accuracy = accuracy_score(Gen2_targets, Gen2_T1_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen2_targets))\ncm = metrics.confusion_matrix(Gen2_targets, Gen2_T1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f73fdab1cbd558d708916ca10928f48a5d5aa09c","_cell_guid":"5bbeaea8-2cc3-43c4-b78a-9158c0394d56"},"cell_type":"markdown","source":"One clear improvement over Generation 1 is that for all types, there are at least some correct predictions.\n\nIn this case, there are perfect matches for poison, grass, water, ghost and bug, with good match for normal. So far, it looks like Water, Grass, Bug and Normal types are the easiest to predict.\n\nOnce again, problems with water / ice and fire / electric appear, along with a new rock / steel problem. This latter is understandable, since they are both heavy with high defenses, features which are both included in the model.\n\nSome types like Psychic and Ground are a bit of a mess, but at least nothing is completely misclassified this time.\n\nDespite there fairly good overall accuracy, there are a few worrying mistakes, like mistaking a fairy for dark, and fire for water."},{"metadata":{"_uuid":"ed4aeab96f606037d7f9203b63ee769f6b8b9c53","_cell_guid":"c5b2bba7-37d3-4b91-b25d-7270c5676896"},"cell_type":"markdown","source":"# Generation 3\n\nFor Generation 3, I was able to get ~63% accuracy by using 14 features:\n base_egg_steps, base_total, capture_rate, experience_growth, height_m, hp, percentage_male, sp_attack, egg_group_1, egg_group_2, color_id, habitat_id,  Ability3, and Genderless."},{"metadata":{"_uuid":"42c23491c7814f06cafb29af6db9d1b45c68e635","collapsed":true,"_kg_hide-input":true,"_cell_guid":"c829ba92-c28e-4520-860b-b73e9899bcee","trusted":false},"cell_type":"code","source":"#Generation 3 model\nuse_feat=list([ 'base_egg_steps',  'base_total','capture_rate', 'experience_growth',\n               'height_m', 'hp','percentage_male', 'sp_attack','egg_group_1',\n               'egg_group_2', 'color_id', 'habitat_id',  'Ability3', 'Genderless'])\nsns.set(font_scale=0.8)\nmodel_xgb.fit(Gens_not3_features[use_feat], Gens_not3_targets)\nGen3_T1_pred = model_xgb.predict(Gen3_features[use_feat])\n\n# evaluate predictions\ntest_accuracy = accuracy_score(Gen3_targets, Gen3_T1_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen3_targets))\ncm = metrics.confusion_matrix(Gen3_targets, Gen3_T1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ded301865958c2622b9612cec3f748a4d79d2ced","_cell_guid":"c38ca884-2716-48d0-88ce-08254c9cc045"},"cell_type":"markdown","source":"As the prediction accuracy gets lower, the confusion matrix gets messier. As with the other cases, there are a few types with good or perfect accuracy, but we're getting an increasing number of types with predictions all over the place, or that fail to get any predictions correct at all.\n\nAs with Generation 2, Ghost type is predicted with 100% accuracy, in contrast to the complete failure in Generation 1. I suspect this might be because the Ghosts in Generation 1 were dual type, whilst those in Generations 2 and 3 are pure Ghost.\n\nUnlike previous generations, there is no confusion between Fire and Electric this time, with both getting 100% accuracy. Although there are some incorrect predictions associated with these types for other Pokemon. Bug, Water and Fighting also do well, although they all have some misclassifications. Looking at the mistakes, like water or rock for a bug type, I do wonder if it's related to their second type again.\n\nUnexpectedly, Grass and Normal both have issues this time, often getting confused for each other.\n\nThe Ice / Water  and Rock / Steel confusions still seem to persist, although in both cases the amount of correct classifications seems to be getting worse.\n\nDark and Ground predictions fail completely, with no correct predictions, and others like Dragon or poison are regularly mistaken for many different types."},{"metadata":{"_uuid":"887b42a849dd34e7984f8ab1a8b373bcc75f2b40","_cell_guid":"54bd38d0-4b4c-40ee-9cbc-23c8f9a20d97"},"cell_type":"markdown","source":"# Generation 4\n\nFor Generation 4, I was able to build a model with ~67% accuracy, which used 11 features:\nbase_egg_steps, hp, sp_attack, sp_defense, is_legendary, egg_group_1, egg_group_2, shape_id, Ability1, Ability3,  and speed."},{"metadata":{"_uuid":"95894fa56042c635030d44231c9849689e51d808","collapsed":true,"_kg_hide-input":true,"_cell_guid":"2a768602-e669-4977-9646-565f930faa53","trusted":false},"cell_type":"code","source":"#Generation 4 model\nuse_feat=list(['base_egg_steps','hp','sp_attack','sp_defense','is_legendary'\n               ,'egg_group_1', 'egg_group_2', 'shape_id','Ability1',  'Ability3','speed'])\nsns.set(font_scale=0.8)\nmodel_xgb.fit(Gens_not4_features[use_feat], Gens_not4_targets)\nGen4_T1_pred = model_xgb.predict(Gen4_features[use_feat])\n\n# evaluate predictions\ntest_accuracy = accuracy_score(Gen4_targets, Gen4_T1_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen4_targets))\ncm = metrics.confusion_matrix(Gen4_targets, Gen4_T1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8c14f771cd882943962d37ac6994eebac221d17","_cell_guid":"1c000efe-f2e8-4459-b77e-10e5d442424a"},"cell_type":"markdown","source":"As usual, the algorithm performs better on some types than others.\n\nBug, Fighting, Dragon, Fairy, and rock are all predicted with 100% accuracy, with Water, Pyschic and possibly Fire also performing well.\n\nEvery type has at least one correct prediction, although Electric and Poison appear to perform particularly badly. Others like Normal have reasonable accuracy, but show a range of misclassifications.\n\nUnlike previous Generations, the Ice / Water and Rock / Steel confusions do not appear, suggesting something different about those Pokemon in this generation. The Electric / Fire problem is still present though.\n\nSome of the misclassifications here are particularly worrying, because they get the complete opposite types, for example Fire mistaken for Water, or Grass and Ice for Fire."},{"metadata":{"_uuid":"e09d6d5bc4fd0772220c5263b9a229e89da0adf9","_cell_guid":"7392f289-60ae-4d4f-89bd-8bb73faa5025"},"cell_type":"markdown","source":"# Generation 5\nFor Generation 5, I was able to build a model with ~56% accuracy, which used 10 features:\n\nGenderless, hp, base_egg_steps, is_legendary, egg_group_1, egg_group_2, sp_defense, base_happiness, Ability2, and color_id.\n\nUnfortunately, this appears to be the limits of the accuracy with the current features."},{"metadata":{"_uuid":"92965c93182ab40e17f915c1ec0f06e9c918dc3c","collapsed":true,"_kg_hide-input":true,"_cell_guid":"0fa8b6ff-ffa9-4f04-8aaf-f0f39197ac6d","trusted":false},"cell_type":"code","source":"#Generation 5 model\n#10 feat for 56.41\nuse_feat=list(['Genderless', 'hp','base_egg_steps','is_legendary','egg_group_1',\n               'egg_group_2','sp_defense','base_happiness', 'Ability2','color_id'])\nsns.set(font_scale=0.8)\nmodel_xgb.fit(Gens_not5_features[use_feat], Gens_not5_targets)\nGen5_T1_pred = model_xgb.predict(Gen5_features[use_feat])\n\n# evaluate predictions\ntest_accuracy = accuracy_score(Gen5_targets, Gen5_T1_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen5_targets))\ncm = metrics.confusion_matrix(Gen5_targets, Gen5_T1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84209c7623274bf05bef2ba4b5b9a9151ed4c255","_cell_guid":"0bb16185-2276-4543-9d65-c724f5950795"},"cell_type":"markdown","source":"As the prediction accuracy edges ever closer to 50%, I'm expecting to see some serious problems crop up in the confusion matrix. This is especially clear here.\n\nWhilst Bug and Grass are still consistent performers, along with good results for Normal, Fire, Fighting and Rock, most of the others are a complete mess.\n\nPoison, Flying and Electric all get 0% accuracy, with flying always misclassified as Dragon. Most other types are all over the place, most surprising of which is Water, which usually does well. Dark and Ground for example are misclassified as nearly half the other types. \n\nOne possible explanation for the poor performance for Generation 5 is that is was meant as somewhat of a reboot of the series. Unlike most other Pokemon games, it featured none of the old Pokemon in the main game, with the region initially entirely populated by new Pokemon."},{"metadata":{"_uuid":"d451ff488ec5bd20d06644bead9099f916ea0c54","_cell_guid":"4dd41514-762e-4a2c-9bba-20495b50bf01"},"cell_type":"markdown","source":"# Generation 6\n\nFor Generation 6, I was barely able to get the accuracy above 50%, with my best result at ~53%, using 15 features:\n\n base_egg_steps, base_happiness, base_total, capture_rate, height_m, sp_defense, speed, weight_kg, egg_group_1, color_id, shape_id, habitat_id,  Ability2, Ability3, and Genderless.\n \nBy comparison,  a much simpler model, only using 6 features was able to get 51.39% accuracy. Namely:\n\ncapture_rate, egg_group_1,  color_id, shape_id, Ability2, Ability3\n\nGiven that all 6 are contained in the more complicated model, I question the usefulness of adding 9 extra features to gain about 1.5% accuracy."},{"metadata":{"_uuid":"d1140e4a12803d7e40033e6e39555360b3b13e1b","collapsed":true,"_kg_hide-input":true,"_cell_guid":"200ea445-77ab-45af-9507-4d0bfbeb291a","trusted":false},"cell_type":"code","source":"#Generation 6 model\n#52.78 at the moment.\nuse_feat=list([ 'base_egg_steps', 'base_happiness', 'base_total',\n       'capture_rate',   'height_m', \n        'sp_defense', 'speed', 'weight_kg',\n        'egg_group_1',  'color_id', 'shape_id',\n      'habitat_id',  'Ability2', 'Ability3', 'Genderless'])\n#51.39\n#use_feat=list(['capture_rate', 'egg_group_1',  'color_id', 'shape_id','Ability2', 'Ability3'])\nsns.set(font_scale=0.8)\nmodel_xgb.fit(Gens_not6_features[use_feat], Gens_not6_targets)\nGen6_T1_pred = model_xgb.predict(Gen6_features[use_feat])\n\n# evaluate predictions\ntest_accuracy = accuracy_score(Gen6_targets, Gen6_T1_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen6_targets))\ncm = metrics.confusion_matrix(Gen6_targets, Gen6_T1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e10e26f6c35ef23a7690293c0b351df17be698d","_cell_guid":"293dbd9f-82c0-4e82-8ec0-b0649a6f0f1a"},"cell_type":"markdown","source":"Even though the accuracy is slightly worse than Generation 5, the confusion matrix looks a bit less messier. The algorithm is apparently a bit more consistent about what it gets wrong.\n\nSurprisingly, 6 types get 100% accuracy, Bug, Dragon, Ice, Grass, Water and Electric.\n\nHowever, 5 also get 0% accuracy, Steel, Dark, Fairy, Poison and Flying.\n\nOthers like Psychic, Fire, Normal and Ghost are more of a mixed bag.\n\nMany of the incorrect predictons appear to be due to assigning Water, Grass or Normal to other types, possible suggesting the model is overfitting to those types in the training data."},{"metadata":{"_uuid":"44f7c36bad3b917a79f2f0660bd1a20d20c1bf71","_cell_guid":"e5425b34-b215-4629-ab0b-c94e79422f62"},"cell_type":"markdown","source":"# Generation 7\nFor Generation 7, I found that a simple model just using 2 features, egg_group_1 and experience_growth, gave the best accuracy, of ~59%. Any additional features only reduced this accuracy."},{"metadata":{"_uuid":"23d048c987030856cc545752a8ff5757a937fe4c","collapsed":true,"_kg_hide-input":true,"_cell_guid":"10dafb60-1906-4902-beb9-5a9ad26f5c28","trusted":false},"cell_type":"code","source":"#Generation 7 model\n#58.75 best so far\nuse_feat=list(['egg_group_1','experience_growth'])\nsns.set(font_scale=0.8)\nmodel_xgb.fit(Gens_not7_features[use_feat], Gens_not7_targets)\nGen7_T1_pred = model_xgb.predict(Gen7_features[use_feat])\n\n# evaluate predictions\ntest_accuracy = accuracy_score(Gen7_targets, Gen7_T1_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen7_targets))\ncm = metrics.confusion_matrix(Gen7_targets, Gen7_T1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecca4dd2d93d1136f12a0a20d36406f62b9530cb","_cell_guid":"180e9b9d-5cbb-41bb-b35e-506dc93b7453"},"cell_type":"markdown","source":"Since this is a fairly barebones model, I would expect that it makes a fairly narrow range of predictions, and sacrifices accuracy on certain types, to improve it on others. The confusion matrix shows that this is mostly true.\n\n7 whole types are excluded from the model, with no predictions made for any of Dark, Ground, Fairy, Electric, Fighting, Poison or Steel. I assume that Egg group and Experience growth are simply too narrows to identify any of these. Instead, the model has mostly focused on 3 or 4 types, Normal, Psychic, Water and Rock. I'm not surprised that Normal and Water are picked out like this, since Water for example has very obvious egg groups. How the model got to Psychic and Rock seems less clear, but I think lots of Psychic Pokemon tend to fall into the 'Humanoid' egg group. \n\nPossibly due to the rarity of the type, the model gets 100% accuracy for Dragon, along with good accuracy for Normal, Water and Pyschic. The latter ones are to be expected, given the over-abundance of predictions for those types. At least some of them are bound to be correct!\n\nFor all the remaining types, the narrow model means it never made incorrect assignments of types like Grass, Fire and Bug, even if it wasn't always able to correctly identify all examples of that type in the first place. "},{"metadata":{"_uuid":"2d089b924080245a3d53d647614208342632cc2b","_cell_guid":"05695be6-eb5e-40df-8dbf-249355a8aeb6"},"cell_type":"markdown","source":"# Type 1 Summary\n\nWith models developed for all 7 generations, are there any general trends amongst the results? Are certain types harder to predict than others? Are certain Generations more complicated than others? Do certain features appear a lot in the models, or barely at all?\n\nIf we just rank the Generations based on overall accuracy, it's clear that earlier Generations are easier to predict, with the best result for Generation 2 (74%). Everything before Generation 5 manages over 60% accuracy (1: 70.20%, 2: 74%, 3: 63%, 4 67.29%), whilst those after fall below (5: 56.41 %, 6: 52.78 %, 7: 58.75%), with Generation 6 being the worst. \n\nThe good performance on Generations 1 & 2 is likely because the series was just getting started, and some of the designs were slightly less outlandish from now. They also established general ideas for future generations (starters, early normal / flying Pokemon, Legendaries), which get repeated throughout. As mentioned earlier, it is likely that Generations 5 and onwards are harder to predict, due to the soft-reboot of Generation 5, and the changes to the design team (new members etc).\n\n4 types stand out as generally having high accuracy across all the models, although they do sometimes dip in certain Generations. These are Normal, Water, Bug and Grass. For the latter 3, this is likely because they all have their own Egg groups (or several for Water), which would help a lot with the classifications. I assume Normal is a baseline type, which appears fairly average compared to everything else.\n\nA few models do well on other types, like Ghost, Dragon, Fire and Electric, although other times the former two are missed completely, or the latter two are mistaken for each other. Dragon and Ghost are rare enough that sometimes only single families exist per Generation, so it's likely the models would either get everything right, or everything wrong. I'm still not sure why Fire and Electric get confused for each other so often.\n\nIt's a bit more difficult to say which types are hard to predict, because they usually have a mixed bag across generations. Poison seems to stand out, with 0% accuracy for many generations. I assume this is due to the relative rarity of Type 1 poison Pokemon, with it usually appearing instead as Type 2. Ice also seems particularly difficult, often misclassified as Water.\n\nThe Ice / Water problem, and others like Rock / Steel are understandable given their similarities. Many Pokemon are mixed Water / Ice, and likely share similar traits. Rock and Steel both tend to be heavy, with strong defenses, with rock sometimes becoming Steel as the Pokemon evolves. As mentioned several times earlier, there are also cases of complete opposite misclassifications, like Water / Fire or Fire / Ice, so I'd be interested to dig into why those happened, because I'd have thought they were far enough separated as to not happen.\n\nEgg groups seems to be the most powerful predictors for type, with Egg group 1 appearing in all the models, and Egg group 2 in most of them. As mentioned several times, this is not surprising, because several types basically have Egg groups assigned to them (Water, Bug, Grass) . Problems can arise when Pokemon the Pokemon has this as their second Type, or when they belong to the Egg group, but don't have the type at all. An example of the latter would be Inkay, who is Dark / Psychic, but belongs to 2 Water groups.\n\nAbilities are also fairly common features, with Ability 3, the  Hidden ability, appearing a surprising amount. This is once again, like due to the fact that some types have exclusive abilities, which would be an even stronger effect for the rare hidden abilities. Confusion would come from others, like Pressure, which is fairly generic across types.\n\nColour and Shape also appear a lot, which intuitively make sense. Fire Pokemon tend to be Red, Water tend to be Blue, and Flying Pokemon tend to be Bird shaped.\n\nBefore doing this, I'd expected the actual main stats, HP, Attack, Special Attack, Defense, Special Defense and Speed to be much more useful at making predictions than they actually were. Several of them appear across the models, but in only about half of them.\n\nOther less intuitive features like base happiness, capture rates, experience growth, or egg steps appear in a few models, which surprised me, because I didn't know too much about the range of some of them before this project. These can often be indicators of rare types and Pokemon, whose values for these features appear outside the norm.\n\nFeatures related to Gender and Legendary status are relatively rare. This is likely because Legendaries span most types now, and that when there are gender ratio differences, it's hard to confine them to certain types.\n\nPhysical attributes like height and weight rarely appear in the models, likely because these are probably afterthoughts to the design process."},{"metadata":{"_uuid":"643dd0191db51f0086163cc2dc679e123d984906","_cell_guid":"a457f584-60de-42bf-bd84-09ee302087ab"},"cell_type":"markdown","source":"# Predictions of Type 2\n\nWith my preliminary analysis for Type 1 now complete, I turned my attention to Type 2, and repeated the process. Before doing any work, I expected that Type 2 was slightly harder to predict, because some secondary types can be slightly more esoteric, and I had to introduce a new 'None' category. It also turned out that the None category introduces some serious imbalance issues to the dataset, and drastically shift the baseline predictions."},{"metadata":{"_uuid":"69fea8f7d3bf12f728276750f21435c806eba5a9","_cell_guid":"9c10bd7c-e3ed-4fd6-9eb8-e8b2cca558b8"},"cell_type":"markdown","source":"# Full Pokedex"},{"metadata":{"_uuid":"99b7b21dc22d5bb4f1fc686fdfe386308403db6d","collapsed":true,"_kg_hide-input":true,"_cell_guid":"836b9d9e-c05d-4259-a06f-741e593d037b","trusted":false},"cell_type":"code","source":"use_feat=list(['speed', 'weight_kg','Ability1','Ability3']) \n#-> 100%\n#use_feat = list(['Ability1','weight_kg','defense']) \n#-> 99.88%\n\n#use_feat = list(['weight_kg','base_total','defense']) \n#-> 99.88%\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"644524f54f39afd409c8154e2c4580daa525bad2","collapsed":true,"_kg_hide-input":true,"_cell_guid":"18a83a34-07e9-4058-872a-53a08d560a42","trusted":false},"cell_type":"code","source":"sns.set(font_scale=0.8)\nplt.figure(figsize=(40,40))\nmodel_xgb.fit(features[use_feat], targets2)\ny_pred = model_xgb.predict(features[use_feat])\n# evaluate predictions\naccuracy = accuracy_score(targets2, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nxgb.plot_importance(model_xgb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"225bfd87931030f0d00139e26ea70aa445dc08e0","_cell_guid":"4d2ab9db-9832-4d3a-afd2-69d587e52ffb"},"cell_type":"markdown","source":"Fitting to the full Pokedex was relatively easy, with 100% accuracy possible with just 4 features. The 3 feature models I developed for Type 1 only perform marginally worse, at 99.88% accuracy, meaning only 1 Pokemon is misclassified. At first glance, this suggests that Type 2 is marginally more difficult to predict than Type 1."},{"metadata":{"_uuid":"673ccdb42d8cfdbdc18647637cb3350a5415f5f9","_cell_guid":"d447189c-ff3c-49bf-8032-53c03e8add7f"},"cell_type":"markdown","source":"# Predicting Across Generations"},{"metadata":{"_uuid":"f9f9f226c12092cff4d36a8d9dcb0a70d3fbc44d","collapsed":true,"_kg_hide-input":true,"_cell_guid":"1e7cc681-195e-49be-8feb-7b568b93704e","trusted":false},"cell_type":"code","source":"Gen1_targets2=targets2[0:151]\nGen2_targets2=targets2[151:251]\nGen3_targets2=targets2[251:386]\nGen4_targets2=targets2[386:493]\nGen5_targets2=targets2[493:649]\nGen6_targets2=targets2[649:721]\nGen7_targets2=targets2[721:801]\nGen1_targets2=np.ravel(Gen1_targets2)\nGen2_targets2=np.ravel(Gen2_targets2)\nGen3_targets2=np.ravel(Gen3_targets2)\nGen4_targets2=np.ravel(Gen4_targets2)\nGen5_targets2=np.ravel(Gen5_targets2)\nGen6_targets2=np.ravel(Gen6_targets2)\nGen7_targets2=np.ravel(Gen7_targets2)\nGens_not1_targets2=np.concatenate((Gen2_targets2,Gen3_targets2,Gen4_targets2,Gen5_targets2,Gen6_targets2,Gen7_targets2),axis=0)\nGens_not2_targets2=np.concatenate((Gen1_targets2,Gen3_targets2,Gen4_targets2,Gen5_targets2,Gen6_targets2,Gen7_targets2),axis=0)\nGens_not3_targets2=np.concatenate((Gen2_targets2,Gen1_targets2,Gen4_targets2,Gen5_targets2,Gen6_targets2,Gen7_targets2),axis=0)\nGens_not4_targets2=np.concatenate((Gen2_targets2,Gen3_targets2,Gen1_targets2,Gen5_targets2,Gen6_targets2,Gen7_targets2),axis=0)\nGens_not5_targets2=np.concatenate((Gen2_targets2,Gen3_targets2,Gen4_targets2,Gen1_targets2,Gen6_targets2,Gen7_targets2),axis=0)\nGens_not6_targets2=np.concatenate((Gen2_targets2,Gen3_targets2,Gen4_targets2,Gen5_targets2,Gen1_targets2,Gen7_targets2),axis=0)\nGens_not7_targets2=np.concatenate((Gen2_targets2,Gen3_targets2,Gen4_targets2,Gen5_targets2,Gen6_targets2,Gen1_targets2),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50c540d8a0344991979a18f7d20066f52bfbeb67","_cell_guid":"f68f2544-e4ba-49d7-931f-3d3d42b8adc0"},"cell_type":"markdown","source":"When I first started looking at Type 2, I hadn't realised quite how many Pokemon were lacking a second type. This leads to a fairly imbalanced type distribution compared to Type 1, and means that I need to use a different baseline to compare prediction against. Namely, what would the accuracy be if I simply guessed 'none' for every single Pokemon?\n\nIn the full Pokedex, none appears nearly 400 times as a 2nd type, flying nearly 100 times, then the rest usually about 30 or less, with Normal being the rarest. That's quite a serious imbalance for none and flying! One possible solution to this problem is to add weightings to the model, but I did not find this particularly helped. Tests with Generation 2 did not improve the overall accuracy, but did take less features to reach that accuracy. As such, I have modelled without weights for now."},{"metadata":{"_uuid":"8aeb1031267371fdcf84d41394d0c1c56c80377a","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"4d47913f-e40a-4321-8843-aa37c69b715f","trusted":false,"collapsed":true},"cell_type":"code","source":"#Count the number of times each type appears\nfrom collections import Counter\nCounter(targets2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f6dc91eb240d2492967113b33a516ce6b6a3928","collapsed":true,"_kg_hide-input":true,"_cell_guid":"1a97ab78-9662-4a31-9810-06c751e019d2","trusted":false},"cell_type":"code","source":"print (\"Full Pokedex none percentage\")\nprint(Counter(targets2)['none']/len(targets2)*100 )\nprint (\"Generation 1 none percentage\")\nprint(Counter(Gen1_targets2)['none']/len(Gen1_targets2)*100)\nprint (\"Generation 2 none percentage\")\nprint(Counter(Gen2_targets2)['none']/len(Gen2_targets2)*100)\nprint (\"Generation 3 none percentage\")\nprint(Counter(Gen3_targets2)['none']/len(Gen3_targets2)*100)\nprint (\"Generation 4 none percentage\")\nprint(Counter(Gen4_targets2)['none']/len(Gen4_targets2)*100)\nprint (\"Generation 5 none percentage\")\nprint(Counter(Gen5_targets2)['none']/len(Gen5_targets2)*100)\nprint (\"Generation 6 none percentage\")\nprint(Counter(Gen6_targets2)['none']/len(Gen6_targets2)*100)\nprint (\"Generation 7 none percentage\")\nprint(Counter(Gen7_targets2)['none']/len(Gen7_targets2)*100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f4ae7ef29c55817edea87c334db2b2ca82f81f1","_cell_guid":"4b71a2a6-ae21-4705-a0c6-e5b38c31070b"},"cell_type":"markdown","source":"Most generations have nearly a 50% none rate, with the later ones dropping down slightly, to ~36% by Generation 7. This means that in order for my model to make any meaningful improvements, it really has to beat these baselines, otherwise a model that simply guessed 'None' every time would beat it. \n\nAn alternative might be to look at how well the model does on predicting the types which do exist, since this is likely a separate problem from predicting the nones. In fact, this turned out to be a good method for feature selection, because improving the type accuracy tended to improve the full model accuracy in the long run, even with a few dips to the none accuracy here and there.\n\nTo start with, I simply made predictions using all the features, which managed accuracies of 55.63 % / 60.00 % / 47.41 % / 52.34 % / 55.77 % / 34.72 % / 43.75 % for Generations 1-7. Most made no meaningful improvements over the 'all none' model, or actually made a worse model! At the very least, we need to have models better than just guessing None all the time.\n\nInitial testing showed that it's relatively easy to get correct predictions for Flying Pokemon, by using for example shape_id, since most Flying Pokemon have it as their secondary type. Adding more features often doesn't improve the overall accuracy though. This was because new correct type guesses were often counter-balanced with incorrect guesses which should have been none.\n\nAs before, I tried to find the best feature selections, but the process is non-exhaustive, so I may have missed some minor improvements. Additionally, just for time reasons, I limited the extent of some of my searches."},{"metadata":{"_uuid":"6b0e0559ef4622351b5fd2ea6503773007298c6b","_kg_hide-input":true,"_cell_guid":"94e822fb-b894-47ae-add1-06bb0c0ac461","trusted":false,"collapsed":true},"cell_type":"code","source":"#Getting weights for generations other than 2.\nweights = np.zeros(len(Gens_not2_targets2))\nfor i in range(len(Gens_not2_targets2)):\n    weights[i]=Counter(Gens_not2_targets2)['none']/Counter(Gens_not2_targets2)[Gens_not2_targets2[i]]\n#weights","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-output":true,"_uuid":"7379912d08c9018f6421b11ce0bed882ae90c4c4","collapsed":true,"_kg_hide-input":true,"_cell_guid":"6b887ab6-6014-47b9-ba6e-95ff9fdd809b","trusted":false},"cell_type":"code","source":"#none_list=[i for i, j in enumerate(Gen7_targets2) if j == 'none']\n#type_list=[i for i, j in enumerate(Gen7_targets2) if j != 'none']\n#Iterate forwards\n#use_feat=list(['attack', 'base_egg_steps', 'base_happiness', 'base_total',\n#       'capture_rate', 'defense', 'experience_growth', 'height_m', 'hp',\n#       'percentage_male', 'sp_attack', 'sp_defense', 'speed', 'weight_kg',\n#       'is_legendary', 'egg_group_1', 'egg_group_2', 'color_id', 'shape_id',\n#      'habitat_id', 'Ability1', 'Ability2', 'Ability3', 'Genderless'])\n#use_feat=list(['attack',  'base_total',\n#        'defense', 'experience_growth', 'height_m', 'hp',\n#        'sp_attack', 'sp_defense', 'speed',\n#        'color_id',\n#     'Ability1'])\n#for i in use_feat:\n#    feats=['weight_kg','capture_rate','base_happiness', 'Genderless','base_egg_steps','Ability3', 'shape_id','egg_group_2', 'egg_group_1',\n#           'habitat_id','percentage_male', 'Ability2' ,'is_legendary']\n#    feats.insert(13,i)\n#    print(\"Adding\")\n#    print(i)\n#    model_xgb.fit(Gens_not7_features[feats], Gens_not7_targets2)\n#    test_pred = model_xgb.predict(Gen7_features[feats])\n#    test_accuracy = accuracy_score(Gen7_targets2, test_pred)\n#    print(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\n#    test_type_accuracy = accuracy_score(Gen7_targets2[type_list], test_pred[type_list])\n#    print(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\n#    test_none_accuracy = accuracy_score(Gen7_targets2[none_list], test_pred[none_list])\n#    print(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b797cd768c66aa2fc7297d19d01baafc849f659","collapsed":true,"_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"050a0029-68f1-4f3c-ae84-8cc2ccb46d6f","trusted":false},"cell_type":"code","source":"#Iterate backwards\n#use_feat=list(['attack', 'base_egg_steps', 'base_happiness', 'base_total',\n#       'capture_rate', 'defense', 'experience_growth', 'height_m', 'hp',\n#       'percentage_male', 'sp_attack', 'sp_defense', 'speed', 'weight_kg',\n#       'is_legendary', 'egg_group_1', 'egg_group_2', 'color_id', 'shape_id',\n#      'habitat_id', 'Ability1', 'Ability2', 'Ability3', 'Genderless'])\n#\n#use_feat=list(['shape_id','sp_attack','habitat_id', 'egg_group_2'])\n#\n#for i in use_feat:\n#    feats=use_feat.copy()\n#    print (\"Removing\")\n#    print(i)\n#    feats.remove(i)\n#    model_xgb.fit(Gens_not6_features[feats], Gens_not6_targets2)\n#    test_pred = model_xgb.predict(Gen6_features[feats])\n#    test_accuracy = accuracy_score(Gen6_targets2, test_pred)\n#    print(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\n#    test_type_accuracy = accuracy_score(Gen6_targets2[type_list], test_pred[type_list])\n#    print(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\n#    test_none_accuracy = accuracy_score(Gen6_targets2[none_list], test_pred[none_list])\n#    print(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32c85f2db07e92c1dd4e1a005435b1dfd219b855","_cell_guid":"bba22125-8219-4ccb-a698-2da4541e7604"},"cell_type":"markdown","source":"# Generation 1\n\nI was able to reach ~69% total accuracy, and ~43% accuracy on Pokemon with a 2nd type, using 9 features:\n\nAbility3, shape_id, egg_group_2, Ability1, is_legendary, Ability2, experience_growth, base_egg_steps, and percentage_male\n\nIt appears that features related to Abilities, and breeding in some form are important."},{"metadata":{"_uuid":"5a2412beacc18e3b1f9d8e95baab09e2b9d7dd78","_kg_hide-output":false,"_kg_hide-input":true,"_cell_guid":"2a4de849-314a-4283-8331-60d780dc8ebd","trusted":false,"collapsed":true},"cell_type":"code","source":"none_list=[i for i, j in enumerate(Gen1_targets2) if j == 'none']\ntype_list=[i for i, j in enumerate(Gen1_targets2) if j != 'none']\nsns.set(font_scale=0.8)\n#43.28/68.87\nuse_feat=list(['Ability3', 'shape_id','egg_group_2','Ability1','is_legendary','Ability2', 'experience_growth', 'base_egg_steps','percentage_male'])\n#41.79 / 68.21 (save for later)\n#use_feat=list(['egg_group_2','Ability3', 'shape_id','Ability1','Ability2', 'percentage_male','experience_growth'])\nmodel_xgb.fit(Gens_not1_features[use_feat], Gens_not1_targets2)\ntrain_pred=model_xgb.predict(Gens_not1_features[use_feat])\nGen1_T2_pred = model_xgb.predict(Gen1_features[use_feat])\n# evaluate predictions\ntrain_accuracy = accuracy_score(Gens_not1_targets2, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(Gen1_targets2, Gen1_T2_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\ntest_type_accuracy = accuracy_score(Gen1_targets2[type_list], Gen1_T2_pred[type_list])\nprint(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\ntest_none_accuracy = accuracy_score(Gen1_targets2[none_list], Gen1_T2_pred[none_list])\nprint(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen1_targets2))\ncm = metrics.confusion_matrix(Gen1_targets2, Gen1_T2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26063ad70e28a451c9a8c9f6b4382c9af51cbf12","_cell_guid":"b3748cce-b0dc-4262-98f3-6454cbe96ceb"},"cell_type":"markdown","source":"As expected, the current model vastly overestimates the number of Pokemon which lack a 2nd type, assigning none for many types which should exist. Notable examples are Poison and Fighting, which are practically never identified as any type. In a few cases, it incorrectly assigns other types where there should be none, such as steel, grass or fighting.\n\nSteel, Rock and Fairy all have 100% accuracy, with reasonable performance for Flying, middling for Psychic, and bad for Ground.\n\nSome types, like Ice, Grass, Fighting, Water, and Poison are never correctly predicted by the model.\n\n2 misclassifications particularly stand out to me, which I think are explained by their type 1 pairings. Water is always misclassified as Rock, which might have something to do with the fact the Fossil Pokemon are all Rock / Water, so the algorithm is getting the order wrong. A closer look at the predictions shows that this is the case. Since Poison is often paired with Grass as the 2nd type, it doesn't surprise me that grass is always misclassified as poison here.\n\nSomething interesting to note when looking closer at the predictions is that the Charmander family is predicted to all be part Fighting, likely due to the abundance of Fire / Fighting starters."},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"54ed7db6d3c07b433de7697207080415bf71e902"},"cell_type":"code","source":"Simpler_XGB_predictions_df=pd.DataFrame()\nSimpler_XGB_predictions_df[\"Type1\"]=0\nSimpler_XGB_predictions_df[\"Type1\"]=Gen1_T1_pred\nSimpler_XGB_predictions_df[\"Type2\"]=0\nSimpler_XGB_predictions_df[\"Type2\"]=Gen1_T2_pred\nSimpler_XGB_predictions_df.to_csv(\"Simpler_XGB_Predictions.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98a4a9b87660f448b211f23a1fa4b4d459cd0c65","_cell_guid":"1f6726a4-fb37-401c-9273-0616fef00f84"},"cell_type":"markdown","source":"# Generation 2\nI found that a 9 feature model was able to get 75% overall accuracy, and 53% accuracy on those Pokemon with 2nd types. These features included:\n\nshape_id, Ability1, Ability2, experience_growth, Ability3, percentage_male, egg_group_2, base_happiness and color_id.\n\nInterestingly, 7 of these are shared with the Generation 1 features, suggesting these models are slightly more transferable than those for Type 1.\n\nI found that by adding weightings to the model, I was able to get the same accuracy for only 7 features, dropping base_happiness and egg_group 2. However, I did not find models that improved the accuracy yet."},{"metadata":{"_uuid":"9113657bda8bc20819dc7794f0df681a57e7515c","collapsed":true,"_kg_hide-input":true,"_cell_guid":"2a1249f5-d3f0-413c-bb3c-92d2e62c566b","trusted":false},"cell_type":"code","source":"none_list=[i for i, j in enumerate(Gen2_targets2) if j == 'none']\ntype_list=[i for i, j in enumerate(Gen2_targets2) if j != 'none']\nsns.set(font_scale=0.8)\nuse_feat=list(['shape_id','Ability1','Ability2','experience_growth','Ability3','percentage_male','egg_group_2','base_happiness','color_id'])\n#75% total accuracy, with 53% on types\n\n# Gen 2 with weights\n#feats=['shape_id','Ability1','Ability2','Ability3','percentage_male','color_id','experience_growth']\n#Also 75%\nmodel_xgb.fit(Gens_not2_features[use_feat], Gens_not2_targets2)\ntrain_pred=model_xgb.predict(Gens_not2_features[use_feat])\nGen2_T2_pred = model_xgb.predict(Gen2_features[use_feat])\n# evaluate predictions\ntrain_accuracy = accuracy_score(Gens_not2_targets2, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(Gen2_targets2, Gen2_T2_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\ntest_type_accuracy = accuracy_score(Gen2_targets2[type_list], Gen2_T2_pred[type_list])\nprint(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\ntest_none_accuracy = accuracy_score(Gen2_targets2[none_list], Gen2_T2_pred[none_list])\nprint(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen2_targets2))\ncm = metrics.confusion_matrix(Gen2_targets2, Gen2_T2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9467db98e369fe394f2e1a5df1b67fcb2f4127a","_cell_guid":"29686899-86d5-4733-a841-879efbac4457"},"cell_type":"markdown","source":"As before, None is massively over-predicted, with many types purely predicted as None. This includes Poison, Fire, Fighting, Dragon, Dark, and Electric. However, when predictions are made for Pokemon with 2nd types, they are actually fairly accurate.\n\nFairy, Ice and Grass are predicted with 100% accuracy, with good performance for flying, and reasonable for Psychic and Ground. In most cases, the incorrect predictions for these types are simply due to being assigned None. This just leaves Rock and Steel, which get misclassified as Flying for some reason. As far as I can tell, it thinks Scizor and Shuckle have their 2nd type as Flying, possibly due to this being a common pairing with Bug type.\n\nThankfully, this time the model doesn't think Cyndaquil and co are part fighting!"},{"metadata":{"_uuid":"b7f5327e72574460f6f0d1302ee85cd3398f66ff","_cell_guid":"fc7a421e-d43e-4536-8fd6-7cfc2615479e"},"cell_type":"markdown","source":"# Generation 3\n\nThe best model I've found so far included just 3 features, and got an overall accuracy of ~59%, with ~29% Type accuracy. More complicated models, with up to 12 features were only able to reproduce these values. I did not look any further. The 3 features are:\n\nexperience_growth, shape_id, and Ability2.\n\nThese are all common to the previous 2 models.\n\nSince this is a very simple model, I expect it will do well on 2 or 3 types, and basically ignore the rest."},{"metadata":{"_uuid":"804231fbaa33648dad83d90d215519b2db9b3d39","collapsed":true,"_kg_hide-input":true,"_cell_guid":"afcf4025-facf-4a20-b2c3-c947a51e8b63","trusted":false},"cell_type":"code","source":"none_list=[i for i, j in enumerate(Gen3_targets2) if j == 'none']\ntype_list=[i for i, j in enumerate(Gen3_targets2) if j != 'none']\nsns.set(font_scale=0.8)\n#58.52/28.79\nuse_feat=list(['experience_growth','shape_id','Ability2'])\nmodel_xgb.fit(Gens_not3_features[use_feat], Gens_not3_targets2)\ntrain_pred=model_xgb.predict(Gens_not3_features[use_feat])\nGen3_T2_pred = model_xgb.predict(Gen3_features[use_feat])\n# evaluate predictions\ntrain_accuracy = accuracy_score(Gens_not3_targets2, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(Gen3_targets2, Gen3_T2_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\ntest_type_accuracy = accuracy_score(Gen3_targets2[type_list], Gen3_T2_pred[type_list])\nprint(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\ntest_none_accuracy = accuracy_score(Gen3_targets2[none_list], Gen3_T2_pred[none_list])\nprint(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen3_targets2))\ncm = metrics.confusion_matrix(Gen3_targets2, Gen3_T2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f86a35148f6dba7d9e5229bfba15c05f6b668f05","_cell_guid":"a2731c25-d734-4dab-87fb-44dccb7727a3"},"cell_type":"markdown","source":"As expected, this model ignores most of the types, with 8 types completely empty, and vastly overpredicts a lack of a second type. However, when it does make predictions about other types, they tend to be good.\n\nFairy and Rock have a 100% accuracy, whilst Flying, Bug and Poison are decent.\n\nThe model also predicts too many Flying types, incorrectly assigning types like Dragon and poison.\n\nThis is generally a pretty poor model, but I couldn't find anything better."},{"metadata":{"_uuid":"801446550c2c4a1ab4dd4564d74cd900fefed4c0","_cell_guid":"5445cc06-e49a-4025-82d4-6aca6a942678"},"cell_type":"markdown","source":"# Generation 4\n\nMy best model so far for Generation 4 used 11 features, the majority of which are common to other models.\n\ndefense, Ability1, Ability2, is_legendary, Genderless, base_happiness, percentage_male, shape_id,  base_egg_steps, Ability3 and egg_group_1.\n\nThis has an overall accuracy of ~64% and a Type accuracy of ~37%."},{"metadata":{"_uuid":"01d99e3114c62d15a40f413a8d3d41e23033d748","collapsed":true,"_kg_hide-input":true,"_cell_guid":"ed1daaf3-5ab8-4f7e-ac48-8705c72e9de0","trusted":false},"cell_type":"code","source":"none_list=[i for i, j in enumerate(Gen4_targets2) if j == 'none']\ntype_list=[i for i, j in enumerate(Gen4_targets2) if j != 'none']\nsns.set(font_scale=0.8)\n# Full 63.55 / Type 37.04\nuse_feat=list(['defense','Ability1','Ability2','is_legendary', 'Genderless','base_happiness','percentage_male',\n           'shape_id', 'base_egg_steps','Ability3','egg_group_1'])\n\nmodel_xgb.fit(Gens_not4_features[use_feat], Gens_not4_targets2)\ntrain_pred=model_xgb.predict(Gens_not4_features[use_feat])\nGen4_T2_pred = model_xgb.predict(Gen4_features[use_feat])\n# evaluate predictions\ntrain_accuracy = accuracy_score(Gens_not4_targets2, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(Gen4_targets2, Gen4_T2_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\ntest_type_accuracy = accuracy_score(Gen4_targets2[type_list], Gen4_T2_pred[type_list])\nprint(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\ntest_none_accuracy = accuracy_score(Gen4_targets2[none_list], Gen4_T2_pred[none_list])\nprint(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen4_targets2))\ncm = metrics.confusion_matrix(Gen4_targets2, Gen4_T2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3faded678fa2fe2656312833c6d95f9802ed92c3","_cell_guid":"eaf55f61-8240-47fb-9202-616043af5ad4"},"cell_type":"markdown","source":"This model performs well on a few types, but leaves most of the types blank, with many wrongly classified as None. In nearly all cases that it predicts a type, it gets the prediction correct, with the one exception of misclassifying dark as flying. As with the other models, the main problem is that it simply defaults to None in too many cases.\n\nPoison, Rock, Fairy, and Psychic all perform well, with 100% accuracy for each. Flying is also predicted correctly with reasonable accuracy. Steel, Ground and Grass are less successful, but as mentioned above, this is due to the fact that the model can't gain enough information to assign any label other than None to most of the Pokemon.\n\nThe bug entry can be ignored, because I think there's a divide by zero issue going on in the code somewhere."},{"metadata":{"_uuid":"3923dddbf9b57bb4fc26db88beebd22df69a0f9e","_cell_guid":"27d8e2f0-9ef1-4cbc-b2c4-8a303c929c71"},"cell_type":"markdown","source":"# Generation 5\n\nThe best model I found for Generation 5 included 14 features, and got an overall accuracy of 61.54%, and a type accuracy of 30.67%. The features included:\n\nsp_attack, capture_rate, percentage_male, egg_group_1, is_legendary, shape_id, Ability1, base_happiness, hp, egg_group_2, sp_defense, Ability3, speed, and base_total\n\n\nIt's also notable that 60.26% accuracy could be achieved with only a 2 feature model (shape_id and experience_growth), and 29.33 % type accuracy with a 3 feature model (shape_id, Ability1, hp), so it's questionable that the small gains are really worth the huge added complexity."},{"metadata":{"_uuid":"9ad999b8fe947f925aaca567e410363be1c94524","collapsed":true,"_kg_hide-input":true,"_cell_guid":"e96d4462-0f92-45fe-97ea-4286f5dad1ee","trusted":false},"cell_type":"code","source":"none_list=[i for i, j in enumerate(Gen5_targets2) if j == 'none']\ntype_list=[i for i, j in enumerate(Gen5_targets2) if j != 'none']\nsns.set(font_scale=0.8)\n#2 feat model\n# 21.33 / 60.26 %\n#use_feat=list(['shape_id','experience_growth'])\n# 3 feat model\n#29.33/58.33\n#use_feat=list(['shape_id','Ability1','hp'])\n#14 feat model\n#30.67/61.54\nuse_feat=list(['sp_attack', 'capture_rate', 'percentage_male','egg_group_1','is_legendary','shape_id', 'Ability1', 'base_happiness','hp','egg_group_2',\n           'sp_defense', 'Ability3', 'speed', 'base_total'])\n\nmodel_xgb.fit(Gens_not5_features[use_feat], Gens_not5_targets2)\ntrain_pred=model_xgb.predict(Gens_not5_features[use_feat])\nGen5_T2_pred = model_xgb.predict(Gen5_features[use_feat])\n# evaluate predictions\ntrain_accuracy = accuracy_score(Gens_not5_targets2, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(Gen5_targets2, Gen5_T2_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\ntest_type_accuracy = accuracy_score(Gen5_targets2[type_list], Gen5_T2_pred[type_list])\nprint(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\ntest_none_accuracy = accuracy_score(Gen5_targets2[none_list], Gen5_T2_pred[none_list])\nprint(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen5_targets2))\ncm = metrics.confusion_matrix(Gen5_targets2,Gen5_T2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4645a76c530811689a36611c888404ed1f2f60e7","_cell_guid":"ad22dff5-381d-4d9d-bb65-1026f0e63d5c"},"cell_type":"markdown","source":"As always the None predictions dominate this model, with huge numbers of types misidentified as None.\n\nPoison, Flying and Pyschic all have good accuracy, with Psychic the only 100% in this set. It's also worth noting that Poison and Flying are over-predicted in this model, with several other Pokemon wrongly assigned these types.\n\nSeveral other types, such as Rock, Electric, Fighting and Steel have a few correct guesses. The rest are either guessed wrong, or simply assigned None. A notable mistake in this model is all the Ground Pokemon predicted as Grass."},{"metadata":{"_uuid":"77545bbf623d11048e8dc87287bc4aeca2e4c397","_cell_guid":"6a433618-b05f-425c-9824-2b780aea64ab"},"cell_type":"markdown","source":"# Generation 6\n\nThe best model I was able to find achieved an overall accuracy of 51.39 %, and a Type accuracy of 21.95 %, using 10 features:\n\nexperience_growth, base_happiness, Ability2, egg_group_1, shape_id, sp_attack, habitat_id, egg_group_2, is_legendary, and height_m.\n\nIt's clear that this is still a fairly bad model, since it barely makes over 50% accuracy, and those related to actual types can't even get it right 1/4th of the time.\n\nIt' s also questionable if this complexity is worth it, because a model with just the shape_id was able to get a 50% overall accuracy."},{"metadata":{"_uuid":"198f3d1e36f556392678faf705fbaa87d154f21d","collapsed":true,"_kg_hide-input":true,"_cell_guid":"73af1657-ccab-42b0-98d1-eb4c26921c2a","trusted":false},"cell_type":"code","source":"none_list=[i for i, j in enumerate(Gen6_targets2) if j == 'none']\ntype_list=[i for i, j in enumerate(Gen6_targets2) if j != 'none']\nsns.set(font_scale=0.8)\n# Best\n#21.95/51.39\nuse_feat=list(['experience_growth','base_happiness','Ability2', 'egg_group_1','shape_id','sp_attack','habitat_id', 'egg_group_2','is_legendary'\n          , 'height_m'])\nmodel_xgb.fit(Gens_not6_features[use_feat], Gens_not6_targets2)\ntrain_pred=model_xgb.predict(Gens_not6_features[use_feat])\nGen6_T2_pred = model_xgb.predict(Gen6_features[use_feat])\n# evaluate predictions\ntrain_accuracy = accuracy_score(Gens_not6_targets2, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(Gen6_targets2, Gen6_T2_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\ntest_type_accuracy = accuracy_score(Gen6_targets2[type_list], Gen6_T2_pred[type_list])\nprint(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\ntest_none_accuracy = accuracy_score(Gen6_targets2[none_list], Gen6_T2_pred[none_list])\nprint(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen6_targets2))\ncm = metrics.confusion_matrix(Gen6_targets2, Gen6_T2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d7266b3d4e276e4e8541e9483e76fbc4436900b","_cell_guid":"61025cb1-9181-4ffd-9f37-ac5cb9652a6d"},"cell_type":"markdown","source":"As always this model has too many predictions of None, and too few predictions for most other types. It appears to perform well with relatively rare second types, like Fairy, Dragon and Ghost, in addition to 100% accuracy for Fighting. As in previous models, it can get predictions of these types correct, but still assigns some of them to None.\n\nMost other types lack any predictions at all.\n\nThere also seems to be some confusion over Dragon and Flying types, with both making incorrect predictions of the other type."},{"metadata":{"_uuid":"a0cdb506da731a232c43d28d9dcdff9dca67db60","_cell_guid":"edc5ef2d-b69e-4ebf-bb32-54c6269636b7"},"cell_type":"markdown","source":"# Generation 7\n\nThe best model I found only achieved a 50% overall accuracy, and 29.41 % Type accuracy, using 6 features:\n\nbase_egg_steps, Ability3, shape_id, egg_group_2, egg_group_1,and habitat_id.\n\nMore complicated models just tended to get a lower accuracy for both measure, up to 14 features, where I stopped looking."},{"metadata":{"_uuid":"f0759d3ff2752edd51000b943d76faede6d6954d","collapsed":true,"_kg_hide-input":true,"_cell_guid":"b9185ed8-7d02-49f1-b608-856332302955","trusted":false},"cell_type":"code","source":"none_list=[i for i, j in enumerate(Gen7_targets2) if j == 'none']\ntype_list=[i for i, j in enumerate(Gen7_targets2) if j != 'none']\nsns.set(font_scale=0.8)\n#50%/29.41 %\nuse_feat=list(['base_egg_steps','Ability3', 'shape_id','egg_group_2', 'egg_group_1','habitat_id'])\n\nmodel_xgb.fit(Gens_not7_features[use_feat], Gens_not7_targets2)\ntrain_pred=model_xgb.predict(Gens_not7_features[use_feat])\nGen7_T2_pred = model_xgb.predict(Gen7_features[use_feat])\n# evaluate predictions\ntrain_accuracy = accuracy_score(Gens_not7_targets2, train_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\ntest_accuracy = accuracy_score(Gen7_targets2, Gen7_T2_pred)\nprint(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100.0))\ntest_type_accuracy = accuracy_score(Gen7_targets2[type_list], Gen7_T2_pred[type_list])\nprint(\"Test Type Accuracy: %.2f%%\" % (test_type_accuracy * 100.0))\ntest_none_accuracy = accuracy_score(Gen7_targets2[none_list], Gen7_T2_pred[none_list])\nprint(\"Test None Accuracy: %.2f%%\" % (test_none_accuracy * 100.0))\nxgb.plot_importance(model_xgb)\n# Output a plot of the confusion matrix.\nlabels =list(set(Gen7_targets2))\ncm = metrics.confusion_matrix(Gen7_targets2, Gen7_T2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e88c8cfa30b896b4776a2c13e73bba5add98af65","_cell_guid":"9eddc3e6-b75d-446a-948a-295911d5d7f1"},"cell_type":"markdown","source":"The model appears to be predicting Ground quite a lot, because it not only gets all the Ground Pokemon correct, but also incorrectly assigns all Fire and Bug to Ground as well.\n\nOutside of this Fairy, Flying and Dragon all fare decently.\n\nEverything else is pretty much empty, or completely wrong. Most notably for Ice and Poison, which are always assigned to other single types.\n\nAlso a few types are incorrectly assigned to Pokemon that don't have them."},{"metadata":{"_uuid":"283ee62bc1bc79e24b56974db4b3bda3dfd7de7e","_cell_guid":"dda4ec63-54d4-4a07-ae83-11ce986436db"},"cell_type":"markdown","source":"# Type 2 Overall\n\nThe overall accuracy of the Type 2 models are not significantly different from those for Type 1, but the large number of None entries makes this a far less impressive achievement. The Generations can be ordered: 2 (75%) > 1 (69%) > 4 (64%) > 5 (60%) > 3 (59%) > 6 (51%) > 7 (50%). It's also notable that Generation 6 has the worst predictions of those Pokemon with types, at 22%\n\nNone aside, most types are badly predicted by these models. Some notable exceptions are Rock and Fairy, which usually show high accuracies, with Pyschic and Flying also performing well in general. Steel and Poison are often correctly predicted, but not as frequently as they appear in that Generation.\n\nMost of the good features also appeared for Type 1, and many are shared between the 7 models. Shape, Abilities, and Egg groups are in nearly all the models, with Experience Growth, percentage male and base happiness also appearing in several. Direct stats, like HP, only appear rarely, and some like weight, not at all."},{"metadata":{"_uuid":"6e7867a942b6423ae6e94ab0c7f19c8650160508","collapsed":true,"_cell_guid":"7c812c03-fcde-424c-83b7-1495c7914920"},"cell_type":"markdown","source":"# Type 1 & Type 2 combined\n\nAs a final example, I combined my predictions for all 7 Generations back together to create a new Machine Learning based Pokedex.\n\nAs well as checking the overall accuracy for each type, I also explored whether a type was guessed correctly, but in the wrong slot."},{"metadata":{"_uuid":"1241a14704e8fa41255620bb74f98a4e7800e8a3","collapsed":true,"_kg_hide-input":true,"_cell_guid":"6600c9ac-cf0b-42a1-b1d9-e16404ffb00b","trusted":false},"cell_type":"code","source":"#Combine Gens 1-7 predictions\nType1_pred=np.concatenate((Gen1_T1_pred,Gen2_T1_pred,Gen3_T1_pred,Gen4_T1_pred,Gen5_T1_pred,Gen6_T1_pred,Gen7_T1_pred),axis=0)\nType2_pred=np.concatenate((Gen1_T2_pred,Gen2_T2_pred,Gen3_T2_pred,Gen4_T2_pred,Gen5_T2_pred,Gen6_T2_pred,Gen7_T2_pred),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc7f860ebc80eb39356946e70417ccbda6f64813","collapsed":true,"_kg_hide-input":true,"_cell_guid":"32e3f853-c1a7-49cb-b885-83d9b7de371a","trusted":false},"cell_type":"code","source":"#Get accuracies:\nprint(\"Overall Accuracy\")\nType1_accuracy = accuracy_score(targets, Type1_pred)\nprint(\"Type 1 Accuracy: %.2f%%\" % (Type1_accuracy * 100.0))\nType2_accuracy = accuracy_score(targets2, Type2_pred)\nprint(\"Type 2 Accuracy: %.2f%%\" % (Type2_accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfff791eb9f4bc5f21cabbbdfce3610250be0c35","collapsed":true,"_kg_hide-input":true,"_cell_guid":"55925c96-3e94-46f6-b8cf-3c62dada27b5","trusted":false},"cell_type":"code","source":"labels =list(set(targets))\ncm = metrics.confusion_matrix(targets, Type1_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Type 1 Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e243699f60ba8244ec6846e6d0d872859fc2764","_cell_guid":"4fa55f8d-c36a-4b94-8c48-929e9ff2f115"},"cell_type":"markdown","source":"For Type 1, nearly every single type has the greatest density of predictions for the correct type, with a few exceptions.\n\nWater, Bug and Grass are the types predicted with the highest accuracy, whilst Flying is the worst, with a 0% accuracy. The success of the former 3 is likely related to the predictive power of the Egg groups. The poor performance for Flying is likely because it usually exists as a sub-type.\n\nThe only other type with a significant proportion of wrong predictions, is Fairy, which is predicted at Grass more often than Fairy. At first glance, I'm not sure why this is, but guess the two types might share some similarities?\n\nFire and Electric are commonly confused for each other, which I still don't quite understand. Similarly, Ghost and Pyschic are often confused for each other, probably due to both being special focused types that often mirror each other. The Ice  / Water and Rock / Steel confusions are also visible, likely due to the similarities between the types."},{"metadata":{"_uuid":"6fa6cbf710aa5d986e2417208baa181415710856","collapsed":true,"_cell_guid":"e54d4095-45d2-4302-8ef9-e2c9652b3c9a","trusted":false},"cell_type":"code","source":"labels =list(set(targets2))\ncm = metrics.confusion_matrix(targets2, Type2_pred,labels)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\nsns.set(font_scale=4)\nplt.figure(figsize=(20,20))\nax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\nax.set_aspect(1)\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Type 2 Confusion matrix\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"335cf8a946201b0ee3f9e6c6d4e5df0b7ad1b489","_cell_guid":"2472b771-70c9-4702-a0e6-e6ad15316cbc"},"cell_type":"markdown","source":"None clearly dominates the Type 2 predictions, so it's hard to say too much about these results. The fact flying is the second most abundant sub-type is also clear.\n\nDark, Bug, Fire, Water and Normal are the worst performing sub-types, with no correct predictions for any of them.\n\nFlying, Fairy and Rock are mostly predicted correctly, with the wrong predictions often as None.\n\nThe remaining types all have some correct predictions, but are dominated by incorrect assignments to None."},{"metadata":{"_uuid":"4d4c8ab80c48c7014ff84a2f8b4f0c7eefd097d9","_cell_guid":"a7da2e06-2663-48ea-86e1-fdb7990e047e"},"cell_type":"markdown","source":"I noticed for Generation 1 earlier, that there's the possibility some of the types have been assigned correctly, but in the wrong order, so I'm curious how often this has happened."},{"metadata":{"_uuid":"f5346507b7ebfd77bfbc91e06dfa863ca29af94d","collapsed":true,"_cell_guid":"8b7fda52-a54c-4b21-b8ff-a9047fac6bd7","trusted":false},"cell_type":"code","source":"#Any mismatch?\nprint(\"Right type, but wrong order?\")\nType1_accuracy = accuracy_score(targets2, Type1_pred)\nprint(\"Type 1 predictions which match Type 2: %.2f%%\" % (Type1_accuracy * 100.0))\nType2_accuracy = accuracy_score(targets, Type2_pred)\nprint(\"Type 2 predictions which match Type 1: %.2f%%\" % (Type2_accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee9016b6c99fa27098e84c47d40dc48947814f9c","_cell_guid":"b409a010-2668-401d-87e8-97239599ae1b"},"cell_type":"markdown","source":"It's a fairly rare occurance for both Types 1 and 2, at < 5%, but it does happen occasionally.\n\nTo look at the Machine Learning Pokedex is more detail, I want to know other details like, how many Pokemon it got completely correct, how many half correct, and how many it got correct, but in the wrong order."},{"metadata":{"_uuid":"f7e88de39a0a3bec126bf7899344c8df23b7bce9","collapsed":true,"_kg_hide-input":true,"_cell_guid":"bd2eb0ee-5165-4237-a84d-7f64728879df","trusted":false},"cell_type":"code","source":"ML_dex_df=Name_df.copy()\nML_dex_df[\"Type 1\"]=targets\nML_dex_df[\"Type 2\"]=targets2\nML_dex_df[\"Predicted Type 1\"]=Type1_pred\nML_dex_df[\"Predicted Type 2\"]=Type2_pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29d64173f4563d97f5ca071c208854bf436b4051","collapsed":true,"_kg_hide-input":true,"_cell_guid":"ecb47519-769b-4830-ae61-20f80e515622","trusted":false},"cell_type":"code","source":"ML_dex_df[\"Type 1 Correct\"] = np.where(ML_dex_df[\"Type 1\"]==ML_dex_df[\"Predicted Type 1\"], 1, 0)\nML_dex_df[\"Type 2 Correct\"] = np.where(ML_dex_df[\"Type 2\"]==ML_dex_df[\"Predicted Type 2\"], 1, 0)\nML_dex_df[\"Type 1 Mismatch\"]= np.where(ML_dex_df[\"Type 1\"]==ML_dex_df[\"Predicted Type 2\"], 1, 0)\nML_dex_df[\"Type 2 Mismatch\"]= np.where(ML_dex_df[\"Type 2\"]==ML_dex_df[\"Predicted Type 1\"], 1, 0)\nML_dex_df[\"Both Correct\"]   = np.where((ML_dex_df[\"Type 1 Correct\"]==1) & (ML_dex_df[\"Type 2 Correct\"]==1), 1, 0)\nML_dex_df[\"Wrong Order\"]   = np.where((ML_dex_df[\"Type 1 Mismatch\"]==1) & (ML_dex_df[\"Type 2 Mismatch\"]==1), 1,0)\nML_dex_df[\"All Wrong\"]   = np.where((ML_dex_df[\"Type 1 Mismatch\"]==0) & (ML_dex_df[\"Type 2 Mismatch\"]==0)\n                                    &(ML_dex_df[\"Type 1 Correct\"]==0) & (ML_dex_df[\"Type 2 Correct\"]==0), 1,0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6cd9e9bc1e3d52a38988df0931c995daa2b6cb4","collapsed":true,"_cell_guid":"1a495fb5-6170-4904-b9fc-45467b3aecfd","trusted":false},"cell_type":"code","source":"print(\"The number of Pokemon predicted correctly is:\")\nprint(ML_dex_df[\"Both Correct\"].sum())\nprint(\"The number of Pokemon predicted correctly, but in the wrong order is:\")\nprint(ML_dex_df[\"Wrong Order\"].sum())\nprint(\"The number of Pokemon predicted completely incorrectly is:\")\nprint(ML_dex_df[\"All Wrong\"].sum())\nprint(\"The number of Pokemon predicted half right is therefore:\")\nprint(\"379\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfb1d56df29863fd510aace6425e618c42ec6587","_cell_guid":"a390b731-cbcb-4c1e-9683-ab36cdc566ac"},"cell_type":"markdown","source":"I'm suprised the model only got 87 Pokemon completely wrong, given the accuracy for each type was only ~60%. I would guess this means that lots of errors for 1 type lined up with \nThe Pokemon with types in the wrong order are all Rock / Water fossil Pokemon"},{"metadata":{"_uuid":"62a3370e36dcef96ad05bf45d38a7aa8418f11a3","collapsed":true,"_kg_hide-input":true,"_cell_guid":"b46c72d1-0ec0-4461-8173-dcd99b375236","trusted":false},"cell_type":"code","source":"Mix_up = ML_dex_df[(ML_dex_df['Wrong Order'] == 1)].index.tolist()\nWrong = ML_dex_df[(ML_dex_df['All Wrong'] == 1)].index.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"177977189287e235ca205f976ce4c652b95c6881","collapsed":true,"_kg_hide-input":true,"_cell_guid":"82c7ca1a-4878-43be-9d90-01a8cf897c0d","trusted":false},"cell_type":"code","source":"Mixed_up= ML_dex_df.name.iloc[Mix_up]\nprint(list(Mixed_up))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9796ff5756f3522ff6ef762a304caa67f8512b8b","_cell_guid":"f158f2c7-414c-46a1-a23a-f0830030f494"},"cell_type":"markdown","source":"The Pokemon which the model made no correct predictions for are:"},{"metadata":{"_uuid":"afb991d814bdcf222b9d164ea55bff8870d70105","collapsed":true,"_kg_hide-input":true,"_cell_guid":"fddd2b6a-cd04-4b81-929e-50824a1314e8","trusted":false},"cell_type":"code","source":"Failed = ML_dex_df.name.iloc[Wrong]\nprint(list(Failed))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10757397c54287a11c7fbb2502019cf0f45b10bd","_cell_guid":"882f8de7-4b78-4ad7-8486-791b0d39b3ac"},"cell_type":"markdown","source":"Lots of the incorrect predictions apply to the more unique Pokemon, such as Legendaries (mostly from later generations), Ultra Beasts, or rare combinations like Sableye.\n\nSome of the mistakes are really odd, like Water / Flying for Ditto.\n\nIt's no surprise that Inkay and Malamar confuse the mode, since they're both Dark / Psychic, but in 2 Water Egg groups, leading to the Water / Rock predictions."},{"metadata":{"_uuid":"b32009b5c1cf5e7ddcff07f9a42cec9424132c02","collapsed":true,"_cell_guid":"aa3b0ee7-b16b-4a77-a974-87259dab877a"},"cell_type":"markdown","source":"# Future Prospects\n\nTo extend this work further, I plan to revisit Generation 1 again, and try to improve the overall accuracy for both Type slots. This will likely be done by adding more features, such as evolutionary information, or by doing feature engineering."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}