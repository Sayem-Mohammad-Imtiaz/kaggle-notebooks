{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import mode\n\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.metrics import auc\nfrom IPython.display import Image\nimport os\n!ls ../input/\n\nimport os\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_colwidth',500)\npd.set_option('display.max_columns',5000)\n\nencoder = LabelEncoder()\nfrom IPython.display import Image\nimport os\n!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ncampaign = pd.read_csv('../input/campaign_data.csv')\nitems = pd.read_csv('../input/item_data.csv')\ncoupons = pd.read_csv('../input/coupon_item_mapping.csv')\ncust_demo = pd.read_csv('../input/customer_demographics.csv')\ncust_tran = pd.read_csv('../input/customer_transaction_data.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, campaign.shape, items.shape, coupons.shape, cust_demo.shape, cust_tran.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Dataframe')\nprint(train.isnull().sum())\nprint('======================')\nprint('Campaign Dataframe')\nprint(campaign.isnull().sum())\nprint('======================')\nprint('Items Dataframe')\nprint(items.isnull().sum())\nprint('======================')\nprint('Coupons Dataframe')\nprint(coupons.isnull().sum())\nprint('======================')\nprint('Customer Demographics Dataframe')\nprint(cust_demo.isnull().sum())\nprint('======================')\nprint('Customer Transaction Dataframe')\nprint(cust_tran.isnull().sum())\nprint('======================')\n\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.redemption_status.value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value=train['redemption_status'].value_counts().plot(kind='bar')\nplt.ylabel('redemption_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inference\n\nThe dataset is highly imbalanced. As of now proceeding with the same imbalanced data."},{"metadata":{},"cell_type":"markdown","source":"# Cleaning, Preprocessing and Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Customer Demographic Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.marital_status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.family_size.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.no_of_children.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The below lines of code is to get rid of the + and keeping 5+ as 5 and 3+ as 3 and converting the columns to int data type.\n#type of family size = int64 ... Cant apply astype as we have 5+ as family size\n#no of children = int64 ... we need to ignore the NaN values while converting to float\ncust_demo['family_size'] = cust_demo.family_size.apply(lambda x: int(re.sub('\\+','',x)))\ncust_demo['no_of_children'] = cust_demo.no_of_children.apply(lambda x: int(re.sub('\\+','',x)) if pd.notna(x) else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling NaN values for marital_status\n\n#customers with family size =1 will be single\ncust_demo.loc[pd.isnull(cust_demo.marital_status) & (cust_demo.family_size == 1),'marital_status'] = 'Single'\n\n#customers whos family size - no of childrens == 1, will also be single \n#This is applicable where there is only 1 parent --- We treat 1 parent as Single\ncust_demo.loc[(cust_demo.family_size - cust_demo.no_of_children == 1) & pd.isnull(cust_demo.marital_status),'marital_status'] = 'Single'\n\n#from the orignal data we have 186 of 196 customers with diff of 2 in their family size and number of childrens as\n#Married (see the below cell) and hence where ever the difference is 2 and marital status is NaN and No of Children is \n#NaN we impute the Mariatl Status with Married\ncust_demo.loc[(pd.isnull(cust_demo.marital_status)) & ((cust_demo.family_size - cust_demo.no_of_children) == 2)  \n              & (pd.notnull(cust_demo.no_of_children)),'marital_status'] = 'Married'\n\n#original data shows customers with fam size == 2, and NaN in no of childrens are majorly Married (see below cell skipping 1 cell)\ncust_demo.loc[pd.isnull(cust_demo.marital_status) & (pd.isnull(cust_demo.no_of_children)) \n              & (cust_demo.family_size ==2),'marital_status'] = 'Married'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = cust_demo.marital_status.groupby((cust_demo.family_size - cust_demo.no_of_children) == 2).value_counts()\nprint(a[True])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#b = cust_demo.marital_status.groupby((cust_demo.family_size) == 2 & pd.isnull(cust_demo.no_of_children)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.marital_status.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FillingNaN values for no of children\n\n#Married people with family_size ==2 will have 0 childrens\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.marital_status == 'Married') & (cust_demo.family_size == 2),'no_of_children'] = 0\n\n#customers with family size 1 will have zero childrens\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.family_size == 1), 'no_of_children'] = 0\n\n#singles with family size == 2, will probably have 1 child\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.family_size == 2),'no_of_children'] = 1\n\ncust_demo['no_of_children']=cust_demo['no_of_children'].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.no_of_children.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding Marital Status --- 0 is Single and 1 is Married\ncust_demo[\"marital_status\"] = encoder.fit_transform(cust_demo[\"marital_status\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding age_range ... 18-25 is 0, 26-35 is 1, 36-45 is 2, 46-55 is 3, 56-70 is 4 and 70+ is 5\ncust_demo[\"age_range\"] = encoder.fit_transform(cust_demo[\"age_range\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_demo.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Campaign Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"campaign.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"campaign.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"campaign.campaign_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding Campaign type\ncampaign[\"campaign_type\"] = encoder.fit_transform(campaign.campaign_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the date columns to date time\ncampaign['start_date'] = pd.to_datetime(campaign['start_date'], format = '%d/%m/%y')\ncampaign['end_date'] = pd.to_datetime(campaign['end_date'], format = '%d/%m/%y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a new column campaign_duration\ncampaign[\"campaign_duration\"] = campaign[\"end_date\"] - campaign[\"start_date\"]\ncampaign[\"campaign_duration\"] = campaign[\"campaign_duration\"].apply(lambda x: x.days) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"campaign.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Customer Transaction Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_tran.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_tran.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the date column into date time\n#Reset the index of the DataFrame, and use the default one instead.\n#If the DataFrame has a MultiIndex, this method can remove one or more levels.\ncust_tran['date'] = pd.to_datetime(cust_tran['date'])\ncust_tran = cust_tran.sort_values('date').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_tran.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating 3 new columns from the date column\ncust_tran['day'] = cust_tran[\"date\"].apply(lambda x: x.day)\ncust_tran['dow'] = cust_tran[\"date\"].apply(lambda x: x.weekday())\ncust_tran['month'] = cust_tran[\"date\"].apply(lambda x: x.month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_tran.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Given selling_price and other_discount are for the entire transaction. Hence getting the Actual value of the transaction.\ncust_tran.selling_price = cust_tran.selling_price/cust_tran.quantity\ncust_tran.other_discount = cust_tran.other_discount/cust_tran.quantity\ncust_tran.selling_price = cust_tran.selling_price - cust_tran.other_discount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inserting a new column to know if the coupon was used or not\ncust_tran['coupon_used'] = cust_tran.coupon_discount.apply(lambda x: 1 if x !=0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_tran.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Items Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.brand_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding the brand_type and category columns\nitems.brand_type = encoder.fit_transform(items[\"brand_type\"])\nitems.category = encoder.fit_transform(items[\"category\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing with Coupoun Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"coupons.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we can Start with Merging the Data Frames As all the Dataframes have data in the proper format"},{"metadata":{},"cell_type":"markdown","source":"## Let us see the schema first before proceeding"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/Schema.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging Train and Test Data with Other Data"},{"metadata":{},"cell_type":"markdown","source":"1. Merge coupon item data and items data on item_id\n2. Aggregate transactions by item_id\n3. Merge 1 and 2 on item_id\n4. Aggregate 3 on coupon_id\n5. Merge 4 and train on coupon_id\n6. Aggregate transactions on customer_id\n7. Merge 5 with campaign data on campaign_id\n8. Merge 7 with customer demographic data on customer_id\n9. Merge 6 with 8 on customer_id respectively"},{"metadata":{},"cell_type":"markdown","source":"### Step 1: \nMerging coupons and items data on 'item_id'"},{"metadata":{"trusted":true},"cell_type":"code","source":"coupons_items = pd.merge(coupons, items, on=\"item_id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coupons_items.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2:\nAggregate the customer transaction by 'item_id'\n\nBefore getting into the below code ... Understand the working of pd.pivot_table here\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html\n\nFollowing Blog is an excellent demonstration of the working of Pivot tables in Python\nhttps://pbpython.com/pandas-pivot-table-explained.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_tran.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate transactions by item_id by mean for a particular customer\ntransactions1 = pd.pivot_table(cust_tran, index = \"item_id\", \n               values=['customer_id','quantity','selling_price', 'other_discount','coupon_discount','coupon_used'],\n               aggfunc={'customer_id':lambda x: len(set(x)),\n                        'quantity':np.mean,\n                        'selling_price':np.mean,\n                        'other_discount':np.mean,\n                        'coupon_discount':np.mean,\n                        'coupon_used': np.sum\n                        } )\ntransactions1.reset_index(inplace=True)\ntransactions1.rename(columns={'customer_id': 'no_of_customers'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate transactions by item_id by sum for a particular customer\ntransactions2 = pd.pivot_table(cust_tran, index = \"item_id\", \n               values=['customer_id','quantity','selling_price', 'other_discount','coupon_discount'],\n               aggfunc={'customer_id':len,\n                        'quantity':np.sum,\n                        'selling_price':np.sum,\n                        'other_discount':np.sum,\n                        'coupon_discount':np.sum,\n                        } )\ntransactions2.reset_index(inplace=True)\ntransactions2.rename(columns={'customer_id': 't_counts', 'quantity':'qu_sum',\n                             'selling_price':'price_sum', 'other_discount':'od_sum',\n                             'coupon_discount':'cd_sum'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions1 = pd.merge(transactions1, transactions2, on='item_id',how='left' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions1['total_discount_mean'] = transactions1['coupon_discount'] + transactions1['other_discount']\ntransactions1['total_discount_sum'] = transactions1['od_sum'] + transactions1['cd_sum']\ntransactions1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3:\nMerge coupon_items and transaction1 on 'item_id'"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_coupon_trans = pd.merge(coupons_items, transactions1, on='item_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_coupon_trans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_coupon_trans.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4:\nNow Aggregating item_coupon_trans on 'coupon_id'"},{"metadata":{"trusted":true},"cell_type":"code","source":"coupon = pd.pivot_table(item_coupon_trans, index =\"coupon_id\",\n                         values=[ 'item_id', 'brand', 'brand_type', 'category',\n       'coupon_discount', 'coupon_used', 'no_of_customers', 'other_discount',\n       'quantity', 'selling_price', 'cd_sum', 't_counts', 'od_sum', 'qu_sum',\n       'price_sum', 'total_discount_mean', 'total_discount_sum'],\n              aggfunc={'item_id':lambda x: len(set(x)),\n                       'brand':lambda x: mode(x)[0][0],\n                       'brand_type':lambda x: mode(x)[0][0],\n                       'category':lambda x: mode(x)[0][0],\n                       'coupon_discount':np.mean,\n                       'no_of_customers':np.mean,\n                       'other_discount':np.mean,\n                       'quantity':np.mean,\n                       'selling_price':np.mean,\n                      'coupon_used': np.sum,\n                       'cd_sum': np.sum,\n                       't_counts': np.sum,\n                       'od_sum': np.sum,\n                       'qu_sum': np.sum,\n                       'price_sum': np.sum,\n                       'total_discount_mean': np.mean,\n                       'total_discount_sum': np.sum\n                      })\ncoupon.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coupon.rename(columns={'item_id':'item_counts'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coupon.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 6:\nAggregating cust_trans on 'coupon_id'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate transactions by customer_id\ntransactions3 = pd.pivot_table(cust_tran, index = \"customer_id\", \n               values=['item_id','quantity','selling_price', 'other_discount','coupon_discount','coupon_used','day','dow','month'],\n               aggfunc={'item_id':lambda x: len(set(x)),\n                        'quantity':np.mean,\n                        'selling_price':np.mean,\n                        'other_discount':np.mean,\n                        'coupon_discount':np.mean,\n                        'coupon_used': np.sum,\n                        'day':lambda x: mode(x)[0][0],\n                        'dow':lambda x: mode(x)[0][0],\n                        'month':lambda x: mode(x)[0][0]}\n              )\ntransactions3.reset_index(inplace=True)\ntransactions3.rename(columns={'item_id': 'no_of_items'}, inplace=True)\ntransactions3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate transactions by customer_id by sum\ntransactions4 = pd.pivot_table(cust_tran, index = \"customer_id\", \n               values=['item_id','quantity','selling_price', 'other_discount','coupon_discount'],\n               aggfunc={'item_id':len,\n                        'quantity':np.sum,\n                        'selling_price':np.sum,\n                        'other_discount':np.sum,\n                        'coupon_discount':np.sum}\n              )\ntransactions4.reset_index(inplace=True)\ntransactions4.rename(columns={'item_id': 'customer_id_count','quantity':'qa_sum','selling_price':'pprice_sum',\n                             'other_discount':'odd_sum','coupon_discount':'cdd_sum'  }, inplace=True)\ntransactions4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions = pd.merge(transactions3, transactions4, on='customer_id', how='left')\ntransactions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 5,7,8,9\n\nMerge 4 and train on coupon_id\n\nMerge 5 with campaign data on campaign_id\n\nMerge 7 with customer demographic data on customer_id\n\nMerge 6 with 8 on customer_id respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_all(df): \n    df=  pd.merge(df, coupon, on=\"coupon_id\", how=\"left\")\n    df = pd.merge(df, campaign, on=\"campaign_id\", how=\"left\")\n    df = pd.merge(df, cust_demo, on=\"customer_id\", how=\"left\")\n    df = pd.merge(df, transactions, on='customer_id', how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = merge_all(train)\ntest = merge_all(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To save the final file after merging the data\n##train.to_csv('FinalData.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data is Ready here, But we need to check for Null Values which could have been created while merging."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nThere are missing values in:\n\n1. age_range\n2. marital_status\n3. rented\n4. family_size\n5. no_of_children\n6. income_bracke"},{"metadata":{},"cell_type":"markdown","source":"### As all of these columns are present in cust_demo dataframe, filling the NaN's with the mode."},{"metadata":{"trusted":true},"cell_type":"code","source":"def deal_na(df):\n    for col in cust_demo.columns.tolist()[1:]:\n        df[col].fillna(mode(df[col]).mode[0], inplace=True)\n    return df\n\ntrain = deal_na(train)\ntest = deal_na(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now Dropping off the Unwanted Columns and making the data ready for Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test['id']\ntarget = train['redemption_status']\ntrain.drop(['id','campaign_id','start_date','end_date', 'redemption_status'], axis=1, inplace=True)\ntest.drop(['id','campaign_id','start_date','end_date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA\n## Dimensionality Reductionm"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train\ny=target\ncol_names = ['cd_sum','coupon_discount_x', 'coupon_used_x', 'item_counts', 'no_of_customers',\n       'od_sum', 'other_discount_x', 'price_sum', 'qu_sum', 'quantity_x',\n       'selling_price_x', 't_counts', 'total_discount_mean',\n       'total_discount_sum', 'campaign_type', 'campaign_duration',\n        'family_size', 'no_of_children',\n       'income_bracket', 'coupon_discount_y', 'coupon_used_y',\n       'no_of_items', 'other_discount_y', 'quantity_y',\n       'selling_price_y', 'cdd_sum', 'customer_id_count', 'odd_sum', 'qa_sum',\n       'pprice_sum']\nfeatures = x[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\nx[col_names] = features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cov_matrix = np.cov(x.T)\ncov_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eigenvalues,eigenvectors = np.linalg.eig(cov_matrix)\neigenvalues,eigenvectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eig_pairs = [(eigenvalues[i],eigenvectors[:,i]) for i in range(len(eigenvalues))]\neig_pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eig_pairs.sort()\neig_pairs.reverse()\neig_pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eigenvale_sort = [eig_pairs[i][0] for i in range(len(eigenvalues))]\neigenvector_sort = [eig_pairs[i][1] for i in range(len(eigenvalues))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tot = sum(eigenvalues)\nvar_exp = [(i/tot)*100 for i in sorted(eigenvalues,reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\ncum_var_exp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_reduce = np.array(eigenvector_sort[0:5]).transpose()\np_reduce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pc_data = np.dot(x,p_reduce)\npc_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pc_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(pc_data,y,test_size=0.3,random_state=1990)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape,y_train.shape,x_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building for PCA dataset - To be Covered Are:\n1. Logistic Regression\n2. kNN\n3. Naive Bayes\n4. Decision Tree\n5. Random Forest\n6. Logistic Regression -- Bagged\n7. kNN -- Bagged\n8. Naive Bayes - Bagged\n9. Decision Tree -- Bagged\n10. Logistic Regression -- Boosted\n11. Naive Bayes -- Boosted\n12. Decision Tree -- Boosted\n13. Random Forest --Boosted\n14. Gradient Boosting Classifier\n15. Light GBM (LGBM)"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(x_train,y_train)\ny_pred_LR = LR.predict(x_test)\nprint(classification_report(y_test,y_pred_LR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test,y_pred_LR)\nModel = ['Logistic Regression']\nROC_AUC_Accuracy = [roc_auc_score(y_test,y_pred_LR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \nresults=confusion_matrix(y_test,y_pred_LR)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_LR) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_LR) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_LR ):\n    cm = metrics.confusion_matrix( y_test,y_pred_LR )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_LR )\n\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_LR):\n    if (len(y_test.shape) != len(y_pred_LR.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_LR.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_LR)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    conf_mat = create_conf_mat(y_test,y_pred_LR)\n    sns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.title('Actual vs. Predicted Confusion Matrix')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = LR.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# kNN"},{"metadata":{},"cell_type":"markdown","source":"## Caution --- Algorithm - 'brute' was crashing the system, hence removed it."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'n_neighbors': range(1,5),\n    'weights': ['uniform','distance'],\n    'algorithm': ['ball_tree','kd_tree'],\n    'p': [1,2,3]\n}\n\nknn = KNeighborsClassifier()\n\ngs = GridSearchCV(estimator=knn,n_jobs=-1,cv=3,param_grid=params,scoring='recall')\ngs.fit(pc_data,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(**gs.best_params_)\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_knn))\nModel.append('k-Nearest-Neighbours')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_knn)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_knn) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_knn) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_knn ):\n    cm = metrics.confusion_matrix( y_test,y_pred_knn )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_knn )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_knn):\n    if (len(y_test.shape) != len(y_pred_knn.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_knn.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_knn)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_knn)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = knn.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_nb))\nModel.append('Naive Bayes')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_nb)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_nb) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_nb) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_nb ):\n    cm = metrics.confusion_matrix( y_test,y_pred_nb )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_nb )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_nb):\n    if (len(y_test.shape) != len(y_pred_nb.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_nb.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_nb)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_nb)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = nb.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':range(1,10),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\ndt = DecisionTreeClassifier()\n\ngs = GridSearchCV(estimator=dt,n_jobs=-1,cv=3,param_grid=params,scoring='recall')\ngs.fit(pc_data,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(**gs.best_params_)\ndt.fit(x_train,y_train)\ny_pred_dt = dt.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_dt))\nModel.append('Decision Tree')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_dt):\n    cm = metrics.confusion_matrix( y_test,y_pred_dt )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_dt)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_dt) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_dt) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_dt )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_dt):\n    if (len(y_test.shape) != len(y_pred_dt.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_dt.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_dt)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_dt)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = dt.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'n_estimators':range(10,100,10),\n    'criterion':['gini','entropy'],\n    'max_depth':range(2,10,1),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\nrf = RandomForestClassifier()\n\nrs = RandomizedSearchCV(estimator=rf,param_distributions=params,cv=3,scoring='recall',n_jobs=-1)\nrs.fit(pc_data,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(**rs.best_params_)\nrf.fit(x_train,y_train)\ny_pred_rf = rf.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_rf))\nModel.append('Random Forest')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_rf)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_rf) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_rf) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_rf ):\n    cm = metrics.confusion_matrix( y_test,y_pred_rf )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_rf )\nprint(\"confusion matrix = \\n\",mat_pruned)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_rf):\n    if (len(y_test.shape) != len(y_pred_rf.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_rf.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_rf)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = rf.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now using the Bagging Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Bag = BaggingClassifier(base_estimator=LR,n_estimators=100,n_jobs=-1,random_state=1)\nknn_Bag = BaggingClassifier(base_estimator=knn,n_estimators=100,n_jobs=-1,random_state=1)\nnb_Bag = BaggingClassifier(base_estimator=nb,n_estimators=100,n_jobs=-1,random_state=1)\ndt_Bag = BaggingClassifier(base_estimator=dt,n_estimators=100,n_jobs=-1,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nfor model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\n    roc_acc = []\n    for train,test in kf.split(pc_data,y):\n        x_train = pc_data[train,:]\n        x_test = pc_data[test,:]\n        y_train = y[train]\n        y_test = y[test]\n        model.fit(x_train,y_train)\n        y_pred = model.predict(x_test)\n        score = roc_auc_score(y_test,y_pred)\n        fpr,tpr,_ = roc_curve(y_test,y_pred)\n        roc_acc.append(auc(fpr,tpr))\n    Model.append(name)\n    ROC_AUC_Accuracy.append(np.mean(roc_acc))\n    print('The AUC Score for')\n    print('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now Moving Towards Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMModel,LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Boost = AdaBoostClassifier(base_estimator=LR,n_estimators=100,learning_rate=0.01,random_state=1)\nknn_Boost = AdaBoostClassifier(base_estimator=knn,n_estimators=100,learning_rate=0.01,random_state=1)\nnb_Boost = AdaBoostClassifier(base_estimator=nb,n_estimators=100,learning_rate=0.01,random_state=1)\ndt_Boost = AdaBoostClassifier(base_estimator=dt,n_estimators=100,learning_rate=0.01,random_state=1)\nrf_Boost = AdaBoostClassifier(base_estimator=rf,n_estimators=100,learning_rate=0.01,random_state=1)\ngb_Boost = GradientBoostingClassifier(n_estimators=100,learning_rate=0.01)\nlgbm = LGBMClassifier(objective='binary',n_estimators=100,reg_alpha=2,reg_lambda=5,random_state=1,learning_rate=0.01,is_unbalance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc\nkf = KFold(n_splits=3,shuffle=True,random_state=1)\nfor model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\n    roc_acc = []\n    for train,test in kf.split(pc_data,y):\n        x_train = pc_data[train,:]\n        x_test = pc_data[test,:]\n        y_train = y[train]\n        y_test = y[test]\n        model.fit(x_train,y_train)\n        y_pred = model.predict(x_test)\n        score = roc_auc_score(y_test,y_pred)\n        fpr,tpr,_ = roc_curve(y_test,y_pred)\n        roc_acc.append(auc(fpr,tpr))\n    Model.append(name)\n    ROC_AUC_Accuracy.append(np.mean(roc_acc))\n    print('The AUC Score for')\n    print('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'Model - PCA Data':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}