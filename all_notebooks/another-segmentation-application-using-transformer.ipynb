{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#########################################################\n# nothing new Just Another Application to show          #\n# Ecoder - Decoder Neural Networks                      #\n#########################################################","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Making essential imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\n\n\n## defining a frame for image and mask storage\nframObjTrain = {'img' : [],\n           'mask' : []\n          }\n\nframObjValidation = {'img' : [],\n           'mask' : []\n          }\n\n## defining data Loader function\ndef LoadData( frameObj = None, imgPath = None, maskPath = None, shape = 256):\n    imgNames = os.listdir(imgPath)\n    maskNames = []\n    \n    ## generating mask names\n    for mem in imgNames:\n        maskNames.append(re.sub('\\.jpg', '.png', mem))\n    \n    imgAddr = imgPath + '/'\n    maskAddr = maskPath + '/'\n    \n    for i in range (len(imgNames)):\n        try:\n            img = plt.imread(imgAddr + imgNames[i])\n            mask = plt.imread(maskAddr + maskNames[i])\n            \n        except:\n            continue\n        img = cv2.resize(img, (shape, shape))\n        mask = cv2.resize(mask, (shape, shape))\n        \n        frameObj['img'].append(img)\n        frameObj['mask'].append(mask)\n        \n    return frameObj\n        \n    \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"framObjTrain = LoadData( framObjTrain, imgPath = '/kaggle/input/clothing-coparsing-dataset/images', \n                        maskPath = '/kaggle/input/clothing-coparsing-dataset/labels/pixel_level_labels_colored'\n                         , shape = 256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## displaying data loaded by our function\nplt.subplot(1,2,1)\nplt.imshow(framObjTrain['img'][1])\nplt.subplot(1,2,2)\nplt.imshow(framObjTrain['mask'][1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## defining our CNN for encoding and decoding\n\nmyTransformer = tf.keras.models.Sequential([\n## defining encoder \n    tf.keras.layers.Input(shape= (256, 256, 3)),\n    tf.keras.layers.Conv2D(filters = 16, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.MaxPool2D(pool_size = (2, 2)),\n    \n    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), strides = (2,2), activation = 'relu', padding = 'valid'),\n    tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), activation = 'relu', padding = 'same'),\n    tf.keras.layers.MaxPool2D(pool_size = (2, 2)),\n    \n    tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 512, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    \n    ## defining decoder path\n    tf.keras.layers.UpSampling2D(size = (2,2)),\n    tf.keras.layers.Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    \n    tf.keras.layers.UpSampling2D(size = (2,2)),\n    tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.UpSampling2D(size = (2,2)),\n    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.UpSampling2D(size = (2,2)),\n    tf.keras.layers.Conv2D(filters = 16, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    tf.keras.layers.Conv2D(filters = 3, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n    \n    \n    \n])\n\nmyTransformer.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5), loss = 'mean_absolute_error', metrics = ['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## trainign our model\nretVal = myTransformer.fit(np.array(framObjTrain['img']), np.array(framObjTrain['mask']), epochs = 100, verbose = 0)\nplt.plot(retVal.history['loss'], label = 'training_loss')\nplt.plot(retVal.history['acc'], label = 'training_accuracy')\nplt.legend()\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict16 (valMap, model, shape = 256):\n    ## getting and proccessing val data\n    img = valMap['img']\n    mask = valMap['mask']\n    mask = mask[0:16]\n    \n    imgProc = img [0:16]\n    imgProc = np.array(img)\n    \n    predictions = model.predict(imgProc)\n    for i in range(len(predictions)):\n        predictions[i] = cv2.merge((predictions[i,:,:,0],predictions[i,:,:,1],predictions[i,:,:,2]))\n    \n    return predictions, imgProc, mask\n\n\ndef Plotter(img, predMask, groundTruth):\n    plt.figure(figsize=(7,7))\n    \n    plt.subplot(1,3,1)\n    plt.imshow(img)\n    plt.title('image')\n    \n    plt.subplot(1,3,2)\n    plt.imshow(predMask)\n    plt.title('Predicted Mask')\n    \n    plt.subplot(1,3,3)\n    plt.imshow(groundTruth)\n    plt.title('actual Mask')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sixteenPrediction, actuals, masks = predict16(framObjTrain, myTransformer)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plotter(actuals[1], sixteenPrediction[1], masks[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plotter(actuals[2], sixteenPrediction[2], masks[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plotter(actuals[3], sixteenPrediction[3], masks[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plotter(actuals[9], sixteenPrediction[9], masks[9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plotter(actuals[14], sixteenPrediction[14], masks[14])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plotter(actuals[10], sixteenPrediction[10], masks[10])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}