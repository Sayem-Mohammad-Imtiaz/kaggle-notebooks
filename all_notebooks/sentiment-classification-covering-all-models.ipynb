{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n\n* [Data Preprocessing](#1)\n\n    - [Downloading IMDB data](#2)\n    \n    - [Data cleaning](#3)\n    \n    - [Shuffling data](#4)\n    \n    - [Removing ctopwords](#5)\n    \n    - [Lemmatization](#6)\n    \n    \n* [Machine Learning models](#7)\n\n    - [One-hot encoding](#8)  \n    \n    - [Naive Bayes](#9) \n         \n    - [Linear Models](#12) \n             \n    - [K-nearest neighbours](#15)\n    \n    - [Support Vector Classifier](#16)\n    \n    - [Word count features](#17)\n    \n    - [n-gram features](#18)\n    \n    - [tf-idf features](#19)\n    \n    \n* [Deep Learning models](#20)\n\n    - [Custom word embeddings](#21) \n\n    - [Vanilla LSTM](#23)   \n        \n    - [Using all hidden states](#24)\n        \n    - [Bidirectional LSTM](#25)   \n             \n    - [Convolutional neural networks](#26)  \n    \n    - [Pre-trained Glove embeddings](#27)  \n    \n    - [Freezing the embedding layer](#28)    \n    \n    - [Fine-tuning the Glove embeddings](#29)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I have attempted to accomodate all recipes that can be used for Text Sentiment Classification. I will start by downloading the raw data, then process it, and finally train various Machine Learning and Deep Learning models on it. \n\nThis notebook can be used as reference for all kinds of binary sentiment classification tasks. It aims to cover as much as possible, but for every model covered in it, there is still good scope for performance improvement, by working further on hyperparameter tuning.","execution_count":null},{"metadata":{"id":"F6fVZEo3E4uY"},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Refined data for IMDB reviews is avilable, but since I want this notebook to be as comprehensive as possible, I will download raw data and then process it step-by-step.","execution_count":null},{"metadata":{"id":"nE2yZscap0UD"},"cell_type":"markdown","source":"<a id=\"2\"></a>\n## Downloading IMDB data","execution_count":null},{"metadata":{"id":"P-t4-Vjkp2Bh","trusted":true},"cell_type":"code","source":"import requests\n\nurl = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\ntarget_path = 'aclImdb_v1.tar.gz'\n\nresponse = requests.get(url, stream=True)\nif response.status_code == 200:\n    with open(target_path, 'wb') as f:\n        f.write(response.raw.read())","execution_count":null,"outputs":[]},{"metadata":{"id":"ldmnsPfN7Wq8","trusted":true},"cell_type":"code","source":"import tarfile\ntar = tarfile.open(\"aclImdb_v1.tar.gz\")\ntar.extractall()\ntar.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The extracted .tar file generated two folders - 'Train', and 'Test'. Each folder contains 25,000 text files each, grouped into two folders 'pos' and 'neg', that contain 12,500 files each.\n\nThe following snippet reads from one of the folders, and creates a list of texts, and the corrsponding list of binary targets.","execution_count":null},{"metadata":{"id":"cp2lFFiCEHeK","trusted":true},"cell_type":"code","source":"import glob \nclasses = {'pos':1, 'neg':0}\n\ndef read_txt(file_path):\n  with open(file_path, 'r') as file:\n    text = file.read()\n  file.close()\n  return text\n\ndef populate(main_folder):\n  all_txts, all_sentiments = [], []\n  for class_ in classes:\n    directory = \"aclImdb/{}/{}\".format(main_folder, class_)\n    file_paths = glob.glob(directory + '/*.txt')\n    txts = [read_txt(path) for path in file_paths]\n    sentiments = [classes[class_] for _ in range(len(txts))]\n    all_txts.extend(txts)\n    all_sentiments.extend(sentiments)\n  return all_txts, all_sentiments","execution_count":null,"outputs":[]},{"metadata":{"id":"DI0Cvy9W_n-P","trusted":true},"cell_type":"code","source":"X_train, y_train = populate('train')\nX_test, y_test = populate('test')","execution_count":null,"outputs":[]},{"metadata":{"id":"opz4VWDxPQaP","outputId":"5643acf3-84c2-4d58-cb4b-c8368458bd95","trusted":true},"cell_type":"code","source":"print(len(X_train))\nprint(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"uuWHnlE2RWbO"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n## Data Cleaning","execution_count":null},{"metadata":{"id":"1in2Vy-OOC1m","outputId":"e530b55c-ac28-4a60-c4f7-25d00efd16b0","trusted":true},"cell_type":"code","source":"print(X_train[5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to do three things first:\n\n(1) Get rid of the HTML tags (characters enclosed by <>)\n\n(2) Get rid of numbers and other special characters\n\n(3) Lowercase all the remaining alphabet characters","execution_count":null},{"metadata":{"id":"90AvvgiUQP_-","trusted":true},"cell_type":"code","source":"import re\n\nextra_chars = re.compile(\"[0-9.;:!\\'?,%\\\"()\\[\\]]\")\nhtml_tags = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n\ndef clean(texts):\n    texts = [extra_chars.sub(\"\", text.lower()) for text in texts]\n    texts = [html_tags.sub(\" \", text) for text in texts]\n    return texts","execution_count":null,"outputs":[]},{"metadata":{"id":"zyDs2mU-cM2v","trusted":true},"cell_type":"code","source":"X_train = clean(X_train)\nX_test = clean(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZX_LLoKxOvQs"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n## Shuffling Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Recall that in both our extracted train and test data, we have first 12,500 samples belonging to 'pos' sentiment, and the remaining 12,500 samples to the 'neg' sentiment. Our models will do better if we trained on data where the samples follow a mixed order.","execution_count":null},{"metadata":{"id":"h3QgSZAWO7VW","trusted":true},"cell_type":"code","source":"import random\n\ndef shuffle_set(X, y):\n  all_data = list(zip(X, y))\n  random.shuffle(all_data)\n  X_shuff, y_shuff = [list(item) for item in zip(*all_data)]\n  return X_shuff, y_shuff\n\nX_train, y_train = shuffle_set(X_train, y_train)\nX_test, y_test = shuffle_set(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"_Lp5-kHaBwiP"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n## Removing stopwords","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will now get rid of common words such as 'the', 'of', 'at' etc (known as stopwords). They don't really contribute much useful information and also their removal will reduce our feature size.","execution_count":null},{"metadata":{"id":"gazHn3rnIDuf","outputId":"38c98cbb-5b40-4d8e-97a6-00805d0a5bb7","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"F2qXr9WHHIaF","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nstop_words = set(stopwords.words('english')) \n\ndef filter_text(text):\n  words = text.split()\n  return ' '.join([w for w in words if w not in stop_words])","execution_count":null,"outputs":[]},{"metadata":{"id":"2N6sG02eIJU8","trusted":true},"cell_type":"code","source":"X_train = [filter_text(text) for text in X_train]\nX_test = [filter_text(text) for text in X_test]","execution_count":null,"outputs":[]},{"metadata":{"id":"miaFhqitJPiN"},"cell_type":"markdown","source":"<a id=\"6\"></a>\n## Lemmatization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For training our models, all words will have to be encoded with some sort of numerical representation. The size of that word-to-number vocabulary would be huge. We can reduce it to some extent by lemmatizing the words. \n\nrocks ---> rock, \n\nbetter ---> good, \n\nwalked ---> walk.\n\nNote that Stemming is another popular method.\n\nRead more - https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8","execution_count":null},{"metadata":{"id":"1DJDWSIeJ2g-","outputId":"09ef81c6-104f-4201-eb9b-99f4b653c6c7","trusted":true},"cell_type":"code","source":"nltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"id":"Crplcyq1JOcH","trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \n\ndef lemmatize(text):\n  words = text.split()\n  return ' '.join([lemmatizer.lemmatize(w) for w in words])","execution_count":null,"outputs":[]},{"metadata":{"id":"JhWAaZukJuR1","trusted":true},"cell_type":"code","source":"X_train = [lemmatize(text) for text in X_train]\nX_test = [lemmatize(text) for text in X_test]","execution_count":null,"outputs":[]},{"metadata":{"id":"dPdTZsqBcf4W","outputId":"2b06c90c-1259-4696-e6ba-c6d091e8ef8c","trusted":true},"cell_type":"code","source":"print(X_train[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"5ZZV1nRpEoqf"},"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Machine Learning models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before, we can fit or train any model on our data, we need to transform all the textual data into numerical form. The most common is one-hot encoding, where each text is represented by a sequence of binary data. The length of every sequence is equal to the size of the entire vocabulary. \n\nEach position in any sequence corresponds to a particular word. '1' tells you that the word is present in that text, and '0' denotes its absence. It is important to note that here the order of the words does not matter. We are simply treating any text sequence as a bag-of-words (BOW).","execution_count":null},{"metadata":{"id":"kz4PCFSZE0Wy"},"cell_type":"markdown","source":"<a id=\"8\"></a>\n## One-Hot Encoding","execution_count":null},{"metadata":{"id":"XM_0bsgHCeMk","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(binary=True)\n\nvectorizer.fit(X_train)\n\nX_train_onehot = vectorizer.transform(X_train)\n\nX_test_onehot = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"GZk376kaF8Yf","outputId":"c4c1e440-6804-48b2-c94d-a5015cda04b1","trusted":true},"cell_type":"code","source":"print(X_train_onehot.shape)\nprint(X_test_onehot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at 20 words from the word vocabulary with their corresponding indices in the vocabulary.","execution_count":null},{"metadata":{"id":"JBKH9GT3L3B7","outputId":"73c2c65e-b31e-4314-9e45-29da1e1cec20","trusted":true},"cell_type":"code","source":"word_dict = vectorizer.vocabulary_\n\nprint({k: word_dict[k] for k in list(word_dict)[:20]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will start with the simple Naive Bayes models. They are simple probabilistic models that make use of the Bayes theorem.\n\nLearn more - https://www.youtube.com/watch?v=EGKeC2S44Rs","execution_count":null},{"metadata":{"id":"dnafmquyN-SD"},"cell_type":"markdown","source":"<a id=\"9\"></a>\n## Naive Bayes ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will fit two Naive Bayes models - Multinomial and Bernoulli. Bernoulli models the presence/absence of a feature. Multinomial models the number of counts of a feature. Recall that we have one-hot features currently. So, this data does not give a fair chance to Multinomial NB right now, and we will return to it again later in the notebook.","execution_count":null},{"metadata":{"id":"z1sEa04JWafc"},"cell_type":"markdown","source":"<a id=\"10\"></a>\n### Multinomial","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will use two evaluation metrics thoughout this notebook. Accuracy is the ratio of the true predictions (postive or negative) and the total number of predictions made.\n\nF1 score is the harmonic mean of precision and recall.\n\nRead more - https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9","execution_count":null},{"metadata":{"id":"Ao3BZXxlJO67","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following code-snippet will be used everytime we fit a machine learning model on the training set, and then evaluate the metrics over the test set.","execution_count":null},{"metadata":{"id":"9lWzCNPaIfbe","trusted":true},"cell_type":"code","source":"def fit_and_test(classifier, X_train, y_train, X_test, y_test, only_return_accuracy=False):\n\n  classifier.fit(X_train, y_train)\n\n  y_hat = classifier.predict(X_test)\n\n  print('accuracy:', accuracy_score(y_test, y_hat))\n\n  if not only_return_accuracy:\n    print('f1_score:', f1_score(y_test, y_hat))","execution_count":null,"outputs":[]},{"metadata":{"id":"mGY5XeWoMq5D","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"id":"OAU-j8kaODV6","outputId":"9cce1637-5221-4854-9a82-d714391604c7","trusted":true},"cell_type":"code","source":"mnb = MultinomialNB()\nfit_and_test(mnb, X_train_onehot, y_train, X_test_onehot, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"JQmg__mUWeao"},"cell_type":"markdown","source":"<a id=\"11\"></a>\n### Bernoulli","execution_count":null},{"metadata":{"id":"cdO0yA30XW6H","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB","execution_count":null,"outputs":[]},{"metadata":{"id":"7XJ-TGQKU76O","outputId":"00ea4a37-5fb5-49f6-cd42-9af7f2f8c433","trusted":true},"cell_type":"code","source":"bnb = BernoulliNB()\nfit_and_test(bnb, X_train_onehot, y_train, X_test_onehot, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will move to linear models. Simply put, in any linear model, the target variable is just some form of linear combination of input features. In logistic regression, we have the logistic aka sigmoid function at the output side, that squishes the output to between 0 and 1. When we keep a threshold (usually equal to 0.5), we can obtain binary values, 0 and 1. This is how regression is used for binary classification tasks.\n\nRead more - https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc","execution_count":null},{"metadata":{"id":"bchGG72GVJSY"},"cell_type":"markdown","source":"<a id=\"12\"></a>\n## Linear Models","execution_count":null},{"metadata":{"id":"M2t_S_KUdqjY"},"cell_type":"markdown","source":"<a id=\"13\"></a>\n### Logistic Regression","execution_count":null},{"metadata":{"id":"ztUAXHiQXKIq","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a regularisation hyperparameter 'c', that we will grid search over.","execution_count":null},{"metadata":{"id":"pC1QRdLBXvzt","outputId":"e9ac29ab-2842-464f-c22a-7a5a016ef52c","trusted":true},"cell_type":"code","source":"for c in [0.01, 0.02, 0.05, 0.25, 0.5, 0.75, 1]:\n\n  lr = LogisticRegression(C=c, max_iter=1000)\n\n  print (f'At C = {c}:-', end=' ')\n\n  fit_and_test(lr, X_train_onehot, y_train, X_test_onehot, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{"id":"3KsdRwnRcFNd","outputId":"bb728917-879b-42cf-97bb-c068cd662d46","trusted":true},"cell_type":"code","source":"lr_best = LogisticRegression(C=0.05, max_iter=1000)\nfit_and_test(lr_best, X_train_onehot, y_train, X_test_onehot, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD-Regression is also a simple linear model that is basically regression as well. The only difference is in the training algorithm. Logistic Regression employs any of the ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers to help its parameter learn better values. SGDRegressor, as its name suggests, makes use of the more commonly known Stochastic gradient descent algorithm. The same is used by neural networks in their backpropagation process as well.","execution_count":null},{"metadata":{"id":"Zd3zLVhod3af"},"cell_type":"markdown","source":"<a id=\"14\"></a>\n### SGDRegressor","execution_count":null},{"metadata":{"id":"A0klnh0BeHzl","outputId":"e8a951ca-a91e-4bc0-92e3-b4c594300a22","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\n\nfit_and_test(sgd, X_train_onehot, y_train, X_test_onehot, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"y9CqFX21s6BT"},"cell_type":"markdown","source":"<a id=\"15\"></a>\n## K-Nearest Neighbours","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"KNN is a very simple model that simply stores its training data as points in a feature space. For making predictions for any new data point, it maps that data point onto that stored feature space, selects a certain number of closest neighbours, and returns the average value of them.\n\nThis number of neighbours is a hyperparameter, which we will grid search over.","execution_count":null},{"metadata":{"id":"M9HZVEOntAiQ","outputId":"c09ced7f-cf58-43e3-ccfc-269030830df8","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nneighbours = [10, 20, 50, 100, 500]\n\nfor k in neighbours:\n\n  knn = KNeighborsClassifier(n_neighbors=k)\n\n  print (f'At K = {k}:-', end=' ')\n\n  fit_and_test(knn, X_train_onehot, y_train, X_test_onehot, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{"id":"k0w2IgOTtyCX","outputId":"5f486937-1b8a-4db6-da72-951097e2f1ac","trusted":true},"cell_type":"code","source":"knn_best = KNeighborsClassifier(n_neighbors=50)\n\nfit_and_test(knn_best, X_train_onehot, y_train, X_test_onehot, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"mf2GKfukfO47"},"cell_type":"markdown","source":"<a id=\"16\"></a>\n## Support Vector Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (look at the below snapshot).\n\nRead more - https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/","execution_count":null},{"metadata":{"id":"-1ebaOQEfJCh","trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"id":"YGorNfJpBLkd","outputId":"f2d89ed4-1e60-4ee8-b8b3-33db2f3d952b","trusted":true},"cell_type":"code","source":"for c in [0.01, 0.02, 0.05, 0.25, 0.5, 0.75, 1]:\n\n  svc = LinearSVC(C=c, max_iter=5000)\n\n  print (f'At C = {c}:-', end=' ')\n\n  fit_and_test(svc, X_train_onehot, y_train, X_test_onehot, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scikit-Learn also provides a classification_report where we can obtain all the evaluation metrics for each class in our data.","execution_count":null},{"metadata":{"id":"rZ3GXSA3Lx-H","trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"id":"2S7k8hrRfklF","outputId":"a5955b0c-cebf-46eb-d80f-4131b3528e36","trusted":true},"cell_type":"code","source":"svc_best = LinearSVC(max_iter=5000, C=0.01)\n\nsvc_best.fit(X_train_onehot, y_train)\ny_hat = svc_best.predict(X_test_onehot)\n\nreport = classification_report(y_test, y_hat, output_dict=True)\n\nprint('positive: ', report['1'])\nprint('negative: ', report['0'])","execution_count":null,"outputs":[]},{"metadata":{"id":"DROh-9bVO4_Q"},"cell_type":"markdown","source":"<a id=\"17\"></a>\n## WordCount Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, we will see another way of encoding text data into numbers. Instead of simply indicating the presence or absence of any word, we will represent its exact count it the text. We simply have to set binary = False in the CountVectorizer method that we used earlier.","execution_count":null},{"metadata":{"id":"33wH9wCmPEwK","trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(binary=False)\n\nvectorizer.fit(X_train)\n\nX_train_wc = vectorizer.transform(X_train)\n\nX_test_wc = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB()\nfit_and_test(mnb, X_train_wc, y_train, X_test_wc, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"2qbJ_WwlPOCi","outputId":"e0dbc8e0-888a-4371-add6-b3057dfea1be","trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.05, max_iter=1000)\nfit_and_test(lr, X_train_wc, y_train, X_test_wc, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q3rrmXh6Qhlm","outputId":"332aa15b-3d3a-4e49-b75b-6c15629c8be3","trusted":true},"cell_type":"code","source":"svc = LinearSVC(max_iter=5000, C=0.01)\nfit_and_test(svc, X_train_wc, y_train, X_test_wc, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"cgOkLVPELV0T"},"cell_type":"markdown","source":"<a id=\"18\"></a>\n## n-gram features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will go to the next level now. Inplace of using each word as a feature, we can use combinations of words as features. Such representation will help us to capture the information about which words appear together and how they affect the overall sentiment. \n\nFor example, consider the text \"That action scene was terribly enthrilling\". \n\nNow, a model based on a plain one-word BOW models may view the word \"terribly\" as a negative indicator, but a bi-gram data (n=2) will correctly interpret the usage of that word in its context \"terribly enthrilling\". Note that  we are still not considering any review text in its entire sequence, but still n-grams representations have more contextual information comparitively.","execution_count":null},{"metadata":{"id":"IY9EzybLLQw-","trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n\nvectorizer.fit(X_train)\n\nX_train_2gram = vectorizer.transform(X_train)\n\nX_test_2gram = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting ngram_range to (1, 2) creates both single word and two consecutive word features. If one only wants to have bi-gram fearures, they have to set the range to (2, 2).","execution_count":null},{"metadata":{"id":"40bFCznHNP-Z","outputId":"e72ddcfd-c508-42b6-d0b6-6910400b1c5a","trusted":true},"cell_type":"code","source":"for c in [0.01, 0.02, 0.05, 0.25, 0.5, 0.75, 1]:\n\n  lr = LogisticRegression(C=c, max_iter=1000)\n\n  print (f'At C = {c}:-', end=' ')\n\n  fit_and_test(lr, X_train_2gram, y_train, X_test_2gram, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{"id":"bkHFtlMCOKWl","outputId":"96a76231-f41b-440d-cf3e-b694a0080ce6","trusted":true},"cell_type":"code","source":"for c in [0.01, 0.02, 0.05, 0.25, 0.5, 0.75, 1]:\n\n  svc = LinearSVC(C=c, max_iter=5000)\n\n  print (f'At C = {c}:-', end=' ')\n\n  fit_and_test(svc, X_train_2gram, y_train, X_test_2gram, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{"id":"SkhT-aElSer0"},"cell_type":"markdown","source":"<a id=\"19\"></a>\n## tf-idf features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The last BOW represenation that we will consider is term frequency–inverse document frequency (tf-idf), that indicates how important a word is to a document in a collection or corpus. ","execution_count":null},{"metadata":{"id":"S9i3-wvESmUf","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X_train)\n\nX_train_tf = vectorizer.transform(X_train)\n\nX_test_tf = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"DtJXgscXS6i1","outputId":"74bab2c9-53f4-447d-ed51-d69677b3f812","trusted":true},"cell_type":"code","source":"for c in [0.01, 0.02, 0.05, 0.25, 0.5, 0.75, 1]:\n\n  lr = LogisticRegression(C=c, max_iter=1000)\n\n  print (f'At C = {c}:-', end=' ')\n  \n  fit_and_test(lr, X_train_tf, y_train, X_test_tf, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{"id":"UzaptgqDTDrw","outputId":"52f74a7e-6fed-4a1f-b5ca-37690b57c4cc","trusted":true},"cell_type":"code","source":"for c in [0.01, 0.02, 0.05, 0.25, 0.5, 0.75, 1]:\n\n  svc = LinearSVC(C=c, max_iter=5000)\n\n  print (f'At C = {c}:-', end=' ')\n\n  fit_and_test(svc, X_train_tf, y_train, X_test_tf, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{"id":"SSANofEgTcbE"},"cell_type":"markdown","source":"<a id=\"20\"></a>\n# Deep Learning models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In all the deep learning models, we will have to make use of textual data sequentially. Each word can be represented by a vector of a certain length. The representation has to be meaningful in such a way that each vector captures a dimension of the word's meaning, and semantically similar words have similar vectors.\n\nRead more - https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf\n\nThere are two ways to obtain these vectors (also called word-embeddings):-\n\n(1) We can simply create a word-dictionary of all the existing words in our data, where each word is mapped with a number. Then we create an embedding matrix with the dimensions (num_words, length_of_each_embedding_vector). This matrix is essentially a look-up table, where for every word's index number in the dictionary, the matrix returns the embedding vector for that word. During the training process, the contents of this matrix are updated as well.\n\n(2) In this method too, we have an embedding matrix. But this time, the matrix contains word-embeddings that are already trained on some other corpus. We may choose to either keep the values of this matrix forzen during the training process, or train it to tweak the values a little so as to suit our data better.","execution_count":null},{"metadata":{"id":"Bf6YYiLbUYgA"},"cell_type":"markdown","source":"<a id=\"21\"></a>\n## Custom Word Embeddings","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will use the Tokenizer method of the Tensorflow package. It creates a word-dictionary and then maps the words to their index numbers. It has another functionality, using which we will restrict our vocabulary to the most frequent 5000 words in the data.","execution_count":null},{"metadata":{"id":"OaGU8DxbcyA9","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nMAX_NUM_WORDS = 5000\n\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\nX_train.extend(X_test)\ntokenizer.fit_on_texts(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the 20 most frequent words in our data.","execution_count":null},{"metadata":{"id":"puUD5Oqqg2lm","outputId":"7074beb4-5a59-4846-f446-4f521fd9da23","trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\n\nprint([(w, i) for w, i in word_index.items()] [:20])","execution_count":null,"outputs":[]},{"metadata":{"id":"DDe1ikRfhGyU","trusted":true},"cell_type":"code","source":"num_words = min(MAX_NUM_WORDS, len(word_index) + 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For encoding words with index numbers, I have done so in batches, since the RAM size was not sufficient for operating on the entire data at once.","execution_count":null},{"metadata":{"id":"tzkHbpJDUoAG","trusted":true},"cell_type":"code","source":"batch_size = 128\nl = len(X_train)\n\ni = 0\nwhile (i <= l-1):\n\n  if (i + batch_size) >= (l-1):\n    X_train[i:] = tokenizer.texts_to_sequences(X_train[i:])\n  \n  else:\n    X_train[i:i+batch_size] = tokenizer.texts_to_sequences(X_train[i:i+batch_size])\n  \n  i += batch_size\n\nX_train, X_test = X_train[:l//2], X_train[l//2:]","execution_count":null,"outputs":[]},{"metadata":{"id":"ebnMsHvsUtUx","outputId":"2531880b-4a35-4e82-8714-608344a94957","trusted":true},"cell_type":"code","source":"print(X_train[10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For faster and more convenient training of Neural Network models, we usually keep the training data uniform. For that, we will need to have the same sequence length for all our reviews. So, we will pad the sequences that are short and truncate the ones that are long. To choose a good sequence length, let's see the distribution of the length of reviews in our data.","execution_count":null},{"metadata":{"id":"uAhcKnr7fuIZ","outputId":"dd3ae32c-8958-4ae2-aa0f-0631755797ae","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nseq_lengths = [len(seq) for seq in X_train]\n\nplt.figure(figsize=(10, 4))\nplt.hist(seq_lengths)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most sequences have a length less than 200. LSTMs generally perform well up until sequences with 100 steps. I tried training LSTM models with various sequence lengths, and found that the number 120 gave good results.","execution_count":null},{"metadata":{"id":"NDwhQhKNW06N","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQ_LEN = 120\n\nX_train = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding='pre', truncating='pre')\n\nX_test = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding='pre', truncating='pre')","execution_count":null,"outputs":[]},{"metadata":{"id":"AX6A4T0fhQ8y","outputId":"ac970978-038c-4e3e-ce15-596435daef13","trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"tCeONtEBUWV0","outputId":"a9074794-cfbe-41ff-a574-214fdfc169b5","trusted":true},"cell_type":"code","source":"print(X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"CbRpucDFhpq1","trusted":true},"cell_type":"code","source":"import numpy as np\ny_train = np.array(y_train)\ny_test = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use two kinds of Neural Network Architectures - RNNs, and CNNs. Also, in RNNs, LSTMs will be used in particular. I am presuming you are familiar with the working of these models.\n\nIn case you aren't, here is a good place to start (follow the order):\n\n(1) https://pathmind.com/wiki/neural-network\n\n(2) https://pathmind.com/wiki/lstm\n\n(3) https://pathmind.com/wiki/convolutional-network","execution_count":null},{"metadata":{"id":"bY0TyNmED6G1"},"cell_type":"markdown","source":"<a id=\"22\"></a>\n## Recurrent Neural Networks","execution_count":null},{"metadata":{"id":"uXeb07LxZ-Dk"},"cell_type":"markdown","source":"<a id=\"23\"></a>\n### Vanilla LSTM network","execution_count":null},{"metadata":{"id":"Uy8pmurdXodo","trusted":true},"cell_type":"code","source":"#Embedding matrix first dimension\nV = num_words\n\n#Embedding matrix second dimension\nD = 50\n\n#Hidden state length\nM = 100\n\n#Number of steps\nT = MAX_SEQ_LEN","execution_count":null,"outputs":[]},{"metadata":{"id":"4pnsNxEVkdXL","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import Embedding, LSTM\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"id":"ADWNaXvYYGva","outputId":"26ca375f-0594-47b8-ce09-0e8f79fd0d7b","trusted":true},"cell_type":"code","source":"i = Input(shape=(T,))\nx = Embedding(V, D)(i)\nx = LSTM(M)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have used a callback method where the model checks the evalution metric over the validation set after every epoch, and if the metric is better than that achieved in any of the previous epochs, then the model (with its weights) right after that epoch are saved. I have found this to be a convenient way to prevent over-fitting.","execution_count":null},{"metadata":{"id":"Lv-Si2WBDoIL","trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code for compiling, training, and testing is the same for all models in this section.","execution_count":null},{"metadata":{"id":"PjDqQnY6Cuh2","trusted":true},"cell_type":"code","source":"def train_and_test(model, label, batch_size, epochs):\n\n  save_at = label + \".hdf5\"\n\n  save_best = ModelCheckpoint(save_at, monitor='val_loss', verbose=1, \n                              save_best_only=True, save_weights_only=False, mode='min')\n\n  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n  s = len(X_test)//2\n\n  model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n            validation_data=(X_test[:s], y_test[:s]), callbacks=[save_best])\n\n  trained_model = load_model(save_at)\n  y_hat = model.predict(X_test[s:])\n  y_hat  = (y_hat > 0.5)*1\n\n  print('\\n')\n  print('-'*100)\n  print(f\"Test Results for '{save_at}' model\")\n  print('accuracy:', accuracy_score(y_test[s:], y_hat))\n  print('f1_score:', f1_score(y_test[s:], y_hat))","execution_count":null,"outputs":[]},{"metadata":{"id":"bKWXTpwdED1_","outputId":"508b0ff3-0d91-4d0b-ceb5-ac8fa9664a75","trusted":true},"cell_type":"code","source":"train_and_test(model, 'simple_lstm', batch_size=128, epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"id":"F5mZqB-m3PS8"},"cell_type":"markdown","source":"<a id=\"24\"></a>\n### Using all Hidden States","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will create the LSTM model, where we will concatenate the hidden states from all the steps,and find the average of each unit in the hidden state vector, using the GlobalAvergePooling layer. One can also try using the GlobalMaxPooling layer instead.\n\nIn the previous LSTM model, only the last hidden state was used for making classification. Even though, the last hidden state is dependent on all the previous hidden states as well, the dependence is usually weak with hidden states from the initial steps. By concatenating them, our model has access to all the hidden states for making classifications.\n\nThis can be very easily done by setting 'return_sequences' to 'True' in the LSTM layer.","execution_count":null},{"metadata":{"id":"vJrs-Ykg_V5r","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import GlobalAveragePooling1D","execution_count":null,"outputs":[]},{"metadata":{"id":"eUvP7EzukGNu","outputId":"29815a61-259b-4c44-a55a-f97c772f59a0","trusted":true},"cell_type":"code","source":"i = Input(shape=(T,))\nx = Embedding(V, D)(i)\nx = LSTM(M, return_sequences=True)(x)\n\nx = GlobalAveragePooling1D()(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"kzjwnWlgE3au","outputId":"b807f207-c2c2-404b-fdbb-e8cb9cd369f4","trusted":true},"cell_type":"code","source":"train_and_test(model, 'lstm_all_hidden_states', batch_size=128, epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"id":"qTtSR9OV-oGW"},"cell_type":"markdown","source":"<a id=\"25\"></a>\n### Bidirectional LSTM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The bidirectional model has two hidden states, one over which the sequence is run in its original sense, and another hidden state, over which the sequence is run backwards. The two states are then concatenated.","execution_count":null},{"metadata":{"id":"6kTgzxTm7w52","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"id":"KLVmU1ZlArv4","outputId":"644747f7-7f0c-4747-8677-5287b7315843","trusted":true},"cell_type":"code","source":"i = Input(shape=(T,))\nx = Embedding(V, D)(i)\n\nx = Bidirectional(LSTM(M, return_sequences=True))(x)\n\nx = GlobalAveragePooling1D()(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"5mRAlq9_FWv7","outputId":"75e758ad-659a-4816-bd4b-7d64fd142639","trusted":true},"cell_type":"code","source":"train_and_test(model, 'lstm_bidirectional', batch_size=128, epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"id":"IqrEfJHiDukg"},"cell_type":"markdown","source":"<a id=\"26\"></a>\n## Convolutional Neural Networks","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"CNNs are more commonly used for image data (Conv2D layers). For time sequence data which is one-dimensional in nature, we can use a Conv1D layer.","execution_count":null},{"metadata":{"id":"1h-eQCp5Doog","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"id":"O6xl3hkgEUTw","outputId":"81cc8b9c-1c90-4aa6-afb7-45dc06e93a14","trusted":true},"cell_type":"code","source":"i = Input(shape=(T,))\nx = Embedding(V, D)(i)\n\nx = Conv1D(16, 3, activation='relu')(x)\nx = MaxPooling1D()(x)\nx = BatchNormalization()(x)\n\nx = Conv1D(16, 3, activation='relu')(x)\nx = MaxPooling1D()(x)\nx = BatchNormalization()(x)\n\nx = Conv1D(16, 3, activation='relu')(x)\nx = MaxPooling1D()(x)\nx = BatchNormalization()(x)\n\nx = GlobalAveragePooling1D()(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"cYG40FbLFxna","outputId":"3cefae37-29dc-4028-e1da-c1fb22eeba5d","trusted":true},"cell_type":"code","source":"train_and_test(model, 'cnn', batch_size=128, epochs=8)","execution_count":null,"outputs":[]},{"metadata":{"id":"L3lxp69RavwN"},"cell_type":"markdown","source":"<a id=\"27\"></a>\n## Pre-trained Glove Embeddings","execution_count":null},{"metadata":{"id":"5AMx8RRJzv4J","trusted":true},"cell_type":"code","source":"! mkdir glove","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the popular Glove word embeddings. The following snippet can be used for downloading them. This server however is not very stable. So, I have shared my Google Drive link of the embeddings.","execution_count":null},{"metadata":{"id":"NfU81EOYbUxQ","trusted":true},"cell_type":"code","source":"# import zipfile, io\n\n# data_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n# r = requests.get(data_url)\n# z = zipfile.ZipFile(io.BytesIO(r.content))\n# z.extractall('glove/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! conda install -y gdown","execution_count":null,"outputs":[]},{"metadata":{"id":"QvUXOi3T1r-x","outputId":"8eda62b6-b3f1-47a3-90b2-9412e6569a48","trusted":true},"cell_type":"code","source":"import gdown\n\nurl = \"https://drive.google.com/uc?id=18WgSks6St7KVDgY4Y2e29dHhEcD-9SWK\"\n\noutput = 'glove/glove.6B.100d.txt'\n\ngdown.download(url, output, quiet=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Glove embeddings of lengths 100, 200, and 300 are avilable. I have used the first one.","execution_count":null},{"metadata":{"id":"aeQBzoONazcg","trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 100\n\nembeddings_index = {}\nwith open('glove/glove.6B.100d.txt') as f:\n  for line in f:\n    word, coeff = line.split(maxsplit=1)\n    coeff = np.fromstring(coeff, 'f', sep=' ')\n    embeddings_index[word] = coeff","execution_count":null,"outputs":[]},{"metadata":{"id":"n_cSco52dZb6","trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  #(num_words, length of each word embedding)\n\nfor word, i in word_index.items():\n  if i >= num_words:\n    continue\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:               # words not found in embedding index will be all-zeros.\n    embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"id":"KkNGecLXkzB_"},"cell_type":"markdown","source":"<a id=\"28\"></a>\n### Freezing the Embedding layer","execution_count":null},{"metadata":{"id":"bJYcCE_-mU7H","trusted":true},"cell_type":"code","source":"from tensorflow.keras.initializers import Constant","execution_count":null,"outputs":[]},{"metadata":{"id":"g32veGCpBLCc","trusted":true},"cell_type":"code","source":"#Embedding matrix second dimension\nD = EMBEDDING_DIM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The embedding matrix be directly loaded into the embedding layer. We can freeze it by setting 'trainable' to 'False.","execution_count":null},{"metadata":{"id":"V23a7vbPkcHr","outputId":"604a627f-cc36-4902-9c19-7f66cec12239","trusted":true},"cell_type":"code","source":"i = Input(shape=(T,))\n\nx = Embedding(V, D, \n              embeddings_initializer=Constant(embedding_matrix),\n              trainable=False)(i)\n\nx = LSTM(M)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"qhUM3cI9GFOe","outputId":"d8c28c04-e72b-48b8-9fbb-a020248da3ae","trusted":true},"cell_type":"code","source":"train_and_test(model, 'lstm_glove', batch_size=128, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"xAzWO8FYAy01"},"cell_type":"markdown","source":"<a id=\"29\"></a>\n### FIne-tuning the the Glove Embeddings","execution_count":null},{"metadata":{"id":"0uLYKjBhAl7A","outputId":"6ba564f3-c6a0-4be2-9351-266000d02d8a","trusted":true},"cell_type":"code","source":"i = Input(shape=(T,))\n\nx = Embedding(V, D, \n              embeddings_initializer=Constant(embedding_matrix),\n              trainable=True)(i)\n\nx = LSTM(M)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"yO7sy5pdGVoI","outputId":"f5a33abd-0954-4ebb-bd23-7c27fd909367","trusted":true},"cell_type":"code","source":"train_and_test(model, 'lstm_glove_trainable', batch_size=128, epochs=10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}