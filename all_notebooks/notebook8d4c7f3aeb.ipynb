{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n'''import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T11:10:59.243058Z","iopub.execute_input":"2021-07-30T11:10:59.2436Z","iopub.status.idle":"2021-07-30T11:28:18.782564Z","shell.execute_reply.started":"2021-07-30T11:10:59.243481Z","shell.execute_reply":"2021-07-30T11:28:18.780463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install packages\n!pip install nltk --user\n!pip install owlready2 --user\n!pip install pronto --user\n!pip install ipynb-py-convert --user\n!pip install langdetect --user\n!pip install contractions --user\n!pip install inflect --user\n!pip install num2words --user\n!pip install tables --user\n!pip install h5py --user\n!pip install sentence-transformers --user\n!pip install pandas --user\n!pip install tqdm --user\n!pip install seaborn --user\n!pip install numpy --user\n!pip install scipy --user\n!pip install matplotlib --user\n!pip install numpy --user\n!pip install bottleneck --user\n!pip install pandarallel --user\n!pip install wordcloud --user\n!pip install  --user spacy\n!pip install --user https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:08:40.259142Z","iopub.execute_input":"2021-07-30T14:08:40.259501Z","iopub.status.idle":"2021-07-30T14:12:24.729382Z","shell.execute_reply.started":"2021-07-30T14:08:40.259472Z","shell.execute_reply":"2021-07-30T14:12:24.728135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nimport glob\nimport itertools\nimport json\nimport pickle\nimport os\nimport re\n\nimport bs4\nimport contractions\nimport inflect\nfrom langdetect import detect\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk import tokenize\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np\nimport pandas as pd\nfrom pandarallel import pandarallel\nfrom PIL import Image\nimport requests\nimport seaborn as sns\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Span\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Initialize pandarallel\npandarallel.initialize(use_memory_fs=False,nb_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:12:24.733083Z","iopub.execute_input":"2021-07-30T14:12:24.733472Z","iopub.status.idle":"2021-07-30T14:12:41.782839Z","shell.execute_reply.started":"2021-07-30T14:12:24.733427Z","shell.execute_reply":"2021-07-30T14:12:41.780786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pandas options\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)\npd.options.mode.chained_assignment = None \n\ntqdm.pandas()\n\n# make temp dir to save intermidiate data\nif not os.path.exists('../data'):\n    os.mkdir('../data')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:12:50.506377Z","iopub.execute_input":"2021-07-30T14:12:50.506777Z","iopub.status.idle":"2021-07-30T14:12:50.515317Z","shell.execute_reply.started":"2021-07-30T14:12:50.506743Z","shell.execute_reply":"2021-07-30T14:12:50.513735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Help functions and class\n# help function to generate file path\ndef filepath(*args):\n    if len(args) < 1:\n        return None\n    elif len(args) == 1:\n        return args[0]\n    else:\n        return f'{args[0]}/{filepath(*args[1:])}'\n\n# Add time bar to loop\ndef addtimebar(L, threshold=1000):\n    if len(L) > threshold:\n        return tqdm(L)\n    else:\n        return L\n\n# File Reader Class\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            try:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            except KeyError:\n                pass\n                    \n            # Body text\n            try:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            except KeyError:\n                pass\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n\n# Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\n## composition function\n## example: compose(f1,f2,f3)(x, y) = f3(f2(f1(x, y)))\ndef compose(*funcs):\n    *funcs, penultimate, last = funcs\n    if funcs:\n        penultimate = compose(*funcs, penultimate)\n    return lambda *args: penultimate(last(*args))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:12:53.015138Z","iopub.execute_input":"2021-07-30T14:12:53.015567Z","iopub.status.idle":"2021-07-30T14:12:53.033459Z","shell.execute_reply.started":"2021-07-30T14:12:53.015531Z","shell.execute_reply":"2021-07-30T14:12:53.031539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# file path\npath = \"/kaggle/input/CORD-19-research-challenge/\"  # may need to change when submit to kaggle\nmeta = \"metadata.csv\"\n# path for all json files\nall_jsons = glob.glob(filepath(path, '**', '*.json'), recursive=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:12:54.731449Z","iopub.execute_input":"2021-07-30T14:12:54.731837Z","iopub.status.idle":"2021-07-30T16:09:56.64504Z","shell.execute_reply.started":"2021-07-30T14:12:54.731803Z","shell.execute_reply":"2021-07-30T16:09:56.642771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data.frame for meta data\nmeta_df = pd.read_csv(filepath(path, meta),\n                      dtype={'pubmed_id': str,\n                             'Microsoft Academic Paper ID': str,\n                             'doi': str,\n                             'journal':str\n                             },\n                     low_memory=False)\nprint(len(meta_df)) # number of lines in meta_df_all\nmeta_df.head(n=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:09:56.647971Z","iopub.execute_input":"2021-07-30T16:09:56.648404Z","iopub.status.idle":"2021-07-30T16:10:31.71295Z","shell.execute_reply.started":"2021-07-30T16:09:56.648366Z","shell.execute_reply":"2021-07-30T16:10:31.711749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look the first line of text data\nfirst_row = FileReader(all_jsons[0])\nprint(first_row)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:10:31.714912Z","iopub.execute_input":"2021-07-30T16:10:31.715264Z","iopub.status.idle":"2021-07-30T16:10:31.733471Z","shell.execute_reply.started":"2021-07-30T16:10:31.715231Z","shell.execute_reply":"2021-07-30T16:10:31.731999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the text data into DataFrame\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'publish_time':[], 'journal': [], 'abstract_summary': []}\nfor entry in addtimebar(all_jsons):\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add publish time\n    try:\n        publish_time = get_breaks(meta_data['publish_time'].values[0], 40)\n        dict_['publish_time'].append(publish_time)\n    # if publish time was not provided\n    except Exception as e:\n        dict_['publish_time'].append(meta_data['publish_time'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title',  'journal', 'publish_time', 'abstract_summary'])\ndf_covid.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T16:10:31.735764Z","iopub.execute_input":"2021-07-30T16:10:31.736168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save data\ndf_covid.to_pickle('../data/df_kaggle_all.pkl')\n# load saved data\n# with open('../data/df_kaggle_all.pkl', 'rb') as fp:\n#    df_covid = pickle.load(fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to check if text of certain column in dataframe is written in certain language \ndef is_lang(row, item, lang, dropNA=True):\n    if (row[item] != None and row[item] != '' and row[item] != 'None' and isinstance(row[item], str)):\n        try:\n            return detect(row[item]) == lang\n        except Exception as e:\n            #print(\"Non-readable entity will be droped from data.frame\")\n            return False\n    else:\n        return not dropNA\n\n# select article written in certain language \ndef select_article_lang_multi(df, basedon='abstract', lang='en'):\n    return df[df.parallel_apply(lambda text: is_lang(text, basedon, lang), axis=1)]\n\ndf_covid_eng = select_article_lang_multi(df_covid)\nprint('Number of English Articles: {}/{}'.format(len(df_covid_eng), len(df_covid)))\ndf_covid_eng.head(n=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save intermidiate data\ndf_covid_eng.to_pickle('../data/df_kaggle_all_eng.pkl')\n# load saved data\n# with open('../data/df_kaggle_all_eng.pkl', 'rb') as fp:\n#    df_covid_eng = pickle.load(fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre-processing functions\n## text level processors\ndef replace_brackets_with_whitespace(text):\n    text = text.replace('(', '')\n    text = text.replace(')', '')\n    text = text.replace('[', '')\n    text = text.replace(']', '')\n    return text\n\ndef replace_contractions(text):\n    return contractions.fix(text)\n\n# remove special characters\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|’||“|\\?|%|>|<', '', text)\n    t = re.sub('/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\n## word level processors:\ndef to_lowercase(word):\n    return word.lower()\n\ndef do_stemming(stemmer):\n    return lambda word: stemmer.stem(word)\n\ndef do_lemmatizing(lemmatizer):\n    return lambda word: lemmatizer.lemmatize(word, pos='v') \n\n# help function to test if word is stopword\ndef is_stopword(word):\n    return word in stopwords.words('english')\n\n# function to process word\ndef process_word_by(word_cleanner, uniqueYN):\n    def cond(word):\n        return (len(word) > 1 and\n                not is_stopword(word) and\n                not word.isnumeric() and\n                word.isalnum() and\n                word != len(word) * word[0])\n\n    def clean_byword(text):\n        return list(take_unique(uniqueYN)((word_cleanner(word) for word in text if cond(word))))\n\n    return clean_byword\n\n# function to decide making a set (unique words) from text or not\ndef take_unique(YN):\n    return set if YN else lambda x:x\n\n# function to pre_processing the text\n## compose text and word processors by combine every individual processor together \ntext_processor = compose(replace_brackets_with_whitespace, replace_contractions, strip_characters)\nword_processor = compose(to_lowercase, do_lemmatizing(WordNetLemmatizer()), do_stemming(PorterStemmer())) # it is crucial to do stemming after lemmatization\n\n## pre_processing function taking a dataframe and text and word processor functions as input and clean the text and tokenize the specified column\ndef pre_processing(df, text_tools, word_tools):\n    def inner(col, uniqueYN=False):\n        return df[col].parallel_apply(text_tools).parallel_apply(nltk.word_tokenize).parallel_apply(process_word_by(word_tools,uniqueYN=uniqueYN))\n    return inner","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort by publish time\ntokenized_df = df_covid_eng.sort_values(by='publish_time', ascending=False)\ntokenized_df.head(n=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# created processor function with chosen text and work processors and apply it to all articles to clean and tokenize all abstracts\nprocessor = pre_processing(tokenized_df, text_processor, word_processor)\ntokenized_df['abstract_token'] = processor('abstract')\n\n# reset index (this is necessary for cosine similarity search)\ntokenized_df = tokenized_df.reset_index(drop=True)\n\n# Our processor function is a generic procedure to clean and tokenize any column with user specified column name, such as 'abstract' or 'body_text'\n# Because processing body_text takes too long, we only process abstract\n# tokenized_df['body_text_token'] = processor('body_text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the dataframe to ../data/\ntokenized_df.to_pickle('../data/df_kaggle_all_eng_tokenized.pkl')\n# with open('../data/df_kaggle_all_eng_tokenized.pkl', 'rb') as fp:\n#    tokenized_df = pickle.load(fp)\n\n# have a look at the head of the cleanned and tokenized abstract column\ntokenized_df.head()['abstract_token']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_nK_words(corpus, K=1, n=None):\n    vec1 = CountVectorizer(max_df=0.7,stop_words=stopwords.words('english'), ngram_range=(K,K),  \n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_nK_words(corpus, K=1, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\n#Barplot of most freq words\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Top bi-grams\ntop2_words = get_top_nK_words(corpus, K=2, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)\n#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)\nfig = h.get_figure()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top3_words = get_top_nK_words(corpus, K=3, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)\n#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)\nfig = j.get_figure()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute TF-IDF scores for word vectors\ndef tfidf_(df):\n    myvectorizer = TfidfVectorizer()\n    vectors = myvectorizer.fit_transform(df['abstract_token'].parallel_apply(lambda x:' '.join(x))).toarray()\n    feature_names = myvectorizer.get_feature_names()\n    veclist = vectors.tolist()\n    out_tfidf = pd.DataFrame(veclist, columns=feature_names)\n    return out_tfidf\n\ntfidf_(tokenized_df[:20]).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using sklearn is 10 times faster than self-written script\n# extract key-words with tfidf score\ntfidf_scores_df = tfidf_(tokenized_df[:20])\nN = 15 # Number of min/max values \nu = np.argpartition(tfidf_scores_df, axis=1, kth=N).values\nv = tfidf_scores_df.columns.values[u].reshape(u.shape)\nmaxdf = pd.DataFrame(v[:,-N:]).rename(columns=lambda x: f'Max{x+1}')\nmaxdf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert query token to vector\ndef gen_vector_T(tokens):\n    Q = np.zeros((len(vocabulary)))    \n    x= tfidf.transform(tokens)\n    #print(tokens[0].split(','))\n    for token in tokens[0].split(','):\n        #print(token)\n        try:\n            ind = vocabulary.index(token)\n            Q[ind]  = x[0, tfidf.vocabulary_[token]]\n        except:\n            pass\n    return Q\n\n# calculate cosine similarity\ndef cosine_sim(a, b):\n    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n    return cos_sim\n\n# Function to get transformed tfidf model\ndef tfidf_tran(mydf):\n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform(mydf['abstract_token'].parallel_apply(lambda x:' '.join(x)))\n    return vectors\n\n# Define wordLemmatizer\n# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\ndef wordLemmatizer(data):\n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['J'] = wn.ADJ\n    tag_map['V'] = wn.VERB\n    tag_map['R'] = wn.ADV\n    file_clean_k =pd.DataFrame()\n    for index,entry in enumerate(data):\n        \n        # Declaring Empty List to store the words that follow the rules for this step\n        Final_words = []\n        # Initializing WordNetLemmatizer()\n        word_Lemmatized = WordNetLemmatizer()\n        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n        for word, tag in nltk.pos_tag(entry):\n            # Below condition is to check for Stop words and consider only alphabets\n            if len(word)>1 and word not in stopwords.words('english') and word.isalpha():\n                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n                Final_words.append(word_Final)\n            # The final processed set of words for each iteration will be stored in 'text_final'\n                file_clean_k.loc[index,'Keyword_final'] = str(Final_words).lower()\n                file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n    return file_clean_k","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_similarity_T(k, query, text_token_df):\n    preprocessed_query  = re.sub(\"\\W+\", \" \", query).strip()\n    tokens = nltk.word_tokenize(text_processor(str(preprocessed_query).lower()))\n    tokens = [word_processor(token) for token in tokens if \n              len(token) > 1 and\n              not is_stopword(token) and\n              not token.isnumeric() and\n              token.isalnum() and\n              token != len(token) * token[0]]\n    q_df = pd.DataFrame(columns=['q_clean'])\n    q_df.loc[0,'q_clean'] =tokens\n    q_df['q_clean'] = wordLemmatizer(q_df.q_clean)\n    d_cosines = []\n    #print(q_df['q_clean'])\n    query_vector = gen_vector_T(q_df['q_clean'])\n    #print(query_vector)\n    #print(q_df['q_clean'])\n    #print(sum(query_vector))\n    for d in tfidf_tran.A:\n        d_cosines.append(cosine_sim(query_vector, d))\n    #print(d_cosines)              \n    out = np.array(d_cosines).argsort()[-k:][::-1]\n    d_cosines.sort()\n    #print(out)\n    a = pd.DataFrame()\n    firsttime=True\n    for i,index in enumerate(out):\n        try:\n            a.loc[i, 'Paper ID'] = text_token_df['paper_id'][index]\n            a.loc[i,'Title'] = text_token_df['title'][index]\n            a.loc[i, 'Summary'] = text_token_df['abstract_summary'][index]\n        except KeyError as e:\n            if firsttime:\n                print(\"Fewer matches are found than requested {}\".format(k))\n                firsttime=not firsttime\n                pass\n    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n        a.loc[j,'Score'] = simScore\n    return a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create Vocabulary\nvocabulary = set()\nfor tokens in tokenized_df.abstract_token:\n    vocabulary.update(tokens)\nvocabulary = list(vocabulary) \n\n# Intializating the tfIdf model\ntfidf = TfidfVectorizer(vocabulary=vocabulary)\n\n# Transform the TfIdf model\ntfidf_tran=tfidf.fit_transform(tokenized_df['abstract_token'].parallel_apply(lambda x:' '.join(x)))\n\n# search engine using cosine similarity + TF-IDF\nTFIDF_output = cosine_similarity_T(20000,'SARS-CoV-2 Covid-19 HCoV-19 Covid corona 2019-nCoV sars cov2 ncov wuhan coronavirus pneumonia',tokenized_df)\nTFIDF_output_significant = TFIDF_output[TFIDF_output['Score'] > 0]\nTFIDF_output_significant.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the dataframe to ../data/\nTFIDF_output_significant.to_pickle('../data/TFIDF_output_significant_all.pkl')\n\n# The amount of the most significant search results\nlen(TFIDF_output_significant)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_top = 500\ntop_to_print = 10\n\n#with open('../data/TFIDF_output_significant_all.pkl', 'rb') as fp:\n#    TFIDF_output_significant = pickle.load(fp)\nwith open('../data/df_kaggle_all_eng.pkl', 'rb') as fp:\n    df_covid_eng = pickle.load(fp)\n\ndf_covid_eng.drop_duplicates(subset=['paper_id'], inplace=True)    \nTFIDF_output_significant.drop_duplicates(subset=['Paper ID'], inplace=True)\n\npapers_to_embed = df_covid_eng.loc[df_covid_eng['paper_id'].isin(\n    TFIDF_output_significant['Paper ID'])].copy()\n\nsort_papers = TFIDF_output_significant.loc[\n    TFIDF_output_significant['Paper ID'].isin(\n        papers_to_embed['paper_id'])].sort_values(\n    by='Score', ascending=False)['Paper ID'].to_list()\npapers_to_embed = papers_to_embed.set_index('paper_id').loc[sort_papers].reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas(desc='Combining abstracts and body text')\npapers_to_embed['combined_text'] = papers_to_embed.progress_apply(\n        lambda x: x['abstract'] + ' ' + x['body_text'], axis=1)\n\ntqdm.pandas(desc='Splitting abstracts into sentences')\npapers_to_embed['abstract_sentence'] = papers_to_embed[\n    'abstract'].progress_apply(tokenize.sent_tokenize)\n    \ntqdm.pandas(desc='Splitting papers into sentences')\npapers_to_embed['combined_text_sentence'] = papers_to_embed[\n    'combined_text'].progress_apply(tokenize.sent_tokenize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\nsent_to_embed_abstr = list(itertools.chain(\n    *papers_to_embed['abstract_sentence']))\nsent_to_embed_comb = list(itertools.chain(\n    *papers_to_embed['combined_text_sentence'].iloc[:get_top]))\n\nabstract_embed = np.array(embedder.encode(\n    sent_to_embed_abstr, batch_size=64, show_progress_bar=True))\ncomb_text_embed = np.array(embedder.encode(\n    sent_to_embed_comb, batch_size=64, show_progress_bar=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save intermidiate data in case needed\nnp.save('../data/abstr_data_encodings', abstract_embed)\nnp.save('../data/comb_text_data_encodings', comb_text_embed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions = [\n    ('Evidence of animal infection with SARS-CoV-2 and its transmission to '\n     'other hosts, including the spill-over to humans.')\n]\n\nquestions_embed = np.array(embedder.encode(\n    questions, batch_size=64, show_progress_bar=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_abstr = cosine_similarity(\n    abstract_embed, questions_embed).squeeze()\nsimilarity_comb = cosine_similarity(\n    comb_text_embed, questions_embed).squeeze()\n\nsort_args_abstr = np.argsort(similarity_abstr)[::-1]\nsim_sort_abstr = similarity_abstr[sort_args_abstr]\nsort_args_comb = np.argsort(similarity_comb)[::-1]\nsim_sort_comb = similarity_comb[sort_args_comb]\n\npaper_id_abst = np.array(list(itertools.chain(\n    *papers_to_embed.progress_apply(\n        lambda x: [x['paper_id']] * len(x['abstract_sentence']), \n        axis=1).tolist())))\npaper_id_comb = np.array(list(itertools.chain(\n    *papers_to_embed.iloc[:get_top].progress_apply(\n        lambda x: [x['paper_id']] * len(x['combined_text_sentence']), \n        axis=1).tolist())))\n\ninterest_paper_id_abstr = paper_id_abst[sort_args_abstr]\ninterest_sentences_abstr = np.array(\n    sent_to_embed_abstr)[sort_args_abstr]\ninterest_abstracts = papers_to_embed.set_index('paper_id').loc[\n    interest_paper_id_abstr]['abstract'].tolist()\n\ninterest_paper_id_comb = paper_id_comb[sort_args_comb]\ninterest_sentences_comb = np.array(\n    sent_to_embed_comb)[sort_args_comb]\ninterest_comb_text = papers_to_embed.set_index('paper_id').loc[\n    interest_paper_id_comb]['combined_text'].tolist() \n\nwith open('interesting_papers_based_on_abstract.txt', 'w') as f:\n    for paper, sent, abst, metric in zip(\n        interest_paper_id_abstr, interest_sentences_abstr, interest_abstracts, sim_sort_abstr):\n        _ = f.write('Paper ID: ' + paper + '\\n')\n        _ = f.write('Important sentence: ' + sent + '\\n')\n        # _ = f.write('Associated abstract: ' + abst + '\\n')    \n        _ = f.write('Cosine Similarity metric: ' + '{0:.3f}'.format(metric) + '\\n')\n        _ = f.write('\\n')       \n        \nwith open('interesting_papers_based_on_comb_text.txt', 'w') as f:\n    for paper, sent, comb_text, metric in zip(\n        interest_paper_id_comb, interest_sentences_comb, interest_comb_text, sim_sort_comb):\n        _ = f.write('Paper ID: ' + paper + '\\n')\n        _ = f.write('Important sentence: ' + sent + '\\n')\n        _ = f.write('Cosine Similarity metric: ' + '{0:.3f}'.format(metric) + '\\n')\n        # _ = f.write('Associated body text: ' + comb_text + '\\n')    \n        _ = f.write('\\n')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Results based on abstract:')\nprint('\"\"\"')\nwith open('interesting_papers_based_on_abstract.txt', 'r') as f:\n    print('\\n'.join(f.read().splitlines()[:4*top_to_print]))\nprint('\"\"\"') \nprint('')\nprint('Results based on abstract and body text:')\nprint('\"\"\"')    \nwith open('interesting_papers_based_on_comb_text.txt', 'r') as f:\n    print('\\n'.join(f.read().splitlines()[:4*top_to_print]))\nprint('\"\"\"') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows_to_sample = np.random.randint(len(comb_text_embed), size=1000)\n\nsentences_subset = np.array(sent_to_embed_comb)[rows_to_sample].tolist()\nembeddings_subset = comb_text_embed[rows_to_sample] \n\n# Perform kmean clustering\nnum_clusters = 5\nclustering_model = KMeans(n_clusters=num_clusters)\n_ = clustering_model.fit(embeddings_subset)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = [[] for i in range(num_clusters)]\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    clustered_sentences[cluster_id].append(sentences_subset[sentence_id])\n\nfor i, cluster in enumerate(clustered_sentences):\n    print(\"Cluster \", i+1)\n    print(cluster[:10])\n    print(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_to_read = './interesting_papers_based_on_comb_text.txt'\ncontent = None\nwith open(file_to_read) as f:\n    content = f.readlines()\ncontent = [x.strip() for x in content] \ncontent = [string for string in content if string != \"\"]\ntop_results = content[0:100] # Select the first n elements.\nselected_top_sentences = []\nfor elem in top_results:\n    if elem.startswith('Important sentence:'):\n        selected_top_sentences.append(elem.replace('Important sentence:','').strip())\n# Select the first n sentences.\nselected_top_sentences = selected_top_sentences[0:20]\n\n# Some settings for the plot.\npd.set_option('display.max_colwidth', 200)\n\n# The main idea is to go through a sentence and extract the subject and the object \n# as and when they are encountered.\ndef get_entities(sent):\n    ## chunk 1\n    ent1 = \"\"\n    ent2 = \"\"\n    prv_tok_dep = \"\"  # dependency tag of previous token in the sentence\n    prv_tok_text = \"\"  # previous token in the sentence\n    prefix = \"\"\n    modifier = \"\"\n    for tok in nlp(sent):\n        ## chunk 2\n        # if token is a punctuation mark then move on to the next token\n        if tok.dep_ != \"punct\":\n            # check: token is a compound word or not\n            if tok.dep_ == \"compound\":\n                prefix = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    prefix = prv_tok_text + \" \" + tok.text\n            # check: token is a modifier or not\n            if tok.dep_.endswith(\"mod\") == True:\n                modifier = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    modifier = prv_tok_text + \" \" + tok.text\n            ## chunk 3\n            if tok.dep_.find(\"subj\") == True:\n                ent1 = modifier + \" \" + prefix + \" \" + tok.text\n                prefix = \"\"\n                modifier = \"\"\n                prv_tok_dep = \"\"\n                prv_tok_text = \"\"\n            ## chunk 4\n            if tok.dep_.find(\"obj\") == True:\n                ent2 = modifier + \" \" + prefix + \" \" + tok.text\n            ## chunk 5\n            # update variables\n            prv_tok_dep = tok.dep_\n            prv_tok_text = tok.text\n    return [ent1.strip(), ent2.strip()]\n\n# Relation/Predicate Extraction.\n# The hypothesis is that the predicate is actually the main verb in a sentence.\ndef get_relation(sent):\n  doc = nlp(sent)\n  # Matcher class object\n  matcher = Matcher(nlp.vocab)\n  #define the pattern\n  pattern = [{'DEP':'ROOT'},\n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},\n            {'POS':'ADJ','OP':\"?\"}]\n  matcher.add(\"matching_1\", None, pattern)\n  matches = matcher(doc)\n  k = len(matches) - 1\n  span = doc[matches[k][1]:matches[k][2]]\n  return(span.text)\n\nentity_pairs = []\n\nfor i in tqdm(selected_top_sentences):\n  entity_pairs.append(get_entities(i))\n\n# extract relationship\nrelations = [get_relation(i) for i in tqdm(selected_top_sentences)]\nprint(relations)\n\n# extract subject\nsource = [i[0] for i in entity_pairs]\nprint(source)\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\nprint(target)\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n\n# Use the library networkx to build graph.\n# Create a directed graph from the dataframe first.\nsentence_graph = nx.from_pandas_edgelist(kg_df, \n                                         \"source\", \n                                         \"target\",\n                                         edge_attr=True, \n                                         create_using=nx.MultiDiGraph())\nplt.figure(figsize=(12,9)) # Change this to make plotting area for knowledge graph bigger or smaller.\n# For parameters of spring_layout, see \n# https://networkx.github.io/documentation/stable/reference/generated/networkx.drawing.layout.spring_layout.html\n#pos = nx.spring_layout(sentence_graph)\npos = nx.spring_layout(sentence_graph,\n                      k = 1.3,\n                      iterations = 100,\n                      fixed = None,\n                      center = (0,0),\n                      scale = 4)\nnx.draw(sentence_graph, \n        with_labels=True, \n        node_color='skyblue', \n        edge_cmap=plt.cm.Blues, \n        pos=pos)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}