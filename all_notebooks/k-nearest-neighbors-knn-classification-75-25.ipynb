{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Algorithm\nK nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(url=\"https://www.kdnuggets.com/wp-content/uploads/rapidminer-knn-image1.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we need to represent the class line with 1-0. For this, I assigned 0 to normal data and 1 to abnormal data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"class\"] = [ 1 if each == \"Abnormal\" else 0 for each in df[\"class\"]]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abnormal = df[df[\"class\"] == 1]\nnormal = df[df[\"class\"] == 0]\n\n# scatter plot\nplt.scatter(abnormal.sacral_slope,abnormal.pelvic_radius,color=\"red\",label=\"Abnormal\",alpha=0.5)\nplt.scatter(normal.sacral_slope,normal.pelvic_radius,color=\"blue\",label=\"Normal\",alpha=0.5)\nplt.xlabel(\"sacral_slope\")\nplt.ylabel(\"pelvic_radius\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1- Choose K value <br>\n2- Find the nearest data points in K <br>\n3- Calculate how many of the class nearest neighbors in K <br>\n4- Determine which class of point or data we tested belongs to","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"class\"].values\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = df.drop([\"class\"],axis=1) # axis=1 for columns\nx_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization: I represent the data to a value between 0 and 1 for more accurate processing.\nx = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data))\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3) # n_neighbors => key value\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{} nn score: {} \".format(3,knn.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I said an estimated 3. Here, a method can be used to find the n_neighbors value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# find k value\nscore_list = []\n\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen in the graph, the most accurate value is the number 13. So we write the value 13 in the n_neighbors section.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{} KNN score: {} \".format(13,knn.score(x_test,y_test)*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}