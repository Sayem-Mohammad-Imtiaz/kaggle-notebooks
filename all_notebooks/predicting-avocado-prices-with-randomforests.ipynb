{"cells":[{"metadata":{"_uuid":"d5e7fab9f3d1c82d8143fe845db0b88930b770bc"},"cell_type":"markdown","source":"## Common imports\nImport common python libraries"},{"metadata":{"trusted":true,"_uuid":"479a97e5ac30ec673ee0a290f98079db6593cb74","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c346ce55539136e1615fe06b5b207d2f26dcedf2"},"cell_type":"markdown","source":"## Load data\n\nWe load the data for avocado pricing."},{"metadata":{"trusted":true,"_uuid":"ed1e08eca3a1d91d0ee7ac1ddb271969811e8026"},"cell_type":"code","source":"data = pd.read_csv(\"../input/avocado.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84b63410f407b82dce79a2d528f26ec9ab1b47e4"},"cell_type":"markdown","source":"## Selecting feature and target columns"},{"metadata":{"trusted":true,"_uuid":"0e4ede9eb000f6bb8eda73f84d91bda2fef5bd2c","collapsed":true},"cell_type":"code","source":"# make sure there aren't any missing values, if there are we need to use an Imputer\nassert [col for col in data.columns if data[col].isnull().any()] == []","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa44efd4384bc14f542155c2ba540f32ecc6dd9d"},"cell_type":"markdown","source":"We will first create a model using the numerical data. Later we will add the categorical data to a different model."},{"metadata":{"trusted":true,"_uuid":"b0c378c1d925548a98942aa13637d0d090b96fb5","collapsed":true},"cell_type":"code","source":"dates = [(int(mm), int(dd)) for mm, dd in [d.rsplit('-')[1:] for d in data['Date']]] \n\ndata['month'] = pd.Series([mm[0] for mm in dates])\ndata['day'] = pd.Series([dd[1] for dd in dates])\n\nfeatures_num = ['Total Volume', '4046', '4225', '4770', 'Total Bags',\n                'Small Bags', 'Large Bags', 'XLarge Bags', 'year',\n                'month', 'day']\ntarget = ['AveragePrice']\n\nX = data[features_num].values\ny = data[target].values.ravel()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"029a45d4a5f0ed22fea98e391bbc659fea50192d"},"cell_type":"markdown","source":"## Time to split train and test data\nXtrain will be 80% of data and Xtest will be 20% of data"},{"metadata":{"trusted":true,"_uuid":"9c9d2f29514c3535433fc5682f56a3170e44316b","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e11986eade97bac697875fac51b63e5fff7eda45"},"cell_type":"markdown","source":"## Train a Random Forest Regressor model"},{"metadata":{"trusted":true,"_uuid":"19f8d84444033f206cd2fc3aabfcb51b2e759dd5"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\nclf = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=1, n_estimators=150))\n\nscores = cross_val_score(clf, Xtrain, ytrain, cv=3, n_jobs=-1)\nprint(f\"{round(np.mean(scores),3)*100}% accuracy\")\n\nclf.fit(Xtrain,ytrain)\n\nprint(mean_squared_error(y_pred=clf.predict(Xtest), y_true=ytest))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7923412d8930c9542b211d24cd99a26943b8f7e4"},"cell_type":"markdown","source":"Our RandomForestRegressor has a mse of 0.0214, and this is without using the categorical part of the data."},{"metadata":{"_uuid":"96c2617690a626f7a462d42ef2cdb408303e00d2"},"cell_type":"markdown","source":"## Train a model with XGBoost"},{"metadata":{"trusted":true,"_uuid":"dfc8d7e1c1eff1836b1f615b833b9afef692aba0"},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nclf = make_pipeline(StandardScaler(), XGBRegressor(n_estimators=1000, learning_rate=0.2, early_stopping_rounds=5))\n\nscores = cross_val_score(clf, Xtrain, ytrain, cv=3, n_jobs=-1)\nprint(f\"{round(np.mean(scores),3)*100}% accuracy\")\n\nclf.fit(Xtrain, ytrain)\n\nprint(mean_squared_error(y_pred=clf.predict(Xtest),y_true=ytest))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ee47b6375abe4c6e9a0b04ca7964ddc7a69196"},"cell_type":"markdown","source":"This model uses XGBoost and performs worse than with Random forests. Its mse is 0.0286."},{"metadata":{"_uuid":"318dee3905ed9c7296c0c319f434217d5d848a1a"},"cell_type":"markdown","source":"## We will now use the categorical data\nLet's see if we get better performance"},{"metadata":{"trusted":true,"_uuid":"adc8c5b739e68e9f48acf84d880eb3ebe146e2cd"},"cell_type":"code","source":"data_with_categorical= pd.get_dummies(data.drop(columns=['Unnamed: 0', 'Date'], axis=1))\nX = data_with_categorical.drop(columns='AveragePrice', axis=1).values\ny = data_with_categorical['AveragePrice'].values.ravel()\n\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)\n\nclf = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=1, n_estimators=100))\n\nscores = cross_val_score(clf, Xtrain, ytrain, cv=3, n_jobs=-1)\nprint(f\"{round(np.mean(scores),3)*100}% accuracy\")\n\nclf.fit(Xtrain,ytrain)\n\nprint(mean_squared_error(y_pred=clf.predict(Xtest), y_true=ytest))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eb2214b859f25824b6a8a93ef1ae9e1e8a93e79"},"cell_type":"markdown","source":"Our new Random forest regressor model has a mse of 0.0153, that's a 29% drop."},{"metadata":{"_uuid":"4eb29e77edb38df99e518c1372349c190d9add9e"},"cell_type":"markdown","source":"## What happens with XGBoost now?"},{"metadata":{"trusted":true,"_uuid":"f865a354058c0ce3df5dff01316824188b15d63a"},"cell_type":"code","source":"clf = make_pipeline(StandardScaler(), XGBRegressor(n_estimators=1000, learning_rate=0.5, early_stopping_rounds=5))\n\nscores = cross_val_score(clf, Xtrain, ytrain, cv=3, n_jobs=-1)\nprint(f\"{round(np.mean(scores),3)*100}% accuracy\")\n\nclf.fit(Xtrain, ytrain)\n\nprint(mean_squared_error(y_pred=clf.predict(Xtest),y_true=ytest))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03856baa2202bfc4608e652d8f8aaea79288a3fd"},"cell_type":"markdown","source":"Still, our Random forests model performs better. This XGBoost model now has a mse of 0.0159, a 44% drop.\n"},{"metadata":{"trusted":false,"_uuid":"93d80051bf7761b068a96491117c36bbc526fd56","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}