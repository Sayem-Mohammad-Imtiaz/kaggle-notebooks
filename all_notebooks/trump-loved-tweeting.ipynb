{"cells":[{"metadata":{},"cell_type":"markdown","source":"As the name of dataset says ( \"all-trumps-twitter-insults\" ) this dataset just contain those tweets which Donal J. Trump wasn't so Polite :D .Let's see what was the problem and who made him angry :)\n\n<br>\n\nthis dataset is so much similar to [this comp](https://www.kaggle.com/c/tweet-sentiment-extraction). we can assume insult part is same as selected text in the comp. \n\nif you were in that competition feel free to show your EDA's here too.\n\n\nI inspired by many notebooks there such as :\n* https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model/\n* https://www.kaggle.com/rahul253801/eda-of-trump-s-twitter-insults\n* https://www.kaggle.com/shahules/complete-eda-baseline-model-0-708-lb"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom collections import Counter\n\nimport spacy\nfrom spacy.lang.en import English\n\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nfrom plotly.offline import download_plotlyjs, plot, init_notebook_mode, iplot\ninit_notebook_mode(connected=True)# initiate notebook for offline plot\nimport plotly.graph_objs as go\nimport os\nimport plotly.express as px\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nfrom nltk.corpus import stopwords\nimport plotly.offline as py\nimport nltk\nfrom nltk.corpus import stopwords\npy.init_notebook_mode(connected=True)\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"tweets_csv = pd.read_csv('/kaggle/input/all-trumps-twitter-insults-20152021/trump_insult_tweets_2014_to_2021.csv')\ntweets_csv.drop(columns=['Unnamed: 0'], inplace=True)\ntweets_csv['date']= pd.to_datetime(tweets_csv['date'])\ntweets_csv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# create meta data\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ntweets_csv['jacard_score'] = tweets_csv.apply(lambda x: jaccard(x['tweet'], x['insult']), axis=1)\n\ntweets_csv['tweet_num_word'] = tweets_csv['tweet'].apply(lambda x : len(str(x).split()))\ntweets_csv['insult_num_word'] = tweets_csv['insult'].apply(lambda x : len(str(x).split()))\ntweets_csv.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's check jacard score and same rows first"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_csv.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"focuse on first and second rows. they are same tweets. but 2 parts were selected as insult. BUT second row has jacard_score 0.03 while first row has 0.0, WHY?\n\nif you look closer, insult for first row was 'fool' but we have 'fool,' in text of the tweet. So this is the reason. maybe need a better function to calculte jacard. but for now forget about this problem.\n\nLet's check how many of our rows are same Tweets with different insult part\n\n\n** TIP ** : I know trump love CAPSLOCK and tweet many times in uppercase but I will lower case all the words."},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"duplicate = tweets_csv[tweets_csv.duplicated(['tweet', 'date'])] \nduplicate['tweet'] = duplicate['tweet'].str.lower()\nduplicate['insult'] = duplicate['insult'].str.lower()\nduplicate.shape, tweets_csv.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Half of data are same in tweet and date ( means they are one tweet but different insults are there )it means we have just 4678 unique tweets."},{"metadata":{},"cell_type":"markdown","source":"# Insult length"},{"metadata":{},"cell_type":"markdown","source":"Let's see how many word of a tweet were insults."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\np1 = sns.kdeplot(tweets_csv['tweet_num_word'], shade = True, color='r').set_title('Kernel dirtribution of number of words')\np1 = sns.kdeplot(tweets_csv['insult_num_word'], shade=True, color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"most of the time insults are just smething around 8 words of tweet. So he is not so Rude :)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\np1 = sns.kdeplot(tweets_csv['jacard_score'], shade = True, color='r').set_title('Kernel dirtribution of jacard score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We expect to see jacard under 0.5 but we see a peak around 1. Weird? So let's see what are those?"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_csv[tweets_csv['jacard_score'] > 0.8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So As we can see these are the tweets which their insult part and tweet part are some how same.\n\nLet's see these tweets target."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"target_full_insult = tweets_csv[tweets_csv['jacard_score'] > 0.8]\nplt.figure(figsize=(12,6))\nplt.xticks(rotation=30)\ng = sns.countplot(x='target',data=target_full_insult, order=target_full_insult.target.value_counts().iloc[:9].index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Targets\n\nLet's see who made him angry:D. first let's see how many target we have"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"tweets_csv.target.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are 866 target. Let's see how many times trump tweeted against them."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tweets_csv.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace = go.Bar(x=tweets_csv.target.value_counts().index[:25], y=tweets_csv.target.value_counts(),\n              marker=dict(\n                  opacity=0.8,\n                  color=np.arange(25)\n              ))\n\nfig = go.Figure(data=[trace])\nfig.update_layout(title=\"Top 25 Targets\")\nfig.update_xaxes(title=\"Target\")\nfig.update_yaxes(title=\"Frequency\")\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So As we can remember he had a lot of conflicts with Media and democrate."},{"metadata":{},"cell_type":"markdown","source":"# Let's see Trump's vocabulary"},{"metadata":{},"cell_type":"markdown","source":"## most common words in tweets"},{"metadata":{},"cell_type":"markdown","source":"Let's show most common words, but first we need 2 steps : \n\n1. ignore duplicated rows ( we did it before )\n2. remove stop words"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"nlp = spacy.load(\"en\")\n# Let's add some stop word to list.\nnlp.Defaults.stop_words |= {\"The\",\"&\", \"-\", \"A\"}\ndef remove_stop(row):\n    return [word for word  in row if word not in nlp.Defaults.stop_words]\n\nduplicate['tweet_removed_stop_word'] = duplicate['tweet'].apply(lambda x:str(x).split()).apply(lambda x:remove_stop(x))\nduplicate['insult_removed_stop_word'] = duplicate['insult'].apply(lambda x:str(x).split()).apply(lambda x:remove_stop(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"top_words = Counter([item for row in duplicate['tweet_removed_stop_word'] for item in row])\ntop_wrods_df = pd.DataFrame(top_words.most_common(20))\ntop_wrods_df.columns = ['word', 'count']\ntop_wrods_df.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.bar(top_wrods_df, x=\"count\", y=\"word\", title='Most words in tweets', orientation='h', \n             width=700, height=700,color='word')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.treemap(top_wrods_df, path=['word'], values='count',title='Tree Of Most Common words in tweets')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## most common words in insults"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"top_words = Counter([item for row in duplicate['insult_removed_stop_word'] for item in row])\ntop_wrods_df = pd.DataFrame(top_words.most_common(20))\ntop_wrods_df.columns = ['word', 'count']\ntop_wrods_df.style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.bar(top_wrods_df, x=\"count\", y=\"word\", title='Most Commmon Words in insult', orientation='h', \n             width=700, height=700,color='word')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.treemap(top_wrods_df, path=['word'], values='count',title='Tree Of Most Common Words in insults')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see Fake, is so much repeated. As you can see he is not so rude :).\n\nhe had so much attention to election.\n\nBut my question is how insult part is selected ?"},{"metadata":{},"cell_type":"markdown","source":"## Word cloud in insults and tweets"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pos_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask4.jpg'))\nstopwords = set(STOPWORDS)\nmore_stopwords = {'I', \"i\"}\nstopwords = stopwords.union(more_stopwords)\n\nwordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = 200,\n                    max_font_size = 100, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = pos_mask)\nwordcloud.generate(str(duplicate.tweet))\nplt.figure(figsize=(16.0,9.0))\nplt.imshow(wordcloud)\nplt.title('word cloud for tweets', fontdict={'size': 16,  \n                                  'verticalalignment': 'bottom'})","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pos_mask = np.array(Image.open('/kaggle/input/masksforwordclouds/twitter_mask4.jpg'))\nstopwords = set(STOPWORDS)\nmore_stopwords = {'I', \"i\"}\nstopwords = stopwords.union(more_stopwords)\n\nwordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = 200,\n                    max_font_size = 100, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = pos_mask)\nwordcloud.generate(str(duplicate.insult))\nplt.figure(figsize=(16.0,9.0))\nplt.imshow(wordcloud)\nplt.title('word cloud for insult  ', fontdict={'size': 16,  \n                                  'verticalalignment': 'bottom'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Check Rare words"},{"metadata":{},"cell_type":"markdown","source":"I didn't focus on text_cleaning so much and you can see some punctutation and other things there. You can clean your data more and get better results here.\n\nThis will be useful for you to double check your text dataset by this approach"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef plot_n_first_rare_word(main_df, column, from_index, to_index):\n    '''\n    main_df : is dataframe you want to check\n    column : is the column you want to check it's rare words\n    from_index, to_index :  you will set a range to see\n    for example if you set from_index : 200 and to index : 300  you will see range 200 to 300\n    '''\n    if to_index < from_index: assert print(\"from_index must be greater than from_index\")\n    rare_words = Counter([item for row in main_df[column] for item in row])\n    rare_words_df = pd.DataFrame(rare_words.most_common()[-to_index:-from_index])\n    rare_words_df.columns = ['word', 'count']\n    fig = px.treemap(rare_words_df, path=['word'], values='count',title=f'plot for range of {from_index} to {to_index} rare words')\n    fig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plot_n_first_rare_word(duplicate, \"tweet_removed_stop_word\",100, 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check ngrams"},{"metadata":{},"cell_type":"markdown","source":"Checking ngrams are so useful. Always check them"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\npy.init_notebook_mode(connected=True)\n\ndef get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n),stop_words=stop).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:20]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"new=tweets_csv['tweet']\ntop_n_bigrams=get_top_ngram(new,2)[:20]\npd.DataFrame(top_n_bigrams, columns=['n-gram','count']).set_index('n-gram').plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"new=tweets_csv['tweet']\ntop_n_bigrams=get_top_ngram(new,3)[:20]\npd.DataFrame(top_n_bigrams, columns=['n-gram','count']).set_index('n-gram').plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this was our plots for ngrams"},{"metadata":{},"cell_type":"markdown","source":"# a little work with time"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"datecount_data = tweets_csv.date.value_counts().resample(\"M\").count()\ntrace1 = go.Scatter(x=datecount_data.index, y=datecount_data)\nfig = go.Figure(data=[trace1])\n\nfig.update_xaxes(title=\"Year\")\nfig.update_yaxes(title=\"Tweet Count\")\n\niplot(fig)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}