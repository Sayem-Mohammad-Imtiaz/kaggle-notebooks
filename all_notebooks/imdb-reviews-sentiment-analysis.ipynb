{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Embedding, Dense, Activation, Flatten, GRU, Masking","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reading data\nfile_path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\ndata = pd.read_csv(file_path).to_numpy()\nfile_length = len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters for training & testing\nvocabulary_size = 10000\nsequence_length = 200\ntrain_test_split = 0.8\noutput_embeddings = 60\ndense_layer_1_size = 70\ndense_layer_2_size = 2\nbatch_size = 1000\nsteps_per_epoch = 40\nepochs = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separating data\nnp.random.shuffle(data)\nseparator_index = int(file_length * train_test_split)\n\ntrain_data = data[:separator_index]\ntest_data = data[separator_index:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using keras preprocessing to get tokens \ntokenizer = Tokenizer(vocabulary_size)\ntokens = tokenizer.fit_on_texts(train_data[:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# method that prepares model input data\ndef get_model_input_data(text_data, labels = None):\n    sequences = tokenizer.texts_to_sequences(text_data)\n    model_input_array = np.zeros((len(sequences), sequence_length))\n    for i, sequence in enumerate(sequences):\n        model_input_array[i,:min(len(sequence), 200)] = sequence[:200]\n    \n    if labels is None:\n        return model_input_array\n    else:\n        return (model_input_array, np.array([1. if label == 'positive' else 0. for label in labels]))\n\n#get_model_input_data(train_data[:20,0], train_data[:20, 1])[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# method that returns batch\ndef get_train_data():\n    while True:\n        batch = np.random.permutation(train_data)[:batch_size]\n        yield get_model_input_data(batch[:,0], batch[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\ninputs = Input((sequence_length))\nx = Embedding(vocabulary_size, output_embeddings)(inputs)\nx = Flatten()(x)\nx = Dense(dense_layer_1_size, activation=\"relu\")(x)\nx = Dense(dense_layer_2_size)(x)\noutputs = Activation(\"softmax\")(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.summary()\n\n# training definition\nmodel.compile(\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(), #\"categorical_crossentropy\"\n    optimizer = tf.keras.optimizers.Adam(), #\"adam\"\n    metrics = [\"accuracy\"]\n)\n\n# training\nresults = model.fit_generator(get_train_data(), steps_per_epoch=steps_per_epoch, epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing testing data\ndef get_model_result(model):\n    prepared_test_data = get_model_input_data(test_data[:,0])\n    y_true = np.array([1. if label == 'positive' else 0. for label in test_data[:,1]])\n    y_pred = np.argmax(model.predict(prepared_test_data), axis=1)\n    return (y_true==y_pred).sum()/y_true.shape[0]\n\nprint(\"Test result - model: \" + str(get_model_result(model)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RNN model\ninputs = Input(shape=(sequence_length,))\nx = Embedding(vocabulary_size, output_embeddings)(inputs)\nx = GRU(units=40, return_sequences=True)(x)\nx = Flatten()(x)\nx = Dense(units=110, activation=\"relu\")(x)\nx = Dense(dense_layer_2_size)(x)\noutputs = Activation(\"softmax\")(x)\n\nmodel_rnn = Model(inputs=inputs, outputs=outputs)\nmodel_rnn.summary()\n\n# training definition\nmodel_rnn.compile(\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(), #\"categorical_crossentropy\"\n    optimizer = tf.keras.optimizers.Adam(), #\"adam\"\n    metrics = [\"accuracy\"]\n)\n\n# training\nresults = model_rnn.fit_generator(get_train_data(), steps_per_epoch=steps_per_epoch, epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# results for GRU\nprint(\"Test result - RNN model: \" + str(get_model_result(model_rnn)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}