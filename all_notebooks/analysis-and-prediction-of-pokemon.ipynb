{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pokemon dataset of seaborn\n#### In this Notebook, I have performed Exploratory Data Analysis(EDA), Feature Engineering and Model Fitting\n\n> I have used numpy, pandas, seaborn and matplotlib libraries for visulization in EDA\n\n> In Feature Engineering, I imputed missing values, removed outliers and Labeled categorical features and scaled all values.\n\n> In Model Fitting Section, I used most basic classification algorithm i.e. Logistic Regression and one of the most complex classification algorithm i.e. Random Forrest Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### importing libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### loading data-set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/pokemon/Pokemon.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUMERICAL FEATURE\n# CATEGORICAL FEATURE\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### here we can see we have:\n> Total 800 Rows and 12 columns ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df['Name'].unique()))\nprint('We can see here, coulmn Name has all unique features')\nprint('='*50)\n\nprint(df['Type 1'].unique())\nprint(len(df['Type 1'].unique()))\nprint('='*50)\n\nprint(df['Type 2'].unique())\nprint(len(df['Type 2'].unique()))\nprint('='*50)\n\nprint(df['Generation'].unique())\nprint(len(df['Generation'].unique()))\nprint('this is a discrete feature BTW')\nprint('='*50)\n\n# we have:\n# 3 categorical feature\n# 1 binary (target feature) {we will convert it into discrete afterwards}\n# 1 discrete feature\n# 7 continous feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Legendary'].replace(True,1,inplace=True)\ndf['Legendary'].replace(False,0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [feature for feature in df.columns if df[feature].dtype == 'O' and feature not in 'Name']\ncategorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continous = [feature for feature in df.columns if df[feature].dtype != 'O' and feature not in 'Generation'+'Legendary']\nprint(continous)\ndiscrete = ['Generation','Legendary']\nprint(discrete)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Legendary.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graphs for continous numerical features\n# we make this graph for finding if any type of distribution is here in the feature\n\nfor feature in continous:\n  plt.figure(figsize=(7,5))\n  plt.hist(df[feature],bins=40)\n  plt.xlabel(feature)\n  plt.ylabel('count')\n  plt.title(feature)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### no gaussion or log normal distribution found in above graphs...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ralation of continous with target with help of scatter plot\n\nfor feature in continous:\n    plt.scatter(df[feature],df['Legendary'])\n    plt.title('correlation between ' +feature+ ' and Legendary')\n    plt.xlabel(feature)\n    plt.ylabel('Legendary')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb=df.groupby('Type 1')\nprint(gb['Legendary'].value_counts())\nprint('='*65)\ngb=df.groupby('Type 2')\ngb['Legendary'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### visualizing the above data below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graph for categorical features\n\nplt.figure(figsize=(13,7))\nsns.countplot(data=df,x='Type 1',hue='Legendary')\nplt.show()\nplt.figure(figsize=(13,7))\nsns.countplot(data=df,x='Type 2',hue='Legendary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = df.groupby('Generation')\ngb['Legendary'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing the above data below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graphs for discrete numerical features\n\nplt.figure(figsize=(13,7))\nsns.countplot(data=df,x='Type 1',hue='Legendary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this graph lets us see corelation of features, between features and with the target feature.\n\nplt.figure(figsize=(13,7))\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking outliers with help of box plot \n\nfor feature in continous:\n    sns.boxplot(x=feature,data=df)\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/pokemon/Pokemon.csv')\n\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()\n\ndf['Type 2']\n# this is a mathematical way to see find outliers, and many other things we can infer from it!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing all the necessary libraries for of feature engineering\n\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False,handle_unknown='error')\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=False) #setting with_mean False, is for a reason !\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here, we have missing values in Type 2, what we will do here is:\n# fill it with 'No Type' and not with mode. As there are many pokemons with no Type 2 abilty.\n\ndf.iloc[:,3].fillna(value='No Type', inplace=True)\nprint(df.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I dropped all unique features as they are not going get us prediction.\n\ndf.drop(columns=['Name','#'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I encoded all the values of categorical feature to numerical feature\n\ntemp = encoder.fit_transform(df.iloc[:,0:2])\ntemp = pd.DataFrame(temp)\ndf = pd.concat([df,temp],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see if everything is A-OK\n\ndf ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now Split X and y\n\ny = df['Legendary']\nX = df.drop(columns=['Type 1','Type 2','Legendary'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's Scale our X for better model ! I have used StandardScaler, you can use any other here and see interesting effects !\n\ntemp=scaler.fit_transform(X)\nX = pd.DataFrame(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting our data for into train and test, for testing our accuracy\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# As our target is to classify, if the pokemon is legendary or not, We are going to use LogisticRegression !\n\nfrom sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\n\nlg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see our accuracy on train data !\n\nlg.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, Let's check our accuracy on test data !\n\nlg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking our Model by \n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(lg, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see our accuracy on train data !\n\nrfc.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, Let's check our accuracy on test data !\n\nrfc.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(rfc, X_test, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}