{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This nootebook is made together with @raj2600 as part of our machine learning club project\n# We built an abstractive text summarization model which uses\n\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, TimeDistributed, LSTM, Embedding, Input, Concatenate\nfrom keras import Model\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='latin-1')\ndata_more = pd.read_csv('/kaggle/input/news-summary/news_summary_more.csv', encoding='latin-1')\ndata = pd.concat([data, data_more], axis=0).reset_index(drop=True)\ndata","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"StopWords = set(stopwords.words('english'))\ndef preprocess(text):\n    new_text = text.lower()\n    new_text = re.sub(r'\\([^)]*\\)', '', new_text)\n    new_text = re.sub('\"','', new_text)\n    new_text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_text.split(\" \")])    \n    new_text = re.sub(r\"'s\\b\",\"\",new_text)\n    new_text = re.sub(\"[^a-zA-Z]\", \" \", new_text) \n    new_text = ' '.join([word for word in new_text.split() if word not in StopWords])\n    new_text = ' '.join([word for word in new_text.split() if len(word) >= 3])\n    return new_text\n\ntext_cleaned = []\nsumm_cleaned = []\nfor text in data['text']:\n    text_cleaned.append(preprocess(text))\nfor summary in data['headlines']:\n    summ_cleaned.append(preprocess(summary))\nclean_df = pd.DataFrame()\nclean_df['text'] = text_cleaned\nclean_df['headline'] = summ_cleaned\nclean_df['headline'].replace('', np.nan, inplace=True)\nclean_df.dropna(axis=0, inplace=True)\nclean_df['headline'] = clean_df['headline'].apply(lambda x: '<START>' + ' '+ x + ' '+ '<END>')\nfor i in range(10):\n    print('News: ', clean_df['text'][i])\n    print('Headline:', clean_df['headline'][i])\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len_news = max([len(text.split()) for text in clean_df['text']])\nmax_len_headline = max([len(text.split()) for text in clean_df['headline']])\nprint(max_len_news, max_len_headline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(clean_df['text'], clean_df['headline'], test_size=0.2, random_state=0)\nnews_tokenizer = Tokenizer()\nnews_tokenizer.fit_on_texts(list(X_train))\nx_train_seq = news_tokenizer.texts_to_sequences(X_train)\nx_test_seq = news_tokenizer.texts_to_sequences(X_test)\nx_train_pad = pad_sequences(x_train_seq, maxlen=max_len_news, padding='post')\nx_test_pad = pad_sequences(x_test_seq, maxlen=max_len_news, padding='post')\nnews_vocab = len(news_tokenizer.word_index) + 1\n\nheadline_tokenizer = Tokenizer()\nheadline_tokenizer.fit_on_texts(list(y_train))\ny_train_seq = headline_tokenizer.texts_to_sequences(y_train)\ny_test_seq = headline_tokenizer.texts_to_sequences(y_test)\ny_train_pad = pad_sequences(y_train_seq, maxlen=max_len_headline)\ny_test_pad = pad_sequences(y_test_seq, maxlen=max_len_headline)\nheadline_vocab = len(headline_tokenizer.word_index) + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model definition\nAttention Layers are required to remember context of some information. As there is no Keras implementation for Attention Layer we have found a third-party resource to aid in our model. \nReference: https://github.com/thushv89/attention_keras/blob/master/layers/attention.py","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n\n            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n            # <= batch_size*en_seq_len, latent_dim\n            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n            if verbose:\n                print('wa.s>',W_a_dot_s.shape)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>',U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n            if verbose:\n                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        def create_inital_state(inputs, hidden_size):\n            # We are not using initial states, but need to pass something to K.rnn funciton\n            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n            return fake_state\n\n        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()\n\nembedding_dim = 300\nlatent_dim = 500\n\n# encoder\nencoder_input = Input(shape=(max_len_news, ))\nencoder_emb = Embedding(news_vocab, embedding_dim, trainable=True)(encoder_input)\n\n# 1st LSTM layer\nencoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True)\ny_1, a_1, c_1 = encoder_lstm1(encoder_emb)\n\n# 2nd LSTM layer\nencoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True)\ny_2, a_2, c_2 = encoder_lstm2(y_1)\n\n# 3rd lstm layer\nencoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True)\nencoder_output, a_enc, c_enc = encoder_lstm3(y_2)\n\n#decoder\ndecoder_input = Input(shape=(None,))\ndecoder_emb = Embedding(headline_vocab, embedding_dim, trainable=True)(decoder_input)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_output, decoder_fwd, decoder_back = decoder_lstm(decoder_emb, initial_state=[a_enc, c_enc])\n\n\n# attention layers\nattn_layer = AttentionLayer(name='attention_layer')\nattn_out, attn_states = attn_layer([encoder_output, decoder_output])\n\ndecoder_concat_inpt = Concatenate(axis=-1, name='concat_layer')([decoder_output, attn_out])\n\ndecoder_dense = TimeDistributed(Dense(headline_vocab, activation='softmax'))\ndecoder_output = decoder_dense(decoder_concat_inpt)\n\n# defining the model\nmodel = Model([encoder_input, decoder_input], decoder_output)\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this takes lot of time, so we sit back and relax\n\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\nhistory=model.fit([x_train_pad,y_train_pad[:,:-1]], y_train_pad.reshape(y_train_pad.shape[0],y_train_pad.shape[1], 1)[:,1:] ,epochs=50,callbacks=[callback],batch_size=512, validation_data=([x_test_pad,y_test_pad[:,:-1]], y_test_pad.reshape(y_test_pad.shape[0],y_test_pad.shape[1], 1)[:,1:]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('/kaggle/working/final_model.h5') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing training ans test loss functions\n\n\nfrom matplotlib import pyplot \npyplot.plot(history.history['loss'], label='train') \npyplot.plot(history.history['val_loss'], label='test') \npyplot.legend() \npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoder Inference\nencoder_model = Model(inputs=encoder_input,outputs=[encoder_output, a_enc, c_enc])\n\n# Decoder Inference\n# Below tensors hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_len_news,latent_dim))\n\n\n\n\n# Predicting the next word in the sequence\n# Setting the initial states to the previous time step states\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_emb, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n# Attention Inference\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\n# Dense softmax layer to calculate probability distribution over target vocab\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n\n# Final Decoder model\ndecoder_model = Model(\n[decoder_input] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_model = Model(inputs=encoder_input, outputs=[encoder_output, a_enc, c_enc])\n\ndecoder_initial_state_a = Input(shape=(latent_dim,))\ndecoder_initial_state_c = Input(shape=(latent_dim,))\ndecoder_hidden_state = Input(shape=(max_len_news, latent_dim))\n\n#decoder_lstm = LSTM(latent_dim, return_state=True, return_sequences=True)\ndecoder_out, decoder_a, decoder_c = decoder_lstm(decoder_emb, initial_state=[decoder_initial_state_a, decoder_initial_state_c])\ndecoder_final = decoder_dense(decoder_out)\ndecoder_model = Model([decoder_input]+[decoder_hidden_state, decoder_initial_state_a, decoder_initial_state_c], [decoder_final]+[decoder_a, decoder_c])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decoded_sequence(input_seq):\n    encoder_out, encoder_a, encoder_c = encoder_model.predict(input_seq)\n    next_input = np.zeros((1,1))\n    next_input[0,0] = headline_tokenizer.word_index['start']\n    output_seq = ''\n    stop = False\n    while not stop:\n        decoded_out, trans_state_a, trans_state_c = decoder_model.predict([next_input] + [encoder_out, encoder_a, encoder_c])\n        output_idx = np.argmax(decoded_out[0, -1, :])\n        #output_token = headline_tokenizer.index_word[output_idx]\n        if output_idx == headline_tokenizer.word_index['end']: \n            stop = True\n        elif output_idx>0 and output_idx != headline_tokenizer.word_index['start'] :\n            output_token = headline_tokenizer.index_word[output_idx]\n            output_seq = output_seq + ' ' + output_token\n            \n        #next_input = np.zeros((1,1))\n        next_input[0,0] = output_idx\n        encoder_a, encoder_c = trans_state_a, trans_state_c\n        \n    return output_seq\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_target_word_index= headline_tokenizer.index_word \nreverse_source_word_index= news_tokenizer.index_word \ntarget_word_index= headline_tokenizer.word_index\n\n\n# Function to implement inference\n\n\n\ndef decode_sequence(input_seq):\n    # Encoding input as state vectors\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n\n    # Generating empty target sequence of length 1\n    target_seq = np.zeros((1,1))\n\n    # Taking the 'start' word as the first word of the target sequence\n    target_seq[0, 0] = target_word_index['start']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        try:\n            sampled_token = reverse_target_word_index[sampled_token_index]\n        except:\n            sampled_token = reverse_target_word_index[np.random.randint(1, len(reverse_target_word_index))]\n        if(sampled_token!='end'):\n            decoded_sentence += ' '+sampled_token\n\n            # Exit condition: either hit max length or find stop word.\n            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_highlight-1)):\n                stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(x_test_pad)):\n    print('News:', X_test.iloc[i])\n    print('Actual Headline:', y_test.iloc[i])\n    print('Predicted Headline:', decoded_sequence(x_test_pad[i].reshape(1, max_len_news)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hi = headline_tokenizer.index_words()\nhi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"headline_tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}