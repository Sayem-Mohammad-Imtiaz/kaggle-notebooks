{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve, roc_auc_score\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings  \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:11.127539Z","iopub.execute_input":"2021-05-26T06:49:11.127926Z","iopub.status.idle":"2021-05-26T06:49:12.234189Z","shell.execute_reply.started":"2021-05-26T06:49:11.127848Z","shell.execute_reply":"2021-05-26T06:49:12.233175Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><strong>Content</strong></div>\n\n\n<div class=\"list-group\">\n    <a class=\"list-group-item list-group-item-action\" href=\"#ds\">The Dataset</a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#eda\">Exploratory Data Analysis</a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#dpp\">Data Pre-Processing</a>\n    <a class=\"list-group-item list-group-item-action\" href=\"#mtt\">Model Training & Testting</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" id='ds'><strong>The Dataset</strong></div>\n\nThe Breast Cancer datasets is available UCI machine learning repository maintained by the University of California, Irvine. The dataset contains 569 samples of malignant and benign tumor cells.\n\nThe first two columns in the dataset contain unique ID and the corresponding diagnosis (M=malignant, B=benign), respectively. The columns 3-32 contain 30 features that have been computed from digitized images of the cell nuclei.\n\n* M = Malignant (Cancerous) - Present (M)\n* B = Benign (Not Cancerous) - Absent (B)","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.235906Z","iopub.execute_input":"2021-05-26T06:49:12.236241Z","iopub.status.idle":"2021-05-26T06:49:12.259827Z","shell.execute_reply.started":"2021-05-26T06:49:12.236211Z","shell.execute_reply":"2021-05-26T06:49:12.259004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" id='eda'><strong>Exploratory Data Analysis </strong></div>","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.261232Z","iopub.execute_input":"2021-05-26T06:49:12.261527Z","iopub.status.idle":"2021-05-26T06:49:12.315597Z","shell.execute_reply.started":"2021-05-26T06:49:12.261498Z","shell.execute_reply":"2021-05-26T06:49:12.314774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset contain 2 categorical varaibles (id, diagnosis) and 30 numerical vairables.\n\n**Numerical Data:** *Data have meaning as a measurement, such as a person’s height, weight, IQ, or blood pressure; or they’re a count, such as the number of stock shares a person owns, how many teeth a dog has, or how many pages you can read of your favorite book before you fall asleep.*\n\n**Categorical Data:** *Data represent characteristics such as a person’s gender, marital status, hometown, or the types of movies they like. Categorical data can take on numerical values (such as “1” indicating male and “2” indicating female), but those numbers don’t have mathematical meaning.*\n\nreferance: https://www.dummies.com/education/math/statistics/types-of-statistical-data-numerical-categorical-and-ordinal/","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.316972Z","iopub.execute_input":"2021-05-26T06:49:12.317262Z","iopub.status.idle":"2021-05-26T06:49:12.3387Z","shell.execute_reply.started":"2021-05-26T06:49:12.317234Z","shell.execute_reply":"2021-05-26T06:49:12.337628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset contains 33 columns and 569 rows. There is mysterious column \"Unnamed\" probably an error in the dataset, let's drop the unnecessary that column.","metadata":{}},{"cell_type":"code","source":"data.drop(['id'], axis=1, inplace=True)\n\ndata.drop(['Unnamed: 32'], axis=1 , inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.342855Z","iopub.execute_input":"2021-05-26T06:49:12.343165Z","iopub.status.idle":"2021-05-26T06:49:12.350264Z","shell.execute_reply.started":"2021-05-26T06:49:12.343138Z","shell.execute_reply":"2021-05-26T06:49:12.349038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.353224Z","iopub.execute_input":"2021-05-26T06:49:12.353526Z","iopub.status.idle":"2021-05-26T06:49:12.369247Z","shell.execute_reply.started":"2021-05-26T06:49:12.353498Z","shell.execute_reply":"2021-05-26T06:49:12.368215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing values avaliable in the dataset. Let's study the target variable.","metadata":{}},{"cell_type":"code","source":"data.groupby(['diagnosis'])['diagnosis'].count()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.370572Z","iopub.execute_input":"2021-05-26T06:49:12.370863Z","iopub.status.idle":"2021-05-26T06:49:12.381057Z","shell.execute_reply.started":"2021-05-26T06:49:12.370836Z","shell.execute_reply":"2021-05-26T06:49:12.380041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(12, 6))\n\nsns.countplot(x='diagnosis', data=data, ax=axes[0])\nplt.pie(data.groupby('diagnosis')['diagnosis'].count(), labels=['B','M'], autopct='%1.1f%%')\n\nfig.suptitle('Class Distribution', fontsize=12)\naxes[0].set_xlabel('Diagnosis')\naxes[0].set_ylabel('Count')\n\naxes[1].set_xlabel('')\naxes[1].set_ylabel('')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.383695Z","iopub.execute_input":"2021-05-26T06:49:12.384044Z","iopub.status.idle":"2021-05-26T06:49:12.713234Z","shell.execute_reply.started":"2021-05-26T06:49:12.384007Z","shell.execute_reply":"2021-05-26T06:49:12.712394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.714213Z","iopub.execute_input":"2021-05-26T06:49:12.714578Z","iopub.status.idle":"2021-05-26T06:49:12.801547Z","shell.execute_reply.started":"2021-05-26T06:49:12.714551Z","shell.execute_reply":"2021-05-26T06:49:12.800734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotDistribution(columns):\n    fig, axes=plt.subplots(ncols=2, nrows=len(columns), figsize=(20, 30))\n    fig.tight_layout(pad = 4.0)\n\n    for i, column in enumerate(columns):\n        sns.distplot(data.loc[data.diagnosis=='M', column], label='Melignant', ax=axes[i][0])\n        sns.distplot(data.loc[data.diagnosis=='B', column], label='Benign', ax=axes[i][1])\n\n        for j in range(2):\n            axes[i][j].tick_params(axis='x', labelsize=12)\n            axes[i][j].tick_params(axis='y', labelsize=12)\n\n        axes[0][0].set_title('Melignant', fontsize=13)\n        axes[0][1].set_title('Benign', fontsize=13)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.802567Z","iopub.execute_input":"2021-05-26T06:49:12.802971Z","iopub.status.idle":"2021-05-26T06:49:12.811018Z","shell.execute_reply.started":"2021-05-26T06:49:12.802929Z","shell.execute_reply":"2021-05-26T06:49:12.809933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotDistribution(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean'])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:12.812111Z","iopub.execute_input":"2021-05-26T06:49:12.81241Z","iopub.status.idle":"2021-05-26T06:49:16.804465Z","shell.execute_reply.started":"2021-05-26T06:49:12.812383Z","shell.execute_reply":"2021-05-26T06:49:16.802493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotDistribution(['radius_se', 'texture_se', 'perimeter_se', 'area_se',\n       'smoothness_se', 'compactness_se', 'concavity_se',\n       'concave points_se', 'symmetry_se', 'fractal_dimension_se'])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:16.806109Z","iopub.execute_input":"2021-05-26T06:49:16.806502Z","iopub.status.idle":"2021-05-26T06:49:21.264307Z","shell.execute_reply.started":"2021-05-26T06:49:16.806465Z","shell.execute_reply":"2021-05-26T06:49:21.262877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotDistribution(['radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n       'smoothness_worst', 'compactness_worst', 'concavity_worst',\n       'concave points_worst', 'symmetry_worst',\n       'fractal_dimension_worst'])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:21.265976Z","iopub.execute_input":"2021-05-26T06:49:21.266353Z","iopub.status.idle":"2021-05-26T06:49:25.514752Z","shell.execute_reply.started":"2021-05-26T06:49:21.266316Z","shell.execute_reply":"2021-05-26T06:49:25.513673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to distribution plots, most of the variables are not normally distributed. However, Logistic Regression doesn't require normally distributed IVs. On the other hand, most of the variables contain outliters. \n\n**What are outliers in the data?**\n\n> An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n\n\nLet's draw box plots to further clarify this.","metadata":{}},{"cell_type":"code","source":"def plotBoxplot(columns, data):\n    fig, axes = plt.subplots(ncols=3, nrows=4, figsize=(20,20))\n    fig.tight_layout(pad=4.0)\n\n    col = 0\n    row = 0\n    colors = ['#bad9e9', '#7ab6d6', '#3c8abd']\n\n    for i, column in enumerate(columns):\n        sns.boxplot(y=column, data=data, ax=axes[row][col], color=colors[col])\n\n        if (i + 1) % 3 == 0:\n            row += 1\n            col = 0\n        else:\n            col += 1","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:25.516573Z","iopub.execute_input":"2021-05-26T06:49:25.517007Z","iopub.status.idle":"2021-05-26T06:49:25.52653Z","shell.execute_reply.started":"2021-05-26T06:49:25.516966Z","shell.execute_reply":"2021-05-26T06:49:25.525372Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotBoxplot(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean'], data)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:25.528254Z","iopub.execute_input":"2021-05-26T06:49:25.528691Z","iopub.status.idle":"2021-05-26T06:49:27.138145Z","shell.execute_reply.started":"2021-05-26T06:49:25.528646Z","shell.execute_reply":"2021-05-26T06:49:27.137004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotBoxplot(['radius_se', 'texture_se', 'perimeter_se', 'area_se',\n       'smoothness_se', 'compactness_se', 'concavity_se',\n       'concave points_se', 'symmetry_se', 'fractal_dimension_se'], data)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:27.139478Z","iopub.execute_input":"2021-05-26T06:49:27.139755Z","iopub.status.idle":"2021-05-26T06:49:28.96496Z","shell.execute_reply.started":"2021-05-26T06:49:27.139728Z","shell.execute_reply":"2021-05-26T06:49:28.964085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotBoxplot(['radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n       'smoothness_worst', 'compactness_worst', 'concavity_worst',\n       'concave points_worst', 'symmetry_worst',\n       'fractal_dimension_worst'], data)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:28.966331Z","iopub.execute_input":"2021-05-26T06:49:28.966632Z","iopub.status.idle":"2021-05-26T06:49:30.597903Z","shell.execute_reply.started":"2021-05-26T06:49:28.966602Z","shell.execute_reply":"2021-05-26T06:49:30.597023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Outliers should be investigated carefully. Often they contain valuable information about the process under investigation or the data gathering and recording process. Before considering the possible elimination of these points from the data, one should try to understand why they appeared and whether it is likely similar values will continue to appear. Of course, outliers are often bad data points. https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm","metadata":{}},{"cell_type":"markdown","source":"Logistic regression assumes that there is no severe multicollinearity among the explanatory variables. Multicollinearity occurs when two or more explanatory variables are highly correlated to each other, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the model. Let's check correlation between features.","metadata":{}},{"cell_type":"code","source":"corr = data.corr()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:30.599263Z","iopub.execute_input":"2021-05-26T06:49:30.599556Z","iopub.status.idle":"2021-05-26T06:49:30.606263Z","shell.execute_reply.started":"2021-05-26T06:49:30.599526Z","shell.execute_reply":"2021-05-26T06:49:30.605299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='GnBu')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:30.607512Z","iopub.execute_input":"2021-05-26T06:49:30.607812Z","iopub.status.idle":"2021-05-26T06:49:35.970316Z","shell.execute_reply.started":"2021-05-26T06:49:30.607783Z","shell.execute_reply":"2021-05-26T06:49:35.969362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" id='dpp'><strong>Data Pre-Proecessing</strong></div>\n\nAccording to the correlation plot, there are some highly correlated features such as radius_mean, area_worst, perimeter_worst. The below method select highly correlated features and remove them from the dataset: https://www.dezyre.com/recipes/drop-out-highly-correlated-features-in-python\n\nAs you see in correlation plot, there are some correlated variables in the dataset.\n\n1. radius_mean, perimeter_mean and area_mean are correlated -> i will use area_mean\n2. compactness_mean, concavity_mean and concave points_mean are correlated -> will use concavity_mean\n2. texture_mean and texture_worst are correlated -> i wil use texture_mean\n3. radius_se, perimeter_se and area_se are correlated -> i will use area_se\n4. radius_worst, perimeter_worst and area_worst are correlated -> i will use area_worst\n5. compactness_se, concavity_se and concave points_se are correlated -> i will use concavity_se\n6. compactness_worst, concavity_worst and concave points_worst are correlated -> i will use concavity_worst\n7. area_worst and area_mean are correlated -> i will use area_mean\n8. concavity_mean and concavity_worst are correlated -> i will use concavity_worst","metadata":{}},{"cell_type":"code","source":"data['diagnosis'] = (data['diagnosis'] == 'M').astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:35.971704Z","iopub.execute_input":"2021-05-26T06:49:35.97202Z","iopub.status.idle":"2021-05-26T06:49:35.977451Z","shell.execute_reply.started":"2021-05-26T06:49:35.971992Z","shell.execute_reply":"2021-05-26T06:49:35.976273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataProcessed = data.drop(['diagnosis'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:35.979023Z","iopub.execute_input":"2021-05-26T06:49:35.97936Z","iopub.status.idle":"2021-05-26T06:49:35.998108Z","shell.execute_reply.started":"2021-05-26T06:49:35.97933Z","shell.execute_reply":"2021-05-26T06:49:35.997107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropList = ['radius_mean', 'perimeter_mean', 'compactness_mean', 'concave points_mean', 'radius_worst','perimeter_worst', 'texture_worst','perimeter_se','radius_se','compactness_se','concave points_se','compactness_worst','concave points_worst', 'area_worst', 'concavity_mean']\ndataProcessed = dataProcessed.drop(dropList, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:35.999791Z","iopub.execute_input":"2021-05-26T06:49:36.000186Z","iopub.status.idle":"2021-05-26T06:49:36.014252Z","shell.execute_reply.started":"2021-05-26T06:49:36.000153Z","shell.execute_reply":"2021-05-26T06:49:36.013213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataProcessed.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:36.015987Z","iopub.execute_input":"2021-05-26T06:49:36.016307Z","iopub.status.idle":"2021-05-26T06:49:36.03666Z","shell.execute_reply.started":"2021-05-26T06:49:36.016275Z","shell.execute_reply":"2021-05-26T06:49:36.035544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = dataProcessed.corr()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:36.038181Z","iopub.execute_input":"2021-05-26T06:49:36.038482Z","iopub.status.idle":"2021-05-26T06:49:36.043403Z","shell.execute_reply.started":"2021-05-26T06:49:36.03845Z","shell.execute_reply":"2021-05-26T06:49:36.042342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='GnBu')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:36.044497Z","iopub.execute_input":"2021-05-26T06:49:36.044797Z","iopub.status.idle":"2021-05-26T06:49:37.666273Z","shell.execute_reply.started":"2021-05-26T06:49:36.044762Z","shell.execute_reply":"2021-05-26T06:49:37.665263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After dropping correlated features, we were able to reduce features into 15. As discussed earlier, there are some outliers in the dataset and Logistic Regression is sensitive to outliers. Therefore, let's treat outliers. There are many different ways to treat outliers, since this dataset is smaller I will use the data imputation technique. Here another great article about outliers: https://www.linkedin.com/pulse/techniques-outlier-detection-treatment-suhas-jk","metadata":{}},{"cell_type":"code","source":"dataProcessed.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:37.667574Z","iopub.execute_input":"2021-05-26T06:49:37.667877Z","iopub.status.idle":"2021-05-26T06:49:37.690433Z","shell.execute_reply.started":"2021-05-26T06:49:37.667839Z","shell.execute_reply":"2021-05-26T06:49:37.689598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outlierLimit(column):\n    q1, q3 = np.nanpercentile(column, [25, 75])\n    iqr = q3 - q1\n    \n    upLimit = q3 + 1.5 * iqr\n    loLimit = q1 - 1.5 * iqr\n    \n    return upLimit, loLimit","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:37.691564Z","iopub.execute_input":"2021-05-26T06:49:37.692032Z","iopub.status.idle":"2021-05-26T06:49:37.700214Z","shell.execute_reply.started":"2021-05-26T06:49:37.691985Z","shell.execute_reply":"2021-05-26T06:49:37.698828Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in dataProcessed.columns:\n    if dataProcessed[column].dtype != 'object':\n        upLimit, loLimit = outlierLimit(dataProcessed[column])\n        dataProcessed[column] = np.where((dataProcessed[column] > upLimit) | (dataProcessed[column] < loLimit), np.nan, dataProcessed[column])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:37.701596Z","iopub.execute_input":"2021-05-26T06:49:37.702011Z","iopub.status.idle":"2021-05-26T06:49:37.732591Z","shell.execute_reply.started":"2021-05-26T06:49:37.701974Z","shell.execute_reply":"2021-05-26T06:49:37.731683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://www.whatissixsigma.net/wp-content/uploads/2015/07/Box-Plot-Diagram-to-identify-Outliers-figure-1.png)\n\nUsing the above function, we are selecting outliers that above and below the upper limit and lower limit. Then change into missing value if vale is above or lower the limit. For more info read this https://www.whatissixsigma.net/box-plot-diagram-to-identify-outliers/","metadata":{}},{"cell_type":"code","source":"dataProcessed.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:37.733899Z","iopub.execute_input":"2021-05-26T06:49:37.734229Z","iopub.status.idle":"2021-05-26T06:49:37.742732Z","shell.execute_reply.started":"2021-05-26T06:49:37.734199Z","shell.execute_reply":"2021-05-26T06:49:37.741769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you can see there are missing values in our dataset. Like I told you earlier, now we can treat the missing values using k-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=4)\ndataProcessed.iloc[:, :] = imputer.fit_transform(dataProcessed)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:37.744591Z","iopub.execute_input":"2021-05-26T06:49:37.744914Z","iopub.status.idle":"2021-05-26T06:49:38.121038Z","shell.execute_reply.started":"2021-05-26T06:49:37.744851Z","shell.execute_reply":"2021-05-26T06:49:38.120056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataProcessed.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.122752Z","iopub.execute_input":"2021-05-26T06:49:38.123071Z","iopub.status.idle":"2021-05-26T06:49:38.13062Z","shell.execute_reply.started":"2021-05-26T06:49:38.123042Z","shell.execute_reply":"2021-05-26T06:49:38.129865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataProcessed.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.135625Z","iopub.execute_input":"2021-05-26T06:49:38.135929Z","iopub.status.idle":"2021-05-26T06:49:38.176861Z","shell.execute_reply.started":"2021-05-26T06:49:38.1359Z","shell.execute_reply":"2021-05-26T06:49:38.175636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though magnitudes are not the same, Logistic regression is not sensitive to the magnitude of features. Therefore, we do not require data normalization or standardization.\n\n* https://digitalcommons.georgiasouthern.edu/information-tech-facpubs/111/\n* https://builtin.com/data-science/when-and-why-standardize-your-data\n* https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" id='mtt'><strong>Model Training & Testing</strong></div>","metadata":{}},{"cell_type":"code","source":"Y = data['diagnosis']\nX = dataProcessed","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.182128Z","iopub.execute_input":"2021-05-26T06:49:38.182587Z","iopub.status.idle":"2021-05-26T06:49:38.188432Z","shell.execute_reply.started":"2021-05-26T06:49:38.182476Z","shell.execute_reply":"2021-05-26T06:49:38.187507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, random_state=50)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.189721Z","iopub.execute_input":"2021-05-26T06:49:38.190042Z","iopub.status.idle":"2021-05-26T06:49:38.204682Z","shell.execute_reply.started":"2021-05-26T06:49:38.190005Z","shell.execute_reply":"2021-05-26T06:49:38.203615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20200522215734/download35.jpg)\n\n**Advantages of Logistic Regression**\n\n1. Doesn't require normally distributted IVs\n2. Support IVs in any measuremet scale (numerical, categorical)\n3. DV doesn't need linear relationship with IVs \n\n**Assumptions of Logistic Regression**\n\n1. Sample Size - Small samples with large number of predictors will reduce power\n2. No Multicollinearity - IVs that are correlated with other IVs\n3. No Extreme Outliers - Estimates of the logistic regression are sensitive to the unusual observations","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score \nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state=50)\nlr.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.206686Z","iopub.execute_input":"2021-05-26T06:49:38.206991Z","iopub.status.idle":"2021-05-26T06:49:38.256404Z","shell.execute_reply.started":"2021-05-26T06:49:38.206934Z","shell.execute_reply":"2021-05-26T06:49:38.255433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yPredict = lr.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.257601Z","iopub.execute_input":"2021-05-26T06:49:38.257894Z","iopub.status.idle":"2021-05-26T06:49:38.264765Z","shell.execute_reply.started":"2021-05-26T06:49:38.257866Z","shell.execute_reply":"2021-05-26T06:49:38.263645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy: {}'.format(accuracy_score(y_test, yPredict)))\nprint('Recall: {}'.format(recall_score(y_test, yPredict)))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.266298Z","iopub.execute_input":"2021-05-26T06:49:38.266656Z","iopub.status.idle":"2021-05-26T06:49:38.282262Z","shell.execute_reply.started":"2021-05-26T06:49:38.26662Z","shell.execute_reply":"2021-05-26T06:49:38.281287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recall is 94% and it is a pretty good result without any optimizations. However, according to the scope of this task, we need to identify cancerous samples as much as possible(true positive). We can check the number of identified patients using a confusion matrix. Let's understand confusion matrix:\n\n* TP(True Positive) - correctly predicted, samples contain cancerous cells\n* TN(True Negetive) - correctly predicted, samples do not contain cancerous cells\n* FP(False Positive) - incorrectly predicted, samples do not have cancerous cells (0 -> 1)\n* FN(False Negetive) - incorrectly predicted, samples have cancerous cell (1 -> 0)","metadata":{}},{"cell_type":"code","source":"def drawConfusionMatrix(confusion):\n    groups = ['TN','FP','FN','TP']\n\n    counts = ['{0:0.0f}'.format(value) for value in confusion.flatten()]\n    labels = np.asarray([f'{v1}\\n{v2}' for v1, v2 in zip(groups, counts)]).reshape(2, 2)\n\n    sns.heatmap(confusion, annot=labels, cmap='Blues', cbar=False, fmt='')","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.283324Z","iopub.execute_input":"2021-05-26T06:49:38.283613Z","iopub.status.idle":"2021-05-26T06:49:38.29438Z","shell.execute_reply.started":"2021-05-26T06:49:38.283578Z","shell.execute_reply":"2021-05-26T06:49:38.293446Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drawConfusionMatrix(confusion_matrix(y_test, yPredict))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.295741Z","iopub.execute_input":"2021-05-26T06:49:38.296156Z","iopub.status.idle":"2021-05-26T06:49:38.395693Z","shell.execute_reply.started":"2021-05-26T06:49:38.296123Z","shell.execute_reply":"2021-05-26T06:49:38.394537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the result, model has been able to correctly classify 56 samples and incorrectly classify 3 samples as normal. To improve this model we need to improve recall by reducing False Negative(FN). As you know, the default threshold is 0.5 and we can tune this hyperparameter. To do that let's plot the ROC curve first.","metadata":{}},{"cell_type":"code","source":"lr.predict_proba(x_test)[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.396921Z","iopub.execute_input":"2021-05-26T06:49:38.397202Z","iopub.status.idle":"2021-05-26T06:49:38.409107Z","shell.execute_reply.started":"2021-05-26T06:49:38.397176Z","shell.execute_reply":"2021-05-26T06:49:38.407603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(x_test)[:,1])\n\nplt.plot([0,1], [0,1], linestyle='--', label='No Skill')\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate ==>')\nplt.ylabel('True Positive Rate ==>')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.410996Z","iopub.execute_input":"2021-05-26T06:49:38.411403Z","iopub.status.idle":"2021-05-26T06:49:38.594107Z","shell.execute_reply.started":"2021-05-26T06:49:38.41136Z","shell.execute_reply":"2021-05-26T06:49:38.593053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(y_test, lr.predict_proba(x_test)[:,1])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.59607Z","iopub.execute_input":"2021-05-26T06:49:38.596494Z","iopub.status.idle":"2021-05-26T06:49:38.608518Z","shell.execute_reply.started":"2021-05-26T06:49:38.596449Z","shell.execute_reply":"2021-05-26T06:49:38.607297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.609823Z","iopub.execute_input":"2021-05-26T06:49:38.610244Z","iopub.status.idle":"2021-05-26T06:49:38.621455Z","shell.execute_reply.started":"2021-05-26T06:49:38.610191Z","shell.execute_reply":"2021-05-26T06:49:38.620349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recalls = []\nfor th in thresholds:\n    predictTh = np.where(lr.predict_proba(x_test)[:,1] > th, 1, 0)\n    recalls.append(recall_score(y_test, predictTh))\n    \nrecalls","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.622748Z","iopub.execute_input":"2021-05-26T06:49:38.623165Z","iopub.status.idle":"2021-05-26T06:49:38.690574Z","shell.execute_reply.started":"2021-05-26T06:49:38.623123Z","shell.execute_reply":"2021-05-26T06:49:38.689541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recallDf = pd.concat([pd.Series(thresholds), pd.Series(recalls)], axis=1)\nrecallDf.columns = ['threshold', 'recall']\nrecallDf.sort_values(by='recall', ascending=False, inplace=True)\n\nrecallDf","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.691922Z","iopub.execute_input":"2021-05-26T06:49:38.692246Z","iopub.status.idle":"2021-05-26T06:49:38.708208Z","shell.execute_reply.started":"2021-05-26T06:49:38.692215Z","shell.execute_reply":"2021-05-26T06:49:38.707048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\nsns.barplot(x='threshold', y='recall', data=recallDf)\n\nplt.title('Recall vs Treshold')\nplt.xticks(rotation=90)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:38.709853Z","iopub.execute_input":"2021-05-26T06:49:38.710273Z","iopub.status.idle":"2021-05-26T06:49:39.140062Z","shell.execute_reply.started":"2021-05-26T06:49:38.710231Z","shell.execute_reply":"2021-05-26T06:49:39.138916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which threshold do we need to select? it depends on the trade-off that we want to make. If you are more concerned about high sensitivity pick the threshold that maximizes the true positive rate. In this case, I would like to have higher sensitivity and some level of false-positive rate. Therefore, I would like to select threshold => 0.284187.","metadata":{}},{"cell_type":"code","source":"yPredictTh = np.where(lr.predict_proba(x_test)[:,1] > thresholds[15], 1, 0) ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:39.141455Z","iopub.execute_input":"2021-05-26T06:49:39.141883Z","iopub.status.idle":"2021-05-26T06:49:39.150205Z","shell.execute_reply.started":"2021-05-26T06:49:39.141841Z","shell.execute_reply":"2021-05-26T06:49:39.148938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drawConfusionMatrix(confusion_matrix(y_test, yPredictTh))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:39.152136Z","iopub.execute_input":"2021-05-26T06:49:39.152561Z","iopub.status.idle":"2021-05-26T06:49:39.248904Z","shell.execute_reply.started":"2021-05-26T06:49:39.152518Z","shell.execute_reply":"2021-05-26T06:49:39.247672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy: {}'.format(accuracy_score(y_test, yPredictTh)))\nprint('Recall: {}'.format(recall_score(y_test, yPredictTh)))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T06:49:39.250589Z","iopub.execute_input":"2021-05-26T06:49:39.251035Z","iopub.status.idle":"2021-05-26T06:49:39.261222Z","shell.execute_reply.started":"2021-05-26T06:49:39.250989Z","shell.execute_reply":"2021-05-26T06:49:39.260144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you see above, after we adjust the threshold value our prediction is better than the previous one and recall has been improved by1.7%. \n\nHope you've enjoyed my work, if you like my work and need to share something with me leave a comment :)","metadata":{}}]}