{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport regex as re\nfrom sklearn.svm import SVC\nfrom wordcloud import WordCloud, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read data\n\n# Read csv\n# Use cp1252 as the dataset is not suitable to be read with utf8 encoding\ntrain = pd.read_csv(\"../input/email-classification-nlp/SMS_train.csv\", encoding='cp1252')\ntest = pd.read_csv(\"../input/email-classification-nlp/SMS_test.csv\",encoding='cp1252')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(x):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    return tokenizer.tokenize(x)\n                                \ndef stemmer(x):\n    stemmer = PorterStemmer()\n    return ' '.join([stemmer.stem(word) for word in x])\n \ndef lemmatize(x):\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word) for word in x])\n\nstop_words = stopwords.words('english')\n\n#  Preprocess train dataset\n# remove special characters from text column\ntrain.Message_body = train.Message_body.str.replace('[#,@,&]', '')\n# Remove digits\ntrain.Message_body = train.Message_body.str.replace(' \\d+ ','')\n#Remove www\ntrain.Message_body = train.Message_body.str.replace('w{3}','')\n# remove urls\ntrain.Message_body = train.Message_body.str.replace(\"http\\S+\", \"\")\n# remove multiple spaces with single space\ntrain.Message_body = train.Message_body.str.replace('\\s+', ' ')\n#remove all single characters\ntrain.Message_body = train.Message_body.str.replace(r'\\s+[a-zA-Z]\\s+', '')\ntrain['tokens'] = train['Message_body'].map(tokenize)\ntrain['lemma'] = train['tokens'].map(lemmatize)\ntrain['stems'] = train['tokens'].map(stemmer)\n\n#  Preprocess test dataset\n# remove special characters from text column\ntest.Message_body = test.Message_body.str.replace('[#,@,&]', '')\n#Remove digits\ntest.Message_body = test.Message_body.str.replace(' \\d+ ','')\n#Remove www\ntest.Message_body = test.Message_body.str.replace('w{3}','')\n# remove urls\ntest.Message_body = test.Message_body.str.replace(\"http\\S+\", \"\")\n# remove multiple spaces with single space\ntest.Message_body = test.Message_body.str.replace('\\s+', ' ')\n#remove all single characters\ntest.Message_body = test.Message_body.str.replace(r'\\s+[a-zA-Z]\\s+', '')\ntest['tokens'] = test['Message_body'].map(tokenize)\ntest['lemma'] = test['tokens'].map(lemmatize)\ntest['stems'] = test['tokens'].map(stemmer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WordCloud for spam marked emails in train set\n\n# Get a string of e-mails\nmessage_body_spam = \",\".join(spam_mail.lower() for spam_mail in train.Message_body[train.Label == 'Spam'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=70, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(message_body_spam)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in spam mails',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WordCloud for Non-spam marked emails in train set\n\n# Get a string of e-mails\nmessage_body_spam = \",\".join(spam_mail.lower() for spam_mail in train.Message_body[train.Label == 'Non-Spam'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=70, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(message_body_spam)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in not spam mails',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labels = train['Label']\ndata_tweets = train['lemma']\n\ntrain_X, test_X, train_y, test_y = train_test_split(data_tweets, \n                                                    data_labels, \n                                                    test_size=0.25, \n                                                    random_state = 42)\n\nval_y = test['Label']\nval_X = test['lemma']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_mnnb = Pipeline(steps = [('tf', TfidfVectorizer()), ('mnnb', MultinomialNB())])\n\n# Create Parameter Grid\npgrid_mnnb = {\n 'tf__max_features' : [1000, 2000, 3000],\n 'tf__stop_words' : ['english', None],\n 'tf__ngram_range' : [(1,1),(1,2)],\n 'tf__use_idf' : [True, False],\n 'mnnb__alpha' : [0.1, 0.5, 1]\n}\n\n# Apply GridSearch to Pipeline to find the best parameters\ngs_mnnb = GridSearchCV(pipe_mnnb, pgrid_mnnb, cv=5, n_jobs=-1, verbose=2)\n\n# Fit the model\ngs_mnnb.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the best parameters for our model\ngs_mnnb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Score of train set', gs_mnnb.score(train_X, train_y))\nprint('Score of test set',gs_mnnb.score(test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Naive Bayes Predictions on val set and confusion matrix\npreds_mnnb = gs_mnnb.predict(val_X)\ntest['preds'] = preds_mnnb\n\n# Generate confusion matrix\nmatrix_nb = plot_confusion_matrix(gs_mnnb, test_X, test_y,\n                                 cmap=plt.cm.Blues,\n                                 normalize='true')\n\nplt.title('Confusion matrix for NB classifier')\nplt.show(matrix_nb)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_lgrg = Pipeline(steps = [('tf', TfidfVectorizer()), ('lgrg', LogisticRegression())])\n\n# Create Parameter Grid\npgrid_lgrg = {\n 'tf__max_features' : [1000, 2000, 3000],\n 'tf__ngram_range' : [(1,1),(1,2)],\n 'tf__use_idf' : [True, False],\n 'lgrg__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n 'lgrg__class_weight' : ['balanced', None],\n 'lgrg__C' : [1.0, 0.9]\n}\n\n# Apply GridSearch to Pipeline to find the best parameters\ngs_lgrg = GridSearchCV(pipe_lgrg, pgrid_lgrg, cv=5, n_jobs=-1, verbose=2)\n\n# Fit the model\ngs_lgrg.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_lgrg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Score of train set', gs_lgrg.score(train_X, train_y))\nprint('Score of test set',gs_lgrg.score(test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LR Predictions on val set and confusion matrix\npreds_lgrg = gs_lgrg.predict(val_X)\ntest['preds'] = preds_lgrg\n\n#conf_lgrg = confusion_matrix(val_y, preds_lgrg)\n#conf_lgrg\n\n# Generate confusion matrix\nmatrix_lr = plot_confusion_matrix(gs_lgrg, test_X, test_y,\n                                 cmap=plt.cm.Blues,\n                                 normalize='true')\n\nplt.title('Confusion matrix for LR classifier')\nplt.show(matrix_lr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_svc = Pipeline(steps = [('tf', TfidfVectorizer()), ('svc', SVC())])\n\n# Create Parameter Grid\npgrid_svc = {\n 'tf__max_features' : [1000, 2000, 3000],\n 'tf__ngram_range' : [(1,1),(1,2)],\n 'tf__use_idf' : [True, False],\n 'svc__kernel' : ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n 'svc__decision_function_shape' : ['ovo', 'ovr'],\n 'svc__C' : [1.0, 0.9, 0.8, 0.7]\n}\n\n# Apply GridSearch to Pipeline to find the best parameters\ngs_svc = GridSearchCV(pipe_svc, pgrid_svc, cv=5, n_jobs=-1, verbose=2)\n\n# Fit the model\ngs_svc.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_svc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Score of train set', gs_svc.score(train_X, train_y))\nprint('Score of test set',gs_svc.score(test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LR Predictions on val set and confusion matrix\npreds_svc = gs_svc.predict(val_X)\ntest['preds'] = preds_svc\n\n# Generate confusion matrix\nmatrix_svc = plot_confusion_matrix(gs_svc, test_X, test_y,\n                                 cmap=plt.cm.Blues,\n                                 normalize='true')\n\nplt.title('Confusion matrix for SVC classifier')\nplt.show(matrix_svc)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choose the best model based on score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List with our trained models\nmodels = []\n\nmodels.append(gs_mnnb)\nmodels.append(gs_lgrg)\nmodels.append(gs_svc)\n\n# Build a list of (score, model) tuples\nscores = [(model.score(test_X, test_y), model) for model in models]\n\n# sort it on score\nscores = sorted(scores, key=lambda x: x[0], reverse=True)\n\nprint('Results for the three models: ')\nfor item in scores:\n    print('The model {} has reached {} accuracy on test set'.format(item[1].estimator[1], round(item[0], 2)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the model with the best score, which is the\n# the second element of the first item\nbest_model = scores[0][1]\nprint(best_model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}