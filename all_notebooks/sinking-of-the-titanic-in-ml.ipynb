{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# Sinking of the Titanic\n\nThis is my second attempt at this competition. I am reading and learning a lot and this time around I wanted to focus more on data preprocessing, engineering and better feature selection. As saying goes, 'Model will be as good as data is'. In my last attempt - <a>https://www.kaggle.com/uguess/titanic-ml-from-disaster</a>, I received the score of 0.71 so my goal this time would be to at least achieve 75% accuracy in prediction. Like always, comments and suggestions are greatly appreciated.\n\nSo lets divide the work into following sections:\n1. Load and view data\n2. Understand feature-label relations with visualization\n3. Data preprocessing and feature engineering\n4. Build various models and compare\n5. Visualize the best model prediction\n6. File Submission"},{"metadata":{"_cell_guid":"01719e1c-1e2b-4bad-8bc6-3daa69e302b3","_uuid":"7158e0b1dfb9a971ee2151acb93d39361e66cc37"},"cell_type":"markdown","source":"## 1. Load and view data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"# Import libraries for linear algebra and loading data\nimport numpy as np\nimport pandas as pd\n\n# Load training and test data into dataframes\norig_training_set = pd.read_csv('../input/train.csv')\norig_test_set = pd.read_csv('../input/test.csv')","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"449b0ccc-a71c-4edf-a76a-1732923a68c0","_uuid":"d7a56d9025128b18ce5c509a96a688c0f1a0b7f4","scrolled":true,"trusted":true},"cell_type":"code","source":"# View training data\norig_training_set.head(n=10)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"8d43f166-b971-4100-8a55-a0732ad89874","_uuid":"f86fc6355231a03c3514583ab08a08e64a47ca0f","trusted":true},"cell_type":"code","source":"# View test data\norig_test_set.head(n=10)","execution_count":93,"outputs":[]},{"metadata":{"_cell_guid":"59f3adc6-f2a4-4795-8136-52db18479f61","_uuid":"24df3f242a80533416a9efe1aa77babcb52dce57"},"cell_type":"markdown","source":"In given training data, **Survived** is label (prediction) and all the rest are features (predictors) but all the provided features do not always contribute to help build model that can make better prediction. For example, PassengerId feature has no relation with survival or death of passengers, Ticket feature is not meaningful and will not help towards better prediction. So, lets drop these features out of our training and test data"},{"metadata":{"_cell_guid":"58e920cb-df2f-42fa-8971-b964eaf281f7","_uuid":"fb7599f6cecb57dfd41451f3e105b4fabb73016f","trusted":true},"cell_type":"code","source":"# Drop unnecessary columns from training and test set\ntraining_set = orig_training_set.drop(['Name', 'PassengerId', 'Ticket'], axis = 1)\ntest_set = orig_test_set.drop(['Name', 'PassengerId', 'Ticket'], axis = 1)\n\nprint(training_set.columns)\nprint(test_set.columns)","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"37888b5b-bd03-4fb3-ba9f-b5f62ceb0585","_uuid":"78f3f8d31fc30c3800b003ecc4bfee35ed1d1671","scrolled":true,"trusted":true},"cell_type":"code","source":"# View statistical information about the training data\ntraining_stats = training_set.describe(include='all')\nprint(training_stats)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"5dde30a9-1236-4bb5-a074-b0b955f0c40f","_uuid":"744263fd5f3b631be253ab41b52c6d2336d47785","trusted":true},"cell_type":"code","source":"# View statistical information about the test data\ntest_stats = test_set.describe(include='all')\nprint(test_stats)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"b4a36087-b5ce-4b67-a873-dbec4930600a","_uuid":"779e0215698e7821b50b17dd07e92eee379618c1","trusted":true},"cell_type":"code","source":"# Lets focus on count index of these stats\ntraining_stats.loc['count', :] ","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"42feba63-62b9-443c-8a81-788c30d09222","_uuid":"d03006e0d6c98b3c35c308d07f9765347bf4b637","trusted":true},"cell_type":"code","source":"test_stats.loc['count', :]","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"11e27130-38fa-4e73-9d8d-2f94c0337d5b","_uuid":"5808248378865ea6fdd523366324a8468a81509f"},"cell_type":"markdown","source":"The count data from above tells us that Age, Cabin and Embarked data are missing values from training set and Age, Fare and Cabin are missing values from test set. The Cabin feature in both cases are missing many values and also Cabin feature will not help us predict our labels more accurately so lets drop this feature as well."},{"metadata":{"_cell_guid":"5001b7fc-f3d4-43de-b79f-c57bfec476fc","_uuid":"832d93dcea2459bbfb71c3354713184c52853df3","collapsed":true,"trusted":true},"cell_type":"code","source":"# Drop Cabin feature from training and test set\ntraining_set = training_set.drop(['Cabin'], axis = 1)\ntest_set = test_set.drop(['Cabin'], axis = 1)\n\nprint(training_set.columns)\nprint(test_set.columns)","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"7ac94b09-91d2-4e75-836e-3a6e96a59444","_uuid":"93f9d1ea881eff7fef8068ed0b0ea8e1753673ec","collapsed":true,"trusted":true},"cell_type":"code","source":"# Update variable that carries statistical information\ntraining_stats = training_set.describe(include='all')\ntest_stats = test_set.describe(include='all')","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"1e624279-cf19-401d-b4f7-ed9d1884183c","_uuid":"8f37d63b99f6c764ddd97961683ae9e9eaa940b5"},"cell_type":"markdown","source":"## 2. Understand feature-label relations with visualization\n\nNow lets do some data visualization to better understand and examine the correlation between various feature and label. \n\n### Pclass Versus Survived"},{"metadata":{"_cell_guid":"8b94d2ac-7455-47aa-8999-452454042823","_uuid":"8d97c8ce0d3a60a1ea37ace9ab6109087c07e464","trusted":true},"cell_type":"code","source":"# Import data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.barplot(x=training_set.Pclass, y=training_set.Survived)\nplt.ylabel('Passengers Survived(%)')","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"b95f716b-863b-4ea8-b5d0-0ae78188e212","_uuid":"55725e274b0d97b455de13e4e9290d2d5ed39043"},"cell_type":"markdown","source":"We can see from the bar plot that the Survival percentages are more for Pclass of 1 than Pclass of 2 than Pclass of 3. This gives us a good understanding that higher the socio-economic status of the passengers higher the chances for  their survival.\n\n### Sex Versus Survived"},{"metadata":{"_cell_guid":"4a88cd3b-edae-4831-9754-64eb5a475c5a","_uuid":"5b909461bf2fa520e9ca2022d6edbffb05a7c867","trusted":true},"cell_type":"code","source":"sns.barplot(x=training_set.Survived, y=training_set.Sex)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"000a2634-71f0-4cca-a118-b6efd710810b","_uuid":"46c28430fda496c34424d57c15dccd69b5c6a45c"},"cell_type":"markdown","source":"We can clearly see that male passengers have fewer chances of survival than females.\n\n### Age Versus Survived"},{"metadata":{"_cell_guid":"03cae8d1-c70a-449d-802f-bdbfb8249e44","_uuid":"dd393c4a4e4b388d9e242c42b939f60e182e8b7a","trusted":true},"cell_type":"code","source":"sns.kdeplot(\n        training_set.loc[training_set['Survived'] == 0, 'Age'].dropna(), \n        color='red', \n        label='Did not survive')\nsns.kdeplot(\n        training_set.loc[training_set['Survived'] == 1, 'Age'].dropna(), \n        color='green', \n        label='Survived')\nplt.xlabel('Age')\nplt.ylabel('Passengers Survived(%)')","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"e887be99-7425-4657-888b-16adc3e63662","_uuid":"bed9c66610621bbf0190185e04437493d79bb41a"},"cell_type":"markdown","source":"Here we see that passengers are mostly comprised of ages between 20 and 40. Also we see that Age feature has some outliers i.e. Age > ~62 years old.\n\n### Embarked Versus Survived"},{"metadata":{"_cell_guid":"24d42398-1df1-435f-a736-56a894de3f15","_uuid":"c54f4e7de06b63e512858329a6f1f61e3104a1fe","trusted":true},"cell_type":"code","source":"sns.pointplot(training_set.Embarked, training_set.Survived)","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"6bc2d63d-3593-494d-abe6-0733a6276ab5","_uuid":"3a7cf3a817e2c4fc2853abd3c3c4cf8093208cc2"},"cell_type":"markdown","source":"Looks like most of the passengers who survived the disaster embarked from Cherbourg (C).\n\n### SibSp and Parch vs Survived"},{"metadata":{"_cell_guid":"5b50a38b-ae64-4f81-94b5-6bee8b870d91","_uuid":"55581a9668000967a256bba86952458ed6df3378","trusted":true},"cell_type":"code","source":"sns.regplot(x=training_set.SibSp, y=training_set.Survived, color='r')","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"1db720a7-c34c-4180-b93e-817329a013f5","_uuid":"9c2e16ab8bb3e6ce0f7f9fe57fa5446668e2d69e","trusted":true},"cell_type":"code","source":"sns.regplot(x=training_set.Parch, y=training_set.Survived, color='b')","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"90ef713c-1bf1-47fa-9cfb-5a9851260a71","_uuid":"71756cce104a6617dab4cde9eb7d68765dae177f"},"cell_type":"markdown","source":"We see that relation between SibSp and Survived and between Parch and Survived does not provide us good understanding of data. Survival rate slightly decreases with increase in SibSp whereas it increases with increase in Parch. We will be combining these two features into HasFamily features in the data preprocessing step.\n\n### Fare Versus Survived"},{"metadata":{"_cell_guid":"3bb517d0-e680-4802-bb25-6739f57a408f","_uuid":"3282660afb5f21c42e27b910db40765142735b14","trusted":true},"cell_type":"code","source":"sns.swarmplot(x=training_set.Survived, y=training_set.Fare)","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"b01b8103-b73d-434c-a81b-075589be7ed9","_uuid":"a288b6ae22c06651b3b0ea9d9cf645eb0ca81549"},"cell_type":"markdown","source":"There is not really useful information we can extract from Fare versus Survived but we can get a rough idea that percentage of passengers who survived paid more higher fares than the percentage of passenger who did not survived the disaster.\n\nSo, now that we have better understanding of the data let's move on to the data preprocessing and feature engineering\n\n## 2. Data preprocessing and feature engineering"},{"metadata":{"_cell_guid":"9608479b-c100-4e27-88e5-3465c1e0572a","_uuid":"c927a48a837968e6d4d8b803bcd00da20ee5bc80","trusted":true},"cell_type":"code","source":"# lets view the stats on training and test data again to do some analysis\nprint(training_stats)\nprint(test_stats)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"542bc622-4411-4d0c-9c96-89cfb9524067","_uuid":"c9f4ff1101bb3996c65e64d14ac5fe03344a7b11"},"cell_type":"markdown","source":"As we know from previous analysis that Age and Embarked features are missing values from training set and Age and Fare features are missing values from test set. We will usually look at the distribution of other data for the same feature to assist us fill these missing values. Data Visualization will help us determine what method to use."},{"metadata":{"_cell_guid":"362794a3-36d6-435b-966d-58c39a832345","_uuid":"f1764345660ba21d2bde7749af26355d4fca07ee","trusted":true},"cell_type":"code","source":"sns.distplot(training_set['Age'].dropna(), bins=20, rug=True, kde=True)","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"2366f251-b0fd-4143-bc91-3422271c40b0","_uuid":"431fa3f8481cea312084502b5a8a7658252394c3"},"cell_type":"markdown","source":"From the distribution plot above, we see that the mean of Age values will be most appropriate to use to fill missing values."},{"metadata":{"_cell_guid":"01fb175d-c2d3-40d8-9d1a-e0c9e3dd5f96","_uuid":"059f2ed20ef98c2c2d48ccc5565a13a6d8e2f333","trusted":true},"cell_type":"code","source":"training_set['Age'] = training_set.Age.fillna(training_stats.loc['mean', 'Age'])\ntest_set['Age'] = test_set.Age.fillna(test_stats.loc['mean', 'Age'])\n\ntraining_stats = training_set.describe(include='all')\ntest_stats = test_set.describe(include='all')\n\nprint(training_stats.loc['count', 'Age'])\nprint(test_stats.loc['count', 'Age'])","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"02bf1dce-ca29-4bca-ac3f-342a2f19b719","_uuid":"37780de696f252365fec609e10db4230684eb23b","trusted":true},"cell_type":"code","source":"sns.countplot(x=training_set.Embarked, palette=\"Greens_d\");","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"8b6ff8ca-6fba-44cd-94e0-7a800b0b2f36","_uuid":"2a4865893333518f14389388992cfaa18f6b8eee"},"cell_type":"markdown","source":"Since majority of the passengers embarked from Southampton, we will be using mode to fill missing values for Embarked feature in training set."},{"metadata":{"_cell_guid":"b80c061f-4459-4f85-acbe-79beb5335bcc","_kg_hide-output":true,"_uuid":"704eedc6694001bce2fe9ad4a5ec6f15430c2575","trusted":true},"cell_type":"code","source":"from statistics import mode\nmode_embarked = mode(training_set['Embarked'])\ntraining_set['Embarked'] = training_set['Embarked'].fillna(mode_embarked)\n\ntraining_stats = training_set.describe(include='all')\n\nprint(training_stats.loc['count', 'Embarked'])","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"a061e0fd-3943-414f-b13e-e1d78b031c22","_uuid":"eff69c77997147a9e731621032baf4bc7bb3a00a"},"cell_type":"markdown","source":"For a missing value for Fare feature in test set, we will just use passenger with similar feature values to fill it. Note that we really don't need to do all this but this since Fare feature is only missing a value and it may not really contribute a whole lot for our model to understand the data but it would be a good practice and we can use similar logic for other predictions."},{"metadata":{"_cell_guid":"950f121b-80aa-4c96-9ebc-b381abec3d5d","_uuid":"8b5343f1fc846cb3dc0d201474ba69f944c48d32","scrolled":true,"trusted":true},"cell_type":"code","source":"# Understand the relation between empty Fare feature value and other features values\nempty_fare = test_set[test_set['Fare'].isnull()]\nprint(empty_fare)","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"edbb7e4f-091e-4e15-b87f-58fc73731410","_uuid":"0a65fb7e17c2e3860f5a6c8f13978664239f14c1"},"cell_type":"markdown","source":"So, the passenger with missing Fare value is Pclass of 3, with no SibSp and Parch and Embarked from Southampton. We will pick another passenger with same features values and use that fare feature value to fill this one."},{"metadata":{"_cell_guid":"baf6014b-65a0-4046-8ba8-ddd06fe4a3a0","_uuid":"b9469ad999ccc9d4d3dd07da9de0b556776ecf7d","scrolled":true,"trusted":true},"cell_type":"code","source":"use_fare = test_set[(test_set['Pclass'] == 3) & \n                    (test_set['SibSp'] == 0) & \n                    (test_set['Parch'] == 0) &\n                    (test_set['Embarked'] == 'S')]\ntest_set['Fare'] = test_set['Fare'].fillna(use_fare['Fare'].iloc[0]);\ntest_stats = test_set.describe(include='all')\n\nprint(test_stats.loc['count', 'Fare'])","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"69868ab4-a08f-4994-9c22-7f43e08ec3f7","_uuid":"2ed43fd8f7047493ba9167a81e5c3f777d647f4c"},"cell_type":"markdown","source":"So now we have a complete training and test data with no missing values. So let's move on to feature engineering.\n\n<u>Feature Engineering</u> is the process of using the knowledge of given data to engineer features such that it helps create better features for machine learning algorithms. The better the features are, the better the model will be.\n\nLet's begin feature engineering with **Age Feature**. Currently Age is a continous feature, we will be dividing these into different age groups hence turning them into categorical feature, namely, Kid/Teenager (less than 20), Young/Adult (20-40), Mature (40-60) and Elderly (>60).\n\nThe way I am going to add these age groups is by creating four different columns instead of one column. The reason behind this is before we train our model using this data, the values have to be encoded and followed by One Hot Encoding. I will write more about this later. "},{"metadata":{"_cell_guid":"82eefa1d-beda-456e-a56d-4a525b9c6e05","_uuid":"543f61ab4e7a25ad2825749995d2e33937d83e8a","scrolled":true,"trusted":true},"cell_type":"code","source":"# Engineer the Age data, drop the Age feature and add engineered categorical Age feature.\n# If a passenger is less than or equal to 20, then Kid/Teenager, if greater than 20 and\n# less than or equal to 40 then Young/Adult and so on...\ntraining_set['Kid/Teenager'] = np.where(training_set['Age'] <= 20, 1, 0)\ntraining_set['Young/Adult'] = np.where((training_set['Age'] > 20) & (training_set['Age'] <= 40), 1, 0)\ntraining_set['Mature'] = np.where((training_set['Age'] > 40) & (training_set['Age'] <= 60), 1, 0)\ntraining_set['Elderly'] = np.where(training_set['Age'] > 60, 1, 0)\n\ntest_set['Kid/Teenager'] = np.where(test_set['Age'] <= 20, 1, 0)\ntest_set['Young/Adult'] = np.where((test_set['Age'] > 20) & (test_set['Age'] <= 40), 1, 0)\ntest_set['Mature'] = np.where((test_set['Age'] > 40) & (test_set['Age'] <= 60), 1, 0)\ntest_set['Elderly'] = np.where(test_set['Age'] > 60, 1, 0)\n\n# Now we can drop the Age column\ntraining_set = training_set.drop(['Age'], axis=1)\ntest_set = test_set.drop(['Age'], axis=1)\n\n# Lets view training data now\ntraining_set.head(n=10)","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"925ace6e-e264-44c8-8f4f-3807fa8d0e67","_uuid":"6dbe4701d04d86076bec5f72752fc34aec31c5dc","trusted":true},"cell_type":"code","source":"# Lets view test data now\ntest_set.head(n=10)","execution_count":37,"outputs":[]},{"metadata":{"_cell_guid":"91749e05-2aea-49aa-8f1a-b3d12dea02ed","_uuid":"fea6e0f83f982fd1f9bb8f519fc4fcdad580720e"},"cell_type":"markdown","source":"Now let's engineer **SibSp and Parch feature**. From the plots above, we couldn't extract useful relations between these feature and Survived label. So, let's combine them into one feature - **HasFamily** and see if we are are able to understand the relation. If the passenger has sibling/spouse or parent/child then the HasFamily feature will be true (1) and if not, false (0)."},{"metadata":{"_cell_guid":"dba97847-bccd-44ea-933e-d4225ed22ba3","_uuid":"bf3cab1776252afc70ebdbfb6905abd28613521e","trusted":true},"cell_type":"code","source":"training_set['HasFamily'] = np.where(training_set['SibSp'] + training_set['Parch'] > 0, 1, 0)\ntest_set['HasFamily'] = np.where(test_set['SibSp'] + test_set['Parch'] > 0, 1, 0)\n\n# Now we can drop SibSp and Parch columns\ntraining_set = training_set.drop(['SibSp', 'Parch'], axis=1)\ntest_set = test_set.drop(['SibSp', 'Parch'], axis=1)\n\n# Lets view training data now\ntraining_set.head(n=10)","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"eac54fb2-ed3a-490a-b5ff-f2c39995a5f8","_uuid":"5c0e7747bccd102a016aeaf640d9108df2927ad1","trusted":true},"cell_type":"code","source":"# Lets view test data now\ntest_set.head(n=10)","execution_count":39,"outputs":[]},{"metadata":{"_cell_guid":"debb3354-a8d8-478e-b286-601e0faa718d","_uuid":"d22c1e67eb6013efef3cf49c155f7606e4e06d19"},"cell_type":"markdown","source":"Let's take a quick look at the relation between HasFamily and Survived now. \n\n### HasFamily Versus Survived"},{"metadata":{"_cell_guid":"34bd417a-cfd3-4386-afb3-564233976a28","_uuid":"13a93c80859a5fc9f009eba4a2d91bc278be8b87","trusted":true},"cell_type":"code","source":"# We can compare percentages between passengers, who has family, survived the disaster versus did not\nhasfamily = len(training_set[(training_set['HasFamily'] == 1)])\nfam_survived = (len(training_set[(training_set['HasFamily'] == 1) & (training_set['Survived'] == 1)]) / hasfamily) * 100\nfam_didnotsurvive = (len(training_set[(training_set['HasFamily'] == 1) & (training_set['Survived'] == 0)]) / hasfamily) * 100\n\nprint (\"{0:.2f}\".format(fam_survived) + \"% of passenger with family survived\")\nprint (\"{0:.2f}\".format(fam_didnotsurvive) + \"% of passenger with family did not survive\")","execution_count":40,"outputs":[]},{"metadata":{"_cell_guid":"472d31c7-e337-4517-8bcf-77c91968ce89","_uuid":"83bc4cc81b508a3b257e837f28da4f5af6b52263","trusted":true},"cell_type":"code","source":"# Now lets compare percentages between passengers, who do not have family, survived the disaster versus did not\nnofamily = len(training_set[(training_set['HasFamily'] == 0)])\nnofam_survived = (len(training_set[(training_set['HasFamily'] == 0) & (training_set['Survived'] == 1)]) / nofamily) * 100\nnofam_didnotsurvive = (len(training_set[(training_set['HasFamily'] == 0) & (training_set['Survived'] == 0)]) / nofamily) * 100\n\nprint (\"{0:.2f}\".format(nofam_survived) + \"% of passenger with no family survived\")\nprint (\"{0:.2f}\".format(nofam_didnotsurvive) + \"% of passenger with no family did not survive\")","execution_count":41,"outputs":[]},{"metadata":{"_cell_guid":"dd148ff7-b78d-476b-b5f8-485410d06fc7","_uuid":"cf9530e2b155a3fcac9543f5cbb19b3b1a86aed0"},"cell_type":"markdown","source":"So, there is no real difference in survival chances if you have a family but surprisingly with you have no family then there are almost 70% chances that a passenger did not survive. We will explore both options with HasFamily and without HasFamily to see if we should keep or drop this feature later when building and comparing models."},{"metadata":{"_uuid":"7d73a06f309882064456a31a8745708436b6847f"},"cell_type":"markdown","source":"The last one to engineer is the **Fare feature**. We will extract Fare-Group feature out of Fare feature by dividing it into four groups that represents range of fare feature values. "},{"metadata":{"trusted":true,"_uuid":"0d3a50fcf3716711a78b3a99ecb0c5675415781d"},"cell_type":"code","source":"# Apply pandas qcut to Fare feature\nfare_groups = pd.qcut(training_set['Fare'], 4)\nfare_groups.unique()","execution_count":56,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41e34fb94dde2a207389e0deeec9c26158f745f6"},"cell_type":"code","source":"# Lets assign values to these fare group feature as we did to age feature\ntraining_set['Fare-Group1'] = np.where(training_set['Fare'] <= 7.91, 1, 0)\ntraining_set['Fare-Group2'] = np.where((training_set['Fare'] > 7.91) & (training_set['Fare'] <= 14.454), 1, 0)\ntraining_set['Fare-Group3'] = np.where((training_set['Fare'] > 14.454) & (training_set['Fare'] <= 31), 1, 0)\ntraining_set['Fare-Group4'] = np.where(training_set['Fare'] > 31, 1, 0)\n\ntest_set['Fare-Group1'] = np.where(test_set['Fare'] <= 7.91, 1, 0)\ntest_set['Fare-Group2'] = np.where((test_set['Fare'] > 7.91) & (test_set['Fare'] <= 14.545), 1, 0)\ntest_set['Fare-Group3'] = np.where((test_set['Fare'] > 14.454) & (test_set['Fare'] <= 31), 1, 0)\ntest_set['Fare-Group4'] = np.where(test_set['Fare'] > 31, 1, 0)\n\n# View the training data\ntraining_set.head(n=10)","execution_count":62,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b4d884bd186cf120f70baef2542f97a5624fa6c"},"cell_type":"code","source":"# View the test data\ntest_set.head(n=10)","execution_count":69,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"33ef7546b54d56d893612354785b3be06add726b"},"cell_type":"code","source":"# We can now drop the Fare feature out of training and test data\ntraining_set = training_set.drop(['Fare'], axis = 1)\ntest_set = test_set.drop(['Fare'], axis = 1)","execution_count":68,"outputs":[]},{"metadata":{"_cell_guid":"f86bbb46-cb5d-4a21-86c1-fc46967aeedd","_uuid":"c2ce8f6235505e24413425e840c4885da7cf6186"},"cell_type":"markdown","source":"Now that feature engineering is complete, let's encode all the feature, apply one hot encoder (if needed), avoid dummy variable trap and prepare data to build models. The term **Encoding** refers to converting the data into integer form such that machine learning models can intepret them. For example, converting Sex feature into 1 (male) and 0 (female), converting Embarked feature into 1 (Q), 2  S) and 3 (C) by applying encoding. "},{"metadata":{"_cell_guid":"b3593ea6-6970-447b-8f34-b5ef3e7743ec","_uuid":"243129689c239d02dd6cfa673294513c1be6d5d1","trusted":true},"cell_type":"code","source":"# Encode categorical feature - Sex\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntraining_set['Sex'] = encoder.fit_transform(training_set['Sex'])\n\n# Lets view the training data now\ntraining_set.head(n=10)","execution_count":70,"outputs":[]},{"metadata":{"_cell_guid":"d35c9f8c-6702-4135-872a-d2002ca28c21","_uuid":"fe0753be73279fbf3e442f23bc615d682becf62f","trusted":true},"cell_type":"code","source":"# encode test data Sex feature\nencoder = LabelEncoder()\ntest_set['Sex'] = encoder.fit_transform(test_set['Sex'])\n\n# Lets view the training data now\ntest_set.head(n=10)","execution_count":71,"outputs":[]},{"metadata":{"_cell_guid":"998e645a-5bb2-4bbe-8e8d-9e2d71d58da7","_uuid":"389a3e534a978b20bd618cb08721912deb5781c4","trusted":true},"cell_type":"code","source":"# Now lets encode Embarked feature\nencoder = LabelEncoder()\ntraining_set['Embarked'] = encoder.fit_transform(training_set['Embarked'])\n\n# Lets view the training data now\ntraining_set.head(n=10)","execution_count":72,"outputs":[]},{"metadata":{"_cell_guid":"cf0b621b-6be7-40d2-b479-5dba729ebe58","_uuid":"a292d04b802b89260322470df1b1579a844e7ede","trusted":true},"cell_type":"code","source":"# encode test data Embarked feature\nencoder = LabelEncoder()\ntest_set['Embarked'] = encoder.fit_transform(test_set['Embarked'])\n\n# Lets view the training data now\ntest_set.head(n=10)","execution_count":73,"outputs":[]},{"metadata":{"_cell_guid":"e5248879-fde0-4a33-8523-3f1d5a6fbefb","_uuid":"b72ee63780ca63655b66d33d4373c2cb2c0fc630"},"cell_type":"markdown","source":"Therefore, we now see that all of the features have been converted into integer form but there is an issue with the Embarked feature. We have more than two different integer values for Embarked feature. So, in order to eliminate issues of model assuming one Embarked feature value being greater than other i.e. assuming one feature value is more important than others, we will be producing new features converting these into binary form. This is known as **One Hot Encoding**. These new features are also called dummy variables."},{"metadata":{"_cell_guid":"b39ef11a-7ea5-4c5b-b408-2da6d16b5e09","_uuid":"b0f04c8c7a6aef178e28cb66878d64480fccabad","trusted":true},"cell_type":"code","source":"# Apply One-Hot encoding to Embarked feature in training data\n#training_set = pd.get_dummies(data=training_set, prefix=['Embarked'], columns=['Embarked'])\n#training_set.head(n=10)","execution_count":74,"outputs":[]},{"metadata":{"_cell_guid":"f3b3ea64-c7e6-4647-b1e7-5c4dabdb8eca","_uuid":"bb713748e52f6ace1a14dbb432dd8070f527a09d","trusted":true},"cell_type":"code","source":"# Apply One-Hot encoding to Embarked feature in test data\n#test_set = pd.get_dummies(data=test_set, prefix=['Embarked'], columns=['Embarked'])\n#test_set.head(n=10)","execution_count":75,"outputs":[]},{"metadata":{"_cell_guid":"c7972313-83bf-47bb-8daa-6bda34b30af0","_uuid":"ec7ad9036631703f00b479d7335357c521a19fab","trusted":true},"cell_type":"code","source":"# Apply One-Hot Encoding to Pclass feature in training data\n#training_set = pd.get_dummies(data=training_set, prefix=['Pclass'], columns=['Pclass'])\n#training_set.head(n=10)","execution_count":76,"outputs":[]},{"metadata":{"_cell_guid":"c91bfc72-ed68-4adf-8d91-a4d12e455906","_uuid":"e9fa25f4fa7e15adca3c21bfe113dc1d44960ae5","trusted":true},"cell_type":"code","source":"# Apply One-Hot Encoding to Pclass feature in training data\n#test_set = pd.get_dummies(data=test_set, prefix=['Pclass'], columns=['Pclass'])\n#test_set.head(n=10)","execution_count":77,"outputs":[]},{"metadata":{"_cell_guid":"82ea4a33-b44a-48eb-b8ed-574f92463d20","_uuid":"23a5d6e9ff28bdd02a276b98f2f52af4df2bf790"},"cell_type":"markdown","source":"Now One Hot Encoding is complete and also the pandas get dummies class took care of dropping the Embarked and Pclass feature that we one-hot encoded. Now lets eliminate dummy variable trap.  Dummy Variable trap refers to avoiding perfect multicollinearity. This can be done by simply dropping a column out of columns that were one hot encoded. For example, in our data, lets frop the Embarked_2 feature and Pclass_3 feature."},{"metadata":{"_cell_guid":"6252695b-31d6-4eed-9dd4-b76fa3bc686c","_uuid":"bbb60d4048882466bd34c0104e014234256f7053","trusted":true},"cell_type":"code","source":"# Drop columns to avoid dummy variable trap in our training set\n#training_set = training_set.drop(['Embarked_2', 'Pclass_3', 'Elderly', 'Fare-Group4'], axis=1)\n#training_set.head(n=10)","execution_count":78,"outputs":[]},{"metadata":{"_cell_guid":"4b97f819-71fb-4af7-bb0b-cf71e6caa2fa","_uuid":"03c2e310d5629d324eba9cdf217645cd633957f2","trusted":true},"cell_type":"code","source":"# Drop columns to avoid dummy variable trap in our test set\n#test_set = test_set.drop(['Embarked_2', 'Pclass_3', 'Elderly', 'Fare-Group4'], axis=1)\n#test_set.head(n=10)","execution_count":79,"outputs":[]},{"metadata":{"_cell_guid":"538284d1-0df8-4ce5-9306-79381bbac849","_uuid":"2d2a736c1d7efba0d24fb6494453e80bd7f64bc9"},"cell_type":"markdown","source":"Finally, now we have to apply feature scaling to the data. **Feature scaling** is applied in order to eliminate the possibility of one feature dominating the other feature because of the values they contain. But before we do that let's seperate our data into features and label and apply scaling afterwards."},{"metadata":{"_cell_guid":"ce0158f4-8783-4316-92b4-f262001e389b","_uuid":"95fec0e4dfb4bc643bcc36cfa2b727f43b9aadb9","trusted":true},"cell_type":"code","source":"# All the columns in our training data expect Survived represent our features\nfeatures_train = training_set.loc[:, training_set.columns != 'Survived'].values\nlabel_train = training_set.loc[:, training_set.columns == 'Survived'].values\n# features is now ndarray type (Sparse matrix of shape) since our model (classifiers) take this type while fitting\nprint(features_train[0])","execution_count":91,"outputs":[]},{"metadata":{"_cell_guid":"500ad133-b1dd-44f9-9261-d3f2869fdbfa","_uuid":"92fc2f3a381b0543773362b847e220d28f772f77","trusted":true},"cell_type":"code","source":"# labels\nprint(label_train[0])","execution_count":92,"outputs":[]},{"metadata":{"_cell_guid":"2777bcca-416c-45e4-819e-07099a63bac6","_uuid":"648d281f9e7d05196bc181afeb165baa6cc76056","trusted":true},"cell_type":"code","source":"# Apply feature scaling to our features_train\n#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n#features_train = scaler.fit_transform(features_train)\n#print(features_train)","execution_count":82,"outputs":[]},{"metadata":{"_cell_guid":"a81fa349-a163-43de-a827-04d410d8e68b","_uuid":"0439e5b71ee6b3c00d38fb3115bce5c99088ba8a"},"cell_type":"markdown","source":"## 4. Build various models and compare\n\nThe data preprocessing and feature engineering is complete and data has been prepared to start building machine learning models and comparing them. From the given data we can tell that this is a **Classification** problem. Classification is an supervised machine learning algorithm where the prediction values (label) are in discrete form. For example, in this data, we are predicting either a passenger with some features survived or did not suvive the titanic disaster. The other type is a Regression problem where the prediction values are in continous form. For example, predicting a measure of rainfall volume based on the activities of clouds for weather.\n\nWe will be utilizing following classification algorithms to build our model:\n1. K-Nearest Neighbors\n2. Support Vector Machines\n3. Naive Bayes\n4. Decision Trees\n5. Random Forest"},{"metadata":{"_cell_guid":"0c14199d-f523-409e-b1d9-7ee6460db686","_uuid":"837b8e941836918a82e5d16fbdbd9a9058ba6f9f","collapsed":true,"trusted":true},"cell_type":"code","source":"# First lets prepare few things to avoid redundant coding and for result visualization\n\n# Create a dataframe to store algorithms and their accuracies\nalgo_accuracy = pd.DataFrame(columns = ['Algorithm', 'Accuracy'])\n\n# Create a method that gets the mean accuracy score obtained by the classifer (model)\n# And store the accuracy in algo_accuracy\ndef get_store_accuracy(classifier, clf_name, X, y):\n    accuracy = classifier.score(X, y) * 100\n    algo_accuracy.loc[len(algo_accuracy)] = [clf_name, \"{0:.2f}\".format(accuracy) + ' %']","execution_count":83,"outputs":[]},{"metadata":{"_cell_guid":"8069ead4-f2c4-45f0-884e-5c1b5e6f076e","_uuid":"56563e8a9dcd24d1c896267acbdff668eb69a556","collapsed":true,"trusted":true},"cell_type":"code","source":"# 1. K-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn.fit(features_train, label_train.ravel())\n\n# store accuracy\nget_store_accuracy(knn, 'K-Nearest Neighbors', features_train, label_train)","execution_count":84,"outputs":[]},{"metadata":{"_cell_guid":"29d4ae54-3c73-4e7b-872f-02cdf0e5aa9c","_uuid":"9e5bb8aee906d2d5080040cff9fb5dde16d6f550","collapsed":true,"trusted":true},"cell_type":"code","source":"# 2. Support Vector Machines\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='rbf')\nsvc.fit(features_train, label_train.ravel())\n\n# store accuracy\nget_store_accuracy(svc, 'Support Vector Machines', features_train, label_train)","execution_count":85,"outputs":[]},{"metadata":{"_cell_guid":"81e9d6ef-f38f-4364-b70c-e326e317b471","_uuid":"911e3c90db45b4587c308311e2dc73ceac9001f0","collapsed":true,"trusted":true},"cell_type":"code","source":"# 3. Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(features_train, label_train.ravel())\n\n# store accuracy\nget_store_accuracy(nb, 'Naive Bayes', features_train, label_train)","execution_count":86,"outputs":[]},{"metadata":{"_cell_guid":"6ed0a0a5-a37d-4b7d-a9c5-a1157bbb351b","_uuid":"f3c0ee91026989aeb7ea2aff4a6051e4c67d0a85","collapsed":true,"trusted":true},"cell_type":"code","source":"# 4. Decision Trees\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(criterion = 'entropy')\ndecision_tree.fit(features_train, label_train.ravel())\n\n# store accuracy\nget_store_accuracy(decision_tree, 'Decision Trees', features_train, label_train)","execution_count":87,"outputs":[]},{"metadata":{"_cell_guid":"476df737-5d76-4bd8-b747-094a3e9b5506","_uuid":"baa13a0dc6d4dfa459f5733b05c0f8aee808cbe4","collapsed":true,"trusted":true},"cell_type":"code","source":"# 4. Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators = 20, criterion = 'entropy')\nrandom_forest.fit(features_train, label_train.ravel())\n\n# store accuracy\nget_store_accuracy(random_forest, 'Random Forest', features_train, label_train)","execution_count":89,"outputs":[]},{"metadata":{"_cell_guid":"7a98c80f-4833-4b7e-b163-a1119b2a9a12","_uuid":"7dd22a1f15035aa738f41cc406e20ff0130df803","trusted":true},"cell_type":"code","source":"# Lets view and compare the Algorithms and their accuracies\nalgo_accuracy.head(n=10)","execution_count":90,"outputs":[]},{"metadata":{"_cell_guid":"93cb9e8d-f544-43bf-b3fb-c0dc5308f2ac","_uuid":"7b4f5edcffe5bd3cc00b8871e5c40a9aa73dbeb6"},"cell_type":"markdown","source":"From above accuracy data, we can see clearly that Decision Trees and Random Forest are the best classifiers (model) for this analysis. "},{"metadata":{"_cell_guid":"53fe6cbd-da0b-40a6-85b3-14079b097a26","_uuid":"98ea3e8d1efcdfaa26e4f3153c0c51f54da4f87d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Use decision tree classifier to predict the test results\ndecision_tree_preds = decision_tree.predict(test_set)\n\n# Build a submission file\nsubmission = pd.DataFrame({\n        \"PassengerId\": orig_test_set[\"PassengerId\"],\n        \"Survived\": decision_tree_preds\n    })\nsubmission.to_csv('Preds_Titanic_V.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}