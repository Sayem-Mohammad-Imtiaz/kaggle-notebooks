{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T10:49:29.703705Z","iopub.execute_input":"2021-06-10T10:49:29.704251Z","iopub.status.idle":"2021-06-10T10:49:29.718208Z","shell.execute_reply.started":"2021-06-10T10:49:29.704164Z","shell.execute_reply":"2021-06-10T10:49:29.717311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:50:57.885362Z","iopub.execute_input":"2021-06-10T10:50:57.886112Z","iopub.status.idle":"2021-06-10T10:50:58.778656Z","shell.execute_reply.started":"2021-06-10T10:50:57.886063Z","shell.execute_reply":"2021-06-10T10:50:58.777544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/social-network-ads/Social_Network_Ads.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:25:17.074262Z","iopub.execute_input":"2021-06-10T12:25:17.074676Z","iopub.status.idle":"2021-06-10T12:25:17.084582Z","shell.execute_reply.started":"2021-06-10T12:25:17.074643Z","shell.execute_reply":"2021-06-10T12:25:17.083497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:25:17.724426Z","iopub.execute_input":"2021-06-10T12:25:17.724796Z","iopub.status.idle":"2021-06-10T12:25:17.739879Z","shell.execute_reply.started":"2021-06-10T12:25:17.724766Z","shell.execute_reply":"2021-06-10T12:25:17.738656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:25:18.922737Z","iopub.execute_input":"2021-06-10T12:25:18.92309Z","iopub.status.idle":"2021-06-10T12:25:18.949328Z","shell.execute_reply.started":"2021-06-10T12:25:18.92306Z","shell.execute_reply":"2021-06-10T12:25:18.948235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:25:20.107581Z","iopub.execute_input":"2021-06-10T12:25:20.107958Z","iopub.status.idle":"2021-06-10T12:25:20.117672Z","shell.execute_reply.started":"2021-06-10T12:25:20.107926Z","shell.execute_reply":"2021-06-10T12:25:20.116681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this dataset purchased is the dependent feature where are \"userid, Gender, Age and EstimatedSalary\" is the independent feature. We will not consider userid and gender in our model building as this is not expressing much with dependent variable. ","metadata":{}},{"cell_type":"code","source":"#Split X and y\nX=df.iloc[:,2:4]\ny=df['Purchased']\nprint(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:13.953104Z","iopub.execute_input":"2021-06-10T12:26:13.953475Z","iopub.status.idle":"2021-06-10T12:26:13.9597Z","shell.execute_reply.started":"2021-06-10T12:26:13.953431Z","shell.execute_reply":"2021-06-10T12:26:13.958618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split X and y into train and test\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=21)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:20.382253Z","iopub.execute_input":"2021-06-10T12:26:20.382612Z","iopub.status.idle":"2021-06-10T12:26:20.390706Z","shell.execute_reply.started":"2021-06-10T12:26:20.382581Z","shell.execute_reply":"2021-06-10T12:26:20.389708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we perfouniquethe feature scalling as Age and Estimated salary both having different range. If you don't do feature scalling then your\n#Estimated salary will do dominent Age feature when the model finds the nearest neighbour to a datapoint in data space.\n\nfrom sklearn.preprocessing import StandardScaler\nSC=StandardScaler()\nX_train=SC.fit_transform(X_train)\nX_test=SC.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:22.525235Z","iopub.execute_input":"2021-06-10T12:26:22.525597Z","iopub.status.idle":"2021-06-10T12:26:22.539858Z","shell.execute_reply.started":"2021-06-10T12:26:22.525564Z","shell.execute_reply":"2021-06-10T12:26:22.538729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finally we make the model using logistic regression\nfrom sklearn.linear_model import LogisticRegression\nmodel_lr=LogisticRegression()\nmodel_lr.fit(X_train,y_train)\nprint(model_lr.intercept_)\nprint(model_lr.coef_)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:24.143344Z","iopub.execute_input":"2021-06-10T12:26:24.143748Z","iopub.status.idle":"2021-06-10T12:26:24.155655Z","shell.execute_reply.started":"2021-06-10T12:26:24.143718Z","shell.execute_reply":"2021-06-10T12:26:24.154502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validate the model with X test and check the performance of the model using confusion metrix\nfrom sklearn.metrics import confusion_matrix\ny_predict=model_lr.predict(X_test)\nprint(\"Confusion Metrix for this model is: \")\nprint(confusion_matrix(y_test,y_predict))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:26.205114Z","iopub.execute_input":"2021-06-10T12:26:26.205506Z","iopub.status.idle":"2021-06-10T12:26:26.212834Z","shell.execute_reply.started":"2021-06-10T12:26:26.205471Z","shell.execute_reply":"2021-06-10T12:26:26.21199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of 100 test data True Positive + True Negetive = 58+27\n\nFalse Positive + False Negetive = 9+6\n\nPerformance Measure : Accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(f\"Accuracy Score is {accuracy_score(y_test,y_predict)}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:49.382521Z","iopub.execute_input":"2021-06-10T12:26:49.382888Z","iopub.status.idle":"2021-06-10T12:26:49.38928Z","shell.execute_reply.started":"2021-06-10T12:26:49.382859Z","shell.execute_reply":"2021-06-10T12:26:49.388295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we have to visualize the performance of our model test dataset\nfrom matplotlib.colors import ListedColormap\nx_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=x_set[:,0].min()-1,stop=x_set[:,0].max()+1,step=0.01),\nnp.arange(start=x_set[:,1].min()-1,stop=x_set[:,1].max()+1,step=0.01))\n\nplt.contourf(X1,X2,model_lr.predict(\n             np.array([X1.ravel(), X2.ravel()]).T).reshape(\n             X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\n      \nplt.title('Classifier (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:51.480839Z","iopub.execute_input":"2021-06-10T12:26:51.481181Z","iopub.status.idle":"2021-06-10T12:26:51.768826Z","shell.execute_reply.started":"2021-06-10T12:26:51.481153Z","shell.execute_reply":"2021-06-10T12:26:51.768084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we have to visualize the performance of our model train dataset\nfrom matplotlib.colors import ListedColormap\nx_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=x_set[:,0].min()-1,stop=x_set[:,0].max()+1,step=0.01),\nnp.arange(start=x_set[:,1].min()-1,stop=x_set[:,1].max()+1,step=0.01))\n\nplt.contourf(X1,X2,model_lr.predict(\n             np.array([X1.ravel(), X2.ravel()]).T).reshape(\n             X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\n      \nplt.title('Classifier (Train set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T12:26:53.303824Z","iopub.execute_input":"2021-06-10T12:26:53.304204Z","iopub.status.idle":"2021-06-10T12:26:53.612098Z","shell.execute_reply.started":"2021-06-10T12:26:53.30417Z","shell.execute_reply":"2021-06-10T12:26:53.611049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graph it is clear visible the line which divides green and purple line.\n\n    **Green Line : Purchased SUV cars**\n    \n    **Purple Line : Didn't purchase SUV cars**\n\n**Here X-axis gives Age and Y-axis gives the esimated salary. The graph plots the dependent variable data point purchased based on this two independent variables**\n\n**Data Points lies on green area is purchased SUV car. Where are data points lies on purple area is not purchased SUV car.**\n\n**Older age is having high estimated salary purchased more SUV where as younger age having less salary is not purchased SUV car.**\n\n**Here few green points are lying on purple area and few purple points are lying on green area. That means few younger age people having high estimated salary and purchased SUV where as few older age people having less salary is not purchasing SUV**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}