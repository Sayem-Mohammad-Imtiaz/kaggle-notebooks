{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Car Insurance Fraud Claim Detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, roc_curve, precision_recall_curve, auc\nfrom sklearn.tree import DecisionTreeClassifier\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\n\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv(\"../input/auto-insurance-claims-data/insurance_claims.csv\")\n\ndrop_columns = [\"_c39\", \"auto_model\", \"policy_bind_date\", \"policy_state\", \"incident_date\",\n               \"incident_state\", \"incident_city\", \"incident_location\", \"policy_csl\"]\n\ndata = data.drop(drop_columns, axis=1)\n\nnew_response = []\nresponse = data.iloc[:, -1]\nfor i in range(len(response)):\n    new_response.append(1 if response[i]=='Y' else 0)\n\ndata[\"fraud_reported\"] = pd.Series(new_response)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false},"cell_type":"markdown","source":"We will be dropping insignificant features like: \n\n- policy_bind_date\n- policy_state\n- incident_date\n- auto_model\n- _c39\n- policy_csl"},{"metadata":{"scrolled":false},"cell_type":"markdown","source":"# Performing initial EDA."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data.age)\nplt.title(\"Histogram of age of the customers\")\nplt.xlabel(\"Age of the customers\")\nplt.ylabel(\"Number of customers\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data.insured_sex)\nplt.title(\"Histogram of gender count of the customers\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Number of customers\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false},"cell_type":"markdown","source":"- We can see that male and female are almost in the same proportion."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data.fraud_reported)\nplt.title(\"Histogram of fraud reported\")\nplt.xlabel(\"response\")\nplt.ylabel(\"Number of responses\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false},"cell_type":"markdown","source":"- We can also see that there is a significant class imbalance, we will be using SMOTE, Synthetic Minority Oversampling Technique to add additional minority class data points."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = data.iloc[:,:-1]\nresponse = data.iloc[:, -1]\n\n# new_response = []\n\n# for i in range(len(response)):\n#     new_response.append(1 if response[i]=='Y' else 0)\n    \n# response = pd.Series(new_response)\n\ncategorical_data = predictors.select_dtypes(exclude=\"number\")\ncategorical_predictors = categorical_data.columns\n\npredictors = predictors.drop(categorical_predictors, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false},"cell_type":"markdown","source":"- Since most of our data is categorical we have two options, assign a integer value to each level of the categorical variable or one-hot encode these categorical variables. \n\n- One major drawback of assigning integer value to each level is that it adds additional charecteristics to the data. For example let's say we have a variable with levels as BMW, Mazda, Mercedes and Subaru and we are assigning 0, 1, 2 and 3 integer values to them respectively. When we apply any model the model considers these values as continuous and assumes an unwanted hierarchy like BMQ < Mazda < Mercedes < Subaru, which might not be the case at all.\n\n- Hence we go with one hot encoding of categorical variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_data = pd.get_dummies(categorical_data)\npredictors = predictors.join(one_hot_data)\n\npredictor_columns = predictors.columns\nresponse_columns = response\n\npredictors_train, predictors_test, response_train, response_test = train_test_split(predictors,\n                                                                                    response,\n                                                                                    test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false},"cell_type":"markdown","source":"- Since we have class imbalance in the data, we perform minority class oversampling using SMOTE, Synthetic Minority Oversampling Technique, which uses K nearest neighbors to come up with new samples in the minority class."},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=24)\npredictors, response = sm.fit_resample(predictors_train, response_train)\n\npredictors_train = pd.DataFrame(predictors, columns=predictor_columns)\nresponse_train = pd.Series(response)\n\nmodel_preds = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(predictors_train, response_train)\npredictions_test = model.predict(predictors_test)\npredictions_train = model.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Logistic Regression\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(predictors_train, response_train)\n\npredictions_train = knn.predict(predictors_train)\npredictions_test = knn.predict(predictors_test)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"K Nearest Neighbor\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Since it has a lot of categorical variables and the dataset is also not huge we \n## will use decision trees to get more accuracy.\n\npredictors_train, predictors_test, response_train, response_test = train_test_split(predictors,response,test_size=0.3)\n\ntree = DecisionTreeClassifier()\ntree.fit(predictors_train, response_train)\npredictions_test = tree.predict(predictors_test)\npredictions_train = tree.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Decision Tree\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(predictors_train, response_train)\npredictions_test = random_forest.predict(predictors_test)\npredictions_train = random_forest.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Random Forest\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LinearDiscriminantAnalysis()\nlda.fit(predictors_train, response_train)\npredictions_test = lda.predict(predictors_test)\npredictions_train = lda.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision_score(predictions_test, response_test)))\nprint(\"Recall = \"+str(recall_score(predictions_test, response_test)))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Linear Discriminant Analysis\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Receiving Operator Charecteristic"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.title(\"ROC curve for various classifiers:\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\n\nfor key, value in model_preds.items():\n    model_list = model_preds[key]\n    plt.plot(model_list[0], model_list[1], label=key)\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we can see from the above ROC curves and results LDA is performing well when compared to all the classifiers.\n- KNN is performing the worst out of all the classifiers.\n- I was hoping to get better results with Random Forests but with this size of the data I am not surprised with this result. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}