{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"eabf5109-b5c5-5a7b-ee8e-b569f6ea21ac"},"source":"# Pima Indians Diabetes Database"},{"cell_type":"markdown","metadata":{"_cell_guid":"7544dfdd-23c9-963b-ad0a-903ef7023c04"},"source":"## Predict the onset of diabetes based on diagnostic measures"},{"cell_type":"markdown","metadata":{"_cell_guid":"dee01e2f-c5cb-6cd3-280d-dc8b8fbf78f4"},"source":"The previous notebook is no longer accessible because I had 2 accounts and I did not know that this was forbidden. So I post this one online.For good advices for people like me who begin in machine learning there is this web site http://machinelearningmastery.com/python-machine-learning-mini-course/ given by Jason Brownlee. If you have any advices or any comments don't hesitate."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0a1377e-97da-bf0b-0e89-8537e5a30b07"},"outputs":[],"source":"# Import Libraries\nimport numpy\nimport pandas\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom math import *\nfrom sklearn.metrics import classification_report\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"852d2285-e046-38d3-48b6-68676c4a8cea"},"source":"First, load the data and let's look at some informations about the dataset. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c91702c7-64e6-4ad7-1aeb-702c9da66976"},"outputs":[],"source":"# Number of times pregnant\n# Plasma glucose concentration\n# Diastolic blood pressure\n# Triceps skin fold thickness\n# 2-Hour serum insulin\n# Body mass index\n# Diabetes pedigree function\n# Age\n# Class\nrdm_state = 99\ndata = pandas.read_csv('../input/diabetes.csv')\nprint(data.shape)\nprint(data.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b2e5be2-235a-d021-7038-37484c362524"},"outputs":[],"source":"data.info() # We verify the different informations like missing value."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afa782ec-8c4c-c5dc-08da-6d96442d159c"},"outputs":[],"source":"data.describe() # Complete description of the data."},{"cell_type":"markdown","metadata":{"_cell_guid":"ace6744e-4440-68e1-41ec-6aee54994d62"},"source":"We can see for Glucose, BloodPressure, SkinThickness, Insulin, BMI the minimum is 0 for some fields we will replace them with the mean of the fields not equal to zero. Thank to <a href=\"https://www.kaggle.com/atulnet\">Atul A</a> because it was by looking at his work that I saw this problem. I put the link to access it. <a href=\"https://www.kaggle.com/atulnet/d/uciml/pima-indians-diabetes-database/pima-diabetes-keras-implementation\">Pima Diabetes - Keras implementation</a>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e43f492-3504-2b3f-82e0-4e7db37e30f9"},"outputs":[],"source":"fields = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor field in fields :\n    print('field %s : num 0-entries: %d' % (field, len(data.loc[ data[field] == 0, field ])))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08850019-c904-787f-68f3-ecc9dfa41776"},"outputs":[],"source":"def replace_zero_field(data, field):\n    nonzero_vals = data.loc[data[field] != 0, field]\n    avg = nonzero_vals.median()\n    length = len(data.loc[ data[field] == 0, field])   # num of 0-entries\n    data.loc[ data[field] == 0, field ] = avg\n    print('Field: %s; fixed %d entries with value: %.3f' % (field,length, avg))\n\nfor field in fields :\n    replace_zero_field(data,field)\nprint()\nfor field in fields :\n    print('Field %s : num 0-entries: %d' % (field, len(data.loc[ data[field] == 0, field ])))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d53a1ddc-1800-a6d4-c44d-8376053f9d06"},"outputs":[],"source":"data.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f5db7122-9279-10ee-43c5-4f04291fb086"},"source":"Now we convert the data into an array and split it in two. We separate the array into input and output components."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6f25ecb-c817-c80d-6b59-17b0d604a07f"},"outputs":[],"source":"data_arr = data.values\nX = data_arr[:,0:8]\nY = data_arr[:,8]\nprint(data_arr.shape)\nprint(X.shape)\nprint(Y.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"45a612e3-8acd-8d73-ded3-05bf4da68414"},"source":"Then we split the input and output components into train and test data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"128eb496-e6d8-be01-ed10-fefd8844ca5c","collapsed":true},"outputs":[],"source":"test_size = 0.25\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=rdm_state)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2a4289e7-5b29-c6ba-8cc0-6e50d0854a15"},"source":"Now we search what is the best model for our classification problem."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee7740d5-6a25-17e2-f166-b9eb0f96f634"},"outputs":[],"source":"#Model we are testing \nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('RF',RandomForestClassifier(n_estimators=120, max_features=7)))\nmodels.append(('SVC',svm.SVC(kernel='linear')))\nmodels.append(('QDA',QuadraticDiscriminantAnalysis()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1cf9d04-e78e-1a18-0187-34390348aead"},"outputs":[],"source":"results = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=rdm_state)\n    cv_results = cross_val_score(model, X_train,Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(name, round(100*cv_results.mean(),2),\"%\",\"(+/- \", round(100*cv_results.std(),2),\"% )\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"6cfae419-30c5-17ae-83bf-6ff3c61102c6"},"source":"Preprocessing the data and see the effect on the differents models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d04d584-ed94-7c6c-e59e-5fd386fa24ba","collapsed":true},"outputs":[],"source":"#Normalize X\nnormalized_X = preprocessing.normalize(X)\nX_train, X_test, Y_train, Y_test = train_test_split(normalized_X, Y, test_size=test_size, random_state=rdm_state)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38386d1b-7b4d-8329-beaa-c5a48e0ebbc9"},"outputs":[],"source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=rdm_state)\n    cv_results = cross_val_score(model, X_train,Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(name, round(100*cv_results.mean(),2),\"%\",\"(+/- \", round(100*cv_results.std(),2),\"% )\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"867f597d-03e6-ce1a-6b58-0224d46bdb1b","collapsed":true},"outputs":[],"source":"#Rescale X\nstandardized_X = preprocessing.scale(X)\nX_train, X_test, Y_train, Y_test = train_test_split(standardized_X, Y, test_size=test_size, random_state=rdm_state)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce570042-acd2-84af-74f4-603bef1c583f"},"outputs":[],"source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=rdm_state)\n    cv_results = cross_val_score(model, X_train,Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(name, round(100*cv_results.mean(),2),\"%\",\"(+/- \", round(100*cv_results.std(),2),\"% )\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"445e2f76-8595-1287-4552-26da7e840428"},"source":"It seems like the SVC and Log Regression performs best. We can now do some parameter optimization, let's see the result for these two models with the train and test data without preprocessing for beginning, then rescale the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78aa4c3b-6981-eff4-8396-0a85cf45b430"},"outputs":[],"source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=rdm_state)\n\nmdl = svm.SVC()\n\n# prepare a range of values to test\nparam_grid = [\n  {'C': [0.99,0.1,1,10], 'kernel': ['linear']}\n ]\n\ngrid = GridSearchCV(estimator=mdl, param_grid=param_grid,cv=5,scoring='precision')\ngrid.fit(X_train, Y_train)\n# summarize the results of the grid search\nprint(\"Best score SVC : \",round(100*grid.best_score_,2),\"%\")\nprint(\"Best estimator for SVC parameter C : \",grid.best_estimator_.C)\n\nmdl = LogisticRegression()\n\n# prepare a range of values to test\nparam_grid = [\n  {'C': [0.99,0.1,1,10]}\n ]\n\ngrid1 = GridSearchCV(estimator=mdl, param_grid=param_grid,cv=5)\ngrid1.fit(X_train, Y_train)\n# summarize the results of the grid search\nprint(\"Best score linear regression : \",round(100*grid1.best_score_,2),\"%\")\nprint(\"Best estimator for linear regression parameter C : \",grid.best_estimator_.C)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e2d66ae8-1a6f-51f8-4619-8da42889ce1c"},"source":"## Results with log regression and SVC : "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fba3540-15f7-9525-cd27-fa0169d636b6","collapsed":true},"outputs":[],"source":"def cross_valid(model, X_test, y_test, nb_folds):\n    fold_size = X_test.shape[0] // nb_folds\n    scores = []\n    for i in range(nb_folds):\n        beg = i * fold_size\n        end = (i + 1) * fold_size\n        scores.append(model.score(X_test[beg:end],y_test[beg:end]))\n    return 'Score : {}% (+/- {}%)'.format(round(numpy.mean(scores)*100,2),round(numpy.std(scores)*100,2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44a8f7d4-1d63-411f-ad6b-0fa06f35cafb"},"outputs":[],"source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=rdm_state)\n\nkfold = KFold(n_splits=10, random_state=rdm_state)\nlog = LogisticRegression(C=grid1.best_estimator_.C)\nlog.fit(X_train,Y_train)\nprint(\"Cross validation train data : \",cross_valid(log,X_train,Y_train,5))\nprint(\"Accuracy test data : \",cross_valid(log,X_test,Y_test,5))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd6140bb-01c2-9f69-66a8-72cba3a5ee02"},"outputs":[],"source":"X_train, X_test, Y_train, Y_test = train_test_split(standardized_X, Y, test_size=test_size, random_state=rdm_state)\n\nkfold = KFold(n_splits=10, random_state=rdm_state)\nlog = LogisticRegression(C=grid1.best_estimator_.C)\nlog.fit(X_train,Y_train)\nprint(\"Cross validation train data : \",cross_valid(log,X_train,Y_train,5))\nprint(\"Accuracy test data : \",cross_valid(log,X_test,Y_test,5))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3861b47b-4cb5-eb85-4aea-9580f4d951ce"},"outputs":[],"source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=rdm_state)\n\nkfold = KFold(n_splits=10, random_state=rdm_state)\nsvc = svm.SVC(kernel='linear',C=grid.best_estimator_.C)\nsvc.fit(X_train,Y_train)\nprint(\"Cross validation train data : \",cross_valid(svc,X_train,Y_train,5))\nprint(\"Accuracy test data : \",cross_valid(svc,X_test,Y_test,5))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f91db68-ff84-1191-b409-1b3667aed379"},"outputs":[],"source":"X_train, X_test, Y_train, Y_test = train_test_split(standardized_X, Y, test_size=test_size, random_state=rdm_state)\n\nkfold = KFold(n_splits=10, random_state=rdm_state)\nsvc = svm.SVC(kernel='linear',C=grid.best_estimator_.C)\nsvc.fit(X_train,Y_train)\nprint(\"Cross validation train data : \",cross_valid(svc,X_train,Y_train,5))\nprint(\"Accuracy test data : \",cross_valid(svc,X_test,Y_test,5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d54c10ab-f8b1-c7a0-bfa8-5c47e8f4fd3d"},"source":"# Importance of features :"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e23dc0ef-7853-c267-6b43-98df9db4e036"},"outputs":[],"source":"featuresImportances = log.coef_\nindicesColumns = numpy.argsort(featuresImportances,axis=None)\nindicesColumns = indicesColumns[::-1] #reverse the list\nfeaturesImportances = featuresImportances[0]\n\n#From the most to the least important\nfor i in range(data.shape[1]-1):\n    print(i+1,data.columns.values[indicesColumns[i]],\":\",featuresImportances[indicesColumns[i]])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a24223a-6c35-3170-9468-b61050097dd3"},"outputs":[],"source":"featuresImportances = log.coef_\nindicesColumns = numpy.argsort(featuresImportances,axis=None)\nindicesColumns = indicesColumns[::-1]\nfeaturesImportances = featuresImportances[0]\n\n#From the most to the least important\nfor i in range(data.shape[1]-1):\n    print(i+1,data.columns.values[indicesColumns[i]],\":\",featuresImportances[indicesColumns[i]])"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2fa2010-7020-4a0c-34a4-9f03d94e022a"},"source":"# Best result with preprocessing and using linear regression\n## Cross validation train data :  77.22 % (+/- 4.96 %)\n## Accuracy test data : 79.47 %"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}