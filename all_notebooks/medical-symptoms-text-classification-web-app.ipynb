{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Medical Symptoms Text Classification\n\n> The adoption of natural language processing in healthcare is rising because of its recognized potential to search, analyze and interpret mammoth amounts of patient datasets. Using advanced medical algorithms, machine learning in healthcare and NLP technology services have the potential to harness relevant insights and concepts from data that was previously considered buried in text form. NLP in healthcare media can accurately give voice to the unstructured data of the healthcare universe, giving incredible insight into understanding quality, improving methods, and better results for patients.\n\n![](https://i.imgur.com/SJPzebD.png)\n\nThe web app is available at: https://medical-symptoms-classifier.herokuapp.com/\n\nSource Code: https://github.com/gabbygab1233/Medical-Symptoms-Classifier","metadata":{}},{"cell_type":"code","source":"#!pip install pyspellchecker\n#!pip install neattext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport altair as alt\nimport pickle\nimport string\nimport spacy\nimport nltk \nimport re\n\nfrom sklearn.naive_bayes import *\nfrom sklearn.ensemble import *\nfrom sklearn.neighbors import *\nfrom sklearn.tree import *\nfrom sklearn.calibration import *\nfrom sklearn.linear_model import *\nfrom sklearn.multiclass import *\nfrom sklearn.svm import *\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom nltk.stem import WordNetLemmatizer \nfrom collections import Counter\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc, roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n#nltk.download('stopwords') \nsns.set(style='whitegrid')\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n# Load data\ndf = pd.read_csv('../input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent/overview-of-recordings.csv')\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Exploratory Data Analysis**](http://)","metadata":{}},{"cell_type":"code","source":"#Analyze Data\ndef explore_data(df):\n    print(f\"The data contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n    print('\\n')\n    print('Dataset columns:',df.columns)\n    print('\\n')\n    print(df.info())\n\nexplore_data(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Checking for Nan Values and duplicates**Â¶](http://)","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def checking_removing_duplicates(df):\n    count_dups = df.duplicated().sum()\n    print(\"Number of Duplicates: \", count_dups)\n    if count_dups >= 1:\n        df.drop_duplicates(inplace=True)\n        print('Duplicate values removed!')\n    else:\n        print('No Duplicate values')\nchecking_removing_duplicates(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Different types of analysis require different data format**](http://)\n* **Corpus** - a collection of text\n* **Document-Term Matrix** - word counts in matrix format","metadata":{}},{"cell_type":"markdown","source":"# **Corpus**","metadata":{}},{"cell_type":"code","source":"df_text = df[['phrase', 'prompt']]\ndf_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Document-Term Matrix**","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer()\ndf_cv = cv.fit_transform(df_text.phrase)\ndata_dtm = pd.DataFrame(df_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = df_text.index\ndata_dtm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add features\n# Number of characters in the text\ndf_text['phrase_length'] = df_text['phrase'].apply(len)\n# Number of words in the text\ndf_text['phrase_num_words'] = df_text['phrase'].apply(lambda x: len(x.split()))\n# Average length of the words in the text\ndf_text[\"mean_word_len\"] = df_text[\"phrase\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n# Number of non-stopwords in the text\ndf_text['phrase_non_stopwords'] = df_text['phrase'].apply(lambda x: len([t for t in x.split() if t not in STOP_WORDS]))\ndf_text.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_dist = df_text['prompt'].value_counts().reset_index().rename(columns={'index':'Label', 'prompt':'Count'})\ncat_dist.drop(0, axis=0, inplace=True)\nalt.Chart(cat_dist).mark_bar(opacity=0.7).encode(\n    x=alt.X('Count', title='Count'),\n    y=alt.Y('Label', sort='-x',title='Category'),\n    tooltip=['Label','Count']\n).properties(height=800,width=700,title=\"Class Distribution\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = df_text['prompt'].values\ncounter = Counter(target)\nfor k,v in counter.items():\n    per = v / len(target) * 100\n    print('Class=%s, Count=%d, Percentage=%.3f%%' % (k, v, per))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alt.data_transformers.disable_max_rows()\nalt.Chart(df_text).mark_bar(color=\"violet\",opacity=0.7,\n    interpolate='step').encode(\n    alt.X(\"phrase_length:Q\",  bin=alt.Bin(maxbins=100), title='Phrase Length Class'),\n    alt.Y('count()', axis=alt.Axis(labels=False), title='Frequency'),\n    tooltip=['phrase_length']\n).properties(\n    height=400,\n    width=700, title=\"Length Distribution\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Text Cleaning**](http://)\nYou cannot go straight from raw text to fitting a machine learning or deep learning model. You\nmust clean your text first, which means splitting it into words and handling punctuation and\ncase.\n* Load the raw text\n\n* Split into tokens\n\n* Convert to lowercase\n\n* Remove punctuation from each token.\n\n* Filter out remaining tokens that are not alphabetic.\n\n* Filter out tokens that are stop words\n\n* Filter out short tokens by checking their length\n\n## **Additional Text Cleaning Considerations**\n* Extracting text from markup like HTML, PDF, or other structured document formats.\n\n* Transliteration of characters from other languages into English.\n\n* Decoding Unicode characters into a normalized form, such as UTF8.\n\n* Handling of domain specific words, phrases, and acronyms.\n\n* Handling or removing numbers, such as dates and amounts.\n\n* Locating and correcting common typos and misspellings.","metadata":{}},{"cell_type":"code","source":"def clean_txt(docs):\n    lemmatizer = WordNetLemmatizer() \n    # split into words\n    speech_words = nltk.word_tokenize(docs)\n    # convert to lower case\n    lower_text = [w.lower() for w in speech_words]\n    # prepare regex for char filtering\n    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n    # remove punctuation from each word\n    stripped = [re_punc.sub('', w) for w in lower_text]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter out stop words\n    words = [w for w in words if not w in  list(STOP_WORDS)]\n    # filter out short tokens\n    words = [word for word in words if len(word) > 2]\n    #Stemm all the words in the sentence\n    lem_words = [lemmatizer.lemmatize(word) for word in words]\n    combined_text = ' '.join(lem_words)\n    return combined_text\n\n# Cleaning the text data\ndf_text['cleaned_phrase'] = df_text['phrase'].apply(clean_txt)\ndf_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_splits = FreqDist(df_text['phrase'])\nprint(f\"***** 10 most common strings ***** \\n{freq_splits.most_common(10)}\", \"\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Wordcloud for each class**](http://)","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nplt.rcParams['figure.figsize'] = [10, 6]\n# generate word cloud and show it\n\nfor x in df_text.prompt.unique():\n    wc = WordCloud(background_color=\"black\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)\n    wc.generate(df_text.cleaned_phrase[(df_text.cleaned_phrase.notnull()) & (df_text.prompt == x)].to_string())\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.title(x, fontdict={'size':16,'weight':'bold'})\n    plt.axis(\"off\")\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Text Data Preparation and Model Training](http://)\nText data requires special preparation before you can start using it for predictive modeling. The\ntext must be parsed to remove words, called tokenization. Then the words need to be encoded\nas integers or floating point values for use as input to a machine learning algorithm, called\nfeature extraction (or vectorization).\n","metadata":{}},{"cell_type":"code","source":"# Spot-Check Normalized Text Models\ndef NormalizedTextModel(nameOfvect):\n    if nameOfvect == 'countvect':\n        vectorizer = CountVectorizer()\n    elif nameOfvect =='tfvect':\n        vectorizer = TfidfVectorizer()\n    elif nameOfvect == 'hashvect':\n        vectorizer = HashingVectorizer()\n\n    pipelines = []\n    pipelines.append((nameOfvect+'MultinomialNB'  , Pipeline([('Vectorizer', vectorizer),('NB'  , MultinomialNB())])))\n    pipelines.append((nameOfvect+'CCCV' , Pipeline([('Vectorizer', vectorizer),('CCCV' , CalibratedClassifierCV())])))\n    pipelines.append((nameOfvect+'KNN' , Pipeline([('Vectorizer', vectorizer),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfvect+'CART', Pipeline([('Vectorizer', vectorizer),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfvect+'PAC'  , Pipeline([('Vectorizer', vectorizer),('PAC'  , PassiveAggressiveClassifier())])))\n    pipelines.append((nameOfvect+'SVM' , Pipeline([('Vectorizer', vectorizer),('RC' , RidgeClassifier())])))\n    pipelines.append((nameOfvect+'AB'  , Pipeline([('Vectorizer', vectorizer),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfvect+'GBM' , Pipeline([('Vectorizer', vectorizer),('GMB' , GradientBoostingClassifier())])))\n    pipelines.append((nameOfvect+'RF'  , Pipeline([('Vectorizer', vectorizer),('RF'  , RandomForestClassifier())])))\n    pipelines.append((nameOfvect+'ET'  , Pipeline([('Vectorizer', vectorizer),('ET'  , ExtraTreesClassifier())])))\n    pipelines.append((nameOfvect+'SGD'  , Pipeline([('Vectorizer', vectorizer),('SGD'  , SGDClassifier())])))\n    pipelines.append((nameOfvect+'OVRC'  , Pipeline([('Vectorizer', vectorizer),('OVRC'  , OneVsRestClassifier(LogisticRegression()))])))\n    pipelines.append((nameOfvect+'Bagging'  , Pipeline([('Vectorizer', vectorizer),('Bagging'  , BaggingClassifier())])))\n    pipelines.append((nameOfvect+'NN'  , Pipeline([('Vectorizer', vectorizer),('NN'  , MLPClassifier())])))\n    #pipelines.append((nameOfvect+'xgboost', Pipeline([('Vectorizer', vectorizer), ('xgboost', XGBClassifier())])))\n    return pipelines\n\n# Traing model \ndef fit_model(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = KFold(n_splits=num_folds)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n\n# Split data to training and validation set\ndef read_in_and_split_data(data, features,target):\n    X = data[features]\n    y = data[target]\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n    return X_train, X_test, y_train, y_test\n\nX = 'cleaned_phrase'\ntarget_class = 'prompt'\nX_train, X_test, y_train, y_test = read_in_and_split_data(df_text, X, target_class)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Bag of Words Model**](http://)\nWe cannot work with text directly when using machine learning algorithms. Instead, we need\nto convert the text to numbers. We may want to perform classification of documents, so each\ndocument is an input and a class label is the output for our predictive algorithm. Algorithms\ntake vectors of numbers as input, therefore we need to convert documents to fixed-length vectors\nof numbers.","metadata":{}},{"cell_type":"markdown","source":"## [**Word Counts with countvectorizer** ](http://)\n- The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.","metadata":{}},{"cell_type":"code","source":"# sample text\nsample_text_count = X_train[:10]\n# create the transform\nvectorizer = CountVectorizer()\n# tokenize and build vocab\nvectorizer.fit(sample_text_count)\n# summarize\nprint(vectorizer.vocabulary_)\n# encode document\nvector = vectorizer.transform(sample_text_count)\n# summarize encoded vector\nprint(vector.shape)\nprint(type(vector))\nprint(vector.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Spot-Check Algorithms with Countvectorizer**](http://)","metadata":{}},{"cell_type":"code","source":"# Contvectorizer\nmodels = NormalizedTextModel('countvect')\nfit_model(X_train, y_train, models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [**Word Frequencies with TfidfVectorizer** ](http://)\n- Word counts are a good starting point, but are very basic. One issue with simple counts is that some words like the will appear many times and their large counts will not be very meaningful in the encoded vectors. An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF.\n\t* **Term Frequency**: This summarizes how often a given word appears within a document.\n\t* **Inverse Document Frequency**: This downscales words that appear a lot across documents.","metadata":{}},{"cell_type":"code","source":"# sample text\nsample_text_Tfid = X_train[:10]\n# create the transform\nvectorizer = TfidfVectorizer()\n# tokenize and build vocab\nvectorizer.fit(sample_text_Tfid)\n# summarize\nprint(vectorizer.vocabulary_)\nprint(vectorizer.idf_)\n# encode document\nvector = vectorizer.transform(sample_text_Tfid)\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Spot-Check Algorithms with TfidfVectorizer**](http://)","metadata":{}},{"cell_type":"code","source":"# TfidfVectorizer\nmodels = NormalizedTextModel('tfvect')\nfit_model(X_train, y_train, models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [**Hashing with HashingVectorizer**](http://)\n- Counts and frequencies can be very useful, but one limitation of these methods is that the\nvocabulary can become very large. This, in turn, will require large vectors for encoding\ndocuments and impose large requirements on memory and slow down algorithms. A clever work\naround is to use a one way hash of words to convert them to integers. The clever part is that\nno vocabulary is required and you can choose an arbitrary-long fixed length vector.","metadata":{}},{"cell_type":"code","source":"# sample text\nsample_text_hash = X_train[:10]\n# create the transform\nvectorizer = HashingVectorizer(n_features=20)\n# encode document\nvector = vectorizer.transform(sample_text_hash)\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Spot-Check Algorithms  with HashingVectorizer**](http://)","metadata":{}},{"cell_type":"code","source":"# TfidfVectorizer\n#models = NormalizedTextModel('hashvect')\n#fit_model(X_train, y_train, models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Fine tuning**](http://)","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX_train_1 = vectorizer.fit_transform(X_train)\nmodel = BaggingClassifier()\nn_estimators = [10, 100, 1000]\n#learning_rate= [0.1, 0.001, 0.0001]\n#max_depth = [4,5,6]\n#min_child_weight=[4,5,6]\n\n#define grid search\ngrid = dict(n_estimators=n_estimators)\ncv = KFold(n_splits=10)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\ngrid_result = grid_search.fit(X_train_1, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [**Predict unseen data**](http://)","metadata":{}},{"cell_type":"code","source":"def classification_metrics(model, y_test, y_pred):\n    print(f\"Training Accuracy Score: {model.score(X_train, y_train) * 100:.1f}%\")\n    print(f\"Validation Accuracy Score: {model.score(X_test, y_test) * 100:.1f}%\")\n    \n    conf_matrix = confusion_matrix(y_test, y_pred)\n    fig,ax = plt.subplots(figsize=(8,6))\n    sns.heatmap(pd.DataFrame(conf_matrix), annot = True, cmap = 'YlGnBu',fmt = 'g')\n    ax.xaxis.set_label_position('top')\n    plt.tight_layout()\n    plt.title('Confusion matrix for Logisitic Regression Model', fontsize=20, y=1.1)\n    plt.ylabel('Actual label', fontsize=15)\n    plt.xlabel('Predicted label', fontsize=15)\n    plt.show()\n    print(classification_report(y_test, y_pred))\n\ntext_clf = Pipeline([('vect', TfidfVectorizer()),('bagging', BaggingClassifier(n_estimators=10))])\nmodel = text_clf.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nclassification_metrics(model,y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n* https://www.foreseemed.com/natural-language-processing-in-healthcare\n* https://appen.com/datasets/audio-recording-and-transcription-for-medical-scenarios/\n* https://www.kaggle.com/paultimothymooney/medical-speech-transcription-and-intent","metadata":{}}]}