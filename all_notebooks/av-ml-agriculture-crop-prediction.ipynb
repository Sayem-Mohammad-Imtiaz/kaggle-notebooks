{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a\n\nhttps://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n\nhttps://towardsdatascience.com/backpropagation-and-batch-normalization-in-feedforward-neural-networks-explained-901fd6e5393e\n\nhttps://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n\nhttps://towardsdatascience.com/dealing-with-multiclass-data-78a1a27c5dcc\n\nhttps://machinelearningmastery.com/voting-ensembles-with-python/\n\nhttps://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n\nhttps://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n\nhttps://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n\nhttps://towardsdatascience.com/predictive-modeling-and-multiclass-classification-a4d2c428a2eb\n\nhttps://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\nhttps://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n\n\nhttps://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/av-jantahack-machine-learning-in-agriculture/train.csv')\ndf_test=pd.read_csv('/kaggle/input/av-jantahack-machine-learning-in-agriculture/test.csv')\ndf_sample=pd.read_csv('/kaggle/input/av-jantahack-machine-learning-in-agriculture/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the Values of various independent columns ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Crop_Type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Variable \ndf_train['Crop_Damage'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train['Soil_Type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Pesticide_Use_Category'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Number_Doses_Week'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Season'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color=sns.color_palette()[1]\nsns.countplot(data=df_train,x='Crop_Damage',color=color)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow!!! Highly baised set ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_train['Estimated_Insects_Count'],bins=10,kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df_train['Crop_Damage'],y=df_train['Estimated_Insects_Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Estimated_Insects_Count'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Number_Weeks_Used.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Number_Weeks_Quit'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12));\nsns.heatmap(df_train.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above chart we see that crop damage has a decent corelation with the numerical variables \n\nEstimated_Insects_Count , Number_Weeks_quit and Number weeks used ,Pesticide_Use_Catergory \n\nSo lets magnify this effects by using SKleanr polynomal feature extractor ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Let do some Feature Engineering \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['is_test']=1\ndf_train['is_test']=0\n\ndata=pd.concat([df_train,df_test]).reset_index(drop=True)\ndata.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets one hot encode the categorical variable \n\ndata=pd.get_dummies(data,columns=['Crop_Type','Soil_Type','Season','Pesticide_Use_Category'])\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a Linear Regression model to Predict Null Values in Number_Weeks_Used column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let build a Simple Lasso Liner regression model to predict the missing values \n\n# Split Data into Train and Test sets \nnull_train=data[data['Number_Weeks_Used'].notnull()]\nnull_test=data[data['Number_Weeks_Used'].isnull()]\n\n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(null_train.drop(columns=['ID','is_test','Crop_Damage','Number_Weeks_Used'],axis=1),\n                                                                             null_train['Number_Weeks_Used'].values,random_state=7)\n\n#Normalize the features \n\nfor col in ['Estimated_Insects_Count','Number_Weeks_Quit', 'Number_Doses_Week']:\n    scaler=preprocessing.StandardScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    null_test.loc[:,col]=scaler.transform(null_test[col].values.reshape(-1,1))\n    \n# Normalize Y variable \n\nscaler=preprocessing.StandardScaler()\nscaler.fit(y_train.reshape(-1,1))\ny_train=scaler.transform(y_train.reshape(-1,1))\ny_val=scaler.transform(y_val.reshape(-1,1))\n\n#Define model \n\nlr=linear_model.LassoCV()\nlr.fit(X_train,y_train)\n\nprint('The R2 score for Lasso model is {}'.format(lr.score(X_val,y_val)))\n\nnull_predict=lr.predict(null_test.drop(columns=['ID','is_test','Crop_Damage','Number_Weeks_Used'],axis=1))\n\nnull_test.loc[:,'Number_Weeks_Used']=scaler.inverse_transform(null_predict.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_train=null_train[['ID','Number_Weeks_Used']]\nnull_test=null_test[['ID','Number_Weeks_Used']]\n\ndata_lasso=pd.concat([null_train,null_test]).reset_index(drop=True)\ndata_lasso.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.merge(data,data_lasso,how='left',on='ID')\ndata.drop(axis=1,columns='Number_Weeks_Used_x',inplace=True)\n\ndata.loc[data['Number_Weeks_Used_y']<0,'Number_Weeks_Used_y']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets enchance the power of numerical columns using Skleanr polynomial \n\n#polynomial=preprocessing.PolynomialFeatures(degree=2,include_bias=False)\n\n#polynomial.fit(data[['Estimated_Insects_Count','Number_Weeks_Quit','Number_Doses_Week','Number_Weeks_Used_y']])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#poly=polynomial.transform(data[['Estimated_Insects_Count','Number_Weeks_Quit','Number_Doses_Week','Number_Weeks_Used_y']])\n#data=pd.concat([data,pd.DataFrame(poly)],axis=1)\n\n#data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating some additional Features \n\n#data['crop_soil_pest']=data['Crop_Type']+data['Pesticide_Use_Category']+data['Soil_Type']\n#data['crop_soil_pest_season']=data['Crop_Type']+data['Pesticide_Use_Category']+data['Soil_Type']+data['Season']\n\n#data['crop_soil']=data['Crop_Type']+data['Soil_Type']\n#data['soil_pest']=data['Soil_Type']+data['Pesticide_Use_Category']\n#data['crop_pest']=data['Crop_Type']+data['Pesticide_Use_Category']\n#data['Pest_season']=data['Pesticide_Use_Category']+data['Season']\n\n\n#data['Total_pest_used']=data['Number_Doses_Week']*data['Number_Weeks_Used']\n#data['Total_pest_quit']=data['Number_Doses_Week']*data['Number_Weeks_Quit']\n\n#data['Estimated_Insects_weeks_Used']=data['Estimated_Insects_Count']*data['Number_Weeks_Used']\n#data['Estimated_Insects_Used_1']=data['Estimated_Insects_Count']*data['Total_pest_used']\n#data['Estimated_Insects_Used_2']=data['Estimated_Insects_Count']*data['Total_pest_quit']\n\n\n#data['mean1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].mean(axis=1)\n#data['sum1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].sum(axis=1)\n#data['std1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].std(axis=1)\n#data['kurt1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].kurtosis(axis=1)\n#data['median1']=data[['crop_soil_pest','crop_soil_pest_season','crop_soil','soil_pest','crop_pest','Pest_season']].median(axis=1)\n\n#data['mean2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].mean(axis=1)\n#data['sum2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].sum(axis=1)\n#data['std2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].std(axis=1)\n#data['kurt2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].kurtosis(axis=1)\n#data['median2']=data[['Total_pest_used','Total_pest_quit','Estimated_Insects_weeks_Used','Estimated_Insects_Used_1','Estimated_Insects_Used_2']].median(axis=1)\n\n#data['Estimated_Insects_cut']=pd.cut(data['Estimated_Insects_Count'],bins=4,labels=[0,1,2,3])\n#data['Estimated_Insects_cut']=data['Estimated_Insects_cut'].astype(int)\n\n\n\n\ndata['Estimated_Insects_Count_square']=data['Estimated_Insects_Count']*data['Estimated_Insects_Count']\ndata['Number_Weeks_Used_y_square']=data['Number_Weeks_Used_y']*data['Number_Weeks_Used_y']\n#data['Number_Doses_Week_square']=data['Number_Doses_Week']*data['Number_Doses_Week']\ndata['Number_Weeks_Quit_square']=data['Number_Weeks_Quit']*data['Number_Weeks_Quit']\n\ndata['Estimated_Insects_doses']=data['Estimated_Insects_Count']*data['Number_Doses_Week']\ndata['Estimated_Insects_used']=data['Estimated_Insects_Count']*data['Number_Weeks_Used_y']\ndata['Estimated_Insects_quit']=data['Estimated_Insects_Count']*data['Number_Weeks_Quit']\n\ndata['Number_Weeks_Quit_Used']=data['Number_Weeks_Used_y']*data['Number_Weeks_Quit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sepeate the data \n\ntrain=data[data['is_test']!=1]\ntrain.drop('is_test',axis=1,inplace=True)\n\ntest=data[data['is_test']==1]\ntest.drop(columns=['Crop_Damage','is_test'],axis=1,inplace=True)\n\ntest.shape,train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\n\nX=train.drop(columns=['Crop_Damage','ID'],axis=1)\ny=train['Crop_Damage']\n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,stratify=y,random_state=101,test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Various Models for Ensembling ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dummy Classifier \n\nTo see how much we can improve over a baseline ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\nclf = DummyClassifier(strategy='stratified',random_state=101)\n\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_val)\nprint('Accuracy of a random classifier is: %.2f%%'%(metrics.accuracy_score(y_val,y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB Classifier Baseline ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nclf = XGBClassifier(objective='multi:softmax',n_jobs=-1, max_depth=6,n_estimators=300,num_class=3)\n\nXGB_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    clf.fit(X.loc[train_idx,:],y[train_idx])\n    predict=clf.predict(X.loc[val_idx,:])\n    XGB_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of XGBoost Baseline is {}'.format(np.mean(XGB_prediction)*100))\n\npredict=clf.predict_proba(test.drop(columns='ID',axis=1))\n\nXGB_baseline=pd.DataFrame(predict,columns=['XBG_baseline_0','XGB_baseline_1','XGB_baseline_2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nimport lightgbm as lgb\n\nclf = lgb.LGBMClassifier(boosting_type='gbdt', class_weight='balanced', colsample_bytree=0.5,\n                importance_type='split', learning_rate=0.1, max_depth=4,\n                min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=5,\n                min_split_gain=0.0, n_estimators=650, n_jobs=-1, num_class=3,\n                num_leaves=63, objective='multiclass', random_state=42,\n                reg_alpha=2, reg_lambda=0, silent=True, subsample=0.7,\n                subsample_for_bin=200000, subsample_freq=0, verbose=1)\n\nlgbm_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    clf.fit(X.loc[train_idx,:],y[train_idx])\n    predict=clf.predict(X.loc[val_idx,:])\n    lgbm_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of LGBM Baseline is {}'.format(np.mean(lgbm_prediction)*100))\n\n#predict=clf.predict_proba(test.drop(columns='ID',axis=1))\n\n#XGB_baseline=pd.DataFrame(predict,columns=['XBG_baseline_0','XGB_baseline_1','XGB_baseline_2'])","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Baseline ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf_baseline=ensemble.RandomForestClassifier(n_estimators=300,random_state=101,class_weight='balanced')\n\nrf_prediction_baseline=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    rf_baseline.fit(X.loc[train_idx,:],y[train_idx])\n    predict=rf_baseline.predict(X.loc[val_idx,:])\n    rf_prediction_baseline.append(metrics.accuracy_score(y[val_idx],predict))\n\nprint('Accuracy of Random Forest Baseline is {}'.format(np.mean(rf_prediction_baseline)))\n\n\npredict=rf_baseline.predict_proba(test.drop(columns='ID',axis=1))\n\nRF_baseline=pd.DataFrame(predict,columns=['RF_baseline_0','RF_baseline_1','RF_baseline_2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest with Optimized with GridSearchCV \n\n#### uncomment the below cell to run Grid Search CV ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier= ensemble.RandomForestClassifier(class_weight='balanced',random_state=101)\n\n#param_grid={\n#    'n_estimators':[50,100,150,200,250],\n#    'criterion' : ['gini','entropy'],\n#    'max_depth':[5,10,15,20,25],\n#    'min_samples_split':[2,3,5],\n    #'max_features':['auto', 'sqrt', 'log2']\n     \n#}\n\n#model=model_selection.GridSearchCV(\n#                        estimator=classifier,\n#                        param_grid=param_grid,\n#                        scoring='accuracy',\n#                        cv=5,\n#                        refit=True,\n #                       verbose=5,\n #                       n_jobs=-1\n #                       )\n\n#model.fit(X,y)\n\n\n#print('Best Scorer{}'.format(model.best_score_))\n\n#print('/n')\n\n#print('Best Parameters{}'.format(model.best_params_))\n\n#predict=model.predict_proba(test.drop(columns='ID',axis=1))\n\n#RF_Tuned=pd.DataFrame(predict,columns=['RF_Tuned_0','RF_Tuned_1','RF_Tuned_2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Result of Grid Search CV \n\n> Best Parameters{'criterion': 'entropy', 'max_depth': 25, 'min_samples_split': 2, 'n_estimators': 250}","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implementing the Grid Search Version \n\nrf_tuned=ensemble.RandomForestClassifier(n_estimators=250,random_state=101,criterion='entropy',max_depth=25,min_samples_split=2,\n                                         class_weight='balanced')\n\nrf_prediction_tuned=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    rf_tuned.fit(X.loc[train_idx,:],y[train_idx])\n    predict=rf_tuned.predict(X.loc[val_idx,:])\n    rf_prediction_tuned.append(metrics.accuracy_score(y[val_idx],predict))\n\nprint('Accuracy of Random Forest Tuned by Grid Search is {}'.format(np.mean(rf_prediction_tuned)))\n\n\npredict=rf_tuned.predict_proba(test.drop(columns='ID',axis=1))\n\nRF_tuned=pd.DataFrame(predict,columns=['RF_tuned_0','RF_tuned_1','RF_tuned_2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB Boost optimized with RandomSearch CV ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import xgboost as xgb\n#from sklearn.model_selection import RandomizedSearchCV\n\n#params = {\n#        'learning_rate': np.arange(0,1,0.1),\n#       'n_estimators':np.arange(50,250,50),\n#       'colsample_bytree': np.arange(0.4,1,0.2),\n#        'max_depth': np.arange(5,15,5),\n#        'reg_lambda':np.arange(0.5,1,0.5)\n#        }\n\n#xgb = xgb.XGBClassifier(objective='multi:softmax',\\\n #                    nthread=1,num_class=3)\n\n#folds = 5\n#param_comb = 100\n\n\n#kfold = model_selection.KFold(n_splits=folds, shuffle = True, random_state = 101)\n\n#random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=-1, cv=kfold.split(X,y), verbose=5, random_state=101,refit=True )\n\n#random_search.fit(X, y)\n\n\n#print(\"The best score is {}\".format(random_search.best_score_ ))\n\n#print('/n')\n\n#print ('The best paramerts are {}'.format(random_search.best_params_))\n\n\n#predict=random_search.predict_proba(test.drop(columns='ID',axis=1))\n\n#XGB_Tuned=pd.DataFrame(predict,columns=['XGB_Tuned_0','XGB_Tuned_1','XGB_Tuned_2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The best paramerts from RandomSearch CV\n\n> {'subsample': 0.8000000000000002, 'reg_lambda': 0.5, 'n_estimators': 100, 'min_child_weight': 11, 'max_depth': 10, 'learning_rate': 1300, 'gamma': 2.0, 'colsample_bytree': 0.6000000000000001}","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# XGB Boost Tuned after Random Grid CV ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nclf_tuned = XGBClassifier(objective='multi:softmax',n_jobs=-1,n_estimators=100,num_class=3,colsample_bytree= 0.6\n                         ,gamma=2,max_depth=10,min_child_weight=11,reg_lambda=0.5,subsample=0.8)\n\nXGB_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    clf_tuned.fit(X.loc[train_idx,:],y[train_idx])\n    predict=clf_tuned.predict(X.loc[val_idx,:])\n    XGB_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of XGBoost Tuned by Random Search is {}'.format(np.mean(XGB_prediction)))\n\npredict=clf_tuned.predict_proba(test.drop(columns='ID',axis=1))\n\nXGB_Tuned=pd.DataFrame(predict,columns=['XGB_Tuned_0','XGB_Tuned_1','XGB_Tuned_2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extra Tree Classifier Baseline ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntree_classifier=ensemble.ExtraTreesClassifier(n_estimators=250,max_depth=5)\n\ntree_classifier_prediction=[]\n\nkfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n\nfor train_idx,val_idx in kfold.split(X=X,y=y):\n    tree_classifier.fit(X.loc[train_idx,:],y[train_idx])\n    predict=tree_classifier.predict(X.loc[val_idx,:])\n    tree_classifier_prediction.append(metrics.accuracy_score(y[val_idx],predict))\n    \n\nprint('Accuracy of Extra Tree CLassifier is {}'.format(np.mean(tree_classifier_prediction)))\n\npredict=tree_classifier.predict_proba(test.drop(columns='ID',axis=1))\n\nET_baseline=pd.DataFrame(predict,columns=['ET_baseline_0','ET_baseline_1','ET_baseline_2'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Preparation for NN \n\n\n\n# Making catergorical variables as one-hot encoding \n\n#data=pd.get_dummies(data,columns=['Pesticide_Use_Category','Season','crop_soil_pest','crop_soil_pest_season','crop_soil'\\\n                                  # ,'soil_pest','crop_pest','Pest_season'],drop_first=True)\n\nTrain=data[data['is_test']!=1]\nTrain.drop(columns=['is_test','ID'],axis=1,inplace=True)\n\nX=Train.drop('Crop_Damage',axis=1)\ny=Train['Crop_Damage']\n\nTest=data[data['is_test']==1]\nTest.drop(columns=['is_test','Crop_Damage'],axis=1,inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Oversampling using SMOTE \n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,random_state=101,test_size=0.20)\n\nnormalize_col=['Estimated_Insects_Count','Number_Doses_Week','Number_Weeks_Used_y','Number_Weeks_Quit',\n              'Estimated_Insects_Count_square','Number_Weeks_Used_y_square','Number_Weeks_Quit_square','Estimated_Insects_doses',\n       'Estimated_Insects_used', 'Estimated_Insects_quit','Number_Weeks_Quit_Used']\n\nfor col in normalize_col:\n    scaler=preprocessing.MinMaxScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    Test.loc[:,col]=scaler.transform(Test[col].values.reshape(-1,1))\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote=SMOTE('minority')\nX_sm,y_sm=smote.fit_sample(X_train,y_train)   \n\n\ny_train=pd.get_dummies(y_train)\ny_val=pd.get_dummies(y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom keras.regularizers import l2,l1\nfrom keras import layers\nfrom keras.layers import BatchNormalization\nfrom keras import optimizers\n\n# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(units = 200, kernel_initializer = 'he_normal', activation = 'relu', input_dim = X_train.shape[1]))\n\nmodel.add(layers.Dropout(0.05))\n\nmodel.add(BatchNormalization())\n\n# Adding second hidden layer\nmodel.add(Dense(units = 200,kernel_initializer = 'he_normal', activation = 'relu'))\n\nmodel.add(layers.Dropout(0.05))\n\nmodel.add(BatchNormalization())\n\n#Adding third hidden layer \nmodel.add(Dense(units = 100,kernel_initializer = 'he_normal', activation = 'relu'))\n\nmodel.add(layers.Dropout(0.1))\n\nmodel.add(BatchNormalization())\n\n\n# Adding the output layer\nmodel.add(Dense(units = 3, kernel_initializer = 'he_normal', activation = 'softmax'))\n\n# Compiling the ANN\n\nadam=optimizers.Adam(lr=0.0001)\n\nmodel.compile(optimizer =adam, loss = 'categorical_crossentropy',metrics=['accuracy'])\n\nmodel.fit(X_train,y_train,batch_size=128,epochs=110,validation_data=(X_val,y_val),verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=model.predict_proba(Test.drop('ID',axis=1))\n\nNN=pd.DataFrame(predict,columns=['NN_0','NN_1','NN_2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test.drop(columns=['Crop_Damage_0','Crop_Damage_1','Crop_Damage_2'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensemble of Various Model Public Leaderboard : 0.844\n\n#Test['Crop_Damage_0']=(0.5*NN['NN_0']+0.0*RF_tuned2['RF_tuned2_0']+0.0*XGB_baseline['XBG_baseline_0']+0.0*RF_baseline['RF_baseline_0']+0*RF_tuned['RF_tuned_0']+0.45*XGB_Tuned['XGB_Tuned_0']+0.25*ET_baseline['ET_baseline_0'])\n#Test['Crop_Damage_1']=(0.5*NN['NN_1']+0.0*RF_tuned2['RF_tuned2_1']+0.0*XGB_baseline['XGB_baseline_1']+0.0*RF_baseline['RF_baseline_1']+0*RF_tuned['RF_tuned_1']+0.45*XGB_Tuned['XGB_Tuned_1']+0.25*ET_baseline['ET_baseline_1'])\n#Test['Crop_Damage_2']=(0.5*NN['NN_2']+0.0*RF_tuned2['RF_tuned2_2']+0.0*XGB_baseline['XGB_baseline_2']+0.0*RF_baseline['RF_baseline_2']+0*RF_tuned['RF_tuned_2']+0.45*XGB_Tuned['XGB_Tuned_2']+0.25*ET_baseline['ET_baseline_2'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test['Crop_Damage_0']=(0.4*NN['NN_0']+0.4*XGB_Tuned['XGB_Tuned_0']+0.2*ET_baseline['ET_baseline_0']).values.reshape(-1,1)\nTest['Crop_Damage_1']=(0.4*NN['NN_1']+0.4*XGB_Tuned['XGB_Tuned_1']+0.2*ET_baseline['ET_baseline_1']).values.reshape(-1,1)\nTest['Crop_Damage_2']=(0.4*NN['NN_2']+0.4*XGB_Tuned['XGB_Tuned_2']+0.2*ET_baseline['ET_baseline_2']).values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test['Crop_Damage']=Test[['Crop_Damage_0','Crop_Damage_1','Crop_Damage_2']].idxmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test['Crop_Damage'].replace({'Crop_Damage_0':'0','Crop_Damage_1':'1','Crop_Damage_2':'2'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTest[['ID','Crop_Damage']].to_csv('/kaggle/working/Ensemble_weighted_NN+XGB+ET.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}