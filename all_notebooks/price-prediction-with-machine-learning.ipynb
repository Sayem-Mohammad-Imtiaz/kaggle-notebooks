{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf_train = pd.read_csv(\"../input/mobile-price-classification/train.csv\")\ndf_test = pd.read_csv(\"../input/mobile-price-classification/test.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df_train.shape #2000 records with 20 features.\n#(This really reminds me of the markstrat game in business school)\ndf_test.shape #1000 records 20 features with ID. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets take a look of battery power in different price ranges\nimport plotly.express as px\nfig = px.histogram(df_train[\"battery_power\"], color = df_train[\"price_range\"], width=600, height=400)\nfig.update_layout(yaxis_range=[0,80])\nfig.show()\n#Cannot tell much but class 3 does have larger battery power. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets take a look of bluetooth feature in different price ranges\ndf_blue = df_train.groupby([\"price_range\",\"blue\"])[\"blue\"].count()\npd.DataFrame(df_blue)\n#I dont think I can see much correlation between bluetooth and price ranges. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Similarly, lets take a look on all the features. \ndf_cs = df_train.groupby([\"price_range\"],as_index=False).mean()\n#Most values are really close. px_height and px_width are important features. \n#Normalization is definitely necessary for related models. \ndf_cs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation plot\n#Beautiful Correlation Plot\nfrom string import ascii_letters\n\nsns.set(style=\"white\")\n\n# Generate a large random dataset\nrs = np.random.RandomState(33)\n\n# Compute the correlation matrix\ncorr = df_train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### No strong correlation was found.\n### Weak correlations found between target and the following vars: battery power, px height and wildth,  ram. \n","metadata":{}},{"cell_type":"markdown","source":"# Machine Learning Modeling","metadata":{}},{"cell_type":"code","source":"num_columns = []\nfor i in df_train.columns:\n    if df_train[i].nunique()>=3 and i!=\"price_range\":\n        num_columns.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\n\n# 20 features with 2000 records, multi-classification problem with low Collinearity, \n# the suitable models came in my mind are RF, Logistic Regression, XGBoost, SVM and GBM.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import plot_tree, plot_importance\n\n#CrossValidation and Metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\ndf_train[num_columns] = stats.zscore(df_train[num_columns])\nX = df_train.drop(\"price_range\", axis=1)\ny = df_train[\"price_range\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3, random_state=42)\n\n\ncro_val_acc, train_scores, test_scores = [], [], []\n\n\nmodel_names = [\"RandomForestClassifier\", \"LogisticRegression\", \"XGBoost\",\"SVM\",\"GradientBoostingClassifer\"]\nmodels = {\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"LogisticRegression\": LogisticRegression(),\n    \"XGBClassifier\": XGBClassifier(verbosity = 0),\n    \"SVM\": SVC(),\n    \"GradientBoostingClassifer\": GradientBoostingClassifier()\n}\n\nfor i in models:\n    print(i)\n    print(\"\\n\")\n    \n    model = models[i]\n    cv = KFold(n_splits=5, shuffle=True)\n    cv_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv)\n    cv_mean_score = np.mean(cv_scores)\n    print(i,\"  \",\"cross validation accuracy\", cv_mean_score)\n    cro_val_acc.append(cv_mean_score)\n    \n    model.fit(X_train, y_train)\n    train_score = model.score(X_train, y_train)\n    train_scores.append(train_score)\n    print(f\"Train Score:{train_score*100}\")\n    print(\"\\n\")\n    \n    test_score = model.score(X_test, y_test)\n    print(f\"Test Score:{test_score*100}\")\n    print(\"\\n\")\n    test_scores.append(test_score)\n    \n    y_pred = model.predict(X_test)\n    conf_matrix = confusion_matrix(y_pred, y_test)\n    print(conf_matrix)\n    print(\"\\n\")\n    \n    cla_report = classification_report(y_test, y_pred, output_dict=True)\n    print(pd.DataFrame(cla_report).transpose())\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Performance Analysis","metadata":{}},{"cell_type":"markdown","source":"#### Tree based models all have overfitting problems","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(16,8))\nsns.set_style('darkgrid')\nplt.title('Model Performance', fontweight='bold', size=20)\n\nbarWidth = 0.20\n \nb1 = train_scores\nb2 = test_scores\nb3 = cro_val_acc\n \nr1 = np.arange(len(b1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, b1, color='blue', width=barWidth, edgecolor='white', label='train',capsize=18)\nplt.bar(r2, b2, color='red', width=barWidth, edgecolor='white', label='test',capsize=18)\nplt.bar(r3, b3, color='grey', width=barWidth, edgecolor='white', label='cv_accuracy',capsize=18)\n\nplt.ylim([0.8,1])\n \n\nplt.xlabel('Models', fontweight='bold', size = 20)\nplt.ylabel('Scores', fontweight='bold', size = 20)\nplt.xticks([r + barWidth for r in range(len(b1))], model_names)\n \nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(model_names)):\n    print(f'Accuracy {model_names[i]}'.ljust(80, ' '))\n    print(round(test_scores[i],3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression performs the best. \nThe rest of the models have overfitting problems. ","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning for XGBoost","metadata":{}},{"cell_type":"code","source":"# Lets give XGBoost another try with Hyperparater Tuning\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'learning_rate': [0.02, 0.1],\n    'max_depth': [5, 8, 11],\n    'colsample_bytree': [0.7],\n    'n_estimators' : [500, 800,1000],\n    'objective': ['multi:softmax']\n}\n\nxgb_model = XGBClassifier(verbosity = 0)\n\nclf = GridSearchCV(estimator = xgb_model,\n                       param_grid = params,\n                       cv=5, \n                       scoring = \"accuracy\")\nclf.fit(X_train, y_train)\nclf.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = XGBClassifier(colsample_bytree=0.7,\n learning_rate=0.1,\n max_depth=5,\n n_estimators=1000,\n verbosity = 0)\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n#Not much improvement. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import plot_importance\nplot_importance(xgb_model, max_num_features=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AutoML comparison","metadata":{}},{"cell_type":"code","source":"h2o.cluster().shutdown()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\nh2o_df = h2o.H2OFrame(df_train)\nh2o_df[\"price_range\"] = h2o_df[\"price_range\"].asfactor()\ntrain, test = h2o_df.split_frame(ratios=[.7])\n# Identify predictors and response\nx = train.columns\ny = \"price_range\"\nx.remove(y)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()\n\naml = H2OAutoML(max_runtime_secs=600,\n                exclude_algos=['DeepLearning'],\n                seed=1,\n                stopping_metric='mean_per_class_error',\n                sort_metric='mean_per_class_error',\n                project_name='Price_Range_Prediction'\n)\n\n%time aml.train(x=x, y=y, training_frame=train)\n# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(5) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIn case H2O bug comes up, I copied the result here. \n\nStackedEnsemble_BestOfFamily_AutoML_20210506_034057\t0.0476932\nGLM_1_AutoML_20210506_034057\t0.0484978\t0.185706\t0.220634\t0.0486792\tnan\tnan\n\n\nStackedEnsemble_AllModels\t0.0906175\t0.383616\t0.340883\t0.116201\tnan\tnan\n\nGBM_grid__1_1\t0.0954304\t0.242366\t0.267777\t0.0717045\tnan\tnan\n\nXGBoost_grid__1_0.0962082\t0.238954\t0.267441\t0.0715246\tnan\tnan\n\n\t0\t1\t2\t3\tError\tRate\n    \n0  \t147.0\t3.0\t0.0\t0.0\t0.020000\t3 / 150\n\n1  \t7.0\t148.0\t1.0\t0.0\t0.051282\t8 / 156\n\n2  \t0.0\t4.0\t133.0\t6.0\t0.069930\t10 / 143\n\n3  \t0.0\t0.0\t0.0\t148.0\t0.000000\t0 / 148\n\n4  \t154.0\t155.0\t134.0\t154.0\t0.035176\t21 / 597","metadata":{}},{"cell_type":"code","source":"model_id = aml.leader.model_id\nmodel = h2o.get_model(model_id)\nperf = model.model_performance(test)\nperf.confusion_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}