{"cells":[{"metadata":{"_cell_guid":"8f80f922-983f-4c43-88dc-47cc235213c7","_uuid":"31b308c23b0f7aae9687dbb7b9e72accb299f7a0"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h1>Stock Market Price Prediction with New Data</h1>\n\n<p><b>Breif Overview:</b> \n    <br/><br/>\nThe model created below is for prediction the stock prices of a Company.\n<br/>\nThere are two datasets\n<br/><br/>\n1. Stock Prices Dataset for Dow Jones Inc\n<br/><br/>\n2. Top 25 headlines for everyday for the past 8 years\n<br/><br/>\nThe notebook is briefly summarized as follows:\n<br/><br/>\n1. Data Preparation - Preparing data for evaluation.\n<br/><br/>\n2. Data Quality Checks - Performing basic checks on data for better understanding of data.\n<br/><br/>\n3. Feature inspection and filtering - Correlation and feature Mutual information plots against the target variable. Inspection of the Binary, categorical and other variables.\n<br/><br/>\n4. Feature importance ranking via learning models\n<br/><br/>\n5. Training - training data against multiple machine learning algorthms and fine tuning a couple of algorithms for accuracy\n    <br/>\n</p> </div>"},{"metadata":{"_cell_guid":"753c7c7e-9532-485c-b594-7d3dc968d355","_uuid":"beb30caa6b428e2d23ec218757916b042a4fe2b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nfrom matplotlib import pyplot\n#from pandas import read_csv, set_option\nfrom pandas import Series, datetime\nfrom pandas.tools.plotting import scatter_matrix, autocorrelation_plot\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, TimeSeriesSplit\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport random\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom xgboost import XGBClassifier\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb0ed3be-170b-417e-9c40-a739d7602fcf","_uuid":"dc2f721a37fbbe051eac5149e12cfe2fd8a4204a"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h3>1. Data Preparation:</h3>\n<br/>\nImported all the necessary modules for the project\n<br/><br/>\nLoaded the dataset as a dataframe and parsed the date column to be read by the dataframe as dates type\nChecked the top 5 rows of the dataframe to see how the columns are aligned.\n<br/><br/>\nThe 'combined_stock_data.csv' initially only had the headlines(Top1 through Top25). Each row was iterated over an algorithm which generated the Subjectivity, Objectivity, Positive, Negative, Neutral sentiments of the respective headlines of each row.\n<br/><br/>\nThe algorithm was accepting only a single sentence and was providing the respective sentiments in percentage. I modified the algorithm iterate over all of the individuals rows and simultaneously create the Subjectivity, Objectivity, Negative, Positive, Neutral values and assign itself to the columns in the dataframe.\n<br/><br/>\nThe headlines Top1 through Top25 were concatenated and then passed on to the algorithm\n<br/><br/>\nThe original algorithm : https://github.com/nik0spapp/usent\n<br/><br/>\nModified algorithm : https://github.com/ShreyamsJain/Stock-Price-Prediction-Model/blob/master/Sentence_Polarity/sentiment.py\n<br/>\n</p>\n</div>"},{"metadata":{"_cell_guid":"ad52177a-8beb-42aa-a2ff-ed2fc1145c4e","_uuid":"f02a83dfb1b6eaf9aae47dffd881508e082a70b2","trusted":true},"cell_type":"code","source":"# Loading the dataset to a dataframe\nsentence_file = \"../input/headlinespolarity/combined_stock_data.csv\"\nsentence_df = pd.read_csv(sentence_file, parse_dates=[1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29bebcce-0739-4640-99be-aae365fd2a6b","_uuid":"76f4245494deea58e81bf79997de2a2eda3696fd","trusted":true},"cell_type":"code","source":"sentence_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e7ac6f4-b56c-4c07-973f-a659c300e802","_uuid":"7f58e84d56f7cccbf31ddcdf2b3913e6fead05f8"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nChecked the datatypes of all of the columns. Below is the list of data types\n<p>\n</div>"},{"metadata":{"_cell_guid":"51cfb668-b3e4-40d1-9211-fe27650f7e75","_uuid":"cd95cc6f3a27542a597619ac830b88563106bafd","trusted":true},"cell_type":"code","source":"# Check the shape and data types of the dataframe\nprint(sentence_df.shape)\nprint(sentence_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bf5c86e-96fe-41d8-aed4-b74bfa35138b","_uuid":"f4863ece4805efeb0ad4cda94616aa7ffca26ed2"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nLoad the Dow Jones dataset to a dataframe stock_data which contains 8 years of Stock Price data.\n<br/><br/>\nParse the date as a date type and check the top 5 rows of the dataframe.\n<br/><br/>\nChecked the top 5 rows of the dataframe\n</p>\n</div>"},{"metadata":{"_cell_guid":"c3220239-587e-4177-a7d0-806b1fceacde","_uuid":"a45877b8c79a4b82b2be3b8762392f7da828ab10","trusted":true},"cell_type":"code","source":"# Load the stock prices dataset into a dataframe and check the top 5 rows\nstock_prices = \"../input/stocknews/DJIA_table.csv\"\nstock_data = pd.read_csv(stock_prices, parse_dates=[0])\nstock_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76747a2f-d07f-4507-b360-28125451e672","_uuid":"27c5192b2682799a1cb779652d22610534f11d15"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nChecked the shape and datatypes of the loaded dataset\n</p>\n</div>"},{"metadata":{"_cell_guid":"812d3a7e-ca1b-4674-a3f6-b7fab9890c9e","_uuid":"36ab4e5504dc3165eafb9e67ac661f0fbdc2822f","trusted":true},"cell_type":"code","source":"# Check the shape and datatypes of the stock prices dataframe\nprint(stock_data.shape)\nprint(stock_data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a9c26bd-38e4-4119-8175-8e5ac4c6387d","_uuid":"6e7ebe093e51d93130b6c95425a1cb80ab017ace"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nMerged the 5 columns(Subjectivity, Objectivity, Positive, Negative, Neutral) with the stock_data dataframe.\n<br/><br/>\nValidated the merged dataframe to see the 2 dataframes are concatenated by checking the top 5 rows of the merged_dataframe.\n</p>\n</div> "},{"metadata":{"_cell_guid":"d3a9b4c5-b36c-4d52-b2cf-9b94df974b6d","_uuid":"36c52bdf10b30cfc1a6f33bfbb9b28cf1fa98a37","trusted":true},"cell_type":"code","source":"# Create a dataframe by merging the headlines and the stock prices dataframe\nmerged_dataframe = sentence_df[['Date', 'Label', 'Subjectivity', 'Objectivity', 'Positive', 'Negative', 'Neutral']].merge(stock_data, how='inner', on='Date', left_index=True)\n# Check the shape and top 5 rows of the merged dataframe\nprint(merged_dataframe.shape)\nmerged_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6fa16eb-3456-439f-90ab-6c6879e8a5c3","_uuid":"4b7a0445514836133280d33d9af4ec7fbea33a9d"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nWe have the Label(i.e the output column) column in the 2nd position.\n<br/><br/>\nLets move it to the end of the dataframe to have a clear view of inputs and outputs\n</p>\n</div>"},{"metadata":{"_cell_guid":"0553ad04-a5e7-4498-b6d8-9da9e13b602d","_uuid":"d2fd797ea4f48d701df5ceaa93428c6397cee25f","trusted":true},"cell_type":"code","source":"# Push the Label column to the end of the dataframe\ncols = list(merged_dataframe)\nprint(cols)\ncols.append(cols.pop(cols.index('Label')))\nmerged_dataframe = merged_dataframe.ix[:, cols]\nmerged_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f640516-ad70-432c-8bc5-1d1b8adf0a2e","_uuid":"861a4b4dac2ae2b62a931e53853d0db87f1c0078"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nWe have the volumn column in Integer format. Lets change it to float, same as the rest of the columns so we do not have any difficulties in making calculations at a later point.\n</p>\n</div>"},{"metadata":{"_cell_guid":"d8755480-5501-442a-976a-07c7e01af645","_uuid":"64d08ffb756bdfa027c82138c3acf5c781288f76","trusted":true},"cell_type":"code","source":"# Change the datatype of the volume column to float\n#merged_dataframe['Date'] = pd.to_datetime(merged_dataframe['Date'])\nmerged_dataframe['Volume'] = merged_dataframe['Volume'].astype(float)\nprint(cols)\n#merged_dataframe = merged_dataframe.set_index(['Date'])\nmerged_dataframe.index = merged_dataframe.index.sort_values()\nmerged_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"303521cc-acfd-4bf3-9cd4-7fb3939e6425","_uuid":"f44f205b7286771d1bd2be2a59e9889d435e9c10"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\n<h3>2. Data Quality Checks:</h3>\n<br/>\nChecked the statistics of individual columns in the dataframe.\n<br/><br/>\nAs you can see below there are no outliers in any of the columns, however, some of the columns have NaN values\n</p>\n</div>"},{"metadata":{"_cell_guid":"b63aeb60-0594-49a0-a64e-30e74c3f60da","_uuid":"c99a007b729cf96c0b5bd523207307ebbeeccaf9","trusted":true},"cell_type":"code","source":"# Check the statistics of the columns of the merged dataframe and check for outliers\nprint(merged_dataframe.describe())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24f78545-5e25-475c-a667-d990e3fbf9a4","_uuid":"1e76cfd8e6f07a9abe1352f1fc9453bcc296f2f7"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nPlotted histograms for individual columns to see the distribution of values.\n<br/><br/>\nThe x axis is the column values and the y axis is the frequency of those values.\n</p>\n</div>"},{"metadata":{"_cell_guid":"cb757964-b848-499d-8517-d1f99b09f68c","_uuid":"abaef34564c5753f19b7402441827857552796de","trusted":true},"cell_type":"code","source":"# Plot a histogram for all the columns of the dataframe. This shows the frequency of values in all the columns\nsns.set()\nmerged_dataframe.hist(sharex = False, sharey = False, xlabelsize = 4, ylabelsize = 4, figsize=(10, 10))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9845c134-3e70-4168-9fd0-6191449eca42","_uuid":"993f86363c0b55f707ebea03d3d99e519d827db6"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nPlot 1: Scatter plot of Stock Prices vs the Subjectivity.<br/>\n        Stock Value of 0 means the Stock Value reduced since the previous day.<br/>\n        Stock Value of 1 means the Stock Value increased or remained the same since the previous day.\n<br/>        \nPlot 2: Scatter plot of Stock Prices vs the Objectivity.<br/>\n        Stock Value of 0 means the Stock Value reduced since the previous day.<br/>\n        Stock Value of 1 means the Stock Value increased or remained the same since the previous day.\n<br/>                \nPlot 3: Histogram of Subjectivity column.<br/>\n        The x axis are the values of Subjectivity and y axis is its respective frequency.<br/>\n        The plot seems to be normally distributed.\n<br/>       \nPlot 4: Histogram of Objectivity column.<br/>\n        The x axis are the values of Objectivity and y axis is its respective frequency.<br/>\n        The plot seems to be normally distributed.<br/>\n    </p></div>"},{"metadata":{"_cell_guid":"a12031e0-f610-44f7-a339-a299bbe6efde","_uuid":"7b2aa29136fffed22a6b8ff4b82d78c36f9408f6","trusted":true},"cell_type":"code","source":"pyplot.scatter(merged_dataframe['Subjectivity'], merged_dataframe['Label'])\npyplot.xlabel('Subjectivity')\npyplot.ylabel('Stock Price Up or Down 0: Down, 1: Up')\npyplot.show()\npyplot.scatter(merged_dataframe['Objectivity'], merged_dataframe['Label'])\npyplot.xlabel('Objectivity')\npyplot.ylabel('Stock Price Up or Down 0: Down, 1: Up')\npyplot.show()\nmerged_dataframe['Subjectivity'].plot('hist')\npyplot.xlabel('Subjectivity')\npyplot.ylabel('Frequency')\npyplot.show()\nmerged_dataframe['Objectivity'].plot('hist')\npyplot.xlabel('Subjectivity')\npyplot.ylabel('Frequency')\npyplot.show()\nprint(\"Size of the Labels column\")\nprint(merged_dataframe.groupby('Label').size())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2cc4343-be16-44a5-b08c-a94c017a94dc","_uuid":"7fc30252326a91f80242d2fcbc14877f14ae5cc3"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\"><p>\n<h3>3.Feature inspection and filtering</h3>\n<br/>\nLets check for NaN values in individual columns of the dataframe.\n</p>\n</div>"},{"metadata":{"_cell_guid":"9ff2f1fd-995e-4c59-a9cb-70b3e9202e99","_uuid":"82f3d3a156a9290650c97dcd588678f615295487","trusted":true},"cell_type":"code","source":"md_copy = merged_dataframe\nmd_copy = md_copy.replace(-1, np.NaN)\nimport missingno as msno\n# Nullity or missing values by columns\nmsno.matrix(df=md_copy.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eab7da0c-92bd-4c92-890f-1387f35568cd","_uuid":"a0c827789a3e74e33e2a2774870cc8d7338f2c95"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\n<h4>Correlation Map for features:</h4>\n<br/>\nNow, we will plot a heat map and a scatter matrix to see the correlation of the columns with each other.\n<br/><br/>\nYou can see the heat map with pearson correlation values in the plot below.\n<br/><br/>\nThis gave me a better understanding to see if there are any dependant variables or if any of the variables are highly correlated.\n<br/><br/>\nSome variables Subjectivity, Objectivity are negatively correlated. There are very few variables which seem to have a very high correlation. Thus, at this point we can conclude that we do not need any sort of dimensionality reduction technique to be applied.\n</p>\n</div>"},{"metadata":{"_cell_guid":"2aec418c-5b88-42ab-a0fc-9c88a743a30e","scrolled":false,"_uuid":"5a0ccae6a9fab23565e8a926d73ca926007dd259","trusted":true},"cell_type":"code","source":"colormap = pyplot.cm.afmhot\npyplot.figure(figsize=(16,12))\npyplot.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(merged_dataframe.corr(),linewidths=0.1,vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7af577e-dea8-41d4-9f13-860705048f5f","_uuid":"90df12bc5f52c85f482d2b5312f1185c00c603a2","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\n\nbin_col = merged_dataframe.columns\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_count = 0\n    one_count = 0\n    for ix, val in merged_dataframe[col].iteritems():\n        if merged_dataframe.loc[ix, 'Label'] == 0:\n            zero_count += 1\n        else:\n            one_count += 1\n    zero_list.append(zero_count)\n    one_list.append(one_count)\n    \ntrace1 = go.Bar(\n    x=bin_col,\n    y=zero_list ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=bin_col,\n    y=one_list,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23a3f7fe-c18f-420e-a562-0fd8bafb5697","_uuid":"7f1967f3dc254bd0e13eb66e201bdf913f8f88e3"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\n<h3>4. Training</h3>\n<br/>\nRecheck the dataframe to see if the dataset is ready for train.\n<br/><br/>\nThere are certain NaN values in many columns of the dataframe.\n<br/><br/>\nReplace the NaN values with the mean values of the respective column.\n<br/><br/>\nSplit the merged dataframe to inputs(X) and outputs(y)\n<br/><br/>\nIn our dataset, we have columns Subjectivity through Adj Close as inputs and the Label column output.\n<br/><br/>\nNow, we will split our dataset to training and test samples. Lets train out model on first 80% of the data \nand test our prediction model on remaining 20% of the data.\n<br/><br/>\nAs this is a time series, it is important we do not randomly pick training and testing samples.\n<br/><br/>\nLets consider a few machine learning algorithms to perform our training on.\nLogistic Regression\nLinear Discriminant Analysis\nK Nearest Neighbors\nDecision trees\nNaive Bayes\nSupport Vector Classifier\nRandom Forest Classifier\n<br/><br/>\nLets add all of these classifiers to a list 'models'\n<br/><br/>\nAfter splitting the dataset, we can see that there are 1393 samples for training and 597 samples for testing\n</p>\n</div>"},{"metadata":{"_cell_guid":"bdfdadf2-2bab-4b29-a3b5-9fcbe7a106bb","_uuid":"11c076755df2cad209dca2723527bb6ba9bc71cf","trusted":true},"cell_type":"code","source":"# Print the datatypes and count of the dataframe\nprint(merged_dataframe.dtypes)\nprint(merged_dataframe.count())\n# Change the NaN values to the mean value of that column\nnan_list = ['Subjectivity', 'Objectivity', 'Positive', 'Negative', 'Neutral']\nfor col in nan_list:\n    merged_dataframe[col] = merged_dataframe[col].fillna(merged_dataframe[col].mean())\n\n# Recheck the count\nprint(merged_dataframe.count())\n# Separate the dataframe for input(X) and output variables(y)\nX = merged_dataframe.loc[:,'Subjectivity':'Adj Close']\ny = merged_dataframe.loc[:,'Label']\n# Set the validation size, i.e the test set to 20%\nvalidation_size = 0.20\n# Split the dataset to test and train sets\n# Split the initial 70% of the data as training set and the remaining 30% data as the testing set\ntrain_size = int(len(X.index) * 0.7)\nprint(len(y))\nprint(train_size)\nX_train, X_test = X.loc[0:train_size, :], X.loc[train_size: len(X.index), :]\ny_train, y_test = y[0:train_size+1], y.loc[train_size: len(X.index)]\nprint('Observations: %d' % (len(X.index)))\nprint('X Training Observations: %d' % (len(X_train.index)))\nprint('X Testing Observations: %d' % (len(X_test.index)))\nprint('y Training Observations: %d' % (len(y_train)))\nprint('y Testing Observations: %d' % (len(y_test)))\npyplot.plot(X_train['Objectivity'])\npyplot.plot([None for i in X_train['Objectivity']] + [x for x in X_test['Objectivity']])\npyplot.show()\nnum_folds = 10\nscoring = 'accuracy'\n# Append the models to the models list\nmodels = []\nmodels.append(('LR' , LogisticRegression()))\nmodels.append(('LDA' , LinearDiscriminantAnalysis()))\nmodels.append(('KNN' , KNeighborsClassifier()))\nmodels.append(('CART' , DecisionTreeClassifier()))\nmodels.append(('NB' , GaussianNB()))\nmodels.append(('SVM' , SVC()))\nmodels.append(('RF' , RandomForestClassifier(n_estimators=50)))\nmodels.append(('XGBoost', XGBClassifier()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4c432246-60ab-4553-a138-881f9c2eb046","_uuid":"e12c71a94c1173f333d34eecc767055038f17fdb"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nNow, we will iterate over all of the machine learning classifiers and in each loop , we will train against the\nalgorithm, predict the outputs with inputs from the testing split.\n<br/><br/>\nThe actual and the predicted outputs are compared to calculate the accuracy.\n<br/><br/>\nWe see that LDA seems to be giving a high accuracy score, but accuracy is still not the most trustworthy measure.\n</p></div>"},{"metadata":{"_cell_guid":"b8160f68-03a6-4f38-9d09-9ab60f144ba5","_uuid":"f1267c86f04d6b57a21022e8fc58ea073734ad86","trusted":true},"cell_type":"code","source":"# Evaluate each algorithm for accuracy\nresults = []\nnames = []\n'''\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=42)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg) '''\n\nfor name, model in models:\n    clf = model\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    accu_score = accuracy_score(y_test, y_pred)\n    print(name + \": \" + str(accu_score))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7909402a-ea7d-47e2-acde-aa131a62d59f","_uuid":"d40e06fdc86c110f0b7e991998c37ec402caaa1b"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nAs data distributions are in varying ranges, it would be good to scale all of our data and then use it to train our \nalgorithm.\n<br/><br/>\nLets print out the accuracy score, confusion matrix.\n    </p></div>"},{"metadata":{"_cell_guid":"0205a494-a785-414e-835a-2768ab5aee5f","_uuid":"e624dab4727b70aa409e0448f744aa8938c5f1b3","trusted":true},"cell_type":"code","source":"# prepare the model LDA\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel_lda = LinearDiscriminantAnalysis()\nmodel_lda.fit(rescaledX, y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model_lda.predict(rescaledValidationX)\nprint(\"accuracy score:\")\nprint(accuracy_score(y_test, predictions))\nprint(\"confusion matrix: \")\nprint(confusion_matrix(y_test, predictions))\nprint(\"classification report: \")\nprint(classification_report(y_test, predictions))\n\nmodel_xgb = XGBClassifier()\nmodel_xgb.fit(rescaledX, y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model_xgb.predict(rescaledValidationX)\nprint(\"accuracy score:\")\nprint(accuracy_score(y_test, predictions))\nprint(\"confusion matrix: \")\nprint(confusion_matrix(y_test, predictions))\nprint(\"classification report: \")\nprint(classification_report(y_test, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c701b10b-e9ba-45ed-b483-a8bf12ff8021","_uuid":"8abc110725cf391395b5928a075a8576e7870292","trusted":true},"cell_type":"code","source":"# Generating the ROC curve\ny_pred_proba = model_lda.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nprint(\"roc auc is :\" + str(roc_auc))\npyplot.plot([0, 1], [0, 1], 'k--')\npyplot.plot(fpr, tpr)\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.title('ROC Curve')\npyplot.show()\n\n# AUC score using cross validation\nkfold_val = KFold(n_splits=num_folds, random_state=42)\nauc_score = cross_val_score(model_lda, X_test, y_test, cv=kfold_val, scoring='roc_auc')\nprint(\"AUC using cross val: \" + str(auc_score))\nmean_auc = np.mean(auc_score)\nprint(\"Mean AUC score is: \" + str(mean_auc))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"962361a6-3e98-4983-b49e-303548a81c27","_uuid":"d24a4e01a00a622edc1a19213f02077130723c9e","trusted":true},"cell_type":"code","source":"# Scaling Random Forests\n\nmodel_rf = RandomForestClassifier(n_estimators=1000)\nmodel_rf.fit(rescaledX, y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model_rf.predict(rescaledValidationX)\nprint(\"accuracy score:\")\nprint(accuracy_score(y_test, predictions))\nprint(\"confusion matrix: \")\nprint(confusion_matrix(y_test, predictions))\nprint(\"classification report: \")\nprint(classification_report(y_test, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5315ffa-62e6-4c52-835e-d25967793f40","_uuid":"0f177c02c38ef5081997b347803f9884dc6e4461"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\n<h3>5. Feature Importances:</h3>\n<br/>    \nBelow you can find the feature with highest to least important features plotted in the graph.\n<br/><br/>\nThis is for XGBoost.\n</p></div>"},{"metadata":{"_cell_guid":"e535e555-8da0-4cf0-b1d0-1ef8b51d95eb","_uuid":"f4459cee5be0caef36ccba1d4766b49b1198b3d5","trusted":true},"cell_type":"code","source":"features = merged_dataframe.drop(['Label'],axis=1).columns.values\n\nx, y = (list(x) for x in zip(*sorted(zip(model_xgb.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Feature importance for XGBoost',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances for XGBoost',\n     width = 1000, height = 1000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b724f2e-80b3-4fd9-b97b-2d2cc472047d","_uuid":"6dc8efa4a177d7d3d9e59a1235451a598e9a9755"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\nBelow is the feature importance graph for Random Forests.\n</p>\n</div>"},{"metadata":{"_cell_guid":"3e7a8a14-b213-41e9-864e-b6340c8d4cb7","scrolled":false,"_uuid":"2d25e8b08e457b0cee268b079bff54493dfd84d8","trusted":true},"cell_type":"code","source":"x, y = (list(x) for x in zip(*sorted(zip(model_rf.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Feature importance for Random Forests',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances for Random Forests',\n     width = 1000, height = 1000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab2c0b25-d06c-40b4-b221-b00d23d9b90f","_uuid":"28e4b8a0852d6906f7d7bb9d92ccfa4264951a1c"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\n<h3>Fine Tuning XGBoost</h3>\n<br>\nAs of now the model that seems to be performing the best is the XGBoost model.\n<br/><br/>\nLets see if we can fine tune it further to increase the accuracy of the model.\n</p></div>"},{"metadata":{"_cell_guid":"60962a15-1c86-41bf-8c26-ef4e7a379fe4","scrolled":false,"_uuid":"f2e017318fca766f0fc06ad5131b57a21913f5a7","trusted":true},"cell_type":"code","source":"# XGBoost on Stock Price dataset, Tune n_estimators and max_depth\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib\n\nmatplotlib.use('Agg')\nmodel = XGBClassifier()\nn_estimators = [150, 200, 250, 450, 500, 550, 1000]\nmax_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nprint(max_depth)\nbest_depth = 0\nbest_estimator = 0\nmax_score = 0\nfor n in n_estimators:\n    for md in max_depth:\n        model = XGBClassifier(n_estimators=n, max_depth=md)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        score = accuracy_score(y_test, y_pred)\n        if score > max_score:\n            max_score = score\n            best_depth = md\n            best_estimator = n\n        print(\"Score is \" + str(score) + \" at depth of \" + str(md) + \" and estimator \" + str(n))\nprint(\"Best score is \" + str(max_score) + \" at depth of \" + str(best_depth) + \" and estimator of \" + str(best_estimator))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72c9390e-3db6-478b-b53e-e2bdb9deff8a","_uuid":"2f12ed6c598ad15b2b7e8c67a6003c8e5b34053b"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h3> Fine tuning with important features:</h3>"},{"metadata":{"_cell_guid":"58995d46-2950-4414-93d8-f7ae38c7207c","_uuid":"2d18f4829a1ab74a9691cef007328c03c75b42cf","trusted":true},"cell_type":"code","source":"imp_features_df = merged_dataframe[['Low', \"Neutral\", 'Close', 'Objectivity', 'Date']]\nXi_train, Xi_test = X.loc[0:train_size, :], X.loc[train_size: len(X.index), :]\nclf = XGBClassifier(n_estimators=500, max_depth=3)\nclf.fit(Xi_train, y_train)\nyi_pred = clf.predict(Xi_test)\nscore = accuracy_score(y_test, yi_pred)\nprint(\"Score is \"+ str(score))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9665a091-e698-4df9-9573-6959c2dd9490","_uuid":"706734a489eeaa8783d1d00c3a0c017ff3ef7ad2"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h3>PCA transformation:</h3>\n    </div>"},{"metadata":{"_cell_guid":"67d053a4-dc02-4086-9315-ec14cd12db0d","_uuid":"752e6c265fd1e9b885f6d4b6b166cc2b23c3e531","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npca.fit(X)\ntransformed = pca.transform(X)\n\ntransformed.shape\nprint(type(transformed))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc81a32a-2dae-4030-ae69-5b3e94c8eedd","_uuid":"59d83c3b159ffe928017296398df60a62e6360c4","trusted":true},"cell_type":"code","source":"pca_df = pd.DataFrame(transformed)\n\nX_train_pca, X_test_pca = pca_df.loc[0:train_size, :], pca_df.loc[train_size: len(X.index), :]\n\nclf = XGBClassifier(n_estimators=500, max_depth=3)\nclf.fit(X_train_pca, y_train)\ny_pred_pca = clf.predict(X_test_pca)\nscore = accuracy_score(y_test, y_pred_pca)\nprint(\"Score is \"+ str(score))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a064ee8-bcaa-4939-80c6-d88a89464863","_uuid":"8d3c85259402544163076f23afca7451c148e0a2","trusted":true},"cell_type":"code","source":"pca_matrix = confusion_matrix(y_test, y_pred_pca)\npca_report = classification_report(y_test, y_pred_pca)\nprint(\"Confusion Matrix: \\n\" + str(pca_matrix))\nprint(\"Classification report: \\n\" + str(pca_report))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a57c5e86-3c36-4147-ae34-c824810d2fb4","_uuid":"4f0494def5eaecdac1b00aa6ce3a8c0c1753616d","trusted":true},"cell_type":"code","source":"# Generating the ROC curve\ny_pred_proba_pca = clf.predict_proba(X_test_pca)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_pca)\nroc_auc = auc(fpr, tpr)\nprint(\"AUC score is \" + str(roc_auc))\n\n# Plot ROC curve\nprint(\"roc auc is :\" + str(roc_auc))\npyplot.plot([0, 1], [0, 1], 'k--')\npyplot.plot(fpr, tpr)\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.title('ROC Curve')\npyplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"502ea190-2e32-4fae-9378-c2137c2bffb1","_uuid":"af96fd82c109e844b17bbd33411e324a260dd137"},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<p>\n    <br/>\nNow lets try and train our data using a TimeSeriesSplit which is specifically used for splitting the dataset to \ntraining and testing datasets.\n<br/><br/>\nBy specifying the number of splits, we can split the data on a sample of 40%, 70% and 100% of the dataset.\n<br/><br/>\nThe plots below shows the splits of the datasets and the respective number of samples in each split.\n</p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}