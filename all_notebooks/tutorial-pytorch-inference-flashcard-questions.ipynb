{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">PyTorch Tutorial Series<br> Inference Series Part I</h1>\n<br>","metadata":{}},{"cell_type":"markdown","source":"OKOKOK, I admit, with the influx of pretty notebooks recently, I cannot stop to \"copy\" their style. After all, aesthetic pleasing notes (notebooks) make me want to read it more.\n\n---\n\nThis notebook is part I of the PyTorch Tutorial Inference Series. I will detail on how to save and load weights in PyTorch. I will split this notebook into two parts:\n\n1. First part: The real inference in action - for now, I will just add SETI as the main competition. However, I intend to share this notebook across multiple different competitions.\n\n2. The tutorial on PyTorch.\n\n---\n\nI have added a back to top button for each section for easy navigation.","metadata":{}},{"cell_type":"markdown","source":"References:\n\n1. [Using args and kwargs](https://note.nkmk.me/en/python-args-kwargs-usage/#:~:text=In%20Python%2C%20by%20adding%20*%20and,arguments)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation</center></h3>\n\n    \n* [Dependencies](#1)\n* [Configurations](#2)\n* [Seeding](#3)\n* [Utility](#30)\n    * [Numpy to Latex](#31)\n* [Loading Files](#4)\n* [Dataset](#5)\n* [Augmentations](#5)\n* [Model Instantiation](#7)    \n* [Inference by Folds](#8)\n \n    \n* [Saving and Loading Model Weights](#20)\n    \n    \n* [Dissecting Inference by Folds](#60)\n    * [Problem Settings](#61)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dependencies</h1>","metadata":{}},{"cell_type":"code","source":"!pip install -q git+https://github.com/rwightman/pytorch-image-models.git\n!pip install -q torchsummary\n!pip install -q -U git+https://github.com/albu/albumentations --no-cache-dir\n!pip install -q neptune-client \n\nfrom IPython.display import clear_output \nclear_output()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:17:38.284227Z","iopub.execute_input":"2021-06-20T09:17:38.284691Z","iopub.status.idle":"2021-06-20T09:18:32.50156Z","shell.execute_reply.started":"2021-06-20T09:17:38.284604Z","shell.execute_reply":"2021-06-20T09:18:32.500421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\ngeffnet_path = '../input/hongnangeffnet/gen-efficientnet-pytorch-master-hongnan'\n# timm_path = '../input/pytorchimagemodelsmasteroct302020/pytorch-image-models-master'\ntimm_path = '../input/pytorch-image-models/pytorch-image-models-master'\nvit_path = '../input/vision-transformer-pytorch/VisionTransformer-Pytorch'\nsys_paths = [geffnet_path, timm_path, vit_path]\nfor paths in sys_paths:\n    sys.path.append(paths)\n\nimport geffnet\nimport timm\nfrom vision_transformer_pytorch import VisionTransformer","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-20T09:18:32.505847Z","iopub.execute_input":"2021-06-20T09:18:32.506188Z","iopub.status.idle":"2021-06-20T09:18:35.565999Z","shell.execute_reply.started":"2021-06-20T09:18:32.506155Z","shell.execute_reply":"2021-06-20T09:18:35.564667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport os\nimport random\nimport warnings\nfrom typing import *\nfrom tqdm.notebook import tqdm\nimport albumentations\nimport cv2\n# import neptune.new as neptune\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport timm\nimport torch\nimport torch.nn.functional as F\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.optimizer import Optimizer\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchsummary import summary\nfrom torchvision import models\nfrom tqdm.notebook import tqdm\n\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import clear_output\nfrom torch.optim.lr_scheduler import (CosineAnnealingLR,\n                                      CosineAnnealingWarmRestarts,\n                                      ReduceLROnPlateau)\nfrom torch.utils.data import DataLoader, Dataset\n\nclear_output()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-20T09:18:35.57154Z","iopub.execute_input":"2021-06-20T09:18:35.574217Z","iopub.status.idle":"2021-06-20T09:18:37.893005Z","shell.execute_reply.started":"2021-06-20T09:18:35.57417Z","shell.execute_reply":"2021-06-20T09:18:37.892092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Configurations</h1>","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    \"COMPETITION_NAME\": \"SETI Breakthrough Listen - E.T. Signal Search\",\n    \"MODEL\": {\"MODEL_FACTORY\": \"timm\", \"MODEL_NAME\": \"efficientnet_b0\"},\n    \"WORKSPACE\": \"Kaggle\",\n    \"DATA\": {\n        \"TARGET_COL_NAME\": \"target\",\n        \"IMAGE_COL_NAME\": \"id\",\n        \"NUM_CLASSES\": 1,\n        \"CLASS_LIST\": [0, 1],\n        \"IMAGE_SIZE\": 640,\n        \"CHANNEL_MODE\": \"spatial_6ch\",\n        \"USE_MIXUP\": True\n    },\n    \"CROSS_VALIDATION\": {\"SCHEMA\": 'StratifiedKFold', \"NUM_FOLDS\": 4},\n    \"TRAIN\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": True,  # using random sampler\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        },\n        \"SETTINGS\": {\n            \"IMAGE_SIZE\": 640,\n            \"NUM_EPOCHS\": 3,\n            \"USE_AMP\": True,\n            \"USE_GRAD_ACCUM\": False,\n            \"ACCUMULATION_STEP\": 1,\n            \"DEBUG\": True,\n            \"VERBOSE\": True,\n            \"VERBOSE_STEP\": 10,\n        },\n    },\n    \"VALIDATION\": {\n        \"DATALOADER\": {\n            \"batch_size\": 32,\n            \"shuffle\": False,\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        }\n    },\n    \"TEST\": {\n        \"DATALOADER\": {\n            \"batch_size\": 64,\n            \"shuffle\": False,\n            \"num_workers\": 4,\n            \"drop_last\": False,\n        }\n    },\n    \"OPTIMIZER\": {\n        \"NAME\": \"AdamW\",\n        \"OPTIMIZER_PARAMS\": {\"lr\": 1e-4, \"eps\": 1.0e-8, \"weight_decay\": 1.0e-3},\n    },\n    \"SCHEDULER\": {\n        \"NAME\": \"CosineAnnealingWarmRestarts\",\n        \"SCHEDULER_PARAMS\": {\n            \"T_0\": 4,\n            \"T_mult\": 1,\n            \"eta_min\": 1.0e-7,\n            \"last_epoch\": -1,\n            \"verbose\": True,\n        },\n        \"CUSTOM\": \"GradualWarmupSchedulerV2\",\n        \"CUSTOM_PARAMS\": {\"multiplier\": 10, \"total_epoch\": 1},\n        \"VAL_STEP\": False,\n    },\n    \"CRITERION_TRAIN\": {\n        \"NAME\": \"BCEWithLogitsLoss\",\n        \"LOSS_PARAMS\": {\n            \"weight\": None,\n            \"size_average\": None,\n            \"reduce\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None\n        },\n    },\n    \"CRITERION_VALIDATION\": {\n        \"NAME\": \"BCEWithLogitsLoss\",\n        \"LOSS_PARAMS\": {\n            \"weight\": None,\n            \"size_average\": None,\n            \"reduce\": None,\n            \"reduction\": \"mean\",\n            \"pos_weight\": None\n        },\n    },\n    \"TRAIN_TRANSFORMS\": {\n        \"VerticalFlip\": {\"p\": 0.5},\n        \"HorizontalFlip\": {\"p\": 0.5},\n        \"Resize\": {\"height\": 640, \"width\": 640, \"p\": 1},\n    },\n    \"VALID_TRANSFORMS\": {\n        \"Resize\": {\"height\": 640, \"width\": 640, \"p\": 1},\n    },\n    \"TEST_TRANSFORMS\": {\n        \"Resize\": {\"height\": 640, \"width\": 640, \"p\": 1},\n    },\n    \"TTA_TRANSFORMS\": [{\n        \"Resize\": {\"height\": 640, \"width\": 640, \"p\": 1},\n    },\n        {\n        \"Resize\": {\"height\": 640, \"width\": 640, \"p\": 1},\n    }],\n    \"PATH\": {\n        \"DATA_DIR\": \"/content/\",\n        \"TRAIN_CSV\": \"../input/seti-breakthrough-listen/train_labels.csv\",\n        \"TRAIN_PATH\": \"../input/seti-breakthrough-listen/train\",\n\n        \"TEST_CSV\": \"../input/seti-breakthrough-listen/sample_submission.csv\",\n        \"TEST_PATH\": \"../input/seti-breakthrough-listen/test\",\n        \"SAMPLE_SUBMISSION_CSV\": \"../input/seti-breakthrough-listen/sample_submission.csv\",\n        \"SAVE_WEIGHT_PATH\": \"../input/et41-efficientnetb0\",\n        \"OOF_PATH\": \"./\",\n        \"LOG_PATH\": \"./log.txt\"\n    },\n    \"SEED\": 19921930,\n    \"DEVICE\": \"cuda\",\n    \"GPU\": \"V100\",\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:35:40.69551Z","iopub.execute_input":"2021-06-19T11:35:40.696125Z","iopub.status.idle":"2021-06-19T11:35:40.706468Z","shell.execute_reply.started":"2021-06-19T11:35:40.696083Z","shell.execute_reply":"2021-06-19T11:35:40.705807Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = CONFIG\ndevice = config['DEVICE']","metadata":{"execution":{"iopub.status.busy":"2021-06-19T12:13:36.896486Z","iopub.execute_input":"2021-06-19T12:13:36.896852Z","iopub.status.idle":"2021-06-19T12:13:36.901581Z","shell.execute_reply.started":"2021-06-19T12:13:36.896822Z","shell.execute_reply":"2021-06-19T12:13:36.900466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Seeding</h1>","metadata":{}},{"cell_type":"code","source":"def seed_all(seed: int = 1930):\n    \"\"\"Seed all random number generators.\"\"\"\n    print(\"Using Seed Number {}\".format(seed))\n\n    os.environ[\"PYTHONHASHSEED\"] = str(\n        seed\n    )  # set PYTHONHASHSEED env var at fixed value\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n    np.random.seed(seed)  # for numpy pseudo-random generator\n    # set fixed value for python built-in pseudo-random generator\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = False\n\n\ndef seed_worker(_worker_id):\n    \"\"\"Seed a worker with the given ID.\"\"\"\n    worker_seed = torch.initial_seed() % 2 ** 32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(config['SEED'])","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"30\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Utility Functions</h1>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"31\"></a>\n\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Converting Numpy Arrays into Latex Form</h2>\n\n[How to convert Numpy Array into Latex form](https://stackoverflow.com/questions/17129290/numpy-2d-and-1d-array-to-latex-bmatrix)","metadata":{}},{"cell_type":"code","source":"def bmatrix(a):\n    \"\"\"Returns a LaTeX bmatrix\n\n    :a: numpy array\n    :returns: LaTeX bmatrix as a string\n    \"\"\"\n    if len(a.shape) > 2:\n        raise ValueError('bmatrix can at most display two dimensions')\n    lines = np.array2string(a, max_line_width=np.infty).replace('[', '').replace(']', '').splitlines()\n    rv = [r'\\begin{bmatrix}']\n    rv += ['  ' + ' & '.join(l.split()) + r'\\\\' for l in lines]\n    rv +=  [r'\\end{bmatrix}']\n    return '\\n'.join(rv)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:41:47.037092Z","iopub.execute_input":"2021-06-20T09:41:47.03752Z","iopub.status.idle":"2021-06-20T09:41:47.048179Z","shell.execute_reply.started":"2021-06-20T09:41:47.037489Z","shell.execute_reply":"2021-06-20T09:41:47.04727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Loading Data</h1>","metadata":{}},{"cell_type":"code","source":"def get_file_path(image_id, train_path=None, test_path=None, *args):\n\n    # args handle special cases like the one we see here, where the folders are nested in SETI\n    # in this SETI, we apply on image_id so image_info is used to generate\n\n    if train_path is not None:\n        return os.path.join(train_path, '{}/{}.npy'.format(image_id[0], image_id))\n        # return config['PATH']['TRAIN_PATH']\n\n    if test_path is not None:\n        # return config['PATH']['TEST_PATH']\n        return os.path.join(test_path, '{}/{}.npy'.format(image_id[0], image_id))\n\n\ntrain_path = config['PATH']['TRAIN_PATH']\ntest_path = config['PATH']['TEST_PATH']\ntrain = pd.read_csv(CONFIG['PATH']['TRAIN_CSV'])\ntest = pd.read_csv(CONFIG['PATH']['TEST_CSV'])\n\ntrain['file_path'] = train['id'].apply(get_file_path, train_path=train_path)\ntest['file_path'] = test['id'].apply(get_file_path, test_path=test_path)\n\ndisplay(train.head(3))\ndisplay(test.head(3))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T12:04:31.045028Z","iopub.execute_input":"2021-06-19T12:04:31.045468Z","iopub.status.idle":"2021-06-19T12:04:31.257592Z","shell.execute_reply.started":"2021-06-19T12:04:31.045438Z","shell.execute_reply":"2021-06-19T12:04:31.256982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset</h1>","metadata":{}},{"cell_type":"markdown","source":"`Dataset` may be unique to each competition. Here is an architecture that I used here. As you can see, in SETI, we may train on different channels, which may not be the case in most Image Classification tasks. Therefore, we defined `Dataset` differently here.","metadata":{}},{"cell_type":"code","source":"class AlienDataset(Dataset):\n    def __init__(self, df, config, transform=None, mode='train'):\n        self.df = df  # this assumes we have a df to begin with and not getting files from directory directly\n        self.config = config\n        # this line assumes you have a column called file_path, considering putting inside config\n        self.file_names = df['file_path'].values\n        self.labels = df[config['DATA']['TARGET_COL_NAME']].values\n        self.transform = transform\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = np.load(self.file_names[idx])  # -> (6, 273, 256)\n\n        if self.config['DATA']['CHANNEL_MODE'] == 'spatial_6ch':\n            image = image.astype(np.float32)\n            image = np.vstack(image)  # no transpose here (1638, 256)\n            # image = np.vstack(image).transpose((1, 0)) -> (256, 1638)\n\n        elif self.config['DATA']['CHANNEL_MODE'] == 'spatial_3ch':\n            image = image[::2].astype(np.float32)\n            image = np.vstack(image).transpose((1, 0))\n\n        elif self.config['DATA']['CHANNEL_MODE'] == '6_channel':\n            image = image.astype(np.float32)\n            image = np.transpose(image, (1, 2, 0))\n\n        elif self.config['DATA']['CHANNEL_MODE'] == '3_channel':\n            image = image[::2].astype(np.float32)\n            image = np.transpose(image, (1, 2, 0))\n\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = torch.from_numpy(image).float()\n\n        if self.mode == 'test':\n            return image\n        else:\n            label = torch.tensor(self.labels[idx]).float()\n            return image, label\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Augmentations</h1>","metadata":{}},{"cell_type":"code","source":"class Transform:\n\n    def __init__(self, aug_kwargs: Dict):\n        albu_augs = [getattr(albumentations, name)(**kwargs)\n                     for name, kwargs in aug_kwargs.items()]\n        albu_augs.append(ToTensorV2(p=1))\n\n        self.transform = albumentations.Compose(albu_augs)\n\n    def __call__(self, image):\n        image = self.transform(image=image)[\"image\"]\n        return image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Model Instantiation</h1>","metadata":{}},{"cell_type":"markdown","source":"We are using custom activation in our custom `head`. A `head` is a jargon for the `classifier` layer in a CNN. In general, people finetune a pretrained model by simply removing the last classifier layer and replace it with the correct number of classes.","metadata":{}},{"cell_type":"code","source":"sigmoid = torch.nn.Sigmoid()\n\n\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass Swish_Module(torch.nn.Module):\n    def forward(self, x):\n        return Swish.apply(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, defining a `model` class may not necessarily be enough for reusability in every competition. However, a generic architecture as presented below should suffice for most.","metadata":{}},{"cell_type":"code","source":"class AlienSingleHead(torch.nn.Module):\n    \"\"\"A custom model.\"\"\"\n\n    def __init__(\n        self,\n        config: type,\n        pretrained: bool = True,\n    ):\n        \"\"\"Construct a custom model.\"\"\"\n        super().__init__()\n        self.config = config\n        self.pretrained = pretrained\n        \n        print(\"Pretrained is {}\".format(self.pretrained))\n\n        self.activation = Swish_Module()\n        \n        self.architecture = {\n            \"backbone\": None,\n            \"bottleneck\": None,\n            \"classifier_head\": None,\n        }\n\n        def __setattr__(self, name, value):\n            self.model.__setattr__(self, name, value)\n\n        _model_factory = (\n            timm.create_model\n            if self.config[\"MODEL\"][\"MODEL_FACTORY\"] == \"timm\"\n            else geffnet.create_model\n        )\n        \n        if config['DATA']['CHANNEL_MODE'] == 'spatial_6ch' or config['DATA']['CHANNEL_MODE'] == 'spatial_3ch':\n\n            self.model = _model_factory(\n                model_name=self.config[\"MODEL\"][\"MODEL_NAME\"],\n                pretrained=self.pretrained, in_chans=1) # set channel = 1 since we using spatial\n\n        else:\n            self.model = _model_factory(\n                            model_name=self.config[\"MODEL\"][\"MODEL_NAME\"],\n                            pretrained=self.pretrained, in_chans=3) # set channel = 1 since we using spatial\n\n        # reset head\n        self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n        \n        # after resetting, there is no longer any classifier head, therefore it is the backbone now.\n        self.architecture[\"backbone\"] = self.model\n        \n        # get out features of the last cnn layer from backbone, which is also the in features of the next layer\n        self.in_features = self.architecture[\"backbone\"].num_features\n\n        self.single_head_fc = torch.nn.Sequential(\n            torch.nn.Linear(self.in_features, self.in_features),\n            self.activation,\n            torch.nn.Dropout(p=0.5),\n            torch.nn.Linear(self.in_features, self.config[\"DATA\"][\"NUM_CLASSES\"]),\n        )\n        self.architecture[\"classifier_head\"] = self.single_head_fc\n\n\n    # feature map after cnn layer\n    def extract_features(self, x):\n        feature_logits = self.architecture[\"backbone\"](x)\n        # TODO: caution, if you use forward_features, then you need reshape. See test.py\n        return feature_logits\n\n    def forward(self, x):\n        feature_logits = self.extract_features(x)\n        classifier_logits = self.architecture[\"classifier_head\"](feature_logits)\n        return classifier_logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A good practice often overlooked is to do a simpel `forward-pass` in your `model`. This prevents bugs later.","metadata":{}},{"cell_type":"code","source":"model_forward_pass = AlienSingleHead(config,pretrained=False)\ntrain_dataset = AlienDataset(train, config, transform=Transform(config[\"TRAIN_TRANSFORMS\"]))\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n                          num_workers=4, pin_memory=True, drop_last=True)\n\nfor image, label in train_loader:\n    output = model_forward_pass(image)\n    print(output)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Inference By Folds</h1>","metadata":{}},{"cell_type":"markdown","source":"Inference by fold function has remained faithful to me throughout competitions, I only ever have to worry about changing `sigmoid` to `softmax` depending on the model's settings. One thing to note is that, I save a lot of things in the `model` `state_dict`. In particular, I save `oof` predictions in my `state_dict`. This makes me pulling out `oof` predictions easy when I accidentally didn't manage to save my `oof` during training, which happens often.\n\nGo down to my example section for better understanding how it works under the hood.","metadata":{}},{"cell_type":"code","source":"def inference_by_fold(config, model, state_dicts, test_loader):\n    model.to(device)\n    model.eval()\n    \n    all_folds_preds = []\n    \n    with torch.no_grad():\n        \n        for fold_num, state in enumerate(state_dicts):\n            if \"model_state_dict\" not in state:\n                model.load_state_dict(state)\n            else:\n                model.load_state_dict(state[\"model_state_dict\"])\n\n            current_fold_preds = []\n            for data in tqdm(test_loader, position=0, leave=True):\n                images = data\n                images = images.to(device)\n                logits = model(images)\n\n                sigmoid_preds = logits.sigmoid().detach().cpu().numpy()\n                current_fold_preds.append(sigmoid_preds)\n\n            current_fold_preds = np.concatenate(current_fold_preds, axis=0)\n            all_folds_preds.append(current_fold_preds)\n        avg_preds = np.mean(all_folds_preds, axis=0)\n    return avg_preds\n\ndef LoadTestSet(test_df: pd.DataFrame, config):\n    \"\"\"Train the model on the given fold.\"\"\"\n    model = AlienSingleHead(config,pretrained=False)\n    model.to(device)\n\n\n    # consider adding args or kwargs here to accomodate multiple tta transforms\n    def test_transforms(config=config, tta=False):\n        \n        transforms_dict = {}\n        transforms_test = Transform(config[\"TEST_TRANSFORMS\"])\n        transforms_dict['transforms_test'] = transforms_test # this step is guaranteed, but tta isn't since we may not use it\n        \n        if tta is not False:\n            transforms_dict['transforms_tta'] = []\n            \n            for tta_configs in config[\"TEST_TRANSFORMS\"]:\n                transforms_dict['transforms_tta'].append(Transform(tta_configs))\n        \n        return transforms_dict\n\n    # transforms_test, transforms_tta_test = test_transforms(image_size=config['DATA']['IMAGE_SIZE'])\n    \n    transforms_test = test_transforms(config, tta=False)['transforms_test']\n    \n    test_dataset = AlienDataset(df=test,config=config, mode='test', transform=transforms_test)\n    # tta_test_dataset = AlienTrainDataset(df=test,config=config, mode='test', transform=transforms_tta_test)\n    \n    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, \n                             num_workers=4, pin_memory=True)\n    \n#     tta_test_loader = torch.utils.data.DataLoader(\n#         tta_test_dataset, batch_size=64, shuffle=False, num_workers=4\n#     )\n\n    \n    weights = [\"../input/et-alien-weights/efficientnet_b0_fold_1.pt\"]\n    \n\n    state_dicts = [torch.load(path)['model_state_dict'] for path in weights]\n\n    predictions = inference_by_fold(config=config, model=model, state_dicts = state_dicts, test_loader=test_loader)\n    test['target'] = predictions\n    test[['id', 'target']].to_csv('submission.csv', index=False)\n    test.head()\n\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12,6))\n    plt.hist(test.target,bins=100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LoadTestSet(test, config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"60\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dissecting Inference By Folds</h1>","metadata":{}},{"cell_type":"markdown","source":"[Reference I on np.concatenate](https://stackoverflow.com/questions/63722692/what-does-numpy-concatenate-do-with-a-single-argument)","metadata":{}},{"cell_type":"markdown","source":"When I started out, I had a hard time understanding what is the meaning of \"averaging predictions by folds\". This could be attributed to me jumping the gun too soon. Here, I give a full overview with a simple example to illustrate this idea.\n\n<a id=\"61\"></a>\n\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Problem Settings</h2>\n\n- Batch Size: 2\n- Number of Test Images: 3\n- Number of Folds: 5\n- Classification Problem with Softmax Activation\n- Number of classes: 5\n- all_preds_array: each inner array is 2d and is of shape (3, 5) - 3 rows and 5 columns where 3 is the number of images, and 5 the number of predictions per image.\n\nI purposely made the `batch_size` to be 2 and total number of images to predict on to be 3. This is to tell you that your `test_loader` will take two loops to complete the predictions.","metadata":{}},{"cell_type":"markdown","source":"The below `all_preds_array` is a dummy predictions I made on the 3 images across 5 folds/models. Note that there are 5 inner arrays in this, as an example, we can take `all_preds_array[0]` to be the first fold predictions across all 3 images. This inner array is a 2d array of 3 by 5, where the first row, `all_preds_array[0][0]` is the 5 predictions outputed by `softmax` for image number 1. And to be more pedantic,`all_preds_array[0][0]` being 0.01 just means for image 1, fold 1 predicts the image's probability to be class 1 is 0.01, and `all_preds_array[0][1]` just mean for the same image 1, fold 1 predicts the image's probability to be class 2 is 0.02. Note that if you sum up each row, it must add up to 1 in this case, because we are using `softmax`, where the predictions for all 5 class must sum up to 1.\n\n---\n\nSo we have 5 folds, and let us put focus on just image 1 for simplicity. We then have perform inference 5 times on image 1 using the same model, holding everything else constant. \n\n- Image 1 Fold 1 Predictions: `[0.01, 0.02, 0.03, 0.9, 0.04]`\n- Image 1 Fold 2 Predictions: `[0.03, 0.05, 0.01, 0.88, 0.03]`\n- Image 1 Fold 3 Predictions: `[0.05, 0.03, 0.08, 0.83, 0.01]`\n- Image 1 Fold 4 Predictions: `[0.02, 0.01, 0.03, 0.92, 0.02]`\n- Image 1 Fold 5 Predictions: `[0.01, 0.02, 0.02, 0.93, 0.02]`\n\nWe then add all 5 folds up and **average** them. This is akin to performing a mean ensemble. In general, we average the predictions. This usually produces a more robust result (can you explain why?)\n\n---\n\n> So we have trained 5 models because we used 5 fold cross validation. For each of the 5 fold models that we have, we will load each fold model's weights and use them to make predictions on the unseen test images. As a result, we have 5 predictions for each test image. To be clear, if you have 1000 unseen test images, named $$i~~~ \\forall i \\in {1,2,...,1000}$$ then for each image $i$, there will be 5 predictions each for it, we can call them as such $$P(i_{j}) ~~~ \\forall j \\in {1,2,3,4,5}$$ where $j$ represents the number of folds. Thus, by convention, we take the **average/mean** of these 5 sets of predictions and take the average value/probability as the final prediction value. This score is then submitted to Kaggle and we get what we called the LB score, which should correlate to your CV/OOF score.\n\n","metadata":{}},{"cell_type":"code","source":"all_preds_array = \\\nnp.array([np.array([[0.01, 0.02, 0.03, 0.9 , 0.04],\n           [0.02, 0.8 , 0.05, 0.06, 0.07],\n           [0.01, 0.8 , 0.03, 0.09, 0.07]]), \n \n np.array([[0.03,   0.05  , 0.01  , 0.88  ,  0.03],\n           [0.01  , 0.82  , 0.02  , 0.07  ,  0.08],\n           [0.005 , 0.81  , 0.0205, 0.0555, 0.109]]),\n \n np.array([[0.05, 0.03, 0.08, 0.83, 0.01],\n           [0.05, 0.89, 0.01, 0.02, 0.03],\n           [0.03, 0.78, 0.05, 0.02, 0.12]]),\n \n np.array([[0.02, 0.01, 0.03, 0.92, 0.02],\n           [0.05, 0.85, 0.01, 0.01, 0.08],\n           [0.02, 0.88, 0.03, 0.05, 0.02]]), \n \n np.array([[0.01, 0.02, 0.02, 0.93, 0.02],\n           [0.03, 0.76, 0.06, 0.06, 0.09],\n           [0.02, 0.83, 0.01, 0.07, 0.07]])])\n\n# we use np.mean on axis=0 to calculate the average. axis=0 just means we take the mean of each row.\nprint(np.mean(all_preds_array, axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:41:04.457644Z","iopub.execute_input":"2021-06-20T09:41:04.458041Z","iopub.status.idle":"2021-06-20T09:41:04.471165Z","shell.execute_reply.started":"2021-06-20T09:41:04.457979Z","shell.execute_reply":"2021-06-20T09:41:04.469811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hide the input cell below to demonstrate how the batch works in the previous example:","metadata":{}},{"cell_type":"code","source":"### Fold 1 ###\n### First Batch: 2 Predictions ###\nfold_1_pred_1 = np.array(\n    [[0.01, 0.02, 0.03, 0.9, 0.04], [0.02, 0.8, 0.05, 0.06, 0.07]])\n\n### Second Batch: 1 Prediction ###\n\nfold_1_pred_2 = np.array([[0.01, 0.8, 0.03, 0.09, 0.07]])\n\n\n### Fold 2 ###\n### First Batch: 2 Predictions ###\n\nfold_2_pred_1 = np.array([[0.03, 0.05, 0.01, 0.88, 0.03], [\n                         0.01, 0.82, 0.02, 0.07, 0.08]])\n\n### Second Batch: 1 Prediction ###\n\nfold_2_pred_2 = np.array([[0.005, 0.81, 0.0205, 0.0555, 0.109]])\n\n\n### Fold 3 ###\n### First Batch: 2 Predictions ###\n\nfold_3_pred_1 = np.array([[0.05, 0.03, 0.08, 0.83, 0.01], [\n                         0.05, 0.89, 0.01, 0.02, 0.03]])\n\n### Second Batch: 1 Prediction ###\n\nfold_3_pred_2 = np.array([[0.03, 0.78, 0.05, 0.02, 0.12]])\n\n\n### Fold 4 ###\n### First Batch: 2 Predictions ###\n\nfold_4_pred_1 = np.array([[0.02, 0.01, 0.03, 0.92, 0.02], [\n                         0.05, 0.85, 0.01, 0.01, 0.08]])\n\n### Second Batch: 1 Prediction ###\n\nfold_4_pred_2 = np.array([[0.02, 0.88, 0.03, 0.05, 0.02]])\n\n\n### Fold 5 ###\n### First Batch: 2 Predictions ###\n\nfold_5_pred_1 = np.array([[0.01, 0.02, 0.02, 0.93, 0.02], [\n                         0.03, 0.76, 0.06, 0.06, 0.09]])\n\n### Second Batch: 1 Prediction ###\n\nfold_5_pred_2 = np.array([[0.02, 0.83, 0.01, 0.07, 0.07]])\n\n\n### This list should contain the predictions of all 5 folds ###\nall_folds_preds = []\n\n\nfold_1_preds = []\n\n### Fold 1 ###\n### All Batches: Total of 3 Predictions in the following format ###\n\nfold_1_preds = [fold_1_pred_1, fold_1_pred_2]\n#print('\\nfold_1_preds before np.concatenate\\n',fold_1_preds)\n\n### Concatenate because previous format is not good, we want it to be a numpy array ###\n\nfold_1_preds = np.concatenate(fold_1_preds, axis=0)\n\n# Something good to know, concatenate works exactly the same as such: #\n# The idea is you concatenate two list, over the axis 0 which is rows. #\n\nfold_1_preds_ = np.concatenate([fold_1_pred_1, fold_1_pred_2], axis=0)\n#print('fold_1_preds after np.concatenate\\n',fold_1_preds)\n#print('fold_1_preds after np.concatenate using different method\\n',fold_1_preds_)\n\nall_folds_preds.append(fold_1_preds)\nprint('All Folds Pred list after Fold 1 is \\n\\n{}\\n\\n'.format(all_folds_preds))\n###################################################################################\n\nfold_2_preds = []\n\n### Fold 2 ###\n### All Batches: Total of 3 Predictions in the following format ###\n\nfold_2_preds = [fold_2_pred_1, fold_2_pred_2]\nfold_2_preds = np.concatenate(fold_2_preds, axis=0)\n\nall_folds_preds.append(fold_2_preds)\nprint('All Folds Pred list after Fold 1+2 is \\n\\n{}\\n\\n'.format(all_folds_preds))\n\n\nfold_3_preds = []\n\n### Fold 3 ###\n### All Batches: Total of 3 Predictions in the following format ###\n\nfold_3_preds = [fold_3_pred_1, fold_3_pred_2]\nfold_3_preds = np.concatenate(fold_3_preds, axis=0)\nall_folds_preds.append(fold_3_preds)\nprint('All Folds Pred list after fold 1+2+3 is \\n\\n{}\\n\\n'.format(all_folds_preds))\n\n\nfold_4_preds = []\n\n### Fold 4 ###\n### All Batches: Total of 3 Predictions in the following format ###\n\nfold_4_preds = [fold_4_pred_1, fold_4_pred_2]\nfold_4_preds = np.concatenate(fold_4_preds, axis=0)\nall_folds_preds.append(fold_4_preds)\nprint('All Folds Pred list after fold 1+2+3+4 is \\n\\n{}\\n\\n'.format(all_folds_preds))\n\n\nfold_5_preds = []\n\n### Fold 5 ###\n### All Batches: Total of 3 Predictions in the following format ###\n\nfold_5_preds = [fold_5_pred_1, fold_5_pred_2]\nfold_5_preds\nfold_5_preds = np.concatenate(fold_5_preds, axis=0)\nfold_5_preds\nall_folds_preds.append(fold_5_preds)\nprint('All Folds Pred list after fold 1+2+3+4+5 is \\n\\n{}\\n\\n'.format(all_folds_preds))\n\n\n###### Finally, we take np.mean over all_folds_preds over row wise calculation ######\n###### Do not use concat here! ######\navg_pred_without_concat = np.mean(all_folds_preds, axis=0)\nprint('Average predictions is \\n\\n{}'.format(avg_pred_without_concat))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-20T09:51:38.31606Z","iopub.execute_input":"2021-06-20T09:51:38.316441Z","iopub.status.idle":"2021-06-20T09:51:38.352353Z","shell.execute_reply.started":"2021-06-20T09:51:38.31641Z","shell.execute_reply":"2021-06-20T09:51:38.351115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting to matrix in latex form\n\nprint((bmatrix(fold_1_preds)+'\\n'))\nprint((bmatrix(fold_2_preds)+'\\n'))\nprint((bmatrix(fold_3_preds)+'\\n'))\nprint((bmatrix(fold_4_preds)+'\\n'))\nprint((bmatrix(fold_5_preds)+'\\n'))\n\nfive_folds_add = (fold_1_preds+fold_2_preds+fold_3_preds+fold_4_preds+fold_5_preds)\n# five_folds_add\nprint((bmatrix(five_folds_add)+'\\n'))\n\nfive_folds_add_avg = (fold_1_preds+fold_2_preds+fold_3_preds+fold_4_preds+fold_5_preds)/5\nfive_folds_add_avg\n# print((bmatrix(five_folds_add_avg)+'\\n'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-20T09:51:42.317954Z","iopub.execute_input":"2021-06-20T09:51:42.318375Z","iopub.status.idle":"2021-06-20T09:51:42.337244Z","shell.execute_reply.started":"2021-06-20T09:51:42.318343Z","shell.execute_reply":"2021-06-20T09:51:42.335657Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"### Step By Step\n\nWe create the `inference_by_fold` function; a step by step explanation based on my own test set is as follows:\n\n#### Qn 1: Why do we call `model.to(device)` in PyTorch?\n\n- We equip the `model` to `device`, telling PyTorch whether we are using GPU/CPU;\n\n#### Qn 2: Why do we set model to `eval()` mode in PyTorch during inference/validation?\n\n- Subsequently, set `model` to `eval()` mode as we are in inference phase; This is extremely important because if you DO NOT set it `eval` mode, then your model predictions during inference will be off. Why? Well I can spend the whole day explaining, but the simple idea is in your model, there are many **regularization** methods like `nn.Dropout()` or common `nn.BatchNorm1d(2d)` layers. Then during inference, if your model mode is `train`, then your predictions will experience DROPOUT as well. This will lead to different, and possibly worse predictions every time you inference.\n\n> What eval mode does to BatchNorm: During training, this layer keeps a running estimate of its computed mean and variance. The running sum is kept with a default momentum of 0.1. During evaluation, this running mean/variance is used for normalization.\n\n#### Qn 3: Why do we `@torch.no_grad` in PyTorch? \n\n- [Answer](https://stackoverflow.com/questions/63351268/torch-no-grad-affects-on-model-accuracy): Since we are in inference phase, we say `torch.no_grad()` because we are no longer computing or storing gradients anymore.","metadata":{}},{"cell_type":"markdown","source":"With the above questions cleared, we can go back to our function.\n\n1. We initiate with an empty array `all_folds_preds=[]` where the final expectation of this array should contain \n2. Initiate with an empty list: `, where the final expectation of this `list` should contain 5 `numpy array` with each `array` having a shape of `3 by 5`, or `n by 5` where `n` represents the total number of test images. Alternatively, we can also do what we did in the previous section, instead of simply `appending`, we can simply add each fold's predictions, and divide by 5 later. (We can explore this, but for now we stick to the below code).\n\n3. Here is where we start looping through each fold's model's and make predictions; since there are 5 folds, this implies we have 5 models, and `states_dicts` is a `list` holding 5 `dictionaries (information of each model's weights)` of each model. The subsequent `if-else` clause can be ignored since in future, I will standardize the way I save model to be strictly the latter: `model.load_state_dict(state['model_state_dict'])`\n\n        for fold_num, state in enumerate(states_dicts):\n            if 'model_state_dict' not in state:\n                model.load_state_dict(state)\n            else:\n                model.load_state_dict(state['model_state_dict'])\n                \n4. Initiate with another empty list: `current_fold_preds = []` where the end result of it is all the predictions of fold $i$ contained in this list in the following format:\n\n        [array([[0.01, 0.02, 0.02, 0.93, 0.02],\n                [0.03, 0.76, 0.06, 0.06, 0.09]]),\n                \n         array([[0.02, 0.83, 0.01, 0.07, 0.07]])]\n         \n    Note that the `list` only contains 2 entries, where the first entry is an array of shape (2,5), and the second entry is an array of shape (1,5). This is because there are only 3 test images, and hence 3 predictions. Furthermore, the reason of them being split up is because we set the `batch_size` to be 2, so when you iterate through the `DataLoader`, the first for loop will append the first 2 images' predictions, and the second loop will append the remaining 1 image's predictions.\n    \n    \n5. This part should be relatively easy to understand. In essence, we loop through the `DataLoader/TestLoader` and predict for each batch of images by calling `model(images)`, which will in turn return you **logits** because that is how we defined it in our **Model**. Subsequently, we convert the **logits** into **softmax predictions**. The naming might be not suggestive enough, but both **logits and softmax_preds** are **tensor array and numpy array respectively**.\n\n            for data in tqdm(test_loader, position=0, leave=True):\n                img_ids, images, labels = data\n                images = images.to(device)\n                \n                logits = model(images)\n                softmax_preds = torch.nn.Softmax(dim=1)(input=logits).to('cpu').numpy()\n                #print('softmax predictions for fold {} is {}'.format(fold_num+1,softmax_preds))\n                # do not use argmax here as we still need these softmax probabilities for averaging.\n                current_fold_preds.append(softmax_preds)\n                #print(current_fold_preds)\n                \n 6. After finishing each inner loop, we then `concatenate` the `current_fold_preds` and `append` it to `all_folds_preds`. The reason for `concatenate` is to convert the `list` into `array` as follows: You can compare and contrast this with point 4.\n\n \n            current_fold_preds = np.concatenate(current_fold_preds, axis = 0)\n            all_folds_preds.append(current_fold_preds)\n            \n            \n             array([[0.01, 0.02, 0.02, 0.93, 0.02],\n                    [0.03, 0.76, 0.06, 0.06, 0.09],\n                    [0.02, 0.83, 0.01, 0.07, 0.07]])\n                    \n                    \n7. At this junction, we are edging towards the end. What we have described above can be summarized compactly as follows: \n\n    Outer Loop: We loop through each model's (5 folds = 5 models)  `states_dicts` and for each fold/model,\n\n    Inner Loop: We loop through the `DataLoader` to predict all the images in the test set.\n    \n    Final results: `all_folds_preds = [fold_1_preds, fold_2_preds, fold_3_preds, fold_4_preds, fold_5_preds]`\n    \n    Finally, we average the `all_folds_preds` using `avg_preds = np.mean(all_folds_preds, axis=0)` to get our averaged predictions: \n    \n        [[0.024  0.026  0.034  0.892  0.024 ]\n         [0.032  0.824  0.03   0.044  0.07  ]\n         [0.017  0.82   0.0281 0.0571 0.0778]]\n         \n    Which is similar to our matrix:\n    \n    $$\\text{Dividing/Averaging all 5 Fold's Predictions}\\begin{bmatrix}\n  0.024 & 0.026 & 0.034 & 0.892 & 0.024\\\\\n  0.032 & 0.824 & 0.03 & 0.044 & 0.07\\\\\n  0.017 & 0.82 & 0.0281 & 0.0571 & 0.0778\\\\\n\\end{bmatrix}$$\n\n\n**Final note**\n\nIf you only pass in one fold/model, this inference function will still work.","metadata":{}},{"cell_type":"markdown","source":"**Numpy Shape Rows vs Columns**\n\n\nThere are 3 images in the test set, our final predictions for all 3 images across all 5 folds are presented below, with `shape` to be `[3,5]` which means 3 rows and 5 columns, where $\\text{row}_i$ represents (1 by 5) vector containing 5 predictions in probabilities of how likely each class is; to put it even more explicit, if fold 1's prediction on the first image (call it $\\text{image}_1$) is $[0.01, 0.02, 0.03, 0.9 , 0.04]$, then it means $\\text{image}_1$ being class 1 is $1\\%$, class 2 is $2\\%$, class 3 is $3\\%$, class 4 is $90\\%$ and class 5, $4\\%$.\n\n\nOne with Linear Algebra background can envision a (3 by 5) 2-dimensional array akin to a (3 x 5) **Matrix**.\n\nSo we are clear, the 5 matrices below represents the predictions of each fold. \n\n\n$$\\text{Fold 1 Predictions}=\\begin{bmatrix}\n  0.01 & 0.02 & 0.03 & 0.9 & 0.04\\\\\n  0.02 & 0.8 & 0.05 & 0.06 & 0.07\\\\\n  0.01 & 0.8 & 0.03 & 0.09 & 0.07\\\\\n\\end{bmatrix}$$\n\n\n$$\\text{Fold 2 Predictions}\\begin{bmatrix}\n  0.03 & 0.05 & 0.01 & 0.88 & 0.03\\\\\n  0.01 & 0.82 & 0.02 & 0.07 & 0.08\\\\\n  0.005 & 0.81 & 0.0205 & 0.0555 & 0.109\\\\\n\\end{bmatrix}$$\n\n\n$$\\text{Fold 3 Predictions}=\\begin{bmatrix}\n  0.05 & 0.03 & 0.08 & 0.83 & 0.01\\\\\n  0.05 & 0.89 & 0.01 & 0.02 & 0.03\\\\\n  0.03 & 0.78 & 0.05 & 0.02 & 0.12\\\\\n\\end{bmatrix}$$\n\n$$\\text{Fold 4 Predictions}=\\begin{bmatrix}\n  0.02 & 0.01 & 0.03 & 0.92 & 0.02\\\\\n  0.05 & 0.85 & 0.01 & 0.01 & 0.08\\\\\n  0.02 & 0.88 & 0.03 & 0.05 & 0.02\\\\\n\\end{bmatrix}$$\n\n$$\\text{Fold 5 Predictions}\\begin{bmatrix}\n  0.01 & 0.02 & 0.02 & 0.93 & 0.02\\\\\n  0.03 & 0.76 & 0.06 & 0.06 & 0.09\\\\\n  0.02 & 0.83 & 0.01 & 0.07 & 0.07\\\\\n\\end{bmatrix}$$\n\n\n\nAll we are left to do is to add these 5 matrices, and divide by 5, as follows:\n\n$$\\text{Adding all 5 Fold's Predictions}\\begin{bmatrix}\n  0.12 & 0.13 & 0.17 & 4.46 & 0.12\\\\\n  0.16 & 4.12 & 0.15 & 0.22 & 0.35\\\\\n  0.085 & 4.1 & 0.1405 & 0.2855 & 0.389\\\\\n\\end{bmatrix}$$\n\n\nDividing by 5 (averaging):\n\n\n$$\\text{Dividing/Averaging all 5 Fold's Predictions}\\begin{bmatrix}\n  0.024 & 0.026 & 0.034 & 0.892 & 0.024\\\\\n  0.032 & 0.824 & 0.03 & 0.044 & 0.07\\\\\n  0.017 & 0.82 & 0.0281 & 0.0571 & 0.0778\\\\\n\\end{bmatrix}$$","metadata":{}},{"cell_type":"markdown","source":"# Inference Function","metadata":{}},{"cell_type":"code","source":"def inference_by_fold(model, states_dicts, test_loader, device):\n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        all_folds_preds = []\n        for fold_num, state in enumerate(states_dicts):\n            if 'model_state_dict' not in state:\n                model.load_state_dict(state)\n            else:\n                model.load_state_dict(state['model_state_dict'])\n                \n            current_fold_preds = []\n            for data in tqdm(test_loader, position=0, leave=True):\n                img_ids, images, labels = data\n                images = images.to(device)\n                \n                logits = model(images)\n                softmax_preds = torch.nn.Softmax(dim=1)(input=logits).to('cpu').numpy()\n                # print('softmax predictions for fold {} is {}'.format(fold_num+1,softmax_preds))\n                # do not use argmax here as we still need these softmax probabilities for averaging.\n                current_fold_preds.append(softmax_preds)\n                #print(current_fold_preds)\n            \n            current_fold_preds = np.concatenate(current_fold_preds, axis = 0)\n            all_folds_preds.append(current_fold_preds)\n            \n\n        avg_preds = np.mean(all_folds_preds, axis=0)\n        #print(\"Averaging all 5 folds of softmax predictions\", avg_preds)\n\n    \n    return avg_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create the `inference_by_fold` function; a step by step explanation based on my own test set is as follows:\n\n1. We equip the `model` to `device`, telling PyTorch whether we using GPU/CPU; subsequently, set `model` to `eval()` mode as we are in inference phase; then since we are inference phase, we say `torch.no_grad()` because we are no longer computing or storing gradients anymore.\n\n        model.to(device)\n        model.eval()\n        with torch.no_grad():\n        \n2. Initiate with an empty list: `all_folds_preds=[]`, where the final expectation of this `list` should contain 5 `numpy array` with each `array` having a shape of `3 by 5`, or `n by 5` where `n` represents the total number of test images. Alternatively, we can also do what we did in the previous section, instead of simply `appending`, we can simply add each fold's predictions, and divide by 5 later. (We can explore this, but for now we stick to the below code).\n\n3. Here is where we start looping through each fold's model's and make predictions; since there are 5 folds, this implies we have 5 models, and `states_dicts` is a `list` holding 5 `dictionaries (information of each model's weights)` of each model. The subsequent `if-else` clause can be ignored since in future, I will standardize the way I save model to be strictly the latter: `model.load_state_dict(state['model_state_dict'])`\n\n        for fold_num, state in enumerate(states_dicts):\n            if 'model_state_dict' not in state:\n                model.load_state_dict(state)\n            else:\n                model.load_state_dict(state['model_state_dict'])\n                \n4. Initiate with another empty list: `current_fold_preds = []` where the end result of it is all the predictions of fold $i$ contained in this list in the following format:\n\n        [array([[0.01, 0.02, 0.02, 0.93, 0.02],\n                [0.03, 0.76, 0.06, 0.06, 0.09]]),\n                \n         array([[0.02, 0.83, 0.01, 0.07, 0.07]])]\n         \n    Note that the `list` only contains 2 entries, where the first entry is an array of shape (2,5), and the second entry is an array of shape (1,5). This is because there are only 3 test images, and hence 3 predictions. Furthermore, the reason of them being split up is because we set the `batch_size` to be 2, so when you iterate through the `DataLoader`, the first for loop will append the first 2 images' predictions, and the second loop will append the remaining 1 image's predictions.\n    \n    \n5. This part should be relatively easy to understand. In essence, we loop through the `DataLoader/TestLoader` and predict for each batch of images by calling `model(images)`, which will in turn return you **logits** because that is how we defined it in our **Model**. Subsequently, we convert the **logits** into **softmax predictions**. The naming might be not suggestive enough, but both **logits and softmax_preds** are **tensor array and numpy array respectively**.\n\n            for data in tqdm(test_loader, position=0, leave=True):\n                img_ids, images, labels = data\n                images = images.to(device)\n                \n                logits = model(images)\n                softmax_preds = torch.nn.Softmax(dim=1)(input=logits).to('cpu').numpy()\n                #print('softmax predictions for fold {} is {}'.format(fold_num+1,softmax_preds))\n                # do not use argmax here as we still need these softmax probabilities for averaging.\n                current_fold_preds.append(softmax_preds)\n                #print(current_fold_preds)\n                \n 6. After finishing each inner loop, we then `concatenate` the `current_fold_preds` and `append` it to `all_folds_preds`. The reason for `concatenate` is to convert the `list` into `array` as follows: You can compare and contrast this with point 4.\n\n \n            current_fold_preds = np.concatenate(current_fold_preds, axis = 0)\n            all_folds_preds.append(current_fold_preds)\n            \n            \n             array([[0.01, 0.02, 0.02, 0.93, 0.02],\n                    [0.03, 0.76, 0.06, 0.06, 0.09],\n                    [0.02, 0.83, 0.01, 0.07, 0.07]])\n                    \n                    \n7. At this junction, we are edging towards the end. What we have described above can be summarized compactly as follows: \n\n    Outer Loop: We loop through each model's (5 folds = 5 models)  `states_dicts` and for each fold/model,\n\n    Inner Loop: We loop through the `DataLoader` to predict all the images in the test set.\n    \n    Final results: `all_folds_preds = [fold_1_preds, fold_2_preds, fold_3_preds, fold_4_preds, fold_5_preds]`\n    \n    Finally, we average the `all_folds_preds` using `avg_preds = np.mean(all_folds_preds, axis=0)` to get our averaged predictions: \n    \n        [[0.024  0.026  0.034  0.892  0.024 ]\n         [0.032  0.824  0.03   0.044  0.07  ]\n         [0.017  0.82   0.0281 0.0571 0.0778]]\n         \n    Which is similar to our matrix:\n    \n    $$\\text{Dividing/Averaging all 5 Fold's Predictions}\\begin{bmatrix}\n  0.024 & 0.026 & 0.034 & 0.892 & 0.024\\\\\n  0.032 & 0.824 & 0.03 & 0.044 & 0.07\\\\\n  0.017 & 0.82 & 0.0281 & 0.0571 & 0.0778\\\\\n\\end{bmatrix}$$\n\n\n**Final note**\n\nIf you only pass in one fold/model, this inference function will still work.","metadata":{}},{"cell_type":"markdown","source":"# My test images","metadata":{}},{"cell_type":"markdown","source":"It is good practice to write `torch.load(path, map_location)`, in the event that you may inference in CPU.","metadata":{}},{"cell_type":"code","source":"# my_own_test_images = [['11252426.jpg',-1], ['11574961.jpg',-1], ['218377.jpg',-1]] # labels are not impt\n# test_df = pd.DataFrame(my_own_test_images, columns = ['image_id', 'label'])\n\n# # Changing my test path here because I am lazy.\n# config.test_path = '../input/cassava-test-images'\n# states_dicts  = [torch.load(os.path.join(config.weights_dir,'{}_fold_{}_best_val.pt'.format(config.effnet,fold)),map_location=torch.device(config.device)) for fold in config.train_folds_used]\n# own_test_dataset = Cassava(test_df, transforms=get_test_transforms(), test=True)\n# own_test_loader = DataLoader(own_test_dataset, batch_size=2, shuffle=False, num_workers=4)\n\n# mymodel  = CustomEfficientNet(config, pretrained=False)\n# my_predictions = inference_by_fold(mymodel, states_dicts, own_test_loader, config.device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For one fold","metadata":{}},{"cell_type":"code","source":"# sample = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# weights_dir = '../input/cassavahongnan/'\n# model = CustomEfficientNet(config, pretrained=False)\n# states_dicts = [torch.load('../input/cassavahongnan/tf_efficientnet_b3_ns_best_loss_fold_0.pt')]\n# test_dataset = Cassava(sample, transforms=get_test_transforms(), test=True)\n# test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n# predictions = inference(model, states_dicts, test_loader, device)\n# # submission\n# sample['label'] = predictions.argmax(1)\n# sample[['image_id', 'label']].to_csv('submission.csv', index=False)\n# sample.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For Multiple Folds","metadata":{}},{"cell_type":"markdown","source":"Constant Reminder: So we have trained 5 models because we used 5 fold cross validation. For each of the 5 fold models that we have, we will load each fold model's weights and use them to make predictions on the unseen test images. As a result, we have 5 predictions for each test image. To be clear, if you have 1000 unseen test images, named $$i~~~ \\forall i \\in {1,2,...,1000}$$ then for each image $i$, there will be 5 predictions each for it, we can call them as such $$P(i_{j}) ~~~ \\forall j \\in {1,2,3,4,5}$$ where $j$ represents the number of folds. Thus, by convention, we take the **average/mean** of these 5 sets of predictions and take the average value/probability as the final prediction value. This score is then submitted to Kaggle and we get what we called the LB score, which should correlate to your CV/OOF score.","metadata":{}},{"cell_type":"markdown","source":"# Submission Cautions\n\nWhen inferencing, I think do not print out anything to debug if not it might take too much ram/gpu. \n\nAnd last but not least, remember to use [`argmax(1)`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) on the predictions because we want a label. Note very carefully that `argmax(1)` returns the index of the array in which it has the largest value. In other words, if you have an array as follows:\n\n    array([[0.00694168, 0.02380404, 0.19128187, 0.00819152, 0.7697809 ]]\n    \nThen calling `argmax(1)` on it will return you the fifth element, which is of **index 4**, corresponding to our class/label 4 in this competition. However one should be cautious as sometimes, the labels of a competition can be as such: 1, 2, 3, 4, 5 instead of 0, 1, 2, 3, 4. And in this case our prediction's `argmax(1)` will still return 4, which **DOES NOT CORRESPOND** to the correct class (5). In this case, one can create a mapping.","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# inference and submission\n# ====================================================\n\n\n# model = CustomEfficientNet(config, pretrained=False)\n# # change test path back\n# config.test_path = '../input/cassava-leaf-disease-classification/test_images'\n# states_dicts = [torch.load(os.path.join(config.weights_dir,'{}_fold_{}_best_val.pt'.format(config.effnet,fold)),map_location=torch.device(config.device)) for fold in config.train_folds_used]\n# test_dataset = Cassava(sample, transforms=get_test_transforms(), test=True)\n# test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n# predictions = inference_by_fold(model, states_dicts, test_loader, config.device)\n# predictions\n# # submission\n# sample['label'] = predictions.argmax(1)\n# sample[['image_id', 'label']].to_csv('submission.csv', index=False)\n# sample.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"20\"></a>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#3aaa80; border-radius: 100px 100px; text-align:center\">Saving and Loading Models' Weights</h1>","metadata":{}},{"cell_type":"markdown","source":"More often than not, we don't have resources to run the model day and night. Kaggle or Colab disconnects you once it reaches a certain number of hours. This bothers me when I just started out because my model has not **converged** yet.\n\nTherefore, itis important to be able to **checkpoint** or **save** our model, for two main reasons:\n\n**1. Save the model's weights and use it later to inference or make predictions.**\n\n**2. Save the model's weights as a checkpoint and use it later to resume training.**","metadata":{}},{"cell_type":"markdown","source":"I will be using example from both the PyTorch website and a book that I bought. Please find below for references:\n\n1. https://pytorch.org/tutorials/\n\n2. Deep Learning with PyTorch - Step by Step A Beginner's Guide - Daniel Voigt","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#top\">Back to top</a>","metadata":{}},{"cell_type":"markdown","source":"### Question\n\nNow, I am unsure if this has been answered, I remember my buddy telling me that anecdotally, one can \"reset\" the LR a little when one resumes training from a model's checkpoint.\n\nIt is a bit counter-intuitive to me, but say I trained a model for 16 epochs, with a custom scheduler, say OneCycleLr + Adam or something, then when you resume training, should we reset the initial learning rate? I would think resetting is counter-intuitive as the purpose of the learning rate is to tune it such that your model can slowly converge to the minima (global one if the function is convex).\n\nSo the question is:\n\nShould I reset the learning rate when resume training, if yes, reset to what?\n\nIf we should not reset, what is a good way to \"extract\" the last learning rate, as some scheduler depends on factors like epochs…\n\n\nUncle CPMP's advice will add in and credit later.\n","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, scheduler, scaler, epoch, fold, seed, fname=fname):\n    checkpoint = {\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict(),\n        'scaler': scaler.state_dict(),\n        'epoch': epoch,\n        'fold':fold,\n        'seed':seed,\n        }\n    torch.save(checkpoint, '../checkpoints/%s/%s_%d_%d.pt' % (fname, fname, fold, seed))\n\ndef load_checkpoint(fold, seed, fname):\n    model = create_model().to(device)\n    optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=MAX_LR)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, \n                                              pct_start=PCT_START, \n                                              div_factor=DIV_FACTOR \n                                              max_lr=MAX_LR, \n                                              epochs=EPOCHS, \n                                              steps_per_epoch=int(np.ceil(len(train_data_loader)/GRADIENT_ACCUMULATION)))\n    scaler = GradScaler()\n    checkpoint = torch.load('../checkpoints/%s/%s_%d_%d.pt' % (fname, fname, fold, seed))\n    model.load_state_dict(checkpoint['model'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    scheduler.load_state_dict(checkpoint['scheduler'])\n    scaler.load_state_dict(checkpoint['scaler'])\n    return model, optimizer, scheduler, scaler, epoch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n## What is a state_dict? \n\nIn PyTorch, the learnable parameters (i.e. weights and biases) of an\n``torch.nn.Module`` model are contained in the model’s *parameters*\n(accessed with ``model.parameters()``). A *state_dict* is simply a\nPython dictionary object that maps each layer to its parameter tensor.\nNote that only layers with learnable parameters (convolutional layers,\nlinear layers, etc.) and registered buffers (batchnorm's running_mean)\nhave entries in the model’s *state_dict*. Optimizer\nobjects (``torch.optim``) also have a *state_dict*, which contains\ninformation about the optimizer’s state, as well as the hyperparameters\nused.\n\nBecause *state_dict* objects are Python dictionaries, they can be easily\nsaved, updated, altered, and restored, adding a great deal of modularity\nto PyTorch models and optimizers.","metadata":{}},{"cell_type":"markdown","source":"## Simple Example\n\n`in_channels` is the number of channels of the input to the convolutional layer. So, for example, in the case of the convolutional layer that applies to the image, `in_channels` refers to the number of channels of the image. In the case of an RGB image, `in_channels == 3` (red, green and blue); in the case of a gray image, `in_channels == 1`.\n\n`out_channels` is the number of feature maps, which is often equivalent to the number of kernels that you apply to the input. See [here](https://stats.stackexchange.com/a/292064/82135) for more info. \n\n`kernel_size` is just the size of the kernel, usually `3x3` or `5x5`.","metadata":{}},{"cell_type":"markdown","source":"Convolutional Layer : Consider a convolutional layer which takes “l” feature maps as the input and has “k” feature maps as output. The filter size is $n*m$.\n\nHere the input has l=32 feature maps as inputs, k=64 feature maps as outputs and filter size is n=3 and m=3. It is important to understand, that we don’t simply have a $3*3$ filter, but actually, we have $3*3*32$ filter, as our input has 32 dimensions. And as an output from first conv layer, we learn 64 different $3*3*32$ filters which total weights is $$n*m*k*l$$ Then there is a term called bias for each feature map. So, the total number of parameters are $$(n*m*l+1)*k$$\n\nThink of the convolutional layer as a $nxn$ matrix, and $nxn = n^2$ is the weights (a.k.a what values to take when you apply the convolution.","metadata":{}},{"cell_type":"code","source":"# Define model\nclass TheModelClass(nn.Module):\n    def __init__(self):\n        super(TheModelClass, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels =6, kernel_size =5)\n        self.pool = nn.MaxPool2d(2, 2)\n        # here in channels = 6 because out channels of previous is 6.\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Initialize model\nmodel = TheModelClass()\n\n# Initialize optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n\n# Print optimizer's state_dict\nprint(\"Optimizer's state_dict:\")\nfor var_name in optimizer.state_dict():\n    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Link to print parameters/weights for each layer](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model)\n\n[Calculation of parameters in CNN](https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca)\n\n[LeNet Architecture](https://engmrk.com/lenet-5-a-classic-cnn-architecture/#:~:text=The%20LeNet%2D5%20architecture%20consists,and%20finally%20a%20softmax%20classifier.)","metadata":{}},{"cell_type":"code","source":"from prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        table.add_row([name, param])\n        total_params+=param\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\n    \ncount_parameters(model)\n\n# indeed, 5x5x3x6 = 450, note the input here is 3 channels.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading and Saving\n\n**Save**\n\nIt is not a secret that one should use `torch.save` to save a model's weights in the following manner:\n\n`torch.save(model.state_dict(), PATH)`\n\n\n\n**Load**\n\nAnd similarly, one should use `torch.load` to load a model's weights.\n\n    model = TheModelClass(*args, **kwargs)\n    model.load_state_dict(torch.load(PATH))\n    model.eval()\n \n \n**Advanced Save and Load**\n\nOne can also make what you save more sophisticated:\n\n    torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': loss,\n                ...\n                }, PATH)\n                \n                \nAnd to load them, we do it as follows:\n\n    model = TheModelClass(*args, **kwargs)\n    optimizer = TheOptimizerClass(*args, **kwargs)\n\n    checkpoint = torch.load(PATH)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    loss = checkpoint['loss']\n\n    model.eval()\n    # - or -\n    model.train()\n\n**Caution**\n\nNotice that the `load_state_dict()` function takes a dictionary object, **NOT a path to a saved object**. This means that you must deserialize the saved `state_dict` before you pass it to the `load_state_dict()` function. For example, you **CANNOT** load using `model.load_state_dict(PATH)`.","metadata":{}},{"cell_type":"markdown","source":"## **Loading and Saving on GPU**\n\n\n**Save:**\n\n      torch.save(model.state_dict(), PATH)\n\n**Load:**\n\n\n\n       device = torch.device(\"cuda\")\n       model = TheModelClass(*args, **kwargs)\n       model.load_state_dict(torch.load(PATH))\n       model.to(device)\n \nWhen loading a model on a GPU that was trained and saved on GPU, simply\nconvert the initialized ``model`` to a CUDA optimized model using\n``model.to(torch.device('cuda'))``. Also, be sure to use the\n``.to(torch.device('cuda'))`` function on all model inputs to prepare\nthe data for the model. Note that calling ``my_tensor.to(device)``\nreturns a new copy of ``my_tensor`` on GPU. It does NOT overwrite\n``my_tensor``. Therefore, remember to manually overwrite tensors:\n``my_tensor = my_tensor.to(torch.device('cuda'))``. Make sure to call `input = input.to(device)` on any input tensors that you feed to the model, as you will see later.   ","metadata":{}},{"cell_type":"markdown","source":"# Using it on a real example","metadata":{}},{"cell_type":"markdown","source":"**Reminder on what is a state_dict**\n\nIn PyTorch, the learnable parameters (i.e. weights and biases) of an\n``torch.nn.Module`` model are contained in the model’s *parameters*\n(accessed with ``model.parameters()``). A *state_dict* is simply a\nPython dictionary object that maps each layer to its parameter tensor.\nNote that only layers with learnable parameters (convolutional layers,\nlinear layers, etc.) and registered buffers (batchnorm's running_mean)\nhave entries in the model’s *state_dict*. Optimizer\nobjects (``torch.optim``) also have a *state_dict*, which contains\ninformation about the optimizer’s state, as well as the hyperparameters\nused.","metadata":{"trusted":true}},{"cell_type":"code","source":"# # First, we define the model, since we will be loading our own \"pretrained weights\", there is no need\n# # for one to set pretrained = True here, moreover, this competition is a No-Internet competition.\n\n# effnet_model = CustomEfficientNet(config, pretrained=False)\n\n\n# # The below code displays the parameter name, along with its size and tensors. \n# # Since the model is set to pretrained = False, the weights are initialized randomly (Xavier or something)\n# # However, if you set pretrained=True, the weights are fixed because it is already pretrained.\n\n# def count_parameters(model):\n#     table = PrettyTable([\"Modules\", \"Size of Modules\", \"Number of Parameters\"])\n#     total_params = 0\n#     for param_name in model.state_dict():\n#         param_tensor_size = model.state_dict()[param_name].size()\n#         num_of_params = model.state_dict()[param_name].numel()\n#         table.add_row([param_name, param_tensor_size, num_of_params])\n#         total_params+=num_of_params\n#     print(table)\n#     print(f\"Total Trainable Params: {total_params}\")\n#     return total_params\n    \n# count_parameters(effnet_model)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Secondly, we use torch.load to load the pretrained weights that we trained on. Make sure the weights were\n# trained from the same EffficientNet, there was a time that I took a EfficientNetB3 weight and used it on\n# EfficientNetB0 model. It will not prompt you an error and disaster ensues.\n\n# state_dict_fold_1 = torch.load('../input/flowers/tf_efficientnet_b5_ns_fold_1_best_val.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CHECK EACH LAYER MATCHES IN SIZE.","metadata":{}},{"cell_type":"code","source":"# # Thirdly, state_dict_fold_1 is an OrderedDict, before we proceed, it is very important to call\n# # load_state_dict on the model! Because if not, we will get the same results as the previous \n# # count_parameters(effnet_model) when the model has yet to be trained!\n\n# from collections import OrderedDict\n\n# effnet_model.load_state_dict(state_dict_fold_1)\n# # note that after you load the weights using load_state_dict, the effnet_model.state_dict changes!\n\n# # Note that state_dict_fold_1 and effnet_model.state_dict() are equal!\n# # Check the keys of both dict matches\n# assert len(state_dict_fold_1) == len(effnet_model.state_dict())\n\n# count = 0\n# for param_name_1,param_name_2 in zip(state_dict_fold_1.keys(), effnet_model.state_dict().keys()):\n#     if param_name_1 == param_name_2:\n#         count+=1\n# assert count==len(state_dict_fold_1) == len(effnet_model.state_dict())\n\n# # ok the above code shows both have same param names, now to check if each tensor match:\n\n# tensor_match_count = 0\n# for param_name in state_dict_fold_1.keys() & effnet_model.state_dict().keys():\n#     state_dict_fold_1_tensor = state_dict_fold_1[param_name].to(config.device)\n#     effnet_model_state_dict = effnet_model.state_dict()[param_name].to(config.device)\n#     if torch.all(torch.eq(state_dict_fold_1_tensor, effnet_model_state_dict)):\n#         tensor_match_count+=1\n\n# assert tensor_match_count==count==len(state_dict_fold_1) == len(effnet_model.state_dict())\n# # so indeed all tensors matched.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[References](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175614)\n\n[Understanding TTA](https://stepup.ai/test_time_data_augmentation/)","metadata":{}}]}