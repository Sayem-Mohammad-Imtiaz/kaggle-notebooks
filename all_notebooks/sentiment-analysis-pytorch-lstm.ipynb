{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis using LSTM","metadata":{}},{"cell_type":"markdown","source":"## What is Sentiment Analysis:\nthe process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n\nI think this result from google dictionary gives a very succinct definition. I don’t have to re-emphasize how important sentiment analysis has become. So, here we will build a classifier on IMDB movie dataset using a Deep Learning technique called RNN.\n\nI’m outlining a step-by-step process for how Recurrent Neural Networks (RNN) can be implemented using Long Short Term Memory (LSTM) architecture:\n\n1. Load in and visualize the data\n2. Data Processing — convert to lower case, Remove punctuation etc.\n5. Tokenize — Create Vocab to Int mapping dictionary\n6. Tokenize — Encode the words\n7. Tokenize — Encode the labels\n8. Analyze Reviews Length\n9. Removing Outliers — Getting rid of extremely long or short reviews\n10. Padding / Truncating the remaining data\n11. Training, Validation, Test Dataset Split\n12. Dataloaders and Batching\n13. Define the LSTM Network Architecture\n14. Define the Model Class\n15. Training the Network\n16. Testing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport string\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:49:55.817032Z","iopub.execute_input":"2021-07-23T09:49:55.817371Z","iopub.status.idle":"2021-07-23T09:49:55.824325Z","shell.execute_reply.started":"2021-07-23T09:49:55.817337Z","shell.execute_reply":"2021-07-23T09:49:55.823406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1) Load in and visualize the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:49:55.825818Z","iopub.execute_input":"2021-07-23T09:49:55.826467Z","iopub.status.idle":"2021-07-23T09:49:56.417802Z","shell.execute_reply.started":"2021-07-23T09:49:55.826424Z","shell.execute_reply":"2021-07-23T09:49:56.416578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2) Data Processing — convert to lower case, Remove punctuation etc","metadata":{}},{"cell_type":"code","source":"def data_preprocessing(text):\n    text = text.lower()\n    text = re.sub('<.*?>', '', text) # Remove HTML from text\n    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n    text = [word for word in text.split() if word not in stop_words]\n    text = ' '.join(text)\n    return text\n\ndf['cleaned_reviews'] = df['review'].apply(data_preprocessing)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:49:56.420192Z","iopub.execute_input":"2021-07-23T09:49:56.420618Z","iopub.status.idle":"2021-07-23T09:50:06.611789Z","shell.execute_reply.started":"2021-07-23T09:49:56.420577Z","shell.execute_reply":"2021-07-23T09:50:06.610999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5) Tokenize — Create Vocab to Int mapping dictionary\nIn most of the NLP tasks, you will create an index mapping dictionary in such a way that your frequently occurring words are assigned lower indexes. One of the most common way of doing this is to use Counter method from Collections library.","metadata":{}},{"cell_type":"code","source":"corpus = [word for text in df['cleaned_reviews'] for word in text.split()]\ncount_words = Counter(corpus)\nsorted_words = count_words.most_common()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:06.613419Z","iopub.execute_input":"2021-07-23T09:50:06.613812Z","iopub.status.idle":"2021-07-23T09:50:08.592694Z","shell.execute_reply.started":"2021-07-23T09:50:06.613771Z","shell.execute_reply":"2021-07-23T09:50:08.59159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keys = []\nvalues = []\nfor key, value in sorted_words[:20]:\n    keys.append(key)\n    values.append(value)\n    \nplt.figure(figsize=(12, 5))\nplt.bar(keys, values)\nplt.title('Top 20 most common words', size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:08.594242Z","iopub.execute_input":"2021-07-23T09:50:08.594854Z","iopub.status.idle":"2021-07-23T09:50:08.829811Z","shell.execute_reply.started":"2021-07-23T09:50:08.594811Z","shell.execute_reply":"2021-07-23T09:50:08.828867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a small trick here, in this mapping index will start from 0 i.e. mapping of ‘the’ will be 0. But later on we are going to do padding for shorter reviews and conventional choice for padding is 0. So we need to start this indexing from 1","metadata":{}},{"cell_type":"code","source":"vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n\nreviews_int = []\nfor text in df['cleaned_reviews']:\n    r = [vocab_to_int[word] for word in text.split()]\n    reviews_int.append(r)\n\nprint(reviews_int[:1])\ndf['Review int'] = reviews_int","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:08.83114Z","iopub.execute_input":"2021-07-23T09:50:08.831511Z","iopub.status.idle":"2021-07-23T09:50:10.804045Z","shell.execute_reply.started":"2021-07-23T09:50:08.831473Z","shell.execute_reply":"2021-07-23T09:50:10.803085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7) Tokenize — Encode the labels\nThis is simple because we only have 2 output labels. So, we will just label ‘positive’ as 1 and ‘negative’ as 0","metadata":{}},{"cell_type":"code","source":"df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:10.805432Z","iopub.execute_input":"2021-07-23T09:50:10.805794Z","iopub.status.idle":"2021-07-23T09:50:10.85823Z","shell.execute_reply.started":"2021-07-23T09:50:10.805745Z","shell.execute_reply":"2021-07-23T09:50:10.8573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8) Analyze Reviews Length","metadata":{}},{"cell_type":"code","source":"review_len = [len(x) for x in reviews_int]\ndf['Review len'] = review_len\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:10.859519Z","iopub.execute_input":"2021-07-23T09:50:10.85989Z","iopub.status.idle":"2021-07-23T09:50:10.904461Z","shell.execute_reply.started":"2021-07-23T09:50:10.859851Z","shell.execute_reply":"2021-07-23T09:50:10.903189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['Review len'].describe())\n\ndf['Review len'].hist()\nplt.title('Review length distribution', size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:10.907684Z","iopub.execute_input":"2021-07-23T09:50:10.908291Z","iopub.status.idle":"2021-07-23T09:50:11.096388Z","shell.execute_reply.started":"2021-07-23T09:50:10.908235Z","shell.execute_reply":"2021-07-23T09:50:11.095421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Observations : </b>a) Mean review length = 226 b) Most of the reviews less than 500 words or more d) There are quite a few reviews that are extremely long, we can manually investigate them to check whether we need to include or exclude them from our analysis","metadata":{}},{"cell_type":"markdown","source":"## 10) Padding / Truncating the remaining data\nTo deal with both short and long reviews, we will pad or truncate all our reviews to a specific length. We define this length by Sequence Length. This sequence length is same as number of time steps for LSTM layer.\n\nFor reviews shorter than seq_length, we will pad with 0s. For reviews longer than seq_length we will truncate them to the first seq_length words.","metadata":{}},{"cell_type":"code","source":"def Padding(review_int, seq_len):\n    '''\n    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n    '''\n    features = np.zeros((len(reviews_int), seq_len), dtype = int)\n    for i, review in enumerate(review_int):\n        if len(review) <= seq_len:\n            zeros = list(np.zeros(seq_len - len(review)))\n            new = zeros + review\n        else:\n            new = review[: seq_len]\n        features[i, :] = np.array(new)\n            \n    return features","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:11.098132Z","iopub.execute_input":"2021-07-23T09:50:11.098509Z","iopub.status.idle":"2021-07-23T09:50:11.105419Z","shell.execute_reply.started":"2021-07-23T09:50:11.098471Z","shell.execute_reply":"2021-07-23T09:50:11.104344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = Padding(reviews_int, 200)\nprint(features[0, :])","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:11.106654Z","iopub.execute_input":"2021-07-23T09:50:11.1072Z","iopub.status.idle":"2021-07-23T09:50:14.78213Z","shell.execute_reply.started":"2021-07-23T09:50:11.107162Z","shell.execute_reply":"2021-07-23T09:50:14.780654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11) Training, Validation, Test Dataset Split\nOnce we have got our data in nice shape, we will split it into training, validation and test sets\n\n<b>train= 80% | valid = 10% | test = 10% </b>","metadata":{}},{"cell_type":"code","source":"X_train, X_remain, y_train, y_remain = train_test_split(features, df['sentiment'].to_numpy(), test_size=0.2, random_state=1)\nX_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:14.783446Z","iopub.execute_input":"2021-07-23T09:50:14.783813Z","iopub.status.idle":"2021-07-23T09:50:14.830199Z","shell.execute_reply.started":"2021-07-23T09:50:14.78377Z","shell.execute_reply":"2021-07-23T09:50:14.829343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12) Dataloaders and Batching\nAfter creating our training, test and validation data. Next step is to create dataloaders for this data. We can use generator function for batching our data into batches instead we will use a TensorDataset. This is one of a very useful utility in PyTorch for using our data with DataLoaders with exact same ease as of torchvision datasets","metadata":{}},{"cell_type":"code","source":"# create tensor dataset\ntrain_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\ntest_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\nvalid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n\n# dataloaders\nbatch_size = 50\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:14.831536Z","iopub.execute_input":"2021-07-23T09:50:14.831916Z","iopub.status.idle":"2021-07-23T09:50:14.838818Z","shell.execute_reply.started":"2021-07-23T09:50:14.83188Z","shell.execute_reply":"2021-07-23T09:50:14.837544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:14.840373Z","iopub.execute_input":"2021-07-23T09:50:14.841009Z","iopub.status.idle":"2021-07-23T09:50:14.856323Z","shell.execute_reply.started":"2021-07-23T09:50:14.840911Z","shell.execute_reply":"2021-07-23T09:50:14.855433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13) Define the LSTM Network Architecture\n\n<img src='https://miro.medium.com/max/700/1*SICYykT7ybua1gVJDNlajw.png'>\n\nThe layers are as follows:\n\n0. Tokenize : This is not a layer for LSTM network but a mandatory step of converting our words into tokens (integers)\n1. Embedding Layer: that converts our word tokens (integers) into embedding of specific size\n2. LSTM Layer: defined by hidden state dims and number of layers\n3. Fully Connected Layer: that maps output of LSTM layer to a desired output size\n4. Sigmoid Activation Layer: that turns all output values in a value between 0 and 1\n5. Output: Sigmoid output from the last timestep is considered as the final output of this network\n","metadata":{}},{"cell_type":"markdown","source":"## 14) Define the Model Class","metadata":{}},{"cell_type":"code","source":"class sentimentLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n    \n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super().__init__()\n        \n        self.output_size = output_size\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        \n        # Embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # Linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n        \n        #embedding and lstm_out\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        #stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # Dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        #sigmoid function\n        sig_out = self.sigmoid(out)\n        \n        # reshape to be batch size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        h0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n        hidden = (h0,c0)\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:14.857547Z","iopub.execute_input":"2021-07-23T09:50:14.857893Z","iopub.status.idle":"2021-07-23T09:50:14.86926Z","shell.execute_reply.started":"2021-07-23T09:50:14.857859Z","shell.execute_reply":"2021-07-23T09:50:14.868063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")\n\n# Instantiate the model w/ hyperparams\nvocab_size = len(vocab_to_int) + 1\noutput_size = 1\nembedding_dim = 64\nhidden_dim = 256\nn_layers = 2\n\nmodel = sentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nmodel = model.to(device)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:14.870634Z","iopub.execute_input":"2021-07-23T09:50:14.870996Z","iopub.status.idle":"2021-07-23T09:50:15.06743Z","shell.execute_reply.started":"2021-07-23T09:50:14.87096Z","shell.execute_reply":"2021-07-23T09:50:15.066435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"lr=0.001\n\ncriterion = nn.BCELoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred == label.squeeze()).item()\n\nclip = 5\nepochs = 2\nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device), labels.to(device)   \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        \n        model.zero_grad()\n        output,h = model(inputs,h)\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        # calculating accuracy\n        accuracy = acc(output,labels)\n        train_acc += accuracy\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n \n    \n        \n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader:\n        val_h = tuple([each.data for each in val_h])\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        output, val_h = model(inputs, val_h)\n        val_loss = criterion(output.squeeze(), labels.float())\n\n        val_losses.append(val_loss.item())\n            \n        accuracy = acc(output,labels)\n        val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc/len(train_loader.dataset)\n    epoch_val_acc = val_acc/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '../working/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:50:15.068733Z","iopub.execute_input":"2021-07-23T09:50:15.069227Z","iopub.status.idle":"2021-07-23T09:51:23.955693Z","shell.execute_reply.started":"2021-07-23T09:50:15.069187Z","shell.execute_reply":"2021-07-23T09:51:23.954257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 16) Testing","metadata":{}},{"cell_type":"code","source":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\ntest_h = model.init_hidden(batch_size)\n\nmodel.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    test_h = tuple([each.data for each in test_h])\n\n    inputs, labels = inputs.to(device), labels.to(device)\n    \n    output, test_h = model(inputs, test_h)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:51:23.957014Z","iopub.execute_input":"2021-07-23T09:51:23.957381Z","iopub.status.idle":"2021-07-23T09:51:25.085251Z","shell.execute_reply.started":"2021-07-23T09:51:23.95734Z","shell.execute_reply":"2021-07-23T09:51:25.084374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank you ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}