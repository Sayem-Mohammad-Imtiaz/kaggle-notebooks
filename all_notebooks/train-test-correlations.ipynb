{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2760fa2c-b321-1681-279d-958e3440b7c1"},"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"fff3d7bb-2642-22fa-6847-d0ca85481548"},"source":"**This report aims at applying descriptive statistics and Machine learning techniques to find trends in Churn data**\n\n**Meaning of Churn** -Churn rate (sometimes called attrition rate), in its broadest sense, is a measure of the number of individuals or items moving out of a collective group over a specific period. It is one of two primary factors that determine the steady-state level of customers a business will support.\n\n**What I have implemented:**\n\n 1. Found best correlations\n 2. Trained and tested the classifier which can predict parameter \"Exited\"(\"Exited\" shows if the customer will leave you or not)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e589c398-3663-8117-7055-363715b99001"},"outputs":[],"source":"import pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import svm \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.naive_bayes import GaussianNB"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26bbf510-13c3-7feb-2c61-db9c9a8dff19"},"outputs":[],"source":"dataframe = pd.read_csv(\"../input/Churn_Modelling.csv\")\ndataframe.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"717ec2fe-f8fe-a5b7-a13a-107285961a93"},"outputs":[],"source":"# The value \"14\" displayes the number of columns\ndataframe.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"146c422d-2956-3c18-f022-603dd224aeec"},"outputs":[],"source":"#You may have noticed that we have got some string fields in dataset\n#In order to make to predictions we should convert this fields to integer\n#But first lets see the unique values of theese fields\nprint(\"Geo unique values\",dataframe['Geography'].unique())\nprint(\"Gender unique values\",dataframe['Gender'].unique())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f62392bd-3ace-242f-8d91-dff501c5ebde"},"outputs":[],"source":"#Field convertation\ndataframe['Geography'].replace(\"France\",1,inplace= True)\ndataframe['Geography'].replace(\"Spain\",2,inplace = True)\ndataframe['Geography'].replace(\"Germany\",3,inplace=True)\ndataframe['Gender'].replace(\"Female\",0,inplace = True)\ndataframe['Gender'].replace(\"Male\",1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9cfe4ec-9e01-0c5b-3ab5-ecf6841e2805"},"outputs":[],"source":"#Now check if everything was transformed correctly\nprint(\"Geo unique values\",dataframe['Geography'].unique())\nprint(\"Gender unique values\",dataframe['Gender'].unique())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f312423-cd53-5323-4c88-7debc218c240"},"outputs":[],"source":"#In order to understand what \"X\" best contributes to \"Y\" result we should use correlation matrix\nimport seaborn as sns\ncorr = dataframe.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nsns.plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24f86e05-9348-570e-1341-11ec20e6885d"},"outputs":[],"source":"X1 = dataframe['Balance'] \nX2 = dataframe['Age']\nX3 = dataframe['Geography']\nY = dataframe['Exited']\nX = dataframe[['Balance','Age','Geography']]\n#See how range of X looks like \nX.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"464b7d64-fc3f-dff7-a910-6bee87debc55"},"outputs":[],"source":"#Lets count the top 3 positive correlation parametrs\n#The values are:(0.11853277,0.15377058,0.28532304)\nprint ('Balance and Exited',np.corrcoef(X1,Y),'\\n')\nprint ('Age and Exited',np.corrcoef(X2,Y),'\\n')\nprint ('Geography and Exited',np.corrcoef(X3,Y),'\\n')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2786e65-15aa-43ed-5ea3-77c57de8d6d7"},"outputs":[],"source":"#Lets split the data\n# I prefer to use train_test_split for cross-validation\n# This peace will prove us if we have overfitting \nX_train, X_test, y_train, y_test = train_test_split(\n  X, Y, test_size=0.4, random_state=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bfa98044-d2e8-f7d6-0ed0-b2aad21f6b15"},"outputs":[],"source":"#Now train and find out the prediction rate \nclf = GaussianNB()\nclf = clf.fit(X_train ,y_train)\nclf.score(X_test, y_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"71b9070b-3f82-cb51-a9bc-16add08f85a9"},"source":"**Lets make some visualisations**\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ba536ce-a556-37ab-665a-f86c00a33d6f"},"outputs":[],"source":"#Male:1,Female:0\nax = sns.countplot(x=\"Gender\", data=dataframe,\n                   \n                   facecolor=(0, 0, 0, 0),\n                   linewidth=5,\n                  edgecolor=sns.color_palette(\"dark\", 3))\nax.set_title('Gender')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2eca062c-b1dd-397b-f5ba-808358d4643f"},"outputs":[],"source":"#France:1,Spain:2,Germany:3\nax = sns.countplot(x=\"Geography\", data=dataframe,\n                   \n                   facecolor=(0, 0, 0, 0),\n                   linewidth=5,\n                  edgecolor=sns.color_palette(\"dark\", 3))\nax.set_title('Geography')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6238142-fd33-1593-9e5b-5c7dcb0f6b88"},"outputs":[],"source":"#Another type of correlation matrix \ncorrelation = dataframe.corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n\nplt.title('Correlation between different fearures')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c2f5b28-7d66-f812-a3a1-68c89a937880"},"source":"**I tested some other models in Matlab and found out that \"Boost trees\" is the best possible solution**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ba76d43-bc1e-c0ee-302d-ba199b727d6c"},"outputs":[],"source":"#Using ensemble baggin tree classifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nbagging = BaggingClassifier(KNeighborsClassifier(),\n                            max_samples=0.5, max_features=0.5)\n\nbagging = bagging.fit(X_train ,y_train)\nbagging.score(X_test, y_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5684a51c-c842-77c0-c739-e1bf29064d52"},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"1ad80703-eda7-539c-d0b6-4d8233cbefa1"},"source":"**Lets try Random Forest:**\nI tested some \"n_estimators\" and max_features parameters and the variant of Random Forest below show the best result."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53110496-abc3-ff24-5a40-5f8e4ff113d4"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=70,max_features = 3,max_depth=5,n_jobs=-1)\nrf.fit(X_train ,y_train)\nrf.score(X_test, y_test)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}