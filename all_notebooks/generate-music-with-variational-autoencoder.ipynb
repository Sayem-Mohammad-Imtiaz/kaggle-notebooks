{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Generate music with Variational AutoEncoder\n![iame_intro](https://i.ytimg.com/vi/WI1xExDWVF0/maxresdefault.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThis work is inspired by the research paper [Jukebox: A Generative Model for Music](https://cdn.openai.com/papers/jukebox.pdf). In this notebook, I have developed a generative model that could generate music from a variational autoencoder trained with a category of music. I have selected the jazz and classical music categories. The entire model is implemented in TensorFlow and used Librosa for audio processing. The input sampling rate is 3000 for processing the audio file into a readable array.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What is Variational AutoEncoder?\n VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. A variational autoencoder (VAE) provides a probabilistic manner for describing an observation in latent space.\n![VAE](https://miro.medium.com/max/3080/1*82EghOQR2Z5uuwUjFiVV2A.png) \n\nSource -  [The Intuition Behind Variational Autoencoders](https://medium.com/@realityenginesai/understanding-variational-autoencoders-and-their-applications-81a4f99efc0d), [Variational autoencoders.](https://www.jeremyjordan.me/variational-autoencoders/)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n!pip install tensorflow-addons\n!pip install --upgrade --ignore-installed tensorflow\n!pip install -q imageio\n!pip install -q git+https://github.com/tensorflow/docs","execution_count":null,"outputs":[]},{"metadata":{"id":"9dW17ecr5IYC","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import librosa\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers \n\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom IPython.display import clear_output\n\nimport glob\nimport imageio\nimport time\nimport IPython.display as ipd\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"id":"Rf_iJJoWEkEF","trusted":true},"cell_type":"code","source":"seed=123\ntf.compat.v1.set_random_seed(seed)\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"id":"BLdVyNVHrFF1","trusted":true},"cell_type":"code","source":"train_size = 60000\nBATCH_SIZE = 8\ntest_size = 10000\nepochs = 20\n# set the dimensionality of the latent space to a plane for visualization later\nlatent_dim = 2\nnum_examples_to_generate = 2\n\nBASE_PATH = '../input/gtzan-dataset-music-genre-classification/Data/genres_original'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing","execution_count":null},{"metadata":{"id":"NtThS3oVWU85","trusted":true},"cell_type":"code","source":"def DatasetLoader(class_):\n    music_list = np.array(sorted(os.listdir(BASE_PATH+'/'+class_)))\n    train_music_1 = list(music_list[[0,52,19,39,71,12,75,85,3,45,24,46,88]]) #99,10,66,76,41\n    train_music_2 = list(music_list[[4,43,56,55,45,31,11,13,70,37,21,78]]) #65,32,53,22,19,80,89,\n    TrackSet_1 = [(BASE_PATH)+'/'+class_+'/%s'%(x) for x in train_music_1]\n    TrackSet_2 = [(BASE_PATH)+'/'+class_+'/%s'%(x) for x in train_music_2]\n\n    return TrackSet_1, TrackSet_2","execution_count":null,"outputs":[]},{"metadata":{"id":"72nbIzueCYWq","trusted":true},"cell_type":"code","source":"def load(file_):\n    data_, sampling_rate = librosa.load(file_,sr=3000, offset=0.0, duration=30)\n    data_ = data_.reshape(1,90001)\n    return data_\nmap_data = lambda filename: tf.compat.v1.py_func(load, [filename], [tf.float32])","execution_count":null,"outputs":[]},{"metadata":{"id":"Z-q0fDhfeek_","trusted":true},"cell_type":"code","source":"TrackSet_1, TrackSet_2 = DatasetLoader('jazz')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sample original music","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = TrackSet_1[1]\nsample_, sampling_rate = librosa.load(sample,sr=3000, offset=0.0, duration=30)\nipd.Audio(sample_,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"id":"tTb_ETmUB80m","outputId":"f4bdb49c-8183-4b8a-cc3a-bcbe89fa5369","trusted":true},"cell_type":"code","source":"import librosa.display\nplt.figure(figsize=(18,15))\nfor i in range(4):\n    plt.subplot(4, 4, i + 1)\n    j = load(TrackSet_1[i])\n    librosa.display.waveplot(j[0], sr=3000)\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"WFrXrtoFPX1r","outputId":"9395bfcc-2015-4e42-c8b8-f83a497295f7","trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((TrackSet_1))\n    .map(map_data, num_parallel_calls=AUTOTUNE)\n    .shuffle(3)\n    .batch(BATCH_SIZE)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((TrackSet_2))\n    .map(map_data, num_parallel_calls=AUTOTUNE)\n    .shuffle(3)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explaining the concept","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Encoder network\nThis defines the approximate posterior distribution *q(z|x)*, which takes as input an observation and outputs a set of parameters for specifying the conditional distribution of the latent representation *z*. In this example, we simply model the distribution as a diagonal Gaussian, and the network outputs the mean and log-variance parameters of a factorized Gaussian. We output log-variance instead of the variance directly for numerical stability.\n## Decoder network\nThis defines the conditional distribution of the observation *p(x|z)*, which takes a latent sample *z* as input and outputs the parameters for a conditional distribution of the observation. We model the latent distribution prior *p(z)* as a unit Gaussian.\n## Reparameterization\nTo generate a sample *z* for the decoder during training, we can sample from the latent distribution defined by the parameters outputted by the encoder, given an input observation *x*. However, this sampling operation creates a bottleneck because backpropagation cannot flow through a random node.\n\nTo address this, we use a reparameterization trick. In our example, we approximate *z* using the decoder parameters and another parameter *ϵ* as follows:\n\nz = μ + σ.ϵ\n\nSource: [Tensorflow](https://www.tensorflow.org/tutorials/generative/cvae)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Network architecture","execution_count":null},{"metadata":{"id":"qo4mCvIvvl25","trusted":true},"cell_type":"code","source":"class Resnet1DBlock(tf.keras.Model):\n    def __init__(self, kernel_size, filters,type='encode'):\n        super(Resnet1DBlock, self).__init__(name='')\n    \n        if type=='encode':\n            self.conv1a = layers.Conv1D(filters, kernel_size, 2,padding=\"same\")\n            self.conv1b = layers.Conv1D(filters, kernel_size, 1,padding=\"same\")\n            self.norm1a = tfa.layers.InstanceNormalization()\n            self.norm1b = tfa.layers.InstanceNormalization()\n        if type=='decode':\n            self.conv1a = layers.Conv1DTranspose(filters, kernel_size, 1,padding=\"same\")\n            self.conv1b = layers.Conv1DTranspose(filters, kernel_size, 1,padding=\"same\")\n            self.norm1a = tf.keras.layers.BatchNormalization()\n            self.norm1b = tf.keras.layers.BatchNormalization()\n        else:\n            return None\n\n    def call(self, input_tensor):\n        x = tf.nn.relu(input_tensor)\n        x = self.conv1a(x)\n        x = self.norm1a(x)\n        x = layers.LeakyReLU(0.4)(x)\n\n        x = self.conv1b(x)\n        x = self.norm1b(x)\n        x = layers.LeakyReLU(0.4)(x)\n\n        x += input_tensor\n        return tf.nn.relu(x)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"unLIGpdE-6t-","trusted":true},"cell_type":"code","source":"class CVAE(tf.keras.Model):\n    \"\"\"Convolutional variational autoencoder.\"\"\"\n\n    def __init__(self, latent_dim):\n        super(CVAE, self).__init__()\n        self.latent_dim = latent_dim\n        self.encoder = tf.keras.Sequential(\n            [\n                tf.keras.layers.InputLayer(input_shape=(1,90001)),\n                layers.Conv1D(64,1,2),\n                Resnet1DBlock(64,1),\n                layers.Conv1D(128,1,2),\n                Resnet1DBlock(128,1),\n                layers.Conv1D(128,1,2),\n                Resnet1DBlock(128,1),\n                layers.Conv1D(256,1,2),\n                Resnet1DBlock(256,1),\n                # No activation\n                layers.Flatten(),\n                layers.Dense(latent_dim+latent_dim)\n\n            ]\n        )\n        self.decoder = tf.keras.Sequential(\n            [\n                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n                layers.Reshape(target_shape=(1,latent_dim)),\n                Resnet1DBlock(512,1,'decode'),\n                layers.Conv1DTranspose(512,1,1),\n                Resnet1DBlock(256,1,'decode'),\n                layers.Conv1DTranspose(256,1,1),\n                Resnet1DBlock(128,1,'decode'),\n                layers.Conv1DTranspose(128,1,1),\n                Resnet1DBlock(64,1,'decode'),\n                layers.Conv1DTranspose(64,1,1),\n                # No activation\n                layers.Conv1DTranspose(90001,1,1),\n            ]\n        )\n    @tf.function\n    def sample(self, eps=None):\n        if eps is None:\n            eps = tf.random.normal(shape=(200, self.latent_dim))\n        return self.decode(eps, apply_sigmoid=True)\n    @tf.function\n    def encode(self, x):\n        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n        return mean, logvar\n    @tf.function\n    def reparameterize(self, mean, logvar):\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n    @tf.function\n    def decode(self, z, apply_sigmoid=False):\n        logits = self.decoder(z)\n        if apply_sigmoid:\n            probs = tf.sigmoid(logits)\n            return probs\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"id":"HjF7biGGEKML","trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(0.0003,beta_1=0.9, beta_2=0.999,epsilon=1e-08)","execution_count":null,"outputs":[]},{"metadata":{"id":"o-RrkE6mG9Zp","trusted":true},"cell_type":"code","source":"@tf.function\ndef log_normal_pdf(sample, mean, logvar, raxis=1):\n    log2pi = tf.math.log(2. * np.pi)\n    return tf.reduce_sum(\n         -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n          axis=raxis)","execution_count":null,"outputs":[]},{"metadata":{"id":"KHmERgizHMAQ","trusted":true},"cell_type":"code","source":"@tf.function\ndef compute_loss(model, x):\n    mean, logvar = model.encode(x)\n    z = model.reparameterize(mean, logvar)\n    x_logit = model.decode(z)\n    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n    logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2])\n    logpz = log_normal_pdf(z, 0., 0.)\n    logqz_x = log_normal_pdf(z, mean, logvar)\n    return -tf.reduce_mean(logpx_z + logpz - logqz_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss Optimization\nHere we have optimized two lossess, the **KL loss** and **reconstruction loss**.<br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## KL Loss\n\nThe KL divergence tells us how well the probability distribution Q approximates the probability distribution P by calculating the cross-entropy minus the entropy. Intuitively, you can think of that as the statistical measure of how one distribution differs from another.\nIn VAE, let X be the data we want to model, z be latent variable, P(X) be the probability distribution of data, P(z) be the probability distribution of the latent variable and P(X|z) be the distribution of generating data given latent variable.\n\nIn the case of variational autoencoders, our objective is to infer P(z)\nfrom P(z|X). P(z|X) is the probability distribution that projects our data into latent space. But since we do not have the distribution P(z|X), we estimate it using its simpler estimation Q.\n\nNow while training our VAE, the encoder should try to learn the simpler distribution Q(z|X)\nsuch that it is as close as possible to the actual distribution P(z|X). This is where we use KL divergence as a measure of a difference between two probability distributions. The VAE objective function thus includes this KL divergence term that needs to be minimized.\n\n*DKL[Q(z|X)||P(z|X)] = E[ logQ(z|X) − logP(z|X) ]*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Reconstruction loss\nAs the name suggest, it measures the reconstruction of original input x. This network can be trained by minimizing the reconstruction error, which measures the differences between our original input and the consequent reconstruction.","execution_count":null},{"metadata":{"id":"PWi7z22ZHO_l","trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(model, x, optimizer):\n    \n    \"\"\"Executes one training step and returns the loss.\n\n       This function computes the loss and gradients, and uses the latter to\n       update the model's parameters.\n     \"\"\"\n    with tf.GradientTape() as tape:\n            mean, logvar = model.encode(x)\n            z = model.reparameterize(mean, logvar)\n            x_logit = model.decode(z)\n            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n            logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2])\n            logpz = log_normal_pdf(z, 0., 0.)\n            logqz_x = log_normal_pdf(z, mean, logvar)\n            loss_KL = -tf.reduce_mean(logpx_z + logpz - logqz_x)\n            reconstruction_loss = tf.reduce_mean(\n                     tf.keras.losses.binary_crossentropy(x, x_logit)\n                 )\n            total_loss = reconstruction_loss+ loss_KL\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))","execution_count":null,"outputs":[]},{"metadata":{"id":"zqtNDolCHSao","trusted":true},"cell_type":"code","source":"# keeping the random vector constant for generation (prediction) so\n# it will be easier to see the improvement.\nrandom_vector_for_generation = tf.random.normal(\n    shape=[num_examples_to_generate, latent_dim])\nmodel = CVAE(latent_dim)","execution_count":null,"outputs":[]},{"metadata":{"id":"5YymBIlcnMmQ","trusted":true},"cell_type":"code","source":"import librosa.display\nsave_music=[]\ndef generate_and_save_images(model, epoch, test_sample):\n    mean, logvar = model.encode(test_sample)\n    z = model.reparameterize(mean, logvar)\n    predictions = model.sample(z)\n    fig = plt.figure(figsize=(18, 15))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i + 1)\n        wave = np.asarray(predictions[i])\n        if epoch>14:\n            save_music.append(wave)\n        librosa.display.waveplot(wave[0], sr=3000)\n\n    # tight_layout minimizes the overlap between 2 sub-plots\n    plt.savefig('jazz_{:04d}.png'.format(epoch))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qB-85OsqoU2B","trusted":true},"cell_type":"code","source":"# Pick a sample of the test set for generating output images\nassert BATCH_SIZE >= num_examples_to_generate\nfor test_batch in test_dataset.take(1):\n    test_sample = test_batch[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model","execution_count":null},{"metadata":{"id":"hTt1sUZMYADG","outputId":"df665fc3-de1b-4864-9858-53a671b030a2","trusted":true},"cell_type":"code","source":"generate_and_save_images(model, 0, test_sample)\n\nfor epoch in range(1, epochs + 1):\n    start_time = time.time()\n    for train_x in train_dataset:\n        train_x = np.asarray(train_x)[0]\n        train_step(model, train_x, optimizer)\n    end_time = time.time()\n\n    loss = tf.keras.metrics.Mean()\n    for test_x in test_dataset:\n        test_x = np.asarray(test_x)[0]\n        loss(compute_loss(model, test_x))\n    display.clear_output(wait=False)\n    elbo = -loss.result()\n    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'.format(epoch, elbo, end_time - start_time))\n    generate_and_save_images(model, epoch, test_sample)","execution_count":null,"outputs":[]},{"metadata":{"id":"pQcs-r6cIoRd","trusted":true},"cell_type":"code","source":"anim_file_1 = 'jazz_cvae.gif'\n\nwith imageio.get_writer(anim_file_1, mode='I') as writer:\n    filenames = glob.glob('jazz*.png')\n    filenames = sorted(filenames)\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization","execution_count":null},{"metadata":{"id":"4lTUTQTmIuRC","outputId":"4e1e5790-033b-4c47-d561-34b4198ec84d","trusted":true},"cell_type":"code","source":"import tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generated Music - Jazz\nLet us listen to the music generated by our model, trained only with jazz music.","execution_count":null},{"metadata":{"id":"7VlU5tcEf-cI","trusted":true},"cell_type":"code","source":"l1=save_music[13][0]\nipd.Audio(l1,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"id":"Pu4LXmx1jF-2","outputId":"3aa95bb4-eb5a-4ffb-9820-24e5db17bf59","trusted":true},"cell_type":"code","source":"l2=save_music[19][0]\nipd.Audio(l1,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l3=save_music[14][0]\nipd.Audio(l3,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l4=save_music[15][0]\nipd.Audio(l4,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l5=save_music[18][0]\nipd.Audio(l5,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l6=save_music[17][0]\nipd.Audio(l6,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.compat.v1.reset_default_graph()\ntf.compat.v1.set_random_seed(seed)\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Example\nLet's try out another example with classical music. The code shown below is the same as above, its just the same experiment over a different music category.","execution_count":null},{"metadata":{"id":"GzuXzwyej7YR","trusted":true},"cell_type":"code","source":"TrackSet_3, TrackSet_4 = DatasetLoader('classical')","execution_count":null,"outputs":[]},{"metadata":{"id":"Q6r8z_nkMV5x","trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((TrackSet_3))\n    .map(map_data, num_parallel_calls=AUTOTUNE)\n    .shuffle(3)\n    .batch(BATCH_SIZE)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((TrackSet_4))\n    .map(map_data, num_parallel_calls=AUTOTUNE)\n    .shuffle(3)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef log_normal_pdf(sample, mean, logvar, raxis=1):\n    log2pi = tf.math.log(2. * np.pi)\n    return tf.reduce_sum(\n         -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n          axis=raxis)\n@tf.function\ndef compute_loss(model, x):\n    mean, logvar = model.encode(x)\n    z = model.reparameterize(mean, logvar)\n    x_logit = model.decode(z)\n    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n    logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2])\n    logpz = log_normal_pdf(z, 0., 0.)\n    logqz_x = log_normal_pdf(z, mean, logvar)\n    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n@tf.function\ndef train_step(model, x, optimizer):\n    \n    \"\"\"Executes one training step and returns the loss.\n\n       This function computes the loss and gradients, and uses the latter to\n       update the model's parameters.\n     \"\"\"\n    with tf.GradientTape() as tape:\n            mean, logvar = model.encode(x)\n            z = model.reparameterize(mean, logvar)\n            x_logit = model.decode(z)\n            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n            logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2])\n            logpz = log_normal_pdf(z, 0., 0.)\n            logqz_x = log_normal_pdf(z, mean, logvar)\n            loss_KL = -tf.reduce_mean(logpx_z + logpz - logqz_x)\n            reconstruction_loss = tf.reduce_mean(\n                     tf.keras.losses.binary_crossentropy(x, x_logit)\n                 )\n            total_loss = reconstruction_loss+ loss_KL\n    gradients = tape.gradient(total_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))","execution_count":null,"outputs":[]},{"metadata":{"id":"SqseIbNKMe2X","trusted":true},"cell_type":"code","source":"random_vector_for_generation = tf.random.normal(\n    shape=[num_examples_to_generate, latent_dim])\nmodel = CVAE(latent_dim)","execution_count":null,"outputs":[]},{"metadata":{"id":"faLIyex_MtX8","trusted":true},"cell_type":"code","source":"import librosa.display\nsave_music_U=[]\ndef generate_and_save_images(model, epoch, test_sample):\n    mean, logvar = model.encode(test_sample)\n    z = model.reparameterize(mean, logvar)\n    predictions = model.sample(z)\n    fig = plt.figure(figsize=(18, 15))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i + 1)\n        wave = np.asarray(predictions[i])\n        if epoch>15:\n            save_music_U.append(wave)\n        librosa.display.waveplot(wave[0], sr=3000)\n\n    # tight_layout minimizes the overlap between 2 sub-plots\n    plt.savefig('classical_{:04d}.png'.format(epoch))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZsIIwIOhNCVr","trusted":true},"cell_type":"code","source":"assert BATCH_SIZE >= num_examples_to_generate\nfor test_batch in test_dataset.take(1):\n    test_sample = test_batch[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"6rkkmQCnNHpH","trusted":true},"cell_type":"code","source":"generate_and_save_images(model, 0, test_sample)\n\nfor epoch in range(1, epochs + 1):\n    start_time = time.time()\n    for train_x in train_dataset:\n        train_x = np.asarray(train_x)[0]\n        train_step(model, train_x, optimizer)\n    end_time = time.time()\n\n    loss = tf.keras.metrics.Mean()\n    for test_x in test_dataset:\n        test_x = np.asarray(test_x)[0]\n        loss(compute_loss(model, test_x))\n    display.clear_output(wait=False)\n    elbo = -loss.result()\n    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'.format(epoch, elbo, end_time - start_time))\n    generate_and_save_images(model, epoch, test_sample)","execution_count":null,"outputs":[]},{"metadata":{"id":"WAKyCXFrNO8M","trusted":true},"cell_type":"code","source":"anim_file_2 = 'classical_cvae.gif'\n\nwith imageio.get_writer(anim_file_2, mode='I') as writer:\n    filenames = glob.glob('classical*.png')\n    filenames = sorted(filenames)\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)","execution_count":null,"outputs":[]},{"metadata":{"id":"1cKznq16Q5Gg","trusted":true},"cell_type":"code","source":"embed.embed_file(anim_file_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generated music - Classical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m1=save_music_U[13][0]\nipd.Audio(m1,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m2=save_music_U[19][0]\nipd.Audio(m2,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m3=save_music_U[14][0]\nipd.Audio(m3,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m4=save_music_U[15][0]\nipd.Audio(m4,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m5=save_music_U[18][0]\nipd.Audio(m5,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m6=save_music_U[17][0]\nipd.Audio(m6,rate=3000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n**WoW! the generated music sounds really good....**\n![conc](https://i.ytimg.com/vi/PdzFMIDzp2I/maxresdefault.jpg)\n<br>\nFirst of all, I got to learn a lot while working on this project. There are a lot of aspects discussed in the research paper [Jukebox: A Generative Model for Music](https://cdn.openai.com/papers/jukebox.pdf), which could be implemented for better performance and promising result. One of the aspects is the optimization of **Spectral loss**. I have implemented the model which works only for music generation, for generating a lyrical song this model won't generate a promising result. For generating lyrical songs birectional lstm, transformer networks and attention layers would be a better choice according to me for contructing the network architecture. Also, I have trained my model against a particular category of music, this solution could also be made more interesting by training the model against two or more categories of music.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Thank you.\n![thank you](https://www.scienceabc.com/wp-content/uploads/2016/06/orkestra-Music-conductor.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Please <span style=\"color:red\">Up-Vote</span> and <span style=\"color:red\">Share</span> this notebook if you like it or find the content informative. Also, let me know your opinions and suggestions in the comment section below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}