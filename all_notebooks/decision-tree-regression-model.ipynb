{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pwd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## About Data and Objective\n\n#### Use Decision Trees to build a regressor & Grid Search to find the optimal value for the hyperparameters for the given dataset, and evaluate your model on the appropriate metrics try and predict gas consumption (in millions of gallons) in 48 US states based upon:\n1. Gas Tax (in cents)\n2. Per Capita Income (dollars)\n3. Paved Highways (in miles) &\n4. The proportion of the population with a driver license.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Importing Packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\nimport pylab as pl\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Fuel_cons=pd.read_csv(\"../input/petrol_consumption.csv\") #Importing Data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Fuel_cons.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Structure and Datatypes of Dataset along with Summary Statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fuel_cons.shape)\nFuel_cons.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = '{:.4f}'.format\ndata_summary=Fuel_cons.describe()\ndata_summary.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Checking for Outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k, v in Fuel_cons.items():\n    q1 = v.quantile(0.25)\n    q3 = v.quantile(0.75)\n    irq = q3 - q1\n    v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n    perc = np.shape(v_col)[0] * 100.0 / np.shape(Fuel_cons)[0]\n    print(\"Column %s outliers = %.2f%%\" % (k, perc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nFuel_cons.boxplot(patch_artist=True,vert=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Correlation Chart","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_corr=Fuel_cons.corr()\nmy_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.heatmap(my_corr,linewidth=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Understanding Data: Exploratory Data Analysis \n\nCalculating Correlation, P-value and Regression plot\n\nTo understand the spread of datapoints this regression plot has been plotted along with pearson coefficients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(Fuel_cons['Petrol_tax'], Fuel_cons['Petrol_Consumption'])\nprint(\"The Pearson Correlation Coefficient of Petrol_tax is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"Petrol_tax\", y=\"Petrol_Consumption\", data=Fuel_cons)\nplt.ylim(0,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(Fuel_cons['Average_income'], Fuel_cons['Petrol_Consumption'])\nprint(\"The Pearson Correlation Coefficient of Petrol_tax is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"Average_income\", y=\"Petrol_Consumption\", data=Fuel_cons)\nplt.ylim(0,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(Fuel_cons['Paved_Highways'], Fuel_cons['Petrol_Consumption'])\nprint(\"The Pearson Correlation Coefficient of Petrol_tax is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"Paved_Highways\", y=\"Petrol_Consumption\", data=Fuel_cons)\nplt.ylim(0,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(Fuel_cons['Population_Driver_licence(%)'], Fuel_cons['Petrol_Consumption'])\nprint(\"The Pearson Correlation Coefficient of Petrol_tax is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"Population_Driver_licence(%)\", y=\"Petrol_Consumption\", data=Fuel_cons)\nplt.ylim(0,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Knowing about the distribution of Predictors and target of dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(Fuel_cons['Petrol_tax'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(Fuel_cons['Paved_Highways'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(Fuel_cons['Average_income'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(Fuel_cons['Petrol_Consumption'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(Fuel_cons['Population_Driver_licence(%)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Viz 3: The below plot has been plotted in order to show the Petrol Consumption as per tax.\n\nWe can see from the below ploted graph that the bandwith of petrol Consumption with lower petrol tax is smooth shown by (Brown Curve) it has high bandwidth as it has shallow kernel and has high density. When we look at the area under Blue Curve we can see its density and amplitude is low which shows petrol tax with High petrol consumption people are less.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a = sns.FacetGrid(Fuel_cons, hue = 'Petrol_Consumption', aspect=4 )\na.map(sns.kdeplot, 'Petrol_tax', shade= True )\na.set(xlim=(0 ,Fuel_cons['Petrol_tax'].max()))\na.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Viz 4:  The factor plot shows that people mostly taxes are less when there are paved Highways whereas there is high taxation in petrol fuel where there are less Paved Highways. This shows despite of high petrol taxes the condition of highways are not improved which says the improper management and lack of work done by authority.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"axes = sns.factorplot('Petrol_tax','Paved_Highways',data=Fuel_cons, aspect = 2.5, )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Dividing Data into Predictors and Target Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor_var= Fuel_cons[['Petrol_tax','Average_income','Paved_Highways','Population_Driver_licence(%)']] #all columns except the last one\ntarget_var= Fuel_cons['Petrol_Consumption'] #only the last column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor_var.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_var.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Decision Tree without using any external tool for Optimization such as Grid Search CV","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Importing Test Train Split from sklearn package","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now train_test_split will return 4 different parameters. We will name them:\\\nX_trainset, X_testset, y_trainset, y_testset**\n\n**The train_test_split will need the parameters:\\\nX, y, test_size=0.3, and random_state=123.**\n\n**The X and y are the arrays required before the split, the test_size represents the ratio of the testing dataset, and the random_state ensures that we obtain the same splits.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(predictor_var,target_var, test_size=0.30, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Objective 1: Applied Decision Tree algorithm for regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import Decision Tree Regressor and fit the model to the training data.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor(max_depth=4,max_features=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ##### Inside of the regressor, specify criterion=\"mse\" so we can see the mse of each node.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Make predictions and evaluate output.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame({'Actual':Y_test, 'Predicted':predictions})\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the predictions are not accurate. Let's evaluate the prediction accuracy.\n\n##### Evaluating the Prediction Accuracy\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test,predictions))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test,predictions))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test,predictions)))\nprint('r2_score:', metrics.r2_score(Y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Looking after Feature Importances","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.feature_importances_\npd.Series(tree.feature_importances_,index=predictor_var.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing Graphviz from Sklearn library to plot the decission tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = export_graphviz(tree, filled=True, rounded=True, feature_names=predictor_var.columns, out_file=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graphviz.Source(dot_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The trees follow a top-down greedy approach known as recursive binary splitting. We call it as ‘top-down’ because it begins from the top of tree when all the observations are available in a single region and successively splits the predictor space into two new branches down the tree. It is known as ‘greedy’ because, the algorithm cares (looks for best variable available) about only the current split, and not about future splits which will lead to a better tree.\n\nStopping Criteria: The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.\n\nFor this dataset Petrol_consumption is considered as the Target variable and rest are Predictors:\\\n\n**Q.  Why does Tree considered Petrol_tax as Root of Tree?**\\\n**Ans**The reason could be the feature_importances as from above codes we can see Petrol_tax has feature importance value compared to others. So, the Petrol_tax is selected as root of tree and other features as decision nodes.\n\n**The Split :** Purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n\n##### Hyperparameters Considered:   max_depth=4,  max_features=4\n1. The tree uses Petrol Tax features with threshold value of 7.25 to initially divide the samples.\n2. We can see out of 33 Samples the root (Petrol Tax) split into two other decision nodes:\\\n   i.  Where Petrol_tax <= 7.25 then it uses another feature (Paved Highways) with total number of sample of 15 as decision node.\\\n   ii.Where Petrol_tax> 7.25 then it uses feature Average Income as another decision node with Sample count of 18.\n3. The tree considered Mean Squared Error(MSE) as the determining and decision making criteria. \n4. In the above plotted decision tree every node is a conditon how to split values. The least the MSE the better the result. \n5. We can see from decision nodes there are several boxes known as leaf nodes where MSE=0 and thus tree stops branching from that nodes.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###  Using  Gridsearch & Cross Validation appropriately.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Now I will use grid search cv to find the optimal value of hyper_parameters to plot the Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [{\"max_depth\":[3,4,5, None], \"max_features\":[3,4,5,6,7]}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs = GridSearchCV(estimator=DecisionTreeRegressor(random_state=123),param_grid = param_grid,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.cv_results_['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.cv_results_['rank_test_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor(max_depth=3,max_features=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame({'Actual':Y_test, 'Predicted':predictions})\ndf.head(5) #Check the top 5 predictions and actual values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test,predictions))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test,predictions))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test,predictions)))\nprint('r2_score:', metrics.r2_score(Y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use graphviz to plot the dot data as a decision tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = export_graphviz(tree, filled=True, rounded=True, feature_names=predictor_var.columns, out_file=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graphviz.Source(dot_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above tree is the optimised result of the Base tree which we have used earlier in this assignment. To Optimisation I have used Grid Search to prune the tree and find the best estimator as shown below:\n\n##### Hyperparameters Considered:   max_depth=3,  max_features=4\n**The reason for defining\\\ni. Maximum depth of tree(vertical depth) to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\\\nii. max_features these are selected randomly but higher value selection results in Overfitting.**\n\nWe have used the above mentioned parameters to improve decision tree accuracy and reduce MSE. by fitting in our model this can also be called a one way of prune the tree.\n\nObservation\n1. The tree uses Petrol Tax features with threshold value of 7.25 to initially divide the samples.\n2. We can see out of 33 Samples the root (Petrol Tax) split into two other decision nodes:\\\n   i.  Where Petrol_tax <= 7.25 then it uses another feature (Paved Highways) with total number of sample of 15 as decision node.\\\n   ii.Where Petrol_tax> 7.25 then it uses feature Average Income as another decision node with Sample count of 18.\n3. The tree considered Mean Squared Error(MSE) as the determining and decision making criteria. \n4. In the above plotted decision tree every node is a conditon how to split values. The least the MSE the better the result. \n5. We can see from decision nodes there are several boxes known as leaf nodes where MSE=0 and thus tree stops branching from that nodes.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The Comparision Of Decision Tree without Applying GridSearch and One by applying with GridSearch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_Regressor=[['Max_Depth',4,3],['Max_Feature',4,4],['Mean Abs. Error',106.73,96.6],['Mean Square Error',18466.34,15143.73],['Root Mean Square',135.89,123.05],['r2_Score',0.20,0.344]]\nResult_Summary2= pd.DataFrame(DT_Regressor, columns = ['Parameters','Without Grid Search','With Grid Search'])\nResult_Summary2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above comparision Dataframe we can see how Using GridSearch has affected the end result of Decision Tree Model. The Accuracy scores has also increased after substituting the optimum hyperparameters value. The MSE has reduced.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"References:\n\n1. https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n2. https://webfocusinfocenter.informationbuilders.com/wfappent/TLs/TL_rstat/source/DecisionTree47.html\n3. https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}