{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nIn this notebook, we will do some data analysis on the palmer penguins dataset. We'll work through the ML pipeline and create a species classifier.\n- EDA\n- Feature Engineering\n- Modelling","metadata":{}},{"cell_type":"markdown","source":"# Importing some libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import IterativeImputer\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Common functions\ndef apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    mi_scores = mutual_info_regression(X, y, discrete_features=False, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.319939Z","iopub.execute_input":"2021-06-15T11:51:52.32033Z","iopub.status.idle":"2021-06-15T11:51:52.335896Z","shell.execute_reply.started":"2021-06-15T11:51:52.320294Z","shell.execute_reply":"2021-06-15T11:51:52.334597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nLet's take a quick look at the dataset and see what columns do we have. Looking into the sample data, we can see som NaN values, we will fix this later. Aside form that, we can start dropping some columns from the set, like studyName since they do not provide any additional data to training.","metadata":{}},{"cell_type":"code","source":"# Read the dataset\npenguins_data = pd.read_csv(\"../input/palmer-archipelago-antarctica-penguin-data/penguins_lter.csv\")\n\nprint(penguins_data.info())","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-15T11:51:52.337308Z","iopub.execute_input":"2021-06-15T11:51:52.337723Z","iopub.status.idle":"2021-06-15T11:51:52.371588Z","shell.execute_reply.started":"2021-06-15T11:51:52.337693Z","shell.execute_reply":"2021-06-15T11:51:52.370612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's pick some features as our training variables\nfeature_columns = ['Species', 'Island', 'Stage',\n        'Culmen Length (mm)','Culmen Depth (mm)',\n        'Flipper Length (mm)', 'Body Mass (g)', 'Sex']\nX = penguins_data[feature_columns]\n\n# This is likely a data entry mistake, hence we'll set it as a NaN for now\nX.loc[X.Sex == '.', 'Sex'] = np.nan","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.373207Z","iopub.execute_input":"2021-06-15T11:51:52.373486Z","iopub.status.idle":"2021-06-15T11:51:52.379959Z","shell.execute_reply.started":"2021-06-15T11:51:52.37346Z","shell.execute_reply":"2021-06-15T11:51:52.379122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the correlation heatmap\nfig, axs = plt.subplots(nrows=1, figsize=(10, 10))\n\n# most correlated features if too much features\n#corrmat = train.corr()\n#top_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\n#plt.figure(figsize=(10,10))\n#g = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n\nsns.heatmap(X.corr(), ax=axs, annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\naxs.tick_params(axis='x', labelsize=10)\naxs.tick_params(axis='y', labelsize=10)\n    \naxs.set_title('Correlations', size=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.381475Z","iopub.execute_input":"2021-06-15T11:51:52.382016Z","iopub.status.idle":"2021-06-15T11:51:52.714671Z","shell.execute_reply.started":"2021-06-15T11:51:52.381982Z","shell.execute_reply":"2021-06-15T11:51:52.713695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Values\n\nWe can see that there are missing values from the dataset. We will work with the full dataset (excluding the target variable) when fixing missing values, else we risk overfitting to the train/test set with the filled data. From the code ran below, we can see that there isn't any major missing data aside from the comments column. We'll now start working on restoring the values one by one.","metadata":{}},{"cell_type":"code","source":"def display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \ndisplay_missing(X)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.71613Z","iopub.execute_input":"2021-06-15T11:51:52.716558Z","iopub.status.idle":"2021-06-15T11:51:52.72758Z","shell.execute_reply.started":"2021-06-15T11:51:52.716506Z","shell.execute_reply":"2021-06-15T11:51:52.726449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Length & Depth Missing Values\nLooking into the null values for culmen length, we can see that the majority of the labels are null as well. Due to its high nullibility, we can safely drop these rows without much problem.","metadata":{}},{"cell_type":"code","source":"X[X['Culmen Length (mm)'].isnull()]\nX.dropna(how='all', subset=['Culmen Length (mm)'], inplace=True)\ndisplay_missing(X)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.729178Z","iopub.execute_input":"2021-06-15T11:51:52.729592Z","iopub.status.idle":"2021-06-15T11:51:52.742476Z","shell.execute_reply.started":"2021-06-15T11:51:52.729551Z","shell.execute_reply":"2021-06-15T11:51:52.741475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sex & Delta \nA very simple way to do this is to impute the missing values with the modal gender of the dataset. However, let's try to impute it in a smarter fashion. One simple way is to just use k-nearest neighbours to impute these values. However, in this notebook, we will use another imputation technique called MICE. MICE stands for Multiple Imputation by Chained Equation, and as can be figured by the name alone, it uses multiple imputations and performs multiple regression over the sample data and takes the averages of them.","metadata":{}},{"cell_type":"code","source":"# Label Encoding\nlbl = LabelEncoder()\nlabel_encode_features = ['Sex', 'Stage', 'Species']\nfor feature in label_encode_features:\n    X[feature] = lbl.fit_transform(X[feature])\n    \n# One-hot encoding\nX = pd.get_dummies(X, prefix=['Island'], columns=['Island'])\n\n# MICE Impute\nimp = IterativeImputer(verbose=1)\nimputed_X = imp.fit_transform(X)\nimputed_X = pd.DataFrame(imputed_X, columns=X.columns)\nX = imputed_X","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.743956Z","iopub.execute_input":"2021-06-15T11:51:52.744355Z","iopub.status.idle":"2021-06-15T11:51:52.776657Z","shell.execute_reply.started":"2021-06-15T11:51:52.744311Z","shell.execute_reply":"2021-06-15T11:51:52.775547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Failed: Culmen Length (mm) \n> NOTE: It is better to drop the rows entirely in this case because of the lack of data (which I did not check in the first place. The following section will be left here as it's still useful information\n\nWhen it comes to data imputation, the most straightforward way to work on this is to just fill in the missing values with its mean or median. However, we can definitely improve upon this. Since Flipper Length has the highest correlation with Culmen Length, we can apply a regression approach to impute the missing culmen length.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# We can try to group it by species and sex as well.\nculmen_length_grouped = X.groupby(['Sex', 'Species']).median()['Culmen Length (mm)']\n\nfor Species in X.Species.unique():\n    for sex in ['MALE', 'FEMALE']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, culment_length_grouped[sex][species]))\nprint('Median age of all passengers: {}'.format(df_all['Age'].median()))\ndf_all['Culmen Length (mm)'] = df_all.groupby(['Sex', 'Species'])['Culmen Length (mm)'].apply(lambda x: x.fillna(x.median()))\n\"\"\"\n\n#df_all_corr = X.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n#df_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n#df_all_corr[df_all_corr['Feature 1'] == 'Culmen Length (mm)']","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.77798Z","iopub.execute_input":"2021-06-15T11:51:52.778355Z","iopub.status.idle":"2021-06-15T11:51:52.784195Z","shell.execute_reply.started":"2021-06-15T11:51:52.778314Z","shell.execute_reply":"2021-06-15T11:51:52.783249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imputation_regressor(X, y):\n    \"\"\"\n    We create a linear regression model to predict missing values here\n    \"\"\"\n    regress_train_X, regress_test_X, regress_train_y, regress_test_y = train_test_split(X.dropna(), y.dropna(), random_state=1)\n    clf = LinearRegression()\n    clf.fit(regress_train_X, regress_train_y)\n    y_test_pred = clf.predict(regress_test_X)\n    print(f'R2 Score: {r2_score(regress_test_y, y_test_pred)}')\n    return clf\n\n#regress_X = X[['Flipper Length (mm)', 'Body Mass (g)']]\n#regress_y = X['Culmen Length (mm)']\n#clf = imputation_regressor(regress_X, regress_y)\n\n# This doesn't work because flipper length and body mass are all null, still useful information nonetheless\n# X['Culmen Length (mm)'] = clf.predict(regress_X)[X['Culmen Length (mm)'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.786912Z","iopub.execute_input":"2021-06-15T11:51:52.787321Z","iopub.status.idle":"2021-06-15T11:51:52.79378Z","shell.execute_reply.started":"2021-06-15T11:51:52.78728Z","shell.execute_reply":"2021-06-15T11:51:52.792869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The R2 score measures how much better than baseline linear regression performs, where baseline is flat regression against the mean. In this case that baseline performance (an R2 of 0) is the performance of replacing the missing values with the mean of the observed values. In this case, albeit slightly low cross validation scores are seen, it is still statistically significant and slightly better as it introduces less bias compared to naively using the mean or median of the dataset.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\nWith missing values fixed, we can now proceed to feature engineering. Whatever relationships the model can't learn, we can provide ourselves through transformations. That way, we can improve the predictive accuracy of our model overall. First, let's choose features that correlate well with species (target variable).","metadata":{}},{"cell_type":"markdown","source":"### PCA \nAlthough one of the most common uses of PCA is for dimensionality reduction, we can also explore the loadings to create new features. For this notebook, we'll do a PCA based on the continuous features ``Culmen Length``, ``Culmen Depth``, ``Flipper Length``, and ``Body Mass`` since these are physically measurable features. Notably, we'll standardize these features first since these features aren't on the same scale.","metadata":{}},{"cell_type":"code","source":"# We set the species column as the target variable to be predicted\ny = X.Species\nX.drop(columns=['Species'], inplace=True)\n\npca_features = ['Culmen Length (mm)', 'Flipper Length (mm)', 'Culmen Depth (mm)', 'Body Mass (g)']\npca, X_pca, loadings = apply_pca(X[pca_features])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.795502Z","iopub.execute_input":"2021-06-15T11:51:52.795808Z","iopub.status.idle":"2021-06-15T11:51:52.812087Z","shell.execute_reply.started":"2021-06-15T11:51:52.79578Z","shell.execute_reply":"2021-06-15T11:51:52.811016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_scores = make_mi_scores(X_pca, y)\nmi_scores","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.814269Z","iopub.execute_input":"2021-06-15T11:51:52.814661Z","iopub.status.idle":"2021-06-15T11:51:52.844071Z","shell.execute_reply.started":"2021-06-15T11:51:52.814611Z","shell.execute_reply":"2021-06-15T11:51:52.842892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that PC4 is insignificant in this case, hence we'll ignore the last principal component.","metadata":{}},{"cell_type":"markdown","source":"##### Loadings\nWe will try and interpret these loadings prior to deriving new features from this information. Let's look at the principal components one by one.\n1. In PC1, we can see that it has something to do with the total span of the penguin in proportion to its body mass.\n2. In PC2, flipper length and body mass are insignificant. Since we can discard those, we can see that the 2nd principal component has something to do with the culmen size.\n3. In PC3, it has something to do with the span of the culmen in proportion to its entire body.\n\nNow that we interpreted these principal components, we will try to make some new features through transformations.","metadata":{}},{"cell_type":"code","source":"print(loadings)\nX['Culmen Span'] = X['Culmen Length (mm)']/X['Culmen Depth (mm)']\nX['Culmen Size'] = X['Culmen Length (mm)'] * X['Culmen Depth (mm)']","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:51:52.845468Z","iopub.execute_input":"2021-06-15T11:51:52.845844Z","iopub.status.idle":"2021-06-15T11:51:52.856441Z","shell.execute_reply.started":"2021-06-15T11:51:52.845804Z","shell.execute_reply":"2021-06-15T11:51:52.855357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\nNow that all of the ground work has been established, we can now start modelling.\n\n### Random forest classifier\nWe will use a random forest classifier for this notebook. One reason is because it is straightforward and simplicity is key (as opposed to a ANN).\nWe use a random forest as it is generally more accurate than a decision tree. Feature standardization is also a very important step in machine learning that we will not skip. It helps the algorithm quickly learn a better solution to the problem.","metadata":{}},{"cell_type":"markdown","source":"#### Cross Validation\nWe will use a 5-fold cross validation to measure our classifier's accuracy. As we can see, we are achieving >98% accuracy on average.\n\n#### Confusion Matrix\nWe can also look at the classifier's confusion matrix. As can be predicted from the cross validation performance, we only see a single misclassification which is very impressive.","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=10)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nmodel.fit(X_train, y_train)\n\nprint(cross_val_score(model, X, y, cv=5))\nplot_confusion_matrix(model, X_test, y_test)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T11:52:01.916223Z","iopub.execute_input":"2021-06-15T11:52:01.916604Z","iopub.status.idle":"2021-06-15T11:52:02.246523Z","shell.execute_reply.started":"2021-06-15T11:52:01.916569Z","shell.execute_reply":"2021-06-15T11:52:02.245491Z"},"trusted":true},"execution_count":null,"outputs":[]}]}