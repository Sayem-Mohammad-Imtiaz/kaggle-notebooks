{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pretraining generator and critic models"},{"metadata":{},"cell_type":"markdown","source":"Training a GAN can be hard.\nThis is mainly because of the initialization problem, in the beginning neither generator nor critic knows  which way to go, what is to optimize.\nThis is the case of 'blind leading the blind'.\nHere we pretrain both models, before putting them together as a GAN.\n(credits to J. Howard, fast.ai)\n\nCheck out how the crappy images were generated: https://www.kaggle.com/greenahn/crappify-imgs\n\nAnother version of this notebook with simpler loss function (Mean Squared Error) is at: https://www.kaggle.com/greenahn/pretrain-gan-mse\n\nGithub repository: https://github.com/nupam/GANs-for-Image-enhancement/"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\nfrom fastai.vision.gan import *\nimport gc\nfrom torchvision.models import vgg16_bn","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## These folders contain crappy images in different resolution with differnt crappafication logic (randomly selected)\norig_path = Path('../input/flickrproc/hr/hr')\nfnames_df = pd.read_csv('../input/flickrproc/files.csv')\n\nFOLDERS = {256:Path('../input/flickrproc/crappy_256/crappy/'), 320:Path('../input/flickrproc/crappy_320/crappy/'), }\nFOLDERS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting images ready"},{"metadata":{},"cell_type":"markdown","source":"### First, let us have a look at the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"src=ImageList.from_df(fnames_df, path = orig_path, cols='name')\nsrc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_one(r, c, ax):\n    figsize = (6,6)\n    name = Path(src.items[r]).name\n    if c == 0:\n        ax.title.set_text('original')\n        open_image(src.items[r]).show(ax, figsize=figsize)\n    else:\n        ax.title.set_text('crappy ' + str(c))\n        open_image(list(FOLDERS.values())[c-1]/name).show(ax, figsize=figsize)\n        \nplot_multi(plot_one, 15, 3, figsize=(25, 75))\ndel src","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(size, bs, folder=256, split=0.9):\n    folder = FOLDERS[folder]\n    src = ImageImageList.from_df(fnames_df, \n                           path = folder, cols='name')\n    src = src.split_by_idx(np.arange(int(src.items.shape[0]*split), src.items.shape[0]))\n    \n    data = src.label_from_func(lambda x: orig_path/Path(x).name).transform(get_transforms(), size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n    data.c = 3\n    return data\n\ngen_data = get_data(128,32, 256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_data.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generator"},{"metadata":{},"cell_type":"markdown","source":"### Loss Function\nImages generated by only MSE(or MAE) are not pleasing, the seem blurry.\nThey do not capture texture, so in addition to MAE(l1_loss)\nwe also add feature loss based on VGG-16 model, as in the famous paper on neural art transfer, https://arxiv.org/abs/1508.06576 ."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_loss = F.l1_loss\n\nvgg_m = vgg16_bn(True).features.cuda().eval()\nrequires_grad(vgg_m, False)\nblocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n\ndef gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(n, c, -1)\n    return (x @ x.transpose(1,2))/(c*h*w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureLoss(nn.Module):\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        super().__init__()\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target):\n        out_feat = self.make_features(target, clone=True)\n        in_feat = self.make_features(input)\n        self.feat_losses = [base_loss(input,target)]\n        self.feat_losses += [base_loss(f_in, f_out)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model\nThe model used here is a unet with pretrained resnet34\nweight normalization is used for stabalizing the learning process, as batch-normalization adds noise and in GANs it is not desireable"},{"metadata":{"trusted":true},"cell_type":"code","source":"wd = 1e-3\narch = models.resnet34\n\nlearn_gen = unet_learner(gen_data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics, blur=True, norm_type=NormType.Weight, model_dir=\"/kaggle/working\")\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training\nWe train the model in half-precision, becuse its faster and that kind of accuracy is not required."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen = learn_gen.to_fp16()\nlearn_gen.lr_find(end_lr=1) ## LR selection\nlearn_gen.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.fit_one_cycle(2, 1.1e-3, pct_start=0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.unfreeze()\nlearn_gen.lr_find()\nlearn_gen.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.fit_one_cycle(3, slice(1e-5, 8e-4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Increasing Image size\nwe increase the image size to 256 and train again"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.freeze()\ndel learn_gen.data, gen_data\nlearn_gen.data = get_data(bs=32, size=256, folder=256)\nlearn_gen = learn_gen.to_fp16()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.fit_one_cycle(2, 8e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.unfreeze()\nlearn_gen.fit_one_cycle(3, slice(2e-5, 6e-4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the model and having a look at the generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen = learn_gen.to_fp32()\nlearn_gen.save('gen_pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results(rows=10, figsize=(30, 80))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look pictures are already looking awesone.\nIt's way better than what we got from just using MSE only. Let's see if we can make it better using GANs."},{"metadata":{},"cell_type":"markdown","source":"## Saving generated images\nWe are going then to use it for pretraining critic"},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf gen_imgs/\n!mkdir gen_imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_gen = Path('gen_imgs/')\n\ndef save_preds(dl):\n    i=0\n    names = dl.dataset.items\n    \n    for b in tqdm(dl):\n        preds = learn_gen.pred_batch(batch=b, reconstruct=True)\n        for o in preds:\n            o.save(path_gen/Path(names[i]).name)\n            i += 1\n        del preds\n    del names\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del learn_gen.data\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.data = get_data(256, 16, 256, 1.0)\nsave_preds(learn_gen.data.fix_dl)\nopen_image(path_gen.ls()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##free memory\ndel learn_gen\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Critic"},{"metadata":{},"cell_type":"markdown","source":"### Data\nWe use the above generated images as of one class and original images of another."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_critic_data(bs, size=256, split=0.9):\n    \n    def labeler(x):\n        ret = 'generated' if Path(x).parent.name == 'gen_imgs' else 'original'\n        return ret\n    \n    df = fnames_df\n    valid_names = list(df['name'].iloc[int(split*len(df)):])\n    \n    src1 = ImageList.from_df(df, path = Path('gen_imgs'), cols='name')\n    src2 = ImageList.from_df(df, path = orig_path, cols='name')\n    src1.add(items=src2)\n    \n    src = src1.split_by_valid_func(lambda x : Path(x).name in valid_names)\n    data = src.label_from_func(labeler)\n    data = data.transform(get_transforms(), size=size).databunch(bs=bs).normalize(imagenet_stats)\n    \n    data.c = 3\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_critic = get_critic_data(24)\ndata_critic.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model\nModel used is fast.ai gran_critic with default parameters, it uses spectral normalization, which keeps loss from vanishing or exploding."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())\n\ndef create_critic_learner(data, metrics):\n    return   Learner(data_critic, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd, model_dir=\"/kaggle/working/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic = create_critic_learner(data_critic, accuracy_thresh_expand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic.lr_find()\nlearn_critic.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic.fit_one_cycle(1, 1.3e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_critic.save('critic-pre')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete generated images\n!rm -rf gen_imgs/ tmp.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}