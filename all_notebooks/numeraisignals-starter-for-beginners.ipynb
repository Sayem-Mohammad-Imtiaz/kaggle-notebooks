{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://miro.medium.com/max/3840/1*LHuN1tJt-abIpuX14v5M8w.png)\n\n-----------------------------\nwritten by katsu1110\n\n-----------------------------\n\nThis is yet another starter notebook for the [Numerai Signals](https://signals.numer.ai/). \n\nWhat we do here includes:\n\n- fetch US stock price data via YFinance API\n- merge the data with the Numerai Signals' historical targets\n- perform feature engineering (considering stational features)\n- modeling with XGBoost\n- submit (if you want)\n\nIn a kaggle dataset [YFinance Stock Price Data for Numerai Signals](https://www.kaggle.com/code1110/yfinance-stock-price-data-for-numerai-signals), I fetch the stock price data on a daily basis via the YFinance API. So if you are bothered using the API for yourself, just use this dataset (it must be up-to-date).\n\nThis content is largely inspired by the following starter.\n\n>End to end notebook for Numerai Signals using completely free data from Yahoo Finance, by Jason Rosenfeld (jrAI).\n\nhttps://colab.research.google.com/drive/1ECh69C0LDCUnuyvEmNFZ51l_276nkQqo#scrollTo=tTBUzPep2dm3\n\nAlright, let's get it started!","metadata":{}},{"cell_type":"markdown","source":"# Libraries\nLet's import what we need...","metadata":{}},{"cell_type":"code","source":"!pip install numerapi==2.3.8\nimport numerapi","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:31:37.052819Z","iopub.execute_input":"2021-08-09T11:31:37.053459Z","iopub.status.idle":"2021-08-09T11:31:46.089526Z","shell.execute_reply.started":"2021-08-09T11:31:37.053345Z","shell.execute_reply":"2021-08-09T11:31:46.088429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install xgboost==1.3.0.post0\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:31:46.091632Z","iopub.execute_input":"2021-08-09T11:31:46.092033Z","iopub.status.idle":"2021-08-09T11:32:05.39528Z","shell.execute_reply.started":"2021-08-09T11:31:46.091982Z","shell.execute_reply":"2021-08-09T11:32:05.394419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport pathlib\nfrom tqdm.auto import tqdm\nimport joblib\nimport json\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport requests as re\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta, FR\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib_venn import venn2, venn3\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T11:32:05.397501Z","iopub.execute_input":"2021-08-09T11:32:05.397803Z","iopub.status.idle":"2021-08-09T11:32:06.551443Z","shell.execute_reply.started":"2021-08-09T11:32:05.39777Z","shell.execute_reply":"2021-08-09T11:32:06.5506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config\nA simple config and logging setup.","metadata":{}},{"cell_type":"code","source":"today = datetime.now().strftime('%Y-%m-%d')\ntoday","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:06.552784Z","iopub.execute_input":"2021-08-09T11:32:06.553187Z","iopub.status.idle":"2021-08-09T11:32:06.560297Z","shell.execute_reply.started":"2021-08-09T11:32:06.553156Z","shell.execute_reply":"2021-08-09T11:32:06.559647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config class\nclass CFG:\n    \"\"\"\n    Set FETCH_VIA_API = True if you want to fetch the data via API.\n    Otherwise we use the daily-updated one in the kaggle dataset (faster).\n    \"\"\"\n    INPUT_DIR = '../input/yfinance-stock-price-data-for-numerai-signals'\n    OUTPUT_DIR = './'\n    FETCH_VIA_API = False\n    SEED = 46","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:06.561706Z","iopub.execute_input":"2021-08-09T11:32:06.562074Z","iopub.status.idle":"2021-08-09T11:32:06.571155Z","shell.execute_reply.started":"2021-08-09T11:32:06.562044Z","shell.execute_reply":"2021-08-09T11:32:06.570126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logging is always nice for your experiment:)\ndef init_logger(log_file='train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = init_logger(log_file=f'{CFG.OUTPUT_DIR}/{today}.log')\nlogger.info('Start Logging...')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:06.572335Z","iopub.execute_input":"2021-08-09T11:32:06.572655Z","iopub.status.idle":"2021-08-09T11:32:06.588151Z","shell.execute_reply.started":"2021-08-09T11:32:06.572627Z","shell.execute_reply":"2021-08-09T11:32:06.587429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup Numerai API\nFirst of all, let's set up the numerai signals API. \n\nWe can do many things with this API: \n\n- get a ticker map (between yfinance data and numerai historical targets)\n- get the historical targets\n- get your model slot name and model_id (if private key and secret key are provided)\n- submit\n\n(well, maybe more)","metadata":{}},{"cell_type":"markdown","source":"## Get Tickers for Numerai Signals\nLet's first get the ticker map.","metadata":{}},{"cell_type":"code","source":"napi = numerapi.SignalsAPI()\nlogger.info('numerai api setup!')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:06.589225Z","iopub.execute_input":"2021-08-09T11:32:06.58952Z","iopub.status.idle":"2021-08-09T11:32:06.600912Z","shell.execute_reply.started":"2021-08-09T11:32:06.589491Z","shell.execute_reply":"2021-08-09T11:32:06.599971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read in list of active Signals tickers which can change slightly era to era\neligible_tickers = pd.Series(napi.ticker_universe(), name='ticker') \nlogger.info(f\"Number of eligible tickers: {len(eligible_tickers)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:06.604189Z","iopub.execute_input":"2021-08-09T11:32:06.604813Z","iopub.status.idle":"2021-08-09T11:32:06.825391Z","shell.execute_reply.started":"2021-08-09T11:32:06.604766Z","shell.execute_reply":"2021-08-09T11:32:06.824165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read in yahoo to numerai ticker map, still a work in progress, h/t wsouza and \n# this tickermap is a work in progress and not guaranteed to be 100% correct\nticker_map = pd.read_csv('https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv')\nticker_map = ticker_map[ticker_map.bloomberg_ticker.isin(eligible_tickers)]\n\nnumerai_tickers = ticker_map['ticker']\nyfinance_tickers = ticker_map['yahoo']\nlogger.info(f\"Number of eligible tickers in map: {len(ticker_map)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:06.827249Z","iopub.execute_input":"2021-08-09T11:32:06.827599Z","iopub.status.idle":"2021-08-09T11:32:07.040843Z","shell.execute_reply.started":"2021-08-09T11:32:06.827563Z","shell.execute_reply":"2021-08-09T11:32:07.039898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ticker_map.shape)\nticker_map.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:07.042136Z","iopub.execute_input":"2021-08-09T11:32:07.042456Z","iopub.status.idle":"2021-08-09T11:32:07.059393Z","shell.execute_reply.started":"2021-08-09T11:32:07.042425Z","shell.execute_reply":"2021-08-09T11:32:07.058488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This ticker map is necessary for a successful submission if you use yfinance data.","metadata":{}},{"cell_type":"markdown","source":"# Load Stock Price Data\nNow is the time to get the stock price data, fetched via the [YFiance API](https://pypi.org/project/yfinance/).\n\nThe good thing with this API is that it is free of charge.\n\nThe bad thing with this API is that the data is often not complete.\n\nFor a better quality of stock price data, you might want to try out purchasing one from [Quandl](https://www.quandl.com/data/EOD-End-of-Day-US-Stock-Prices/documentation?anchor=overview).\n\nThis is another starter using Quandl data:\nhttps://forum.numer.ai/t/signals-plugging-in-the-data-from-quandl/2431\n\nThis is of course wonderful, but if you are a beginner, why not just start with a free one?","metadata":{}},{"cell_type":"code","source":"# If you want to fetch the data on your own, you can use this function...\n\ndef fetch_yfinance(ticker_map, start='2002-12-01'):\n    \"\"\"\n    # fetch yfinance data\n    :INPUT:\n    - ticker_map : Numerai eligible ticker map (pd.DataFrame)\n    - start : date (str)\n    \n    :OUTPUT:\n    - full_data : pd.DataFrame ('date', 'ticker', 'close', 'raw_close', 'high', 'low', 'open', 'volume')\n    \"\"\"\n    \n    # ticker map\n    numerai_tickers = ticker_map['ticker']\n    yfinance_tickers = ticker_map['yahoo']\n\n    # fetch\n    raw_data = yfinance.download(\n        yfinance_tickers.str.cat(sep=' '), \n        start=start, \n        threads=True\n    ) \n    \n    # format\n    cols = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n    full_data = raw_data[cols].stack().reset_index()\n    full_data.columns = ['date', 'ticker', 'close', 'raw_close', 'high', 'low', 'open', 'volume']\n    \n    # map yfiance ticker to numerai tickers\n    full_data['ticker'] = full_data.ticker.map(\n        dict(zip(yfinance_tickers, numerai_tickers))\n    )\n    return full_data","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:07.06073Z","iopub.execute_input":"2021-08-09T11:32:07.061031Z","iopub.status.idle":"2021-08-09T11:32:07.068542Z","shell.execute_reply.started":"2021-08-09T11:32:07.060997Z","shell.execute_reply":"2021-08-09T11:32:07.067495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif CFG.FETCH_VIA_API: # fetch data via api\n    logger.info('Fetch data via API...may take some time...')\n    !pip install yfinance==0.1.62\n    !pip install simplejson\n    import yfinance\n    import simplejson\n    \n    df = fetch_yfinance(ticker_map, start='2002-12-01')\nelse: # loading from the kaggle dataset (https://www.kaggle.com/code1110/yfinance-stock-price-data-for-numerai-signals)\n    logger.info('Load data from the kaggle dataset...')\n    df = pd.read_csv(pathlib.Path(f'{CFG.INPUT_DIR}/full_data.csv'))\n\nprint(df.shape)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:07.070056Z","iopub.execute_input":"2021-08-09T11:32:07.07045Z","iopub.status.idle":"2021-08-09T11:32:57.617736Z","shell.execute_reply.started":"2021-08-09T11:32:07.070382Z","shell.execute_reply":"2021-08-09T11:32:57.616757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:57.619889Z","iopub.execute_input":"2021-08-09T11:32:57.620355Z","iopub.status.idle":"2021-08-09T11:32:57.638303Z","shell.execute_reply.started":"2021-08-09T11:32:57.620299Z","shell.execute_reply":"2021-08-09T11:32:57.636884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Targets for Numerai Signals\nFor a supervised machine learning, we need a target label. That is available in the Numerai Signals, so we can just fetch it.","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef read_numerai_signals_targets():\n    # read in Signals targets\n    numerai_targets = 'https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val.csv'\n    targets = pd.read_csv(numerai_targets)\n    \n    # to datetime int\n    targets['friday_date'] = pd.to_datetime(targets['friday_date'].astype(str), format='%Y-%m-%d').dt.strftime('%Y%m%d').astype(int)\n    \n#     # train, valid split\n#     train_targets = targets.query('data_type == \"train\"')\n#     valid_targets = targets.query('data_type == \"validation\"')\n    \n    return targets\n\ntargets = read_numerai_signals_targets()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:32:57.639875Z","iopub.execute_input":"2021-08-09T11:32:57.64048Z","iopub.status.idle":"2021-08-09T11:33:40.190861Z","shell.execute_reply.started":"2021-08-09T11:32:57.640432Z","shell.execute_reply":"2021-08-09T11:33:40.189699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(targets.shape, targets['friday_date'].min(), targets['friday_date'].max())\ntargets.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:40.192358Z","iopub.execute_input":"2021-08-09T11:33:40.192995Z","iopub.status.idle":"2021-08-09T11:33:40.222882Z","shell.execute_reply.started":"2021-08-09T11:33:40.192926Z","shell.execute_reply":"2021-08-09T11:33:40.221985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:40.224052Z","iopub.execute_input":"2021-08-09T11:33:40.224352Z","iopub.status.idle":"2021-08-09T11:33:40.235486Z","shell.execute_reply.started":"2021-08-09T11:33:40.224321Z","shell.execute_reply":"2021-08-09T11:33:40.234473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are train and validation...\nfig, ax = plt.subplots(1, 2, figsize=(16, 4))\nax = ax.flatten()\n\nfor i, data_type in enumerate(['train', 'validation']):\n    # slice\n    targets_ = targets.query(f'data_type == \"{data_type}\"')\n    logger.info('*' * 50)\n    logger.info('{} target: {:,} tickers (friday_date: {} - {})'.format(\n        data_type, \n        targets_['ticker'].nunique(),\n        targets_['friday_date'].min(),\n        targets_['friday_date'].max(),\n    ))\n    \n    # plot target\n    ax[i].hist(targets_['target'])\n    ax[i].set_title(f'{data_type}')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:40.236802Z","iopub.execute_input":"2021-08-09T11:33:40.237105Z","iopub.status.idle":"2021-08-09T11:33:42.413663Z","shell.execute_reply.started":"2021-08-09T11:33:40.237076Z","shell.execute_reply":"2021-08-09T11:33:42.412569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target looks exactly like the one from the Numerai Tournament, where both features and targets are given to the participants.\n\nAlso note that the train-validation split is based on time (i.e., Time-Series Split):\n\n- train friday_date: 20030131 - 20121228\n- validation friday_date: 20130104 - 20200228","metadata":{}},{"cell_type":"markdown","source":"## Check Ticker Overlaps\nLet's see if we have enough overlap of tickers between our yfiance stock data and the numerai targets. We need at least 5 tickers for submission.","metadata":{}},{"cell_type":"code","source":"# ticker overlap\nvenn3(\n    [\n        set(df['ticker'].unique().tolist())\n        , set(targets.query('data_type == \"train\"')['ticker'].unique().tolist())\n        , set(targets.query('data_type == \"validation\"')['ticker'].unique().tolist())\n    ],\n    set_labels=('yf price', 'train target', 'valid target')\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:42.415271Z","iopub.execute_input":"2021-08-09T11:33:42.415589Z","iopub.status.idle":"2021-08-09T11:33:44.826244Z","shell.execute_reply.started":"2021-08-09T11:33:42.415557Z","shell.execute_reply":"2021-08-09T11:33:44.825042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ah, yeah, not bad, I guess? \n\nHere I only use our stock price data which have ticker overlaps such that we can build a supervised machine learning model.","metadata":{}},{"cell_type":"code","source":"# select target-only tickers\ndf = df.loc[df['ticker'].isin(targets['ticker'])].reset_index(drop=True)\n\nprint('{:,} tickers: {:,} records'.format(df['ticker'].nunique(), len(df)))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:44.828093Z","iopub.execute_input":"2021-08-09T11:33:44.828552Z","iopub.status.idle":"2021-08-09T11:33:52.893537Z","shell.execute_reply.started":"2021-08-09T11:33:44.828502Z","shell.execute_reply":"2021-08-09T11:33:52.892314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As I mentioned earlier, the yfiance stock data is not complete. Let's see if we have enough records per ticker.","metadata":{}},{"cell_type":"code","source":"record_per_ticker = df.groupby('ticker')['date'].nunique().reset_index().sort_values(by='date')\nrecord_per_ticker","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:52.89507Z","iopub.execute_input":"2021-08-09T11:33:52.895521Z","iopub.status.idle":"2021-08-09T11:33:59.370708Z","shell.execute_reply.started":"2021-08-09T11:33:52.895472Z","shell.execute_reply":"2021-08-09T11:33:59.369802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record_per_ticker['date'].hist()\nprint(record_per_ticker['date'].describe())","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:59.372262Z","iopub.execute_input":"2021-08-09T11:33:59.372607Z","iopub.status.idle":"2021-08-09T11:33:59.552279Z","shell.execute_reply.started":"2021-08-09T11:33:59.372573Z","shell.execute_reply":"2021-08-09T11:33:59.551223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are unfortunately some tickers where the number of records is small. \n\nHere I only use tickers with more than 1,000 records.","metadata":{}},{"cell_type":"code","source":"tickers_with_records = record_per_ticker.query('date >= 1000')['ticker'].values\ndf = df.loc[df['ticker'].isin(tickers_with_records)].reset_index(drop=True)\n\nprint('Here, we use {:,} tickers: {:,} records'.format(df['ticker'].nunique(), len(df)))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:33:59.553745Z","iopub.execute_input":"2021-08-09T11:33:59.554058Z","iopub.status.idle":"2021-08-09T11:34:06.64093Z","shell.execute_reply.started":"2021-08-09T11:33:59.554023Z","shell.execute_reply":"2021-08-09T11:34:06.63999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nYeah finally machine learning part!\n\nHere we generate sets of stock price features. There are some caveats to be aware of:\n\n- **No Leak**: we cannot use a feature which uses the future information (this is a forecasting task!)\n- **Stationaly features**: Our features have to work whenever (scales must be stationaly over the periods of time)\n\nThe implementation of the feature engineering is derived from [J-Quants Tournament](https://japanexchangegroup.github.io/J-Quants-Tutorial/#anchor-2.7). Although this content is in Japanese, I believe this is one of the best resources for feature engineering in the finance domain. \n\nAlso I add the RSI and MACD (PPO) features as a bonus:D\n\nWe generate features per ticker repeatedly. To accelerate the process, we use the parallel processing.","metadata":{}},{"cell_type":"code","source":"# first, fix date column in the yfiance stock data to be friday date (just naming along with numerai targets)\ndf['friday_date'] = df['date'].apply(lambda x : int(str(x).replace('-', '')))\ndf.tail(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:34:06.644485Z","iopub.execute_input":"2021-08-09T11:34:06.644772Z","iopub.status.idle":"2021-08-09T11:34:28.189652Z","shell.execute_reply.started":"2021-08-09T11:34:06.644742Z","shell.execute_reply":"2021-08-09T11:34:28.188581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ready for feature engineering?","metadata":{}},{"cell_type":"code","source":"# technical indicators\ndef RSI(close: pd.DataFrame, period: int = 14) -> pd.Series:\n    # https://gist.github.com/jmoz/1f93b264650376131ed65875782df386\n    \"\"\"See source https://github.com/peerchemist/finta\n    and fix https://www.tradingview.com/wiki/Talk:Relative_Strength_Index_(RSI)\n    Relative Strength Index (RSI) is a momentum oscillator that measures the speed and change of price movements.\n    RSI oscillates between zero and 100. Traditionally, and according to Wilder, RSI is considered overbought when above 70 and oversold when below 30.\n    Signals can also be generated by looking for divergences, failure swings and centerline crossovers.\n    RSI can also be used to identify the general trend.\"\"\"\n\n    delta = close.diff()\n\n    up, down = delta.copy(), delta.copy()\n    up[up < 0] = 0\n    down[down > 0] = 0\n\n    _gain = up.ewm(com=(period - 1), min_periods=period).mean()\n    _loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n\n    RS = _gain / _loss\n    return pd.Series(100 - (100 / (1 + RS)))\n\ndef EMA1(x, n):\n    \"\"\"\n    https://qiita.com/MuAuan/items/b08616a841be25d29817\n    \"\"\"\n    a= 2/(n+1)\n    return pd.Series(x).ewm(alpha=a).mean()\n\ndef MACD(close : pd.DataFrame, span1=12, span2=26, span3=9):\n    \"\"\"\n    Compute MACD\n    # https://www.learnpythonwithrune.org/pandas-calculate-the-moving-average-convergence-divergence-macd-for-a-stock/\n    \"\"\"\n    exp1 = EMA1(close, span1)\n    exp2 = EMA1(close, span2)\n    macd = exp1 - exp2\n    signal = EMA1(macd, span3)\n\n    return macd, signal\n\ndef feature_engineering(ticker='ZEAL DC', df=df):\n    \"\"\"\n    feature engineering\n    \n    :INPUTS:\n    - ticker : numerai ticker name (str)\n    - df : yfinance dataframe (pd.DataFrame)\n    \n    :OUTPUTS:\n    - feature_df : feature engineered dataframe (pd.DataFrame)\n    \"\"\"\n    # init\n    keys = ['friday_date', 'ticker']\n    feature_df = df.query(f'ticker == \"{ticker}\"')\n    \n    # price features\n    new_feats = []\n    for i, f in enumerate(['close', ]):\n        for x in [20, 40, 60, ]:\n            # return\n            feature_df[f\"{f}_return_{x}days\"] = feature_df[\n                f\n            ].pct_change(x)\n\n            # volatility\n            feature_df[f\"{f}_volatility_{x}days\"] = (\n                np.log1p(feature_df[f])\n                .pct_change()\n                .rolling(x)\n                .std()\n            )\n        \n            # kairi mean\n            feature_df[f\"{f}_MA_gap_{x}days\"] = feature_df[f] / (\n                feature_df[f].rolling(x).mean()\n            )\n            \n            # features to use\n            new_feats += [\n                f\"{f}_return_{x}days\", \n                f\"{f}_volatility_{x}days\",\n                f\"{f}_MA_gap_{x}days\",\n                         ]\n\n    # RSI\n    feature_df['RSI'] = RSI(feature_df['close'], 14)\n\n    # MACD\n    macd, macd_signal = MACD(feature_df['close'], 12, 26, 9) \n    feature_df['MACD'] = 100 * macd / macd_signal\n\n    new_feats += ['RSI', 'MACD', ]\n\n    # only new feats\n    feature_df = feature_df[new_feats + keys]\n\n    # fill nan\n    feature_df.fillna(method='ffill', inplace=True) # safe fillna method for a forecasting task\n    feature_df.fillna(method='bfill', inplace=True) # just in case ... making sure no nan\n\n    return feature_df\n\ndef add_features(df):\n    # FE with multiprocessing\n    tickers = df['ticker'].unique().tolist()\n    print('FE for {:,} stocks...using {:,} CPUs...'.format(len(tickers), cpu_count()))\n    start_time = time.time()\n    with Pool(cpu_count()) as p:\n        feature_dfs = list(tqdm(p.imap(feature_engineering, tickers), total=len(tickers)))\n    return pd.concat(feature_dfs)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:34:28.191661Z","iopub.execute_input":"2021-08-09T11:34:28.191952Z","iopub.status.idle":"2021-08-09T11:34:28.211483Z","shell.execute_reply.started":"2021-08-09T11:34:28.191921Z","shell.execute_reply":"2021-08-09T11:34:28.210427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfeature_df = add_features(df)\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:34:28.212867Z","iopub.execute_input":"2021-08-09T11:34:28.213235Z","iopub.status.idle":"2021-08-09T11:53:44.172093Z","shell.execute_reply.started":"2021-08-09T11:34:28.213203Z","shell.execute_reply":"2021-08-09T11:53:44.170842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(feature_df.shape)\nfeature_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:53:44.173933Z","iopub.execute_input":"2021-08-09T11:53:44.174363Z","iopub.status.idle":"2021-08-09T11:53:44.199306Z","shell.execute_reply.started":"2021-08-09T11:53:44.174313Z","shell.execute_reply":"2021-08-09T11:53:44.198332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:53:44.200851Z","iopub.execute_input":"2021-08-09T11:53:44.201165Z","iopub.status.idle":"2021-08-09T11:53:44.223065Z","shell.execute_reply.started":"2021-08-09T11:53:44.201132Z","shell.execute_reply":"2021-08-09T11:53:44.222125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merge Targets and Features\nFeature engineering is done. Let's merge it with the numerai historical targets.","metadata":{}},{"cell_type":"code","source":"# do we have enough overlap with respect to 'friday_date'?\nvenn2([\n    set(feature_df['friday_date'].astype(str).unique().tolist())\n    , set(targets['friday_date'].astype(str).unique().tolist())\n], set_labels=('features_days', 'targets_days'))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:53:44.224324Z","iopub.execute_input":"2021-08-09T11:53:44.224652Z","iopub.status.idle":"2021-08-09T11:54:16.031639Z","shell.execute_reply.started":"2021-08-09T11:53:44.22462Z","shell.execute_reply":"2021-08-09T11:54:16.030507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do we have enough overlap with respect to 'ticker'?\nvenn2([\n    set(feature_df['ticker'].astype(str).unique().tolist())\n    , set(targets['ticker'].astype(str).unique().tolist())\n], set_labels=('features_ticker', 'targets_ticker'))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:54:16.03306Z","iopub.execute_input":"2021-08-09T11:54:16.033338Z","iopub.status.idle":"2021-08-09T11:54:18.35205Z","shell.execute_reply.started":"2021-08-09T11:54:16.03331Z","shell.execute_reply":"2021-08-09T11:54:18.351306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge\nfeature_df['friday_date'] = feature_df['friday_date'].astype(int)\ntargets['friday_date'] = targets['friday_date'].astype(int)\n\nfeature_df = feature_df.merge(\n    targets,\n    how='left',\n    on=['friday_date', 'ticker']\n)\n\nprint(feature_df.shape)\nfeature_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:54:18.353Z","iopub.execute_input":"2021-08-09T11:54:18.353266Z","iopub.status.idle":"2021-08-09T11:54:32.663778Z","shell.execute_reply.started":"2021-08-09T11:54:18.353239Z","shell.execute_reply":"2021-08-09T11:54:32.662704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save (just to make sure that we are on the safe side if yfinance is dead some day...)\nfeature_df.to_pickle(f'{CFG.OUTPUT_DIR}/feature_df.pkl')\nfeature_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T11:54:32.665433Z","iopub.execute_input":"2021-08-09T11:54:32.665862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a merged features + target table! It seems like we are ready for modeling.","metadata":{}},{"cell_type":"markdown","source":"# Modeling\nYay, finally!\n\nHere let's use XGBoost. \n\nThe hyperparameters are derived from the Integration-Test, which is an example yet a strong baseline for the Numerai Tournament.","metadata":{}},{"cell_type":"code","source":"target = 'target'\ndrops = ['data_type', target, 'friday_date', 'ticker']\nfeatures = [f for f in feature_df.columns.values.tolist() if f not in drops]\n\nlogger.info('{:,} features: {}'.format(len(features), features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train-valid split\ntrain_set = {\n    'X': feature_df.query('data_type == \"train\"')[features], \n    'y': feature_df.query('data_type == \"train\"')[target].astype(np.float64)\n}\nval_set = {\n    'X': feature_df.query('data_type == \"validation\"')[features], \n    'y': feature_df.query('data_type == \"validation\"')[target].astype(np.float64)\n}\n\nassert train_set['y'].isna().sum() == 0\nassert val_set['y'].isna().sum() == 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same parameters of the Integration-Test\nimport joblib\nfrom sklearn import utils\nimport xgboost as xgb\nimport operator\n\nparams = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    'colsample_bytree': 0.1,                 \n    'learning_rate': 0.01,\n    'max_depth': 5,\n    'seed': 46,\n    'n_estimators': 2000,\n#     'tree_method': 'gpu_hist' # if you want to use GPU ...\n}\n\n# define \nmodel = xgb.XGBRegressor(**params)\n\n# fit\nmodel.fit(\n    train_set['X'], train_set['y'], \n    eval_set=[(val_set['X'], val_set['y'])],\n    verbose=100, \n    early_stopping_rounds=100,\n)\n\n# save model\njoblib.dump(model, f'{CFG.OUTPUT_DIR}/xgb_model_val.pkl')\nlogger.info('xgb model with early stopping saved!')\n\n# feature importance\nimportance = model.get_booster().get_score(importance_type='gain')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\nfeature_importance_df = pd.DataFrame(importance, columns=['features', 'importance'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature importance\nfig, ax = plt.subplots(1, 1, figsize=(12, 10))\nsns.barplot(\n    x='importance', \n    y='features', \n    data=feature_importance_df.sort_values(by='importance', ascending=False),\n    ax=ax\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like 'price gap the moving average' kinds of features are good signals!","metadata":{}},{"cell_type":"markdown","source":"# Validation Score\nThe following snipets are derived from \n\nhttps://colab.research.google.com/drive/1ECh69C0LDCUnuyvEmNFZ51l_276nkQqo#scrollTo=tTBUzPep2dm3\n\nLet's see how good our model predictions on the validation data are.\n\nGood? It's good, isn't it?","metadata":{}},{"cell_type":"code","source":"# https://colab.research.google.com/drive/1ECh69C0LDCUnuyvEmNFZ51l_276nkQqo#scrollTo=tTBUzPep2dm3\n\ndef score(df, target_name=target, pred_name='prediction'):\n    '''Takes df and calculates spearm correlation from pre-defined cols'''\n    # method=\"first\" breaks ties based on order in array\n    return np.corrcoef(\n        df[target_name],\n        df[pred_name].rank(pct=True, method=\"first\")\n    )[0,1]\n\ndef run_analytics(era_scores):\n    print(f\"Mean Correlation: {era_scores.mean():.4f}\")\n    print(f\"Median Correlation: {era_scores.median():.4f}\")\n    print(f\"Standard Deviation: {era_scores.std():.4f}\")\n    print('\\n')\n    print(f\"Mean Pseudo-Sharpe: {era_scores.mean()/era_scores.std():.4f}\")\n    print(f\"Median Pseudo-Sharpe: {era_scores.median()/era_scores.std():.4f}\")\n    print('\\n')\n    print(f'Hit Rate (% positive eras): {era_scores.apply(lambda x: np.sign(x)).value_counts()[1]/len(era_scores):.2%}')\n\n    era_scores.rolling(10).mean().plot(kind='line', title='Rolling Per Era Correlation Mean', figsize=(15,4))\n    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()\n\n    era_scores.cumsum().plot(title='Cumulative Sum of Era Scores', figsize=(15,4))\n    plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction for the validation set\nvalid_sub = feature_df.query('data_type == \"validation\"')[drops].copy()\nvalid_sub['prediction'] = model.predict(val_set['X'])\n\n# compute score\nval_era_scores = valid_sub.copy()\nval_era_scores['friday_date'] = val_era_scores['friday_date'].astype(str)\nval_era_scores = val_era_scores.loc[val_era_scores['prediction'].isna() == False].groupby(['friday_date']).apply(score)\nrun_analytics(val_era_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, I guess it is fairly good as a starter, isn't it?","metadata":{}},{"cell_type":"markdown","source":"# Submission\nLet's use this trained model to make a submission for the Numerai Signals.\n\nNote that, again, yfinance data is not complete. Sometimes there is no recent data available for many tickers;(\n\nWe need at least 5 tickers for a successful submission. Let's first check if we have at least 5 tickers in which the recent friday_date for them is indeed the recent friday date.","metadata":{}},{"cell_type":"code","source":"# recent friday date?\nrecent_friday = datetime.now() + relativedelta(weekday=FR(-1))\nrecent_friday = int(recent_friday.strftime('%Y%m%d'))\nprint(f'Most recent Friday: {recent_friday}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# in case no recent friday is available...prep the second last\nrecent_friday2 = datetime.now() + relativedelta(weekday=FR(-2))\nrecent_friday2 = int(recent_friday2.strftime('%Y%m%d'))\nprint(f'Second most recent Friday: {recent_friday2}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do we have at least 5 tickers, whose the latest date matches the recent friday?\nticker_date_df = feature_df.groupby('ticker')['friday_date'].max().reset_index()\nif len(ticker_date_df.loc[ticker_date_df['friday_date'] == recent_friday]) >= 5:\n    ticker_date_df = ticker_date_df.loc[ticker_date_df['friday_date'] == recent_friday]\nelse: # use dates later than the second last friday\n    ticker_date_df = ticker_date_df.loc[ticker_date_df['friday_date'] == recent_friday2]\n    recent_friday = recent_friday2\n    \nprint(len(ticker_date_df))\nticker_date_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good! That's fairly enough. So we only perform the inference on those tickers and submit!","metadata":{}},{"cell_type":"code","source":"# live sub\nfeature_df.loc[feature_df['friday_date'] == recent_friday, 'data_type'] = 'live'\ntest_sub = feature_df.query('data_type == \"live\"')[drops].copy()\ntest_sub['prediction'] = model.predict(feature_df.query('data_type == \"live\"')[features])\n\nlogger.info(test_sub.shape)\ntest_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# histogram of prediction\ntest_sub['prediction'].hist(bins=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's submit! What is good with the Numerai Signals is that if you submit your predictions on the validation data, on the website, you can get more information about your model performance such as APY.","metadata":{}},{"cell_type":"code","source":"# To submit, you need to have Numerai account and have API's id and secret key. Also you need to have at least one (numerai signals') model slot.\ndef submit_signal(sub: pd.DataFrame, public_id: str, secret_key: str, slot_name: str):\n    \"\"\"\n    submit numerai signals prediction\n    \"\"\"\n    # setup private API\n    napi = numerapi.SignalsAPI(public_id, secret_key)\n    \n    # write predictions to csv\n    model_id = napi.get_models()[f'{slot_name}']\n    filename = f\"sub_{model_id}.csv\"\n    sub.to_csv(filename, index=False)\n    \n    # submit\n    submission = napi.upload_predictions(filename, model_id=model_id)\n    print(f'Submitted : {slot_name}!')\n    \n# concat valid and test \nsub = pd.concat([valid_sub, test_sub], ignore_index=True)\n\n# rename to 'signal'\nsub.rename(columns={'prediction': 'signal'}, inplace=True)\n\n# select necessary columns\nsub = sub[['ticker', 'friday_date', 'data_type', 'signal']]\n\npublic_id = '<Your Numerai API ID>'\nsecret_key = '<Your Numerai Secret Key>'\nslot_name = '<Your Numerai Signals Submission Slot Name>'\n# submit_signal(sub, public_id, secret_key, slot_name) # uncomment if you submit\n\n# save \nsub.to_csv(f'{CFG.OUTPUT_DIR}/example_submission_{today}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ALL DONE!\n\nOf course, this is just another starter, and there are plenty rooms left to be improved.\n\n- Feature engineering (more stock price, volume features? Be careful for leaks and non-stationality)\n- Modeling (another model? another validation strategy? training period?)\n- Target (Should we simply use the historical targets? Can we make a new one?)\n- Dataset (is yfinance sufficient for stable weekly performance?)\n\n...potentially more...\n\nHave fun with Numerai Signals!","metadata":{}}]}