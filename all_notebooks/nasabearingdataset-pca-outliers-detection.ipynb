{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom matplotlib import gridspec\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Have a look to a sample time serie\nSet No. 2, first file:\n- 2kHz acquisition rate, 1s in total","metadata":{}},{"cell_type":"code","source":"sample2 = pd.read_csv(\"/kaggle/input/nasa-bearing-dataset-aggregated-sets-no-1-2-3/sample_2nd.csv\",index_col=0)\nsample2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure(figsize=(15, 6), dpi=80)\nsample2['Bearing 1'].loc[:1000].plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset preprocessing","metadata":{}},{"cell_type":"code","source":"# Read the CSV file and set first column as the dataframe index\ndataset = pd.read_csv(\"/kaggle/input/nasa-bearing-dataset-aggregated-sets-no-1-2-3/merged_dataset_BearingTest_2.csv\",\n                      index_col=0)\ndataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize the dataset","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\n# Dataset is scaled so that maximum for every column is 1\nscaler = preprocessing.MinMaxScaler()\ndataset_scaled = pd.DataFrame(scaler.fit_transform(dataset), \n                              columns=dataset.columns, \n                              index=dataset.index)\ndataset_scaled.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ticks_span = 50\n\ndataset_scaled.plot(figsize=(18, 4))\nplt.xlabel('Timestamp')\nplt.xticks(np.arange(0, dataset_scaled.shape[0], x_ticks_span), fontsize=10, rotation = 30)\nplt.ylabel('Acceleration')\n\n\nplt.legend(loc=\"upper left\")  \nplt.title('Time series for all 4 accelerometers (normalized signals)', fontweight =\"bold\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build train and test datasets\n- We want the training set contains only \"normal\" data\n- The rest of points will be in the test set, that will contain both \"normal\" and anomalous data","metadata":{}},{"cell_type":"code","source":"print(\"dataset_scaled shape is\",dataset_scaled.shape,\"\\n\\n\", dataset_scaled.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = dataset_scaled[:'2004-02-13 23:52:39']\ndataset_test  = dataset_scaled['2004-02-14 00:02:39':]\n# Random shuffle training data\ndataset_train.sample(frac=1)\n\nprint(\"Train dataset has lenght\", dataset_train.shape[0], \"while test dataset is\", dataset_test.shape[0],\n      \"TOTAL=\", dataset_train.shape[0]+dataset_test.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_ticks_span = 50\n\ndataset_train.plot(figsize = (6,6), title ='Left time series with \"normal\" data (normalized signals)')\nplt.xticks(np.arange(0, dataset_train.shape[0], x_ticks_span), fontsize=10, rotation = 30)\nplt.ylim(0,1)\nplt.legend(loc=\"upper left\")  \nplt.show()\n\ndataset_test.plot(figsize = (18,6), title='Right time series with \"normal\" & \"anomalous\" data (normalized signals)')\nplt.xticks(np.arange(0, dataset_test.shape[0], x_ticks_span), fontsize=10, rotation = 30)\nplt.ylim(0,1)\nplt.legend(loc=\"upper left\")  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3, figsize=(18, 5))\nfig.suptitle('TRAINING SET: Comparison against Bearing 1', fontsize=20)\n\naxes[0].scatter(np.array(dataset_train['Bearing 1']), np.array(dataset_train['Bearing 2']))\naxes[0].set_xlabel('Bearing 1')\naxes[0].set_ylabel('Bearing 2')\naxes[0].set_title('Bearing 1 vs. 2')\naxes[0].set_xlim(0,1)\naxes[0].set_ylim(0,1)\n\naxes[1].scatter(np.array(dataset_train['Bearing 1']), np.array(dataset_train['Bearing 3']))\naxes[1].set_xlabel('Bearing 1')\naxes[1].set_ylabel('Bearing 3')\naxes[1].set_title('Bearings 1 vs. 3')\naxes[1].set_xlim(0,1)\naxes[1].set_ylim(0,1)\n\naxes[2].scatter(np.array(dataset_train['Bearing 1']), np.array(dataset_train['Bearing 3']))\naxes[2].set_xlabel('Bearing 1')\naxes[2].set_ylabel('Bearing 4')\naxes[2].set_title('Bearings 1 vs. 4')\naxes[2].set_xlim(0,1)\naxes[2].set_ylim(0,1)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3, figsize=(15, 5))\nfig.suptitle('TEST SET: Comparison against Bearing 1', fontsize=20)\n\naxes[0].scatter(np.array(dataset_test['Bearing 1']), np.array(dataset_test['Bearing 2']))\naxes[0].set_xlabel('Bearing 1')\naxes[0].set_ylabel('Bearing 2')\naxes[0].set_xlim(0,1)\naxes[0].set_ylim(0,1)\naxes[0].set_title('Bearing 1 vs. 2')\n\naxes[1].scatter(np.array(dataset_test['Bearing 1']), np.array(dataset_test['Bearing 3']))\naxes[1].set_xlabel('Bearing 1')\naxes[1].set_ylabel('Bearing 3')\naxes[1].set_xlim(0,1)\naxes[1].set_ylim(0,1)\naxes[1].set_title('Bearings 1 vs. 3')\n\naxes[2].scatter(np.array(dataset_test['Bearing 1']), np.array(dataset_test['Bearing 4']))\naxes[2].set_xlabel('Bearing 1')\naxes[2].set_ylabel('Bearing 4')\naxes[2].set_xlim(0,1)\naxes[2].set_ylim(0,1)\naxes[2].set_title('Bearings 1 vs. 4')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA model: Principal Components analysis\nApply dimensionality reduction to scale down from 4 dimensions to only 2 signal","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nn_components = 4 # How many dimensions you want to reduce to\npca = PCA(n_components=n_components, svd_solver= 'full')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute all PCA components FOR THE TRAINING SET\nX_train_PCA = pca.fit_transform(dataset_train)\nX_train_PCA = pd.DataFrame(X_train_PCA)\nX_train_PCA.index = dataset_train.index\n\n# Project the TEST SET onto the PCA space\nX_test_PCA = pca.transform(dataset_test)\nX_test_PCA = pd.DataFrame(X_test_PCA)\nX_test_PCA.index = dataset_test.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize=(12, 5))\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\nfig.suptitle('TRAINING & TEST datasets in PCA axes', fontsize=20)\n\nax0 = plt.subplot(gs[0])\nax0.scatter(X_train_PCA.loc[:,0], X_train_PCA.loc[:,1])\nax0.set_xlabel('PC1')\nax0.set_ylabel('PC2')\nax0.set_xlim(-0.1,0.1)\nax0.set_ylim(-0.1,0.1)\nax0.set_title('Training set Principal Components')\n\nax1 = plt.subplot(gs[1])\nax1.scatter(X_test_PCA.loc[:,0], X_test_PCA.loc[:,1])\nax1.set_xlabel('PC1')\nax1.set_ylabel('PC2')\nax1.set_title('Test set Principal Components')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variance ratio (as per training set)\nWe can see that in the PCA space, the variance is maximized along PC1 (explains 93% of the variance) and PC2 (explains 5.5%)\n- Components are listed per importance (greater variance contribution)","metadata":{}},{"cell_type":"code","source":"np.set_printoptions(precision=3, suppress=True) # 3 decimal places and don't use scientific notation\n\npca.fit_transform(dataset_train)\nprint(pca.explained_variance_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(precision=1, suppress=False) # Use scientific notation\nprint(pca.explained_variance_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As per the result above, components 1 and 2 accounts for the 76.8 + 14.1 = 90.9% of the total variance","metadata":{}},{"cell_type":"markdown","source":"## Reduce the analysis to the first two PCA components","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components= 2, svd_solver= 'full')\n\n# Compute (2) PCA most relevant components FOR THE TRAINING SET\nX_train_PCA = pca.fit_transform(dataset_train)\nX_train_PCA = pd.DataFrame(X_train_PCA)\nX_train_PCA.index = dataset_train.index\n\n# Project the TEST SET onto the PCA space (2 dimensions)\nX_test_PCA = pca.transform(dataset_test)\nX_test_PCA = pd.DataFrame(X_test_PCA)\nX_test_PCA.index = dataset_test.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Computing the Mahalanobis distance\nThe Mahalanobis distance is widely used in cluster analysis and classification techniques. In order to use the Mahalanobis distance to classify a test point as belonging to one of N classes, one first estimates the covariance matrix of each class, usually based on samples known to belong to each class.\n\nIn our case, as we are only interested in classifying “normal” vs “anomaly”, we use training data that only contains normal operating conditions to calculate the covariance matrix.\n  - This explains while we built the training dataset as explained above","metadata":{}},{"cell_type":"markdown","source":"### Functions' definition","metadata":{}},{"cell_type":"code","source":"# CALCULATE THE COVARIANCE\ndef cov_matrix(data, verbose=False):\n    covariance_matrix = np.cov(data, rowvar=False)\n    if is_pos_def(covariance_matrix):\n        inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n        if is_pos_def(inv_covariance_matrix):\n            return covariance_matrix, inv_covariance_matrix\n        else:\n            print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n    else:\n        print(\"Error: Covariance Matrix is not positive definite!\")\n        \n# CALCULATE THE MAHALANOBIS DISTANCE\ndef MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n    inv_covariance_matrix = inv_cov_matrix\n    vars_mean = mean_distr\n    diff = data - vars_mean\n    md = []\n    for i in range(len(diff)):\n        md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n    return md\n\n# CHECK IF MATRIX IS POSITIVE DEFINITE\ndef is_pos_def(A):\n    if np.allclose(A, A.T):\n        try:\n            np.linalg.cholesky(A)\n            return True\n        except np.linalg.LinAlgError:\n            return False\n    else:\n        return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up PCA model\nDefine train/test set from the two main principal components:","metadata":{}},{"cell_type":"code","source":"data_train = np.array(X_train_PCA.values)\ndata_test = np.array(X_test_PCA.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate the covariance matrix and its inverse, based on data in the training set:","metadata":{}},{"cell_type":"code","source":"cov_matrix, inv_cov_matrix  = cov_matrix(data_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also calculate the mean value for the input variables in the training set, as this is used later to calculate the Mahalanobis distance to datapoints in the test set:","metadata":{}},{"cell_type":"code","source":"# Mean of each column: PCA1, PCA2\n## - It should be very close to zero\nmean_distr = data_train.mean(axis=0) # axis=0 means that average is computed per column\nnp.set_printoptions(precision=3, suppress=False)\nmean_distr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the covariance matrix and its inverse, we can calculate the Mahalanobis distance for the training data defining “normal conditions”, and find the threshold value to flag datapoints as an anomaly.\nThen calculate the Mahalanobis distance for the datapoints in the test set, and compare that with the anomaly threshold:","metadata":{}},{"cell_type":"code","source":"dist_test = MahalanobisDist(inv_cov_matrix, mean_distr, data_test, verbose=False)\ndist_train = MahalanobisDist(inv_cov_matrix, mean_distr, data_train, verbose=False)\n\nprint(\"Minimum & maximum MD in training set:\", min(dist_train), max(dist_train) )\nprint(\"Minimum & maximum MD in test set    :\", min(dist_test), max(dist_test) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure(figsize=(15, 4), dpi=80)\nplt.plot(np.array(dist_train) , label=\"Baseline (25% of dataset, i.e. first 225 points)\")\nplt.plot(np.array(dist_test)[:600], label=\"Analysis data (first 600 out of 759 points)\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Comparison of test set (analysis set) distance over train set (baseline)\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that there is a large portion of *normal* data in the analysis (test) set. This validates the lenght of the dataset we selected for establishing the baseline (train set).","metadata":{}},{"cell_type":"markdown","source":"## Threshold value for flagging an anomaly","metadata":{}},{"cell_type":"code","source":"# CALCULATE THRESHOLD FOR CLASSIFYING AS ANOMALY\ndef MD_threshold(dist, extreme=False, verbose=False):\n    k = 3. if extreme else 2.\n    threshold = np.mean(dist) * k\n    return threshold\n\nthreshold = MD_threshold(dist_train, extreme = True)\n#  * extreme = True => twice the mean of incoming data (dist_train)\n#  * extreme = False => three times the mean\nprint(\"Threshold value for flagging an anomaly is\", \"{:.2f}\".format(threshold) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The square of the Mahalanobis distance to the centroid of the distribution should follow a χ2 distribution if the assumption of normal distributed input variables is fulfilled. This is also the assumption behind the above calculation of the “threshold value” for flagging an anomaly. As this assumption is not necessarily fulfilled in our case, it is beneficial to visualize the distribution of the Mahalanobis distance to set a good threshold value for flagging anomalies.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nplt.figure()\nsns.distplot(dist_train,\n             bins = 20, \n             kde= True, \n            color = 'green');\nplt.xlim([0.0,5])\nplt.xlabel('Mahalanobis dist')\nplt.title('Mahalanobis distance distribution for the baseline (train set)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above distributions, the calculated threshold value of 3.66 for flagging an anomaly seems reasonable (defined as 3 standard deviations from the center of the distribution).\n\nWe can then save the Mahalanobis distance, as well as the threshold value and “anomaly flag” variable for both train and test data in a dataframe:","metadata":{}},{"cell_type":"markdown","source":"### Outliers in the baseline (*train set*)","metadata":{}},{"cell_type":"code","source":"anomaly_train = pd.DataFrame()\nanomaly_train['Mob dist']= dist_train\nanomaly_train['Thresh'] = threshold\n# If Mob dist above threshold: Flag as anomaly\nanomaly_train['Anomaly'] = anomaly_train['Mob dist'] > anomaly_train['Thresh']\nanomaly_train.index = X_train_PCA.index\n\nn_outliers_train = anomaly_train[ anomaly_train['Anomaly'] == True].shape[0]\nprint(\"There are\", n_outliers_train, \"anomalies in the train set out of\", anomaly_train.shape[0], \"points\")\nanomaly_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomaly_train.plot(logy=True, figsize = (15,6), ylim = [1e-1,1e3], color = ['green','red'])\nplt.xticks(np.arange(0, anomaly_train.shape[0], 50), fontsize=10, rotation = 30)\nplt.title('Baseline plot against anomaly threshold')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Outliers in the analysis set (*test set*)","metadata":{}},{"cell_type":"code","source":"anomaly = pd.DataFrame()\nanomaly['Mob dist']= dist_test\nanomaly['Thresh'] = threshold\n# If Mob dist above threshold: Flag as anomaly\nanomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\nanomaly.index = X_test_PCA.index\n\nn_outliers = anomaly[ anomaly['Anomaly'] == True].shape[0]\nprint(\"There are\", n_outliers, \"anomalies in the test set out of\", anomaly.shape[0], \"points\")\nanomaly_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomaly = pd.DataFrame()\nanomaly['Mob dist']= dist_test\nanomaly['Thresh'] = threshold\n# If Mob dist above threshold: Flag as anomaly\nanomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\nanomaly.index = X_test_PCA.index\nanomaly.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the calculated statistics, any distance above the threshold value will be flagged as an anomaly.\n\nWe can now merge the data in a single dataframe and save it as a .csv file:","metadata":{}},{"cell_type":"code","source":"anomaly_alldata = pd.concat([anomaly_train, anomaly])\nanomaly_alldata.to_csv('Anomaly_distance.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now plot the calculated anomaly metric (Mob dist), and check when it crosses the anomaly threshold (note the logarithmic y-axis).","metadata":{}},{"cell_type":"code","source":"anomaly_alldata.plot(logy=True, figsize = (15,6), ylim = [1e-1,1e3], color = ['green','red'])\nplt.xticks(np.arange(0, anomaly_alldata.shape[0], 50), fontsize=10, rotation = 30)\nplt.title('Whole dataset plot against anomaly threshold')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}