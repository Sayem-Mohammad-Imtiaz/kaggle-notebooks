{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, Normalizer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if 'csv' in filename:\n            print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n* [The UNSW-NB15 dataset description](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/)\n* [Feature visualization and preprocessing](https://www.kaggle.com/khairulislam/unsw-nb15-eda)\n* [Feature importance using RandomForest classifier](https://www.kaggle.com/khairulislam/unsw-nb15-feature-importance)\n* [Performance with other classifiers](https://www.kaggle.com/khairulislam/unsw-nb15-anomaly-detection)"},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def run_lgb(x, y, tr_idx, val_idx, param, num_round=100):\n    lgb_train = lgb.Dataset(x.iloc[tr_idx], y.iloc[tr_idx])\n    x_val, y_val = x.iloc[val_idx], y.iloc[val_idx]\n    validation = lgb.Dataset(x_val, y_val)\n    clf = lgb.train(param, lgb_train, num_round, valid_sets=[validation], early_stopping_rounds=50, verbose_eval=200, feval=lgb_f1_score)\n    return clf\n\ndef false_alarm_rate(y_true, y_pred):\n    CM = metrics.confusion_matrix(y_true, y_pred)\n    TN, FN, TP, FP = CM[0][0], CM[1][0], CM[1][1], CM[0][1]\n    return (FP+FN)/(TP+TN+FP+FN)\n\nlabel = \"Train\"\n\ndef plot_roc(y_true, y_prob):\n    fpr, tpr, _ = metrics.roc_curve(y_true, y_prob)\n    auc = metrics.roc_auc_score(y_true, y_prob)\n    plt.plot(fpr,tpr,label=label+\", auc= %0.2f\" % auc)\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.legend(loc=0)\n    plt.show()\n    plt.savefig(label+\".pdf\")\n    \n    \ndata = {}\ndef results(y_test, y_prob):\n    threshold = 0.5\n    y_pred = np.where(y_prob >= threshold, 1, 0)\n    \n    acc = metrics.accuracy_score(y_test, y_pred)\n    pre = metrics.precision_score(y_test, y_pred)\n    rec = metrics.recall_score(y_test, y_pred) # it is also called detection rate or true positive rate\n    f1 = metrics.f1_score(y_test, y_pred)\n    print(f\"Acc {acc}, Precision {pre}, Recall {rec}, F1-score {f1}\")\n    \n    CM = metrics.confusion_matrix(y_test, y_pred)\n    TN, FN, TP, FP = CM[0][0], CM[1][0], CM[1][1], CM[0][1]\n    # false positive rate\n    FPR = FP/(FP+TN)\n    # false alarm rate \n    FAR = (FP+FN)/(TP+TN+FP+FN)\n    AUC = metrics.roc_auc_score(y_test, y_prob)\n    \n    print(\"FPR {0}, FAR {1}, AUC {2}\".format(FPR, FAR, AUC))\n    # print(metrics.classification_report(y_test, y_pred))\n    # plot_roc(y_test, y_prob)\n    if label != \"\":\n        data[label] = (y_test, y_prob)\n\n    \ndef test_run(x_train, y_train, x_test, y_test, param, num_round=2000):\n    start = time.clock()\n    \n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_validation = lgb.Dataset(x_test, y_test)\n    clf = lgb.train(param, lgb_train, num_round, valid_sets=[lgb_validation], early_stopping_rounds=50, verbose_eval=200, feval=lgb_f1_score)\n    # clf = lgb.train(param, lgb_train, 2000, valid_sets=[lgb_validation], early_stopping_rounds=50, verbose_eval=200)\n    y_prob = clf.predict(x_test, num_iteration=clf.best_iteration)\n    \n    print()\n    results(y_test, y_prob)\n    print(\"Time spent {0}\".format(time.clock() - start))\n    return y_prob\n    \ndef cross_validation(X, Y, param, kf, num_round=2000):\n    start = time.clock()\n    y_probs = []\n    y_vals = []\n\n    # for tr_idx, val_idx in tqdm(kf.split(X, Y), total=folds):\n    for tr_idx, val_idx in kf.split(X, Y):\n        clf = run_lgb(X, Y, tr_idx, val_idx, param, num_round)\n        x_val, y_val = X.iloc[val_idx], Y.iloc[val_idx]\n        y_prob = clf.predict(x_val, num_iteration=clf.best_iteration)\n        \n        y_probs.extend(y_prob)\n        y_vals.extend(y_val)\n\n    print()\n    results(y_vals, np.asarray(y_probs))\n    print(\"Time spent {0}\".format(time.clock() - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root = '../input/data-preprocessing/'\ntrain = pd.read_csv(root + 'train.csv')\ntest = pd.read_csv(root + 'test.csv')\n# separate features and labels\nx_train, y_train = train.drop(['label'], axis=1), train['label']\nx_test, y_test = test.drop(['label'], axis=1), test['label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook as tqdm\n\ndef lgb_accuracy(preds, data):\n    y_true = data.get_label()\n    y_pred = np.round(preds)\n    return 'acc', metrics.accuracy_score(y_true, y_pred), True\n\ndef lgb_f1_score(preds, data):\n    y_true = data.get_label()\n    y_pred = np.round(preds) # scikits f1 doesn't like probabilities\n    return 'f1', metrics.f1_score(y_true, y_pred), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = 10\nseed = 1\nnum_round = 2000\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label = ''\n# param = {\n#     'objective': 'binary', \n#     'learning_rate': 0.1, \n#     \"boost_from_average\":True,\n#     \"metric\": 'binary_logloss' # 'auc'\n# }\n# start = time.clock()\n# # test_run( x_train, y_train, x_train, y_train, param)\n# clf = lgb.train(param, lgb.Dataset(x_train, y_train), 2000, valid_sets=[lgb.Dataset(x_train, y_train)], early_stopping_rounds=50, verbose_eval=200)\n# y_prob = clf.predict(x_train, num_iteration=clf.best_iteration)\n# print()\n# results(y_train, y_prob)\n# print(\"Time spent {0}\".format(time.clock() - start))\n\n\n# y_prob = clf.predict(x_test, num_iteration=clf.best_iteration)\n# print()\n# results(y_test, y_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ten-fold cross validation"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"param = {\n    'objective': 'binary', \n    'learning_rate': 0.1, \n    \"boost_from_average\":True,\n    \"metric\": 'binary_logloss' # 'auc'\n}\nlabel = \"train_ten\"\ncross_validation(x_train, y_train, param, kf, num_round=num_round)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Five-fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"label = 'train_five'\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\ncross_validation(x_train, y_train, param, kf, num_round=num_round)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# label = ''\n# param = {\n#     'objective': 'binary', \n#     'learning_rate': 0.1, \n#     \"boost_from_average\":True,\n#     \"metric\": 'binary_logloss' # 'auc'\n# }\n# y_prob = test_run(x_test, y_test, x_test, y_test, param)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validate on test data\nHere the model trained on test data is being validated using test data."},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"label = \"train_test\"\nparam = {\n    'objective': 'binary',\n    'learning_rate': 0.05, \n    'boost_from_average':True,\n    'is_unbalance':True,\n    \"metric\": 'binary_logloss' # 'auc'\n}\ny_prob = test_run(x_train, y_train, x_test, y_test, param, num_round=num_round)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred = np.where(y_prob >= 0.5, 1, 0)\n# print(metrics.confusion_matrix(y_test, y_pred))\n\n# target_names = ['Normal', 'Anomaly']\n# cm = metrics.confusion_matrix(y_test, y_pred)\n# # Normalize\n# cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n# plt.rc('font', size=20) \n# fig, ax = plt.subplots(figsize=(10,10))\n# sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names)\n\n# plt.ylabel('Actual')\n# plt.xlabel('Predicted')\n \n# plt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ten-fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"label = 'test_ten'\nparam = {\n    'objective': 'binary',\n    'learning_rate': 0.1, \n    \"boost_from_average\":True,\n    # 'is_unbalance':True,\n    # \"feature_fraction\":0.5,\n    \"metric\": 'binary_logloss' # 'auc'\n}\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncross_validation(x_test, y_test, param, kf, num_round=num_round)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Five-fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"label = 'test_five'\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\ncross_validation(x_test, y_test, param, kf, num_round=num_round)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combined data\nHere we combined both train and test set. Then evaluated their ten-fold cross validation performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"total = pd.concat([train, test], axis=0)\nX, Y = total.drop(['label'], axis=1), total['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"param = {\n    'objective': 'binary',\n    'learning_rate': 0.1, \n    \"boost_from_average\":True,\n    # 'is_unbalance':True,\n    # \"bagging_fraction\":0.8,\n    \"feature_fraction\":0.5,\n    # \"bagging_freq\":1,\n    \"metric\": 'binary_logloss' # 'auc'\n}\nlabel = 'combined_ten'\ncross_validation(X, Y, param, kf, num_round=num_round)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(dpi=1200)\nfor value in ['Train', 'Test', 'Combined']:\n    temp = data[value.lower()+'_ten']\n    fpr, tpr, _ = metrics.roc_curve(temp[0], temp[1])\n    auc = metrics.roc_auc_score(temp[0], temp[1])\n    plt.plot(fpr,tpr,label=value+\", auc= %0.4f\" % auc)\n\nplt.plot([0, 1], [0, 1],'r--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\n\nplt.savefig('roc_ten.pdf')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for value in ['Train', 'Test']:\n    temp = data[value.lower()+'_five']\n    fpr, tpr, _ = metrics.roc_curve(temp[0], temp[1])\n    auc = metrics.roc_auc_score(temp[0], temp[1])\n    plt.plot(fpr,tpr,label=value+\", auc= %0.4f\" % auc)\n\nplt.plot([0, 1], [0, 1],'r--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\n\nplt.savefig('roc_five.pdf')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = data['train_test']\nfpr, tpr, _ = metrics.roc_curve(temp[0], temp[1])\nauc = metrics.roc_auc_score(temp[0], temp[1])\nplt.plot(fpr,tpr,label=\"Test, auc= %0.4f\" % auc)\n\nplt.plot([0, 1], [0, 1],'r--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('roc_test.pdf')\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import IFrame, display\nfilepath = \"roc_test.pdf\"\nIFrame(filepath, width=700, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}