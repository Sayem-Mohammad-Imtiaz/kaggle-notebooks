{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../working\"))\nimport pandas as pd\npd.DataFrame([1,2,3,4]).to_csv(\"../working/test.csv\")\nprint(os.listdir(\"../working\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.empty(5, 3)\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.rand(5, 3)\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.zeros(5, 3, dtype=torch.long)\nprint(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor([5.5, 3])\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.new_ones(5, 3, dtype=torch.double)      \nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.randn_like(x, dtype=torch.float)    \nprint(x) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = torch.rand(5, 3)\nprint(x + y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.view(-1,5).mm(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.add_(x)\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x[:, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8)  # the size -1 is inferred from other dimensions\nprint(x.size(), y.size(), z.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.randn(1)\nprint(x)\nprint(x.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.ones(5)\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = a.numpy()\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a.add_(1)\nprint(a)\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")          \ny = torch.ones_like(x, device=device)  \nx = x.to(device)                       \nz = x + y\nprint(z)\nprint(z.to(\"cpu\", torch.double))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Simple Neural Net using Numpy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport time\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random input and output data\nx = np.random.randn(N, D_in)\ny = np.random.randn(N, D_out)\n\n# Randomly initialize weights\nw1 = np.random.randn(D_in, H)\nw2 = np.random.randn(H, D_out)\n\nstart_time=time.time()\nlearning_rate = 1e-6\nfor t in range(1):\n    # Forward pass: compute predicted y\n    h = x.dot(w1)\n    h_relu = np.maximum(h, 0)\n    y_pred = h_relu.dot(w2)\n\n    # Compute and print loss\n    loss = np.square(y_pred - y).sum()\n    print(t, loss)\n\n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.T.dot(grad_y_pred)\n    grad_h_relu = grad_y_pred.dot(w2.T)\n    grad_h = grad_h_relu.copy()\n    grad_h[h < 0] = 0\n    grad_w1 = x.T.dot(grad_h)\n\n    # Update weights\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n    \nend_time=time.time()\nprint(end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport time\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random input and output data\nx = torch.randn(N, D_in, device=device, dtype=dtype)\ny = torch.randn(N, D_out, device=device, dtype=dtype)\n\n# Randomly initialize weights\nw1 = torch.randn(D_in, H, device=device, dtype=dtype)\nw2 = torch.randn(H, D_out, device=device, dtype=dtype)\n\nstart_time=time.time()\nlearning_rate = 1e-6\nfor t in range(1):\n    # Forward pass: compute predicted y\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    print(t, loss)\n\n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h < 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n\n    # Update weights using gradient descent\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n    \nend_time=time.time()\nprint(end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport time\n\ndtype = torch.float\ndevice = torch.device(\"cuda:0\")\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random input and output data\nx = torch.randn(N, D_in, device=device, dtype=dtype)\ny = torch.randn(N, D_out, device=device, dtype=dtype)\n\n# Randomly initialize weights\nw1 = torch.randn(D_in, H, device=device, dtype=dtype)\nw2 = torch.randn(H, D_out, device=device, dtype=dtype)\n\nstart_time=time.time()\nlearning_rate = 1e-6\nfor t in range(1):\n    # Forward pass: compute predicted y\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    print(t, loss)\n\n    # Backprop to compute gradients of w1 and w2 with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h < 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n\n    # Update weights using gradient descent\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n    \nend_time=time.time()\nprint(end_time-start_time)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}