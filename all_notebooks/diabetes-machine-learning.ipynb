{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport scikitplot as plot\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection as sk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing, svm\nfrom sklearn.model_selection import cross_val_score\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom datetime import datetime\nimport datetime as dt\nimport math \nfrom sklearn import metrics\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV as rs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\n#show data\nprint(df)\n#unique number of values per column\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# keep only numerical columns\ndf = df.select_dtypes(include =[np.number]) \n\n# remove the Outlier of skin thickness\nmax_skinthickness = df.SkinThickness.max()\ndf = df[df.SkinThickness!=max_skinthickness]\n\n# replace zeros w mean values\ndef replace_zero(df, field, target):\n    mean_by_target = df.loc[df[field] != 0, [field, target]].groupby(target).mean()\n    df.loc[(df[field] == 0)&(df[target] == 0), field] = mean_by_target.iloc[0][0]\n    df.loc[(df[field] == 0)&(df[target] == 1), field] = mean_by_target.iloc[1][0]\n\n    # run the function\nfor col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','Age','Pregnancies']:   \n    replace_zero(df, col, 'Outcome')  \n\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df[[\"Glucose\", \"Insulin\", \"BMI\",'BloodPressure']]\nlabels = df.Outcome\n\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute correlations\ncorr = df.corr()\n# Exclude duplicate correlations by masking uper right values\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Add diverging colormap\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(250,10, as_cmap=True)\n# Draw correlation plot, shrink legend\nsns.heatmap(corr, mask=mask, cmap=cmap, \n        square=True,\n        linewidths=.5, cbar_kws={\"shrink\": .5},annot=True, annot_kws={\"size\":12}, ax=ax)\nplt.title(\"Diabetes Correlations\")\nplt.show ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Outcome'])\nplt.title(\"Diabetic Measurement 0 and 1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scale/Features/ Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nss = StandardScaler()\nscaler = MinMaxScaler()\nfeatures_train,features_test,labels_train,labels_test = train_test_split(features,labels,stratify=df.Outcome,test_size=0.2)\n\nfeatures_train = scaler.fit_transform(features_train)\nfeatures_test = scaler.fit_transform(features_test)\nfeatures_train= ss.fit_transform(features_train)\nfeatures_test = ss.fit_transform(features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RANDOM FOREST\nrf = RandomForestRegressor()\nrf.fit(features_train,labels_train)\nrf.score(features_test,labels_test)\n#print(rf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DTClassifier\nfrom sklearn.tree import DecisionTreeClassifier \ndtclf = DecisionTreeClassifier()\ndtclf.fit(features_train,labels_train)\ndtclf.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig = plt.figure(figsize = (10,10))\nfigsize = (4,4)\ntree_graph = tree.export_graphviz(dtclf, out_file=None,filled = True)\ngraphviz.Source(tree_graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_para = { 'max_leaf_nodes':[2,10,30],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150], 'criterion':['gini','entropy']}\ndtclf = rs(DecisionTreeClassifier(), tree_para, cv=10)\ndtclf.fit(features_train,labels_train)\ndtclf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtclf = DecisionTreeClassifier(max_leaf_nodes= 10,max_depth= 50,criterion= 'entropy')\ndtclf.fit(features_train,labels_train)\ndtclf.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM\nfrom sklearn import svm\nclf = svm.SVC(kernel=\"linear\")\nclf.fit(features_train,labels_train)\nclf.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001]}\n               \nsvmclf = rs(clf, param_grid, cv=10)\nsvmclf.fit(features_train,labels_train)\nprint(svmclf.best_params_)\n#print(svmclf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svmclf = svm.SVC(C=10,gamma= 1,kernel=\"linear\")\nsvmclf.fit(features_train,labels_train)\nsvmclf.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive Bayes Classifier\nfrom sklearn import naive_bayes\nnbclf = naive_bayes.GaussianNB()\nnbclf.fit(features_train,labels_train)\nnbclf.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Light GBM\nimport lightgbm as lgb\n\nlgbclf = lgb.LGBMClassifier()\nlgbclf.fit(features_train,labels_train)\nlgbclf.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': [200, 400, 500],\n    'max_depth': [15,20,25],\n    'num_leaves': [25, 50, 75]}\n               \nlgbclf = rs(lgbclf, param_grid, cv=10)\nlgbclf.fit(features_train,labels_train)\nprint(lgbclf.best_params_)\n#print(lgbclf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbclf = lgb.LGBMClassifier(num_leaves= 25, n_estimators= 400,max_depth= 15)\nlgbclf.fit(features_train,labels_train)\nprint(lgbclf.score(features_test,labels_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknnclf = KNeighborsClassifier(n_neighbors=2)\nknnclf.fit(features_train,labels_train)\nprint(knnclf.score(features_test,labels_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'leaf_size' : [0,5,10,50], 'n_neighbors':np.arange(1,50)}\n\nknnclf = KNeighborsClassifier()\nknnclf= rs(knnclf,param_grid,cv=10)\nknnclf.fit(features_test,labels_test)\nprint(knnclf.best_params_)\n#print(knnclf.best_estimator_)\n#print(knnclf.score(features_test,labels_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knnclf = KNeighborsClassifier( n_neighbors=3,leaf_size=10, n_jobs=-1)\nknnclf.fit(features_train,labels_train)\n\n#print(cross_val_score(knnclf,features_test,labels_test,cv=10))\nprint(knnclf.score(features_test,labels_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclf1 = LogisticRegression()\nclf1.fit(features_train,labels_train)\nclf1.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100]}\nrs_search =rs(clf1, param_grid, cv=10)\nrs_search.fit(features_train,labels_train)\nprint(rs_search.best_params_)\n#print(rs_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf1 = LogisticRegression(C= .1, n_jobs=-1, penalty='l2',\n                   random_state=10)\nclf1.fit(features_train,labels_train)\nprint(clf1.score(features_test,labels_test))\n#print(cross_val_score(clf1,features_test,labels_test,cv=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBOOST\nfrom xgboost.sklearn import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(features_train,labels_train)\nxgb.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'learning_rate': [.03, 0.05, .07],\n              'max_depth': [5,6,7],\n              'n_estimators':[200,300]}\n#xgb = rs(xgb, param_grid, cv=10)\nxgb = rs(xgb, param_grid, cv=10)\nxgb.fit(features_train,labels_train)\nprint(xgb.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(n_estimators= 200, learning_rate= .03,\n              max_depth= 5)\nxgb.fit(features_train,labels_train)\nxgb.score(features_test,labels_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algos = [\"Support Vector Machine\",\"Decision Tree\",\"Logistic Regression\",\"K Nearest Neighbor\",\"Naive Bayes\",\"Light GBM\",\"XGBRegressor\"]\nclfs = [dtclf, svmclf, nbclf, lgbclf, knnclf, clf1, xgb]\n\nresult = []\n\nfor clff in clfs:\n    clff.fit(features_train,labels_train)\n    acc = clff.score(features_test,labels_test)\n    result.append(acc)\nresult_df = pd.DataFrame(result,index=algos)\nresult_df.columns=[\"Tuned Parameters Accuracy\"]\nresult_df.sort_values(by=\"Tuned Parameters Accuracy\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dtclf, svmclf, nbclf, lgbclf, knnclf, clf1, xgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross Validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nkfold =KFold(n_splits=10)\n\n#algos = [\"Support Vector Machine\",\"Decision Tree\",\"Logistic Regression\",\"K Nearest Neighbor\",\"Naive Bayes\",\"Light GBM\",\"XGBRegressor\"]\n#clfs = [dtclf, svmclf, nbclf, lgbclf, knnclf, clf1, xgb]\n\nalgos = [\"Support Vector Machine\",\"Decision Tree\",\"Logistic Regression\",\"K Nearest Neighbor\",\"Naive Bayes\",\"Light GBM\"]\nclfs = [dtclf, svmclf, nbclf, lgbclf, knnclf, clf1]\n\ncv_results=[]\nfor classifiers in clfs:\n    cv_score = cross_val_score(classifiers,features,labels,cv=kfold,scoring=\"accuracy\")\n    cv_results.append(cv_score.mean())\ncv_mean = pd.DataFrame(cv_results,index=algos)\ncv_mean.columns=[\"K-Fold10,CV Accuracy\"]\ncv_mean.sort_values(by=\"K-Fold10,CV Accuracy\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, log_loss\n#voting_clf = VotingClassifier(estimators=[('SVC', svmclf), ('DTree', dtclf), ('LogReg', clf1), \n#                                          ('NaiveB', nbclf), ('KNN', knnclf), ('LGBM', lgbclf)], voting='hard')\n\nvoting_clf = VotingClassifier(estimators=[('SVC', svmclf),('KNN', knnclf),('NaiveB', nbclf)], voting='hard')\nvoting_clf.fit(features_train,labels_train)\npreds = voting_clf.predict(features_test)\n\nacc =  accuracy_score(labels_test, preds)\nl_loss = log_loss(labels_test, preds)\nf1 = f1_score(labels_test, preds)\ncv= cross_val_score(voting_clf,features,labels,cv=kfold,scoring=\"accuracy\")\n\n#print(\"Accuracy is: \" + str(acc))\nprint(\"Log Loss is: \" + str(l_loss))\nprint(\"F1 Score is: \" + str(f1))\nprint(\"Cross Val Score is: \" + str(cv))\n\n\nvoting_clf = voting_clf.fit(features_train,labels_train)\npred = voting_clf.predict(features_test)\nprint('Accuracy on test:' , (labels_test == pred).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(labels_test,pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"from vecstack import stacking\n#models=dtclf, svmclf, nbclf, lgbclf, knnclf, clf1,xgb\nmodels= svmclf,knnclf, lgbclf\nS_train, S_test = stacking(models,                   \n                           features_train, labels_train, features_test,   \n                           regression=False, \n                           mode='oof_pred_bag',\n                           needs_proba=False,\n                           save_dir=None, \n                           metric=accuracy_score, \n                           n_folds=10, \n                           stratified=True,\n                           shuffle=True,\n                           random_state=0,    \n                           verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def heldout_score(clf, X_test_cv, y_test_cv):\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test_cv)):\n        score[i] = clf.loss_(y_test_cv, y_pred)\n    return score\n\ndef cv_estimate(n_splits=10):\n    cv = KFold(n_splits=n_splits)\n    cv_clf = model\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n    for train, test in cv.split(X_train_cv):\n        cv_clf.fit(X_train_cv.iloc[train], y_train_cv[train])\n        val_scores += heldout_score(cv_clf, X_train_cv.iloc[test], y_train_cv[test])\n    val_scores /= n_splits\n    return val_scores\n\ndef train_clf(lgbm, features_train, labels_train):\n    return clf.fit(features_train, labels_train)\n\ndef train_predict(lgbm,features_train, labels_train, features_test, labels_test):\n    train_clf(lgbm, features_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nparams = {'n_estimators': [200, 400, 500],\n    'max_depth': [15,20,25],\n    'num_leaves': [25, 50, 75]}\nlgbm = lgb.LGBMClassifier()\n\nlgbm = rs(lgbm, params,cv=10)\nlgbm.fit(features_train,labels_train)\nprint(lgbm.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = lgb.LGBMClassifier(num_leaves= 25, n_estimators= 400, max_depth= 25)\nclf_ = lgbm.fit(features_train, labels_train)\ny_pred = clf.predict(labels_test)\n\n#train_predict(lgbm, features_train, labels_train, features_test, labels_test)\nprint('Accuracy is {}'.format(accuracy_score(features_test,y_pred )))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Factors of All data, Diabetic and Non"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the distibution of the data for every feature\nplt.figure(figsize=(10, 10))\n\nfor i, column in enumerate(df.columns, 1):\n    plt.subplot(3, 3, i)\n    df[df[\"Outcome\"] == 0][column].hist(bins=35, color='blue', label='Have Diabetes = NO', alpha=0.6)\n    df[df[\"Outcome\"] == 1][column].hist(bins=35, color='red', label='Have Diabetes = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)\n  \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the distibution of the data for every feature\nplt.figure(figsize=(10, 10))\n\nfor i, column in enumerate(features.columns, 1):\n    plt.subplot(3, 3, i)\n    df[df[\"Outcome\"] == 0][column].hist(bins=35, color='blue', label='Have Diabetes = NO', alpha=0.6)\n    df[df[\"Outcome\"] == 1][column].hist(bins=35, color='red', label='Have Diabetes = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nnames = [\"Decision Tree\",\"Logistic Regression\",\"K Nearest Neighbor\",\"Naive Bayes\",\"Light GBM\"]\nclfs = [dtclf, nbclf, lgbclf, knnclf, clf1]\n    \n    ensemble = SuperLearner(scorer = accuracy_score, \n                            random_state = 0.5, \n                            folds = 10)\n    ensemble.add(clf[0])\n    ensemble.add_meta(lr)\n    ensemble.fit(feature_train, labels_train)\n    preds = ensemble.predict(features_test)\n    accuracy = accuracy_score(preds, labels_test)\n    \n    if accuracy > best_combination[0]:\n        best_combination[0] = accuracy\n        best_combination[1] = clf[1]\n    \n    print(\"Accuracy score: {:.3f} {}\").format(accuracy, clf[1])\n    print(\"\\nBest stacking model is {} with accuracy of: {:.3f}\").format(best_combination[1], best_combination[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Diabetics vs ALL Factors"},{"metadata":{"trusted":true},"cell_type":"code","source":"diabeticage = df['Outcome'] + df['Age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (30,10))\nsns.set_style(\"whitegrid\")\n\nsns.lineplot(x= diabeticage, y= df['Glucose'])\nsns.lineplot(x= diabeticage, y= df['BMI'])\nsns.lineplot(x= diabeticage, y= df['DiabetesPedigreeFunction'])\nsns.lineplot(x= diabeticage, y= df['BloodPressure'])\n\nplt.legend()\nplt.title(\"Diabetics vs ALL Factors\")\nplt.xticks \n\n\n\n\nplt.show ()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}