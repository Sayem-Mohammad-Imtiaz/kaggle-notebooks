{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook we explore metagenomics data. This dataset was created by the team of Edoardo Pasolli, Duy Tin Truong, Faizan Malik, Levi Waldron, and Nicola Segata; they published [a research article in July of 2016](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004977). The authors used 8 publicly available metagenomic datasets, and applied [MetaPhlAn2](https://github.com/segatalab/metaml#metaml---metagenomic-prediction-analysis-based-on-machine-learning) to generate species abundance features. **Here we are only going to focus on one of those datasets for now to predict type 2 diabetes.**\n\n## Logistics behind the Input Data\n\nThis notebook was created to further explore the meta-genomics data on kaggle. The link to the data-set is: https://www.kaggle.com/antaresnyc/metagenomics. The datasets include:\n* abundance.txt: a table containing the abundances of each organism type\n  * the first 210 features include meta-data about the samples\n  * the rest of the features include the abundance data in float-type\n* marker_presence.txt: a table containing the presence of strain-specific markers. \n  * the first 210 features include meta-data about the samples (same as abundance.txt)\n  * In a previous notebook I converted the marker presence feature data into a sparse matrix for easier downloading. This sparse matrix is found on [kaggle](https://www.kaggle.com/sklasfeld/metagenomics-marker-presence-sparse-matrix).\n* markers2clades_DB.txt: a lookup table to associate each marker identifier to the corresponding species.","metadata":{}},{"cell_type":"markdown","source":"In summary we have 210 samples. We know the abundance of the organisms in the sample. If an organism is in a sample we have strain-specific marker information.","metadata":{}},{"cell_type":"markdown","source":"See Table 1 in the paper for a summary of the datasets considered in the experiment.","metadata":{}},{"cell_type":"markdown","source":"## Libraries\nBelow I import some librarys that may be useful and then print the input files","metadata":{}},{"cell_type":"code","source":"import pip\n\n!pip install --upgrade pip\n\ndef import_or_install(package):\n    try:\n        __import__(package)\n    except ImportError:\n        pip.main(['install', package])  ","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:55:24.563504Z","iopub.execute_input":"2021-07-30T14:55:24.563888Z","iopub.status.idle":"2021-07-30T14:55:30.97674Z","shell.execute_reply.started":"2021-07-30T14:55:24.563855Z","shell.execute_reply":"2021-07-30T14:55:30.975782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy\nimport scipy.sparse\n\nimport networkx as nx # creates graph data-structures\n#import_or_install('node2vec')\n#from node2vec import Node2Vec # embeds graphs as vectors\n\n# plot with matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#from plotnine import * # used to plot data\n\n# progress bar\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T14:55:45.02127Z","iopub.execute_input":"2021-07-30T14:55:45.021621Z","iopub.status.idle":"2021-07-30T14:55:45.048964Z","shell.execute_reply.started":"2021-07-30T14:55:45.021591Z","shell.execute_reply":"2021-07-30T14:55:45.047838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# install pytorch libraries\n!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+10.2.html\n!pip install torch-geometric","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-30T14:55:46.617367Z","iopub.execute_input":"2021-07-30T14:55:46.617701Z","iopub.status.idle":"2021-07-30T15:10:13.187127Z","shell.execute_reply.started":"2021-07-30T14:55:46.617659Z","shell.execute_reply":"2021-07-30T15:10:13.186011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\nimport torch\nfrom torch_geometric.utils.convert import from_networkx\nfrom torch_geometric.data import InMemoryDataset\nfrom torch_geometric.data import Data\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:13.189027Z","iopub.execute_input":"2021-07-30T15:10:13.18931Z","iopub.status.idle":"2021-07-30T15:10:15.173936Z","shell.execute_reply.started":"2021-07-30T15:10:13.189283Z","shell.execute_reply":"2021-07-30T15:10:15.173183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"marker_presence_matrix_file=\"/kaggle/input/metagenomics-marker-presence-sparse-matrix/marker_presence_matrix.npz\"\nmarkers2clades_DB_file=\"/kaggle/input/human-metagenomics/markers2clades_DB.csv\"\nabundance_file=\"/kaggle/input/human-metagenomics/abundance.csv\"\nmarker_presence_table_file=\"/kaggle/input/human-metagenomics/marker_presence.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:15.175165Z","iopub.execute_input":"2021-07-30T15:10:15.175417Z","iopub.status.idle":"2021-07-30T15:10:15.179469Z","shell.execute_reply.started":"2021-07-30T15:10:15.175393Z","shell.execute_reply":"2021-07-30T15:10:15.178309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the Data\nTo organize the data we think of it as three different data types:\n1. The meta information (found in the first 210 columns of both the abundance and marker_presense tables)\n2. The abundance data\n3. The marker_presense data\n","metadata":{}},{"cell_type":"markdown","source":"## Cleaning meta features\nRecall that each of the given datasets are made up of multiple datasets which do not all include the same meta information.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# import only the columns with the sample meta-data information\nsamples_df = pd.read_csv(abundance_file,\n                         sep=\",\", dtype=object,usecols=range(0,210))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:15.180569Z","iopub.execute_input":"2021-07-30T15:10:15.18086Z","iopub.status.idle":"2021-07-30T15:10:15.875301Z","shell.execute_reply.started":"2021-07-30T15:10:15.180834Z","shell.execute_reply":"2021-07-30T15:10:15.874563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Different datasets were focused on specific disease and non-disease types","metadata":{}},{"cell_type":"code","source":"# print a table of the different dataset\n# and disease combinations\npd.DataFrame(samples_df.loc[:,['dataset_name','pubmedid','disease']].value_counts()).sort_values('dataset_name')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:15.877823Z","iopub.execute_input":"2021-07-30T15:10:15.878092Z","iopub.status.idle":"2021-07-30T15:10:15.917135Z","shell.execute_reply.started":"2021-07-30T15:10:15.878068Z","shell.execute_reply":"2021-07-30T15:10:15.916283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To start, let's focus on the `t2dmeta_long` dataset.","metadata":{}},{"cell_type":"code","source":"t2dml_samples_df = samples_df.loc[samples_df['dataset_name']==\"t2dmeta_long\",:].copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:15.918377Z","iopub.execute_input":"2021-07-30T15:10:15.918645Z","iopub.status.idle":"2021-07-30T15:10:15.924539Z","shell.execute_reply.started":"2021-07-30T15:10:15.918619Z","shell.execute_reply":"2021-07-30T15:10:15.923844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like `nd`, `na`, `unknown` and `-` all stands for no data. Therefore let's replace these values all with np.NaN","metadata":{}},{"cell_type":"code","source":"t2dml_samples_df = t2dml_samples_df.replace(\"nd\", np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace(\"na\", np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace(\"-\", np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace(' -', np.NaN)\nt2dml_samples_df = t2dml_samples_df.replace('unknown', np.NaN)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:15.925552Z","iopub.execute_input":"2021-07-30T15:10:15.925874Z","iopub.status.idle":"2021-07-30T15:10:15.975614Z","shell.execute_reply.started":"2021-07-30T15:10:15.925847Z","shell.execute_reply":"2021-07-30T15:10:15.97479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can remove all columns that have only 1 value (not including NaN). These do not seem to be too informative anyway.","metadata":{}},{"cell_type":"code","source":"# change the if statement to visualize\nif 1==0:\n    for col in t2dml_samples_df.loc[:, t2dml_samples_df.nunique() == 1].columns:\n        t2dml_samples_df[col].fillna(\"NaN\").value_counts().sort_values().plot(\n            kind = 'bar', title=col)\n        plt.show()\n        \nt2dml_samples_df = t2dml_samples_df.loc[:, t2dml_samples_df.nunique() > 1].copy()\nt2dml_samples_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:15.977124Z","iopub.execute_input":"2021-07-30T15:10:15.977479Z","iopub.status.idle":"2021-07-30T15:10:16.043453Z","shell.execute_reply.started":"2021-07-30T15:10:15.977446Z","shell.execute_reply":"2021-07-30T15:10:16.042521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to convert some columns into booleans. For example if the values are either:\n* \"yes\",\"no\", or null\n* \"y\",\"n\", or null\n* \"positve\", \"negative\", or null\n* \"a\"(affected), \"u\" (unaffected), or null\n\nI want to convert them into `2`, `1`, and `0` respectively.","metadata":{}},{"cell_type":"code","source":"bool_vals={'True':1,\n          'False':-1,\n          'Null':0}\nfor col in t2dml_samples_df.loc[:, t2dml_samples_df.nunique() < 4]:\n    if (\"yes\" in t2dml_samples_df[col].unique() and \"no\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'yes': bool_vals['True'], 'no': bool_vals['False']}})\n    elif (\"y\" in t2dml_samples_df[col].unique() and \"n\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'y': bool_vals['True'], 'n': bool_vals['False']}})\n    elif (\"positive\" in t2dml_samples_df[col].unique() and \"negative\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'positive': bool_vals['True'], 'negative': bool_vals['False']}})\n    elif (\"a\" in t2dml_samples_df[col].unique() and \"u\" in t2dml_samples_df[col].unique()):\n            t2dml_samples_df[col] = t2dml_samples_df[col].fillna(bool_vals['Null'])\n            t2dml_samples_df =t2dml_samples_df.replace({col: {'a': bool_vals['True'], 'u': bool_vals['False']}})","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:16.044527Z","iopub.execute_input":"2021-07-30T15:10:16.04481Z","iopub.status.idle":"2021-07-30T15:10:16.065954Z","shell.execute_reply.started":"2021-07-30T15:10:16.044785Z","shell.execute_reply":"2021-07-30T15:10:16.065161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, for columns that contain 2 values (not including null) I will convert the values to numbers. For example, I will change the column named \"gender\" to \"gender:Female|Male\". The values will be 1 for Female, 2 for Male, and 0 for null.","metadata":{}},{"cell_type":"code","source":"for col in t2dml_samples_df.loc[:, t2dml_samples_df.nunique() == 2].columns:\n    if (not(True in t2dml_samples_df[col].unique() and \n             False in t2dml_samples_df[col].unique())):\n        val_i = 0\n        first_val_null=True\n        first_val = np.NaN\n        while (first_val_null):\n            first_val = t2dml_samples_df[col].unique()[val_i]\n            if first_val == first_val:\n                first_val_null = False\n            else:\n                val_i += 1\n        val_i += 1\n        second_val_null=True\n        second_val= np.NaN\n        while (second_val_null):\n            second_val = t2dml_samples_df[col].unique()[val_i]\n            if second_val == second_val:\n                second_val_null = False\n            else:\n                val_i += 1\n        new_col_name=(\"%s:%s|%s\" % (col,first_val, second_val))\n        # change the column name\n        t2dml_samples_df = (t2dml_samples_df.rename(\n            columns={col:new_col_name}))\n        # change values in the column\n        t2dml_samples_df[new_col_name] = t2dml_samples_df[new_col_name].fillna(bool_vals['Null'])\n        t2dml_samples_df =t2dml_samples_df.replace({new_col_name: {first_val: bool_vals['False'],\n                                                       second_val: bool_vals['True']}})\ncategorical_cols=t2dml_samples_df.loc[:, t2dml_samples_df.nunique() < 20].columns","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:16.067149Z","iopub.execute_input":"2021-07-30T15:10:16.067537Z","iopub.status.idle":"2021-07-30T15:10:16.098655Z","shell.execute_reply.started":"2021-07-30T15:10:16.0675Z","shell.execute_reply":"2021-07-30T15:10:16.097757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building models based on abundance file\nThe abundance data values are reported for different taxonomic ranks (kingdom, phylum, division, class, order, family, genus, and species) of the bacteria phylogenetic tree. The values, which are formatted into a table structure,would make more sense to a machine in a tree format. Therefore, we convert these values into a graph object using the `networkx` libary.","metadata":{}},{"cell_type":"markdown","source":"first we import abundance data into a pandas dataframe. To start we are only looking at data from the `t2dmeta_long` dataset. We create a table with the meta-data features and one with only the abundance values.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# read in abundance values from abundance file\nabundance_df = pd.read_csv(abundance_file,sep=\",\", dtype=object).iloc[:,211:]\n# collect columns with abundance values\nabundance_cols = list(abundance_df.columns)\n# filter for only samples from the `t2dmeta_long` dataset\nt2dml_abundance_values_df = abundance_df.iloc[(\n    list(t2dml_samples_df.index))]\n# merge meta-data features with abundance features\nt2dml_abundance_df = t2dml_samples_df.merge(\n    abundance_df, how='inner',left_index=True, right_index=True)\nt2dml_abundance_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:16.099735Z","iopub.execute_input":"2021-07-30T15:10:16.099993Z","iopub.status.idle":"2021-07-30T15:10:18.946618Z","shell.execute_reply.started":"2021-07-30T15:10:16.099968Z","shell.execute_reply":"2021-07-30T15:10:18.945678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model based on NetworkX\nWe create a dictionary called `graph_dict` to hold the directed graphs for each sample. The key is in the index of the sample in the dataframe, and the value is the `networkx` graph `G`.\n\nWe build 'G' by adding looking at each of the values in each row. If the value is greater than 0 then we can add a node to the graph. If the column name includes information beyond the kingdom then we can add a node to the previous node in the hierarcy. ","metadata":{"execution":{"iopub.status.busy":"2021-06-04T20:19:37.715016Z","iopub.execute_input":"2021-06-04T20:19:37.715481Z","iopub.status.idle":"2021-06-04T20:19:37.727873Z","shell.execute_reply.started":"2021-06-04T20:19:37.715443Z","shell.execute_reply":"2021-06-04T20:19:37.726985Z"}}},{"cell_type":"code","source":"graph_dict = {} # dictionary of graphs per sample from the t2dmeta_long dataset\n#n2v_dict={} # dictionary of vectors from graphs per sample\nfor i in list(t2dml_abundance_values_df.index):# single edge as tuple of two nodes range(len(abundance_df)):\n    G = nx.DiGraph()\n    for tree_str, abundance_val in t2dml_abundance_values_df.loc[i].to_dict().items():\n        if float(abundance_val) > 0:\n            bacteria_list = tree_str.split('|')\n            G.add_node(bacteria_list[-1],amount=abundance_val)\n            if len(bacteria_list) > 1:\n                G.add_edge(*bacteria_list[-2:]) # add single edge as tuple of two nodes\n    graph_dict[i]=G\n    #n2v_dict[i] = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we need to embed these graphs into an architecture that can be imported into a neural network. ","metadata":{}},{"cell_type":"markdown","source":"Ideas for the next step include converting each graph into a vector using the `Node2vec` constructor. See the documentation at: https://github.com/eliorc/node2vec. \n\nAnother idea includes using a PopPhy-CNN. A published architecture found at: https://doi.org/10.1101/257931. The github repo is at https://github.com/derekreiman/PopPhy-CNN.","metadata":{}},{"cell_type":"markdown","source":"## Building model with Pytorch\n\nThe following code was inspired by https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8\n\nThe *torch_geometric.data* module contains a [Data()](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/data.html#Data) class that we will use to create graphs for the abundance data. In our case, the Data() class requires three parameters:\n1. **x**: a torch.LongTensor() containing an numpy array of 2D lists. The 2D list has the attributes/features associated with each node. In our case, the attributes are the bacteria IDs given by the column name, and the features are the abundance values. \n2. **edge_index**: a torch.tensor() of data type `torch.long` containing an array of 2 lists.The first list contains the parent IDs of each node and the second list has the child bacteria IDs with respect to the first list.\n3. **y**: a FloatTensor() which contains the list of target values for each sample\n\nWe use this Data() structure to build a custom dataset below:","metadata":{"execution":{"iopub.status.busy":"2021-07-29T20:09:05.846683Z","iopub.execute_input":"2021-07-29T20:09:05.847068Z","iopub.status.idle":"2021-07-29T20:09:05.853963Z","shell.execute_reply.started":"2021-07-29T20:09:05.847033Z","shell.execute_reply":"2021-07-29T20:09:05.852544Z"}}},{"cell_type":"code","source":"class t2dmlDataset(InMemoryDataset):\n    def __init__(self, root, transform=None, pre_transform=None):\n        super(t2dmlDataset, self).__init__(root, transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        return []\n    @property\n    def processed_file_names(self):\n        return ['../input/human-metagenomics/t2dmlDataset.dataset']\n\n    def download(self):\n        pass\n    \n    def process(self):\n        samples_df = t2dml_samples_df\n        abundance_df = t2dml_abundance_values_df\n        data_list = []\n\n        # get embedding for target values (y)\n        le = preprocessing.LabelEncoder()\n        y = le.fit_transform(samples_df['disease:n|t2d'])\n\n        # get emnedding of all nodes\n        le_nodes = preprocessing.LabelEncoder()\n\n        # encode labels between 0 and n_classes-1 for each bacterial label\n        le_nodes.fit([col.split('|')[-1] for col in  abundance_df.columns])\n        \n        # process by each sample (row)\n        row_range = range(len(abundance_df))\n        for i in tqdm(row_range):\n            node_list = [] # list of [$cur_bacteria_name,$abundance_val]\n            edge_list = [] # list of [$parent_bacteria_name,$cur_bacteria_name]\n            for key, val in abundance_df.iloc[i].to_dict().items():\n                if float(val) > 0:\n                    bacteria_list = key.split('|')\n                    node = [le_nodes.transform([bacteria_list[-1]])[0],float(val)]\n                    node_list.append(node)\n                    if len(bacteria_list) >= 2:\n                        edge_list.append(le_nodes.transform(bacteria_list[-2:]))\n            # convert `y`, `node_list`, and `edge_list` into Tensor formats\n            edge_array = np.array(edge_list)\n            edge_index = torch.tensor([edge_array[:,0],edge_array[:,1]],dtype=torch.long)\n            #print(np.array(node_list))\n            node_features = torch.LongTensor(np.array(node_list))\n            label = torch.FloatTensor([y[i]])\n            # set these Tensors into a pytorch Data() object\n            # which is used to model graphs\n            data = Data(node_features,edge_index=edge_index,y=label)\n            data_list.append(data)\n        \n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:56:21.198127Z","iopub.execute_input":"2021-07-30T15:56:21.198515Z","iopub.status.idle":"2021-07-30T15:56:21.213825Z","shell.execute_reply.started":"2021-07-30T15:56:21.198476Z","shell.execute_reply":"2021-07-30T15:56:21.212566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can build the dataset.","metadata":{}},{"cell_type":"code","source":"%%time\n\ndataset = t2dmlDataset('../input/human-metagenomics')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:56:22.318295Z","iopub.execute_input":"2021-07-30T15:56:22.318743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After building the dataset, we call shuffle() to make sure it has been randomly shuffled and then split it into three sets for training, validation, and testing.","metadata":{}},{"cell_type":"code","source":"dataset = dataset.shuffle()\ntrain_datalist = dataset[len(t2dml_abundance_values_df) // 10:]\ntest_datalist = dataset[:len(t2dml_abundance_values_df) // 10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It may be faster to do below instead...","metadata":{}},{"cell_type":"code","source":"# return a list of pytorch Data() objects which each contain a graph\n# for each row in abundance_df\ndef graph_label(samples_df,abundance_df):\n    \n    # get embedding for target values (y)\n    le = preprocessing.LabelEncoder()\n    y = le.fit_transform(samples_df['disease:n|t2d'])\n    print(y)\n    \n    # get emnedding of all nodes\n    le_nodes = preprocessing.LabelEncoder()\n    \n    # encode labels between 0 and n_classes-1 for each bacterial label\n    le_nodes.fit([col.split('|')[-1] for col in  abundance_df.columns])\n    data_list = []\n    \n    # process by each sample (row)\n    for i in range(len(abundance_df)):\n        node_list = [] # list of [$cur_bacteria_name,$abundance_val]\n        edge_list = [] # list of [$parent_bacteria_name,$cur_bacteria_name]\n        for key, val in abundance_df.iloc[i].to_dict().items():\n            if float(val) > 0:\n                bacteria_list = key.split('|')\n                node = [le_nodes.transform([bacteria_list[-1]])[0],float(val)]\n                node_list.append(node)\n                if len(bacteria_list) >= 2:\n                    edge_list.append(le_nodes.transform(bacteria_list[-2:]))\n        # convert `y`, `node_list`, and `edge_list` into Tensor formats\n        edge_array = np.array(edge_list)\n        edge_index = torch.tensor([edge_array[:,0],edge_array[:,1]],dtype=torch.long)\n        #print(np.array(node_list))\n        node_features = torch.LongTensor(np.array(node_list))\n        label = torch.FloatTensor([y[i]])\n        # set these Tensors into a pytorch Data() object\n        # which is used to model graphs\n        data = Data(node_features,edge_index=edge_index,y=label)\n        data_list.append(data)\n    return data_list","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:10:18.94792Z","iopub.execute_input":"2021-07-30T15:10:18.948189Z","iopub.status.idle":"2021-07-30T15:10:18.958553Z","shell.execute_reply.started":"2021-07-30T15:10:18.948163Z","shell.execute_reply":"2021-07-30T15:10:18.957775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split the data so that 10% is in the training set and 90% is in the testing set. Then we use the DataLoader() function to import these datasets into pytorch.","metadata":{}},{"cell_type":"code","source":"data_list = graph_label(t2dml_samples_df,t2dml_abundance_values_df)\ntrain_datalist = data_list[len(data_list) // 10:]\ntest_datalist = data_list[:len(data_list) // 10]\n\ntrain_loader = DataLoader(train_datalist, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_datalist, batch_size=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we loaded the data, it is time to run some models! See this article for more info: https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8","metadata":{}},{"cell_type":"markdown","source":"## Cleaning Marker Presence file\nOnce we have created a model using the abundance data we can try to add the marker-presense information into the model. However, for now we will put this on hold. Ignore the code below. ","metadata":{}},{"cell_type":"code","source":"%%time \n# Import this file without the first 211 columns (since we already dealt with those previously). \n# This file could be imported as a sparse numpy matrix. it is very large.\nif 1==0:\n    markers_reader = pd.read_csv(\n            marker_presence_table_file,\n            sep=\",\", \n            dtype=object,\n            usecols=range(211,288558),\n            nrows=10)\n    print(markers_reader.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}