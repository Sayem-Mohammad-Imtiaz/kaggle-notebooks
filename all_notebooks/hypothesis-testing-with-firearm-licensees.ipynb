{"nbformat":4,"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","version":"3.6.3","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c148cb6c-f9be-4cbe-91f7-9e821448fbd0","_uuid":"37f0cdd21d38b42324e2d777f65c21fb90b0dfec"},"source":"#  Hypothesis Testing with Firearm Licensees\n\nWarning: this notebook will be a referesher on hypothesis testing and the t-test for those that have already seen these concepts before, but the ideas may be difficult to parse right away if this is your first time grappling with these concepts!\n\n## Hypothesis testing\n\nA *hypothesis* is, as per the scientific method, a statement of something that we believe to be true (or maybe false). When Gallileo posed the 16th century Europeans that the solar system revolves around the sun, not the Earth, he had a theory (heliocentrism) built up on a sequence of hypotheses. You probably did something similar many times over in your fifth-grade science class or high school \"laboratory\".\n\nIn statistics, hypotheses are used to determine the level to which a statistical result is reliable. The fundamental problem is that when we try to measure something about an underlying distribution using a bunch of datapoints, we are almost certainly wrong. However, some answers are more wrong than others, and the less wrong the answer, the more useful the result. Hence what we are interested in is statistical significance: the chance that something that we have measured is wrong by more than some predetermined amount.\n\nSuppose we measure the mean of a distribution using a bunch of data points we have. We instigate a contrarian hypothesis: that this distribution does not have such-and-such mean. This is known as the *null hypothesis*, or $H_0$. The *alternative hypothesis*, $H_a$, is that this distribution does have such-and-such mean. Then we select a *significance level*: how much of a risk we are willing to take on that we are wrong. Suppose we pick $\\alpha = 0.05$ (there is a 5% chance we are wrong). We apply some kind of statistical assay (a *test statistic*) to the data, and get a *p-value* for our mean estimate. The significance level tells us how unlikely (on a $[0, 1]$ probabilistic scale) our result is to occur.\n\nSuppose we got a significance level of 0.01. Since $0.01 < 0.05$, the significance level is smaller than the threshold that we chose beforehand, so we reject the null hypothesis and accept $H_a$: this result is so rare that we're confident enough that the mean we chose is wrong.\n\nThis whole procedure is known as a \"hypothesis test\".\n\n## Test statistics and the t-test\n\nHypothesis test problems are addressed by devising an appropriate [test statistic](https://en.wikipedia.org/wiki/Test_statistic) for the distribution in question: the number that we can actually use to estimate how likely we are to be wrong. The one that's used most commonly (and taught in intro to stats classes in college) is the t-test.\n\nThe t-test has the following formula. Let $\\hat{\\beta}$ be an estimator on some population parameter $\\beta$. Let $\\beta_0$ be the $\\beta$ that we hypothesize in our null hypothesis $H_0$. Let $\\hat{\\sigma}$ be an estimate of the standard error of $\\hat{\\beta}$ (with the formula $\\hat{\\sigma}(\\hat{\\beta}) = \\sqrt{\\frac{\\sigma^2}{n}}$). Then $t$ is defined as:\n\n$$t = \\frac{\\hat{\\beta} - \\beta_0}{\\hat{\\sigma}(\\hat{\\beta})}$$\n\nGiven that we fulfill certain conditions, $t$ is a valid test statistic for $\\hat{\\beta}$, and hence the statistical significance of our null hypothesis $H_0$.\n\nThis test is appropriate for estimates on independent identically distributed variables, whenever (approximately) $n > 30$.\n\nThe variables need to be \"independent\" because one variable cannot influence any of the other variables in the distribution. An example of the opposite occuring&mdash;a dependent variable&mdash;would be a time-series of stock prices. If the stock price for MSFT is 100\\$ today, it is unlikely that it wil be 200\\$ tomorrow, and if it's 200\\$ tomorrow, it's unlikely to be 50\\$ the day after that. An independent variable is one that doesn't do this. For example, the number you draw from the bag in a game of lotto.\n\nThe variables need to be \"identically distributed\" because the test requires that the distribution the numbers are being drawn from doesn't \"drift\" over time.\n\nVariables which fulfill both of these conditions are known as i.i.d variables. Most estimators are i.i.d by definition.\n\n## Central Limit Theorem\n\nThe $n > 30$ condition is interesting. This condition stems from what is known as the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem): given enough draws, the [normalized](https://www.kaggle.com/residentmario/nyc-buildings-part-2-feature-scales-grid-search/) mean of a sequence of I.I.D. variables will converge almost assuredly to the normal distribution $N(\\mu=0, \\sigma^2=1)$. This convergence is so fast that in practice, it's accurate after just 30 or so draws.\n\n## More\n\nThe theory behind why the $t$-test works is very complex (especially the Central Limit Theorem), and something that I will explore in future notebooks. The way that this material is taught usually enschews or hand-waves past *why* it works, leaving it as what it is.\n\n## Hypothesis testing firearm licensees\n\nIn the rest of this notebook I'll apply the t-test to the Firearm Licensees dataset."},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"63fc12f5-ae10-4020-98d7-050289dbffdf","collapsed":true,"_uuid":"c8a8eecc143328d8923dda5902238bc2d3bb921e"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nlicensees = pd.read_csv(\"../input/federal-firearm-licensees.csv\", index_col=0)[1:]\nlicensees.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"30993cb3-7698-47e6-8688-84a5b7942d4a","_uuid":"8c660f62bc93b249ae0eb7aa2490b0a21cf35340"},"source":"Suppose we're interested in measuring the mean number of gun sale licensees by county. If we look at this data, here's what we find:"},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"37d32554-fa46-443b-8935-cddd31ac0c8a","collapsed":true,"_uuid":"5d8fadcdc164efefc545069757b5ec27206bec8e"},"outputs":[],"source":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nlicensees['Premise Zip Code'].value_counts().plot.hist(bins=50)"},{"cell_type":"markdown","metadata":{"_cell_guid":"29e31de4-b02b-47c4-acc0-455945647a50","_uuid":"6d2dfed74fe25685299fd7e030e87a741bd666a3"},"source":"(note that since I'm excluding counties which do not have *any* firearm stores, this is actually a flawed metric, but it's good enough for the purposes of demonstration!)\n\n<!--\nThis chart makes it pretty obvious that the number of gun licensees per country variables wildly between different counties. That is to say, by looking at this histogram, we can conclude that the number of gun shops deviates geographically.\n\nBut suppose that we want to be more authoritative about this observation. We could pose this as a hypothesis. Our null hypothesis will be that \"Gun shops in the United States are equivalently distributed\", while our alternative hypothesis will be that \"Gun shops in the United States are not equivalently distributed\".\n-->\n\nWe'll take the following mean:"},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"f0621999-7a11-456e-b2bf-3b203707f863","collapsed":true,"_uuid":"d0d5ddc4a17c049ff6ce7af25ded1e6f5dc761bb"},"outputs":[],"source":"licensees['Premise Zip Code'].value_counts().mean()"},{"cell_type":"markdown","metadata":{"_cell_guid":"cac869ca-3b33-4941-b4d2-f5ac3fbb3c88","_uuid":"edb389731a6e382de17712a34fdc87ee993fd4c2"},"source":"Here are our hypotheses:\n\n$$H_0: \\bar{n} = 2.75$$\n$$H_a: \\bar{n} \\neq 2.75$$\n\nLet's set our p-level to 0.05. That is, let's say that we're willing to accept a 5% risk that when we reject the null hypothesis we are wrong.\n\nNow we will implement our t-test. Here's a hand implementation first, to see what it looks like. Note that to get the p-value we'll just throw the number at the `scipy` normal distribution built-in because the normal distribution is non-trivial to simulate computationally."},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"bfdcb9aa-c412-4d1f-9911-9f23c7f41a26","collapsed":true,"_uuid":"3c60d86f823fb30bed21892132f0aa93585f1868"},"outputs":[],"source":"X = licensees['Premise Zip Code'].value_counts()"},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"34e557db-c32e-47d2-8795-2f25086aa71d","collapsed":true,"_uuid":"82a35c9729853eb93728b7334b76866d2056cbb8"},"outputs":[],"source":"import numpy as np\nimport scipy.stats as stats\n\ndef t_value(X, h_0):\n    se = np.sqrt(np.var(X) / len(X))\n    return (np.mean(X) - h_0) / se\n\ndef p_value(t):\n    # Two-sided p-value, so we multiply by 2.\n    return stats.norm.sf(abs(t))*2\n\nt = t_value(X, 2.75)\np = p_value(t)"},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"a03cf625-a18b-4bb6-b284-d24577a9b9c0","collapsed":true,"_uuid":"46047dd0d7c0d34ff84bea5ccf627674c66b21b9"},"outputs":[],"source":"t, p"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc587c7a-e2a5-4982-b072-614c1adb5e46","_uuid":"47564776a238ff50d6adbb8505a22a47c811a5ca"},"source":"The $t$ score tells us that our result is 0.36 standard deviations away from the average mean estimator result we can expect. 0.36 standard deviations is not a lot at all though! Our $p$ value tells us that almost 72% of possible mean estimate values are further away from our expectation than the value that we got.\n\nIn other words, a mean value of 2.75 is in the 72nd percentile of closeness.\n\nSince $0.72 > 0.05$, we fail to reject the null hypothesis $H_0$. We conclude that there is strong evidence that $\\bar{n} = 2.75$&mdash;that is, that the mean number of gun shops per US Zip Code is almost 3!\n\nFor reference, here is the usual way of performing this test using `scipy`:"},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"6d020955-739e-429f-80da-4422231808dc","collapsed":true,"_uuid":"717dd2e2c4b18d938b6dec0e3a32f1e9f2540d9e"},"outputs":[],"source":"import scipy.stats as stats\n\nstats.ttest_1samp(a=X, popmean=2.75)"},{"cell_type":"markdown","metadata":{"_cell_guid":"edd29015-478d-48a7-9b8b-b010784be67d","_uuid":"bbe692d06f40cb157187026a4a4f3c9933631c72"},"source":"## Conclusion\n\nHypothesis testing is used extensively in the literature because it is a relatively simple and powerful tool for *making decisions*. Hypothesis testing allows us to state what level of confidence we want to have in some observation about our data, then, in testing that observation, determine whether or not we are satisfied that it is correct.\n\nAnother way of making this decision is to make it into a chart. For example, we could have randomly recomputed the mean of an increasing number of values in the dataset, and used that to determine how confident we are in our result. So for example, we'll take the mean of a single sample from the dataset; then the mean of two samples from the dataset; then three, and so on. Here's how that would look:"},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"c649c533-1ccd-4461-91b7-d47abe1ca18e","collapsed":true,"_uuid":"5b22bd5219179bf9c03133ae7e2a3ddbfcb1ab65"},"outputs":[],"source":"r = (licensees['Premise Zip Code']\n         .value_counts()\n         .sample(len(licensees['Premise Zip Code'].unique()) - 1))\npd.Series(r.cumsum() / np.array(range(1, len(r) + 1))).reset_index(drop=True).plot.line(\n    figsize=(12, 4), linewidth=1\n)"},{"cell_type":"markdown","metadata":{"_cell_guid":"316ebe95-7c38-45a4-acae-e61d32b95834","_uuid":"ebc13830f30831cc3b5ca4f5a4e782e5b3e6a9b3"},"source":"As you can see, the mean of our values stabilizes on the \"true\" value over time. No matter the amount of variance at the beginning of the distribution, by the end we have a very good idea that the real result is approximately 2.75.\n\nHypothesis testing is merely way of quantifying **how sure we are about this approximation**. It's important to do because a computer can't \"look\" at a graph; we mortals can, be we still need it too sometimes, because the graph is oftentimes ambiguous, and sometimes we need to make a lot of decisions potentially very quickly without necessarily looking at all of the graphs.\n\nIn a future notebook we'll look at the intimately related concept of confidence intervals. After that, we'll look at an alternative way of generating this same information that's oftentimes more flexible: bootstrapping. Strap in!"}]}