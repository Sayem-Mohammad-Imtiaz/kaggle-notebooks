{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Boom Bikes Demand Analysis\n\n## Problem Statement\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n* Which variables are significant in predicting the demand for shared bikes.\n* How well those variables describe the bike demands  \nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Business Goals \n\nThe company needs to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Analysis Approach & Conclusions\nThis problem can be solved using Multiple Linear Regression Analysis. The company requires a two fold solution. \n1. A model to predict demand with accuracy.\n2. Insight into the significant relationships that exist between demand and available predictors. \n\nAnalysis is carried out using a Mixed Feature Selection Approach. 15 features are selected algorithmically using Recursive Feature Elimination. Further selection is done manually by looking at multicollinearity and statistical significance of features and overall fit of the model. \nThe 10 most significant features to understand demand have been reported. \n\nThe data set is randomly divided into training and test data. \n`Final Model` built on training data set explains 84% of the variability  and  achieves 81% on test data.  \nThe final relationship between demand and predictors is as follows.    \n* ```cnt``` = 2392.0791 + 1946.7864 * ```yr``` + 444.4907 * ```Saturday``` + 466.0136 * ```winter``` - 890.3115 * ```july``` -1063.6669 * ```spring``` + 296.8008 *  ```workingday``` - 1749.8275 * ```hum``` + 4471.6602 * ```temp``` - 1110.3191 * ```windspeed``` - 1273.7519 * ```light snow/rain```   \n  \nwhere ```temp``` , ```windspeed``` and ```hum``` are normalized. \n\nNote :    \n- Data has been cleaned to drop outliers that might affect the model adversely\n- The model has been verified for Multicollinearity effects. \n- Residual Analysis has been carried out and the model satisfies the assumptions of Linear Regression (Residuals follow a normal distribution, Errors exhibit  homoscedasticity)\n- Q-Q plot between residual distribution and normal distribution shows that residuals approximately follow a normal distribution. Some points significant deviation which deems further analysis\n- Further Lag plot shows there is no auto-correlation in data. \n- Model is stable at 81%(+/-14%) coefficient of determination at 95% CI, ascertained through cross validation.\n- Features in the order of influence has been reported by standardizing all predictor values. \n- Outliers dropped during Data Understanding phase deems further analysis from business perspective.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Table of Contents : \n- [Reading and Understanding Data](#Reading-and-Understanding-Data)     \n    * [Data Quality Checks](#Data-Quality-Checks)  \n    * [Visualizing Continuous Variables](#Visualizing-Continuous-Variables)     \n    * [Outliers in Continuous Variables](#Outliers-in-Continuous-Variables-vs-cnt)\n    * [Visualizing Categorical Variables](#Visualizing-Categorical-Variables)  \n    * [Outliers in Categorical Variables](#Outliers-in-Categorical-Variables-vs-cnt)\n    * [Correlation](#Correlation)\n- [Data Preparation](#Data-Preparation)  \n    * [Creating Indictor Variables](#Creating-Indictor-Variables)  \n    * [Splitting data set into Test & Train subsets](#Splitting-the-data-set-into-Test-&-Train-subsets) \n    * [Scaling Numerical Features](#Scaling-Numerical-Features)\n- [Modelling](#Modelling)   \n    * [Recursive Feature Elimination](#Recursive-Feature-Elimination)\n    * [Manual Elimination](#Manual-Elimination)\n        * [Model 1](#Model-1)\n        * [Model 2](#Model-2)\n        * [Model 3](#Model-3)\n        * [Model 4](#Model-4)\n        * [Model 5](#Model-5)\n        * [Model 6](#Model-6)   \n    * [Verifying MultiCollinearity](#Verifying-MultiCollinearity) \n    * [Final Model](#Final-Model)\n   \n- [Residual Analysis](#Residual-Analysis)\n- [Prediction](#Prediction)\n- [Model Evaluation](#Model-Evaluation)\n- [Model Stability](#Model-Stability)\n- [Top Features](#Top-Features)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading and Understanding Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/boombikes/day.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Quality Checks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- No missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Continuous Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping `instant`,`dteday`,`casual`,`registered` \n\ndata = data.drop(columns=['instant','dteday','casual','registered'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These variables were dropped since ```instant``` is the just the serial number of the record, \n```dteday``` is redundant coz the required data for analysis is contained in mnth,yr\n`casual` + `registered` = `cnt`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary statistics of numerical variables\ndata[['temp','atemp','hum','windspeed']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plots of Continuous variables vs 'cnt'\nsns.set_style(\"whitegrid\")\nsns.pairplot(data=data,x_vars=['temp','atemp','hum','windspeed'],y_vars='cnt',kind='scatter',height=5,aspect=1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The number of rentals per day seem to be increasing with temperature and adjusted temperature\n- adjusted temperature and temperature have similar trends \n- temp vs cnt has two outliers between 15 and 30\n- atemp vs cnt has two outliers between 20 and 35\n- hum vs cnt has two outliers below 20 \n- windspeed vs cnt has one outlier above 30\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Outliers in Continuous Variables vs cnt","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Dropping outliers in continuous variables\n# outliers in temp\ndata = data.drop(index = data[(data['temp'] > 15) & (data['temp'] < 20) & (data['cnt'] < 100)].index)\ndata = data.drop(index = data[(data['temp'] > 25) & (data['temp'] < 30) & (data['cnt'] < 2000)].index)\n\n\n# outliers in atemp\ndata = data.drop(index = data[(data['atemp'] > 20) & (data['atemp'] < 25) & (data['cnt'] < 100)].index)\ndata = data.drop(index = data[(data['atemp'] > 30) & (data['atemp'] < 35) & (data['cnt'] < 2000)].index)\n\n\n#outliers in hum\ndata = data.drop(index = data[(data['hum'] < 20)].index)\n\n#outliers in windspeed\ndata = data.drop(index = data[(data['windspeed'] > 30)].index)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at correlation with continuous variables \ncorrelation = data[['temp','atemp','hum','windspeed','cnt']].corr()['cnt'].apply(lambda x : round(x,4))\ncorrelation = pd.DataFrame(correlation).sort_values(by='cnt',ascending=False)\ncorrelation.drop(index=['cnt'],inplace=True)\n# dropping registered,casual, instant\ncorrelation.style.background_gradient(cmap='GnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ```adjusted temperature``` has the highest positive correlation with ```cnt``` followed by ```temperature```. \n- ```hum``` has the lowest correlation. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation between ```temp``` and ```atemp```\ndata[['temp','atemp']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Since, the correlation between ```temp``` and ```atemp``` is almost 1, one of them could be dropped. \n- ```atemp``` represents adjusted temperature which is an indicator of ```how hot it actually feels like``` which is a compound measure of temperature,humidity and windspeed. [Ref : UK Meteorological Dept](https://blog.metoffice.gov.uk/2012/02/15/what-is-feels-like-temperature/)\n- ```atemp``` might cause bias in data because it's a compound variable, Instead we could use ```temp``` , ```hum```  , ```windspeed```. Hence , dropping ```atemp```. Also it makes business sense to keep ```temp``` and calcuate adjusted temperature from it. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping ```atemp```\ndata = data.drop(columns=['atemp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['temp','hum','windspeed']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There's no signifcant correlation between ```atemp``` and ```hum``` , ```windspeed```.\n- Hence these are not dropped for now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n### Visualizing Categorical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting variables into categorical type \ndata[['season','weathersit','mnth']] = data[['season','weathersit','mnth']].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unique values in each categorical variable / [To check for disguised missing values]\ncat_vars = ['season','yr','mnth','holiday','weekday','workingday','weathersit']\nfor i in cat_vars : \n    print('Unique values in ',i, data[i].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- No disguised missing values exist","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing numbers with labels \nseason_labels = {\n    1 : 'spring',\n    2 : 'summer',\n    3 : 'fall',\n    4 : 'winter'\n}\n\nmnth_labels = {\n    1 : 'january',\n    2 : 'february',\n    3 : 'march',\n    4 : 'april',\n    5 : 'may',\n    6 : 'june',\n    7 : 'july',\n    8 : 'august',\n    9 : 'september',\n    10 : 'october',\n    11 : 'november',\n    12 : 'december'\n}\n\nweekday_labels = { # considering the first row of dteday to be 01-01-2011\n    0 : 'Sunday',\n    1 : 'Monday',\n    2 : 'Tuesday',\n    3 : 'Wednesday',\n    4 : 'Thursday',\n    5 : 'Friday',\n    6 : 'Saturday'\n}\n\nweathersit_labels = {\n    1 : 'clear',\n    2 : 'cloudy',\n    3 : 'light snow/rain'\n}\n\n# replacing numerals with labels \ndata['season'] = data['season'].replace(season_labels)\ndata['mnth'] = data['mnth'].replace(mnth_labels)\ndata['weekday'] = data['weekday'].replace(weekday_labels)\ndata['weathersit'] = data['weathersit'].replace(weathersit_labels)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_vars = ['season','yr','mnth','holiday','weekday',  'workingday','weathersit']\ndata1 = data[cat_vars]\ndata1.loc[:,'cnt'] = data['cnt'].values\ndata1[['yr','holiday','workingday']] = data1[['yr','holiday','workingday']].astype('category')\nplot_dim = [3,3]\nfig,axs = plt.subplots(*plot_dim)\nfig.set_figheight(15)\nfig.set_figwidth(20)\nfor i in range(plot_dim[0]) :  \n    for j in range(plot_dim[1]) :\n        axs[i,j].set(title = i*plot_dim[1]+j)\n        sns.boxplot(data=data1,x='cnt',y=cat_vars[i*plot_dim[1]+j],width=0.4,ax=axs[i,j])\n        if i*plot_dim[1]+j == 6 : \n            break\naxs[2,1].set_axis_off()\naxs[2,2].set_axis_off()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the season vs rentals per day plot , fall has the highest average rentals followed by summer. \n- Looking at year by year rentals, 2019 has had a median 2000 increase in rentals compared to 2018.\n- From the month wise plot, September has the highest rentals, followed by the two months surrounding it. It seems like the trend is explained by seasonal rentals too\n- Holidays show lower rental count compared to working days, with greater variability in demand on holidays. \n- There is no significant difference between rentals vs weekdays, except that Thursdays and sundays have a higher variation in rentals than others.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Outliers in Categorical Variables vs cnt","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping outliers in Categorical Variables \ndata = data.drop(index = data[(data['season'] == 'spring') & (data['cnt'] > 7000)].index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation among variables\nplt.figure(figsize=[10,10])\nsns.heatmap(data.corr(),cmap='GnBu',center=0,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Highest correlation with ```cnt``` is seen in ```temp``` followed by ```yr```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating Indictor Variables ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating indicator variable columns\nseason_indicators = pd.get_dummies(data['season'],drop_first=True)\nmnth_indicators = pd.get_dummies(data['mnth'],drop_first=True)\nweekday_indicators = pd.get_dummies(data['weekday'],drop_first=True)\nweathersit_indicators = pd.get_dummies(data['weathersit'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding indicator variable columns to the dataset . Dropping original columns\ndata = pd.concat([data,season_indicators,mnth_indicators,weekday_indicators,weathersit_indicators],axis=1)\ndata = data.drop(columns=['season','mnth','weekday','weathersit'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"|  Variable \t| Reference Label  \t|\n|---\t|---\t|\n|  season \t|   fall\t|\n|  mnth \t|   april\t|\n|   weekday\t|   Friday\t|\n|   weathersit\t|   clear\t|\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data set into Test & Train subsets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain,dtest = train_test_split(data,train_size=0.7,test_size=0.3,random_state=120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling Numerical Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization of continuous variables\nfrom sklearn.preprocessing import MinMaxScaler \nnumerical_scaler = MinMaxScaler()\nnum_vars = ['temp','hum','windspeed']\n\nnumerical_scaler.fit(dtrain[num_vars])\ndtrain[num_vars] = numerical_scaler.transform(dtrain[num_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### X_train , y_train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = dtrain.pop('cnt')\nX_train = dtrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Approach \n- A mixed approach is followed.    \n- 15 Best columns are chosen using RFE    \n- And then p-value method is followed for further elimination.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Recursive Feature Elimination","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting 15 Features using RFE \n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlr_estimator = LinearRegression()\nrfe = RFE(lr_estimator,n_features_to_select=15, step=1)\nselector = rfe.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RFE Feature Ranking\nrfe_ranking = pd.DataFrame({'rank' : selector.ranking_, 'support': selector.support_, 'features' : X_train.columns}).sort_values(by='rank',ascending=True)\nrfe_ranking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selected Features\nselected_features = rfe_ranking.loc[rfe_ranking['rank'] == 1,'features'].values\nselected_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Manual Elimination","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Following a stepwise elimination\nimport statsmodels.api as sm\ndef ols_fit(y,X) : \n    X_train_sm = sm.add_constant(X)\n    model = sm.OLS(y,X_train_sm).fit()\n    print(model.summary())\n    return model\ndef vif(X) : \n    df = sm.add_constant(X)\n    vif = [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n    vif_frame = pd.DataFrame({'vif' : vif[0:]},index = df.columns).reset_index()\n    print(vif_frame.sort_values(by='vif',ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### Model 1\n- Using features selected by RFE : 'yr', 'Sunday', 'Saturday', 'november', 'january', 'december',\n       'winter', 'july', 'spring', 'holiday', 'workingday', 'hum', 'temp',\n       'windspeed', 'light snow/rain'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_1 = selected_features\nols_fit(y_train,X_train[features_1])\nvif(X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 2 : \n- Dropping ```holiday``` because of high p-value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del_feature = 'holiday'\nselected_features = selected_features[selected_features!=del_feature]\nols_fit(y_train,X_train[selected_features])\nvif(X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 3 : \n- Dropping ```Sunday``` because of high p-value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del_feature = 'Sunday'\nselected_features = selected_features[selected_features!=del_feature]\nols_fit(y_train,X_train[selected_features])\nvif(X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 4 \n- Dropping ```january``` because this information might also be contained in ```winter```.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del_feature = 'january'\nselected_features = selected_features[selected_features!=del_feature]\nols_fit(y_train,X_train[selected_features])\nvif(X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 5 \n- Dropping ```december``` because this information might also be contained in ```winter```.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del_feature = 'december'\nselected_features = selected_features[selected_features!=del_feature]\nols_fit(y_train,X_train[selected_features])\nvif(X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 6 \n- Dropping ```november``` because this information might also be contained in ```winter```.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del_feature = 'november'\nselected_features = selected_features[selected_features!=del_feature]\nfinal_model = ols_fit(y_train,X_train[selected_features])\nvif(X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verifying MultiCollinearity ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vif(X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- VIF < 5 for selected features. No significant multicollinearity observed. Similar indicating comparison of R-squared and adjusted R-squared.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Final Model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = ols_fit(y_train,X_train[selected_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 10 features have been selected. \n- All the features are statistically significant [low p-value]\n- The model over is a good fit with Prob (F-statistic): 4.89e-187\n- The model explains 83.6% variability in the training data. Adjusted R-square being 83.3%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Residual Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Residual Analysis of Trained Data\nX_train_sm = sm.add_constant(X_train[selected_features])\n\ny_train_pred = final_model.predict(X_train_sm)\nfig,ax = plt.subplots(1,2)\nfig.set_figheight(8)\nfig.set_figwidth(16)\n\nax[0].set(title='Frequency Distribution of Residuals')\nsns.distplot(y_train-y_train_pred, bins=30, ax=ax[0])\n\nax[1].set(title='Predicted Values vs Residuals')\n\\\nsns.regplot(y_train_pred,y_train-y_train_pred,ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean of Residuals\n(y_train-y_train_pred).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Residual errors follow a normal distribution with mean=0  \n- Variance of Errors doesnt follow any trends\n- Residual errors are independent of each other since the Predicted values vs Residuals plot doesn't show any trend. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verifying the normality of distribution of residuals \nmean = (y_train-y_train_pred).mean()\nstd = (y_train-y_train_pred).std()\n\nref_normal = np.random.normal(mean,std,(y_train-y_train_pred).shape[0])\n\n\npercs = np.linspace(0,100,21)\nqn_ref_normal = np.percentile(ref_normal, percs)\nqn_residual = np.percentile(y_train - y_train_pred , percs)\n\nplt.plot(qn_ref_normal,qn_residual, ls=\"\", marker=\"o\")\n\nx = np.linspace(np.min((qn_ref_normal.min(),qn_residual.min())), np.max((qn_ref_normal.max(),qn_residual.max())))\nm = plt.plot(x,x, color=\"k\", ls=\"--\")\nplt.title('Q-Q Plot : Reference Normal vs Distribution of Residuals ')\nplt.savefig('q-q-plot.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- This plot further shows that the residual distribution is approximately normal for all test data with values within range of training data. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lag plot to assess independence of data points\nfrom pandas.plotting import lag_plot\nlag_plot(y_train-y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Lagplot of residuals shows no trend. Hence the error terms have constant variance \n\n**Hence, assumptions of Linear Regression are satisfied by this model**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prediction ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = dtest.pop('cnt')\nX_test = dtest\nX_test[num_vars] = numerical_scaler.transform(X_test[num_vars])\nX_test = X_test[selected_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = sm.add_constant(X_test)\ny_test_pred = final_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Actual vs Predicted No of rentals \nfig,ax = plt.subplots()\nfig.set_figheight(8)\nfig.set_figwidth(20)\nl1,=ax.plot(range(len(y_test)),y_test)\nl2, = ax.plot(range(len(y_test_pred)),y_test_pred)\nplt.legend([l1,l2],['Actual','Predicted'])\nplt.title('Predicted vs Actual No of Rentals');\nplt.ylabel('No of Bike Rentals')\nplt.xticks([])\nplt.show()\n\nplt.figure(figsize=[8,8])\nplt.scatter(y_test,y_test_pred);\nplt.title('Predicted vs Actual No of Rentals');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Predicted vs observed value plots shows that the model is reasonably accurate. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error,r2_score\nmse = mean_squared_error(y_test, y_test_pred)\nrsquared_test = r2_score(y_test, y_test_pred)\nrsquared_train = r2_score(y_train, y_train_pred)\nprint('R-squared for train data:',round(rsquared_train,2))\nprint('R-squared for test data:',round(rsquared_test,2))\nprint('Mean Squared Error',round(mse,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Stability ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# R-square using cross validation\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nclr = cross_val_score(lr,X_train[selected_features],y_train,cv=10, scoring='r2')\nclr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"R-square at 0.95 confidence level : %0.2f (+/- %0.2f)\" % (clr.mean(), clr.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardizing numerical variables \n\nfrom sklearn.preprocessing import StandardScaler\nreg_features = selected_features\nscaler = StandardScaler()\ndata = X_train[selected_features]\nstd_num = scaler.fit(data[['temp','windspeed','hum']])\n\n\nstd_X_train = pd.DataFrame(data = scaler.transform(data[['temp','windspeed','hum']]), columns=['temp','windspeed','hum'])\nfor i in reg_features : \n    std_X_train[i] = data[i].values\n\n\nreshaped_y_train = y_train.values.reshape(-1,1)\n\n# Fitting linear regression model \nstd_model = lr.fit(std_X_train, reshaped_y_train)\n\n# Coefficients and intercept\nresult = pd.DataFrame(data = std_model.coef_, columns = std_X_train.columns, index=['MLR Coefficients']).T\nresult = result.sort_values(by='MLR Coefficients',ascending=False)\nprint('\\nIntercept :',std_model.intercept_)\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Upon standardized the values of predictor variables, the above shows that the top features influencing demand are ```temp```, followed by ```yr``` and ```hum```\n* In case of continuous variables, the above data could be interpreted as - With every standard deviation increase in continuous variables, demand increases by `xxx`, when all other modelled paramters are held unchanged. \n* In case of categorical variables, the above data could be interpreted as - Compared to the reference level, the change in demand is `xxx`,, when all other modelled paramters are held unchanged. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Analysis is carried out using a Mixed Feature Selection Approach. 15 features are selected algorithmically using Recursive Feature Elimination. Further selection is done manually by looking at multicollinearity and statistical significance of features and overall fit of the model. \nThe 10 most significant features to understand demand have been reported. \n\nThe data set is randomly divided into training and test data. \n`Final Model` built on training data set explains 84% of the variability  and  achieves 81% on test data.  \nThe final relationship between demand and predictors is as follows.    \n* ```cnt``` = 2392.0791 + 1946.7864 * ```yr``` + 444.4907 * ```Saturday``` + 466.0136 * ```winter``` - 890.3115 * ```july``` -1063.6669 * ```spring``` + 296.8008 *  ```workingday``` - 1749.8275 * ```hum``` + 4471.6602 * ```temp``` - 1110.3191 * ```windspeed``` - 1273.7519 * ```light snow/rain```   \n  \nwhere ```temp``` , ```windspeed``` and ```hum``` are normalized. \n\nNote :    \n- Data has been cleaned to drop outliers that might affect the model adversely\n- The model has been verified for Multicollinearity effects. \n- Residual Analysis has been carried out and the model satisfies the assumptions of Linear Regression (Residuals follow a normal distribution, Errors exhibit  homoscedasticity)\n- Q-Q plot between residual distribution and normal distribution shows that residuals follow a normal distribution for all interpolations. Extraplorations show significant deviation, not affecting Linear Regression applicability. \n- Further Lag plot shows there is no auto-correlation in data. \n- Model is stable at 81%(+/-14%) coefficient of determination at 95% CI, ascertained through cross validation.\n- Features in the order of influence has been reported by standardizing all predictor values. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}