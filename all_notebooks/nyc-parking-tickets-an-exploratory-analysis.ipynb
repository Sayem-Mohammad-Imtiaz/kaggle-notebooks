{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NYC Parking Tickets: An Exploratory Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Objectives of the Case Study"},{"metadata":{},"cell_type":"markdown","source":"- Primarily, this case study is meant as a deep dive into the usage of Spark. As you saw while working with Spark, its syntax behaves differently from a regular Python syntax. One of the major objectives of this case study is to gain familiarity with how analysis works in PySpark as opposed to base Python.\n- Learning the basic idea behind using functions in PySpark can help in using other libraries like SparkR. If you are in a company where R is a primary language, you can easily pick up SparkR syntax and use Spark’s processing power.\n- The process of running a model-building command boils down to a few lines of code. While drawing inferences from data, the most time-consuming step is preparing the data up to the point of model building. So, this case study will focus more on exploratory analysis."},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"Big data analytics allows you to analyse data at scale. It has applications in almost every industry in the world. Let’s consider an unconventional application that you wouldn’t ordinarily encounter.\n\nNew York City is a thriving metropolis. Just like most other metros its size, one of the biggest problems its citizens face is parking. The classic combination of a huge number of cars and cramped geography leads to a huge number of parking tickets.\n\nIn an attempt to scientifically analyse this phenomenon, the NYC Police Department has collected data for parking tickets. Of these, the data files for multiple years are publicly available on Kaggle. We will try and perform some exploratory analysis on a part of this data. Spark will allow us to analyse the full files at high speeds as opposed to taking a series of random samples that will approximate the population. For the scope of this analysis, we will analyse the parking tickets over the year 2017. \n\nNote: Although the broad goal of any analysis of this type is to have better parking and fewer tickets, we are not looking for recommendations on how to reduce the number of parking tickets—there are no specific points reserved for this.\n\nThe purpose of this case study is to conduct an exploratory data analysis that will help you understand the data. Since the size of the dataset is large, your queries will take some time to run, and you will need to identify the correct queries quicker. The questions given below will guide your analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Exploratory Analysis\") \\\n    .getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('/kaggle/input/nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2017.csv')\nparking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary statistics\nparking.describe().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datatype of columns\nparking.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rows\nparking.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns\nlen(parking.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop duplicates\nparking=parking.dropDuplicates()\nparking.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping null values if any\nparking=parking.dropna()\nparking.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking.select('Summons Number').distinct().count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking= parking.toDF(*(c.replace(' ', '_') for c in parking.columns))\nparking.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking.createOrReplaceTempView(\"parkingtable\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.sql('Select * from parkingtable')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total number of tickets for each year\nsql_ticket_year = spark.sql(\"select year(Issue_Date) as year, count(Summons_Number) as no_of_tickets from parkingtable group by year order by year\")\nsql_ticket_year.show(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\n    -- So we have data from 1972 to 2069\n    -- The data is centered around 2016-2017\n    -- For the scope of this analysis, we will analyse the parking tickets over the year 2017. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sql_ticket_year.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are 55 distinct years. As we have to consider data which belongs to 2017. We should consider only 2017.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering only 2017 data\nparking.createOrReplaceTempView(\"tble_view2017\")\nparking=spark.sql(\"select * from tble_view2017 where year(TO_DATE(CAST(UNIX_TIMESTAMP(Issue_Date,'MM/dd/yyyy') AS TIMESTAMP))) = 2017 \")\nparking.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For using SQL, you need to create a temporary view\nparking.createOrReplaceTempView(\"tble_view2017\")\n\n#Showing distribution \nDistribution_on_years= spark.sql(\"SELECT year(Issue_Date) as year,month(Issue_Date) as month,count(*) as Ticket_Frequency FROM tble_view2017 GROUP BY year(Issue_Date),month(Issue_Date) order by Ticket_Frequency desc\")\nDistribution_on_years.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Maximum number of violations are in the month of May. It has been observed that from July to December, there is a significant drop in number of violations.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Number_of_Violations_by_month = Distribution_on_years.toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.clf()\nNumber_of_Violations_by_month.plot(x= 'month', y='Ticket_Frequency', kind='bar')\nplt.title(\"Violations on the basis of month in 2017\")\nplt.xlabel('month')\nplt.ylabel('Ticket_Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Checking_null_values=spark.sql(\"Select count(*) as Number_of_Null_Values from tble_view2017 where Summons_Number is NULL or Plate_ID is NULL or Registration_State is NULL or Issue_Date is NULL or Violation_Code is NULL or Vehicle_Body_Type is NULL or Vehicle_Make is NULL or Violation_Precinct is NULL or Issuer_Precinct is NUll or Violation_Time is NULL \")\nChecking_null_values.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is no field with null value.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking on Plate_ID field to know if there are cases with same plate id.\n\nPlate_Id_Check=spark.sql(\"Select Plate_ID, count(*) as Ticket_Frequency from tble_view2017 group by Plate_ID having count(*)>1 order by Ticket_Frequency desc\")\nPlate_Id_Check.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is one value'BLANKPLATE' which we cannot track. Therefore, we can remove this.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"parking=parking[parking.Plate_ID!='BLANKPLATE']\nparking.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For using SQL, you need to create a temporary view\nparking.createOrReplaceTempView(\"tble_view2017\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see number of violations above 500\nPlate_Id_Above_500=spark.sql(\"Select Plate_ID, count(*) as Ticket_Frequency from tble_view2017 group by Plate_ID having count(*)>=500 order by Ticket_Frequency desc\")\nPlate_Id_Above_500.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot number of violation above 500\nNumber_of_Violations_By_PlateID=Plate_Id_Above_500.toPandas()\nplt.clf()\nNumber_of_Violations_By_PlateID.plot(x= 'Plate_ID', y='Ticket_Frequency', kind='bar')\nplt.title(\"Number of Violations above 500 \")\nplt.xlabel('Plate_ID')\nplt.ylabel('Ticket_Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are 7 Plate ID with more than 500 violations.**"},{"metadata":{},"cell_type":"markdown","source":"## Questions to Be Answered in the Analysis"},{"metadata":{},"cell_type":"markdown","source":"The following analysis should be performed on PySpark mounted on your CoreStack cluster, using the PySpark library. Remember that you need to summarise the analysis with your insights along with the code."},{"metadata":{},"cell_type":"markdown","source":"### Examine the data"},{"metadata":{},"cell_type":"markdown","source":"#### Q1. Find the total number of tickets for the year"},{"metadata":{"trusted":true},"cell_type":"code","source":"q1=spark.sql(\"Select count(*),count(distinct(Summons_Number)) from tble_view2017\")\nq1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As number of distinct Summons_Number is equal to total number of rows in data frame.That means there are no duplicate Summons_Number.**"},{"metadata":{},"cell_type":"markdown","source":"#### Q2. Find out the number of unique states from where the cars that got parking tickets came from. "},{"metadata":{"trusted":true},"cell_type":"code","source":"q2 = spark.sql(\"SELECT distinct(Registration_State), Count(*) as Number_of_Records from tble_view2017 group by Registration_State order by Number_of_Records desc\")\nq2.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q2.show(500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### There are 65 distinct values of Registration_State.\n- There is a numeric entry '99' in the column which should be corrected. We need to replace it with the state having maximum entries.\n- As maximum number of tickets are issued in NY, We will replace 99 by NY."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import when,lit\nparking=parking.withColumn('Registration_State',when(parking[\"Registration_State\"]==\"99\",lit('NY')).otherwise(parking[\"Registration_State\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking.createOrReplaceTempView(\"tble_view2017\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check again for number of distinct Registration_State.\n\nq2=spark.sql(\"SELECT Registration_State, Count(*) as Ticket_Frequency from tble_view2017 group by Registration_State order by Ticket_Frequency desc\")\nq2.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### There are 64 distinct values of Registration_State after replacing '99' with 'NY'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#By using SQL, create a temporary veiw:\nparking.createOrReplaceTempView(\"tble_view2017\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot violation on the basis of Registration State\nq2_for_plot = q2.toPandas()\nplt.figure(figsize=(100,200))\nq2_for_plot.head(10).plot(x='Registration_State', y='Ticket_Frequency', kind='bar')\nplt.title(\"Violations on the basis of Registration State (top 10)\")\nplt.xlabel('Registration State')\nplt.ylabel('Ticket Frequency')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregation tasks"},{"metadata":{},"cell_type":"markdown","source":"### Q1. How often does each violation code occur? Display the frequency of the top five violation codes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Violation Code count\nfrom pyspark.sql.functions import count,desc,countDistinct\nparking.select(countDistinct(\"Violation_Code\")).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency of each violation code occurs\nViolation_Code_count = parking.select(\"Violation_Code\")\\\n  .groupBy(\"Violation_Code\")\\\n.agg(count(\"Violation_Code\")\\\n.alias(\"no_of_tickets\"))\\\n.sort(desc(\"no_of_tickets\"))\n\nViolation_Code_count.show(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 5 Violation code\nq3 = spark.sql(\"SELECT Violation_Code, Count(*) as Ticket_Frequency from tble_view2017 group by Violation_code order by Ticket_Frequency desc\")\nq3.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 5 Violation code\nq3_for_plot = q3.toPandas()\nplt.clf()\nq3_for_plot.head(5).plot(x='Violation_Code', y='Ticket_Frequency', kind='bar')\nplt.title(\"Top Violation Code\")\nplt.xlabel('Violation Code')\nplt.ylabel('Ticket Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each vehicle body type get a parking ticket\nvehicleBodyType = spark.sql(\"SELECT Vehicle_Body_Type, count(*) as Ticket_Frequency from tble_view2017 group by Vehicle_Body_Type order by Ticket_Frequency desc\")\nvehicleBodyType.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot Violations on the basis of Vehicle_Body_Type\nvehicleBodyType_for_plot = vehicleBodyType.toPandas()\nplt.clf()\nvehicleBodyType_for_plot.head(5).plot(x='Vehicle_Body_Type', y='Ticket_Frequency', kind='bar')\nplt.title(\"Violations on the basis of Vehicle_Body_Type\")\nplt.xlabel('Vehicle Body Type')\nplt.ylabel('Ticket Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How about the 'vehicle make'?"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicleMake = spark.sql(\"SELECT Vehicle_Make, count(*) as Ticket_Frequency from tble_view2017 group by Vehicle_Make order by Ticket_Frequency desc\")\nvehicleMake.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot Violations on the basis of Vehicle_Make\nvehicleMake_for_plot = vehicleMake.toPandas()\nplt.clf()\nvehicleMake_for_plot.head(5).plot(x='Vehicle_Make', y='Ticket_Frequency', kind='bar')\nplt.title(\"Violations on the basis of Vehicle_Make\")\nplt.xlabel('Vehicle Make')\nplt.ylabel('Ticket Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q3 :  A precinct is a police station that has a certain zone of the city under its command.Find the (5 highest) frequency of tickets for each of the following:\n#### 1.'Violation Precinct' (this is the precinct of the zone where the violation occurred). Using this, can you make any insights for parking violations in any specific areas of the city?"},{"metadata":{"trusted":true},"cell_type":"code","source":"Violation_Precinct = spark.sql(\"SELECT Violation_Precinct, count(*) as Ticket_Frequency from tble_view2017 group by Violation_Precinct order by Ticket_Frequency desc\")\nViolation_Precinct.show(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Here, you would have noticed that the dataframe has the'Violating Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts. (Hint: Print the top six entries after sorting.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot Violations on the basis of Violation_Precinct\nViolation_Precinct_for_plot = Violation_Precinct.toPandas()\nplt.clf()\nViolation_Precinct_for_plot[Violation_Precinct_for_plot.Violation_Precinct!=0].head(5).plot(x='Violation_Precinct', y='Ticket_Frequency', kind='bar')\nplt.title(\"Violations on the basis of Violation_Precinct\")\nplt.xlabel('Vehicle Precinct')\nplt.ylabel('Ticket Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2 'Issuer Precinct' (this is the precinct that issued the ticket)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Issue_precinct = spark.sql(\"SELECT Issuer_Precinct, count(*) as Ticket_Frequency from tble_view2017 group by Issuer_Precinct order by Ticket_Frequency desc\")  \nIssue_precinct.show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Here, you would have noticed that the dataframe has the 'Issuing Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts. (Hint: Print the top six entries after sorting.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot Violations on the basis of Issue_Precinct\nIssue_Precinct_for_plot = Issue_precinct.toPandas()\nplt.clf()\nIssue_Precinct_for_plot[Issue_Precinct_for_plot.Issuer_Precinct!=0].head(5).plot(x='Issuer_Precinct', y='Ticket_Frequency', kind='bar')\nplt.title(\"Violations on the basis of Issuer Precinct TOP 5\")\nplt.xlabel('Issuer Precinct')\nplt.ylabel('Ticket Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So the top 5 area where most violation occurs are 19, 14, 1, 18 and  114.\n- Similarily,  the top 5 Issuer Precient are 19, 14, 1, 18 and  114"},{"metadata":{},"cell_type":"markdown","source":"### Q4. Find the violation code frequency across three precincts which have issued the most number of tickets.Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts? Hint: In the SQL view, use the 'where' attribute to filter among three precincts"},{"metadata":{},"cell_type":"markdown","source":"#### 4.1 Finding violation code frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"violation_code_freq = spark.sql(\"select Issuer_Precinct,Violation_Code, count(*) as Frequency from tble_view2017 group by Issuer_Precinct, Violation_Code order by Frequency desc\" )\nviolation_code_freq.show(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We are not considering 0. Therefore 18,19,14 are the three issuer precincts with maximum number of violations.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets dive into the Issuer Precinct one by one\n# Issuer Precinct 18 here\nviolation_code_freq_18 = spark.sql(\"select Violation_Code, count(*) as Frequency from tble_view2017 where Issuer_Precinct=18 group by Violation_Code order by Frequency desc\" )\nviolation_code_freq_18.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Issuer Precinct 19 here\nviolation_code_freq_19 = spark.sql(\"select Violation_Code, count(*) as Frequency from tble_view2017 where Issuer_Precinct=19 group by Violation_Code order by Frequency desc\" )\nviolation_code_freq_19.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Issuer Precinct 14 here\nviolation_code_freq_14 = spark.sql(\"select Violation_Code, count(*) as Frequency from tble_view2017 where Issuer_Precinct=14 group by Violation_Code order by Frequency desc\" )\nviolation_code_freq_14.show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Common codes across precincts"},{"metadata":{"trusted":true},"cell_type":"code","source":"common_codes =spark.sql(\"select Violation_Code, count(*) as Frequency from tble_view2017 where Issuer_Precinct in (18,19,14) group by Violation_Code order by Frequency desc\")\ncommon_codes.show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary:**\n    \n- Precinct 18 and Precinct 14 has more less similar top violation code.\n    \n- But Precinct 19 has very different top violation code."},{"metadata":{},"cell_type":"markdown","source":"### Q5.Find out the properties of parking violations across different times of the day:"},{"metadata":{},"cell_type":"markdown","source":"- Find a way to deal with missing values, if any.\n(Hint: Check for the null values using 'isNull' under the SQL. Also, to remove the null values, check the 'dropna' command in the API documentation.)\n\n- The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups.\n\n- Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.\n(Hint: Use the CASE-WHEN in SQL view to segregate into bins. To find the most commonly occurring violations, you can use an approach similar to the one mentioned in the hint for question 4.)\n\n- Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values\nnull_vltime_2017 = spark.sql(\"SELECT count(*) as No_of_Count_Values from tble_view2017 WHERE Violation_Time is NULL\")\nnull_vltime_2017.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for the null value\nfrom pyspark.sql.functions import col\nparking.where(col(\"Violation_Time\").isNull()).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parking.select('Violation_Time').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide 24 hours into six equal discrete bins of time.\nbins=spark.sql(\"SELECT Summons_Number, Violation_Code , Violation_Time, Issuer_Precinct, case when substring(Violation_Time,1,2) in ('00','01','02','03','12') and upper(substring(Violation_Time,-1))='A' then 1 when substring(Violation_Time,1,2) in ('04','05','06','07') and upper(substring(Violation_Time,-1))='A' then 2 when substring(Violation_Time,1,2) in ('08','09','10','11') and upper(substring(Violation_Time,-1))='A' then 3 when substring(Violation_Time,1,2) in ('12','00','01','02','03') and upper(substring(Violation_Time,-1))='P' then 4 when substring(Violation_Time,1,2) in ('04','05','06','07') and upper(substring(Violation_Time,-1))='P' then 5 when substring(Violation_Time,1,2) in ('08','09','10','11') and upper(substring(Violation_Time,-1))='P' then 6 else null end as Violation_Time_bin from tble_view2017 where Violation_Time is not null or (length(Violation_Time)=5 and upper(substring(Violation_Time,-1)) in ('A','P') and substring(Violation_Time,1,2) in ('00','01','02','03','04','05','06','07', '08','09','10','11','12'))\")\nbins.show()          ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bins Details\n\n    Bin       Time Interval\n    1         12:00 AM to 4:00 AM\n    2         4:00 AM to 8:00 AM\n    3         8:00 AM to 12:00 PM\n    4         12:00 PM to 4:00 PM\n    5         4:00 PM to 8:00 PM\n    6         8:00 PM to 12:00 AM"},{"metadata":{"trusted":true},"cell_type":"code","source":"bins.createOrReplaceTempView(\"bins_tbl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# violation code time count\nviolation_code_time_count = spark.sql(\"SELECT Violation_Code,Violation_Time_bin, count(*) count from bins_tbl group by Violation_Code,Violation_Time_bin\")\nviolation_code_time_count.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin1 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from bins_tbl where Violation_Time_bin == 1 group by Violation_Code order by Vio_cnt desc\")\nbin1.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin2 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from bins_tbl where Violation_Time_bin == 2 group by Violation_Code order by Vio_cnt desc\")\nbin2.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin3 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from bins_tbl where Violation_Time_bin == 3 group by Violation_Code order by Vio_cnt desc\")\nbin3.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin4 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from bins_tbl where Violation_Time_bin == 4 group by Violation_Code order by Vio_cnt desc\")\nbin4.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin5 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from bins_tbl where Violation_Time_bin == 5 group by Violation_Code order by Vio_cnt desc\")\nbin5.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin6 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from bins_tbl where Violation_Time_bin == 6 group by Violation_Code order by Vio_cnt desc\")\nbin6.show(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_bin = spark.sql(\"select Violation_Time_bin, count(*) Vio_count from bins_tbl where Violation_Code in (21, 36, 38) group by Violation_Time_bin order by Vio_count desc\")\ntime_bin.show(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bins 3, 4, 5 are having most violations**\n\nThe obvious reason could be, In day time significantly more vehicles were running and hence more violations. "},{"metadata":{},"cell_type":"markdown","source":"### Q6.Let’s try and find some seasonality in this data:"},{"metadata":{},"cell_type":"markdown","source":"#### a)First, divide the year into some number of seasons,and find frequencies of tickets for each season."},{"metadata":{"trusted":true},"cell_type":"code","source":"tickets_seasonality = spark.sql(\"select Violation_Code , Issuer_Precinct, case when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 03 and 05 then 'spring' when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 06 and 08 then 'summer' when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 09 and 11 then 'autumn' when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) in (1,2,12) then 'winter' else 'unknown' end  as season from tble_view2017\")\ntickets_seasonality.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Season Binning Details\n\n    Season    Month intervall\n    \n    spring    March, April, May\n    summer    June, July, August\n    autumn    September, October, November\n    winter    December, January, February"},{"metadata":{"trusted":true},"cell_type":"code","source":"tickets_seasonality.createOrReplaceTempView(\"tickets_seasonality_tbl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tickets_seasonality_freq = spark.sql(\"select season, count(*) as no_of_tickets from tickets_seasonality_tbl group by 1 order by 2 desc\")\ntickets_seasonality_freq.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spring season\nviolation_spring = spark.sql(\"select Violation_Code, count(*) as Frequency from tickets_seasonality_tbl where Issuer_Precinct in (19, 14, 1) and season = 'spring' group by Violation_Code order by Frequency desc\" )\nviolation_spring.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Winter season\nviolation_winter = spark.sql(\"select Violation_Code, count(*) as Frequency from tickets_seasonality_tbl where Issuer_Precinct in (19, 14, 1) and season = 'winter' group by Violation_Code order by Frequency desc\" )\nviolation_winter.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summer season\nviolation_summer = spark.sql(\"select Violation_Code, count(*) as Frequency from tickets_seasonality_tbl where Issuer_Precinct in (19, 14, 1) and season = 'summer' group by Violation_Code order by Frequency desc\" )\nviolation_summer.show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Autumn season\nviolation_autumn = spark.sql(\"select Violation_Code, count(*) as Frequency from tickets_seasonality_tbl where Issuer_Precinct in (19, 14, 1) and season = 'autumn' group by Violation_Code order by Frequency desc\" )\nviolation_autumn.show(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Q7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Let’s take an example of estimating this for the three most commonly occurring codes:"},{"metadata":{},"cell_type":"markdown","source":"#### a). Find total occurrences of the three most common violation codes"},{"metadata":{"trusted":true},"cell_type":"code","source":"common_Violation = spark.sql(\"select Violation_Code, count(*) as Frequency from tble_view2017 group by Violation_Code order by Frequency desc\")\ncommon_Violation.show(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b). Using this information, find the total amount collected for the three violation codes with maximum tickets. State the code which has the highest total collection."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import when\n\ncommon_Violation_fine=common_Violation.withColumn(\"fine\",when(common_Violation.Violation_Code == 21, (common_Violation.Frequency) *55).otherwise((common_Violation.Frequency)*50))\ncommon_Violation_fine.show(3)\nprint('Total collection = ',767740*55+662765*50+541526*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**code with 21 had the highest collection.**"},{"metadata":{},"cell_type":"markdown","source":"#### c).What can you intuitively infer from these findings?\n- Jan to June had the major violation & July  to Dec has a drastic drop.\n- Highest violation &collection was by Code-21(No parking where parking is not allowed by sign, street marking or traffic control device.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.stop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}