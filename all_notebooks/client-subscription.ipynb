{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[2]\").appName(\"Bank subscription\").getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sqlContext = SQLContext(spark.sparkContext)\nsqlContext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = spark.read.csv('/kaggle/input/banking-dataset-marketing-targets/train.csv',sep=\";\", header = True, inferSchema=True)\ntest =  spark.read.csv('/kaggle/input/banking-dataset-marketing-targets/test.csv',sep=\";\",  header = True, inferSchema=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer\nindexer = StringIndexer(inputCols=[\"job\",\"marital\",\"education\",'default','housing','loan','contact','poutcome',\"y\"], outputCols = [\"job_E\",\"marital_E\",\"education_E\",'default_E','housing_E','loan_E','contact_E','poutcome_E',\"Y\"]) \nindexer = indexer.fit(train)\nindexed = indexer.transform(train) \ntest = indexer.transform(test)\nindexed.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_list = [\"job\",\"marital\",\"education\",'default','housing','loan','contact','month','poutcome',\"y\"]\ntrain = indexed.select([column for column in indexed.columns if column not in drop_list])\ntest = test.select([column for column in test.columns if column not in drop_list])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().toPandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe().toPandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_missings(spark_df,sort=True):\n    \"\"\"\n    Counts number of nulls and nans in each column\n    \"\"\"\n    df = spark_df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for (c,c_type) in spark_df.dtypes if c_type not in ('timestamp', 'string', 'date')]).toPandas()\n\n    if len(df) == 0:\n        print(\"There are no any missing values!\")\n        return None\n\n    if sort:\n        return df.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_missings(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_missings(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly observe that both our training and test set doesn't have any form of missing values","metadata":{}},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LinearSVC, RandomForestClassifier, DecisionTreeClassifier,  LogisticRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assembler = VectorAssembler(inputCols=train.columns , outputCol=\"features\") \ntrain = assembler.transform(train)\ntest = assembler.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.select(\"Y\").show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import PCA as PCAml\n\npca = PCAml(k=4, inputCol=\"features\", outputCol=\"pca\")\nmodel = pca.fit(train)\ntrain = model.transform(train)\ntest = model.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = standardScaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rain_forest = RandomForestClassifier(labelCol=\"Y\", featuresCol=\"features_scaled\")\nrain_forest_model = rain_forest.fit(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = LinearSVC(labelCol=\"Y\", featuresCol=\"features_scaled\")\nsvc_model = svc.fit(train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree = DecisionTreeClassifier(labelCol=\"Y\", featuresCol=\"features_scaled\")\ntree_model = tree.fit(train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(labelCol=\"Y\", featuresCol=\"features_scaled\")\nlr_model = lr.fit(train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}