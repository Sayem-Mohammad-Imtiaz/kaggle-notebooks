{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analysis of Toronto Fire Incidents\n\nCity of Toronto provides data throgh their [OpenData Poral](https://www.toronto.ca/city-government/data-research-maps/open-data/). In this notebook we examine the [Fire Incidents](https://open.toronto.ca/dataset/fire-incidents/). We also use [fire station locations](https://open.toronto.ca/dataset/fire-station-locations/) and [area maps](https://open.toronto.ca/dataset/toronto-fire-services-run-areas/). \n\nThis is my first Notebook and Dataset. Your feedback is appreciated.\n\n## About Dataset\nThis dataset provides information similar to what is sent to the Ontario Fire Marshal relating to only fire Incidents to which Toronto Fire responds in more detail than the dataset including all incident types. The Dataset includes only Fire incidents as defined by the Ontario Fire Marshal. For privacy purposes personal information is not provided and exact address have been aggregated to the nearest major or minor intersection. Some incidents have been excluded pursuant to exemptions under Section 8 of Municipal Freedom of Information and Protection of Privacy Act (MFIPPA)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import urllib.request\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport json\nfrom scipy import stats\nfrom matplotlib.patches import Circle, Wedge, Polygon\nfrom matplotlib.collections import PatchCollection\nimport matplotlib.pyplot as plt\nimport shapefile\n%matplotlib notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Specify the path to load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path ='/kaggle/input/toronto-fire-incidents/' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#helper function to get fire area maps as polygons\ndef get_firestaion_areas():\n    with open(path +  'FIRE_RUN_AREA_WGS84.json', 'r') as f:\n        polygons = json.load(f)\n    return polygons\n#helper function to get fire station locations\n\ndef get_firestation_locations():\n    with open(path +'FIRE_FACILITY_WGS84.json', 'r') as f:\n        facilities = json.load(f)\n    fire_stations = np.array(facilities)\n    return fire_stations\n\n# helper function to load incident data\ndef get_fire_incidents_data():\n    df = pd.read_csv(path + 'Fire Incidents Data.csv') \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load indicents as pandas data frame\ndf = get_fire_incidents_data()\n# load the area covered by each fire station as a list of polygons\nareas = get_firestaion_areas()\n# load fire station locatins\nstations = get_firestation_locations()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a data frame for fire incidents and see what data is available"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will drop columns we don't want to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = df.drop([\"Building_Status\",\\\n              \"Business_Impact\",\\\n              \"Count_of_Persons_Rescued\",\\\n              \"Estimated_Number_Of_Persons_Displaced\",\\\n              \"Exposures\",\\\n              \"Ext_agent_app_or_defer_time\",\\\n              \"Extent_Of_Fire\",\\\n              \"Final_Incident_Type\",\\\n              \"Fire_Alarm_System_Impact_on_Evacuation\",\\\n              \"Fire_Alarm_System_Operation\",\\\n              \"Fire_Alarm_System_Presence\",\\\n              \"Ignition_Source\",\\\n              \"Incident_Number\",\\\n              \"Incident_Station_Area\",\\\n              \"Incident_Ward\",\\\n              \"Initial_CAD_Event_Type\",\\\n              \"Intersection\",\\\n              \"Number_of_responding_apparatus\",\\\n              \"Number_of_responding_personnel\",\\\n              \"Property_Use\",\\\n              \"Smoke_Alarm_at_Fire_Origin\",\\\n              \"Smoke_Alarm_at_Fire_Origin_Alarm_Failure\",\\\n              \"Smoke_Alarm_at_Fire_Origin_Alarm_Type\",\\\n              \"Smoke_Alarm_Impact_on_Persons_Evacuating_Impact_on_Evacuation\",\\\n              \"Smoke_Spread\",\\\n              \"Sprinkler_System_Operation\",\\\n              \"Sprinkler_System_Presence\", \\\n              \"Status_of_Fire_On_Arrival\",\\\n              \"TFS_Firefighter_Casualties\",\\\n              \"Level_Of_Origin\"], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at a few rows of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Data\nSome columns are missing data, we are goig to fill them the best we can. Let's see which datais missing."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Latitude'] = df['Latitude'].fillna(df['Latitude'].mean())\ndf['Longitude'] = df['Longitude'].fillna(df['Longitude'].mean())\ndf['Estimated_Dollar_Loss'] = df['Estimated_Dollar_Loss'].fillna(df['Estimated_Dollar_Loss'].median())\ndf['Fire_Under_Control_Time'] = df['Fire_Under_Control_Time'].fillna(df['Last_TFS_Unit_Clear_Time'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing timestamp string with python datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_date_time_from_str(timestamp_str):\n    date_str = timestamp_str[0:timestamp_str.find('T')]\n    time_str = timestamp_str[timestamp_str.find('T')+1:]\n    [y,m,d] = [int(s) for s in date_str.split('-')]\n    [hh,mm,ss] = [int(s) for s in time_str.split(':')]\n    return datetime.datetime(y,m,d,hh,mm,ss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TFS_Alarm_Time'] =df[\"TFS_Alarm_Time\"].apply(lambda x: get_date_time_from_str(x))\ndf['TFS_Arrival_Time'] =df[\"TFS_Arrival_Time\"].apply(lambda x: get_date_time_from_str(x))\ndf['Fire_Under_Control_Time'] =df[\"Fire_Under_Control_Time\"].apply(lambda x: get_date_time_from_str(x))\ndf['Last_TFS_Unit_Clear_Time'] =df[\"Last_TFS_Unit_Clear_Time\"].apply(lambda x: get_date_time_from_str(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert date time to numerical values for processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Day_Of_Week'] = df[\"TFS_Alarm_Time\"].apply(lambda x: x.weekday())\ndf['Alarm_Time_Min_Of_Day'] = df[\"TFS_Alarm_Time\"].apply(lambda x: x.time().hour * 60 + x.time().minute)\ndf['Alarm_Time_Hour_OF_Day'] = df[\"TFS_Alarm_Time\"].apply(lambda x: x.time().hour)\ndf['Alarm_Time_Day_Of_Year'] = df[\"TFS_Alarm_Time\"].apply(lambda x: x.timetuple().tm_yday)\ndf['Response_Time'] = df.apply(lambda x: (x['TFS_Arrival_Time'] - x['TFS_Alarm_Time']).total_seconds(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization\nLet's look at some of the aspects of the data\n\n## Dollor Loss on Map\nLatitude, Longitude and dollor loss is available for most of the data. We are going to plot all fire locations and scale the size of each inicident based on dollor loss "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndollor_loss= df['Estimated_Dollar_Loss'].to_numpy()\n\nsize = np.clip(10 * dollor_loss / np.mean(dollor_loss),1.0,np.max(dollor_loss))\npatches = []\n\nfor polygon in areas:\n    polygon_patch = Polygon(polygon, True)\n    patches.append(polygon_patch)\n\n\ncolors = 100*np.random.rand(len(patches))\np = PatchCollection(patches, alpha=0.4,edgecolor = 'k')\nfig, ax = plt.subplots(figsize=(15, 10))\np.set_array(np.array(colors))\nax.add_collection(p)\nax.set_title('Fire incidents in Toronto. Each circle indicates an incident with size of \\\nthe circle scaled by estimated loss. Maximum loss:%.2f.\\nBlue squares show fire station locations within each area'%dollor_loss.max())\nplt.autoscale(tight=True)\nplt.scatter(df['Longitude'], df['Latitude'] , s=size, c ='r', alpha=0.3, marker = 'o')\nplt.scatter(stations[:,0], stations[:,1] , marker = 's', s = 10, facecolors='none', edgecolors='b')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see the histogram of dollor loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_dollor_loss = np.max(dollor_loss)\nstd_dollor_loss =np.std(dollor_loss)\nhist_low = 0\nhist_high = int( np.mean(dollor_loss))\nbins = range(hist_low,hist_high, (hist_high - hist_low)//10)\n\nh = df.hist(column = [ 'Estimated_Dollar_Loss'], bins=bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Civilian Casualties on map\nLet's plot the number of casualties "},{"metadata":{"trusted":true},"cell_type":"code","source":"cassulties= df['Civilian_Casualties'].to_numpy().astype(float)\nsize = 1000 * cassulties/np.max(cassulties)\n\npatches = []\n\nfor polygon in areas:\n    polygon_patch = Polygon(polygon, True)\n    patches.append(polygon_patch)\n\n\np = PatchCollection(patches, alpha=0.4, edgecolor = 'k')\nfig, ax = plt.subplots(figsize=(15, 10))\np.set_array(np.array(colors))\nax.add_collection(p)\nax.set_title('Fire incidents in Toronto. Each circle indicates an incident with size of \\\nthe circle scaled by number of cassulties. Maximum cassulties:%d.\\nBlue squares show fire station locations within each area'\\\n%df['Civilian_Casualties'].max())\nplt.autoscale(tight=True)\nplt.scatter(df['Longitude'], df['Latitude'], s= size,c='r', alpha=0.3, marker = 'o')\nplt.scatter(stations[:,0], stations[:,1] , marker = 's', s = 10, facecolors='none', edgecolors='b')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fire incident analysis\nIn this section we look at data we have regarding the incident itself. We look at 4 properties of the fire and draw pie charts based on number of occurances of. The properties are: Area_of_Origin, Material_First_Ignited, Possible_Cause, Method_Of_Fire_Control. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There a few columns such as 'Area_of_Origin' where data is specified with a code and \n# tag. such as :'81 - Engine Area'. Tis helper function returns most occured code/tag for\n# a column\ndef get_top_from_list(df,column, size = 10):\n    column_values =  df[column]\n#     create to dictionaries to keep tags and count\n    column_values_tags ={}\n    column_values_count ={}\n# for all values in column split code from tag\n    for a in column_values:\n        split = a.split('-')\n\n        code = int(split[0])\n        tag = split[1]\n\n        if not code in column_values_count:\n            column_values_count[code] = 0\n        else:\n            column_values_count[code]+=1\n\n        if not code in column_values_tags:\n            column_values_tags[code] = tag\n        elif  column_values_tags[code] != tag:\n            print(\"warning mismatch tag and code\")  \n# convert dictionaries to list\n    tag_count = []\n    keys = list(column_values_tags.keys())\n    for k in keys:\n        count = column_values_count[k]\n        tag = column_values_tags[k] + ' :'+ str(count)\n        tag_count.append([count, tag])\n    count_np = np.array(column_values_count)\n\n    tag_count.sort(reverse = True)\n    \n    if size < len(tag_count):\n        sum_rest = 0\n        for a in tag_count[size:]:\n            sum_rest += a[0]\n        del tag_count[size:]\n        tag_count.append([sum_rest, 'others :'+ str(sum_rest)])\n\n    counts = []\n    tags = []\n    for tc in tag_count:\n        counts.append(tc[0])\n        tags.append(tc[1])\n\n    return counts, tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Area_of_Origin\narea_of_origin_count, area_of_origin_tags = get_top_from_list(df,'Area_of_Origin',9)\nmaterial_count, material_tags = get_top_from_list(df,'Material_First_Ignited',9)\ncause_count, cause_tags = get_top_from_list(df,'Possible_Cause',5)\nmethod_count, method_tags = get_top_from_list(df,'Method_Of_Fire_Control',5)\n\n\nfig, ax = plt.subplots(figsize=(15, 15))\nplt.subplot(2,2,1)\nplt.pie(area_of_origin_count,  labels=area_of_origin_tags)\nplt.title('Area of Origin')\n\nplt.subplot(2,2,2)\nplt.pie(material_count,  labels=material_tags)\nplt.title('Material First Ignited')\n\nplt.subplot(2,2,3)\nplt.pie(cause_count,  labels=cause_tags)\nplt.title('Possible Cause')\n\nplt.subplot(2,2,4)\nplt.pie(method_count,  labels=method_tags)\nplt.title('Method Of Fire_Control')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Response Time Analysis:\nIn this section we study factors that affect reponse time. Which is defined as the duration between when a 911 call is made to first responders arrive at the scene"},{"metadata":{},"cell_type":"markdown","source":"We are going to create another data frame \"df_time\" where we put all data needed for time analysis"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_time =df[['Response_Time', 'Latitude','Longitude', 'Alarm_Time_Min_Of_Day', 'Alarm_Time_Day_Of_Year','Day_Of_Week']]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to remove outliers using z_score to remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"z_score = np.abs(stats.zscore(df_time))\ndf_time = df_time[(z_score < 2.5).all(axis=1)]\nnum_data_removed =  len(df)-len(df_time)\nprint(\"removed %d outliers from total of %d records\"%(num_data_removed,len(df)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"l=Let's look at how response time changes based on, time in day, date and day of week"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 3))\n\nplt.subplot(1,2,1)\nplt.hist(df_time['Day_Of_Week'], bins=np.arange(-.5,7.,1.0),  rwidth=0.85)\nplt.xticks(np.arange(7), ('Mon', 'Tue', 'Wed', 'Thur', 'Fri', 'Sat','Sun'))\nplt.title('Number of incidents in each day of week')\nplt.subplot(1,2,2)\nbins=np.arange(-.5,365,30)\nplt.hist(df_time['Alarm_Time_Day_Of_Year'],  bins=bins, align= 'left', rwidth=0.85)\nplt.xticks(bins, ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun','Jul','Aug','Sep','Oct','Nov','Dec',''))\nplt.title('Number of incidents in each month')\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 3))\ns=np.arange(-.5,24)\nplt.hist(df['Alarm_Time_Hour_OF_Day'], bins = 24, rwidth=0.85)\nplt.title('Number of incidents 24 hours')\nplt.xlabel('Time of Day')\nplt.ylabel('ResponseTime(s)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Response time based on location"},{"metadata":{},"cell_type":"markdown","source":"we are going to plot average response time on the map and see how response time changes based on where the incident happened"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.tri as tri\n# -----------------------\n# Interpolation on a grid\n# -----------------------\n# A contour plot of irregularly spaced data coordinates\n# via interpolation on a grid.\nngridx = 150\nngridy = 100\n# Create grid values first.\nxi = np.linspace(df_time['Longitude'].min(),df_time['Longitude'].max(), ngridx)\nyi = np.linspace(df_time['Latitude'].min(), df_time['Latitude'].max(), ngridy)\n\nx = df_time['Longitude']\ny = df_time['Latitude']\nz = df_time['Response_Time']\nfig, ax = plt.subplots(figsize=(15,8))\n\n\ntriang = tri.Triangulation(x,y)\ninterpolator = tri.LinearTriInterpolator(triang, z/60)\nXi, Yi = np.meshgrid(xi, yi)\nzi = interpolator(Xi, Yi)\nax.contour(xi, yi, zi, levels=10, linewidths=0.5, colors='k')\ncntr1 = ax.contourf(xi, yi, zi, levels=10, cmap=\"plasma\")\n\nfig.colorbar(cntr1, ax=ax)\nax.set(xlim=(df_time['Longitude'].min(),df_time['Longitude'].max()),\\\n        ylim=(df_time['Latitude'].min(), df_time['Latitude'].max()))\nax.set_title('Average response time of Toronto Fire Services based on location, in minutes.\\\n\\nBlue squares show fire station locations')\n\n\n# draw fire station area map\npatches = []\n\nfor polygon in areas:\n    polygon_patch = Polygon(polygon)\n    patches.append(polygon_patch)\n\n\np = PatchCollection(patches,facecolor = '',edgecolor = 'k' , linewidths=1.0)\nax.add_collection(p)\nplt.scatter(stations[:,0], stations[:,1] , marker = 's', c='c', s = 5.0)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Response time Estimation\nin this section we try to predict how long is it going take for Toronto Fire Service to arrive after a call is made.\nHere are the features we considered **Latitude, Longitude, Alarm_Time_Min_Of_Day, Alarm_Time_Day_Of_Year, Day_Of_Week**"},{"metadata":{},"cell_type":"markdown","source":"Let's prepare the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata_x = df_time[['Latitude','Longitude', 'Alarm_Time_Min_Of_Day', 'Alarm_Time_Day_Of_Year','Day_Of_Week']]\ndata_y = df_time[['Response_Time']]\n\n# setup normalization\nfrom sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ny_scaler = MinMaxScaler()\n\n\nx_scaler.fit(data_x)\ny_scaler.fit(data_y)\n\ndata_x_scaled = x_scaler.transform(data_x)\ndata_y_scaled = y_scaler.transform(data_y)\n\ndata_y_scaled = np.ravel(data_y_scaled)\n# Split data to train/test\ntrain_x, test_x, train_y, test_y = train_test_split(data_x_scaled, data_y_scaled, test_size=0.1, random_state=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#helper function to evaluate a model\nfrom sklearn.metrics import mean_squared_error\ndef evaluate_model(model, model_name = None):\n    model.fit(train_x,train_y)\n    pred = model.predict(test_x)\n    error = mean_squared_error(test_y, pred)\n    if model_name is not None:\n        print(\"Error for %s, : %.6f\"%(model_name, error))\n    return error\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. In this section we evaluate some regression methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nfrom sklearn import neighbors\nfrom sklearn import tree\nfrom sklearn import svm\nmodels =[LinearRegression(),linear_model.Ridge(),\\\n         neighbors.KNeighborsRegressor(5,weights='uniform'), linear_model.BayesianRidge(),\\\n         tree.DecisionTreeRegressor(max_depth=4),svm.SVR(gamma = 'auto')]\ntags = [\"inearRegression\",\"Ridge\", 'knn5', \"BayesianRidge\", 'tree4','svr']\nfor model,tag in zip(models,tags):\n    error = evaluate_model(model,tag)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}