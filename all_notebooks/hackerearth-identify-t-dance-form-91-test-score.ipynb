{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center><b>HackerEarth Deep Learning challenge: Identify the dance form</b></center>\n\n<center><img src=\"https://media-fastly.hackerearth.com/media/hackathon/hackerearth-deep-learning-challenge-identify-dance-form/images/b163aaca99-DanceForm_FB.jpg\" height=400 width=700/></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center>Timeline - May 21, 07:30 AM IST - Jul 05, 07:30 AM IST</center>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem statement\n\nThis International Dance Day, an event management company organized an evening of Indian classical dance performances to celebrate the rich, eloquent, and elegant art of dance. Post the event, the company planned to create a microsite to promote and raise awareness among the public about these dance forms. However, identifying them from images is a tough nut to crack.\n\n\nYou have been appointed as a Machine Learning Engineer for this project. \n- <font color='red'><b>Build an image tagging Deep Learning model that can help the company classify these images into eight categories of Indian classical dance</b></font>.\n\n## Dataset\n\nThe dataset consists of __364 images__ belonging to 8 categories, namely \n- manipuri, \n- bharatanatyam, \n- odissi, \n- kathakali, \n- kathak, \n- sattriya, \n- kuchipudi, and \n- mohiniyattam.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://qph.fs.quoracdn.net/main-qimg-2ca0fa1346eccd87a882bc1c873e6001.webp\"/></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metric\n- The evaluation metric for this competition is ```Accuracy```.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## My approach ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As the data is very limited for this hackathon if you try rigorous training, model will over fit. \n\n- And here comes in rescue are  the [pretrained model](https://docs.fast.ai/vision.learner.html) \n\n- For the given data set i used [Resnet152 pretrained model](https://www.kaggle.com/pytorch/resnet152).\n\n- Library used is [Fastai](https://www.fast.ai/) which built on top of pytorch.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <center><font color='brown'>Using Progressive Resizing Technique</font></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://www.wisdomrobot.com/wp-content/uploads/2017/02/Diagram-Coins-Business-Coin-Bar-Achievement-Chart-18134-960x675.jpg\"height =400 width=400/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Basic imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To print multiple output in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install this version to avoid the multiple warning \n!pip install \"torch==1.4\" \"torchvision==0.5.0\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#from albumentations import *\nimport cv2\nimport copy\nimport os\nimport torch\nprint(os.listdir(\"../input\"))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport glob\n\n#!pip install pretrainedmodels\nfrom tqdm import tqdm_notebook as tqdm\nfrom torchvision.models import *\n#import pretrainedmodels\n\nfrom pathlib import Path\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\nfrom fastai.callbacks import * \n\n#from utils import *\nimport sys\n\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Driven tasks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## set the data folder\ndata_folder=Path('../input/identify-the-dance-form')\n\nprint(os.listdir(data_folder))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recompute_scale_factor=True\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_path = \"../input/identify-the-dance-form/train\"\ntrain_path = os.path.join(train_data_path, \"*jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_path = \"../input/identify-the-dance-form/test\"\ntest_path = os.path.join(test_data_path, \"*jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files = glob.glob(train_path)\ntrain_images=[]\nfor file in train_files:\n    image = cv2.imread(file)\n#     print(image.shape)\n    train_images.append(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_images))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_files = glob.glob(test_path)\ntest_images=[]\nfor file in test_files:\n    image = cv2.imread(file)\n    print(image.shape)\n    test_images.append(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_images))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above code cell\n\n- All the images are of different sizes, need to resize them to one before training.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## read the csv data files\ntrain_df = pd.read_csv('../input/identify-the-dance-form/train.csv')\ntest_df = pd.read_csv('../input/identify-the-dance-form/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(3)\ntest_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode the target variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target']=train_df['target'].map({'mohiniyattam':0,'odissi':1,'kathakali':2,\n                                           'bharatanatyam':3,'kuchipudi':4,'sattriya':5,\n                                           'kathak':6,'manipuri':7})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we can see that the training data is very less.\n\nSo training the model from the beginning is not feasible, Here comes the pretrained model in the picture so for this we gonna use different pretrained model with ```fastai library```.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <center>Progressive Resizing</center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> *Progressive resizing is a technique for building CNNs that can be very helpful during the training and optimization phases of a machine learning project. The technique appears most prominently in Jeremy Howardâ€™s work, and he uses it to good effect throughout his terrific fast.ai course.(Course Part-1 - Lecture3)*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- It is the technique to sequentially resize all the images while training the CNNs on smaller to bigger image sizes.\n\n-  Best way to use this technique is to train a model with smaller image size say ```128x128```, then use the weights of this model to train another model on images of size ```256x256, 512x512``` and so on. \n\n- Each larger-scale model incorporates the previous smaller-scale model layers and weights in its architecture.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Transformation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will maintain the same transformations tricks to all models of progressive resizing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##transformations to be done to images\n\ntfms = get_transforms(do_flip=True,flip_vert=False ,max_rotate=10.0, max_zoom=1.22, max_lighting=0.22, max_warp=0.4, p_affine=0.75,\n                      p_lighting=0.75)\n\n\ntest_img = ImageList.from_df(test_df, path=data_folder, folder='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create source of train image databunch\nnp.random.seed(45)\n\nsrc = (ImageList.from_df(train_df, path=data_folder, folder='train')\n       .split_by_rand_pct(0.2)\n       #.split_none()\n       .label_from_df()\n       .add_test(test_img))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# considering image size of 128\n\ndata = (src.transform(tfms, size=128,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n        .databunch(path='.', bs=32, device= torch.device('cuda:0')).normalize(imagenet_stats));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.classes)\ndata.show_batch(rows=3, figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a ```Learner```.\n\nPreviously i use ```resnet152``` as base architecture as it performs well so will try that here too,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# acc_02 = partial(accuracy_thresh, thresh=0.2)\n# f_score = partial(fbeta, thresh=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets create learner. tried with resnet152, densenet201, resnet101\n# learn = cnn_learner(data=data, base_arch=models.resnet152, metrics=[FBeta(beta=1, average='macro'), accuracy],\n#                     callback_fns=ShowGraph).mixup()\n\n# will train first without mixup\n\n#lets create learner. tried with resnet152, densenet201, resnet101\n# learn = cnn_learner(data=data, base_arch=models.resnet152, metrics=[FBeta(beta=1, average='macro'), accuracy],\n#                     callback_fns=ShowGraph).mixup()\n\nlearn = cnn_learner(data=data, base_arch=models.resnet50, metrics=[FBeta(beta=1, average='macro'), accuracy],\n                    callback_fns=ShowGraph)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We use the __LR Finder__ to pick a good ```learning rate```.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can fit the head of our network.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr=1e-03","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, max_lr=1e-03)\n\n# learn.fit_one_cycle(5, slice(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, max_lr=1e-04)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-1-resnet-152-img_size-128')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(9, figsize=(15,11))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"..And fine-tune the whole model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# previously is is trained for 1e-4 \nlearn.fit_one_cycle(10, slice(1e-4),wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-2-rn152')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the weight of previous model we will again train it with the new image size-```256*256```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# considering image size of 256\ndata = (src.transform(tfms, size=256,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n        .databunch(path='.', bs=16, device= torch.device('cuda:0')).normalize(imagenet_stats));\n\nlearn.data = data\ndata.train_ds[0][0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As in previous layer we unfreeze the whole model so let's freeze it once again so that we will train \n# for last layers only\nlearn.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=8e-5\n\n# lr=3e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model seems to overfit try to use weight decay wd=0.1\nlearn.fit_one_cycle(10, slice(lr),wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-1-256-rn152')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr=1e-05\n\n# lr=1e-04","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, slice(1e-4, lr/5),wd=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-2-256-rn152')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# considering image size of 512\ndata = (src.transform(tfms, size=512,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n        .databunch(path='.', bs=8, device= torch.device('cuda:0')).normalize(imagenet_stats));\n\nlearn.data = data\ndata.train_ds[0][0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr=1e-03\n\nlr=3e-04","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.fit_one_cycle(15, slice(5e-4, lr/5))\n\nlearn.fit_one_cycle(10, slice(lr),wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-1-512-rn152')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr=1e-05\nlr=3e-06","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, slice(3e-06, lr/10),wd=0.1)\n\n# In next step what you can try is to run for only 10 epochs to avoid overfitting.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-2-512-rn152')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix Check","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy Check","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##learn.TTA improves score further. lets see for the validation set\npred_val,y = learn.TTA(ds_type=DatasetType.Valid)\nvalid_preds = [np.argmax(pred_val[i]) for i in range(len(pred_val))]\nvalid_preds = np.array(valid_preds)\ny = np.array(y)\naccuracy_score(valid_preds,y),f1_score(valid_preds,y, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds,y = learn.TTA(ds_type=DatasetType.Test)\npreds,_ = learn.get_preds(ds_type = DatasetType.Test)\nlabelled_preds = [np.argmax(preds[i]) for i in range(len(preds))]\n\nlabelled_preds = np.array(labelled_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create final submissions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create submission file\ndf = pd.DataFrame({'Image':test_df['Image'], 'target':labelled_preds}, columns=['Image', 'target'])\n\ndf['target']=df['target'].map({0:'mohiniyattam',1:'odissi',2:'kathakali',\n                                           3:'bharatanatyam',4:'kuchipudi',5:'sattriya',\n                                           6:'kathak',7:'manipuri'})\n\ndf.head()\n\ndf.to_csv('submission_mode_resnet-Stage2_512_new.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keypoints:  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- In the very last you see a final submission for image_size=512.\n- It's not necessary that only the model of image_size=512 perform well other size models also performed well and sometime they outperform the model trained on bigger image_size.\n\n- While training you will perform multile techniques, most important thing is keep track of all your parametrs and models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Note: ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center>\n - If this kernel helped you:\n\n    - Do upvote\n    - Do follow\n    - In case you have any query use comment section.\n</center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTkN7ooAwGVuRCg_9axVg1XzVLLvb_e28PR_w&usqp=CAU/'>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}