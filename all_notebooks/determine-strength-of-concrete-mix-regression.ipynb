{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Featurization and Model Tuning\n## Data Description :\nThe concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory.\n### Domain  : Cement manufacturing/Civil Engineering\n### Context : \nThe concrete compressive strength is a highly nonlinear function of age and ingredients.\nThese ingredients include cement, blast furnace slag, fly ash, water,\nsuperplasticizer, coarse aggregate, and fine aggregate"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values={'?','NA','nA','Na','na'}\ndf=pd.read_csv('../input/yeh-concret-data/Concrete_Data_Yeh.csv',na_values=values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Our Dataset has:{} rows and columns:{}'.format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory data quality report"},{"metadata":{},"cell_type":"markdown","source":"### Univariate analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Data types and Names of the independent attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Data contains 9 columns, which are non null and neumeric in nature"},{"metadata":{},"cell_type":"markdown","source":"#### Central Tendencies (Mean, Min- Max(Range)), Standard Deviation, Quantiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Independent variables range  measured in kg in a m3 mixture :\ncement - 102 to 540\nslag - 0 to 359\nash - 0 to 200\nwater - 121 to 247\nsuperplastic - 0 to 32\ncoarseagg - 801 to 1145\nage - 1 to 365\nstrength - 2.3 to 87 \n\n"},{"metadata":{},"cell_type":"markdown","source":"Let's check for duplicates in our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Numbder of duplicate rows in our data is:{}\".format(df.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop_duplicates(subset=None,keep='first',inplace=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am using drop_duplicates from pandas to eliminate duplicate values and retain only the first occuerence of the duplicates for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('After Removing Duplicates Our Dataset has:{} rows and columns:{}'.format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=list(df)\ndf[columns].hist(stacked=True,density=True, bins=100,color='blue', figsize=(16,30), layout=(10,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. From the above Histogram we could see that cement, coarseag,fineagg, strength and water are almost normally distributed.\n2. Age , Ash, superlastic are slightly skewed."},{"metadata":{},"cell_type":"markdown","source":"#### Data Skewness & Distribution of curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,9,figsize=(15,8))\nsns.distplot(df['cement'],ax=ax[0],kde=True,hist=False)\nsns.distplot(df['slag'],ax=ax[1],kde=True,hist=False)\nsns.distplot(df['flyash'],ax=ax[2],kde=True,hist=False)\nsns.distplot(df['water'],ax=ax[3],kde=True,hist=False)\nsns.distplot(df['superplasticizer'],ax=ax[4],kde=True,hist=False)\nsns.distplot(df['coarseaggregate'],ax=ax[5],kde=True,hist=False)\nsns.distplot(df['fineaggregate'],ax=ax[6],kde=True,hist=False)\nsns.distplot(df['age'],ax=ax[7],kde=True,hist=False)\nsns.distplot(df['csMPa'],ax=ax[8],kde=True,hist=False)\nplt.show()\nprint(df.skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In terms of distribution slag, ash, water, superplastic, coarseagg, fineagg , age are all multi gaussian which means they have multiple peaks and valleys.\nStrength seems to be normally distributed, cement has a slightly sharp multiple peaks.\n\n#### Tails\n1. Cement seems to be normally distibuted\n2. slag is slightly skewed towads right\n3. ash is normally distributed\n4. water is slighly skewd towards right\n5. superlastic is skewd towards right\n6. coarseagg is normally distributed\n7. fineagg is normally distributed\n8. age is slighly skewed positively \n9. strength is normally distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no Null values in our dataset."},{"metadata":{},"cell_type":"markdown","source":"#### Check the presence of outliers through box plot\n\nOutliers , are extreme values present in the data.There are outlires in our data for some columns as you can see from the below boxplot, Outliers have an impact on all ML algorithms. We should find ways to fix outliers "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.boxplot(data=df)\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Traget Column call out :\n##### In this dataset our variable of interest is the strength column. It is a continous variable which depends on various other parameters in evaluation of the concrete mixture."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df,hue_order=df['csMPa'],diag_kind='kde');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor=df.corr()\nsns.heatmap(cor,annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Pair plot analysis:\n1. Along the Diagonal, our data has 2-3 gaussians for all the predictor variables. We should do a cluster analysis to understand the grouping and hidden pattern in data.\n2. Our predictors have some reationship and dependencies with target. \n3. From the correlation matrix we could infer that , our variables have less correraltion between each other. This is good, as most ML algorithms assume variables are independent of each other for better prediction.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Cor_Matrix=df.corr().abs()\nCor_Matrix\nupper_tri = Cor_Matrix.where(np.triu(np.ones(Cor_Matrix.shape),k=1).astype(np.bool))\n#print(upper_tri)\nto_drop =[column for column in upper_tri.columns if any(upper_tri[column] > 0.60)]\n\nprint(\"The columns those have more than 0.6 correlation is :\",to_drop[0:6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Inter Quantile Range Calculation"},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1=df.quantile(0.25)\nQ3=df.quantile(0.75)\n\nIQR=Q3-Q1\nIQR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S= df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlier Removal :\n\nWe have used Inter Quantile Range to eliminate outliers, the IQR range calculated by Q3-Q1 is used to eliminate extreme values from the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"S.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now after outlier removal and duplicates removal we are left with 911 records in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.boxplot(data=S)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we do see some outliers again in the final data , but these are not real noise.These are caused because our central tendencies and distribution has been changed or altered after removal of ouliers , and these may not be considered as ouliers now."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering techniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import zscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We will use PCA for dimensionality reduction \n\nTo identify important features among all , we shall do principal component analysis to see the varience captured by each component. And decide how many features might be really required to predict the strength of the concrete mixture\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_for_pca=S.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_for_pca=data_for_pca.apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling is an important aspect in PCA as different unit of measurement affects the calculation of new axis that PCA creates. Standardization or normalization will help create the right Principal compoenents with giving every feature the right weightage.\n\nIn our dataset though all our independent features are measured in kgs, all have different magnitude, hence I am scaling the dataset before using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"Scaler=StandardScaler()\nX_PCA=data_for_pca.drop(['csMPa'],axis=1)\nY_PCA=data_for_pca['csMPa']\nPC=PCA(n_components=8,random_state=12)\ncomp_features=PC.fit(X_PCA)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implementing PCA with all independent features to see the overall varience captured by each component"},{"metadata":{},"cell_type":"markdown","source":"Displaying the covarience Matrix for each compoenent \n\nWhat is Covarience ?\n\nCovariance is just an unstandardized version of correlation.  To compute any correlation, we divide the covariance by the standard deviation of both variables to remove units of measurement.  So a covariance is just a correlation measured in the units of the original variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"covmatrix=np.cov(X_PCA,rowvar=False)\nplt.figure(figsize=(12,7))\nsns.heatmap(covmatrix,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"####################The Eigen Values#########################\")\nprint(PC.explained_variance_)\nprint(\"####################The Eigen Vectors#########################\")\nprint(PC.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(list(range(1,9)),PC.explained_variance_ratio_, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.step(list(range(1,9)),np.cumsum(PC.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Varience Ratio covered by each componenet:{}\".format(PC.explained_variance_ratio_ * 100))\nP_Components=PC.explained_variance_ratio_\nprint(\"The Ideal number of components that could explain:{}% of variance in data is 5\".format(np.sum(P_Components[0:6])*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the graphs above it is evident that 6 components capture just over 97% of data, rather than using all the features we can use just 6 major components on our models to train and predict."},{"metadata":{},"cell_type":"markdown","source":"Now we shall again capture the varience of the data for PCA components 6. This will allow us to train our models on both original data and PCA components seperately."},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA6=PCA(n_components=6)\nPCA6.fit(X_PCA)\nX_PCA_6=PCA6.transform(X_PCA)\nY_PCA_6=Y_PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA6.explained_variance_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA6.components_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we have said earlier in multivariate analysis,  its time for us to explore the mix up of Gaussians in our data.\n\n#### Cluster Analysis using Kfold(Centroid based) and Agglomerative(Hierarchial based) to explore gaussian mix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans,AgglomerativeClustering\nk_values=range(1,10)\nSSE=[]\nfor i in k_values:\n    model=KMeans(n_clusters=i)\n    model.fit(S)\n    SSE.append(model.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(k_values, SSE, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average distortion')\nplt.title('Selecting k with the Elbow Method')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Identify the number of clusters.\n\nThe Lloyd's algorithm or what is known as the elbow method is used to find the number of clusters required to group the data.\n\nAs you could see from the above graph, the bend is clearly visible at k=3, hence the ideal number of groups for this dataset is 3.\n\nAt k=0, the data is highly compressed, as the number of cluster increases the variance in data starts to change and at certain point the \"Elbow Bends\" in this case it is 3, we can preferably argue that is the ideal number of clusters in our data since it almost captures most of the variance. As K value keeps increasing after 3, you could see that each cluster becomes homogeneous and there is no change in variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"S=S.apply(zscore)\nK_Final=KMeans(n_clusters=3)\nK_Final.fit(S)\nPRED=K_Final.predict(S)\nclusters=S.copy()\nclusters['K-Means-Grouping']=PRED\nclusters.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agglomerative is a  connectivity based clustering technique , dendrogram and Cophentic Coefficient is used to identify the number of clusters\n\nDendrogram gives a clear view on convergence of data. The Cophentic correlation captures the original distance between two data points and the dendrogrammatic distance between the data points\n\nHere , I have not used dendograms, I am just comparing the clusters grouped by K means and Agglomertaive."},{"metadata":{"trusted":true},"cell_type":"code","source":"AG=AgglomerativeClustering()\nAG.fit(S)\nclusters['Agglomerative labels']=AG.labels_\nclusters.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K_Means_group=clusters['K-Means-Grouping'].value_counts()\nAgg_Group=clusters['Agglomerative labels'].value_counts()\nfig,ax=plt.subplots(1,2,figsize=(15,5))\nK_Means_group.plot.pie(shadow=True, startangle=120,autopct='%.2f',ax=ax[0])\nAgg_Group.plot.pie(shadow=True, startangle=120,autopct='%.2f',ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you could see from the pie plot, K - Means has identified 3 clusters in our dataset whereas Agglomerative has identified 2 clusters. This is majorly because of the way K -Means(Centroid based) and Agglomerative(Hierarchial Based ) works.\n\nWe will compare the attributes based on the clusters for better understanding..."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['cement'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0]);\nsns.scatterplot(clusters['cement'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);\nfig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['slag'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0]);\nsns.scatterplot(clusters['slag'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['flyash'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0],palette='Reds_r');\nsns.scatterplot(clusters['flyash'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);\nfig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['water'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0]);\nsns.scatterplot(clusters['water'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['superplasticizer'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0]);\nsns.scatterplot(clusters['superplasticizer'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);\nfig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['coarseaggregate'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0]);\nsns.scatterplot(clusters['coarseaggregate'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['fineaggregate'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0]);\nsns.scatterplot(clusters['fineaggregate'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);\nfig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.scatterplot(clusters['age'],clusters['csMPa'],hue=clusters['K-Means-Grouping'],size=clusters['flyash'],ax=ax[0]);\nsns.scatterplot(clusters['age'],clusters['csMPa'],hue=clusters['Agglomerative labels'],size=clusters['flyash'],ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the scatter plot analysis for all the variables with respect to strength we could see that the groupings formed through clustering  evidently prove that similar group of data have similar values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig,ax=plt.subplots(1,2,figsize=(15,5))\nvar = 'age'\nvar2='cement'\nvar3='water'\nvar4='fineaggregate'\nvar5='slag'\nvar6='flyash'\nvar7='superplasticizer'\nvar8='coarseaggregate'\nwith sns.axes_style(\"white\"):\n    plot = sns.lmplot(var,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\n    plot1 = sns.lmplot(var2,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\n    plot2 = sns.lmplot(var3,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\n    plot3 = sns.lmplot(var4,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\n    \nplot.set(ylim = (-3,3));\nplot1.set(ylim=(-3,3));\nplot2.set(ylim=(-3,3));\nplot3.set(ylim=(-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cluster Analysis and their relationship with predictor(Concrete Strength)\n\n1. From the Above plots for Age VS Strength it is very evident  and convincing that Age can be strong preditor in the strength of the concrete mix, As you can see for all the groups of clusters in age we see a strong positive linear relationship between age and strength. We can also infer that as the mixture ages the strength of the concrete increases.\n    1.1. The Line of best fit is also around the mean and the residuals or error is also minimal\n2. Cement seems to have little positive relationsip with strength , But may not be a strong predictor.\n3. Water vs strength (Group 2 has some linear relation ship) whereas, group 0 and group 1 have a slight linear relationship. Hence, water also may not be a strong predictor of strength.\n4. Fineagg, for group 0 and group 1 the line is almost horizontal, which means for value change in fineagg there is no considerable change in strength, But for group 3 there is some relationship for fineagg vs strength. Hence Fineagg may also not be good predictor of concrete strength."},{"metadata":{"trusted":true},"cell_type":"code","source":"with sns.axes_style(\"white\"):\n    plot4 = sns.lmplot(var5,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\n    plot5 = sns.lmplot(var6,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\n    plot6 = sns.lmplot(var7,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\n    plot7 = sns.lmplot(var8,'csMPa',data=clusters,col='K-Means-Grouping',x_estimator=np.mean)\nplot4.set(ylim=(-3,3));\nplot5.set(ylim=(-3,3));\nplot6.set(ylim=(-3,3));\nplot7.set(ylim=(-3,3));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cluster Analysis and their relationship with predictor(Concrete Strength)\n\n1. Slag is almost horizontal for group 0 and group 2, group 1 has a slight relationship which makes slag not a great predictpr of strength.\n2. Ash , suplerplastic and coarseagg all have horizontal data distribution on atleast one or more groups with strength also making them weak predictprs of strength.\n\n### So from our cluster analysis , we could infer that age has a strong realationship with strength for data in all clusters"},{"metadata":{},"cell_type":"markdown","source":"## Model Creation\n\n### For this problem statement, Linear models seems to be a good fit, We are not just going to limit ourselves with linear regression, we are going to explore all the linear models , polynomial models to see which performs best and going to select one.\n\n#### Overview of the next phases :\n1. Scale the data : Thusfar we have been using only the raw data(Except PCA), But when it comes to ML algorithms unit of measurement plays a vital role in model performance. Hence, it is essential to scale the data to avoid one unit & magnitude outweigh the other. In this dataset, there are two units kgs and days, so we ll scale the data.\n2. Split the data into training and test set with random state , to ensure the training model does not get to know the test data.\n3. We will use all Linear model both Gradient descent based and tree based to evalute the performance on training and testing.\n4. Explore Feature importance of models wherever applicable.\n5. We will use both scaled raw data and PCA feature extracted data with feature number of features as 6, which we have done earlier.\n6. Evaluate the scores on all the models on both original scaled data and PCA data and decide the best.\n7. Perform Hyper parameter tuning using both Gridsearchcv and Random serach CV\n8. Finially, do cross validation on the best model to evaluate the model performance on unseen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor\nX_SCALED=S.drop(['csMPa'],axis=1)\nY_SCALED=S['csMPa']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data using train_test_split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train,X_Test,Y_Train,Y_Test=train_test_split(X_SCALED,Y_SCALED,test_size=0.3,random_state=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M1_Linear_Model=LinearRegression()\nM2_Poly=Pipeline([('Poly',PolynomialFeatures(degree=2)),\n               ('Model2',LinearRegression())\n               ])\nM3_SVR=SVR()\nM4_DTREE=DecisionTreeRegressor()\nM5_RF=RandomForestRegressor()\nM6_ADA=AdaBoostRegressor()\nM7_BAG=BaggingRegressor()\nM8_Lasso=Lasso(alpha=0.2)\nM9_Ridge=Ridge()\nM10_Gradient_Booster=GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calling all the required models that could perform linear regression and train our model on training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"M1_Linear_Model.fit(X_Train,Y_Train)\nM2_Poly.fit(X_Train,Y_Train)\nM3_SVR.fit(X_Train,Y_Train)\nM4_DTREE.fit(X_Train,Y_Train)\nM5_RF.fit(X_Train,Y_Train)\nM6_ADA.fit(X_Train,Y_Train)\nM7_BAG.fit(X_Train,Y_Train)\nM8_Lasso.fit(X_Train,Y_Train)\nM9_Ridge.fit(X_Train,Y_Train)\nM10_Gradient_Booster.fit(X_Train,Y_Train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though we have done PCA to identify the features and their corresponding varience , We shall also list out the feature importances captured by each of the models and try to understand how effective is it in prediction of strength.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Importance from Decision Tree, RF, Lasso and Ridge\nFeatures=(['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg','fineagg', 'age'])\nFeatures_Linear_RAW=M1_Linear_Model.coef_\nFeatures_DTREE_RAW=M4_DTREE.feature_importances_\nfeatures_RF_RAW=M5_RF.feature_importances_\nfeatures_lasso_RAW=M8_Lasso.coef_\nfeatures_ridge_RAW=M9_Ridge.coef_\nsummary={'FEATURES':Features,\"Linear\":Features_Linear_RAW,\"Dtree\":Features_DTREE_RAW,'Random Forest':features_RF_RAW,'Lasso':features_lasso_RAW,'Ridge':features_ridge_RAW}\n\n\nFEATURES_DF=pd.DataFrame(summary)\nFEATURES_DF","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above table we could see that\n1. Linear and Ridge(alpha=0.2) alomst has same coefeficients for all features.\n2. Decision Tree and Random forest's coefficients are similar.\n3. Lasso regularization stands out and it made less important features to be zero, This is mainly because of lasso penalizes the error with high value. Lasso does feature selection first and does parameter shrinkage resulting whereas ridge only does parameter shrinkage.\n4. If not PCA , Lasso regularization could also be used to select feature importances , but for this case we have done with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predictions on RAW SCALED and Feature importances\nY_PRED_LINEAR_RAW=M1_Linear_Model.predict(X_Test)\nY_PRED_POLY_RAW=M2_Poly.predict(X_Test)\nY_PRED_SVR_RAW=M3_SVR.predict(X_Test)\nY_PRED_DTREE_RAW=M4_DTREE.predict(X_Test)\nY_PRED_RF_RAW=M5_RF.predict(X_Test)\nY_PRED_ADA_RAW=M6_ADA.predict(X_Test)\nY_PRED_BAG_RAW=M7_BAG.predict(X_Test)\nY_PRED_LASSO_RAW=M8_Lasso.predict(X_Test)\nY_PRED_RIDGE_RAW=M9_Ridge.predict(X_Test)\nY_PRED_GRD_RAW=M10_Gradient_Booster.predict(X_Test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_M1_Linear=M1_Linear_Model.score(X_Train,Y_Train)* 100\nTraining_M2_Poly=M2_Poly.score(X_Train,Y_Train)* 100\nTraining_M3_SVR=M3_SVR.score(X_Train,Y_Train)* 100\nTraining_M4_DTREE=M4_DTREE.score(X_Train,Y_Train)* 100\nTraining_M5_RF=M5_RF.score(X_Train,Y_Train)* 100\nTraining_M6_ADA=M6_ADA.score(X_Train,Y_Train)* 100\nTraining_M7_BAG=M7_BAG.score(X_Train,Y_Train)* 100\nTraining_M8_Lasso=M8_Lasso.score(X_Train,Y_Train)* 100\nTraining_M9_Ridge=M9_Ridge.score(X_Train,Y_Train)* 100\nTraining_M10_Gradient=M10_Gradient_Booster.score(X_Train,Y_Train)* 100\nTest_M1_Linear=M1_Linear_Model.score(X_Test,Y_Test)* 100\nTest_M2_Poly=M2_Poly.score(X_Test,Y_Test)* 100\nTest_M3_SVR=M3_SVR.score(X_Test,Y_Test)* 100\nTest_M4_DTREE=M4_DTREE.score(X_Test,Y_Test)* 100\nTest_M5_RF=M5_RF.score(X_Test,Y_Test)* 100\nTest_M6_ADA=M6_ADA.score(X_Test,Y_Test)* 100\nTest_M7_BAG=M7_BAG.score(X_Test,Y_Test)* 100\nTest_M8_Lasso=M8_Lasso.score(X_Test,Y_Test)* 100\nTest_M9_Ridge=M9_Ridge.score(X_Test,Y_Test)* 100\nTest_M10_Gradient=M10_Gradient_Booster.score(X_Test,Y_Test)* 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the PCA components using train_test_split with 6 features(n_components=6) to see how our models perfrom on the PCA components "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca_train,X_pca_test,Y_pca_train,Y_pca_test=train_test_split(X_PCA_6,Y_PCA_6,test_size=0.3,random_state=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M1_Linear_Model.fit(X_pca_train,Y_pca_train)\nM2_Poly.fit(X_pca_train,Y_pca_train)\nM3_SVR.fit(X_pca_train,Y_pca_train)\nM4_DTREE.fit(X_pca_train,Y_pca_train)\nM5_RF.fit(X_pca_train,Y_pca_train)\nM6_ADA.fit(X_pca_train,Y_pca_train)\nM7_BAG.fit(X_pca_train,Y_pca_train)\nM8_Lasso.fit(X_pca_train,Y_pca_train)\nM9_Ridge.fit(X_pca_train,Y_pca_train)\nM10_Gradient_Booster.fit(X_pca_train,Y_pca_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predictions on PCA SCALED and Feature importances\nY_PRED_LINEAR_PCA=M1_Linear_Model.predict(X_pca_test)\nY_PRED_POLY_PCA=M2_Poly.predict(X_pca_test)\nY_PRED_SVR_PCA=M3_SVR.predict(X_pca_test)\nY_PRED_DTREE_PCA=M4_DTREE.predict(X_pca_test)\nY_PRED_RF_PCA=M5_RF.predict(X_pca_test)\nY_PRED_ADA_PCA=M6_ADA.predict(X_pca_test)\nY_PRED_BAG_PCA=M7_BAG.predict(X_pca_test)\nY_PRED_LASSO_PCA=M8_Lasso.predict(X_pca_test)\nY_PRED_RIDGE_PCA=M9_Ridge.predict(X_pca_test)\nY_PRED_GRD_PCA=M10_Gradient_Booster.predict(X_pca_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Training_PCA_M1_Linear=M1_Linear_Model.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M2_Poly=M2_Poly.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M3_SVR=M3_SVR.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M4_DTREE=M4_DTREE.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M5_RF=M5_RF.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M6_ADA=M6_ADA.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M7_BAG=M7_BAG.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M8_Lasso=M8_Lasso.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M9_Ridge=M9_Ridge.score(X_pca_train,Y_pca_train)* 100\nTraining_PCA_M10_Gradient=M10_Gradient_Booster.score(X_pca_train,Y_pca_train)* 100\nTest_PCA_M1_Linear=M1_Linear_Model.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M2_Poly=M2_Poly.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M3_SVR=M3_SVR.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M4_DTREE=M4_DTREE.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M5_RF=M5_RF.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M6_ADA=M6_ADA.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M7_BAG=M7_BAG.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M8_Lasso=M8_Lasso.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M9_Ridge=M9_Ridge.score(X_pca_test,Y_pca_test)* 100\nTest_PCA_M10_Gradient=M10_Gradient_Booster.score(X_pca_test,Y_pca_test)* 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, with all the score that we have stored in variables for Traing and Testing data on scaled raw and PCA featuures , we will create a table that will display the model scores as a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"DTREE_COEFF_PCA=M4_DTREE.feature_importances_\nRF_COEFF_PCA=M5_RF.feature_importances_\nADA_COEFF_PCA=M6_ADA.feature_importances_\nBAG_COEFF_PCA=M7_BAG.n_features_\nLAS_COEFF_PCA=M8_Lasso.coef_\nRDGE_COEFF_PCA=M9_Ridge.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TAB=pd.DataFrame({'Model_Names':['Linear Regression','Polynomial_regresison','Support Vector Regressor','Decision Tree Regressor','Random Forest Regressor',\n            'Adaboost Regressor','Bagging Regressor','Lasso Regressor','Ridge Regressor','Gradient Boost'],'Training_Score_Scaled_Raw':[Training_M1_Linear,\nTraining_M2_Poly,\nTraining_M3_SVR,\nTraining_M4_DTREE,\nTraining_M5_RF,\nTraining_M6_ADA,\nTraining_M7_BAG,\nTraining_M8_Lasso,\nTraining_M9_Ridge,Training_M10_Gradient],'Testing_Score_Scaled_Raw':[Test_M1_Linear,\nTest_M2_Poly,\nTest_M3_SVR,\nTest_M4_DTREE,\nTest_M5_RF,\nTest_M6_ADA,\nTest_M7_BAG,\nTest_M8_Lasso,\nTest_M9_Ridge,Test_M10_Gradient],'Training_Score_PCA':[Training_PCA_M1_Linear,\nTraining_PCA_M2_Poly,\nTraining_PCA_M3_SVR,\nTraining_PCA_M4_DTREE,\nTraining_PCA_M5_RF,\nTraining_PCA_M6_ADA,\nTraining_PCA_M7_BAG,\nTraining_PCA_M8_Lasso,\nTraining_PCA_M9_Ridge,Training_PCA_M10_Gradient],'Testing_Score_PCA':[Test_PCA_M1_Linear,\nTest_PCA_M2_Poly,\nTest_PCA_M3_SVR,\nTest_PCA_M4_DTREE,\nTest_PCA_M5_RF,\nTest_PCA_M6_ADA,\nTest_PCA_M7_BAG,\nTest_PCA_M8_Lasso,\nTest_PCA_M9_Ridge,Test_PCA_M10_Gradient]})\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TAB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear model hasn't performed that great on both raw as well as PCA components, the scores are not that accurate.\n#### Polynomial Regression on a degree of 2 has been implemented ,which means the number of indepndent variables have been increased. This actually turns the linear equation of \"y=mx + B\" into quadratic(degree=2) , Now we are not going to see for the best straightline, but a curve. For this problem , I am using polynomial with degree 2 as I feel a linear equation suits best for our data from the EDA and higher polynomial function might overfit the data and also we may run into curse of dimensionality with small dataset.\n#### Decision tree is a overfit, as it prodces 99% accuracy with trainiing and with test it chokes to 78% and 69% with raw and PCA components, hence it is not the best model.\n#### Random forest and Gradient boost seems to be slightly overfit, but looks like it generalises well on the test data on both raw and PCA.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best Test score we have achived on Raw Data:\",TAB['Testing_Score_Scaled_Raw'].max())\nprint(\"Best Test score we have achived on PCA components:\",TAB['Testing_Score_PCA'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Techniques employed to squeeze that extra performance out of the model without making it overfit or underfit"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regularization using GridsearchCV\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though Random forest had better score , I am slecting Gradient boost here as the best model , as I feel it generalises well compared to training and testing scores.\n##### So , we ll do hyper parameter tuning on Gradient boost by  Grid search to squeeze that extra performance out of the model without making it overfit or underfit"},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV"},{"metadata":{},"cell_type":"markdown","source":"Every Algorithm has some parameters that are specific to data and some specific to the algorithm , Those parameters which  are specific to algorithm is known as hyperparameters.\nWe shall try various values for these hyperparameters to find which set of values give the best result for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid={'n_estimators':[100,200,300,400,500,600],'learning_rate':[.001,0.01,.1],'max_depth':[1,2,3,4,5],'subsample':[.5,.75,1],'random_state':[1]                      \n           }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator=M10_Gradient_Booster\nGrid_CV=GridSearchCV(estimator=estimator,param_grid=param_grid,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_CV.fit(X_Train,Y_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_CV.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above are the best estimators for our grid search model and we shall use these values for on our final model to check how our scores are improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_CV.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now lets train our best model with best hyperparameters found using GridSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"Final=GradientBoostingRegressor(learning_rate=0.1,max_depth=2,n_estimators=600,random_state=1,subsample=0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Final.fit(X_Train,Y_Train)\nFinal.score(X_Train,Y_Train) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Final.score(X_Test,Y_Test) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As you can see, the training and the test scores of Gradient boost has improved and parameter tuning allows us to build better model\nTraining score without tuning 95%, Testing score without tuning 86 \nTraining score with hyperparameter tuning 98, Testing score with hyperparameter tuning 91%"},{"metadata":{},"cell_type":"markdown","source":"### Model performance range at 95% confidence level \n#### Having bulit the model on hyperparametrs and evaluating them on training and test set does not guarntee the same performance of our model on unseen data.\n#### Hence, it is essential to further do a final evaluation on unseen data. We shall do the same using K fold cross validation "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score,KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K=10\nseed=12\nkfold_Linear=KFold(shuffle=True,n_splits=K,random_state=seed)\naccuracies = cross_val_score(estimator = Final, X = X_SCALED, y = Y_SCALED, cv = kfold_Linear) \naccuracies\nprint(\"K Fold score mean:{}\".format(accuracies.mean()*100))\nprint(\"K Fold score standard deviation:{}\".format(accuracies.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion- \"When the Rubber meets the Road\""},{"metadata":{},"cell_type":"markdown","source":"The hyperparameters tuned model produces a accuracy score of 92.11  % on 10 fold cross validation with a standard deviation of 3.14 \nSo , I conclude that when this model is deployed on unseen data we could get a accuracy of range 88.97% to 95.25%.\nThis score is pretty good and can be trusted at 95% confidence Intravel"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}