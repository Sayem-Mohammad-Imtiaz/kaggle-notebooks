{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importando las librerias necesarias\nfrom sklearn.datasets import fetch_mldata\nimport pandas as pd\nimport numpy\n# Cargando los datos\nmnist_t = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\nmnist_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ver los datos\nmnist_t.head(10)\n# Se obtiene que el label esta en la primera fila","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Procedemos a separar x y y\nx_train = mnist_t.loc[:,'1x1':]\ny_train = mnist_t.loc[:,'label']\nx_test = mnist_test.loc[:,'1x1':]\ny_test = mnist_test.loc[:,'label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Necesitamos estandarizar las dimensiones, a que pca toma en cuenta esto\n# Para eso tenemos el standardscaler de sklearn\nfrom sklearn.preprocessing import StandardScaler\n#Inicializar\nscaler = StandardScaler()\n# Fitearlo en el x_train\nscaler.fit(x_train)\n# Transformar el x_train y el x_test\nx_n_train = scaler.transform(x_train)\nx_n_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nimport time\nvar = [.99, .95, .92, .90]\ncomp = list()\nseg = list()\nacc = list()\nfor i in range(5):\n    if i==0:\n        pca = PCA()\n    else:\n        pca = PCA(var[i-1])\n        \n    # Fitearlo solo al training set\n    pca.fit(x_n_train)\n    \n    # Transformar este\n    x_nn_train = pca.transform(x_n_train)\n    x_nn_test = pca.transform(x_n_test)\n    \n    # Anadimos componentes a nuestra lista de componentes\n    comp.append(pca.n_components_)\n    \n    # Inicializamos logistic regression\n    logisticRegr = LogisticRegression(solver='lbfgs')\n    \n    # Entrenamos con el training set  y calculamos el tiempo de entrenamiento\n    t1 = time.time()\n    logisticRegr.fit(x_nn_train,y_train)\n    seg.append(round(time.time()-t1, 3))\n    \n    # Anadimos el puntaje de instancia a nuestra lista de puntajes\n    acc.append(logisticRegr.score(x_nn_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imprimimos los resultados\nfor i in range(5):\n    print (i+1,\"iteracion: \", \"numero de componentes: \", comp[i], \"tiempo de entrenamiento: \", seg[i], \"Score: \", acc[i])\n    \n# Se puede apreciar que al reducir las dimenciones a las mas influyentes con el PCA, en primer lugar se obtiene menor tiempo de entrenamiento\n# A su vez se muestra que con una varianza del 95% en los componentes aumneta su score a 0.9217.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\npca1 = PCA()\npca2 = PCA(.95)\n\ndn = pca1.fit_transform(x_train)\nrd = pca2.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aprox = pca2.inverse_transform(rd)\n\nplt.figure(figsize=(8,4));\n\nplt.subplot(1, 2, 1);\nplt.imshow(numpy.array(x_train.loc[1,'1x1':]).reshape(28,28),\n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('784 components', fontsize = 14)\nplt.title('Original Image', fontsize = 20);\n\n# 154 principal components\nplt.subplot(1, 2, 2);\nplt.imshow(aprox[1].reshape(28, 28),\n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('331 components', fontsize = 14)\nplt.title('95% of Explained Variance', fontsize = 20);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}