{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url='/kaggle/input/test-file/tested.csv'\ndataframe=pd.read_csv(url)\ndf=dataframe\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.iloc[5].values\nY = df.iloc[10].values\n                  \nprint(X)\nprint(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set seed for reproducibility\nnp.random.seed(0) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look  missing data \ndf.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at  missing data \ndf.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = df.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many total missing values do we have?\ntotal_cells = np.product(df.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\n(total_missing/total_cells) * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all the rows that contain a missing value\ndf.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all columns with at least one missing value\ncolumns_with_na_dropped = df.dropna(axis=1)\ncolumns_with_na_dropped.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just how much data did we lose?\n#columns in original dataset\nprint(df.shape[1])\n#columns with na dropped data set\nprint(columns_with_na_dropped.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filling in missing values automatically\n# get a small subset of the dataset\nsubset_df = df.loc[:, 'Age':'Fare'].head()\nsubset_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace all NA's with 0\nsubset_df.fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace all NA's the value that comes directly after it in the same column, \n# then replace all the reamining na's with 0\nsubset_df.fillna(method = 'bfill', axis=0).fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#7#Write a code to apply functions over all elements in a column.\nprint('Maximum:', df['Age'].max())\nprint('Minimum:', df['Age'].min())\nprint('Mean:', df['Age'].mean())\nprint('Sum:', df['Age'].sum())\nprint('Count:', df['Age'].count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data, set missing values\ndf = pd.read_csv(url, na_values=[np.nan, 'NONE', -999])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop columns\ndf.drop('Age', axis=1).head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop column\ndf.drop(['Age', 'Sex'], axis=1).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete rows, show first two rows of output\ndf.drop(df.columns[1], axis=1).head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete rows, show first two rows of output\ndf[df['Name'] != 'Allison, Miss Helen Loraine'].head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#write a code to delete duplicate rows# # Delete row, show first two rows of output starting from 1\ndf[df.index != 0].head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show number of rows\nprint(\"Number Of Rows In The Original DataFrame:\", len(df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show number of rows\nprint(\"Number Of Rows After Deduping:\", len(df.drop_duplicates()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop duplicates\ndf.drop_duplicates(subset=['Sex'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop duplicates\ndf.drop_duplicates(subset=['Sex'], keep='last')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group rows by the values of the column 'Sex', calculate mean# of each group\ndf.groupby('Sex').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Write a code to group by rows by values.# Group rows\ndf.groupby('Sex')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Write a code to group by rows by values.# Group rows, count rows\ndf.groupby('Survived')['Name'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Write a code to group by rows by values.# Group rows, calculate mean\ndf.groupby(['Sex','Survived'])['Age'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#7# Create date range\ntime_index = pd.date_range('06/06/2017', periods=100000, freq='30S')\ndataframe = pd.DataFrame(index=time_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#write a code to group by rows by time.# Create column of random values\ndataframe['Sale_Amount'] = np.random.randint(1, 10, 100000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#write a code to group by rows by time.# Group rows by week, calculate sum per week\ndataframe.resample('W').sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show three rows\ndataframe.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##write a code to group by rows by time. Group by two weeks, calculate mean\ndataframe.resample('2W').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#write a code to group by rows by time.# Group by month, count rows\ndataframe.resample('M').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by month, count rows\ndataframe.resample('M', label='left').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\ndf = pd.read_csv(url)\n# Create function\ndef uppercase(x):return x.upper()\n#9# Apply function, show two rows\ndf['Name'].apply(uppercase)[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#9# Group rows, apply function to groups\ndf.groupby('Sex').apply(lambda x: x.count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#10# Create DataFrame\ndata_a = {'id': ['1', '2', '3'],'first': ['Alex', 'Amy', 'Allen'],'last': ['Anderson', 'Ackerman', 'Ali']}\ndataframe_a = pd.DataFrame(data_a, columns = ['id', 'first', 'last'])\n# Create DataFrame\ndata_b = {'id': ['4', '5', '6'],'first': ['Billy', 'Brian', 'Bran'],'last': ['Bonder', 'Black', 'Balwner']}\ndataframe_b = pd.DataFrame(data_b, columns = ['id', 'first', 'last'])\n# Concatenate DataFrames by rows\npd.concat([dataframe_a, dataframe_b], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate DataFrames by columns\npd.concat([dataframe_a, dataframe_b], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #Create row\nrow = pd.Series([10, 'Chris', 'Chillon'], index=['id', 'first', 'last'])   \n# Append row\ndataframe_a.append(row, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create DataFrame\nemployee_data = {'employee_id': ['1', '2', '3', '4'],'name': ['Amy Jones', 'Allen Keys', 'Alice Bees','Tim Horton']}\ndataframe_employees = pd.DataFrame(employee_data, columns = ['employee_id','name'])\n# Create DataFrame\nsales_data = {'employee_id': ['3', '4', '5', '6'],'total_sales': [23456, 2512, 2345, 1455]}\ndataframe_sales = pd.DataFrame(sales_data, columns = ['employee_id','total_sales'])\n# Merge DataFrames\npd.merge(dataframe_employees, dataframe_sales, on='employee_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge DataFrames\npd.merge(dataframe_employees, dataframe_sales, on='employee_id', how='outer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge DataFrames\npd.merge(dataframe_employees, dataframe_sales, on='employee_id', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge DataFrames\npd.merge(dataframe_employees,dataframe_sales,left_on='employee_id',right_on='employee_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}