{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Import Packages and TPU Environment Test"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# import packages\nfrom datetime import datetime\nimport os\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport re\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\n# AUTO = tf.data.experimental.AUTOTUNE\n\n\n# Detect TPU, and gives appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I/O with a tool class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a instruct class to save constants\nclass CONFIG:\n    ROOT = \"/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/\"\n    FILE_PATH = os.path.join(ROOT, 'audio_and_txt_files')\n    \n    MFCC_NUM = 40\n    PADDING = 862 # to make the length of all MFCC equal\n    EPOCHES = [50, 250, 100, 50]\n    BATCHSIZE = [2, 128, 2, 64]\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get identity of patients and filenames in folder"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_names = [file_name for file_name in os.listdir(CONFIG.FILE_PATH) if '.wav' in file_name] \nfile_paths = [os.path.join(CONFIG.FILE_PATH, file_name) for file_name in file_names]\npatient_id = []\n\nfor name in file_names:\n    patient_id.append(name.split('_')[0])\n    \npatient_id = np.array(patient_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data and extract MFCC"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(file_name, n_mfcc=CONFIG.MFCC_NUM, pad_width = CONFIG.PADDING):\n    '''\n    param:\n        file_name: os.path.join(CONFIG.FILE_PATH, file_name)\n        n_mfcc: the row of feature tensor, corresponds to the number of time segmentation\n        pad_width: to make sure the shapes for all tensors are same\n    return:\n        mfccs: a tensor with shape[n_mfcc, pad_width, 1]\n    '''\n    try:\n        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=20) \n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n        pad_width = pad_width - mfccs.shape[1]\n        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n        \n    except Exception as e:\n        print(\"Error encountered while parsing file: \", file_name)\n        return None \n     \n    return mfccs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df = pd.read_csv(os.path.join(CONFIG.ROOT, 'patient_diagnosis.csv'), header=None, names = ['id', 'label'])# dtype = {'id': int}\nprint(label_df.head(2))\n\nlabels = [label_df[label_df['id'] == int(x)]['label'].values for x in patient_id] # int 保证数据类型一致，否则返回[]\nprint('labels[0]={}'.format(labels[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [] \n\n# Iterate through each sound file and extract the features\nfor file_path in file_paths:\n    data = extract_features(file_path)\n    features.append(data)\n\nprint('Finished feature extraction from ', len(features), ' files')\nfeatures = np.array(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{} slices（batches）, {} frequencies, {} time point'.format(features.shape[0], features.shape[1], features.shape[2]))\n\nplt.figure(figsize=(10, 4))\nlibrosa.display.specshow(features[7], x_axis='ms',y_axis='mel')\nplt.colorbar()\nplt.title('MFCC')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean data and generate onehot label"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.array(labels)\n\nfeatures_cleaned = np.delete(features, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0) \nlabels_cleaned = np.delete(labels, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n\n# Results Reviewing\nunique_elements, counts_elements = np.unique(labels_cleaned, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))\nprint('There are {} slices（batches）, {} frequencies, {} time point'.format(features_cleaned.shape[0], features_cleaned.shape[1], features_cleaned.shape[2]))\n\nle = LabelEncoder()\ni_labels = le.fit_transform(labels_cleaned)\nonehot_labels = to_categorical(i_labels) \nprint(onehot_labels.shape)\nfeatures2 = np.reshape(features_cleaned, (*features_cleaned.shape,1)) \nprint(features2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(features2, onehot_labels, stratify=onehot_labels,test_size=0.302, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_rows = 40\nnum_columns = 862\nnum_channels = 1\nfrom keras.regularizers import l2 \nnum_labels = onehot_labels.shape[1]\nfilter_size = 2 # it would be better to use 3, but the input image is too small.\nwith strategy.scope():\n    # Construct model \n    model = Sequential()\n    model.add(Conv2D(filters=16, kernel_size=filter_size, input_shape=(num_rows, num_columns, num_channels), activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(0.2))\n\n    model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(0.2))\n\n    model.add(GlobalAveragePooling2D())\n\n    model.add(Dense(num_labels, activation='softmax')) \n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') \n    model.summary()\n\n    score = model.evaluate(x_test, y_test, verbose=1)\n    accuracy = 100*score[1]\n\n    print(\"Pre-training accuracy: %.4f%%\" % accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# seperate training and defining process. This will cause that we can train our model for many epoches without making params zero.\nwith strategy.scope():\n\n    callbacks = [\n        ModelCheckpoint(\n            filepath='mymodel2_{epoch:02d}.h5',\n            save_best_only=True,\n            monitor='val_accuracy',\n            verbose=1)\n    ]\n    start = datetime.now()\n\n#     stats1 = model.fit(x_train, y_train, batch_size=CONFIG.BATCHSIZE[0], epochs=CONFIG.EPOCHES[0],\n#               validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n    \n\n#     stats2 = model.fit(x_train, y_train, batch_size=CONFIG.BATCHSIZE[1], epochs=CONFIG.EPOCHES[1],\n#               validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n    \n#     stats3 = model.fit(x_train, y_train, batch_size=CONFIG.BATCHSIZE[2], epochs=CONFIG.EPOCHES[2],\n#               validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n    \n    stats4 = model.fit(x_train, y_train, batch_size=CONFIG.BATCHSIZE[3], epochs=CONFIG.EPOCHES[3],\n              validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n\n    duration = datetime.now() - start\n    print(\"Training completed in time: \", duration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the model on the training and testing set\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: \", score[1])\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}