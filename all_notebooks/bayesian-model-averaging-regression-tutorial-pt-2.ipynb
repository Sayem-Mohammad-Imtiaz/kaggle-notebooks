{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bayesian Model Averaging Linear Regression Pt. 2\n\nIn this notebook we continue the analysis using Bayeisan Model Averaging (BMA) for regression that we started in the first notebook in this series (https://www.kaggle.com/billbasener/bayesian-model-averaging-regression-tutorial).  In this second installment of the series, we use model averagining to examine prediction of baselball hitters' salaries based on their batting and fielding statistics."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools import add_constant\nfrom sklearn.model_selection import train_test_split\nfrom itertools import combinations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is our class definition for Bayesian Model Averaging.  This is similar to the class provided in the first notebook on BMA with the following modifications:\n1. We added a \"Verbose\" keyward argument that can be used to suppress the output of models and likelihoods while the code is running.\n2. We use the Python library mpmath (http://mpmath.org/) for arbitrary floating point precision computations.  This is important because our likelihoods are very small, and the computations with Bayes theorem result in zeros and NaNs using normal float precision.\n3. We add a .predict() method that uses the BMA averaged coefficients to make predictions.  This method functions like .predict() in the statsmodels OLS.\n4. We added a sampling method known as \"Occam's window\" which is much faster than running all regression models.  Our implementation of Occam's window rejects models that have a likelihood that is 1/20 as high as the most likely model, and only considers new models that are constructed by adding a new regressor variable to a perviously non-rejected model."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from mpmath import mp\nmp.dps = 50\nclass BMA:\n    \n    def __init__(self, y, X, **kwargs):\n        # Setup the basic variables.\n        self.y = y\n        self.X = X\n        self.names = list(X.columns)\n        self.nRows, self.nCols = np.shape(X)\n        self.likelihoods = mp.zeros(self.nCols,1)\n        self.coefficients_mp = mp.zeros(self.nCols,1)\n        self.coefficients = np.zeros(self.nCols)\n        self.probabilities = np.zeros(self.nCols)\n        # Check the max model size. (Max number of predictor variables to use in a model.)\n        # This can be used to reduce the runtime but not doing an exhaustive sampling.\n        if 'MaxVars' in kwargs.keys():\n            self.MaxVars = kwargs['MaxVars']\n        else:\n            self.MaxVars = self.nCols  \n        # Prepare the priors if they are provided.\n        # The priors are provided for the individual regressor variables.\n        # The prior for a model is the product of the priors on the variables in the model.\n        if 'Priors' in kwargs.keys():\n            if np.size(kwargs['Priors']) == self.nCols:\n                self.Priors = kwargs['Priors']\n            else:\n                print(\"WARNING: Provided priors error.  Using equal priors instead.\")\n                print(\"The priors should be a numpy array of length equal tot he number of regressor variables.\")\n                self.Priors = np.ones(self.nCols)  \n        else:\n            self.Priors = np.ones(self.nCols)  \n        if 'Verbose' in kwargs.keys():\n            self.Verbose = kwargs['Verbose'] \n        else:\n            self.Verbose = False \n        \n    def fit(self):\n        # Perform the Bayesian Model Averaging\n        \n        # Initialize the sum of the likelihoods for all the models to zero.  \n        # This will be the 'normalization' denominator in Bayes Theorem.\n        likelighood_sum = 0\n        \n        # To facilitate iterating through all possible models, we start by iterating thorugh\n        # the number of elements in the model.  \n        max_likelihood = 0\n        for num_elements in range(1,self.MaxVars+1): \n            \n            if self.Verbose == True:\n                print(\"Computing BMA for models of size: \", num_elements)\n            \n            # Make a list of all index sets of models of this size.\n            Models_next = list(combinations(list(range(self.nCols)), num_elements)) \n             \n            # Occam's window - compute the models to use for the next iteration\n            # Models_previous: the set of models from the previous iteration that satisfy (likelihhod > max_likelihhod/20)\n            # Models_next:     the set of candidate models for the next iteration\n            # Models_current:  the set of models from Models_next that can be consturcted by adding one new variable\n            #                    to a model from Models_previous\n            if num_elements == 1:\n                Models_current = Models_next\n                Models_previous = []\n            else:\n                idx_keep = np.zeros(len(Models_next))\n                for M_new,idx in zip(Models_next,range(len(Models_next))):\n                    for M_good in Models_previous:\n                        if(all(x in M_new for x in M_good)):\n                            idx_keep[idx] = 1\n                            break\n                        else:\n                            pass\n                Models_current = np.asarray(Models_next)[np.where(idx_keep==1)].tolist()\n                Models_previous = []\n                        \n            \n            # Iterate through all possible models of the given size.\n            for model_index_set in Models_current:\n                \n                # Compute the linear regression for this given model. \n                model_X = self.X.iloc[:,list(model_index_set)]\n                model_regr = OLS(self.y, model_X).fit()\n                \n                # Compute the likelihood (times the prior) for the model. \n                model_likelihood = mp.exp(-model_regr.bic/2)*np.prod(self.Priors[list(model_index_set)])\n                    \n                if (model_likelihood > max_likelihood/20):\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"likelihood=\",model_likelihood)\n                    \n                    # Add this likelihood to the running tally of likelihoods.\n                    likelighood_sum = mp.fadd(likelighood_sum, model_likelihood)\n\n                    # Add this likelihood (times the priors) to the running tally\n                    # of likelihoods for each variable in the model.\n                    for idx, i in zip(model_index_set, range(num_elements)):\n                        self.likelihoods[idx] = mp.fadd(self.likelihoods[idx], model_likelihood, prec=1000)\n                        self.coefficients_mp[idx] = mp.fadd(self.coefficients_mp[idx], model_regr.params[i]*model_likelihood, prec=1000)\n                    Models_previous.append(model_index_set) # add this model to the list of good models\n                    max_likelihood = np.max([max_likelihood,model_likelihood]) # get the new max likelihood if it is this model\n                else:\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"rejected by Occam's window\")\n                    \n\n        # Divide by the denominator in Bayes theorem to normalize the probabilities \n        # sum to one.\n        self.likelighood_sum = likelighood_sum\n        for idx in range(self.nCols):\n            self.probabilities[idx] = mp.fdiv(self.likelihoods[idx],likelighood_sum, prec=1000)\n            self.coefficients[idx] = mp.fdiv(self.coefficients_mp[idx],likelighood_sum, prec=1000)\n        \n        # Return the new BMA object as an output.\n        return self\n    \n    def predict(self, data):\n        data = np.asarray(data)\n        try:\n            result = np.dot(self.coefficients,data)\n        except:\n            result = np.dot(self.coefficients,data.T)\n        return result  \n        \n    def summary(self):\n        # Return the BMA results as a data frame for easy viewing.\n        df = pd.DataFrame([self.names, list(self.probabilities), list(self.coefficients)], \n             [\"Variable Name\", \"Probability\", \"Avg. Coefficient\"]).T\n        return df    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sat-score-data-by-state/Guber1999data.csv')\ndf.head()\nX = df[[\"Spend\", \"StuTeaRat\", \"Salary\", \"PrcntTake\"]]\ny = df[\"SATT\"]\nresult = BMA(y, add_constant(X), Verbose=True).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Hitters = pd.read_csv('../input/hitters/Hitters.csv').dropna() \n# read data set and drop missing values \n#Hitters.info()\n\n\ny = Hitters.Salary\n# Drop Salary (target) and columns for which we created dummy variables\nX_ = Hitters.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n# Define the feature set X.\ndummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\nX = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n#X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n...     add_constant(X), y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_OLS = OLS(y_train, X_train).fit()\nreg_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_BMA = BMA(y_train, X_train, Verbose=True).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_BMA.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_reg_BMA = reg_BMA.predict(X_test)\npred_reg_OLS = reg_OLS.predict(X_test)\npred_reg_OLS = np.asarray(pred_reg_OLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute accuracy\nerrors_BMA = np.asarray(y_test)-pred_reg_BMA\nerrors_OLS = np.asarray(y_test)-pred_reg_OLS\nRMSE_BMA = np.sqrt(np.dot(errors_BMA,errors_BMA)/len(y_test))\nRMSE_OLS = np.sqrt(np.dot(errors_OLS,errors_OLS)/len(y_test))\nprint('Root Mean Squared Error for BMA: ',RMSE_BMA)\nprint('Root Mean Squared Error for OLS: ',RMSE_OLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_sort = np.argsort(np.asarray(y_test))\nimport matplotlib.pyplot as plt\nplt.plot(pred_reg_BMA[idx_sort])\nplt.plot(pred_reg_OLS[idx_sort])\nplt.plot(np.asarray(y_test)[idx_sort])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(errors_BMA)\nplt.plot(errors_OLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"num_iters = 50\nRMSE_BMA_all = np.zeros(num_iters)\nRMSE_OLS_all = np.zeros(num_iters)\nRMSE_BMA_all_mean = np.zeros(num_iters)\nRMSE_OLS_all_mean = np.zeros(num_iters)\n\nfor i in range(num_iters):\n    X_train, X_test, y_train, y_test = train_test_split(add_constant(X), y, random_state=i, test_size=0.33)\n    # fit the models to the training data\n    reg_BMA = BMA(y_train, X_train, Verbose=False).fit()\n    reg_OLS = OLS(y_train, X_train).fit()\n    # predict on the test data\n    pred_reg_BMA = reg_BMA.predict(X_test)\n    pred_reg_OLS = reg_OLS.predict(X_test)\n    pred_reg_OLS = np.asarray(pred_reg_OLS)\n    # Compute Root Mean Squared Error\n    errors_BMA = np.asarray(y_test)-pred_reg_BMA\n    errors_OLS = np.asarray(y_test)-pred_reg_OLS\n    RMSE_BMA = np.sqrt(np.dot(errors_BMA,errors_BMA)/len(y_test))\n    RMSE_OLS = np.sqrt(np.dot(errors_OLS,errors_OLS)/len(y_test))\n    print('Root Mean Squared Error for BMA: ',RMSE_BMA)\n    print('Root Mean Squared Error for OLS: ',RMSE_OLS)\n    RMSE_BMA_all[i] = RMSE_BMA\n    RMSE_OLS_all[i] = RMSE_OLS\n    RMSE_BMA_all_mean[i] = np.sum(RMSE_BMA_all)/(i+1)\n    RMSE_OLS_all_mean[i] = np.sum(RMSE_OLS_all)/(i+1)\n    print('(Mean) Root Mean Squared Error for BMA: ',RMSE_BMA_all_mean[i])\n    print('(Mean) Root Mean Squared Error for OLS: ',RMSE_OLS_all_mean[i])\n    \nprint('(Mean) Root Mean Squared Error for BMA: ',np.mean(RMSE_BMA_all))\nprint('(Mean) Root Mean Squared Error for OLS: ',np.mean(RMSE_BMA_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(RMSE_BMA_all_mean)\nplt.plot(RMSE_OLS_all_mean)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}