{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<B><I>The Code below is my solution to the second lab in the [Supervised Machine Learning Course](http://www.coursera.org/learn/supervised-machine-learning-regression/home/welcome) by IBM on Coursera. Hope you all learn a thing or two from it like i did 😊😊 </I>","metadata":{}},{"cell_type":"markdown","source":"# Machine Learning Foundation\n\n## Course 2, Part b: Regression Setup, Train-test Split LAB ","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nWe will be working with a data set based on [housing prices in Ames, Iowa](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). It was compiled for educational use to be a modernized and expanded alternative to the well-known Boston Housing dataset. This version of the data set has had some missing values filled for convenience.\n\nThere are an extensive number of features, so they've been described in the table below.\n\n### Predictor\n\n* SalePrice: The property's sale price in dollars. \n\n### Features\n\n* MoSold: Month Sold\n* YrSold: Year Sold   \n* SaleType: Type of sale\n* SaleCondition: Condition of sale\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* ...","metadata":{"run_control":{"marked":true}}},{"cell_type":"markdown","source":"## Question 1\n\n* Import the data using Pandas and examine the shape. There are 79 feature columns plus the predictor, the sale price (`SalePrice`). \n* There are three different types: integers (`int64`), floats (`float64`), and strings (`object`, categoricals). Examine how many there are of each data type. ","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Import the data using the file path\ndata = pd.read_csv(\"../input/regression/Ames_Housing_Sales.csv\")\n\ndata.shape","metadata":{"run_control":{"marked":true},"execution":{"iopub.status.busy":"2021-06-29T02:00:52.699069Z","iopub.execute_input":"2021-06-29T02:00:52.699722Z","iopub.status.idle":"2021-06-29T02:00:52.761446Z","shell.execute_reply.started":"2021-06-29T02:00:52.699604Z","shell.execute_reply":"2021-06-29T02:00:52.760432Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"run_control":{"marked":true},"execution":{"iopub.status.busy":"2021-06-29T02:00:59.555771Z","iopub.execute_input":"2021-06-29T02:00:59.556139Z","iopub.status.idle":"2021-06-29T02:00:59.59666Z","shell.execute_reply.started":"2021-06-29T02:00:59.556105Z","shell.execute_reply":"2021-06-29T02:00:59.595724Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 2\n\nA significant challenge, particularly when dealing with data that have many columns, is ensuring each column gets encoded correctly. \n\nThis is particularly true with data columns that are ordered categoricals (ordinals) vs unordered categoricals. Unordered categoricals should be one-hot encoded, however this can significantly increase the number of features and creates features that are highly correlated with each other.\n\nDetermine how many total features would be present, relative to what currently exists, if all string (object) features are one-hot encoded. Recall that the total number of one-hot encoded columns is `n-1`, where `n` is the number of categories.","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"#Create a copy of the dataframe so that changes won't applied to the original one\nX=data.copy()\ny=X.pop('SalePrice') #Extract the target variable from the dataframe\n\n#Select the object/categorical columns \ncat_cols =[col for col in X.columns if X[col].dtype=='object']\n\n#Select the numerical columns \nnum_cols =[col for col in X.columns if X[col].dtype in ['int64','float64']]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:01:08.521771Z","iopub.execute_input":"2021-06-29T02:01:08.522184Z","iopub.status.idle":"2021-06-29T02:01:08.534308Z","shell.execute_reply.started":"2021-06-29T02:01:08.522149Z","shell.execute_reply":"2021-06-29T02:01:08.533108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run the following code for easier interaction with the dataframe\npd.set_option('display.max_columns',None)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:01:12.340227Z","iopub.execute_input":"2021-06-29T02:01:12.340616Z","iopub.status.idle":"2021-06-29T02:01:12.345026Z","shell.execute_reply.started":"2021-06-29T02:01:12.340584Z","shell.execute_reply":"2021-06-29T02:01:12.344271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 3\n\nLet's create a new data set where all of the above categorical features will be one-hot encoded. We can fit this data and see how it affects the results.\n\n* Used the dataframe `.copy()` method to create a completely separate copy of the dataframe for one-hot encoding\n* On this new dataframe, one-hot encode each of the appropriate columns and add it back to the dataframe. Be sure to drop the original column.\n* For the data that are not one-hot encoded, drop the columns that are string categoricals.\n\nFor the first step, numerically encoding the string categoricals, either Scikit-learn;s `LabelEncoder` or `DictVectorizer` can be used. However, the former is probably easier since it doesn't require specifying a numerical value for each category, and we are going to one-hot encode all of the numerical values anyway. (Can you think of a time when `DictVectorizer` might be preferred?)","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"#Encode the categorical variables\nfrom sklearn.preprocessing import OneHotEncoder\nohe=OneHotEncoder(sparse=False)\nenc_cat=pd.DataFrame(ohe.fit_transform(X[cat_cols]))\n\n#One Hot Encoder removes columns names. we put them back uing the code below (note that each column is a unique feature \n#with either the value of 1 or 0)\ncol_names=pd.Series(list(ohe.get_feature_names()))\n#OHE puts x0,x1,.....xn_ besides each column name, remove it\nenc_cat.columns=col_names.str.split(\"_\",expand=True).loc[:,1]\nenc_cat.columns\n\n#Create a new dataframe that has the numerical columns and the encoded categorical ones\nnew_df=pd.concat([X[num_cols],enc_cat],axis=1)\n\n#Check\nnew_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:01:20.525942Z","iopub.execute_input":"2021-06-29T02:01:20.526466Z","iopub.status.idle":"2021-06-29T02:01:21.48923Z","shell.execute_reply.started":"2021-06-29T02:01:20.526416Z","shell.execute_reply":"2021-06-29T02:01:21.488196Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine how many extra columns would be created\nenc_cat.shape[1]-len(cat_cols)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:01:29.688802Z","iopub.execute_input":"2021-06-29T02:01:29.689182Z","iopub.status.idle":"2021-06-29T02:01:29.694092Z","shell.execute_reply.started":"2021-06-29T02:01:29.689145Z","shell.execute_reply":"2021-06-29T02:01:29.693419Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*We may also use pd.get_dummies as shown below BUT for better prediction performance it's better to stick to the sklearn method.For a more detailed answer, check this answer on [stackoverflow](https://stackoverflow.com/questions/36631163/what-are-the-pros-and-cons-between-get-dummies-pandas-and-onehotencoder-sciki)*","metadata":{}},{"cell_type":"code","source":"gd_enc=pd.get_dummies(X[cat_cols])\ngd_enc.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:01:40.358127Z","iopub.execute_input":"2021-06-29T02:01:40.358699Z","iopub.status.idle":"2021-06-29T02:01:40.52009Z","shell.execute_reply.started":"2021-06-29T02:01:40.358664Z","shell.execute_reply":"2021-06-29T02:01:40.519172Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 4\n\n* Create train and test splits of both data sets. To ensure the data gets split the same way, use the same `random_state` in each of the two splits.\n* For each data set, fit a basic linear regression model on the training data. \n* Calculate the mean squared error on both the train and test sets for the respective models. Which model produces smaller error on the test data and why?","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.model_selection import train_test_split\n\n#First predict the saleprice using the original dataset\nx_train,x_test,y_train,y_test=train_test_split(X[num_cols],y,random_state=0)\nlr=LinearRegression()\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)\nmsr_org=mean_squared_error(y_pred,y_test)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true},"execution":{"iopub.status.busy":"2021-06-29T02:02:11.173364Z","iopub.execute_input":"2021-06-29T02:02:11.173745Z","iopub.status.idle":"2021-06-29T02:02:11.365932Z","shell.execute_reply.started":"2021-06-29T02:02:11.173713Z","shell.execute_reply":"2021-06-29T02:02:11.364677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now predict the saleprice using the original dataset\nx_train_ohe,x_test_ohe,y_train_ohe,y_test_ohe=train_test_split(new_df,y,random_state=0)\nlr2=LinearRegression()\nlr2.fit(x_train_ohe,y_train_ohe)\ny_pred_ohe=lr2.predict(x_test_ohe)\nmsr_ohe=mean_squared_error(y_pred_ohe,y_test_ohe)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:04:11.886973Z","iopub.execute_input":"2021-06-29T02:04:11.887354Z","iopub.status.idle":"2021-06-29T02:04:11.957367Z","shell.execute_reply.started":"2021-06-29T02:04:11.887323Z","shell.execute_reply":"2021-06-29T02:04:11.956071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('-For the original dataframe the MSR is:',round(msr_org,2),'\\n','\\n'+'-For the encoded dataframe the MSR is:',msr_ohe)\nprint('\\n'+\"-The mean_squared_error increased by a factor of:\",round(msr_ohe/msr_org,2))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:04:14.583758Z","iopub.execute_input":"2021-06-29T02:04:14.584116Z","iopub.status.idle":"2021-06-29T02:04:14.590517Z","shell.execute_reply.started":"2021-06-29T02:04:14.584087Z","shell.execute_reply":"2021-06-29T02:04:14.589587Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the error values on the one-hot encoded data are very different for the train and test data. In particular, the errors on the test data are much higher. Based on the lecture, this is because the one-hot encoded model is overfitting the data. We will learn how to deal with issues like this in the next lesson.","metadata":{"run_control":{"marked":true}}},{"cell_type":"markdown","source":"## Question 5\n\nFor each of the data sets (one-hot encoded and not encoded):\n\n* Scale the all the non-hot encoded values using one of the following: `StandardScaler`, `MinMaxScaler`, `MaxAbsScaler`.\n* Compare the error calculated on the test sets\n\nBe sure to calculate the skew (to decide if a transformation should be done) and fit the scaler on *ONLY* the training data, but then apply it to both the train and test data identically.","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"#We may answer the above question by using the simple code below\n\n#Step one import the chosen methods and build your function\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler\ndef msr_for_scaled(x,y,scaler):\n    global y_test , y_pred_ss #Declare both the actual and the predicted values as global so they may be used later for \n                              #building plots\n    x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0)\n    x_tr_sca=x_train.copy()\n    x_te_sca=x_test.copy()\n    ss=scaler()\n    sca_tr_val=ss.fit_transform(x_tr_sca)\n    sca_te_val=ss.transform(x_te_sca)\n    lr=LinearRegression()\n    lr.fit(sca_tr_val,y_train)\n    y_pred_ss=lr.predict(sca_te_val)\n    msr_ss=mean_squared_error(y_pred_ss,y_test)\n    return print('The mean square error for prediction using the',scaler(),'method is:',round(msr_ss,2))\n\n#Step two calculate the mean squared error values for each of the scaling methods \nfor scaler in [StandardScaler,MinMaxScaler,MaxAbsScaler]:\n    msr_for_scaled(X[num_cols],y,scaler)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T02:04:32.918588Z","iopub.execute_input":"2021-06-29T02:04:32.919162Z","iopub.status.idle":"2021-06-29T02:04:33.025967Z","shell.execute_reply.started":"2021-06-29T02:04:32.919116Z","shell.execute_reply":"2021-06-29T02:04:33.024778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 6\n\nPlot predictions vs actual for one of the models.","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nsns.set_context('talk')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nax = plt.axes()\n# we are going to use y_test, y_test_pred\nax.scatter(y_test, y_pred_ss, alpha=0.7)\n\nax.set(xlabel='Ground truth', \n       ylabel='Predictions',\n       title='Ames, Iowa House Price Predictions vs Truth, using Linear Regression');","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"run_control":{"marked":true},"execution":{"iopub.status.busy":"2021-06-29T02:05:56.325051Z","iopub.execute_input":"2021-06-29T02:05:56.325437Z","iopub.status.idle":"2021-06-29T02:05:56.480807Z","shell.execute_reply.started":"2021-06-29T02:05:56.325402Z","shell.execute_reply":"2021-06-29T02:05:56.480109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<B><I>That's it. Thanks for your time. Keep on learning and spreading the knowledge</I>\n​\n<I>Until next time 🖐🖐 </I>","metadata":{}},{"cell_type":"markdown","source":"---\n### Machine Learning Foundation (C) 2020 IBM Corporation","metadata":{}}]}