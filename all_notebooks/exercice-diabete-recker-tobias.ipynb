{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque que toutes les données sont complètes. Ceci n'est pas contre pas vraiment le cas, parce que à chaque fois qu'une caractéristique est inconnue ils ont mis 0 comme valeur (une BloodPressure = 0 ne fait pas trop de sens si le patient est encore vivant)."},{"metadata":{},"cell_type":"markdown","source":"Je vais donc essayer de remplacer les 0 dans les colonnes voulues (pas remplacer les 0 dans Outcome bien sûr) par la moyenne des valeurs comme on l'a fait dans l'exemple titanic."},{"metadata":{},"cell_type":"markdown","source":"L'idée est de remplacer tout les 0 par la moyenne de la colonne:"},{"metadata":{"trusted":true},"cell_type":"code","source":"t.BloodPressure = t.BloodPressure.replace([0],t.BloodPressure.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On vient donc de remplacer tout les 0 de la colonne BloodPressure par la moyenne de la colonne. On refait cela pour les colonnes restantes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"t.SkinThickness = t.SkinThickness.replace([0],t.SkinThickness.mean())\nt.Insulin = t.Insulin.replace([0],t.Insulin.mean())\nt.BMI = t.BMI.replace([0],t.BMI.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maintenant que le tableau est rempli, on peut continuer avec la partie machine learning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"t.Outcome.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_malade = (t.Outcome == 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t[nb_malade].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = (t[nb_malade].Outcome.count())/(t.Outcome.count())\nprint(prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"34,9% des patients dans cette liste sont diabétique."},{"metadata":{},"cell_type":"markdown","source":"Je ne constate pas un déséquilibre des distributions puisque les donnés dépendent vraiment de chaque personne. De plus, une transformation en binaire me parait pas réalisable. Du coup, je continue avec la prédiction de patients positifs à l'aide du machine learning:  "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = t.sample(frac=0.8, random_state=1)\ndata_test = t.drop(data_train.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data_train.drop(['Outcome'], axis=1)\ny_train = data_train['Outcome']\nX_test = data_test.drop(['Outcome'], axis=1)\ny_test = data_test['Outcome']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On aplique le principe de la regression logistique:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_lr)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On obtient un résultat pas tellement satisfaisant (76,6%). La méthode de la regression logistique n'est donc peut-être pas la bonne méthode à choisir. On essayera d'appliquer la méthode des arbres de décision:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(t, hue=\"Outcome\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On constate que les courbes sont tous l'un sur l'autre. Une séparation par arbres de décision va donc être difficile. On constate également un déséquilibre des distributions des courbes 'Insulin' (l'hypothèse pris au début qu'il y en a pas de déséquiibre était donc faux). Je vais donc faire une transformation en log pour 'Insulin':  "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(t.Insulin, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t['log_Insulin'] = np.log(t.Insulin+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(t.log_Insulin, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = t.drop(['Insulin'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On refait la regression logistique:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = t.sample(frac=0.8, random_state=1)\ndata_test = t.drop(data_train.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data_train.drop(['Outcome'], axis=1)\ny_train = data_train['Outcome']\nX_test = data_test.drop(['Outcome'], axis=1)\ny_test = data_test['Outcome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_lr)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On constate que le résultat c'est empiré (maintenant 75,3%, avant 76,6%). Je ne sais pas comment améliorer ce résultat. En regardant les diférentes distributions on a déjà constaté que les caractéristiques sont tous l'un sur l'autre, je peux donc m'imaginer que l'algorithme a des difficultés à déterminer le Outcome. Je vais continuer avec la mesure de performance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = lr.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(probas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tprobas = pd.DataFrame(probas,columns=['proba_0','proba_1'])\ntprobas['y'] = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tprobas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.distplot(1-tprobas.proba_0[tprobas.y==0], bins=50)\nsns.distplot(tprobas.proba_1[tprobas.y==1], bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit que la distribution est loin d'être idéale, comme déjà constaté avant. On continue avec la création de la courbe ROC:"},{"metadata":{"trusted":true},"cell_type":"code","source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\nplt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque que la courbe se trouve au milieu de la courbe idéale et de la courbe la moin bien."},{"metadata":{},"cell_type":"markdown","source":"Ajustement des hyperparamètres (Random Forests):"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train, y_train)\ny_rf1 = rf.predict(X_test)\nprint(classification_report(y_test, y_rf1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = t.drop(['Outcome'], axis=1)\ny = t.Outcome","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'n_estimators', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 2)\nplt.xlabel('n_estimators')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La méthode GridSearchCV:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n              'n_estimators': [10, 100, 500],\n              'min_samples_leaf': [1, 20, 50]\n             }\nestimator = ensemble.RandomForestClassifier()\nrf_gs = model_selection.GridSearchCV(estimator, param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf2 = rf_gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_rf2 = rf2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_rf2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"J'ai l'impression que cette méthode n'a pas pu grandemant améliorer le modèle, au contraire, les valeurs ont diminués."},{"metadata":{},"cell_type":"markdown","source":"Pour finaliser ce Notebook, voici le classement de l'importance des différentes caractéristiques:"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rf2.feature_importances_\nindices = np.argsort(importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.title('Importance des caracteristiques')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Annexe XGBoost:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as XGB\nxgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)\nprint(classification_report(y_test, y_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A nouveau pas d'amélioration."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}