{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# \nBasic Text Mining Methods¶\nText can be sentences, strings, words, characters and large documents\nNow lets create a sentence to understand basics of text mining methods.\nOur sentece is \"no woman no cry\" from Bob Marley."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create a text\ntext = \"No woman no tears\"\n\n# length of text ( includes spaces)\nprint(\"length of text: \",len(text))\n\n# split the text\nsplitted_text = text.split() # default split methods splits text according to spaces\nprint(\"Splitted text: \",splitted_text)    # splitted_text is a list that includes words of text sentence\n# each word is called token in text mining world.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n# capitalized words with istitle() method that finds capitalized words\ncapital_words = [ word for word in splitted_text if word.istitle()]\nprint(\"Capitalized words: \",capital_words)\n\n# words which end with \"o\": endswith() method finds last letter of word\nwords_end_with_o =  [word for word in splitted_text if word.endswith(\"o\")]\nprint(\"words end with o: \",words_end_with_o) \n\n# words which starts with \"w\": startswith() method\nwords_start_with_w = [word for word in splitted_text if word.startswith(\"w\")]\nprint(\"words start with w: \",words_start_with_w) \n\n# unique with set() method\nprint(\"unique words: \",set(splitted_text))  # actually the word \"no\" is occured twice bc one word is \"no\" and others \"No\" there is a capital letter at first letter\n\n# make all letters lowercase with lower() method\nlowercase_text = [word.lower() for word in splitted_text]\n\n# then find uniques again with set() method\nprint(\"unique words: \",set(lowercase_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# chech words includes or not includes particular substring or letter\nprint(\"Is w letter in woman word:\", \"w\" in \"woman\")\n\n# check words are upper case or lower case\nprint(\"Is word uppercase:\", \"WOMAN\".isupper())\nprint(\"Is word lowercase:\", \"tears\".islower())\n\n# check words are made of by digits or not\nprint(\"Is word made of by digits: \",\"12345\".isdigit())\n\n# get rid of from white space characters like spaces and tabs or from unwanted letters with strip() method\nprint(\"00000000No tears: \",\"00000000No tears\".strip(\"0\"))\n\n# find particular letter from front \nprint(\"Find particular letter from back: \",\"No tears no\".find(\"o\"))  # at index 1\n\n# find particular letter from back  rfind = reverse find\nprint(\"Find particular letter from back: \",\"No tears no\".rfind(\"o\"))  # at index 8\n\n# replace letter with letter\nprint(\"Replace o with 3 \", \"No tears no\".replace(\"o\",\"3\"))\n\n# find each letter and store them in list\nprint(\"Each letter: \",list(\"No tears\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Cleaning text\ntext1 = \"    Blessed and focussed    \"\nprint(\"Split text: \",text1.split(\" \"))   # as you can see there are unnecessary white space in list\n\n# get rid of from these unnecassary white spaces with strip() method then split\nprint(\"Cleaned text: \",text1.strip().split(\" \"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading files line by line\nf = open(\"../input/religious-and-philosophical-texts/35895-0.txt\",\"r\")\n\n# read first line\nprint(f.readline())\n\n# length of text\ntext3=f.read()\nprint(\"Length of text: \",len(text3))\n\n# Number of lines with splitlines() method\nlines = text3.splitlines()\nprint(\"Number of lines: \",len(lines))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ndata = pd.read_csv(r\"../input/ben-hamners-tweets/benhamner.csv\", encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find which entries contain the word 'appointment'\nprint(\"In his tweets, the rate of occuring kaggle word is: \",sum(data.text.str.contains('kaggle'))/len(data))\n# text\ntext = data.text[1]\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find regular expression on text\n# import regular expression package\nimport re\n# find callouts that starts with @\ncallouts = [word for word in text.split(\" \") if re.search(\"@[A-Za-z0-9_]+\",word)]\nprint(\"callouts: \",callouts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# continue finding regular expressions\n# [A-Za-z0-9_] =\\w\n# We will use \"\\w\" to find callouts and our result will be same because \\w matches with [A-Za-z0-9_] \ncallouts1 = [word for word in text.split(\" \") if re.search(\"@\\w+\",word)]\nprint(\"callouts: \",callouts1)\n# find specific characters like \"w\"\nprint(re.findall(r\"[w]\",text))\n# \"w\"ith, \"w\"indo\"w\", sho\"w\"ing, s\"w\"itches \n\n# do not find specific character like \"w\". We will use \"^\" symbol\nprint(re.findall(r\"[^w]\",text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regular expressions for Dates\ndate = \"15-10-2000\\n09/10/2005\\n15-05-1999\\n05/05/99\\n\\n05/05/199\\n\\n05/05/9\"\nre.findall(r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{1,4}\",date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import natural language tool kit\nimport nltk as nlp\n\n# counting vocabulary of words\ntext = data.text[1]\nsplitted = text.split(\" \")\nprint(\"number of words: \",len(splitted))\n\n# counting unique vocabulary of words\ntext = data.text[1]\nprint(\"number of unique words: \",len(set(splitted)))\n\n# print first five unique words\nprint(\"first 5 unique words: \",list(set(splitted))[:5])\n\n# frequency of words \ndist = nlp.FreqDist(splitted)\nprint(\"frequency of words: \",dist)\n\n# look at keys in dist\nprint(\"words in text: \",dist.keys())\n\n# count how many time a particalar value occurs. Lets look at \"box\"\nprint(\"the word box is occured how many times:\",dist[\"box\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nwords = \"task Tasked tasks tasking\"\nwords_list = words.lower().split(\" \")\nprint(\"normalized words: \",words_list)\n\n# stemming\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in words_list]\nprint(\"roots of task Tasked tasks tasking: \",roots)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stemming\nstemming_word_list = [\"Universal\",\"recognition\",\"Become\",\"being\",\"happened\"]\nporter_stemmer = nlp.PorterStemmer()\nroots = [porter_stemmer.stem(each) for each in stemming_word_list]\nprint(\"result of stemming: \",roots)\n\n# lemmatization\nlemma = nlp.WordNetLemmatizer()\nlemma_roots = [lemma.lemmatize(each) for each in stemming_word_list]\nprint(\"result of lemmatization: \",lemma_roots)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_t = \"You’re in the right place!\"\nprint(\"split the sentece: \", text_t.split(\" \"))  # 5 words\n\n# tokenization with nltk\nprint(\"tokenize with nltk: \",nlp.word_tokenize(text_t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}