{"cells":[{"metadata":{"_cell_guid":"059fafc4-f8e9-cf07-c974-e2e5733f2654"},"cell_type":"markdown","source":"> ### In this guide, we'll walk through the steps involved in principal component analysis (PCA) using plain old Python--without scikit-learn. Afterward, we'll do it the 'easy' way with Scikit-learn's PCA algorithm.\n"},{"metadata":{"_cell_guid":"68a2ac0c-44cf-5243-69ed-695b0c291cb7"},"cell_type":"markdown","source":"# 1) Import libraries"},{"metadata":{"_cell_guid":"bf4d3629-e60d-3eb9-aa4f-791e6585f85c","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2d81588-52e3-b59f-064f-e917306a62d6"},"cell_type":"markdown","source":"# 2) Load the dataset\nTo import the dataset we will use Pandas library.It is the best Python library to play with the dataset and has a lot of functionalities. "},{"metadata":{"_cell_guid":"21c4543f-1ae4-07b9-551d-a8f7ca521334","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/hr-comma-sepcsv/HR_comma_sep.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae4e15c8-72f5-3036-3427-41735f9c8444","trusted":true},"cell_type":"code","source":"df.rename(columns={'sales':'department','Work_accident':'work_accident','time_spend_company':'years_with_company','left':'left_company'},inplace=True)\ncolumns_names=df.columns.tolist()\nprint(\"Columns names:\")\nprint(columns_names)\ndf.describe()\n# df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0441865c-3c67-48b7-b9e6-e89bc3cc2d18"},"cell_type":"markdown","source":"df.columns.tolist() fetches all the columns and then convert it into list type.This step is just to check out all the column names in our data.Columns are also called as features of our datasets."},{"metadata":{"_cell_guid":"e937f0f0-9de1-d057-22ce-0abdcee66766","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8594669f-f6c0-2bcc-5766-ba49010cf452","trusted":true},"cell_type":"code","source":"df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4d53e62-c4ea-cb89-f694-e10bdc47490f","trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"715ae8c7-3c10-ac7b-3d52-fc0c445753c8"},"cell_type":"markdown","source":"**df.corr()** compute pairwise correlation of columns. Correlation shows how each pair of variables is related to each other. Positive values indicate positive correlation. Negative values indicates negative correlation. \n\nThe magnitude of the value indicates the level/strength of correlation (0 <= |x| <= 1).\n"},{"metadata":{"_cell_guid":"ba14fe8d-3e77-d982-7776-a3e9cb67c039"},"cell_type":"markdown","source":"## Visualising correlation using Seaborn library"},{"metadata":{"_cell_guid":"3e75c17d-9c10-a556-3d99-5f32e4f8b4fb","trusted":true},"cell_type":"code","source":"correlation = df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n\nplt.title('Correlation between different fearures')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c760c111-3856-7d3f-03e0-fd403461a591"},"cell_type":"markdown","source":"**Doing some visualisation before moving onto PCA**"},{"metadata":{"_cell_guid":"1412290a-29b9-9f62-6cad-fffbc07bf7fd","trusted":true},"cell_type":"code","source":"df['sales'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15f45340-7d73-58c4-16eb-85a91b236ab8"},"cell_type":"markdown","source":"Here we are printing all the unique values in **sales** columns"},{"metadata":{"_cell_guid":"0216658c-d279-d76c-e9b9-947daf866026","trusted":true},"cell_type":"code","source":"satisfaction_by_dept=df.groupby('department').mean()\nsatisfaction_by_dept.sort_values(by=\"satisfaction_level\", ascending=True, inplace=True)\nsatisfaction_by_dept","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e89f233-180a-2a39-ffeb-d64d1cba4bbd","trusted":true},"cell_type":"code","source":"y_pos = np.arange(len(satisfaction_by_dept.index))\n\nplt.barh(y_pos, satisfaction_by_dept['satisfaction_level'], align='center', alpha=0.8)\nplt.yticks(y_pos, satisfaction_by_dept.index)\n\nplt.xlabel('Satisfaction level')\nplt.title('Mean Satisfaction Level of each department')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"116c4b0b-cac8-52e4-709e-a5ac78752d07"},"cell_type":"markdown","source":"# Principal Component Analysis"},{"metadata":{"_cell_guid":"6653ce6e-0f41-e576-b051-09eb2784c93b","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e037b5d2-eb93-9824-45cb-1bbec730e929","trusted":true},"cell_type":"code","source":"df_drop=df.drop(labels=['department','salary'],axis=1)\ndf_drop.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7cec556b-797e-fc62-e7d9-c78035fda8e2"},"cell_type":"markdown","source":"**df.drop()**  is the method to drop the columns in our dataframe"},{"metadata":{"_cell_guid":"dede7a0b-5084-b7de-4259-fb5fac90c376"},"cell_type":"markdown","source":"Now we need to bring \"left\" column to the front as it is the label and not the feature."},{"metadata":{"_cell_guid":"2af16458-8a20-8a23-fa61-796a0cc2ac6a","trusted":true},"cell_type":"code","source":"cols = df_drop.columns.tolist()\ncols","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"75a23734-4e1a-21eb-e33a-1ee6dd32465c"},"cell_type":"markdown","source":"Here we are converting columns of the dataframe to list so it would be easier for us to reshuffle the columns.We are going to use cols.insert method\n"},{"metadata":{"_cell_guid":"be6e0527-b97e-6dc7-16f0-5a2b803c462b","trusted":true},"cell_type":"code","source":"cols.insert(0, cols.pop(cols.index('left_company')))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55548349-98fb-244c-bdb0-5ffc6723bc4e","trusted":true},"cell_type":"code","source":"cols","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d5a8bee5-82b1-ea41-58a3-07b2a9c100f2","trusted":true},"cell_type":"code","source":"#df_drop = df_drop.reindex(columns=cols)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ee617b8-cbc4-e9ac-5f57-a6dc91b4b8a3"},"cell_type":"markdown","source":"By using df_drop.reindex(columns=cols) we are converting list to columns again"},{"metadata":{"_cell_guid":"3b8a1422-663c-cd2c-fe88-5bb2970ee8a0"},"cell_type":"markdown","source":"Now we are separating features of our dataframe from the labels."},{"metadata":{"_cell_guid":"d4906f59-7d57-5979-b239-7fe91ba63482","trusted":true},"cell_type":"code","source":"X = df_drop.iloc[:,1:8].values\ny = df_drop.iloc[:,0].values\nX","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"99b81187-14ce-5a27-c299-b62baba0ea8b","trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be17babb-5ffe-5c8a-c316-36eb17c67778","trusted":true},"cell_type":"code","source":"np.shape(X)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2568bcdd-9936-f79d-e332-9b1ad62caf3a"},"cell_type":"markdown","source":"Thus X is now a matrix with 14999 rows and 7 columns"},{"metadata":{"_cell_guid":"87b4bbb6-92bb-bbee-9184-d380aa257add","trusted":true},"cell_type":"code","source":"np.shape(y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59ca6356-f025-5e5b-fc56-a97fd03e40be"},"cell_type":"markdown","source":"y is now a matrix with 14999 rows and 1 column"},{"metadata":{"_cell_guid":"c31ea24e-ce38-02d8-031f-3361c57d633f"},"cell_type":"markdown","source":"# 4) Data Standardisation\nStandardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance). It is useful to standardize attributes for a model.\nStandardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data "},{"metadata":{"_cell_guid":"0c518a19-10a5-97ed-b18f-e8e28d43cd6b","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"40ebbdc2-7feb-ba0d-0154-1a47d53d4801"},"cell_type":"markdown","source":"# 5) Computing Eigenvectors and Eigenvalues:\nBefore computing Eigen vectors and values we need to calculate covariance matrix."},{"metadata":{"_cell_guid":"24b83e60-da89-5e7d-8de9-6ea385a8b9f2"},"cell_type":"markdown","source":"## Covariance matrix"},{"metadata":{"_cell_guid":"fd99a8b1-4ad0-de34-16dc-fa1ec6315aec","trusted":true},"cell_type":"code","source":"mean_vec = np.mean(X_std, axis=0)\ncov_matrix = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\nprint('Covariance matrix \\n%s' % cov_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d2ad526-3e39-8b21-efca-c9147f1dcf32","trusted":true},"cell_type":"code","source":"print('NumPy covariance matrix: \\n%s' % np.cov(X_std.T))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fa045e55-3a25-4204-38e9-b3bafdf5a4c3"},"cell_type":"markdown","source":"Equivalently we could have used Numpy np.cov to calculate covariance matrix"},{"metadata":{"_cell_guid":"36f9299f-ff1b-05b3-fc58-f193856003e8","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.heatmap(cov_matrix, vmax=1, square=True, annot=True, cmap='cubehelix')\n\nplt.title('Correlation between different features')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eff69898-d80f-4b7e-6fb2-5ad387d03011"},"cell_type":"markdown","source":"# Eigen decomposition of the covariance matrix"},{"metadata":{"_cell_guid":"aaa8cb48-91db-8f89-afd6-2899a0b7ba35","trusted":true},"cell_type":"code","source":"eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n\nprint('Eigenvectors \\n%s' % eig_vecs)\nprint('\\nEigenvalues \\n%s' % eig_vals)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e301e61-6c3f-34bd-6939-fce77aa98c6f"},"cell_type":"markdown","source":"# 6) Selecting Principal Components\n\nIn order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped."},{"metadata":{"_cell_guid":"4a1a33f8-0b32-ba34-6b70-e0906c381c29","trusted":true},"cell_type":"code","source":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\n#eig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a673654b-a8f4-5050-c773-e84fb8dc1e9e"},"cell_type":"markdown","source":"**Explained Variance**\nAfter sorting the eigenpairs, the next question is \"how many principal components are we going to choose for our new feature subspace?\" A useful measure is the so-called \"explained variance,\" which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components."},{"metadata":{"_cell_guid":"6ecc2703-a0a8-9feb-0026-d37380860b7f","trusted":true},"cell_type":"code","source":"tot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in eig_vals] #sorted(eig_vals, reverse=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pos = [i for i, _ in enumerate(cols[1:])]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13ccc0b4-2e08-9394-7a77-ef31c31fd748","trusted":true},"cell_type":"code","source":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(10, 4))\n\n    plt.barh(x_pos, var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.xlabel('Explained variance ratio')\n    plt.ylabel('Principal components')\n    plt.yticks(x_pos, cols[1:])\n    plt.legend(loc='best')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3020cccc-5ed9-1552-0003-cb51989ea964"},"cell_type":"markdown","source":"The plot above clearly shows that maximum variance (somewhere around 26%) can be explained by the satisfaction_level component alone. The promotion_5years, work_accident, years_with_company, and avg_monthly_hrs components share each about12-15%. The number_projects and last_evaluation components share the least information, but these cannot be ignored--since together they contribute almost 17% of the data."},{"metadata":{"_cell_guid":"490be121-226a-2a21-2dcf-8dab376007dc"},"cell_type":"markdown","source":"### Projection Matrix"},{"metadata":{"_cell_guid":"58c8d070-8fd6-74b4-fd8c-b858d17bb93f"},"cell_type":"markdown","source":"The construction of the projection matrix that will be used to transform the Human resouces analytics data onto the new feature subspace. **Suppose only 1st and 2nd principal component shares the maximum amount of information say around 90%**. Hence we can drop other components. Here, we are reducing the 7-dimensional feature space to a 2-dimensional feature subspace, by choosing the “top 2” eigenvectors with the highest eigenvalues to construct our d×k-dimensional eigenvector matrix W\n"},{"metadata":{"_cell_guid":"bf556dc8-b89a-9f3b-5ec8-1206cc42fdb4","trusted":true},"cell_type":"code","source":"matrix_w = np.hstack((eig_pairs[0][1].reshape(7,1), \n                      eig_pairs[1][1].reshape(7,1)\n                    ))\nprint('Matrix W:\\n', matrix_w)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe78ecb6-61ba-3ce7-2407-f6ad6ca022ba"},"cell_type":"markdown","source":"**Projection Onto the New Feature Space**\nIn this last step we will use the 7×2-dimensional projection matrix W to transform our samples onto the new subspace via the equation\n**Y=X×W**"},{"metadata":{"_cell_guid":"55040b4e-b6f2-fe75-54e8-cf2e5c2961b5","trusted":true},"cell_type":"code","source":"Y = X_std.dot(matrix_w)\nY","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73d741cc-2761-4fd3-8f04-830385ee7e1a"},"cell_type":"markdown","source":"# PCA in scikit-learn"},{"metadata":{"_cell_guid":"2032535a-7c1e-a5b7-9135-80e19c3c8490","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA().fit(X_std)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,7,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fd485349-31dc-fbca-ee6b-ec445cf50349"},"cell_type":"markdown","source":"The above plot shows almost 90% variance by the first 6 components. Therfore we can drop 7th component."},{"metadata":{"_cell_guid":"2b886668-1151-740e-f895-acda6d54bbf0","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA \nsklearn_pca = PCA(n_components=6)\nY_sklearn = sklearn_pca.fit_transform(X_std)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ac6a338-34a3-0b59-ccde-e6522ffa59e7","trusted":true},"cell_type":"code","source":"print(Y_sklearn)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d8f47c5-df6d-748e-f6c0-016200f3fde6","trusted":true},"cell_type":"code","source":"Y_sklearn.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89181b37-5097-e4ec-5237-610b2c47df04"},"cell_type":"markdown","source":"Thus Principal Component Analysis is used to remove the redundant features from the datasets without losing much information.These features are low dimensional in nature.The first component has the highest variance followed by second, third and so on.PCA works best on data set having 3 or higher dimensions. Because, with higher dimensions, it becomes increasingly difficult to make interpretations from the resultant cloud of data."},{"metadata":{"_cell_guid":"1803ba6f-303d-791d-4d81-323624a87f56"},"cell_type":"markdown","source":"You can find my notebook on Github: \n(\"https://github.com/nirajvermafcb/Data-Science-with-python\")"},{"metadata":{"_cell_guid":"5be4384a-7546-b0ad-2542-f87c27309018"},"cell_type":"markdown","source":"Here is my notebook for Principal Component Analysis with Scikit-learn:\n(https://www.kaggle.com/nirajvermafcb/d/nsrose7224/crowdedness-at-the-campus-gym/principal-component-analysis-with-scikit-learn)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}