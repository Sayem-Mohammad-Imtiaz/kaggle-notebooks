{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is an alternative version of Uplift Modeling VS Churn Prediction from my [previous experiment](https://www.kaggle.com/davinwijaya/why-you-should-start-using-uplift-modeling)\n. The difference is in this notebook I use Logistic Regression as the Machine Learning algorithm."},{"metadata":{},"cell_type":"markdown","source":"# 1. Setup\nFirst let's set up the environment and datasets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import the packages and libraries needed for this project\nimport matplotlib as mpl, matplotlib.pyplot as plt, \\\npandas as pd, seaborn as sns, sklearn as sk\nfrom sklearn.metrics import accuracy_score, \\\nconfusion_matrix, multilabel_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Checking for package version\nprint(\"Matplotlib Version\", mpl.__version__)\nprint(\"Pandas Version\", pd.__version__)\nprint(\"Seaborn Version\", sns.__version__)\nprint(\"Sci-kit learn Version\", sk.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the dataset\ndf_data_2 = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf_model_2 = df_data_2.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"Now we've the datasets ready, let's check for null data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore dataset 2\ndf_data_2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null.\ndisplay(df_data_2.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, there is no null data."},{"metadata":{},"cell_type":"markdown","source":"# 3. Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare unwanted features (i.e. columns) that will be dropped.\ndrop_2 = ['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18']\n# drop the features.\ndf_model_2 = df_model_2.drop(drop_2,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename all target features.\ndf_model_2 = df_model_2.rename(columns={'Attrition': 'churn'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename all treatment features.\ndf_model_2 = df_model_2.rename(columns={'OverTime': 'treatment'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare features for label encoding.\nstring2 = ['churn',\n          'treatment',\n          'BusinessTravel']\n# Explore the unique data for label encoding\nfor col in string2:\n    display(col, df_model_2[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manually label the churn status, Yes = 1, and No = 0\ndf_model_2.churn = df_model_2.churn.map({'Yes': 1, 'No': 0})\n\n# Manually label the treatment status (Overtime), Yes = 0 (Overtime), No = 1 (Does not receive Overtime)\ndf_model_2.treatment = df_model_2.treatment.map({'Yes': 0, 'No': 1})\n\n# Declaration BusinessTravel\ndf_model_2.BusinessTravel = df_model_2.BusinessTravel.map({'Non-Travel': 0,\n                                                           'Travel_Rarely': 1,\n                                                           'Travel_Frequently':2})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, let's turn the rest of the string/object data into integer with the magical get_dummies function (One hot encoding) from Pandas package, so we can feed the data into LogisticRegression. Moreover, I add another dataframe df_model_inverse that will be useful for later:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot Encoding:\ndf_model_2, df_model_inverse_2 = pd.get_dummies(df_model_2), pd.get_dummies(df_model_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the treatment's correlation to employee turnover:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_treatment(df:pd.DataFrame):\n    \"\"\"Function to calculate the treatment's correlation\n    \"\"\"\n    correlation = df[['treatment','churn']].corr(method ='pearson') \n    return(pd.DataFrame(round(correlation.loc['churn'] * 100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataset 2:\", correlation_treatment(df_model_2).iloc[0,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, now all of the treatment features are negatively correlated. We will use the positive ones later at the end of this project. Next let's add the four uplift category for each datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def declare_target_class(df:pd.DataFrame):\n    \"\"\"Function for declare the target class\n    \"\"\"\n    #CN:\n    df['target_class'] = 0 \n    #CR:\n    df.loc[(df.treatment == 0) & (df.churn == 0),'target_class'] = 1 \n    #TN:\n    df.loc[(df.treatment == 1) & (df.churn == 1),'target_class'] = 2 \n    #TR:\n    df.loc[(df.treatment == 1) & (df.churn == 0),'target_class'] = 3 \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the four target classes\ndf_model_2 = declare_target_class(df_model_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Machine Learning Modeling"},{"metadata":{},"cell_type":"markdown","source":"Finally we're ready to start the machine learning process:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(df_model:pd.DataFrame):\n    \"\"\"Split data into training data and testing data\n    \"\"\"\n    X = df_model.drop(['churn','target_class'],axis=1)\n    y = df_model.churn\n    z = df_model.target_class\n    X_train, X_test, \\\n    y_train, y_test, \\\n    z_train, z_test = train_test_split(X,\n                                       y,\n                                       z,\n                                       test_size=0.3,\n                                       random_state=42,\n                                       stratify=df_model['treatment'])\n    return X_train,X_test, y_train, y_test, z_train, z_test\n\n\ndef machine_learning(X_train:pd.DataFrame,\n                     X_test:pd.DataFrame,\n                     y_train:pd.DataFrame,\n                     y_test:pd.DataFrame,\n                     z_train:pd.DataFrame,\n                     z_test:pd.DataFrame):\n    \"\"\"Machine learning process consists of \n    data training, and data testing process (i.e. prediction) with Logistic Regression Algorithm\n    \"\"\"\n    # prepare a new DataFrame\n    prediction_results = pd.DataFrame(X_test).copy()\n    \n    # train the ETU model\n    model_etu \\\n    = LogisticRegression().fit(X_train.drop('treatment', axis=1), z_train)\n    # prediction Process for ETU model \n    prediction_etu \\\n    = model_etu.predict(X_test.drop('treatment', axis=1))\n    probability__etu \\\n    = model_etu.predict_proba(X_test.drop('treatment', axis=1))\n    prediction_results['prediction_target_class'] = prediction_etu\n    prediction_results['proba_CN'] = probability__etu[:,0] \n    prediction_results['proba_CR'] = probability__etu[:,1] \n    prediction_results['proba_TN'] = probability__etu[:,2] \n    prediction_results['proba_TR'] = probability__etu[:,3]\n    prediction_results['score_etu'] = prediction_results.eval('\\\n    proba_CN/(proba_CN+proba_CR) \\\n    + proba_TR/(proba_TN+proba_TR) \\\n    - proba_TN/(proba_TN+proba_TR) \\\n    - proba_CR/(proba_CN+proba_CR)')  \n    \n    # add the churn and target class into dataframe as validation data\n    prediction_results['churn'] = y_test\n    prediction_results['target_class'] = z_test\n    return prediction_results\n\n\ndef predict(df_model:pd.DataFrame):\n    \"\"\"Combining data split and machine learning process with Logistic Regression\n    \"\"\"\n    X_train, X_test, y_train, y_test, z_train, z_test = split_data(df_model)\n    prediction_results = machine_learning(X_train,\n                                          X_test,\n                                          y_train,\n                                          y_test,\n                                          z_train,\n                                          z_test)\n    print(\"Prediction has succeeded\")\n    return prediction_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Machine Learning Modelling Process\nprint(\"predicting dataset 2 ...\")\nprediction_results_2 = predict(df_model_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction results are stored in prediction_results_1, prediction_results_2, and prediction_results_3 for dataset 1, dataset 2, and dataset 3, respectively."},{"metadata":{},"cell_type":"markdown","source":"# 5. Evaluating predictive performance"},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate the predictive performance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cm_evaluation(df:pd.DataFrame):\n    \"\"\"Confusion matrix evaluation\n    \"\"\"      \n    print(\"-----------------------------------\")\n    \n    print(\"2. ETU's confusion matrix result:\")   \n    confusion_etu = multilabel_confusion_matrix(df['target_class'], df['prediction_target_class'])\n    print(\"a. CN's confusion matrix:\")  \n    df_cn = pd.DataFrame(confusion_etu[0], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_cn)\n    print(\"b. CR's confusion matrix:\") \n    df_cr = pd.DataFrame(confusion_etu[1], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_cr) \n    print(\"c. TN's confusion matrix:\")\n    df_tn = pd.DataFrame(confusion_etu[2], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_tn) \n    print(\"d. TR's confusion matrix:\") \n    df_tr = pd.DataFrame(confusion_etu[3], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_tr)\n    \n    print(\"===================================\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In [Confusion Matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62), the True Positive and False Negative are the amount of successful predictions and the True Negative and False Positive are the amount of failed predictions. Therefore, let's generate the confusion matrices:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix Evaluation\nprint(\"Dataset 2\")\ncm_evaluation(prediction_results_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's calculate the accuracy result:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_evaluation(df:pd.DataFrame):\n    \"\"\"Accuracy evaluation\n    \"\"\"    \n    \n    akurasi_uplift = accuracy_score(df['target_class'],\n                                    df['prediction_target_class'])\n    print('ETU model accuracy: %.2f%%' % (akurasi_uplift * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy Evaluation Process.\nprint(\"Dataset 2\")\naccuracy_evaluation(prediction_results_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, seems like ETP models are much better than ETU models in terms of prediction accuracy. That makes sense anyway, because ETP models only predict two possible outcomes (The employee is turnover or stay), where ETU models predict four possible outcomes (Persuadables, Sure Things, Lost Causes, and Sleeping Dogs/Do-not-disturbs). But will ETP will also have a better performance in solving the employee turnover (prescriptive performance)? Let's find out."},{"metadata":{},"cell_type":"markdown","source":"# 6. Evaluating prescriptive performance"},{"metadata":{},"cell_type":"markdown","source":"Now let's use the prediction results to solve the problem. As explained before, for ETP model employees are ranked by their turnover probability. Employees with the highest turnover probability will be targeted with a retention campaign (the treatment features declared before). On the other side, the ETU models are ranked by its uplift score with LGWUM's formulation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sorting_data(df:pd.DataFrame):\n    \"\"\"Function to sort data\n    \"\"\"\n    # Set up new DataFrames for ETP model and ETU model\n    df_u = pd.DataFrame({'n':[], 'target_class':[]})\n    df_u['target_class'] = df['target_class']\n    \n    \n    # Add quantiles\n    df_u['n'] = df.score_etu.rank(pct=True, ascending=False)\n    df_u['score'] = df['score_etu']\n    \n    \n    # Ranking the data by deciles\n    df_u = df_u.sort_values(by='n').reset_index(drop=True)\n    return df_u\n\n\ndef calculating_qini(df:pd.DataFrame):\n    \"\"\"Function to measure the Qini value\n    \"\"\"\n    # Calculate the C, T, CR, and TR\n    C, T = sum(df['target_class'] <= 1), sum(df['target_class'] >= 2)\n    df['cr'] = 0\n    df['tr'] = 0\n    df.loc[df.target_class  == 1,'cr'] = 1\n    df.loc[df.target_class  == 3,'tr'] = 1\n    df['cr/c'] = df.cr.cumsum() / C\n    df['tr/t'] = df.tr.cumsum() / T\n    \n    \n    # Calculate & add the qini value into the Dataframe\n    df['uplift'] = df['tr/t'] - df['cr/c']\n    df['random'] = df['n'] * df['uplift'].iloc[-1]\n    # Add q0 into the Dataframe\n    q0 = pd.DataFrame({'n':0, 'uplift':0, 'target_class': None}, index =[0])\n    qini = pd.concat([q0, df]).reset_index(drop = True)\n    return qini\n\n\ndef merging_data(df_u:pd.DataFrame):\n    \"\"\"Function to add the 'Model' column and merge the dataframe into one\n    \"\"\"\n    df_u['model'] = 'ETU'\n    df = pd.concat([df_u]).sort_values(by='n').reset_index(drop = True)\n    return df\n\n\ndef plot_qini(df:pd.DataFrame):\n    \"\"\"Function to plot qini\n    \"\"\"\n    # Define the data that will be plotted\n    order = ['ETU','ETP']\n    ax = sns.lineplot(x='n', y=df.uplift, hue='model', data=df,\n                      style='model', palette=['red','deepskyblue'],\n                      style_order=order, hue_order = order)\n    \n    \n    # Additional plot display settings\n    handles, labels = ax.get_legend_handles_labels()\n    plt.xlabel('Proportion targeted',fontsize=30)\n    plt.ylabel('Uplift',fontsize=30)\n    plt.subplots_adjust(right=1)\n    plt.subplots_adjust(top=1)\n    plt.legend(fontsize=30)\n    ax.tick_params(labelsize=24)\n    ax.legend(handles=handles[1:], labels=labels[1:])\n    ax.plot([0,1], [0,df.loc[len(df) - 1,'uplift']],'--', color='grey')\n    return ax\n\n\ndef evaluation_qini(prediction_results:pd.DataFrame):\n    \"\"\"Function to combine all qini evaluation processes\n    \"\"\"\n    df_u = sorting_data(prediction_results)\n    qini_u = calculating_qini(df_u)\n    qini = merging_data(qini_u)\n    ax = plot_qini(qini)\n    return ax, qini","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Qini evaluation results for DataSet 2 with negative treatment correlation\nax, qini_2 = evaluation_qini(prediction_results_2)\nplt.title('Qini Curve - Dataset 2',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_2_n.pdf', bbox_inches='tight'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The nect process to inverse treatment's parameter.\nThus also inverse the treatment's correlation from negative to positive\nthis is the opposite of previous treatment \"Overtime\" which is the treatment is to target employee with Overtime.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# So now we change the treatment from Overtime to No-overtime, Yes = 1 (Receive No-overtime), No = 0 (Does not receive No-overtime)\ndf_model_inverse_2.treatment = df_model_inverse_2.treatment.replace({0: 1, 1: 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recalculate the treatment correlation\ndisplay(correlation_treatment(df_model_inverse_2).iloc[0,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, now the treatment features are positively correlated with employee turnover. This means, if we target the employees with this treatment, it's more likely that the employee turnover rate will be increased. So it'll be wise to use this treatment carefully. Okay, now let's repeat the prediction procedure once again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the target class feature to all three datasets\ndf_model_inverse_2= declare_target_class(df_model_inverse_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do the prediction process once more time\nprediction_results_inverse_2 = predict(df_model_inverse_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# qini evaluation results for DataSet 2 with positive treatment correlation\nax, qini_inverse_2 = evaluation_qini(prediction_results_inverse_2)\nplt.title('Qini Curve - Dataset 2',fontsize=20)\n\n\n# save into pdf:\nplt.savefig('qini_2_p.jpg', bbox_inches='tight')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}