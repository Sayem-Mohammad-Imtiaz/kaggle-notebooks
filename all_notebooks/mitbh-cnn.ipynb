{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# libraries\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport csv\nimport itertools\nimport collections\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# matplotlib settings\nplt.rcParams[\"figure.figsize\"] = (30,6)\nplt.rcParams['lines.linewidth'] = 1\nplt.rcParams['lines.color'] = 'b'\nplt.rcParams['axes.grid'] = True ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Inputs\ndef get_train_inputs():\n    x = tf.constant(X_train)\n    y = tf.constant(y_train)\n    return x, y\n\n# Test Inputs\ndef get_test_inputs():\n    x = tf.constant(X_test)\n    y = tf.constant(y_test)\n    return x, y\n\n# Eval data\ndef get_eval_data():\n    return tf.constant(X_test)\n\n# Plot matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.figure()\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        #cm[i, j] = 0 if np.isnan(cm[i, j]) else cm[i, j]\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variables\n\npath = '/kaggle/input/mitbit-arrhythmia-database/mitbih_database/'\nwindow_size = 160\nmaximum_counting = 10000\n\nclasses = ['N', 'L', 'R', 'A', 'V', '/']\nn_classes = len(classes)\ncount_classes = [0]*n_classes\n\nX = list()\ny = list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read files\nfilenames = next(os.walk(path))[2]\n\n# Split and save .csv , .txt \nrecords = list()\nannotations = list()\nfilenames.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# segrefating filenames and annotations\nfor f in filenames:\n    filename, file_extension = os.path.splitext(f)\n    \n    # *.csv\n    if(file_extension == '.csv'):\n        records.append(path + filename + file_extension)\n\n    # *.txt\n    else:\n        annotations.append(path + filename + file_extension)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Records\nfor r in range(0,len(records)):\n# for r in range(2, 3):\n    signals = []\n\n    with open(records[r], 'rt') as csvfile:\n        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|') # read CSV file\\\n        row_index = -1\n        for row in spamreader:\n            if(row_index >= 0):\n                signals.insert(row_index, int(row[1]))\n            row_index += 1\n            \n    if r is 1:\n        # Plot each patient's signal\n        plt.title(records[1] + \" Wave\")\n        plt.plot(signals)\n        plt.show()\n\n    # Read anotations: R position and Arrhythmia class\n    with open(annotations[r], 'r') as fileID:\n        data = fileID.readlines() \n        beat = list()\n\n        for d in range(1, len(data)): # 0 index is Chart Head\n            splitted = data[d].split(' ')\n            splitted = filter(None, splitted)\n            next(splitted) # Time... Clipping\n            pos = int(next(splitted)) # Sample ID\n            arrhythmia_type = next(splitted) # Type\n            if(arrhythmia_type in classes):\n                arrhythmia_index = classes.index(arrhythmia_type)\n                if count_classes[arrhythmia_index] > maximum_counting: # avoid overfitting\n                    pass\n                else:\n                    count_classes[arrhythmia_index] += 1\n                    if(window_size < pos and pos < (len(signals) - window_size)):\n                        beat = signals[pos-window_size+1:pos+window_size]\n                        X.append(beat)\n                        y.append(arrhythmia_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data shape\n# print(np.shape(X), np.shape(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting beat\nplt.plot(X[0])\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(X)):\n        X[i].append(y[i])\n\nprint(np.shape(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# representation of classes % wise\nX_train_df = pd.DataFrame(X)\nper_class = X_train_df[X_train_df.shape[1]-1].value_counts()\nprint(per_class)\nplt.figure(figsize=(20,10))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(per_class, labels=['N', 'L', 'R', 'A', 'V', '/'], colors=['tab:blue','tab:orange','tab:purple','tab:olive','tab:green','tab:red'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_percentage()\nfrom sklearn.utils import resample\n\ndf_1=X_train_df[X_train_df[319]==1]\ndf_2=X_train_df[X_train_df[319]==2]\ndf_3=X_train_df[X_train_df[319]==3]\ndf_4=X_train_df[X_train_df[319]==4]\ndf_5=X_train_df[X_train_df[319]==5]\ndf_0=(X_train_df[X_train_df[319]==0]).sample(n=5000,random_state=42)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=5000,random_state=122)\ndf_2_upsample=resample(df_2,replace=True,n_samples=5000,random_state=123)\ndf_3_upsample=resample(df_3,replace=True,n_samples=5000,random_state=124)\ndf_4_upsample=resample(df_4,replace=True,n_samples=5000,random_state=125)\ndf_5_upsample=resample(df_5,replace=True,n_samples=5000,random_state=126)\n\nX_train_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample,df_5_upsample])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"per_class = X_train_df[319].value_counts()\nprint(per_class)\nplt.figure(figsize=(20,10))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(per_class, labels=['N', 'L', 'R', 'A', 'V', '/'], colors=['tab:blue','tab:orange','tab:purple','tab:olive','tab:green','tab:red'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Test Split\ntrain, test = train_test_split(X_train_df, test_size=0.20)\n\nprint(\"X_train : \", len(train))\nprint(\"X_test  : \", len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_train=train[train.shape[1]-1]\ntarget_test=test[test.shape[1]-1]\ny_train=to_categorical(target_train)\ny_test=to_categorical(target_test)\nprint(np.shape(y_train), np.shape(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.iloc[:,:train.shape[1]-1].values\nX_test = test.iloc[:,:test.shape[1]-1].values\nX_train = X_train.reshape(len(X_train), X_train.shape[1],1)\nX_test = X_test.reshape(len(X_test), X_test.shape[1],1)\nprint(np.shape(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, concatenate, MaxPool2D, ELU\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling2D\n# from keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n# from keras import optimizers, losses, activations, models\n\n# model = Sequential()\n# model.add(Conv2D(64, (3,3),strides = (1,1), input_shape = IMAGE_SIZE + [3],kernel_initializer='glorot_uniform'))\n\n\n# model.add(keras.layers.ELU())\n\n# model.add(BatchNormalization())\n\n# model.add(Conv2D(64, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\n# model.add(keras.layers.ELU())\n\n# model.add(BatchNormalization())\n\n# model.add(MaxPool2D(pool_size=(2, 2), strides= (2,2)))\n\n# model.add(Conv2D(128, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\n# model.add(keras.layers.ELU())\n\n# model.add(BatchNormalization())\n\n# model.add(Conv2D(128, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\n# model.add(keras.layers.ELU())\n\n# model.add(BatchNormalization())\n\n# model.add(MaxPool2D(pool_size=(2, 2), strides= (2,2)))\n\n# model.add(Conv2D(256, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\n# model.add(keras.layers.ELU())\n\n# model.add(BatchNormalization())\n\n# model.add(Conv2D(256, (3,3),strides = (1,1),kernel_initializer='glorot_uniform'))\n\n# model.add(keras.layers.ELU())\n\n# model.add(BatchNormalization())\n\n# model.add(MaxPool2D(pool_size=(2, 2), strides= (2,2)))\n\n# model.add(Flatten())\n\n# model.add(Dense(2048))\n\n\n# model.add(keras.layers.ELU())\n\n# model.add(BatchNormalization())\n\n# model.add(Dropout(0.5))\n\n# model.add(Dense(7, activation='softmax'))\n\n# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# print(model.summary())\n\n# gen = ImageDataGenerator()\n\n# test_gen = gen.flow_from_directory(valid_path, target_size=IMAGE_SIZE)\n\n# train_gen = gen.flow_from_directory(train_path, target_size=IMAGE_SIZE)\n\n# train_generator = gen.flow_from_directory(\n#   train_path,\n#   target_size=IMAGE_SIZE,\n#   shuffle=True,\n#   batch_size=batch_size,\n# )\n# valid_generator = gen.flow_from_directory(\n#   valid_path,\n#   target_size=IMAGE_SIZE,\n#   shuffle=True,\n#   batch_size=batch_size,\n# )\n# callbacks_list = [checkpoint]\n\n# r = model.fit_generator(\n#   train_generator,\n#   validation_data=valid_generator,\n#   epochs=50,\n#   steps_per_epoch=356702//batch_size,\n#   validation_steps=39634//batch_size,callbacks=callbacks_list\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Network\ndef network(X_train,y_train,X_test,y_test):\n    im_shape=(X_train.shape[1],1)\n#     model = Sequential()\n#     model.add(Conv1D(64, (6),strides = (1), input_shape = (im_shape),kernel_initializer='glorot_uniform'))\n#     model.add(keras.layers.ELU())\n#     model.add(BatchNormalization())\n#     model.add(Conv1D(64, (3),strides = (1),kernel_initializer='glorot_uniform'))\n#     model.add(keras.layers.ELU())\n#     model.add(BatchNormalization())\n#     model.add(MaxPool2D(pool_size=(2), strides= (2)))\n#     model.add(Conv1D(128, (3),strides = (1),kernel_initializer='glorot_uniform'))\n#     model.add(keras.layers.ELU())\n#     model.add(BatchNormalization())\n#     model.add(Conv1D(128, (3),strides = (1),kernel_initializer='glorot_uniform'))\n#     model.add(keras.layers.ELU())\n#     model.add(BatchNormalization())\n#     model.add(MaxPool2D(pool_size=(2), strides= (2)))\n#     model.add(Conv1D(256, (3),strides = (1),kernel_initializer='glorot_uniform'))\n#     model.add(keras.layers.ELU())\n#     model.add(BatchNormalization())\n#     model.add(Conv1D(256, (3),strides = (1),kernel_initializer='glorot_uniform'))\n#     model.add(keras.layers.ELU())\n#     model.add(BatchNormalization())\n#     model.add(MaxPool2D(pool_size=(2), strides= (2)))\n#     model.add(Flatten())\n#     model.add(Dense(2048))\n#     model.add(keras.layers.ELU())\n#     model.add(BatchNormalization())\n#     model.add(Dropout(0.5))\n#     model.add(Dense(6, activation='softmax'))\n#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    \n    \n    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n    # convolutional layer 1\n    conv1_1=Convolution1D(64, (6), activation='elu', input_shape=im_shape)(inputs_cnn)\n    conv1_1=BatchNormalization()(conv1_1)\n    #pooling layer 1\n    pool1=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n    # convolutional layer 2\n    conv2_1=Convolution1D(128, (3), activation='elu', input_shape=im_shape)(pool1)\n    conv2_1=BatchNormalization()(conv2_1)\n    # convolutional layer 3\n    conv3_1=Convolution1D(128, (3), activation='elu', input_shape=im_shape)(conv2_1)\n    conv3_1=BatchNormalization()(conv3_1)\n    #pooling layer 2\n    pool2=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n    # convolutional layer 4\n    conv4_1=Convolution1D(256, (3), activation='elu', input_shape=im_shape)(pool2)\n    conv4_1=BatchNormalization()(conv4_1)\n    # convolutional layer 5\n    conv5_1=Convolution1D(256, (3), activation='elu', input_shape=im_shape)(conv4_1)\n    conv5_1=BatchNormalization()(conv5_1)\n    #pooling layer 3\n    pool3=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv5_1)\n    # flattened layer 1\n    flatten=Flatten()(pool3)\n    # dense layers\n    dense_end1 = Dense(1024, activation='elu')(flatten)\n#     dense_end2 = Dense(1024, activation='elu')(dense_end1)\n#     dense_end3 = Dense(512, activation='elu')(dense_end2)\n#     dense_end2 = Dense(64, activation='elu')(dense_end1)\n#     dense_end5 = Dense(128, activation='elu')(dense_end4)\n#     dense_end6 = Dense(64, activation='elu')(dense_end5)\n# #     dense_end7 = Dense(32, activation='elu')(dense_end6)\n    #output layers\n    main_output = Dense(6, activation='softmax', name='main_output')(dense_end1)\n    \n    \n    model = Model(inputs= inputs_cnn, outputs=main_output)\n    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n    \n    \n    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\n    history=model.fit(X_train, y_train,epochs=2,callbacks=callbacks, batch_size=32,validation_data=(X_test,y_test))\n    model.load_weights('best_model.h5')\n    return(model,history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(history,X_test,y_test,model):\n    scores = model.evaluate((X_test),y_test, verbose=0)\n    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n    \n    print(history)\n    fig1, ax_acc = plt.subplots()\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model - Accuracy')\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.show()\n    \n    fig2, ax_loss = plt.subplots()\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model- Loss')\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.show()\n    target_names=['0','1','2','3','4','5']\n    \n    y_true=[]\n    for element in y_test:\n        y_true.append(np.argmax(element))\n    prediction_proba=model.predict(X_test)\n    prediction=np.argmax(prediction_proba,axis=1)\n    cnf_matrix = confusion_matrix(y_true, prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nmodel,history=network(X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(history,X_test,y_test,model)\ny_pred=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(10, 10))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'L', 'R', 'A', 'V', '/'],normalize=True,\n                      title='Confusion matrix, with normalization')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}