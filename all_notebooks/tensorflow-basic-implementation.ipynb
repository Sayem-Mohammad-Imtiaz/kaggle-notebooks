{"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"b9855b52-2f9b-49b4-aeb8-0d4355cbae67","_uuid":"3e4255b8cea6af9f27f1d1c203c7c47129434f48"},"source":"This notebook is based on this deeplearning implementation https://www.kaggle.com/maciejsartys/titanic-on-neural-network/data.\nIt is my first notebook for Kaggle,it has a few errors and any feedback and improvements advice is appreciated.","cell_type":"markdown"},{"source":"from subprocess import check_output\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","metadata":{"_cell_guid":"ce1e2b85-6509-478b-abb6-e1fa12e27391","_uuid":"f048e46c3e06e91c95a0c44de5e7bbe55c986af1","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ntrain = pd.read_csv('../input/train.csv')\ntest_challange = pd.read_csv('../input/test.csv')","metadata":{"_cell_guid":"8006f2f2-9c0b-45ab-aad3-4a0c252cb28a","_uuid":"1df2b6999e25c4d0383a494766de19d7e17b434e"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"train.describe()\n","metadata":{"_cell_guid":"534ae811-3c38-4bb2-b268-f16c8e755864","_uuid":"8c22288dae5a306199f173cad3a541966387f479"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"def parseSex(df):\n    df.loc[df['Sex'] == 'male', 'Sex'] = 1\n    df.loc[df['Sex'] == 'female', 'Sex'] = 0\n    return df\ndef addFamilySize(df):\n    df['FamilySize'] = 0\n    df['FamilySize'] = df['Parch'] + df['SibSp']\n    return df\ndef normalize(series):\n    mean = series.mean()\n    stdev= series.std()\n    return (series - mean)/stdev\n\ndef preprocess_data(df):\n    df = parseSex(df)\n    df = addFamilySize(df)\n    age_median = df['Age'].median()\n    df.Age = df.Age.fillna(age_median)\n    df.Age = normalize(df.Age)\n    df.FamilySize = normalize(df.FamilySize)\n    df.Pclass = normalize(df.Pclass)\n    return df\n\ndef accuracy(predictions, labels):\n    predictions=predictions>0.5;\n    return (np.sum(predictions==labels)\n          / predictions.shape[0])","metadata":{"_cell_guid":"ac452077-6215-465a-994e-73bdfd6f5db3","_uuid":"376f2b439950f18f6ebc7103a74323b65d1c4750","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"train = preprocess_data(train)\ntrain, test = train_test_split(train, test_size = 0.2)\npredict_data = preprocess_data(test_challange)\nfeatures_list = ['Pclass', 'Sex', 'Age', 'FamilySize']\ntrain_features = np.float64(train[features_list].values)\ntrain_target = np.float64(train['Survived'].values)\ntrain_target=train_target.reshape(train_target.shape[0],1)\ntest_features = np.float64(test[features_list].values)\ntest_target = np.float64(test.Survived.values)\ntest_target=test_target.reshape(test_target.shape[0],1)\npredict_features=np.float64(predict_data[features_list].values)\n\nprint(train_features.shape)\nprint(train_target.shape)\nprint(test_features.shape)\nprint(test_target.shape)\nprint(predict_features.shape)","metadata":{"_cell_guid":"1fed7483-a846-472e-a91d-c915694fa113","_uuid":"ba74da03ed2cb094f3fe780191e255d2fd61e4ca"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"features_t = train_features.T\ntarget_t = train_target.T.reshape(1, train_target.shape[0])\n\ntest_features_t = test_features.T\ntest_target_t = test_target.T.reshape(1, test_target.shape[0])\n\n\nhidden_nodes_1 = 4\nhidden_nodes_2 = 4\nhidden_nodes_3 = 1","metadata":{"_cell_guid":"ee076724-b90d-4c60-b3d4-a6d0bdfe79db","_uuid":"4a30a370281939e38fca7fd23e522b4b2943933e","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"\nbeta = 0.01\ngraph = tf.Graph()\nwith graph.as_default():   \n    # Input data.\n    # Load the training, validation and test data into constants that are\n    # attached to the graph.\n    tf_train_dataset = tf.constant(train_features)\n    tf_train_labels = tf.constant(train_target)\n    tf_valid_dataset = tf.constant(test_features)\n    tf_test_dataset = tf.constant(test_features)\n    tf_final_dataset = tf.constant(predict_features)\n    weights_1 = tf.Variable(tf.truncated_normal([hidden_nodes_1,hidden_nodes_2], dtype=tf.float64))\n    biases_1 = tf.Variable(tf.zeros([hidden_nodes_2], dtype=tf.float64))\n    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_2,hidden_nodes_3], dtype=tf.float64))\n    biases_2 = tf.Variable(tf.zeros([hidden_nodes_3], dtype=tf.float64))\n    \n    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_3,1], dtype=tf.float64))\n    biases_3 = tf.Variable(tf.zeros([1], dtype=tf.float64))\n    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1 #[712,4]*[4,4]=[712,4]\n    relu_layer_1= tf.nn.relu(logits_1)\n    logits_2 = tf.matmul(relu_layer_1, weights_2) + biases_2 #[712,4]*[4,1]=[712,1]\n    relu_layer_2=tf.nn.relu(logits_2)\n    logits_3=tf.matmul(relu_layer_2,weights_3) + biases_3 #[712,4]*[1,1]=[712,1]\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_3, labels=tf_train_labels))\n    \n    # Loss function with L2 Regularization with beta=0.01\n    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)+ tf.nn.l2_loss(weights_3)\n    loss = tf.reduce_mean(loss + beta * regularizers)\n\n    # Optimizer.\n    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n    # Predictions for the training\n    train_prediction = tf.nn.sigmoid(logits_3)\n    \n    # Predictions for validation \n    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n    relu_layer_1= tf.nn.relu(logits_1)\n    logits_2 = tf.matmul(relu_layer_1, weights_2) + biases_2\n    relu_layer_2=tf.nn.relu(logits_2)\n    logits_3=tf.matmul(relu_layer_2,weights_3) +biases_3\n    \n    valid_prediction = tf.nn.sigmoid(logits_3)\n    \n    test_prediction =  valid_prediction\n    \n     # Predictions for final submision \n    logits_1 = tf.matmul(tf_final_dataset, weights_1) + biases_1\n    relu_layer_1= tf.nn.relu(logits_1)\n    logits_2 = tf.matmul(relu_layer_1, weights_2) + biases_2\n    relu_layer_2=tf.nn.relu(logits_2)\n    logits_3=tf.matmul(relu_layer_2,weights_3) +biases_3\n    \n    final_prediction = tf.nn.sigmoid(logits_3)\n    \n","metadata":{"_cell_guid":"88a4c851-b944-4f0f-8b8b-fcf32556422b","_uuid":"884935e4ad0947b52790ced1e39eb59f068567ae","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"num_steps = 801\nids = predict_data.index.values\nfinal = pd.DataFrame({\n    'PassengerId': ids,\n    'Survived': np.zeros(ids.shape[0])\n})\n\nwith tf.Session(graph=graph) as session:\n    # This is a one-time operation which ensures the parameters get initialized as\n    # we described in the graph: random weights for the matrix, zeros for the\n    # biases. \n    tf.global_variables_initializer().run()\n    print('Initialized')\n    for step in range(num_steps):\n        # Run the computations. We tell .run() that we want to run the optimizer,\n        # and get the loss value and the training predictions returned as numpy\n        # arrays.\n        _, l, predictions = session.run([optimizer, loss, train_prediction])\n        if (step % 100 == 0):\n            print('Loss at step %d: %f' % (step, l))\n            print('Training accuracy: %.1f%%' % accuracy(\n            predictions, train_target))\n            # Calling .eval() on valid_prediction is basically like calling run(), but\n            # just to get that one numpy array. Note that it recomputes all its graph\n            # dependencies.\n            print('Validation accuracy: %.1f%%' % accuracy(\n                train_prediction.eval(), train_target))\n    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_target))\n    final['Survived']=np.int64(final_prediction.eval()>0.5)\nfinal\nfinal.to_csv(\"titanic_predictions.csv\", index=False)","metadata":{"_cell_guid":"db383924-fe09-44e8-aae1-a0b33a40b6be","_uuid":"7e383b1adfcc00f02b4aa8f8d0be842bc8d6b935"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"","metadata":{"_cell_guid":"d18ef7e6-24eb-49af-9779-fdd45e517155","_uuid":"b44bd88232a1ddf9a4c6e2356c70055102158726","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"}],"nbformat_minor":1}