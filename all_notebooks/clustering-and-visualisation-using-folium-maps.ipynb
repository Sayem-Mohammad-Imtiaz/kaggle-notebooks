{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport folium # plotting library\nfrom folium import plugins\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n### Firstly, we will take a quick look into our data in order to understand with what we are working with! And, then, we will clean/filter it! \n\n### Afterwards, we will apply k-Means clustering in order to identify similar airports based on the number of occurences of weather events that had happened in that particular airport! Plus, we are going to use Principal Component Analysis in order to visualise high dimensional data, so that, we can see how our clusters are related in the original space.\n\n### Finally, the final result of clustered airports will be illustrated using the Seaborn library and Folium  Maps\n\n# Data Overview"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/us-weather-events/US_WeatherEvents_2016-2019.csv')\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Severity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data prep for k-Means clustering"},{"metadata":{},"cell_type":"markdown","source":"### Let's filter our data discarding the events that has severity as 'unk' or 'other'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(df['Severity'] != 'UNK') & (df['Severity'] != 'Other')]\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_types = df[['AirportCode','Type']]\n\ndf_types.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here, we are going to group the occurences for each airport!"},{"metadata":{"trusted":true},"cell_type":"code","source":"types = pd.get_dummies(df_types['Type'])\n\ntypes['AirportCode'] = df_types['AirportCode']\n\ntypes = types.groupby('AirportCode').sum().reset_index()\n\ntypes.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# k-Means Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"codes = types[['AirportCode']]\ntypes.drop('AirportCode', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In order to identify the optimal number of clusters, we need to use the Elbow Method! When the slope of the tangent line starts to be almost horizontal, that is the optimal number of cluster!"},{"metadata":{"trusted":true},"cell_type":"code","source":"distortions = []\n\nK = range(1,20)\nfor k in K:\n    kmean = KMeans(n_clusters=k, random_state=0, n_init = 50, max_iter = 500)\n    kmean.fit(types)\n    distortions.append(kmean.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The elbow method seems to suggest 4 or 5 clusters!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# run k-means clustering\nkmeans = KMeans(n_clusters=4, random_state=0).fit(types)\n\ncodes['cluster'] = kmeans.labels_\ncodes.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I am used to apply some dimensionality reduction techniques in order to visualise how our clusters are related in the original high dimensional space! Moreover, we are able to see if the features of our data are linear related among them."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA().fit(types)\npca_types = pca.transform(types)\nprint(\"Variance explained by each component (%): \")\nfor i in range(len(pca.explained_variance_ratio_)):\n      print(\"\\n\",i+1,\"ยบ:\", pca.explained_variance_ratio_[i]*100)\nprint(\"Total sum (%): \",sum(pca.explained_variance_ratio_)*100)\nprint(\"Explained variance of the first two components (%): \",sum(pca.explained_variance_ratio_[0:1])*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since the number of samples are larger than the number of features, we are able to solve all 5 principal components (PC), leading to 100% of the original information being explained by these PC. \n\n### We can see that using the first two components we are able to preserve 63,65% of the original information, therefore, reducing the dimensionality of our data.\n\n### Let's use these PC to visualise our clusters!"},{"metadata":{"trusted":true},"cell_type":"code","source":"c0 = []\nc1 = []\nc2 = []\nc3 = []\n\nfor i in range(len(pca_types)):\n    if kmeans.labels_[i] == 0:\n        c0.append(pca_types[i])\n    if kmeans.labels_[i] == 1:\n        c1.append(pca_types[i])\n    if kmeans.labels_[i] == 2:\n        c2.append(pca_types[i])\n    if kmeans.labels_[i] == 3:\n        c3.append(pca_types[i])\n        \n        \nc0 = np.array(c0)\nc1 = np.array(c1)\nc2 = np.array(c2)\nc3 = np.array(c3)\n\nplt.figure(figsize=(7,7))\nplt.scatter(c0[:,0], c0[:,1], c='red', label='Cluster 0')\nplt.scatter(c1[:,0], c1[:,1], c='blue', label='Cluster 1')\nplt.scatter(c2[:,0], c2[:,1], c='green', label='Cluster 2')\nplt.scatter(c3[:,0], c3[:,1], c='black', label='Cluster 3')\nplt.legend()\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Low dimensional visualization (PCA) - Airports');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see that 4 clusters seems to be reasonable to identify similar samples within our data!\n\n### Let's take a look in the particularity of each cluster using seaborn library!"},{"metadata":{"trusted":true},"cell_type":"code","source":"types['cluster']  = kmeans.labels_\n\ntypes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types.groupby('cluster').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cluster', y='Cold', data=types, kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cluster', y='Fog', data=types, kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cluster', y='Rain', data=types, kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cluster', y='Snow', data=types, kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cluster', y='Storm', data=types, kind='bar');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking into these plots we can see that cluster 0 is the most affected by snow and cold! And cluster 3 is the most affected by rains!"},{"metadata":{},"cell_type":"markdown","source":"# Folium Maps Visualisation by Number of Occurences and Clustering"},{"metadata":{},"cell_type":"markdown","source":"### Firstly, we need to create a map of USA\n\n### We are going to plot two maps: the first one will display airports by their number of weather events that occured in that airport! The size of each mark (of each airport) will vary accordingly to these numbers. The second map will show us the clusters that we had acquired through k-Means!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"latitude = 38.500000\nlongitude = -95.665\n\nmap_USA = folium.Map(location=[latitude, longitude], zoom_start=4)\n\nmap_USA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airports = df[['AirportCode', 'LocationLat','LocationLng','City','State']]\n\nairports.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_occurences = pd.DataFrame(airports['AirportCode'].value_counts())\nnumber_of_occurences.reset_index(inplace=True)\nnumber_of_occurences.columns = ['AirportCode', 'Count']\nnumber_of_occurences.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_occurences = number_of_occurences.merge(airports.drop_duplicates())\n\nnumber_of_occurences = number_of_occurences.merge(codes)\n\nnumber_of_occurences.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"occurences = folium.map.FeatureGroup()\nn_mean = number_of_occurences['Count'].mean()\n\nfor lat, lng, number, city, state in zip(number_of_occurences['LocationLat'],\n                                         number_of_occurences['LocationLng'],\n                                         number_of_occurences['Count'],\n                                         number_of_occurences['City'],\n                                         number_of_occurences['State'],):\n    occurences.add_child(\n        folium.vector_layers.CircleMarker(\n            [lat, lng],\n            radius=number/n_mean*5, # define how big you want the circle markers to be\n            color='yellow',\n            fill=True,\n            fill_color='blue',\n            fill_opacity=0.6,\n            tooltip = str(number)+','+str(city) +','+ str(state)\n        )\n    )\n\nmap_USA.add_child(occurences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that the airports that had registered the greatest number of occurences are in the north of the West Coast!\n\n### But, in general, the airports that are located far away from the coast had suffered less from weather events! However, the state of Colorado seems to be a exception to that :)"},{"metadata":{},"cell_type":"markdown","source":"### Finally, let's see our clusters!"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"map_clusters = folium.Map(location=[latitude, longitude], zoom_start=4)\n\n# set color scheme for the clusters\nx = np.arange(4)\nys = [i + x + (i*x)**2 for i in range(4)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lng, cluster, city, state in zip(number_of_occurences['LocationLat'], number_of_occurences['LocationLng'],  \n                                            number_of_occurences['cluster'],\n                                         number_of_occurences['City'],\n                                         number_of_occurences['State']):\n    #label = folium.Popup(str(city)+ ','+str(state) + '- Cluster ' + str(cluster), parse_html=True)\n    folium.vector_layers.CircleMarker(\n        [lat, lng],\n        radius=5,\n        #popup=label,\n        tooltip = str(city)+ ','+str(state) + '- Cluster ' + str(cluster),\n        color=rainbow[cluster-1],\n        fill=True,\n        fill_color=rainbow[cluster-1],\n        fill_opacity=0.9).add_to(map_clusters)\n       \nmap_clusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that cluster 0, the one most affected by snow events, are mainly located to the north border of USA, very close to Canada!"},{"metadata":{},"cell_type":"markdown","source":"### And here I conclude this notebook suggesting some future work! Maybe the use of 5 clusters may reveal to us a better look into our data. It seems to me that cluster 1 are not well defined! Furthermore, a better investigation of each cluster may show us other differences among them!\n\n### Thank you,\n### Lucas"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}