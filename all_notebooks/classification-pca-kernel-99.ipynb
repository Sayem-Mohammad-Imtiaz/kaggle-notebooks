{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.decomposition import PCA,KernelPCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nimport warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c79e0974ae9048bf8412a74583108d92a433741"},"cell_type":"code","source":"# Importing the dataset\ndata = pd.read_csv('../input/winequality-red.csv')\ndata.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09451941359b0d0639d9f8ae0a2c5ac5318a70a2"},"cell_type":"markdown","source":"#### for improving model performance we need to split all wines types in three category good,fine and bad"},{"metadata":{"trusted":true,"_uuid":"0dd5f67c88157ff2f6fdaebe7fe4bc4ca213fe25"},"cell_type":"code","source":"reviews = []\nfor i in data['quality']:\n    if i >= 1 and i <= 3:\n        reviews.append('1')\n    elif i >= 4 and i <= 7:\n        reviews.append('2')\n    elif i >= 8 and i <= 10:\n        reviews.append('3')\ndata['Reviews'] = reviews\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d683d2dd34bddbf6760584d05169caba112be8a"},"cell_type":"markdown","source":"### Splitting Data \nx = independent values\ny = depended values review is the only one depended yea it's review\n**notice:\nyou can select last index into coulmn index by : -1**"},{"metadata":{"trusted":true,"_uuid":"abebb4d7b85ce055bdfe1783a83c779113439574"},"cell_type":"code","source":"x = data.iloc[:, 0:-2].values\ny = data.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2324a36f25c11654eb3e8c1944db61cf1ff6741"},"cell_type":"markdown","source":"### Creatting the Model\n here we create pip line with Standard scaler step(1) and KernelPCA step(2) and logistic regression model step(3) we will change it later any way"},{"metadata":{"_uuid":"541c8e5292e7f271c115d1b1d633574249f78fd9"},"cell_type":"markdown","source":" ### A-Dimensionality reduction\nyou can read more about it here [Principal component](https://en.wikipedia.org/wiki/Principal_component_analysis) , but simply you can reduce your Dimensions easly , the    dimensions is your columns count and this process try to reduce it with out effect with your data.\nPCA , PCA KERNEL and plotting explained_variance to see the best components number for setting !"},{"metadata":{"trusted":true,"_uuid":"aec61f7f140682072adb32124b7a3150d3f31b4d"},"cell_type":"code","source":"sc_x=StandardScaler()\nx = sc_x.fit_transform(x)\n\npca=PCA()\nx_pca = pca.fit_transform(x)\nplt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5def8619767bd832762a2c605530392f08282b13"},"cell_type":"code","source":"pca_new = PCA(n_components=8)\nx_new = pca_new.fit_transform(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0de6ed0c260c447a798c463c2b46c08d00c0563"},"cell_type":"code","source":"\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d37ed921d571a9bccf12fcaed938959ac9818461"},"cell_type":"code","source":"def classifier(model):\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    score=accuracy_score(y_pred,y_test)\n    return score*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29a238c7ad2c52775488e355ce5162a982772346"},"cell_type":"code","source":"classifier(KNeighborsClassifier(n_neighbors=100)),classifier(RandomForestClassifier(n_estimators=100)),classifier(LogisticRegression()),classifier(GaussianNB())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f3c4601477e172a21faa99234b21707c2b53f8e"},"cell_type":"markdown","source":"Like we see the best for this Case is Random Forest classifier  like always  with estimators count = 100 it's great to see that 99,375% can you do more than that  with the same case lets see what you can do then ?\n\n**Notice: you can get a better result when you Learn how to tunning hyperparametric , Comment if there is thing i miss , and try to get more than that result :\"D**"},{"metadata":{"trusted":true,"_uuid":"b08e6db14ab295f140606c9dacb1cc660149bca5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}