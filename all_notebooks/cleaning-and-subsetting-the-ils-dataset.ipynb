{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction:\n\nOur Dataset is about 3.6GB on disk, and \"wc -l datafile.csv\" tells us that we have 37 768 482 rows. Each line in the file takes up about 3 lines, so we have about 12589494 potential rows to access, in the dataset. \n\nI had problems trying to load this dataset with the Dask framework. Trying to load the entire dataset using pandas on a small laptop was also painful. I make use of the 16Gb of RAM a Kaggle Kernel provides.\n\n**Note:** This script is not currently optimized for efficiency. It takes about ~300s to run. \n\n## Goal: \n\nTidy the dataset, and create a smaller dataframe with redundant information stored in separate files. Users can join this information if they need to. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Imports:\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nimport matplotlib.pyplot as plt\nimport re\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Settings:\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nalt.data_transformers.enable('default', max_rows=None)\n%matplotlib inline \npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 40)\npd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Support functions:\n\n#for each string, and a compiled pattern object, use the findall method.\n#return true if we find ONE SINGLE 5 digit zipcode. False otherwise. \ndef regapp(x,pattObj):\n    hold = pattObj.findall(x)\n    result = False\n    if (len(hold) == 1):\n        result = True\n    return result\n\ndef cleanup(cell):\n    return float(cell.replace(\"$\",\"\"))\n\ndef cutgps(cell,myregex):\n    store = re.findall(myregex,cell)\n    if (store): #empty\n         retVal = store[0]\n    else:\n         retVal = \"NA\" #Some of the entries are missing their lat/long data\n    return retVal\n\n#first, lets extract the GPS coordinates:\n#What can we assume? each column is a non-empty string, at least.\ndef getlatlong(cell, pos):\n    if (cell == \"NA\"):\n        return cell\n    sectionList = cell.replace(\"(\",\"\").replace(\")\",\"\").split(\",\")\n    return float(sectionList[pos]) #0 or 1. Float will clip out spaces for us!\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dfILS = pd.read_csv(\"../input/Iowa_Liquor_Sales.csv\")\ndfILS.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Tidying:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will use this to calculate the percentage of rows lost after cleaning.\nlossDict = {}\nlossDict['fullsize'] = dfILS.shape[0] \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets get some basic information about our data frame. We can already see some columns are upcast (float instead of ints)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking out nulls:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS.isnull().sum() #our number of nas.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We should be able to drop NAs, and still have over 12M rows to choose from.\n#This loss is acceptable.\ndfILS.dropna(inplace=True)\nlossDict['nullloss'] = (lossDict['fullsize']- dfILS.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Column Names need to be tidied up."},{"metadata":{"trusted":true},"cell_type":"code","source":"nameDict = {\"Invoice/Item Number\":\"invoicenumber\"\n,\"Date\":\"date\"\n,\"Store Number\":\"storenumber\"\n,\"Store Name\":\"storename\"\n,\"Address\":\"address\"            \n,\"City\":\"city\"\n,\"Zip Code\":\"zipcode\"\n,\"Store Location\":\"storelocation\"\n,\"County Number\":\"countynumber\"\n,\"County\":\"countyname\"\n,\"Category\":\"categorynumber\"\n,\"Category Name\":\"categoryname\"\n,\"Vendor Name\":\"vendorname\"\n,\"Vendor Number\":\"vendornumber\"\n,\"Item Number\":\"itemnumber\"\n,\"Item Description\":\"itemdescription\"\n,\"Pack\":\"pack\"\n,\"Bottle Volume (ml)\":\"bottlevolumeml\"\n,\"State Bottle Cost\":\"statebottlecost\"\n,\"State Bottle Retail\":\"statebottleretail\"\n,\"Bottles Sold\":\"bottlessold\"\n,\"Sale (Dollars)\":\"saleprice\"\n,\"Volume Sold (Liters)\":\"volumesoldlitre\"\n,\"Volume Sold (Gallons)\":\"volumesoldgallon\"}\n\ndfILS.rename(columns = nameDict,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas has a tendency to upcast a lot of the columns. We need to make the datatypes more specific (example: float64 -> int32). \ncountynumber, category, vendornumber, and zipcode are cast as floats or string objects. Observe below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The following are easy to correct. #the ints are not that big, so we can use int32 instead of 64 to save space.\nconvertList = [\"countynumber\",\"vendornumber\",\"storenumber\",\"categorynumber\",\"pack\",\"bottlevolumeml\",\"bottlessold\",\n               \"volumesoldlitre\",\"volumesoldgallon\",\"itemnumber\"]\n\nfor item in convertList:\n    dfILS = dfILS.astype({item: \"int32\"},inplace=True)    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with Zipcodes:\n\nZipcode is classed as a string object type, because of zipcode anomolies in the data. There are zipcodes of the form \"752-6\". Casting occured because of non-numeric characters. We need to cut out rows that don't conform to a 5 digit code, before casting to int."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS['zipcode'].unique() #if you want to look for yourself.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There appear to be *non numeric characthers (\"712-2\"), floats, strings and ints all mixed together*. Yikes. Upcast to strings,\nfilter anomolies, and then convert whats left to ints."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS['zipcode'] = dfILS['zipcode'].apply(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattObj = re.compile(r\"[0-9]{5}\")\nboolSelect = dfILS['zipcode'].apply(regapp,args=(pattObj,))\nlossDict['ziploss'] = boolSelect.value_counts().loc[False]\ndfILS = dfILS[boolSelect]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to step down type incrementally. string '7723632.0' throws a ValueError."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS['zipcode'] = dfILS['zipcode'].apply(float)\ndfILS['zipcode'] = dfILS['zipcode'].apply(int) \ndfILS = dfILS.astype({\"zipcode\": \"int32\"}, inplace=True) \n#done!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with Sales Columns:"},{"metadata":{},"cell_type":"markdown","source":"Next, we need to clean up the sales columns. They are strings because the dollar sign symbol was included in the spreadsheet. Again,\nthe numbers in sales aren't that large, so lets use a float32 instead of a float64 to save some space. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for columnname in [\"statebottlecost\",\"statebottleretail\",\"saleprice\"]:\n    dfILS[columnname] = dfILS[columnname].apply(cleanup)\n    dfILS.astype({columnname:\"float32\"},inplace=True)\n    \n#dfILS.info() #check to see that the three cols are now floats.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating GPS coordinates from the Store Location Column:\n\n#### Note: This code was unused for the final dataframe (I didn't need it). Uncomment code below to use GPS coordinates.\n\nNext we will split the Store Location Column. There appears to be zipcode and GPS information encoded in these columns. Individual addresses for a store don't matter, as this dataset needs to be aggregated at a larger geographical area to do reasonable modelling. The storelocaiton column will be split (and dropped). We will add a latitude and longitude column, instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"#First, mutate the store location column; replace it with just the GPS substring.\n#myregex = r\"(\\(.+,.+\\))\"\n#dfILS['storelocation'] = dfILS['storelocation'].apply(cutgps,args=(myregex,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#suppressed as I don't need this for tableau!\n#dfILS['latitude'] = dfILS['storelocation'].apply(getlatlong,args=(0,))\n#dfILS['longitude'] = dfILS['storelocation'].apply(getlatlong,args=(1,))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS.drop(columns=[\"storelocation\"], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, our GPS conversion has introduced some NAs - as not every storelocation had GPS coordinates. Lets check string NAs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#boolSelect2 = dfILS['latitude'] == \"NA\"\n#boolSelect2.value_counts() #our number of nas.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"931655 NAs is roughly 10 percent of our data. Should they be dumped? I choose not to. I'll deal with \"NA\"s further down the pipeline. "},{"metadata":{},"cell_type":"markdown","source":"### Summary and Check of Data Tidying:\n\nWe have saved some memory by casting the columns to smaller types."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rowsum = lossDict['nullloss'] + lossDict['ziploss']\nprint(\"Rows Lost: \" + str(rowsum) + \"\\n Percentage Loss: \" + str(rowsum/lossDict['fullsize']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Subsetting and Saving our Data:"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now that our data is tidy, we can subset and save it to .csv files. There are some examples below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfILS.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Putting Redundant Information into Separate Tables:\n\nWe can shrink this data frame significantly, by putting coupled elements in a separate table, that can be joined by the user\nat a later date. In particular:\n\n- store number and store name\n- county number and county name\n- category and category name\n- vendor number and vendor name\n- item number and item description\n\nWe can just store the integer number column, and store a reduced table of unique values in a separate file. Via a left join, we can \nreconstruct the data if needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"def uniquefilewrite(colA,colB,filename):\n    storage = dfILS.drop_duplicates(subset=[colA,colB],keep=\"first\",inplace=False)\n    storage.loc[:,[colA,colB]].to_csv(filename + \".csv\",index=False)\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colA = [\"storenumber\",\"countynumber\",\"categorynumber\",\"vendornumber\",\"itemnumber\"]\ncolB = [\"storename\",\"countyname\",\"categoryname\",\"vendorname\",\"itemdescription\"]\n\nfor tup in list(zip(colA,colB)):\n    uniquefilewrite(tup[0],tup[1],tup[1])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Writing out our dataframe\n\nThe data is now tidy and free of redundant data."},{"metadata":{},"cell_type":"markdown","source":"![](http://)Kaggle Session information indicates our data takes up < 1.4GB of space. Much better!"},{"metadata":{"trusted":true},"cell_type":"code","source":"colList = [\"invoicenumber\",\"date\",\"city\", \"zipcode\", \"storenumber\",\"countynumber\",\n           \"categorynumber\", \"vendornumber\",\"itemnumber\",\"bottlevolumeml\",\"statebottlecost\",\n            \"statebottleretail\", \"bottlessold\", \"saleprice\" ,\"volumesoldlitre\"]\n\ndfILS.loc[:,colList].to_csv(\"ILS_clean.csv\",index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}