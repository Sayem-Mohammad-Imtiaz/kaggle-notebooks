{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Loading Libraries<a id=\"1\"></a> <br>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\ninit_notebook_mode(connected=True)  \nplt.style.use('ggplot')\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom PIL import Image\nimport urllib.request\nimport random\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing <a id=\"2\"></a> <br>"},{"metadata":{},"cell_type":"markdown","source":"Loading the dataset and gathering a glimpse:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/new-york-city-current-job-postings/nyc-jobs.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Columns Description:"},{"metadata":{},"cell_type":"markdown","source":"- **Job ID**: The Unique Job ID for each opening\n- **Posting Type**: The opening type, whether internal or external, for the job.\n- **# of Positions**: The number of positions available for a certain opening\n- **Business Title**: The position the candidate would hold.\n- **Civil Service Title**: The Broad Title the position would be classified under\n- **Title Code No**: The Code for a particular title\n- **Level**: The authority the certain opening would bring with it\n- **Job Category**: Broad Classification of where all the jobs would fall in\n- **Full-time/Part-Time**: Time frame of a job.\n- **Salary Range From**: The beginning salary cap for that particular opening\n- **Salary Range To**: The highest cap for that particular job opening.\n- **Salary Frequency**: The payment factor for the job, hourly or annual\n- **Work Location**: The location of the workplace\n- **Division/Work Unit**: Broad working units for all the jobs \n- **Job Description**: A brief idea of what the job will contain\n- **Minimum Qual Requirements**: The minimum qualifications a candidate must possess for the job\n- **Preferred Skills**: Optimal skills which the posting is looking for\n- **Additional Information**: Any additional information provided with the job opening\n- **Hours/Shift**: The timings for the job\n- **Work Location 1**: Additional information for the work location\n- **Recruitment Contact**: Empty field, supposed to contain numbers\n- **Residency Requirement**: Whether the employee must be a resident of NYC.\n- **Posting date**: When the opening was announced.\n- **Post Until**: The closing date.\n- **Posting Updated**: The time when the posting was updated for the opening.\n- **Process Date**: When the posting process was completed\n\nPhew! That was a lot of columns, well then, let's get to exploring them! "},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n   \n    # Total missing values\n    mis_val = df.isnull().sum()\n    \n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n    \n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    \n    # Rename the columns\n    mis_val_table_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    \n    # Sort the table by percentage of missing descending\n    # .iloc[:, 1]!= 0: filter on missing missing values not equal to zero\n    mis_val_table_columns = mis_val_table_columns[\n        mis_val_table_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(2)  # round(2), keep 2 digits\n    \n    # Print some summary information\n    print(\"Dataset has {} columns.\".format(df.shape[1]) + '\\n' + \n    \"There are {} columns that have missing values.\".format(mis_val_table_columns.shape[0]))\n    \n    # Return the dataframe with missing information\n    return mis_val_table_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Recruitment Contact', 'Hours/Shift', 'Post Until', 'Work Location 1'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see from the above step that Recruitment Contact, Hours/Shift, Post Until, Work Location 1has more than 50% null values, so it's abvious to drop these columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Additional Information'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even 'Additional Information' is not relevant to our requirement, so it has to be removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in ['Job Category','Residency Requirement','Posting Date', 'Posting Updated','Process Date', 'To Apply']:\n    df[column] = df[column].fillna(df[column].mode()[0]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing null values of few variables which has less than 0.1% of null values with mode of respective features"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis<a id=\"3\"></a> <br>\n"},{"metadata":{},"cell_type":"markdown","source":"### Highest High Salary Range <a id=\"9\"></a> <br>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"\nhigh_sal_range = (df.groupby('Civil Service Title')['Salary Range To'].mean().nlargest(10)).reset_index()\n\nfig = px.bar(high_sal_range, y=\"Civil Service Title\", x=\"Salary Range To\", orientation='h', title = \"Highest High Salary Range\",color=  \"Salary Range To\", color_continuous_scale= px.colors.qualitative.G10).update_yaxes(categoryorder=\"total ascending\")\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh. It seems that **Senior General Deputy Manager**, in general, has the highest avergae salary range, ranging upto $230,000 per year!\nNow that's an impressive amount. \n\nMost of the openigns in the top ten highest salary seem to be from executive fields, or higher posts. These are the fields which rake in most of the money, on average, paving way for the high salaries people seem to hear about!"},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_categories = df['Job Category'].value_counts()[:5]\npopular_categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 10 Job Openings via Category <a id=\"15\"></a> <br>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"job_categorydf = df['Job Category'].value_counts(sort=True, ascending=False)[:10].rename_axis('Job Category').reset_index(name='Counts')\njob_categorydf = job_categorydf.sort_values('Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace = go.Scatter(y = job_categorydf['Job Category'],x = job_categorydf['Counts'],mode='markers',\n                   marker=dict(size= job_categorydf['Counts'].values/2,\n                               color = job_categorydf['Counts'].values,\n                               colorscale='Viridis',\n                               showscale=True,\n                               colorbar = dict(title = 'Opening Counts')),\n                   text = job_categorydf['Counts'].values)\n\ndata = [(trace)]\n\nlayout= go.Layout(autosize= False, width = 1000, height = 750,\n                  title= 'Top 10 Job Openings Count',\n                  hovermode= 'closest',\n                  xaxis=dict(showgrid=False,zeroline=False,\n                             showline=False),\n                  yaxis=dict(title= 'Job Openings Count',ticklen= 2,\n                             gridwidth= 5,showgrid=False,\n                             zeroline=True,showline=False),\n                  showlegend= False)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = df._get_numeric_data().columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = list(set(df.columns) - set(num_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"today = pd.datetime.today()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"redudant_cols = ['Job ID', '# Of Positions','Posting Updated','Minimum Qual Requirements','To Apply','Business Title','Level']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[cat_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the business problem given in the problem statement, it can be said that personal information(Posting date,process date,resident details) will be of no use for our employee segregeration"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(redudant_cols,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning and Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_categories(x):\n    l = x.replace('&', ',').split(',')\n    l = [x.strip().rstrip(',') for x in l]\n    key_categories.extend(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_keywords(x, l):\n    x = x.lower()\n    tokens = nltk.word_tokenize(x)\n    stop_words = set(stopwords.words('english'))\n    token_l = [w for w in tokens if not w in stop_words and w.isalpha()]\n    l.extend(token_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preferred_skills(x):\n    kwl = []\n    df[df['Job Category'] == x]['Preferred Skills'].dropna().apply(parse_keywords, l=kwl)\n    kwl = pd.Series(kwl)\n    return kwl.value_counts()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_categories = []\ndf['Job Category'].dropna().apply(parse_categories)\nkey_categories = pd.Series(key_categories)\nkey_categories = key_categories[key_categories!='']\npopular_categories = key_categories.value_counts().iloc[:25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cat'] = key_categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.countplot(y=key_categories, order=popular_categories.index, palette='YlGn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsalary_table = df[['Civil Service Title', 'Salary Range From', 'Salary Range To']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs_highest_high_range = pd.DataFrame(salary_table.groupby(['Civil Service Title'])['Salary Range To'].mean().nlargest(10)).reset_index()\nplt.figure(figsize=(8,6))\nsns.barplot(y='Civil Service Title', x='Salary Range To', data=jobs_highest_high_range, palette='Greys')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud(text):\n    wordcloud = WordCloud(background_color='white',\n                     width=1024, height=720).generate(text)\n    plt.clf()\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_description_keywords = []\ndf['Job Description'].apply(parse_keywords, l=job_description_keywords)\nplt.figure(figsize=(10, 8))\ncounter = Counter(job_description_keywords)\ncommon = [x[0] for x in counter.most_common(40)]\nplot_wordcloud(' '.join(common))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above wordcloud, it can be seen that work, city, project, water, new are most frequently used words in the Job description, whereas staff system,management, planning, design, support e.t.c are required skills which are demanded mostly by the employer"},{"metadata":{"trusted":true},"cell_type":"code","source":"words = []\ncounts = []\nfor letter, count in counter.most_common(10):\n    words.append(letter)\n    counts.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.cm as cm\nfrom matplotlib import rcParams\ncolors = cm.rainbow(np.linspace(0, 1, 10))\nrcParams['figure.figsize'] = 20, 10\n\nplt.title('Top words in the Job description vs their count')\nplt.xlabel('Count')\nplt.ylabel('Words')\nplt.barh(words, counts, color=colors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, here we can remove the words which doesn't necessarily depict any information related to skills"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Posting Date'] = pd.to_datetime(df['Posting Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Process Date'] = pd.to_datetime(df['Process Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there is no column for years of exprience, so we can assume that process date is the date when either latest or new posting has been published by the employer "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['years of exprience'] = df['Process Date'] - df['Posting Date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['years of exprience'] = df['years of exprience'].dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster = df[['cat','Salary Range To','years of exprience']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster['cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster['cat'].fillna('Others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster=df_cluster.replace('\\*','',regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are creating new dataframe with job category, maximum salary for the respective role and years of exprience. Reason of taking max salary instead of mean salary is to categorize those set of job which demands niche skills and higher salary"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the Hopkins statistic\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) / (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the Hopkins measure\nhopkin_df = df_cluster\nhopkins(hopkin_df.drop(['cat'],axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.99 is a good Hopkins score. Hence the data is very much suitable for clustering. Preliminary check is now done.\nWe can do standardisation again or else we can skip this step as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster_std = df_cluster\nX_C = df_cluster_std.drop(['cat'],axis=1)\ndf_cluster_std = StandardScaler().fit_transform(X_C)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-means Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the silhouette score first to identify the ideal number of clusters\n# To perform KMeans clustering \nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nsse_ = []\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k).fit(df_cluster_std)\n    sse_.append([k, silhouette_score(df_cluster_std, kmeans.labels_)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sihouette score reaches a peak at around 4 clusters indicating that it might be the ideal number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#The sihouette score reaches a peak at around 4 clusters indicating that it might be the ideal number of clusters.\n#Let's use the elbow curve method to identify the ideal number of clusters.\nssd = []\nfor num_clusters in list(range(1,10)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50)\n    model_clus.fit(df_cluster_std)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A distinct elbow is formed at around 2-5 clusters. Let's finally create the clusters and see for ourselves which ones fare better"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#K-means with k=4 clusters\nmodel_clus4 = KMeans(n_clusters = 4, max_iter=50)\nmodel_clus4.fit(df_cluster_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat4=df_cluster\ndat4.index = pd.RangeIndex(len(dat4.index))\ndat_km = pd.concat([dat4, pd.Series(model_clus4.labels_)], axis=1)\ndat_km.columns = ['cat','salary_max','exp','ClusterID']\ndat_km","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat_km['ClusterID'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat_km","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One thing we noticed is all distinct clusters are being formed except cluster 1 with more data points\n#Now let's create the cluster means wrt to the various variables mentioned in the question and plot and see how they are related\ndf_final=pd.merge(df,dat_km,on='cat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Along Job category and years of exprience\nsns.scatterplot(x='cat',y='exp',hue='ClusterID',data=df_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Along Job category and years of exprience\nsns_plot = sns.scatterplot(x='Salary Range To',y='exp',hue='cat',data=df_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, it can be seen that different salary ranges based on job category(cat) and years of experience(exp).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sns_plot.get_figure()\nfig.savefig(\"output.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As Job categories are more, x-axis in the graph is not visible but we can make a clear depiction below"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's take a look at those Job category clusters and try to make sense if the clustering process worked well.\ndf_final_on_jobcat = df_final[df_final['ClusterID']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final_on_jobcat['cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"It can be concluded from the above analysis that:\n    \nEngineering :                             51425\nArchitecture :                           50325\nPlanning  :                               24625\n\nhas more number of demand as well as higher salary with respect to niche skills. Whereas for last few job category there are having very less openings coming.\n\n\nHealth Policy         :                       9\nPlanning Building Operations      :           8\nHealth Building Operations  :                6\nHealth Public Safety      :                  6\nCommunity Programs Policy    :               6\nInnovation Policy       :                    4\nHuman Resources Technology   :               4\nHuman Resources Communications    :          4\nHuman Resources Constituent Services :       4\nHuman Resources Health Public Safety  :       1\n\nIt is obvious from the clustering as well as the merged data with cluster information that cluster 1 belongs to those set which has more openings and higher demand with more salary"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}