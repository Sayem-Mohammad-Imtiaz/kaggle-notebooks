{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of Content\n\n1. [Problem statement](#1)\n1. [Import Libraries and dataset](#2)\n1. [Exploratory data analysis (EDA)](#3)\n1. [ Explore Target Variable `RainTomorrow`](#4)\n1. [Explore Categorical Variables](#5)\n1. [Explore Numerical Variables](#6)\n1. [Multivariate Analysis](#7)\n1. [Handling Class Imbalance](#8)\n1. [Splitting of data](#9)\n1. [Feature Engineering](#10)\n1. [Feature Scaling](#11)\n1. [Model training, making predictions and evaluation](#12)\n    - [Logistic Regression](#12.1)\n    - [KNN](#12.2)\n    - [Decision Tree](#12.3)\n    - [Random Forest](#12.4)\n    - [lightGBM](#12.5)\n    - [Catboost](#12.6)\n    - [XGBoost](#12.7)\n    - [Neural Network](#12.8)\n1. [Model Comparison](#13)\n1. [Bias and variance](#14)\n1. [Feature Importance](#15)\n1. [Model Performance on Imbalance dataset](#16)\n    \n    ","metadata":{}},{"cell_type":"markdown","source":"## 1. Problem statement <a class=\"anchor\" id=\"1\"></a>    \nIn this notebook, the problem is to predict that whether or not it will rain tomorrow in Australia.    \nIn order to solve this problem, my approach is to build different classifiers (binary classifiers) and compare them to get the best classifier. Initially, I will explore and process the data and then implement classifiers.    \nSo, let's start this story.","metadata":{}},{"cell_type":"markdown","source":"## 2. Import Libraries and dataset <a class=\"anchor\" id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom IPython.display import display  \nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for data preprocessing\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix,\\\ncohen_kappa_score, plot_confusion_matrix, roc_curve\n\n# import different classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm\nimport catboost\nimport xgboost\nfrom sklearn.neural_network import MLPClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploratory data analysis (EDA) <a class=\"anchor\" id=\"3\"></a>","metadata":{}},{"cell_type":"code","source":"def data_explore(df):\n    display(df.head())\n    print(\"*\" * 30)\n    print(f\"shape of dataset {df.shape}\")\n    print(\"*\" * 30)\n    display(\"Info {}\".format(df.info()))\n    print(\"*\" * 30)\n    print(\"Dtypes: \\n{}\".format(df.dtypes.value_counts()))\n    print(\"*\" * 30)\n    print(df.columns)\n    print(\"*\" * 30)\n    print(\"Number of columns having null values: \", df.isnull().any().sum())\ndata_explore(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# describe for all numeric variables\ndf.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# describe for all categorical variables\ndf.describe(include=['object']).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data type plots\nfig, ax = plt.subplots(1,2,figsize = (12,6))\n\ndf.dtypes.value_counts().plot.pie(explode = [0.05,0.05], autopct = \"%1.0f%%\",\n                                  shadow = True, ax = ax[1])\nax[1].set_title(\"datatype\")\n\ndf.dtypes.value_counts().plot(kind = 'bar', ax = ax[0])\nax[0].set_title(\"datatype\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the dataset, 70% features are numeric (float 64) while rest 30% are categorical (object). ","metadata":{}},{"cell_type":"markdown","source":"## 4. Explore Target Variable `RainTomorrow`  <a class=\"anchor\" id=\"4\"></a>","metadata":{}},{"cell_type":"code","source":"# missing values in the target variable\ndf['RainTomorrow'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 3267 entries in the dataset where target variable `RainTomorrow` is null.    \nWe can't imput these missing values so the only option left is to drop all the enties where target variable is `NaN`.","metadata":{}},{"cell_type":"code","source":"df.dropna(subset=['RainTomorrow'], axis = 0, inplace = True)\n\ndf['RainTomorrow'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df['RainTomorrow'].value_counts())\ndisplay(df['RainTomorrow'].value_counts() * 100 / len(df))\n\ndf['RainTomorrow'].value_counts().plot(kind = 'bar', color = ['skyblue', 'navy'], rot = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# conversion of target variable from categorical to numeric\ndf['RainTomorrow'] = df['RainTomorrow'].map({'No': 0, 'Yes': 1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Findings**    \n* Target variable has 3267 Nan values which were removed from the dataset.\n* 1103116 entries for `No` variable (77.58%)\n* 31877 entries for `Yes` variable  (22.41%)    \n    The dataset is highly imbalanced \n","metadata":{}},{"cell_type":"markdown","source":"## 5. Explore Categorical Variables <a class=\"anchor\" id=\"5\"></a>\n   * Unique values \n   * number of missing values\n   * frequency plot","metadata":{}},{"cell_type":"code","source":"cat_cols = df.select_dtypes('object').columns\ncat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 7 categorical columms ('Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm','RainToday')    \nLets explore these first\n","metadata":{}},{"cell_type":"code","source":"# number of unique values in each categorical column\nfor col in cat_cols:\n    print(col,\"\\t\", df[col].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of missing values in each categorical column\nfor col in cat_cols:\n    print(col, '\\t', df[col].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequency plot of each categorical variable\nplt.figure(figsize = (20,8))\nfor i, col in enumerate(cat_cols[1:]):\n    plt.subplot(2, 3, i+1)\n    sns.countplot(df[col])\n    plt.xticks(rotation = 90)\n    plt.title(f\"{col} has {df[col].nunique()} unique values\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Date**   \n* Date column is categorical and it would not provide any significance if as such. So we could convert it into datetime format and then create separate feature of year, month, and day. ","metadata":{}},{"cell_type":"code","source":"df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\ndf[['Date', 'Year', 'Month', 'Day']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now lets drop Date column\ndf.drop('Date', axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# update cat_cols\ncat_cols = df.select_dtypes('object').columns\ncat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Explore Numerical Variables <a class=\"anchor\" id=\"6\"></a>\n   * missing values check\n   * outlier check\n   * distribution check","metadata":{}},{"cell_type":"code","source":"# numerical columns\nnum_cols = df.select_dtypes(exclude=['object']).columns\nnum_cols = num_cols[:-4]\nnum_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing value check\ndf[num_cols].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outlier check\ndf[num_cols].describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# box plot of numerical variables\nplt.figure(figsize = (15,12))\nfor i, col in enumerate(num_cols):\n    plt.subplot(4, 4, i+1)\n    sns.boxplot(data = df, y = col, whis = 3)\n    plt.title(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns `Rainfall`, `Evaporation`, `WindSpeed9am`, `WindSpeed3am` mainly contains outliers","metadata":{}},{"cell_type":"markdown","source":"**Distribution check**   \n* To check whether the distibution is normal or skewed. ","metadata":{}},{"cell_type":"code","source":"outlier_cols = ['Rainfall', 'Evaporation', 'WindSpeed9am', 'WindSpeed3pm']\n# histogram plot to check distribution\nplt.figure(figsize = (12,10))\nfor i, col in enumerate(outlier_cols):\n    plt.subplot(2, 2, i+1)\n    sns.histplot(data = df,x = col, bins = 20)\n    plt.title(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**    \nThe distribution is skewed in case of all these columns so we will find IQR (Interquantile range) to detect ouliers.","metadata":{}},{"cell_type":"code","source":"def IQR(df, out_cols):\n    for col in out_cols:\n        iqr = df[col].quantile(0.75) - df[col].quantile(0.25)\n        lower =  df[col].quantile(0.25) - (iqr * 3)\n        upper = df[col].quantile(0.75) + (iqr * 3)\n        outlier_percent = round((df[df[col] > upper].shape[0] * 100)/len(df), 2)\n        print( col , '\\t', lower.round(2), '\\t', upper.round(2), \n              '\\t', df[col].min(), '\\t', df[col].max(), '\\t', outlier_percent)\nprint('column \\t\\t lower \\t high \\t min \\t max \\t outlier_percent')\nIQR(df, outlier_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The normal range for `Rainfall` is -2.4 to 3.2 while its min and max values are 0 and 371 so we can limit the higher values only upto 3.2","metadata":{}},{"cell_type":"markdown","source":"## 7. Multivariate Analysis <a class=\"anchor\" id=\"7\"></a>\n   * To discover patterns and relationships between variables in the dataset. \n   * Heatmap of correlation \n   * Pairplot to see the patterns","metadata":{}},{"cell_type":"code","source":"# Heatmap\nsns.set_context('notebook', font_scale=1.0, rc = {'lines.linewidth': 2.5})\nplt.figure(figsize = (15,12))\n\n# mask the duplicate correlation values\nmask = np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask, 1)] = True\n\na = sns.heatmap(df.corr(), mask = mask, annot=True, fmt = '.2f', cmap = 'viridis')\n\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation = 90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation = 30)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**     \nThere are few variables ('MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'WindGustSpeed', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm') which have high correlation with other variables while none with 100% correlation so no need to remove any features.","metadata":{}},{"cell_type":"code","source":"# Pair Plot for higly correlated variables\nsns.pairplot(data = df, vars = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'WindGustSpeed', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm'], \n             kind = 'scatter', \n             diag_kind= 'hist',\n             hue = 'RainTomorrow')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Handling Class Imbalance <a class=\"anchor\" id=\"8\"></a>","metadata":{}},{"cell_type":"code","source":"df['RainTomorrow'].value_counts().plot(kind='bar',color=['blue', 'cyan'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target is highly imbalance so we can either increase the minor class samples or decrease the major class samples. Here I will use oversampling of the minority class.","metadata":{}},{"cell_type":"code","source":"no = df[df['RainTomorrow'] == 0]\nyes = df[df['RainTomorrow'] == 1]\n\nyes_os = resample(yes, replace = True, n_samples=len(no), random_state=21)\n\ndf_os = pd.concat([no, yes_os])\nprint(df_os.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (8,5))\ndf_os['RainTomorrow'].value_counts(normalize = True).plot(kind = 'bar', \n                                                         color = ['skyblue', 'navy'], \n                                                         alpha = 0.9, \n                                                         rot = 0)\nplt.title('balanced dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Splitting of data <a class=\"anchor\" id=\"9\"></a>","metadata":{}},{"cell_type":"code","source":"X = df_os.drop(['RainTomorrow'], axis = 1)\ny = df_os['RainTomorrow']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Feature Engineering <a class=\"anchor\" id=\"10\"></a>\n   1. Imputing missing values\n       - In numerical variables\n       - In categorical variables   \n   2. Treating outliers\n   3. Encoding of categorical variables","metadata":{}},{"cell_type":"markdown","source":"#### 1. Imputing Missing values <a class=\"anchor\" id=\"10.1\"></a>  \n  * Numeric missing values are imputed with median of training dataset as median is more robust to outliers and there would be no data leakage because imputing all the missing values with median of **training dataset**\n  * Categorical missing values are imputed with mode of training dataset.","metadata":{}},{"cell_type":"code","source":"# categorical and numeric missing values in train and test datasets\n\ncat_miss = pd.concat([pd.DataFrame(X_train[cat_cols].isnull().sum()),\n                      pd.DataFrame(X_test[cat_cols].isnull().sum())],\n                     axis = 1)\nnum_miss = pd.concat([pd.DataFrame(X_train[num_cols].isnull().sum()),\n                      pd.DataFrame(X_test[num_cols].isnull().sum())],\n                     axis = 1)\ncat_miss.columns = ['train', 'test']\nnum_miss.columns = ['train', 'test']\ndisplay(cat_miss)\ndisplay(num_miss)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing missing values of numeric columns\nfor df1 in [X_train, X_test]:\n    for col in num_cols:\n        col_median = X_train[col].median()\n        df1[col].fillna(col_median, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[num_cols].isnull().any().sum(), X_test[num_cols].isnull().any().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing misssing values in categorical variables\nfor df1 in [X_train, X_test]:\n    for col in cat_cols[1:]:\n        col_mode = X_train[col].mode()[0]\n        df1[col].fillna(col_mode, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[cat_cols].isnull().any().sum(), X_test[cat_cols].isnull().any().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.isnull().any().sum(), X_test.isnull().any().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the missing values have been imputed, now we will treat the outliers","metadata":{}},{"cell_type":"markdown","source":"#### 2. Treating outliers <a class=\"anchor\" id=\"10.2\"></a>\n  * The max value of 4 columns having outlier is to change to the upper limit of IQR.","metadata":{}},{"cell_type":"code","source":"# treating outliers\ndef max_value(df, col, top):\n    return np.where(df[col]> top, top, df[col])\nfor df in [X_train, X_test]:\n    df['Rainfall'] = max_value(df, 'Rainfall', 3.2)\n    df['Evaporation'] = max_value(df, 'Evaporation', 21.8)\n    df['WindSpeed9am'] = max_value(df, 'WindSpeed9am', 55.0 )\n    df['WindSpeed3pm'] = max_value(df, 'WindSpeed3pm', 57.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['Rainfall'].max(), X_test['Rainfall'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Encoding Categorical Variables <a class=\"anchor\" id=\"10.3\"></a>\nThere are 5 categorical variables which have to be encoded, In this, I am using `get_dummies` with drop first. ","metadata":{}},{"cell_type":"code","source":"for col in cat_cols:\n    print(col , '\\t', X_train[col].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = X_train.select_dtypes(exclude= 'object').columns\n\nX_train[num_cols].shape, X_train[cat_cols].shape, X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical encoding for training dataset\nX_train.shape\nX_train = pd.concat([X_train[num_cols], \n                   pd.get_dummies(X_train['Location'], drop_first=True), \n                   pd.get_dummies(X_train['WindGustDir'], drop_first=True, prefix = 'WindGustDir'), \n                   pd.get_dummies(X_train['WindDir9am'], drop_first=True, prefix = 'WD9am'),\n                pd.get_dummies(X_train['WindDir3pm'], drop_first=True, prefix = 'WD3pm'), \n                    pd.get_dummies(X_train['RainToday'], drop_first=True, prefix ='RainToday')], \n                    axis = 1\n                   )\nX_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical encoding for test dataset\nX_test.shape\nX_test = pd.concat([X_test[num_cols], \n                   pd.get_dummies(X_test['Location'], drop_first=True), \n                   pd.get_dummies(X_test['WindGustDir'], drop_first=True, prefix = 'WindGustDir'), \n                   pd.get_dummies(X_test['WindDir9am'], drop_first=True, prefix = 'WD9am'),\n                   pd.get_dummies(X_test['WindDir3pm'], drop_first=True, prefix = 'WD3pm'), \n                   pd.get_dummies(X_test['RainToday'], drop_first=True, prefix ='RainToday')], \n                   axis = 1\n                   )\nX_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now before diving into Model training we should map all the feature variables onto the same scale using `feature scaling`. ","metadata":{}},{"cell_type":"markdown","source":"## 11. Feature Scaling <a class=\"anchor\" id=\"11\"></a>","metadata":{}},{"cell_type":"code","source":"X_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = X_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# after scaling the dataframe is converted into np array \nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns = [cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**    \nNow max and min value of all the features is 1, 0 and count of all is same indicating the absence of null values. Now our training and test dataset is ready for model build. ","metadata":{}},{"cell_type":"markdown","source":"## 12. Model training, making predictions and evaluation <a class=\"anchor\" id=\"12\"></a>\n    1. Logistic Regression\n    2. KNN\n    3. Decision Tree\n    4. Random Forest\n    5. lightGBM\n    6. Catboost\n    7. XGBoost\n    8. Neural network","metadata":{}},{"cell_type":"code","source":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color = 'red', label = 'ROC')\n    plt.plot([0,1], [0,1], color = 'navy', linestyle = '--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curver')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General method for model training\ndef model_run(clf, X_train, y_train, X_test, y_test, verbose = 1):\n    tic = time.time()\n    \n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred_train = clf.predict(X_train)\n    \n    accuracy_train = accuracy_score(y_train, y_pred_train)\n    accuracy_test = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    coh_kap = cohen_kappa_score(y_test, y_pred)\n    \n    toc = time.time()\n    time_taken = toc-tic\n    \n    print(\"Training Accuracy = {}\".format(accuracy_train.round(2) * 100))\n    print(\"Test Accuracy = {}\".format(accuracy_test.round(2) * 100))\n    print(\"ROC Area under Curve = {}\".format(roc_auc.round(2)))\n    print(\"Cohen's Kappa = {}\".format(coh_kap.round(2)))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    \n    probs = clf.predict_proba(X_test)[:,1]\n    \n    fpr, tpr, threshold = roc_curve(y_test,probs)\n    plot_roc_curve(fpr, tpr)\n    \n    plot_confusion_matrix(clf, X_test, y_test, cmap=plt.cm.Blues, normalize='all')\n    \n    return clf, accuracy_train,accuracy_test, roc_auc, coh_kap, time_taken","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Logistic Regression <a class=\"anchor\" id=\"12.1\"></a>","metadata":{}},{"cell_type":"code","source":"# Logistic regression\nclf_lr = LogisticRegression()\nclf_lr, acc_tr_lr, acc_lr, roc_lr, coh_kap_lr, time_lr = model_run(clf_lr, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. KNN  <a class=\"anchor\" id=\"12.2\"></a>","metadata":{}},{"cell_type":"code","source":"\nclf_knn = KNeighborsClassifier()\nclf_knn, acc_tr_knn, acc_knn, roc_knn, coh_kap_knn, time_knn = model_run(clf_knn, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Decision Tree  <a class=\"anchor\" id=\"12.3\"></a>","metadata":{}},{"cell_type":"code","source":"# Decision Tree\nparam_dt = {'max_depth': 16, 'max_features': 'sqrt'}\n\nclf_dt = DecisionTreeClassifier(**param_dt)\nclf_dt, acc_tr_dt, acc_dt, roc_dt, coh_kap_dt, time_dt = model_run(clf_dt, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Random Forest  <a class=\"anchor\" id=\"12.4\"></a>","metadata":{}},{"cell_type":"code","source":"params_rf = {'max_depth': 16,\n             'min_samples_leaf': 1,\n             'min_samples_split': 2,\n             'n_estimators': 200,\n             'random_state': 21}\nclf_rf = RandomForestClassifier(**params_rf)\nclf_rf, acc_tr_rf, acc_rf, roc_rf, coh_kap_rf, time_rf = model_run(clf_rf, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. LGBM  <a class=\"anchor\" id=\"12.5\"></a>","metadata":{}},{"cell_type":"code","source":"params_lgbm = {'colsample_bytree': 0.95, \n         'max_depth': 16, \n         'min_split_gain': 0.1, \n         'n_estimators': 200, \n         'num_leaves': 50, \n         'reg_alpha': 1.2, \n         'reg_lambda': 1.2, \n         'subsample': 0.95, \n         'subsample_freq': 20}\nclf_lgbm = lightgbm.LGBMClassifier(**params_lgbm)\nclf_lgbm, acc_tr_lgbm, acc_lgbm, roc_lgbm, coh_kap_lgbm, time_lgbm = model_run(clf_lgbm, X_train.values, y_train.values, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. CatBoost  <a class=\"anchor\" id=\"12.6\"></a>","metadata":{}},{"cell_type":"code","source":"params_cboost ={'iterations': 50,\n            'max_depth': 16}\nclf_cbst = catboost.CatBoostClassifier(**params_cboost)\nclf_cbst, acc_tr_cbst, acc_cbst, roc_cbst, coh_kap_cbst, time_cbst = model_run(clf_cbst, X_train, y_train, X_test, y_test, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. XGBoost  <a class=\"anchor\" id=\"12.7\"></a>","metadata":{}},{"cell_type":"code","source":"params_xgb ={'max_depth': 16}\nclf_xgb = xgboost.XGBClassifier(**params_xgb)\nclf_xgb, acc_tr_xgb,acc_xgb, roc_xgb, coh_kap_xgb, time_xgb = model_run(clf_xgb, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Neural Network  <a class=\"anchor\" id=\"12.8\"></a>","metadata":{}},{"cell_type":"code","source":"clf_nn = MLPClassifier(random_state=21, verbose=0)\nclf_nn, acc_tr_nn, acc_nn, roc_nn, coh_kap_nn, time_nn = model_run(clf_nn, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13. Model Comparison  <a class=\"anchor\" id=\"13\"></a>","metadata":{}},{"cell_type":"code","source":"# comparison of accuracy, roc score, coh_kappa score and time\nacc_all = [acc_lr, acc_knn,acc_dt, acc_rf, acc_lgbm, acc_cbst, acc_xgb, acc_nn]\nacc_tr_all = [acc_tr_lr, acc_tr_knn,acc_tr_dt, acc_tr_rf, acc_tr_lgbm, acc_tr_cbst, acc_tr_xgb, acc_tr_nn]\nroc_all = [roc_lr, roc_knn, roc_dt, roc_rf, roc_lgbm, roc_cbst, roc_xgb, roc_nn]\ncoh_kap_all = [coh_kap_lr, coh_kap_knn, coh_kap_dt, coh_kap_rf, coh_kap_lgbm, coh_kap_cbst, coh_kap_xgb, coh_kap_nn]\ntime_taken = [time_lr, time_knn, time_dt, time_rf, time_lgbm, time_cbst, time_xgb, time_nn]\nmodels = ['Logistic Regression','KNN','Decision Tree','Random Forest','LightGBM','Catboost','XGBoost', 'Neural Network' ]\n\nmodel_comp_df = pd.DataFrame({'Model': models,\n                              'Train Accuracy' : acc_tr_all,\n                          'Test Accuracy': acc_all, \n                          'ROC_AUC': roc_all,\n                          'Cohen_kappa': coh_kap_all,\n                          'Time_taken': time_taken})\n\nmodel_comp_df.style.background_gradient(cmap='Blues')\nmodel_comp_df.to_csv('balanced_summery.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 12))\n\n# plot of Test Accuracy of every model\nplt.subplot(221)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'Test Accuracy', palette = 'winter')\nplt.title('Test Accuracy of classifier')\nplt.xticks(rotation = 90)\nplt.ylim(0.5, 1.0)\n\n# plot of Time of every model\nplt.subplot(222)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'Time_taken', palette = 'summer' )\nplt.title('Time taken by classifier')\nplt.xticks(rotation = 90)\n\n# plot of ROC of every model\nplt.subplot(223)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'ROC_AUC', palette = 'winter')\nplt.title('ROC-AUC Score of classifier')\nplt.xticks(rotation = 90)\nplt.ylim(0.5, 1.0)\n\n# plot of Cohen_kappa of every model\nplt.subplot(224)\nsns.barplot(data = model_comp_df, x = 'Model', y = 'Cohen_kappa', palette = 'summer')\nplt.title('Cohen_kappa Score of classifier')\nplt.xticks(rotation = 90)\nplt.ylim(0.5, 1.0)\n\nplt.savefig('balanced_plots.jpeg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Result Discussion**    \n1. Among all the classifiers, XGBoost, Catboost and Random Forest perform best with high accuracy score.\n2. On comparing the time_taken of all the classifiers, it was observed that KNN is taking highest time followed by neural network. \n3. Trends of ROC-AUC score and Cohen_kappa score is similar to that of accuracy.  \n\n<span style=\"color:blue\">**On considering time and accuracy, the best classifier for the current problem is XGBoost.** ","metadata":{}},{"cell_type":"markdown","source":"## 14. Bias and variance of different models <a class=\"anchor\" id=\"14\"></a>","metadata":{}},{"cell_type":"code","source":"model_comp_df['Train Accuracy'] = model_comp_df['Train Accuracy'].apply(lambda x: round(x, 3))\nmodel_comp_df['Test Accuracy'] = model_comp_df['Test Accuracy'].apply(lambda x: round(x, 3))\nmodel_comp_df[['Model','Train Accuracy', 'Test Accuracy']] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**    \n* Logistic regression has high bias and low variance.\n* XGBoost has low bias and low variance.\n\nHence XGBoost is considered as most optimized model for this problem. ","metadata":{}},{"cell_type":"markdown","source":"## 15. Feature Importance <a class=\"anchor\" id=\"15\"></a>","metadata":{}},{"cell_type":"code","source":"# calculating the most important features\nimportance = clf_xgb.feature_importances_\n\nfeat_imp_df = pd.DataFrame({'Features': cols, 'Importance': importance})\n\nf_imp = feat_imp_df[feat_imp_df.sort_values('Importance', ascending=False)['Importance'] >= 0.01].reset_index(drop= True)\nf_imp.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20, 8))\nf_imp.plot.bar( x = 'Features', y = 'Importance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model performance on important features\nimp_feat = f_imp['Features']\n\nclf_xgb_imp = xgboost.XGBClassifier(**params_xgb)\nclf_xgb_imp, acc_tr_xgb_imp,acc_xgb_imp, roc_xgb_imp, coh_kap_xgb_imp, time_xgb_imp = model_run(clf_xgb_imp, X_train[imp_feat], y_train, X_test[imp_feat], y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**    \n* For the dataset containing most important features obtained from XGBoost, the test accuracy is decreased for 94 to 88. ","metadata":{}},{"cell_type":"markdown","source":"## 16. Model Performance on Imbalance dataset <a class=\"anchor\" id=\"16\"></a>","metadata":{}},{"cell_type":"markdown","source":"In order to observe the effect of balancing the dataset I have trained all the classifiers on imbalanced dataset and obtained the model comparison summary. ","metadata":{}},{"cell_type":"code","source":"bal_clf_summ = pd.read_csv('../input/balance-model-summary/balanced_summery.csv', index_col = 0)\nbal_clf_summ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unbal_clf_summ = pd.read_csv('../input/unbalance-model-summary-rainfall/unbalanced_summery.csv', index_col=0)\nunbal_clf_summ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**\nIf we compare the model accuracy before and after balancing the minority class then it is observed that: \n* Models like Logisitic Regression and Decision tree perform better on unbalanced dataset.\n* Models including Catboost and XGBoost show high variance in case of unbalanced dataset as compared to balanced dataset.\n* Model Random Forest perform almost same in both the cases.\n\n<span style=\"color:blue\">**Hence Random Forest can be considered as good model in this case with low bias and low variance**\n","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color:red\">I hope you Liked my kernel. An upvote is a gesture of appreciation and encouragement that fills me with energy to keep improving my efforts ,be kind to show one.","metadata":{}}]}