{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Проект №3. О вкусной и здоровой пище \n**[SF-DST-10] Restaurant Rating prediction Sergey Kuzmenko**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# import","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np \nimport re\nimport json\nimport datetime as dt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nimport unicodedata #для транслитерации не ASCII\nimport unidecode #для транслитерации не ASCII \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport itertools\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Для воспроизводимости результатов зададим:\n# - общий параметр для генерации случайных чисел\nRANDOM_SEED = 20726\n# - общую текущую дату\nCURRENT_DATE = pd.to_datetime('22/02/2020')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_in = pd.read_csv(DATA_DIR+'main_task.csv')\n# Путь к датасету для которого требуется предсказать рейтинг\ndf_kagle = pd.read_csv(DATA_DIR+'kaggle_task.csv')\n#Путь к 'submission.csv'\nsubmission_path = pd.read_csv(DATA_DIR+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DS_IMPORT_SC_DIR = '/kaggle/input/cities-and-countries/'\n# Путь к датасету стран - отсюда берем столицы\ndf_county = pd.read_csv(DS_IMPORT_SC_DIR+'country-list.csv')\n#датасет по городам и странам\ndf_urb_xls = pd.read_excel(DS_IMPORT_SC_DIR+'global-city-population-estimates.xls', index_col=None, header=0, sheet_name = 'CITIES-OVER-300K')\n\n# Путь к датасету, содержащему слова с позитивной окраской\ndf_pos_words = pd.read_csv('/kaggle/input/opinion-lexicon-english/positive-words.txt',skiprows=34, names=['word'])\npos_words_list = df_pos_words['word'].to_list() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FUNC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" def get_season(date):\n    '''\n    Время года из даты\n    '''\n    if (pd.isna(date)):\n        return \"OTHER\"\n    month = date.month\n    if (month > 11 or month <= 3):\n       return \"WINTER\"\n    elif (month == 4 or month == 5):\n       return \"SPRING\"\n    elif (month >=6 and month <= 9):\n       return \"SUMMER\"\n    else:\n       return \"FALL\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def round_of_rating(number):\n    \"\"\"\n    Округляем до 0.5\n    \"\"\"\n    return np.round(number * 2) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_Weighed_Rank_RK(row):\n    '''\n    Вычисляем относительную позицию ресторана среди всех ресторанов города\n    '''\n    Weighed_Rank = row['Ranking'] / row['Restaurants Count']\n\n    return Weighed_Rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_Weighed_Rank(row):\n    i=0\n    city_min = CityMinMax[CityMinMax['City'] == row.City ]['min'].iloc[0]\n    city_max = CityMinMax[CityMinMax['City'] == row.City ]['max'].iloc[0]\n    Weighed_Rank = round(1-(row['Ranking'] - city_min)/(city_max - city_min),3)\n    #print('<---',Weighed_Rank,'--->')\n    return Weighed_Rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rev_time_delta(reviews):\n    '''\n    Вычисляем время между review в днях\n    '''\n    if (pd.notna(reviews)):\n        reviews_dt_list = get_reviews(reviews)['reviews_dt']\n        if reviews_dt_list:\n            return (max(reviews_dt_list) - min(reviews_dt_list)).days\n        else:\n            return dt.timedelta(days=3650).days\n    else:\n        return dt.timedelta(days=3650).days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_reviews(rev):\n    '''\n    Получаем review в виде:\n    review['reviews_txt'][1] - list of reviews\n    review['reviews_dt'][1] - list of reviews dates\n    '''\n    if  not pd.isna(rev): \n        rev = str(rev).replace(\"'\",'\"')\n        rev = rev.replace('], [', '], \"reviews_dt\": [')\n        rev = '{ \"reviews_txt\":' + rev + '}'\n        rev = rev.replace('[[','[').replace(']]',']')\n        d = json.loads(rev)\n\n        d['reviews_dt'] = [dt.datetime.strptime(date, '%m/%d/%Y').date() if len(date.split('/')[2])==4 else dt.datetime.strptime(date, '%m/%d/%y').date() for date in d['reviews_dt']]\n        return d\n    else:\n        return {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cuisines(cuisines):\n    '''\n    Получаем список кухонь в виде:\n    cuisines[0] - list of cusines\n    если был NaN, то возвращается 'Regionl Cusine' -как самая популярная в регионе/городе/стране\n    '''\n    if cuisines == 'NaN': return ['Regionl Cusine']#['Vegetarian Friendly'] # 'Vegetarian Friendly' - самая популярная\n    if  cuisines:\n        cuisines = str(cuisines).replace(\"'\",'\"')\n        return json.loads(cuisines)\n    else:\n        return ['Regionl Cusine'] #return ['Vegetarian Friendly']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allCusines = []\ndef cuisine_styles_count(row):\n    '''\n    Получаем количество кухонь\n    '''\n    global allCusines\n    cusines = get_cuisines(row['Cuisine Style'])\n    \n    if row['Cuisine Style'] != 'NaN':    \n        cusines = get_cuisines(row['Cuisine Style'])\n        allCusines.extend(cusines)\n        cuisines_count =len(cusines)\n    else:\n        cuisines_count = 1\n\n    return cuisines_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanup_string(str_in):\n    '''\n    \"Чистим\" текст в review для последующй десериализации\n    Пока текст в анализе не используется, не успел... \n    '''\n    try:      \n        \n        #middle\n\n        str = str_in.replace(\"', \\\"\",\"⅞\").replace(\"', '\",\"⅞\").replace(\"\\\", '\",\"⅞\").replace(\"\\\", \\\"\",\"⅞\")# \", \n        str = str.replace(\"\\\", \\\"\\\"\",\"⅞\").replace(\"\\\"\\\", '\",\"⅞\").replace(\"\\\", \\'\",\"⅞\").replace(\"\\\"\\\", \\'\",\"⅞\")\n        str = str.replace(\"\\', \\'\",\"⅞\")\n        #middle\n        #left\n        str = str.replace(\"[['\",\"≤\").replace(\"['\",\"⅛\")\n        #left\n        #right\n        str = str.replace(\"']]\",\"≥\").replace(\"']\",\"⅝\")\n        #right\n        #cleanups\n        str = str.replace('\\'', ' ').replace('\\\"', ' ').replace('\\'', ' ').replace('\"', ' ')     \n        str = str.replace(\"\\\\\", \" \").replace(\"[[`\", \"≤\").replace('\\'\"', '\\'').replace('\\'\\\"', '\\'')\n        str = str.replace('\"\\'', '\\'').replace('\\\"\\'', '\\'').replace(\"[''\" ,\"≤\").replace(\"[\\'\\'\" ,\"≤\")\n        str = str.replace('\\'', ' ').replace('\\\"', ' ')\n        str = str.replace('\\'', ' ').replace('\\\"', ' ')\n        str = str.replace('\\'', ' ').replace('\"', ' ')\n        #cleanups\n        #middle\n        str = str.replace(\"⅞\", \"', '\")\n        #middle\n        #left\n        str = str.replace(\"≤\", \"[['\").replace(\"⅛\", \"['\").replace('[[ ', '[[ \\'')\n        #left\n        #right\n        str = str.replace(\"≥\" ,\"']]\").replace(\"⅝\", \"']\").replace(' ]', ' \\']')\n        str = str.replace(', nan]', '\\', \\'nan\\']').replace('[nan, ', '[\\'nan\\', \\'')\n        #right\n    except Exception:\n        print('<----',str_in,'---->')\n    return str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_city_population_and_country():\n    '''\n    Получаем популяцию по городам, а так же ISO код страны по городу (из внешних источников)\n    '''\n    population_city_dict = {}\n    country_city_dict = {}\n    cities = df['City'].unique()\n    # Чистим названия городов от Unicode символов - транслитерацией\n    df_urb_xls['Urban Agglomeration TL'] = df_urb_xls['Urban Agglomeration'].apply(lambda s: ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')) )\n    df_urb_xls['Urban Agglomeration TL'] =  df_urb_xls['Urban Agglomeration'].apply(lambda x: unidecode.unidecode(x))\n\n    for city in cities:\n        vals = df_urb_xls[df_urb_xls['Urban Agglomeration TL'].str.contains(city)]['2015'].max()/1000\n        vals = 0.3 if pd.isna(vals) else vals # т.к. в датасете города от 300тыс\n        population_city_dict[city] = vals\n        country = df_urb_xls[df_urb_xls['Urban Agglomeration TL'].str.contains(city)]['Country Code']\n        country = -1 if country.shape[0] < 1 else country.values[0]\n        country_city_dict[city] = country\n\n    population_city_dict['Luxembourg'] = 0.613894 \n    population_city_dict['Brussels'] = 2.115468 \n    population_city_dict['Geneva'] = 0.686562 \n    population_city_dict['Oporto'] = 0.214349 \n    population_city_dict['Ljubljana'] = 0.279631\n\n\n\n    country_city_dict['Luxembourg'] = 442 \n    country_city_dict['Brussels'] = 56 \n    country_city_dict['Geneva'] = 756 \n    country_city_dict['Oporto'] = 620 \n    country_city_dict['Ljubljana'] = 705\n    \n    #Видимо, эти данные корректнее???\n    population_city_dict = {    'London': 8.173900,\n    'Paris': 2.240621,\n    'Madrid': 3.155360,\n    'Barcelona': 1.593075,\n    'Berlin': 3.326002,\n    'Milan': 1.331586,\n    'Rome': 2.870493,\n    'Prague': 1.272690,\n    'Lisbon': .547733,\n    'Vienna': 1.765649,\n    'Amsterdam': .825080,\n    'Brussels': .144784,\n    'Hamburg': 1.718187,\n    'Munich': 1.364920,\n    'Lyon': .496343,\n    'Stockholm': 1.981263,\n    'Budapest': 1.744665,\n    'Warsaw': 1.720398,\n    'Dublin': .506211 ,\n    'Copenhagen': 1.246611,\n    'Athens': 3.168846,\n    'Edinburgh': .476100,\n    'Zurich': .402275,\n    'Oporto': .221800,\n    'Geneva': .196150,\n    'Krakow': .756183,\n    'Oslo': .673469,\n    'Helsinki': .574579,\n    'Bratislava': .413192,\n    'Luxembourg': .576249,\n    'Ljubljana': .277554\n    }\n    return population_city_dict, country_city_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_capital_city_dict():\n    '''\n    Возвращаем словарь город == столица или нет\n    '''\n    capital_city_dict = {}\n    cities =df['City'].unique()\n    df_county['capital TL'] = df_county['capital'].apply(lambda s: ''.join((c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')) )\n    df_county['capital TL'] =  df_county['capital'].apply(lambda x: unidecode.unidecode(x))\n    for city in cities:\n        vals = df_county[df_county['capital TL'].str.contains(city)]['country'].shape[0]   \n        vals = 0 if pd.isna(vals) or vals==0 else 1 \n        capital_city_dict[city] = vals\n    capital_city_dict['Barcelona'] = 0 # Страна Басков != Испания, дадим Барселоне статус мтолицы, но оставим в Испании ! 0 и не надо врать :)\n    capital_city_dict['Zurich'] = 1 \n    capital_city_dict['Geneva'] = 1 \n    capital_city_dict['Oporto'] = 1 \n    return capital_city_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_cuisine_top_N(cs):\n    '''\n    Возвращаем список кухонь, входящих в основной список кухонь,для остальных Other\n    '''\n    c = get_cuisines(cs)\n    c = set(c)\n\n    shared_cousines=()\n    shared_cousines=c.intersection(topNcusines)\n\n    if len(shared_cousines) != len(c):\n        shared_cousines = list(shared_cousines)\n        shared_cousines.extend(['Other'])\n\n    return list(shared_cousines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def createWordList(line):\n    wordList2 =[]\n    wordList1 = line.split()\n    for word in wordList1:\n        cleanWord = \"\"\n        for char in word:\n            if char in '!,.?\":;0123456789':\n                char = \"\"\n            cleanWord += char\n        wordList2.append(cleanWord.lower())\n    return wordList2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_positive_words_proportion(reviews):\n    '''\n    Количество позитивных слов в приведенных отзывах\n    '''\n    pos_words_count = 0\n    txts=get_reviews(reviews)['reviews_txt']\n    txt = ' '.join(txts)\n    #print(type(txt))\n    words = createWordList(txt)\n    \n    words_count = len(words) if len(words) > 0 else 1\n    words_count = 1\n    pos_words_in_review=set(words).intersection(pos_words_list)\n    for word in words:\n        if word in pos_words_list:\n            #print(word)\n            pos_words_count +=1  \n    return np.round(pos_words_count/words_count,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def list_positive_words(reviews): \n    '''\n    Список уникальных позитивных слов в приведенных отзывах\n    '''\n    txts=get_reviews(reviews)['reviews_txt']\n    txt = ' '.join(txts)\n    #print(type(txt))\n    words = createWordList(txt)\n    \n    words_count = len(words) if len(words) > 0 else 1\n    words_count = 1\n    pos_words_in_review=set(words).intersection(pos_words_list)\n    #print(len(pos_words_in_review))\n    if (len(pos_words_in_review) == 0):\n        return np.NAN\n    else:\n        return list(pos_words_in_review)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# WORK","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_in.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_in.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\n\ndf_kagle['ForTrain'] = False # помечаем где у нас тест\ndf_kagle['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndf_in['ForTrain'] = True # помечаем где у нас трейн\ndf_in = df_in.append(df_kagle, sort=False).reset_index(drop=True)# объединяем","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Выбираем нужные для последующего анализа столбцы\ndf = df_in[['Ranking', 'Rating', 'Number of Reviews', 'City', 'Price Range', 'Cuisine Style', 'Reviews', 'Restaurant_id', 'ID_TA', 'ForTrain' ]].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Reviews[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* ## 1. Обработка NAN и Обработка признаков\nУ наличия пропусков могут быть разные причины, но пропуски нужно либо заполнить, либо исключить из набора полностью. Но с пропусками нужно быть внимательным, **даже отсутствие информации может быть важным признаком!**   \nПо этому перед обработкой NAN лучше вынести информацию о наличии пропуска как отдельный признак ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Посмотрим на признаки и к-во пропусков (NaN)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Посмотрим на признаки и к-во уникальных значений**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Посмотрим на гистограммы числовых признаков**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.ForTrain][['Ranking', 'Rating', 'Number of Reviews','Price Range', 'Restaurant_id']].hist(figsize=(20, 10), bins=100);\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.ForTrain]['Restaurant_id'].apply(lambda x: x.split('_')[1]).astype(int).hist(figsize=(10,5), bins=100);\nplt.tight_layout()\n# Restaurant_id Очень похож на Ranking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf[df.ForTrain]['ID_TA'].apply(lambda x: x.replace('d','')).astype(int).hist(figsize=(10, 5), bins=100);\nplt.tight_layout()\n# Видно Несколько групп - на ID - точно не похоже. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ID_TA'] = df['ID_TA'].apply(lambda x: x[1:]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reviews - убираем NaN и \"причесываем\" текст отзывов \ndf['Reviews_txt_NaN'] = df['Reviews'].apply(lambda x: x ==  '[[], []]')\n\ndf['Reviews'] = df['Reviews'].fillna('[[], []]')\ndf['Reviews'] = df['Reviews'].apply(lambda x: cleanup_string(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Не будем кодировать названия городов - get_dummies справится \n# df[\"City\"] = df[\"City\"].astype('category')\n# #df[\"City\"] = df[\"City\"].cat.codes\n# encoder = LabelEncoder()\n# df['City'] = encoder.fit_transform(df['City'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Перекодируем Price Range и удаляем NaN\ncleanup_nums = {'Price Range':     {\"$\": 1, \"$$ - $$$\": 2, \"$$$$\": 3, np.NaN: 2}} # чаще всего встречается \"$$ - $$$\" == 2\ndf['Price Range NAN'] = df['Price Range'].isna()\ndf.replace(cleanup_nums, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Получаем Cuisines Count, самую популярную кухню, среднее к-во кухонь в ресторане и устраняем NaN\ndf['Cuisine Style NAN'] = df['Cuisine Style'].isna()\ndf['Cuisine Style'] = df['Cuisine Style'].fillna('NaN')\ndf['Cuisines Count'] = df.apply(cuisine_styles_count, axis=1)\n\nmost_popular_cusine = pd.Series(allCusines).value_counts().index[0]\naverage_cousines_count = np.round(df['Cuisines Count'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Reviews\ndf['Number of Reviews NAN'] = df['Number of Reviews'].isna()\nreplace_val = df['Number of Reviews'].mean()\nreplace_val = np.round(replace_val)\ndf['Number of Reviews'] = df['Number of Reviews'].fillna(replace_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA \n[Exploratory Data Analysis](https://ru.wikipedia.org/wiki/Разведочный_анализ_данных) - Анализ данных\nНа этом этапе мы строим графики, ищем закономерности, аномалии, выбросы или связи между признаками.\nВ общем цель этого этапа понять, что эти данные могут нам дать и как признаки могут быть взаимосвязаны между собой.\nПонимание изначальных признаков позволит сгенерировать новые, более сильные и, тем самым, сделать нашу модель лучше.\n![](https://miro.medium.com/max/2598/1*RXdMb7Uk6mGqWqPguHULaQ.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение признака","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,5)\ndf[df.ForTrain]['Ranking'].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.ForTrain]['City'].value_counts(ascending=True).plot(kind='barh');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"А кто-то говорил, что французы любят поесть=) Посмотрим, как изменится распределение в большом городе:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.ForTrain]['Ranking'][df[df.ForTrain]['City'] =='London'].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на топ 10 городов\nfor x in (df[df.ForTrain]['City'].value_counts())[0:10].index:\n    df[df.ForTrain]['Ranking'][df[df.ForTrain]['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n\n>Подумайте как из этого можно сделать признак для вашей модели. Я покажу вам пример, как визуализация помогает находить взаимосвязи. А далее действуйте без подсказок =) \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.ForTrain]['Rating'].value_counts(ascending=True).plot(kind='barh');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной относительно признака","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.ForTrain]['Ranking'][df[df.ForTrain]['Rating'] == 5].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.ForTrain]['Ranking'][df[df.ForTrain]['Rating'] < 4].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Корреляция имеющихся признаков - практически единственным признаком, коррелирующим с Rating является Ranking. Он, в свою очередь, уже имеет слабую корреляцию практически со всеми признаками ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,15)\nsns.heatmap(df[df.ForTrain].drop(['ForTrain'], axis=1).corr(), square=True,\n            annot=True, fmt=\".1f\", linewidths=0.1, cmap=\"RdBu\");\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Поищем другие/дополнительные признаки, привлечем внешние данные **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"population_city_dict = {}\ncountry_city_dict = {}\n# Получаем словари популяции по городам, а так же ISO код страны по городу\npopulation_city_dict, country_city_dict = get_city_population_and_country()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычисляем страну для города в каждой строке\ndf['Country'] = df[\"City\"].apply(lambda x: country_city_dict[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычисляем к-во ресторанов для города в каждой строке\nrestorants_in_city = df.groupby('City')['Ranking'].count().to_dict()\ndf['Restaurants Count'] = df['City'].map(restorants_in_city)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычисляем население (в тыс. чел) для города в каждой строке\ndf['Population'] = df[\"City\"].map(population_city_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычисляем к-во ресторанов на 1000 чел для города в каждой строке\ndf['Restaurants for Population'] = df['Restaurants Count'] / (df['Population']*1000) #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычисляем является ли город столицей в каждой строке\ncapital_city_dict = get_capital_city_dict()\ndf['isCapital'] = df[\"City\"].map(capital_city_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Получаем относительную позицию ресторана среди всех ресторанов города\ndf['Weighed Rank'] = df.apply(lambda x: get_Weighed_Rank_RK(x), axis=1)\n\nCityMinMax = df.groupby('City')['Ranking'].agg([min,max])\nCityMinMax =CityMinMax.reset_index()\ndf['Weighed Rank min max'] = df.apply(lambda x: get_Weighed_Rank(x), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Флаги (1/0) isMostPopCusine - есть ли в ресторане самая популярная кухня; isMultyCusine - к-во кухонь в ресторане больше или столько же чем в среднем\ndf['isMostPopCusine'] = df['Cuisine Style'].apply(lambda x: 1 if most_popular_cusine in x else 0 )\ndf['isMultyCusine'] = df['Cuisines Count'].apply(lambda x: 1 if  x >= average_cousines_count else 0 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RevTimeDelta - время между review в днях\n\ndf['RevTimeDelta'] = df['Reviews'].apply(rev_time_delta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NewestReviewDate - время, прошедшее со момента последнего review до '22/02/2020'\ndf['NewestReviewDate'] = df['Reviews'].apply(lambda x: get_reviews(x)['reviews_dt'])\ndf['NewestReviewDate'] = df['NewestReviewDate'].apply(lambda x: sorted(x,reverse=True)[0] if len(x)!=0 else pd.NaT)\ndf['NewestReviewDate'] = df['NewestReviewDate'].fillna(dt.date(1970,1,1))\ndf['NewestReviewDate'] = df['NewestReviewDate'].apply(lambda x: (CURRENT_DATE.date()-x).total_seconds()//86400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['NewestReviewSeason'] = df['Reviews'].apply(lambda x: get_reviews(x)['reviews_dt'])\ndf['NewestReviewSeason'] = df['NewestReviewSeason'].apply(lambda x: sorted(x,reverse=True)[0] if len(x)!=0 else pd.NaT)\n#df['NewestReviewSeason'] = df['NewestReviewSeason'].fillna(dt.date(2020,2,22))\ndf['NewestReviewSeason'] = df['NewestReviewSeason'].apply(lambda x: get_season(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  'TxtReviewsCount' - к-во отзывов, не сильно улучшает результат, но пусть будут\ndf['TxtReviewsCount'] = df['Reviews'].apply(lambda x: len(get_reviews(x)['reviews_txt']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# К-во позитивных слов в представленных отзывах\ndf['PositiveWords'] = df['Reviews'].apply(lambda x: count_positive_words_proportion(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Список уникальных позитивных слов в представленных отзывах\ndf['PositiveWordsList'] = df['Reviews'].apply(lambda x: list_positive_words(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cusines_in_city={}\ncusines_count_in_city={}\nfor city_name, group in df.groupby('City'):\n\n    cusines = group['Cuisine Style'].apply(get_cuisines)\n   \n    cusines_list = list(itertools.chain.from_iterable(cusines))\n    #cusines_list = [x for x in cusines_list if x != 'Vegetarian Friendly'] # удаляем 'Vegetarian Friendly' - она \"забивает\" все результаты\n    cusines_in_city[city_name] = Counter(cusines_list)\n\nfor city_name in cusines_in_city.keys():\n    cusines_count_in_city[city_name] = len(cusines_in_city[city_name])\n\ndf['Cusines Count In City'] = df['City'].map(cusines_count_in_city)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Weighed Cuisines Count'] = df['Cuisines Count'] / df['Cusines Count In City']  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Most Common Cusine in City'] = df['City'].apply(lambda x: cusines_in_city[x].most_common(1)[0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cuisine Style'] = df.apply(lambda x: x['Cuisine Style'] if x['Cuisine Style NAN'] ==False else [x['Most Common Cusine in City']], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зависимость места ресторана в городе от населения\ndf['Weighed Rank by Population'] = df['Weighed Rank']  / df['Population'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PositiveWords in Reviews'] = df['PositiveWords'] / df['Number of Reviews']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Как часто в городе оставляют отзывы\ndf['NRP'] = df['Number of Reviews'] / df['Population']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ранг ресторана с учетом частоты отзывов в городе\ndf['WRR'] =  df['Weighed Rank']  *  df['NRP'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Relative Price Range'] = df['Price Range'] / df['Weighed Rank']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Средняя цена в городе\nprice_in_city_dict = df.groupby('City')['Price Range'].mean().to_dict()\ndf['Price in City'] = df['City'].map(price_in_city_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сокращаем список кухонь для анализа до N - основных, остальные Other - это почти всегда дает улучшение предсказания\nN=30 #!!!\n\ns = df['Cuisine Style'].apply(lambda x: get_cuisines(x))\nslist =[]\nfor x in s:\n    slist.extend(x)\ntopNcusines = set(pd.Series(slist).value_counts()[:N].index)  \ndf['Cuisine top N'] =df['Cuisine Style'].apply(lambda x: is_cuisine_top_N(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сетевой ID ресторана (похоже, что это ID франшизы, если повторяется более 1-го раза - isNetworkRestorant)\nimport warnings; warnings.simplefilter('ignore')\ndf['Restaurant_net_id'] = df['Restaurant_id'].apply(lambda x: x.split('_')[1])\nNetworkRestorants = df[df['Restaurant_net_id'].isin(df['Restaurant_net_id'].value_counts()[df['Restaurant_net_id'].value_counts()>2].index)]\nNetworkRestorants['isNetworkRestorant'] = True\ndf['isNetworkRestorant'] = NetworkRestorants['isNetworkRestorant']\ndf['isNetworkRestorant'] = df['isNetworkRestorant'].fillna(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Помечаем, входит ли город в N-top городов, если ДА, то пишем его назавние, если НЕТ -Other\ntop_Cityes = df['City'].value_counts()[0:10].index.to_list()\ndf['TopCityes'] = df.City.apply(lambda x: x if x in top_Cityes else 'Other_City')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Устранили NaN и сгенерили новые признаки(пока без dummies), посмотрим какие из них подходят ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Ranking', 'Rating', 'Number of Reviews', 'City', 'Price Range',\n        'Restaurant_id',  'Country',\n       'Restaurants Count', 'Population', 'Restaurants for Population',\n       'Weighed Rank',  'Cuisines Count',\n       'RevTimeDelta', 'NewestReviewDate',\n       'TxtReviewsCount', 'Weighed Rank by Population', 'PositiveWords','PositiveWords in Reviews', 'WRR', 'Price in City', 'Restaurant_net_id']].hist(figsize=(20, 20), bins=100);\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Собираем dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Собираем Dummies: city, price_range, country_range, Cuisine top N\n\ndff = pd.get_dummies(df['Cuisine top N'].apply(pd.Series).stack()).sum(level=0)\ndf_mcc = pd.get_dummies(df['Most Common Cusine in City'], prefix = 'MCC')\ndf_city = pd.get_dummies(df['City'], prefix = 'City Range')\n#df_city = pd.get_dummies(df['TopCityes'], prefix = 'City', dummy_na=True)\ndf_price_range = pd.get_dummies(df['Price Range'], prefix = 'Price Range') \ndf_country_range = pd.get_dummies(df['Country'], prefix = 'Country Range',) \n#df_season_range = pd.get_dummies(df['NewestReviewSeason'], prefix = 'Season',)\n\ndf['PositiveWordsList'] = df['PositiveWordsList'].fillna('NAN')\ndf_positive_words_range = pd.get_dummies(df['PositiveWordsList'].apply(pd.Series).stack(), dummy_na=False).sum(level=0)\n\n\ndf1 = pd.concat([df,dff], axis=1)\ndf1 = pd.concat([df1,df_city], axis=1)\ndf1 = pd.concat([df1,df_price_range], axis=1)\ndf1 = pd.concat([df1,df_country_range], axis=1)\ndf1 = pd.concat([df1,df_mcc], axis=1)\ndf1 = pd.concat([df1,df_positive_words_range], axis=1)\n\n#df1 = pd.concat([df1,df_season_range], axis=1)\n\n#df1 = pd.concat([df1,df_restid_range], axis=1)\n\ncols_cuisine_style = dff.columns\ncols_city = df_city.columns\ncols_price_range =  df_price_range.columns\ncols_country_range =  df_country_range.columns\ncols_mcc =  df_mcc.columns\ncols_positive_words = df_positive_words_range.columns\n#cols_season = df_season_range.columns\n#cols_restid_range =  df_restid_range.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Собираем признаки И Разбиваем датафрейм на части, необходимые для обучения и тестирования модели\ncolumns = [ 'isMultyCusine', 'Price Range NAN', 'Cuisine Style NAN', 'Number of Reviews NAN', 'Number of Reviews', 'Restaurants Count', 'Cuisines Count', 'RevTimeDelta',        'NewestReviewDate', 'PositiveWords',  'Weighed Rank', 'Ranking', 'Cusines Count In City', 'WRR', 'ID_TA', 'Weighed Rank min max', 'Price in City',  'Cuisine Style NAN', 'isCapital', 'Population', 'Restaurants for Population',  ]\n\n#columns = [ 'isMultyCusine', 'Price Range NAN', 'Cuisine Style NAN', 'Number of Reviews NAN', 'Number of Reviews', 'Restaurants Count', 'Cuisines Count', 'RevTimeDelta',        'NewestReviewDate', 'PositiveWords',  'Weighed Rank', 'Ranking', 'Weighed Rank by Population',  'Cusines Count In City', 'WRR', 'ID_TA', 'Weighed Rank min max', 'Price in City', 'Weighed Rank by Population',  ]\n\ncolumns.extend(cols_price_range.tolist())\ncolumns.extend(cols_cuisine_style.tolist())\ncolumns.extend(cols_city.tolist())\ncolumns.extend(cols_country_range.tolist())\n\ncolumns.extend(cols_positive_words.tolist())\n\n#Разбиваем датафрейм на части, необходимые для обучения и тестирования модели\nX = df1[df1.ForTrain][columns]\n\ny = df1[df1.ForTrain]['Rating']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Загружаем специальный инструмент для разбивки:\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Наборы данных с меткой \"train\" будут использоваться для обучения модели, \"test\" - для тестирования.\n# Для тестирования мы будем использовать 20% от исходного датасета.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nregr = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nregr.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = regr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_old = y_pred.copy()\ny_pred = round_of_rating(y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred),  metrics.mean_absolute_error(y_test, y_pred_old) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Вычисляем коэффициент детерминации:\nR_2 = metrics.r2_score(y_test, y_pred)\nprint(R_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(regr.feature_importances_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='barh');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Предсказываем рейтинги на датасете для предсказаний (ForTrain == False)\nX_submission = df1[df1.ForTrain == False][columns]\ny_pred_submission = round_of_rating(regr.predict(X_submission))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Формируем датасет с предсказаниями Restaurant_id -- Rating\nsubmission_df = pd.DataFrame()\nsubmission_df['Restaurant_id'] = df1[df1.ForTrain == False]['Restaurant_id']\nsubmission_df['Rating'] = y_pred_submission\nsubmission_df.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----------------\n# Проверяем годность предсказаний  \nРаспределения достаточно похожи - ОК!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Тренировочный датасет')\ndf[df.ForTrain].Rating.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Предсказания масштабированные в 4 раза')\nsubmission_df.Rating.value_counts()*4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сохраняем предсказания\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}