{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"65f924a3-05e1-17ea-493e-88c5d21db77c"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"301b0680-962d-dc0d-767d-d38e53c72b00"},"outputs":[],"source":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rn\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c3e2495-b681-232d-9d29-1b67c8162f03"},"outputs":[],"source":"#Acquire the data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ncombine = [train_df,test_df]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b358054-8ebc-842f-63b2-6c6fcdb305e6"},"outputs":[],"source":"# Analyze by describing data\nprint(train_df.columns.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"949889ad-0e39-e097-6f68-dfac3fa54b4f"},"outputs":[],"source":"# preview the data\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7727405f-a9b1-21e7-6a8c-03600d37b9b4"},"outputs":[],"source":"# preview the data \ntrain_df.tail()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"427e99a3-a127-c369-5211-8516bc9c15af"},"outputs":[],"source":"train_df.info()\nprint('_'*40)\ntest_df.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac9271bd-14d2-ab02-eafc-dc97b87166fe"},"outputs":[],"source":"train_df.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e0ea82b-5b64-c9ee-b4e3-ab5beac61043"},"outputs":[],"source":"# Assumptions based on analysis\n#--------------------------------------------------------------------------------#\n# Correlating - We want to know how well each feature correlates with survival\n# Some features may need to be dropped because they do not relate survival"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc6aeceb-3535-9002-82a1-3bf4d44d19ee"},"outputs":[],"source":"# Correcting \n#--------------------------------------------------------------------------------#\n# Ticket feature may be dropped from our analysis as contains high ratio of duplicates and may not correlate to survivalvalues\n# PassengerID may be dropped as it does not correlate to survival\n# Cabin feature may be dropped as it contains too many null values\n# Name feature needs to be dropped as well as well.\n# train_df = train_df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\n# test_df  = test_df.drop(['Name','Ticket','Cabin'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9c6a99e-2dc3-61a6-b184-8ca159dbccfa"},"outputs":[],"source":"# Age feature correlates to survival\n# We may want to complete the Embark feature as it may also correlate with survival \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6533aea-406f-acad-47a3-c7666190a455"},"outputs":[],"source":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c4ba812-084f-fea1-f06e-5cb7f52fd1f0"},"outputs":[],"source":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3cd19124-5ae6-999d-f569-25147fd001e9"},"outputs":[],"source":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39dd0c5e-91af-06a9-2b94-aa80258a828a"},"outputs":[],"source":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"933cac95-52ae-8c65-314b-6a3767df2249"},"outputs":[],"source":"# Analyze by visualizing data\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ba7ccbf-e7d4-73e3-0ac2-e32faccd4b99"},"outputs":[],"source":"# Correlating numerical and ordinal features\n# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ac0d2ae-ee29-4998-4108-9ca9ea14eca0"},"outputs":[],"source":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca709b76-1143-bb0a-c825-a76b038a15d9"},"outputs":[],"source":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd33913c-0161-4ec5-69e5-332a1dec71c1"},"outputs":[],"source":"# Wrangle data\n# Correcting by dropping features:\n# By dropping features we are dealing with fewer data points. Speup our note"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db805c7a-fe89-ec3f-a2ca-9c20dba60c72"},"outputs":[],"source":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98af2ee2-0979-03d4-568a-2b996f8aacca"},"outputs":[],"source":"# Creating new feature extracting from existing\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b02b92a-5ea4-9d68-2abb-a103d319ddc5"},"outputs":[],"source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71490bb9-9399-6c9e-6dd1-4f1846fdf124"},"outputs":[],"source":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59bb36ec-4969-49c9-7769-fe75cca8762b"},"outputs":[],"source":"# We can safely drop the Name feature from training and testing datasets\n# We also need the PassengerID feature in the training dataset\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2312fb79-319d-1a44-939e-2fc0f3f9c05a"},"outputs":[],"source":"# Converting a categorical feature\n# Now we can convert features which contain strings to numerical values\n# Doing so so will help us in achieving the feature completing the goal\n# Let's convert Sex feature to new feature called Gender where female=1 and male=0.\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74376104-9f2a-4473-dd02-9db52a467daf"},"outputs":[],"source":"\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ebe8911-e5a4-3309-de50-8d5541e6cccc"},"outputs":[],"source":"# Let's prepare an empty array to contain guessing Age based Pclass and Gender combinations\nguess_ages = np.zeros((2,3))\nguess_ages"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"affcc375-2505-7331-4dfa-a62baacddfb8"},"outputs":[],"source":"# Iterate over Sex and Pclass to calculate guessed values of Age for the six"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"849d3b4a-0522-c45a-4e8e-0b351367d9ba"},"outputs":[],"source":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc52f7fb-cdd9-3c1c-6303-c2fbc0d306e2"},"outputs":[],"source":"# Let us create Age bands and determine correlations with Survived.\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60381064-ab29-45a0-371b-a0daac35ddb7"},"outputs":[],"source":" # Let us replace Age with ordinals based on these bands.\n for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2dc74566-8336-472b-b921-5e1177d8e95a"},"outputs":[],"source":"# We can not remove the AgeBand feature.\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f363ab89-b91c-4d38-8339-f4fb466399af"},"outputs":[],"source":"# Create new feature combining existing features\n# Create new feature which combines Parch and SibSp\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abc31c92-d8ab-1a20-db6d-33099cc133ff"},"outputs":[],"source":"# Create another feature called isAlone\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e831c73-50f1-4fd9-7412-f4532d82ca1d"},"outputs":[],"source":"# Drop Parch , SibSp, and FamilySize features in favor of IsAlone\ntrain_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3436ceee-2622-4367-ca1b-6fbff52b8b05"},"outputs":[],"source":"# Create an artificial feature combining Pclass and Age\nfor dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6143242c-ddcc-7ff7-5e81-826eee18d415"},"outputs":[],"source":"# Completing a categorical feature\n# Embarkation features takes S,Q,C values based on port of embarkation\n# Our training dataset has two missing values\nfreq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3740c2e3-9850-1871-767a-2e1eb0d30b23"},"outputs":[],"source":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f66d047-b4c4-4237-6eb5-050c4357cb10"},"outputs":[],"source":"# we can now convert the EmbarkedFill feature by creating a new numeric Port feature\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfda5e02-f97d-022d-0ecf-c1cb1de273df"},"outputs":[],"source":"# Completing and converting numerical feature\n# We may also want round off the fare to two decimals as it represents currency\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70f6d959-d5ae-d1a2-7141-825c60da9577"},"outputs":[],"source":"# We cannot create FareBand\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"324742db-6495-5190-303b-4293f5353316"},"outputs":[],"source":"# Convert the Fare feature to ordinal values based on the FareBand.\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aaba0d03-d7bc-3ee5-20fe-fec576d686ce"},"outputs":[],"source":"# Test dataset\ntest_df.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a1140b9-69ac-6209-a2ec-81076e463f84"},"outputs":[],"source":" # Model, Predict and Solve\n    # Now we are ready to train a model and predict the required solution\n    # Our problem is classification and regression problem\n    # Now we can narrow down our models to a few. These inculde:\n    \n    # Logistic Regression\n    # KNN or k-Nearest Neighbors\n    # Support Vector Machines\n    # Naive Bayes classifier\n    # Decision Tree\n    # Random Forrest\n    # Perceptron\n    # Artificial neural network\n    # RVM or Relevance Vector Machine\n\nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70a7c894-f286-7c44-7bc5-086ef99fc589"},"outputs":[],"source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f219123-c5d3-b2af-c7fd-331a2ba5ff19"},"outputs":[],"source":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cbe01e5-3829-79b2-2ca1-791ef1c85f9e"},"outputs":[],"source":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"810aea07-fa47-aa52-22dd-1c4dd2a83a3b"},"outputs":[],"source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"609e07f4-e868-7447-facd-8cbcf0243423"},"outputs":[],"source":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ba460c4-d2b7-6738-1f2a-0f20029445c8"},"outputs":[],"source":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cae21ec6-e16e-d785-e752-c31b62407817"},"outputs":[],"source":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4477c14e-92c6-3ff7-32bc-b44a418962d8"},"outputs":[],"source":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ecc9f6d-bf8f-a263-10dd-491399ef22ee"},"outputs":[],"source":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"333df632-4911-10c3-f4e6-953c597abdce"},"outputs":[],"source":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30c9a076-ccc0-10d7-de06-522227b0be94"},"outputs":[],"source":"# Model Evaluation\n# We can rank our evaluation of all models to choose the best one for our problem\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e78992a-2090-4f7f-2920-30db37c2c090"},"outputs":[],"source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}