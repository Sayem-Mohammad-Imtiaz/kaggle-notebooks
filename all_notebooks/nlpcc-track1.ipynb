{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About this notebook\n主要采用五折","metadata":{}},{"cell_type":"code","source":"!pip install xlrd==1.2.0","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:14:46.933252Z","iopub.execute_input":"2021-06-12T08:14:46.933661Z","iopub.status.idle":"2021-06-12T08:14:56.103785Z","shell.execute_reply.started":"2021-06-12T08:14:46.933575Z","shell.execute_reply":"2021-06-12T08:14:56.102759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==3.5.0","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:14:56.106111Z","iopub.execute_input":"2021-06-12T08:14:56.10644Z","iopub.status.idle":"2021-06-12T08:15:07.664865Z","shell.execute_reply.started":"2021-06-12T08:14:56.106408Z","shell.execute_reply":"2021-06-12T08:15:07.663758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport jieba\nimport numpy as np \nimport tensorflow_addons as tfa\nfrom tensorflow_addons.optimizers import AdamW\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input,Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nimport copy\nimport seaborn as sn\nimport random\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt\nfrom string import digits, punctuation\nimport re\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom tensorflow.keras.models import Model\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom transformers import AutoModel\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom sklearn.metrics import classification_report\nimport logging\nimport csv\nfrom transformers import BertTokenizer,BertModel,BertConfig,BertForPreTraining,TFAutoModelWithLMHead\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.metrics import f1_score,confusion_matrix,precision_score,recall_score\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nimport pandas as pd\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T08:15:07.667585Z","iopub.execute_input":"2021-06-12T08:15:07.667885Z","iopub.status.idle":"2021-06-12T08:15:18.22255Z","shell.execute_reply.started":"2021-06-12T08:15:07.667856Z","shell.execute_reply":"2021-06-12T08:15:18.221473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformers.__version__","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:18.224584Z","iopub.execute_input":"2021-06-12T08:15:18.224977Z","iopub.status.idle":"2021-06-12T08:15:18.235058Z","shell.execute_reply.started":"2021-06-12T08:15:18.224934Z","shell.execute_reply":"2021-06-12T08:15:18.233831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n\n\nseed = 1024\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:18.236794Z","iopub.execute_input":"2021-06-12T08:15:18.237373Z","iopub.status.idle":"2021-06-12T08:15:18.248069Z","shell.execute_reply.started":"2021-06-12T08:15:18.237324Z","shell.execute_reply":"2021-06-12T08:15:18.246952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-06-12T08:15:18.249838Z","iopub.execute_input":"2021-06-12T08:15:18.250274Z","iopub.status.idle":"2021-06-12T08:15:18.259034Z","shell.execute_reply.started":"2021-06-12T08:15:18.250234Z","shell.execute_reply":"2021-06-12T08:15:18.258025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen     \n    )\n    \n    return np.array(enc_di['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:18.260562Z","iopub.execute_input":"2021-06-12T08:15:18.260979Z","iopub.status.idle":"2021-06-12T08:15:18.275397Z","shell.execute_reply.started":"2021-06-12T08:15:18.260941Z","shell.execute_reply":"2021-06-12T08:15:18.274536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n#使用五个token\ndef build_model(transformer, max_len=512):\n    \"\"\"\n    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(3, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n \n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=5e-6), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.05), metrics=['accuracy',tfa.metrics.F1Score(num_classes=3,average='weighted')])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:18.278043Z","iopub.execute_input":"2021-06-12T08:15:18.278356Z","iopub.status.idle":"2021-06-12T08:15:18.289881Z","shell.execute_reply.started":"2021-06-12T08:15:18.278329Z","shell.execute_reply":"2021-06-12T08:15:18.288667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#只使用最后一层的cls_token\n# def build_model(transformer, max_len=512):\n#     \"\"\"\n#     https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n#     \"\"\"\n#     input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n#     res = transformer(input_word_ids,output_hidden_states = True)\n#     #hideden_state一共12个，每一层的\n#     sequence_output, hidden_state = res[0],res[2]\n#     cls_token = sequence_output[:, 0, :]#最后一层的cls token\n#     out = Dense(2, activation='softmax')(cls_token)\n    \n#     model = Model(inputs=input_word_ids, outputs=out)\n#     model.compile(Adam(lr=5e-6), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.01), metrics=['accuracy',tfa.metrics.F1Score(num_classes=2,average='weighted')])\n    \n#     return model","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:18.291635Z","iopub.execute_input":"2021-06-12T08:15:18.291914Z","iopub.status.idle":"2021-06-12T08:15:18.303611Z","shell.execute_reply.started":"2021-06-12T08:15:18.291888Z","shell.execute_reply":"2021-06-12T08:15:18.302581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TPU Configs","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\n#调用kaggle上的tpu必要代码\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:18.305393Z","iopub.execute_input":"2021-06-12T08:15:18.305805Z","iopub.status.idle":"2021-06-12T08:15:24.198986Z","shell.execute_reply.started":"2021-06-12T08:15:18.305761Z","shell.execute_reply":"2021-06-12T08:15:24.198043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# 五折交叉验证\nkfold = KFold(n_splits=5, random_state=seed, shuffle=True)#五折交叉验证\n# Configuration\nEPOCHS = 1\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nMAX_LEN = 140\nuse_external1 = False\nuse_external2 = False\nuse_pseudo = False\nuse_valid = True\nDISPLAY_PLOT = True\n# MODEL = 'roberta-base'\n# # \"roberta-base\",\"roberta-large\",\"bert-base-uncased\",\"ernie-2.0-en\"","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:18:50.1294Z","iopub.execute_input":"2021-06-12T08:18:50.129755Z","iopub.status.idle":"2021-06-12T08:18:50.135402Z","shell.execute_reply.started":"2021-06-12T08:18:50.129725Z","shell.execute_reply":"2021-06-12T08:18:50.134566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlpcc-track1-dataset/train.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\ndf_train.columns = ['text_a', 'text_b', 'labels']\ndf_test = pd.read_csv('/kaggle/input/nlpcc-track1-dataset/test.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\ndf_test.columns = ['text_a', 'text_b', 'labels']","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:24.208205Z","iopub.execute_input":"2021-06-12T08:15:24.208639Z","iopub.status.idle":"2021-06-12T08:15:24.281156Z","shell.execute_reply.started":"2021-06-12T08:15:24.208594Z","shell.execute_reply":"2021-06-12T08:15:24.280106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#数据预处理\ndef fake_data_process(data):\n    data.insert(data.shape[1], 'content',\"\")\n    for i,label in enumerate(data['labels']):\n        if(data['labels'][i] == \"Against\"):\n            data['labels'][i] = 0\n        elif(data['labels'][i] == \"Support\"):\n            data['labels'][i] = 1\n        elif(data['labels'][i] == \"Neutral\"):\n            data['labels'][i] = 2\n        data['content'][i] = data['text_a'][i] +'。'+data['text_b'][i]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:24.282538Z","iopub.execute_input":"2021-06-12T08:15:24.282821Z","iopub.status.idle":"2021-06-12T08:15:24.289303Z","shell.execute_reply.started":"2021-06-12T08:15:24.282794Z","shell.execute_reply":"2021-06-12T08:15:24.288493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_data_process(df_train)\nfake_data_process(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:24.2907Z","iopub.execute_input":"2021-06-12T08:15:24.291272Z","iopub.status.idle":"2021-06-12T08:15:31.109012Z","shell.execute_reply.started":"2021-06-12T08:15:24.291239Z","shell.execute_reply":"2021-06-12T08:15:31.107864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.110559Z","iopub.execute_input":"2021-06-12T08:15:31.110987Z","iopub.status.idle":"2021-06-12T08:15:31.136035Z","shell.execute_reply.started":"2021-06-12T08:15:31.110945Z","shell.execute_reply":"2021-06-12T08:15:31.134915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight = {0: 3.1,1: 3.9,2:3.0}","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.137794Z","iopub.execute_input":"2021-06-12T08:15:31.138247Z","iopub.status.idle":"2021-06-12T08:15:31.142987Z","shell.execute_reply.started":"2021-06-12T08:15:31.138202Z","shell.execute_reply":"2021-06-12T08:15:31.142026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (use_valid):\n    df_train = pd.concat([df_train,df_test],ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.144288Z","iopub.execute_input":"2021-06-12T08:15:31.144587Z","iopub.status.idle":"2021-06-12T08:15:31.157878Z","shell.execute_reply.started":"2021-06-12T08:15:31.144557Z","shell.execute_reply":"2021-06-12T08:15:31.156796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.15927Z","iopub.execute_input":"2021-06-12T08:15:31.15991Z","iopub.status.idle":"2021-06-12T08:15:31.179894Z","shell.execute_reply.started":"2021-06-12T08:15:31.159857Z","shell.execute_reply":"2021-06-12T08:15:31.178677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fake_train1.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.181329Z","iopub.execute_input":"2021-06-12T08:15:31.181648Z","iopub.status.idle":"2021-06-12T08:15:31.18879Z","shell.execute_reply.started":"2021-06-12T08:15:31.181601Z","shell.execute_reply":"2021-06-12T08:15:31.187772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build datasets objects","metadata":{}},{"cell_type":"code","source":"def get_train_dataset(x_data,y_data):\n    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n    dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.shuffle(seed)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_valid_dataset(x_data,y_data):\n    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.190421Z","iopub.execute_input":"2021-06-12T08:15:31.190842Z","iopub.status.idle":"2021-06-12T08:15:31.201336Z","shell.execute_reply.started":"2021-06-12T08:15:31.190798Z","shell.execute_reply":"2021-06-12T08:15:31.200221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fake_train_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((x_fake_train, y_fake_train))\n#     .repeat()\n#     .shuffle(2048)\n#     .batch(BATCH_SIZE)\n#     .prefetch(AUTO)\n# )\n\n# fake_valid_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((x_fake_valid, y_fake_valid))\n#     .batch(BATCH_SIZE)\n#     .cache()\n#     .prefetch(AUTO)\n# )","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.202972Z","iopub.execute_input":"2021-06-12T08:15:31.203439Z","iopub.status.idle":"2021-06-12T08:15:31.21254Z","shell.execute_reply.started":"2021-06-12T08:15:31.203382Z","shell.execute_reply":"2021-06-12T08:15:31.211321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lrfn1(epoch):\n    LR_START = 0.00001\n    LR_MAX = 0.00005 \n    LR_MIN = 0.000001\n    LR_RAMPUP_EPOCHS = 6\n    LR_SUSTAIN_EPOCHS = 3\n    LR_EXP_DECAY = .4\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.213851Z","iopub.execute_input":"2021-06-12T08:15:31.214314Z","iopub.status.idle":"2021-06-12T08:15:31.224708Z","shell.execute_reply.started":"2021-06-12T08:15:31.214282Z","shell.execute_reply":"2021-06-12T08:15:31.223564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = [i for i in range(EPOCHS)]\ny_s = [lrfn1(x) for x in rng]\nprint(y_s)\nplt.plot(rng, y_s)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y_s[0], max(y_s), y_s[-1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.229106Z","iopub.execute_input":"2021-06-12T08:15:31.229788Z","iopub.status.idle":"2021-06-12T08:15:31.425294Z","shell.execute_reply.started":"2021-06-12T08:15:31.229737Z","shell.execute_reply":"2021-06-12T08:15:31.424307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lrfn2(epoch):\n    LR_START = 0.000005\n    LR_MIN = 0.000001\n    LR_MAX = 0.00005 \n    LR_MIN = 0.000001\n    LR_RAMPUP_EPOCHS = 6\n    LR_SUSTAIN_EPOCHS = 3\n    LR_EXP_DECAY = .4\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.426987Z","iopub.execute_input":"2021-06-12T08:15:31.427679Z","iopub.status.idle":"2021-06-12T08:15:31.435488Z","shell.execute_reply.started":"2021-06-12T08:15:31.427634Z","shell.execute_reply":"2021-06-12T08:15:31.434387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = [i for i in range(EPOCHS)]\ny_s = [lrfn2(x) for x in rng]\nprint(y_s)\nplt.plot(rng, y_s)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y_s[0], max(y_s), y_s[-1]))\nlr_warm_up = tf.keras.callbacks.LearningRateScheduler(lrfn2, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:15:31.436946Z","iopub.execute_input":"2021-06-12T08:15:31.437309Z","iopub.status.idle":"2021-06-12T08:15:31.596904Z","shell.execute_reply.started":"2021-06-12T08:15:31.43728Z","shell.execute_reply":"2021-06-12T08:15:31.595908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model into the TPU","metadata":{}},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"markdown","source":"First, we train on the subset of the training set, which is completely in English.","metadata":{}},{"cell_type":"code","source":"preds = []\nweights = []\nmodels = [\"hfl/chinese-roberta-wwm-ext-large\",\"hfl/chinese-roberta-wwm-ext-large\",\"hfl/chinese-roberta-wwm-ext-large\",\"hfl/chinese-roberta-wwm-ext-large\",\"hfl/chinese-roberta-wwm-ext-large\"]\nfor fold,(train,valid) in enumerate(kfold.split(df_train.content,df_train.labels)):\n    print('#### FOLD',fold+1)\n    x_train,x_valid,y_train,y_valid = df_train.content[train],df_train.content[valid]\\\n    ,df_train.labels[train],df_train.labels[valid]\n    if (use_pseudo):\n        print(\"use pseudo\")\n        x_train = pd.concat([x_train, df_pseudo_cleaned.tweet]).reset_index(drop=True)\n        y_train = pd.concat([y_train, df_pseudo_cleaned.label]).reset_index(drop=True)\n    if (use_external2):\n        x_train = pd.concat([x_train, fake_external2.tweet]).reset_index(drop=True)\n        y_train = pd.concat([y_train, fake_external2.label]).reset_index(drop=True)\n    \n    tokenizer = AutoTokenizer.from_pretrained(models[fold])\n    \n    x_fake_train = regular_encode(x_train,tokenizer, maxlen=MAX_LEN)\n    x_fake_valid = regular_encode(x_valid,tokenizer,maxlen=MAX_LEN)\n    \n\n    y_fake_train = to_categorical(y_train,3,dtype='int32')\n    y_fake_valid = to_categorical(y_valid,3,dtype='int32')\n    \n    train_dataset = get_train_dataset(x_fake_train,y_fake_train )\n    valid_dataset = get_valid_dataset(x_fake_valid,y_fake_valid)\n    \n    x_fake_test = regular_encode(df_test.content,tokenizer,maxlen=MAX_LEN)\n    y_fake_test = to_categorical(df_test.labels,3,dtype='int32')\n    \n    test_dataset = get_valid_dataset(x_fake_test,y_fake_test)\n    \n    n_steps = x_fake_train.shape[0] // BATCH_SIZE\n\n    # BUILD MODEL\n    K.clear_session()\n    if tpu:\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n    with strategy.scope():\n        transformer_layer = TFAutoModel.from_pretrained(models[fold])\n        model = build_model(transformer_layer, max_len=MAX_LEN)\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_f1_score', verbose=0, save_best_only=True,save_weights_only=True,\n        mode='max', save_freq= 'epoch' )\n    cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor = 'val_accuracy', factor = 0.1, patience = 2, verbose = 2, min_delta = 0.0001, mode = 'max')\n    if(fold == 1 or fold == 4 or  fold == 0 or fold == 2 or fold==3):\n        lr_warm_up = tf.keras.callbacks.LearningRateScheduler(lrfn1, verbose=1)\n    else:\n        lr_warm_up = tf.keras.callbacks.LearningRateScheduler(lrfn1, verbose=1)\n    \n#     x_fake_test = regular_encode(df_test.content,tokenizer,maxlen=MAX_LEN)\n#     y_fake_test = to_categorical(df_test.labels,3,dtype='int32')\n    train_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    callbacks = [cb_lr_schedule,sv,lr_warm_up],\n    validation_data= valid_dataset,\n    class_weight = class_weight,\n    epochs=EPOCHS)\n    \n    \n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)\n    \n        \n#     x_full_train = regular_encode(fake_train1.tweet, tokenizer, maxlen=MAX_LEN)\n    \n \n    x_fake_test = regular_encode(df_test.content,tokenizer,maxlen=MAX_LEN)\n    y_fake_test = to_categorical(df_test.labels,3,dtype='int32')\n    preds.append(model.predict(x_fake_test))\n    weights.append(model.evaluate(valid_dataset)[1])\n    model.evaluate(x=x_fake_test,y=y_fake_test,verbose=1)\n#     if DISPLAY_PLOT:\n#         plt.figure(figsize=(15,5))\n#         plt.plot(np.arange(EPOCHS),train_history.history['f1_score'],'-o',label='Train F1 Score',color='#ff7f0e')\n#         plt.plot(np.arange(EPOCHS),train_history.history['val_f1_score'],'-o',label='Val F1 Score',color='#1f77b4')\n#         x = np.argmax(train_history.history['val_f1_score']); y = np.max( train_history.history['val_f1_score'] )\n#         xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n#         plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max score\\n%.2f'%y,size=14)\n#         plt.ylabel('F1 Score',size=14); plt.xlabel('Epoch',size=14)\n#         plt.legend(loc=2)\n#         plt2 = plt.gca().twinx()\n#         plt2.plot(np.arange(EPOCHS),train_history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n#         plt2.plot(np.arange(EPOCHS),train_history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n#         x = np.argmin( train_history.history['val_loss'] ); y = np.min( train_history.history['val_loss'] )\n#         ydist = plt.ylim()[1] - plt.ylim()[0]\n#         plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n#         plt.ylabel('Loss',size=14)\n#         plt.title('FOLD %i'%(fold+1),size=18)\n#         plt.legend(loc=3)\n#         plt.show()  \n    model.save('fold-%i.h5'%fold) \n    del model","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:18:55.948519Z","iopub.execute_input":"2021-06-12T08:18:55.949022Z","iopub.status.idle":"2021-06-12T08:23:43.883387Z","shell.execute_reply.started":"2021-06-12T08:18:55.94899Z","shell.execute_reply":"2021-06-12T08:23:43.880977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weight_ensemble(weights,predictions):\n    weight_sum = np.sum(weights)  \n    prediction_sum = 0\n    for i in range(len(weights)):\n        prediction_sum += (weights[i]/weight_sum)*predictions[i]\n    print(prediction_sum)\n    np.savez('model_predict_weight',prediction_sum)\n    print(prediction_sum)\n    result = np.argmax(prediction_sum,axis=1)\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:17:54.349018Z","iopub.status.idle":"2021-06-12T08:17:54.349743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_ensemble(predictions):\n    result = np.argmax(np.mean(predictions,axis=0),axis=1)\n    np.savez('model_predict_mean',np.mean(predictions,axis=0))\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:14:05.555857Z","iopub.execute_input":"2021-06-11T09:14:05.556237Z","iopub.status.idle":"2021-06-11T09:14:05.561537Z","shell.execute_reply.started":"2021-06-11T09:14:05.556207Z","shell.execute_reply":"2021-06-11T09:14:05.560231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_result= weight_ensemble(weights,preds)\naccuracy_score(np.array(df_test.labels,dtype='int32'),weight_result)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:14:06.530551Z","iopub.execute_input":"2021-06-11T09:14:06.530937Z","iopub.status.idle":"2021-06-11T09:14:06.545197Z","shell.execute_reply.started":"2021-06-11T09:14:06.530892Z","shell.execute_reply":"2021-06-11T09:14:06.544277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_result","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:14:07.998808Z","iopub.execute_input":"2021-06-11T09:14:07.99918Z","iopub.status.idle":"2021-06-11T09:14:08.008684Z","shell.execute_reply.started":"2021-06-11T09:14:07.999151Z","shell.execute_reply":"2021-06-11T09:14:08.007507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_result = mean_ensemble(preds)\naccuracy_score(np.array(df_test.labels,dtype='int32'),mean_result)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:14:11.604091Z","iopub.execute_input":"2021-06-11T09:14:11.604454Z","iopub.status.idle":"2021-06-11T09:14:11.612605Z","shell.execute_reply.started":"2021-06-11T09:14:11.604422Z","shell.execute_reply":"2021-06-11T09:14:11.611626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_map = {0: 'Against', 1: 'Support', 2: 'Neutral'}\npred = [label_map[x] for x in weight_result]\n\nwith open('./submission.csv', 'w') as f:\n    for x in pred:\n        f.write(x+'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.10853Z","iopub.status.idle":"2021-06-11T08:19:21.109121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = np.argmax(preds[4],axis=1)\n# f1_score(np.array(fake_valid1.label,dtype='int32'),result,average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.109999Z","iopub.status.idle":"2021-06-11T08:19:21.110562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_submission(result):\n    submission = pd.DataFrame(columns=['id','label'])\n    submission.label = result\n    nlist = range(1,result.shape[0]+1)\n    submission.id = nlist\n    submission.to_csv('answer.txt', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.111381Z","iopub.status.idle":"2021-06-11T08:19:21.111875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_submission(weight_result)\n# make_submission(mean_result)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.112623Z","iopub.status.idle":"2021-06-11T08:19:21.113186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.DataFrame(columns=['id','label'])\n# submission.label = weight_result\n# nlist = range(1,weight_result.shape[0]+1)\n# submission.id = nlist\n# submission.to_csv('answer.txt', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.114002Z","iopub.status.idle":"2021-06-11T08:19:21.114544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# MODEL = \"lordtt13/COVID-SciBERT\"\n# tokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n\n# x_fake_valid = regular_encode(fake_valid1.tweet,tokenizer,maxlen=MAX_LEN)\n# x_fake_train = regular_encode(fake_train1.tweet,tokenizer,maxlen=MAX_LEN)\n\n\n\n# y_fake_train = to_categorical(fake_train1.label,2,dtype='int32')\n# y_fake_valid = to_categorical(fake_valid1.label,2,dtype='int32')\n\n# with strategy.scope():\n#     transformer_layer = TFAutoModel.from_pretrained(MODEL)\n#     model = build_model(transformer_layer, max_len=MAX_LEN)\n# model.summary()\n# n_steps = x_fake_train.shape[0] // BATCH_SIZE\n\n# train_dataset = get_train_dataset(x_fake_train,y_fake_train)\n# valid_dataset = get_valid_dataset(x_fake_valid,y_fake_valid)\n# cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n#         monitor = 'val_f1_score', factor = 0.5, patience = 3, verbose = 1, min_delta = 0.0001, mode = 'max')\n# sv = tf.keras.callbacks.ModelCheckpoint(\n#         'best_model.h5', monitor='val_f1_score', verbose=0, save_best_only=True,\n#         save_weights_only=True, mode='max', save_freq='epoch')\n# train_history = model.fit(\n#     train_dataset,\n#     steps_per_epoch=n_steps,\n#     callbacks = [cb_lr_schedule,lr_warm_up,sv],\n#     validation_data= valid_dataset,\n#     epochs=EPOCHS\n#     )\n# print('Loading best model...')\n# model.load_weights('best_model.h5')\n# x_fake_test = regular_encode(fake_valid1.tweet,tokenizer,maxlen=MAX_LEN)\n# score = model.evaluate(valid_dataset)[2]\n# pred = model.predict(x_fake_test)\n# np.savez('single-model',pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.115362Z","iopub.status.idle":"2021-06-11T08:19:21.115926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f1_score(np.array(fake_valid1.label,dtype='int32'),np.argmax(pred,axis=1),average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.116731Z","iopub.status.idle":"2021-06-11T08:19:21.117283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.DataFrame(columns=['id','label'])\n# submission.label = result\n# nlist = range(1,result.shape[0]+1)\n# submission.id = nlist\n# submission.to_csv('answer.txt', sep='\\t', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T08:19:21.118126Z","iopub.status.idle":"2021-06-11T08:19:21.118667Z"},"trusted":true},"execution_count":null,"outputs":[]}]}