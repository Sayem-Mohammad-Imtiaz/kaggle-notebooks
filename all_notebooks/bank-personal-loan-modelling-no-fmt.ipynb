{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as met\nimport sklearn.model_selection as model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df = pd.read_csv('/kaggle/input/bank-loan/Bank_Personal_Loan_Modelling.csv',error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"> <b>EDA</b> - Exploratory Data Analytics</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.apply(lambda val: sum(val.isnull()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"green\">Hence ensured no missing values</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"#07bdf5\"> Observation on given data\nusing info()</font>\n<ol>\n<li>All attributes contains numeric data, may be few are categorical</li>\n<li>All attributes are non-null, hence no missing values</li>\n<li>There are 13 attributes</li>\n</ol>\n<font color=\"#07bdf5\">Using describe()</font>\n<ol>\n<li>Categorical attributes are : Age, Zip,\n    <ul>\n        <li>Binary Categorical(5)</li>\n            <ol> \n                <li><font color=\"#11aaa1\">Personal Loan: Did this customer accept the personal loan offered in the last campaign?</font><font color=\"red\"> &lt;&lt;&lt;--this is the dependent / target attribute</font></li>\n                <li>Securities Account: Does the customer have a securities account with the bank?</li>\n                <li>CD AccountDoes the customer have a certificate of deposit (CD) account with the bank?</li>\n                <li>Online: Does the customer use internet banking facilities?</li>\n                <li>CreditCard: Does the customer use a credit card issued by Bank?</li>\n            </ol>\n        <li>Ordinal Categorical (2)</li>\n            <ol>\n                <li>Education: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional</li>\n                <li>Family: Family size of the customer</li>\n            </ol>\n    </ul>                \n</li>\n<li>Continuous attributes are (5): Experience, Income, CCAvg, Mortgages</li>\n<li> Nominal attributees are : Zip and ID (index column need not be consider, as it is not adding any value to the model) </li>\n    </ol>\n    <font color=\"#07bdf5\">Other observations</font>\n    <ul>\n    <li>Experieice min values is -3, must be a wrong data entry</li>\n    <li>Experience attaribute is normally distributed, where mean id 20, and First quantile(25%) 10, second quantile(50%) 20, third quantile(75%) is 30 and max is 43. Mean and Median is 20, Customer's experience ranges from no experience to max age of 43</li>\n    <li>Similarly analyzing Income Min 8, max 224, 50% is 64 75% is 98 - seems to be right skewed / positevely skewed</li>\n    <li>CCAve average spending min 0 max 10, mean 1.9, mean 1.9, 2nd quantile 1.5, 3rd quantile 2.5 majority data between 3rd and max, i.e., 2.5 to 10 - right skewed / positevely skewed </li>\n    </ul> \n</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=bank_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab as pl\npl.figure(figsize = (16,10))\n\nsns.heatmap(bank_df.corr(), annot=True, fmt='0.2f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"#07bdf5\">Other observations</font>\n<ul>\n    <li>CCAvg and Personal loan moderately corralated</li>\n    <li>Experience and Age are hily coralated</li>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df[[\"Experience\",\"Income\",\"CCAvg\", \"Mortgage\"]].apply(lambda x: pd.Series([(x < 0).sum(), (x > 0).sum(), (x==0).sum()]))\n# 0 - count of negative values\n# 1 - count of positive values\n# 1 - count of zero values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=bank_df.Family,y=bank_df.Income,hue=bank_df[\"Personal Loan\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><font color=\"#07bdf5\">More observations</font>\n    Attribute <b>Experience </b>has <font color=\"red\">52</font> negative values, probably data entry issue</p>\n<p>\n  <font color=\"blue\">  <ul>\n       <li>Take away for Business: Families with income more than USD100,000 are availed the personal loan during previous campaign, Families Income below $100,000 are less likely to avail personal loan </li>\n        </ul></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df[\"Personal Loan\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby([\"Personal Loan\"]).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\">ID column not adding value to the data, hence droping it</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df = bank_df.drop([\"ID\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2><font color=\"blue\"> Creation of Model</font></h2>\n<b>Itreation 1</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting training and testing data\nX_train, X_test = train_test_split(bank_df , test_size=0.3, random_state=1)\ny_train = X_train.pop(\"Personal Loan\")\ny_test = X_test.pop(\"Personal Loan\")\nmodelcrorecard={} #defining a dictionary to comparing different model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b> Logistic Regression</b> <font color=\"maroon\">using StatsModels</font></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_accuracy(confusion_matrix):\n    accuracy = (confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][0]+confusion_matrix[1][1])\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions( X_testLR1, logitmodel ):\n    y_pred_df = pd.DataFrame( { 'actual': y_test,\n                               \"predicted_prob\": logitmodel.predict( sm.add_constant( X_testLR1 ) ) } )\n    return y_pred_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\ndef do_statmodelLR(X_trainLR1,X_testLR1,y_trainLR1,y_testLR1):\n    logit = sm.Logit( y_trainLR1, sm.add_constant( X_trainLR1 ) )\n    logitmodel = logit.fit()\n    y_pred_bank_df = get_predictions(X_testLR1, logitmodel )\n    y_pred_bank_df[\"original\"] = np.array(y_test)\n    y_pred_bank_df['predicted'] = y_pred_bank_df.predicted_prob.map( lambda x: 1 if x > 0.6 else 0)\n    return y_pred_bank_df,logitmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y_df,lgmetric = do_statmodelLR(X_train,X_test,y_train,y_test)\ncon_matrix = confusion_matrix(pred_y_df.original,pred_y_df.predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ndef draw_conmmat( actual, predicted, modelname): #creating this as function as I am planning to reuse\n    cm = metrics.confusion_matrix( actual, predicted, [1,0] )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"Buy Personal Loan\", \"No Buy Personal Loan\"] , yticklabels = [\"Buy Personal Loan\", \"No Buy Personal Loan\"] )\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label - '+str(modelname))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_conmmat( pred_y_df.original, pred_y_df.predicted, \"Logistic Regression using StatsModels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc11 = calculate_accuracy(con_matrix)\nmodelcrorecard[\"LRstatmodel\"]=acc11\nprint(acc11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df.groupby([\"Personal Loan\"]).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\">Original data contains more number of 0's meaning - more number of customer not accepted the personal loan offered in the last campaign, so it will predit - not availed correctly</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total accuracy  (TP+TN)/(TP+FP+TN+FN)\naccuracy = (80+1343)/(80+69+1343+8)\nprint(\"Accuracy :\"+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>Doing LogisticRegression with <font color=\"color\">scikit sklearn,</font> instead of statsmodels.api</b><font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ndef do_LR(X_trainLR,y_trainLR,X_testLR,y_testLR):\n    clf = LogisticRegression(fit_intercept = False, C = 1e9) \n    #scikit-learn's logistic regression performs regularization by default. To negate setting high value for C\n    #did this and verified to verify whether scikit learn can do same as statsmodel\n    clf.fit(X_trainLR,y_trainLR) #Model\n    y_pred = clf.predict(X_testLR) #prediction\n    scikit_score = clf.score(X_testLR,y_testLR)\n    return scikit_score,y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LRScore, LRpred_labels = do_LR(X_train,y_train,X_test,y_test)\nprint(\"Score scikit learn Logistic Regression : \"+str(LRScore))\nmodelcrorecard[\"LRscikitlearn\"]=LRScore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,LRpred_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_conmmat(y_test,LRpred_labels, \"Logistic Regression using scikit learn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>KNN</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef do_KNN(X_train,y_train,y_test, K_value): #defining this as function as I have to reuse\n    NNH = KNeighborsClassifier(n_neighbors= K_value , weights = 'uniform', metric='euclidean' )\n    NNH.fit(X_train, y_train)\n    # For every test data point, predict it's label based on 5 nearest neighbours in this model. The majority class will \n    # be assigned to the test data point\n    KNNpredict = NNH.predict(X_test)\n    score = NNH.score(X_test, y_test)\n    return score,KNNpredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,21): #to find optimum K\n    KNNscore,KNNpredict = do_KNN(X_train,y_train,y_test,i)# applying K values from 1 to 20\n    print(\"When K value is \"+str(i)+\" score = \"+str(KNNscore))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"> When K value is 3 the score is 90% which is higher than other K values, hence the best K value for this model is 3</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying K value as 3\noptimum_kvalue = 3\nKNNscore,KNNpredict = do_KNN(X_train,y_train,y_test,optimum_kvalue)\nprint(\"When K value is \"+str(optimum_kvalue)+\" score = \"+str(KNNscore))\nprint(\"Confusion matrix for KNN is\")\nconf_matrix= confusion_matrix(y_test,KNNpredict)\nmodelcrorecard[\"KNN\"] = KNNscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_conmmat(y_test,KNNpredict,\"KNN\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calculate_accuracy(conf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_knn=(1333+21)/(1333+21+128+18) #Just to verify through manual calculation\nprint(\"KNN Accuracy=\"+str(acc_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>Naïve Bayes</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB \n\ndef do_naive_bayes(X_trainNB,y_trainNB,X_testNB,y_testNB):\n    \n    naive_model = GaussianNB()\n    naive_model.fit(X_trainNB, y_trainNB)\n\n    NBpredict = naive_model.predict(X_testNB)\n    NBscore = naive_model.score(X_testNB,y_testNB)\n    return NBscore,NBpredict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NBscore, NBpredicted_labels = do_naive_bayes(X_train,y_train,X_test,y_test)\nprint(\"Score in Naive Bayes : \",NBscore)\nmodelcrorecard[\"Naive Bayes\"]=NBscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confusion matrix for Naive Bayes : \")\nprint(y_test.shape,\"\\n\\n\",NBpredicted_labels.shape)\n#con_matrix = confusion_matrix(y_test.tolist(), NBpredicted_labels.tolist())\ndraw_conmmat(y_test,NBpredicted_labels,\"Naive Bayes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>Iteration 2</b></font>\n<h2>Removing 52 negative values in Experience column</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_df_corrected = bank_df.copy() #Take a copy of bank_df keeping the original data as is\nbank_df_corrected[bank_df_corrected[[\"Experience\"]] < 0] = -1 # filling negative values as NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After replacing negative values with NaN, if we check no non negative values\nbank_df_corrected[[\"Experience\"]].apply(lambda x: pd.Series([(x < 0).sum(), (x > 0).sum(), (x == 0).sum()]))\n# 0 - count of negative values\n# 1 - count of positive values\n# 1 - count of zero values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find mean of Experience with NaN\nbank_df_corrected[\"Experience\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill negative values with mean\nexp_array = bank_df_corrected[bank_df_corrected[\"Experience\"]!= -1][\"Experience\"]\nbank_df_corrected[\"Experience\"]=bank_df_corrected[\"Experience\"].replace(-1,exp_array.mean())\nprint(exp_array.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\">Earlier the mean was 20 now it has become 20.33 let us replace NaN with mean experieice and verify the mean again</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# After replacing negative values with NaN, if we check no non negative values\nbank_df_corrected[[\"Experience\"]].apply(lambda x: pd.Series([(x < 0).sum(), (x > 0).sum(), (x == 0).sum()]))\n# 0 - count of negative values\n# 1 - count of positive values\n# 1 - count of zero values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, val in modelcrorecard.items():\n    print(key,\"\",val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<table>\n<tr>\n    <td><b>Model</b></td>\n    <td><b>Score</b></td>\n</tr>\n<tr>\n<td>Logistic Regression-statmodel</td>\n<td>0.9453333333333334</td>\n</tr>\n<tr>\n<td>Logistic Regression-scikit learn</td>\n<td>0.9073333333333333</td>\n</tr>\n<tr>\n<td>KNN</td>\n<td>0.9006666666666666</td>\n</tr>\n<tr>\n<td>Naive Bayes</td>\n<td>0.884</td>\n</tr>\n</table>"},{"metadata":{},"cell_type":"markdown","source":"<font color=blue>Now no negative values in Experieince in all 5000 entries (4934+66)</font>"},{"metadata":{},"cell_type":"markdown","source":"<h2><font color=\"blue\">Itertion 2</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting training and testing data from the corrected Dataframe bank_df_corrected\nX_train, X_test = train_test_split(bank_df_corrected , test_size=0.3, random_state=1)\ny_train = X_train.pop(\"Personal Loan\")\ny_test = X_test.pop(\"Personal Loan\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>Logistic Regression using statsmodel api learn</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelcrorecard2={}\n\npred_y_df,lgmetric = do_statmodelLR(X_train,X_test,y_train,y_test)\ncon_matrix = confusion_matrix(pred_y_df.original,pred_y_df.predicted)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc1= calculate_accuracy(con_matrix)\nmodelcrorecard2[\"Logistic Regression-statmodel\"]=acc1\nprint(acc1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>Logistic Regression using scikit learn</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"LRScore, LRpred_labels = do_LR(X_train,y_train,X_test,y_test)\nprint(\"Score scikit learn Logistic Regression : \"+str(LRScore))\ncon_matrix2 = confusion_matrix(y_test,LRpred_labels)\nacc2 = calculate_accuracy(con_matrix2)\nprint(acc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelcrorecard2[\"Logistic Regression-scikit learn\"]=acc2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>KNN</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,20):\n    KNNscore,KNNpredict = do_KNN(X_train,y_train,y_test,i)# applying K values from 1 to 20\n    print(\"When K value is \"+str(i)+\" score = \"+str(KNNscore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimum_kvalue2 = 3 #selection an ODD value\nKNNscore,KNNpredict = do_KNN(X_train,y_train,y_test,optimum_kvalue2)\nprint(\"When K value is \"+str(optimum_kvalue2)+\" score = \"+str(KNNscore))\ncon_matrix3 = confusion_matrix(y_test,KNNpredict)\nacc3 = calculate_accuracy(con_matrix3)\nmodelcrorecard2[\"KNN\"] = acc3\nprint(acc3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\"><b>Naive Bayes</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"NBscore, NBpredicted_labels = do_naive_bayes(X_train,y_train,X_test,y_test)\nprint(\"Score in Naive Bayes : \",NBscore)\ncon_matrix4 = confusion_matrix(y_test,NBpredicted_labels)\nacc4 = calculate_accuracy(con_matrix4)\nmodelcrorecard2[\"Naive Bayes\"]=acc4\nprint(acc4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\">Printing all confusion matrix</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_conmmat(y_test,pred_y_df.predicted,\"Logistic Regression using StatModel\")\ndraw_conmmat(y_test,LRpred_labels,\"Logistic Regression using scikit learn\")\ndraw_conmmat(y_test,KNNpredict,\"KNN\")\ndraw_conmmat(y_test,NBpredicted_labels,\"NBpredicted_labels\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"blue\">Comparing Model scores between two iterations</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, val in modelcrorecard2.items():\n    print(key,\"\",val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='red'>-----------------------<br>\nIteration 1 Results:<br>\n-----------------------<br>\nLRstatmodel  0.9486666666666667<br>\nLRscikitlearn  0.9073333333333333<br>\nKNN  0.9026666666666666<br>\nNaive Bayes  0.884<br></font><font color=blue>\n----------------------<br>\nIteration 2 Results:<br>\n----------------------<br>\nLogistic Regression-statmodel  0.9466666666666667<br>\nLogistic Regression-scikit learn  0.9113333333333333<br>\nKNN  0.9026666666666666<br>\nNaive Bayes  0.884<br></font>"},{"metadata":{},"cell_type":"markdown","source":"<table>\n<tr>\n    <td><b>Model</b></td>\n    <td><b>Score<br>Iteration 1</b></td>\n    <td><b>Score<br>Iteration 2</b></td>\n</tr>\n<tr>\n<td>Logistic Regression-statmodel</td>\n    <td>94.86</td>\n<td>94.66</td>\n</tr>\n<tr>\n<td>Logistic Regression-scikit learn</td>\n    <td>90.73</td>\n<td>91.13</td>\n</tr>\n<tr>\n<td>KNN</td>\n    <td>90.27</td>\n<td>90.27</td>\n</tr>\n<tr>\n<td>Naive Bayes</td>\n    <td>88.4</td>\n<td>88.4</td>\n</tr>\n</table>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"background-color:black;\"><font color=\"yellow\">Inference & Conclusion</font></p>"},{"metadata":{},"cell_type":"markdown","source":"<p>Comparing above results among models, given the problem to compare the results among three classification models(Logistic, K-NN and Naïve Bayes), <font color=\"red\">Logistic Regression</font> statsmodel - seems to give better results for the given data set.</p>\n<p>Point here to note there is slight difference between StatsModel API and scikit learn Logistic Regrission for given dataset statsmodel api works / preforms better </p>\n<p>\n    Logistic Regression perform better as it <font color=\"red\">does not expect the dependent variable to be normally distrubuted,</font> in the give dataset the dependent variable \"Personal Loan\" is not normally distributed.</p><p>\n    Naive Bayes expects all attributes to be independent, here there are few highly dependent attributes were there like Income and Experience, Age and Experience, etc\n    </p>\n    <p>KNN is nonparametric where Logistic Regression is parametric, with limited attribute set Logistic Regression outperforms. However KNN is also equally good while comparing with LogisticRegression scikit learn package\n</p>\n    \n<p><font color=\"#07bdf5\">More observations</font>\n    Attribute <b>Experience </b>has <font color=\"red\">52</font> negative values, probably data entry issue - after replacing them with mean there is a slight increase in score of Logistic Regression using scikit learn from 90.73 to 91.13</p>\n<p>\n    <ul>\n        <li><font color=\"purple\">Take away for Business/Bank:</font> Families with income more than USD100,000 are availed the personal loan during previous campaign, Families Income below $100,000 are less likely to avail personal loan </li>\n        </ul>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}