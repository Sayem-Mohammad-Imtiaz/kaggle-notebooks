{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#@title Default title text\n# Import Libraries\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import chi2 , f_classif \nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n#----------------------------------------------------\n#reading data\ndata = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\n\ndata.describe()\n\n#X Data\nX = data.drop(['diagnosis'], axis=1, inplace=False)\nprint('X Data is \\n' , X.head())\nprint('X shape is ' , X.shape)\n\n#y Data\ny = data['diagnosis']\nprint('y Data is \\n' , y.head())\nprint('y shape is ' , y.shape)\n\n#----------------------------------------------------\n# Cleaning data\n\n'''\nimpute.SimpleImputer(missing_values=nan, strategy='mean’, fill_value=None, verbose=0, copy=True)\n'''\n\n\nImputedModule = SimpleImputer(missing_values = np.nan, strategy ='most_frequent')\nImputedX = ImputedModule.fit(X)\nX = ImputedX.transform(X)\n\n\n#X Data\nprint('X Data is \\n' , X[:570])\n\n#y Data\nprint('y Data is \\n' , y[:570])\n\n#----------------------------------------------------\n#Feature Selection by Percentile\nprint('Original X Shape is ' , X.shape)\nFeatureSelection = SelectPercentile(score_func = chi2, percentile=20) # score_func can = f_classif\nX = FeatureSelection.fit_transform(X, y)\n\n#showing X Dimension \nprint('X Shape is ' , X.shape)\nprint('Selected Features are : ' , FeatureSelection.get_support())\n\n#----------------------------------------------------\n#Normalizing Data\n\nscaler = Normalizer(copy=True, norm='l2') # you can change the norm to 'l1' or 'max' \nX = scaler.fit_transform(X)\n\n#showing data\nprint('X \\n' , X[:570])\nprint('y \\n' , y[:570])\n\n#----------------------------------------------------\n#Splitting data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)\n\n#----------------------------------------------------\n#Applying RandomForestClassifier Model \n\n'''\nensemble.RandomForestClassifier(n_estimators='warn’, criterion=’gini’, max_depth=None,\n                                min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0,\n                                max_features='auto’,max_leaf_nodes=None,min_impurity_decrease=0.0,\n                                min_impurity_split=None, bootstrap=True,oob_score=False, n_jobs=None,\n                                random_state=None, verbose=0,warm_start=False, class_weight=None)\n'''\n\nRandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=100,max_depth=2,random_state=33) #criterion can be also : entropy \nRandomForestClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(X_train, y_train))\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(X_test, y_test))\nprint('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)\nprint('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = RandomForestClassifierModel.predict(X_test)\ny_pred_prob = RandomForestClassifierModel.predict_proba(X_test)\nprint('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:570])\nprint('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:570])\n\n#----------------------------------------------------\n#Applying DecisionTreeClassifier Model \n\n'''\nsklearn.tree.DecisionTreeClassifier(criterion='gini’, splitter=’best’, max_depth=None,min_samples_split=2,\n                                    min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,\n                                    random_state=None, max_leaf_nodes=None,min_impurity_decrease=0.0,\n                                    min_impurity_split=None, class_weight=None,presort=False)\n'''\n\nDecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',max_depth=3,random_state=33) #criterion can be entropy\nDecisionTreeClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('DecisionTreeClassifierModel Train Score is : ' , DecisionTreeClassifierModel.score(X_train, y_train))\nprint('DecisionTreeClassifierModel Test Score is : ' , DecisionTreeClassifierModel.score(X_test, y_test))\nprint('DecisionTreeClassifierModel Classes are : ' , DecisionTreeClassifierModel.classes_)\nprint('DecisionTreeClassifierModel feature importances are : ' , DecisionTreeClassifierModel.feature_importances_)\nprint('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = DecisionTreeClassifierModel.predict(X_test)\ny_pred_prob = DecisionTreeClassifierModel.predict_proba(X_test)\nprint('Predicted Value for DecisionTreeClassifierModel is : ' , y_pred[:570])\nprint('Prediction Probabilities Value for DecisionTreeClassifierModel is : ' , y_pred_prob[:570])\n\n#----------------------------------------------------\n#Calculating Accuracy Score  : ((TP + TN) / float(TP + TN + FP + FN))\nAccScore = accuracy_score(y_test, y_pred, normalize=False)\nprint('Accuracy Score is : ', AccScore)\n\n#----------------------------------------------------\n#Calculating F1 Score  : 2 * (precision * recall) / (precision + recall)\n# f1_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None)\n\nF1Score = f1_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\nprint('F1 Score is : ', F1Score)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T22:54:55.974776Z","iopub.execute_input":"2021-07-17T22:54:55.975241Z","iopub.status.idle":"2021-07-17T22:54:57.68312Z","shell.execute_reply.started":"2021-07-17T22:54:55.975144Z","shell.execute_reply":"2021-07-17T22:54:57.68206Z"},"trusted":true},"execution_count":null,"outputs":[]}]}