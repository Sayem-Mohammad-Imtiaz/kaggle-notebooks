{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for plotting purposes \nfrom sklearn.cluster import KMeans #KMeans to be used\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null values, good %"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = data.iloc[:,[3,4]].values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only non categorical data are considerd. CustomerID is meaningless. Age should be treated differently. (You can be young and still be rich)\n\n"},{"metadata":{},"cell_type":"markdown","source":"### How many clusters for our K-Means ? ****"},{"metadata":{"trusted":true},"cell_type":"code","source":"#interia : Sum of squared distances of samples to their closest cluster center.\n#inertia = [] would be helpful to determine the nb of clusters to choose\ninertia = []\n\ndef get_k_means_opt(N) : \n    \n    for j in range(1,N) : \n        kmeans = KMeans(n_clusters=j, init='k-means++')\n        kmeans.fit_transform(X_data)\n        inertia.append(kmeans.inertia_)\n    return inertia\n        \ninertia = get_k_means_opt(20)\nplt.plot(range(1,20), inertia)\nplt.xlabel('Cluster nb')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last inflection point is for a number of clusters = 5.\nIt's the most economical choice here"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now a model could be built with a 5 clusters \nkmeans = KMeans(n_clusters=5, init='k-means++')\npredicted_clusters = kmeans.fit_predict(X_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing all the clusters \nfor k in range(5) : \n    plt.scatter(X_data[predicted_clusters == k, 0], X_data[predicted_clusters == k, 1] , label = 'Cluster '+str(k+1))\n    plt.xlabel('Annual Income in k$')\n    plt.ylabel('Spending Score')\n    plt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The simplest idea is to target the rich who have a high SS. The poor who want to look rich looks like an easy target too. Further data about their education could help the engineering hacking that should be implemented.\nSpamming their emails with mainstream non necessary gadgets and some local ads ala facebook should do it. \n"},{"metadata":{},"cell_type":"markdown","source":"### But what if we had to use categorical variables ? \nKmodes looks like the easiest anwser that come to mind. This article discusses the methods for clustering mixed datasets of categorical and continuous data : \nhttps://www.tomasbeuzen.com/post/clustering-mixed-data/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}