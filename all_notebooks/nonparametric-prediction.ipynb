{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Emergency-911 Probability Estimation and Methods Reports"},{"metadata":{},"cell_type":"markdown","source":"In this dataset, our **first objective** is to explore the dataset. See if there is and direct correlation between the emergency call location and the emergency type. This file contains the algorithm along with analysis on the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing main libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as st","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import data\n\ndf = pd.read_csv('../input/911.csv', low_memory=False)\ndf = df[['lat', 'lng', 'title']]\n\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since parameters for our prediction are **longitude and latitude** ve can ignore rest of the data for now. Let's plot a graph to see how our data is spread.\n\n**Instead of using all of the elements in \"title\" column and spreading our data thin, we downgraded our data to 3 main type:**\n- **Fire**\n- **EMS**\n- **Traffic**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets.samples_generator import make_blobs\n\n\nfor i in range (0, df.shape[0]):\n    df.iloc[i, 2] = df.iloc[i, 2][: df.iloc[i, 2].find(\":\")]\n    \ntemp = np.unique(df['title'])\ntypes = dict()\nfor type_id in range(0, len(temp)):\n    types[temp[type_id]] = type_id\n\ndel temp\n\ny = df.iloc[:, 2]\ny = y.replace(types)\n\nn_components = len(types)\nX, truth = make_blobs(n_samples=df.shape[0], centers=n_components)\nplt.scatter(df.iloc[:, 0], df.iloc[:, 1], s=50, c = y)\nplt.title(f\"Example of a mixture of {n_components} distributions\")\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Longtitude\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph above, **we cannot see any direct seperation of emergency type according to logtitude and latitude.** It makes finding the form of the density of this distribution not plausable. Instead of trying to find a form, we can try to find a probability depending on the calls around the input location.\n\nWith the approach we will be using **Nonparametric Methods**, we are assuming that calls from similar neighbourhood have similar emergency types. \n\nTypes of Nonparametric Methods we can use are:\n- Histogram Estimator\n- Naive Estimator\n- Kernel Estimator\n- K Nearest Neighbor Estimator\n\n### Case against Histogram Estimator:\nIn histogram estimator, we are dealing with fixed sized location boxes, we will refer these boxes as blocks. In the case of histogram estimator, we will select our neighbourhoods as prefixed sized blocks. Which means, model's prediction on new coordinates will only be effected by the points in that block. This approach may lead the model to not to use the closest points of the new call because they are not in the same block.\n\n### Case against Naive and Kernel Estimators:\nIn these estimation methods, we are dealing with a fixed radius around the new point. In **Naive**, effects of the neighbouring points are weighted binary (1 or 0); if it is in the circle it is 1, if not it is 0. In **Kernel**, effects of the neighbouring points are weighted much smoothly than Naive Estimators. The problem is, if there is no or little amount of neighbours in this points radius, we have very little or nothing to work on and we lost a lot of processing time.\n\n### Case against K Nearest Neighbor Estimator:\nIn K Nearest Neighbor Estimator, we are dealing with a prefixed amount of neighbouring points. Which means, model's prediction on new coordinates will only be effected by these points. If the new point is an outlier or the neighbours are scattered far away from each others, there can be kilometers of distance between the point and the neighbours. \n\n## Estimator Selection\nOut of these estimators, Naive Estimator is a good method to use. Neighbours we are using in this method are more likely to be related to our point rather than KNN and Histogram Estimator. Also, less costly than Kernel Estimator.\n**Down Below we implement olur Naive Estimator:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_estimator_evidence(neighbourhood, point, dataset):\n    '''\n        @arg neighbourhood: float\n        @arg point: Array\n        @arg dataset: pandas.DataFrame\n        \n        returns float\n        \n        neighbourhood:\n            Radius around the given point, given as a float. Too small of a value may cause evidence value to be 0.\n        point:\n            Point of interest.\n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n\n    '''\n    application_matrix = (point[0] - neighbourhood / 2) < dataset['lat']\n    application_matrix *= (point[0] + neighbourhood / 2) >= dataset['lat']\n    application_matrix *= (point[1] - neighbourhood / 2) < dataset['lng']\n    application_matrix *= (point[1] + neighbourhood / 2) >= dataset['lng']\n    \n    return ((sum(application_matrix) / dataset.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_estimator_likelihood(neighbourhood, point, dataset, emergency):\n    '''\n        @arg neighbourhood: float\n        @arg point: array\n        @arg dataset: pandas.DataFrame\n        @arg emergency: String\n        \n        @returns float\n                neighbourhood:\n            Radius around the given point, given as a float. Too small of a value may cause evidence value to be 0.\n        point:\n            Point of interest.\n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n        emergency:\n            Type of emergency occurence.\n    '''\n    emergency_set = dataset[dataset['title'] == emergency]\n    likelihood = naive_estimator_evidence(neighbourhood, point, emergency_set)\n    \n    return (likelihood)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prior(dataset, emergency):\n    '''\n        @arg dataset: pandas.DataFrame\n        @arg emergency: String\n        \n        @returns float\n        \n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n        emergency:\n            Type of emergency occurence.\n    '''\n    emergency_set = dataset[dataset['title'] == emergency]\n    prior = emergency_set.shape[0] / dataset.shape[0]\n    \n    return (prior)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_posterior(prior, likelihood, evidence):\n    '''\n        @arg prior: float\n        @arg likelihood: float\n        @arg evidence: float, evidence != 0\n        \n        @returns float\n        \n        prior:\n            Prior probability value of the posterior to be calculated.\n        likelihood:\n            Likelihood value of the posterior to be calculated.\n        evidence:\n            Evidence value of the posterior to be calculated.\n    '''\n    \n    posterior = prior * likelihood / evidence\n    return(posterior)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_estimator_posterior(neighbourhood, point, dataset, emergency):\n    '''\n        @arg neighbourhood: float\n        @arg point: Array\n        @arg dataset: pandas.DataFrame\n        @arg emergency: String\n        \n        @return float\n        \n        neighbourhood:\n            Radius around the given point, given as a float. Too small of a value may cause evidence value to be 0.\n        point:\n            Point of interest.\n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n        emergency:\n            Type of emergency occurence.\n    '''\n    \n    __evidence = naive_estimator_evidence(neighbourhood, point, df)\n    __likelihood = naive_estimator_likelihood(neighbourhood, point, df, emergency)\n    __prior = prior(df, emergency)\n    __posterior = normalized_posterior(__prior, __likelihood, __evidence)\n    \n    return __posterior","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Final **naive_estimator_posterior** calculates the probability of emergency=\"EMS: BACK PAINS/INJURY\" given point [40.2, -75.2] and neighbourhood koordinate radius of 1 unit. Other probabilities of this kind can be found like this."},{"metadata":{},"cell_type":"markdown","source":"## Improving Naive Estimator\nIf we leave or function like above, and given neighbourhood is too small, there can be situations which posterior is 0, just because we cannot reach to and other neighbor. We can add an expantion rate to our algorith so of probability is 0 we expand. If an event has never occured, expanding while probability is 0 can be dangerous, so we can limit our search with minimum and maximum boundaries of available data. Also we can show how wide of a search we are doing for better judment."},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_estimator_posterior_expandable(neighbourhood, point, dataset, emergency, expantion_rate=0.001):\n    '''\n        @arg neighbourhood: float\n        @arg point: Array\n        @arg dataset: pandas.DataFrame\n        @arg emergency: String\n        @arg expantion_rate: float, default=0.001\n        \n        returns Array[float, float]\n        \n        neighbourhood:\n            Radius around the given point, given as a float. Too small of a value may cause evidence value to be 0.\n        point:\n            Point of interest.\n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n        emergency:\n            Type of emergency occurence.\n        expantion_rate:\n            In case of not finding any available data points inside the given area, function expands its area\n            by the expantion_rate.\n    '''\n    \n    lat_boundaries = [dataset['lat'].min(), dataset['lat'].max()]\n    lng_boundaries = [dataset['lng'].min(), dataset['lng'].max()]\n    \n    __evidence = naive_estimator_evidence(neighbourhood, point, df)\n    \n    while (__evidence == 0):\n        if (lat_boundaries[0] > (point[0] - neighbourhood) and lat_boundaries[1] <= (point[0] + neighbourhood) and lng_boundaries[0] > (point[1] - neighbourhood) and lng_boundaries[1] <= (point[1] + neighbourhood)):\n            return [neighbourhood, 0]\n        \n        neighbourhood += expantion_rate\n        __evidence = naive_estimator_evidence(neighbourhood, point, df)\n    \n    __likelihood = naive_estimator_likelihood(neighbourhood, point, df, emergency)\n    __prior = prior(df, emergency)\n    __posterior = normalized_posterior(__prior, __likelihood, __evidence)\n    \n    while (__posterior == 0):\n        if (lat_boundaries[0] > (point[0] - neighbourhood) and lat_boundaries[1] <= (point[0] + neighbourhood) and lng_boundaries[0] > (point[1] - neighbourhood) and lng_boundaries[1] <= (point[1] + neighbourhood)):\n            return [neighbourhood, 0]\n        \n        neighbourhood += expantion_rate\n        __evidence = naive_estimator_evidence(neighbourhood, point, df)\n        __likelihood = naive_estimator_likelihood(neighbourhood, point, df, emergency)\n        __posterior = normalized_posterior(__prior, __likelihood, __evidence)\n\n    \n    return [__posterior, neighbourhood]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our function can search for at least one point until it reaches the edges of all corners."},{"metadata":{},"cell_type":"markdown","source":"## Kernel Estimator\nIf we want to take it one step further, we can use a non-binary weight function so further a neighbour get from the point, less effective it is. We can use Gaussian kernel for the weight function."},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ndef gaussian_kernel(x):\n    result = (1/math.sqrt(2 * math.pi)) * math.exp(-(x**2)/2)\n    return (result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this kernel only works with one random variable, we can find the euclidian distance between points and then call it with dividing it to radius to get a normalized value inside the circle, between 0 and 1.\n\nWe will rewrite our evidance, likelihood and posterior function to work with gaussian weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"def euclidian_dist(x, y):\n    '''\n        @arg x: pandas.Series\n        @arg y: pandas.Series\n        \n        @returns float\n    '''\n    result = math.sqrt((x[0] - y[0])**2 + (x[1] - y[1])**2)\n    return (result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kernel_estimator_evidence(neighbourhood, point, dataset):\n    '''\n        @arg neighbourhood: float\n        @arg point: pandas.Series\n        @arg dataset: pandas.DataFrame\n        \n        @return float\n        \n        neighbourhood:\n            Radius around the given point, given as a float. Too small of a value may cause evidence value to be 0.\n        point:\n            Point of interest.\n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n    '''\n    application_matrix = (point[0] - neighbourhood / 2) < dataset['lat']\n    application_matrix *= (point[0] + neighbourhood / 2) >= dataset['lat']\n    application_matrix *= (point[1] - neighbourhood / 2) < dataset['lng']\n    application_matrix *= (point[1] + neighbourhood / 2) >= dataset['lng']\n    \n    neighbors = dataset[application_matrix]\n    evidence = 0\n    \n    for i in range (0, neighbors.shape[0]):\n        item = neighbors.iloc[i, :2]\n        distance = euclidian_dist(point, item)\n        gaussian_weight = gaussian_kernel(distance)\n        evidence += gaussian_weight\n    return evidence / dataset.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kernel_estimator_likelihood(neighbourhood, point, dataset, emergency):\n    '''\n        @arg neighbourhood: float\n        @arg point: pandas.Series\n        @arg dataset: pandas.DataFrame\n        @arg emergency: String\n        \n        @return float\n        \n        neighbourhood:\n            Radius around the given point, given as a float. Too small of a value may cause evidence value to be 0.\n        point:\n            Point of interest.\n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n        emergency:\n            Type of emergency occurence.\n    '''\n    emergency_set = dataset[dataset['title'] == emergency]\n    likelihood = kernel_estimator_evidence(neighbourhood, point, emergency_set)\n    \n    return (likelihood)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kernel_estimator_posterior_expandable(neighbourhood, point, dataset, emergency, expantion_rate=0.001):\n    '''\n        @arg neighbourhood: float\n        @arg point: pandas.Series\n        @arg dataset: pandas.DataFrame\n        @arg emergency: String\n        @arg expantion_rate: float\n        \n        @return float\n        \n        neighbourhood:\n            Radius around the given point, given as a float. Too small of a value may cause evidence value to be 0.\n        point:\n            Point of interest.\n        dataset:\n            Available dataset to calculate the probability of @point occuring.\n        emergency:\n            Type of emergency occurence.\n        expantion_rate:\n            In case of not finding any available data points inside the given area, function expands its area\n            by the expantion_rate.\n    '''\n        \n    lat_boundaries = [dataset['lat'].min(), dataset['lat'].max()]\n    lng_boundaries = [dataset['lng'].min(), dataset['lng'].max()]\n    \n    __evidence = kernel_estimator_evidence(neighbourhood, point, df)\n    \n    while (__evidence == 0):\n        if (lat_boundaries[0] > (point[0] - neighbourhood) and lat_boundaries[1] <= (point[0] + neighbourhood) and lng_boundaries[0] > (point[1] - neighbourhood) and lng_boundaries[1] <= (point[1] + neighbourhood)):\n            return [neighbourhood, 0]\n        \n        neighbourhood += expantion_rate\n        __evidence = kernel_estimator_evidence(neighbourhood, point, df)\n    \n    __likelihood = kernel_estimator_likelihood(neighbourhood, point, df, emergency)\n    __prior = prior(df, emergency)\n    __posterior = normalized_posterior(__prior, __likelihood, __evidence)\n    \n    while (__posterior == 0):\n        if (lat_boundaries[0] > (point[0] - neighbourhood) and lat_boundaries[1] <= (point[0] + neighbourhood) and lng_boundaries[0] > (point[1] - neighbourhood) and lng_boundaries[1] <= (point[1] + neighbourhood)):\n            return [neighbourhood, 0]\n        \n        neighbourhood += expantion_rate\n        __evidence = kernel_estimator_evidence(neighbourhood, point, df)\n        __likelihood = kernel_estimator_likelihood(neighbourhood, point, df, emergency)\n        __posterior = normalized_posterior(__prior, __likelihood, __evidence)\n\n    \n    return [__posterior, neighbourhood]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.iloc[:, :2]\ny = df.iloc[:, 2]\ntrain, test = train_test_split(df, test_size=0.2, random_state=21)\n\nX_test = test.iloc[:, :2]\ny_test = test.iloc[:, 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate accuracy for Naive Estimator\n\nresult = []\ntemp = np.unique(df['title'])\n\nfor i in range(0, X_test.shape[0]):\n    best_posterior = 0\n    best_class = ''\n    for title in temp:\n        posterior = naive_estimator_posterior(0.001, X_test.iloc[i, :], train, title)\n        \n        if(posterior > best_posterior):\n            best_posterior = posterior\n            best_class = title\n    result.append(best_class)\n    \nprint(\"Accuracy for naive_estimator_posterior with 0.001 neighbour: \", sum(result == y_test) / len(result) * 100)\n                                              ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate accuracy for Extended Naive Estimator\n\nresult = []\ntemp = np.unique(df['title'])\n\nfor i in range(0, X_test.shape[0]):\n    best_posterior = 0\n    best_class = ''\n    for title in temp:\n        posterior = naive_estimator_posterior_expandable(0.001, X_test.iloc[i, :], train, title, expantion_rate=0.001)\n        \n        if(posterior[0] > best_posterior):\n            best_posterior = posterior[0]\n            best_class = title\n    result.append(best_class)\n    \nprint(\"Accuracy for naive_estimator_posterior_expandable with 0.001 neighbour: \", sum(result == y_test) / len(result) * 100)\n                                              ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Calculate accuracy for Gaussian Kernel Estimator\n\nresult = []\ntemp = np.unique(df['title'])\n\nfor i in range(0, X_test.shape[0]):\n    best_posterior = 0\n    best_class = ''\n    for title in temp:\n        posterior = kernel_estimator_posterior_expandable(0.001, X_test.iloc[i, :], train, title, expantion_rate=0.001)\n        \n        if(posterior[0] > best_posterior):\n            best_posterior = posterior[0]\n            best_class = title\n\n    result.append(best_class)\n    \nprint(\"Accuracy for kernel_estimator_posterior_expandable with 0.001 neighbour: \", sum(result == y_test) / len(result) * 100)\n                                              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since the algorithims above are not optimized for performance, only a percentage of the data set is used with 1/10000th precision on coordinates.**\n\n- **Naive Estimator** and **Extended Naive Estimator** gives similar accuracies. But Extended Naive Estimator can sometimes have a lower accuracy; because when it expands its radius, it has a chance to reach to a neighbour with no relation to the point of interest.\n\n- Even though **Gaussian Kernel Estimator** shares the expantion of its radius propery with the Extended Naive Estimator, it gives better results because it weights data with Gaussian Kernel which means closer to the point of interest, more relevant to the result and farther from the point of interest, less relevant to the result. This can correct some major issues in Estended Naive Estimator and fixes the problem of not finding and neighbours in a given radius with Naive Estimator.\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}