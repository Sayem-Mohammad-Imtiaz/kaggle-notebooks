{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities</h3>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as ex\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport re\nimport string\nfrom wordcloud import STOPWORDS,WordCloud\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\ndef rmse(y,y_hat):\n    return np.sqrt(mean_squared_error(y,y_hat))\n\nplt.rc('figure',figsize=(20,11))\nplt.rc('font',size=12)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Importation And Missing Value Assessment</h3>\n","metadata":{}},{"cell_type":"code","source":"a_data =pd.read_csv('/kaggle/input/amazon-top-50-bestselling-books-2009-2019/bestsellers with categories.csv')\na_data.head(3)\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text Preprocessing\na_data['Name'] = a_data['Name'].apply(lambda x: x.lower())\na_data['Name'] = a_data['Name'].apply(lambda x: x.translate(str.maketrans(' ', ' ', string.punctuation)))\nsid = SentimentIntensityAnalyzer()\na_data['Sentiment'] = a_data.Name.apply(lambda x:sid.polarity_scores(x))\na_data['Positive Sentiment'] = a_data.Sentiment.apply(lambda x: x['pos'])\na_data['Neutral Sentiment'] = a_data.Sentiment.apply(lambda x: x['neu'])\na_data['Negative Sentiment'] = a_data.Sentiment.apply(lambda x: x['neg'])\na_data['Compound Sentiment'] = a_data.Sentiment.apply(lambda x: x['compound'])\na_data.drop(columns=['Sentiment'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h1>\n","metadata":{}},{"cell_type":"code","source":"info = a_data.iloc[:,:-5].describe()\ninfo.loc['median'] = a_data.median()\ninfo.loc['skew'] = a_data.skew()\ninfo.loc['kurtosis'] = a_data.kurt()\n\ninfo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Numeric Variables Distributions</h3>\n","metadata":{}},{"cell_type":"code","source":"ax =sns.distplot(a_data['User Rating'],bins=15)\nax.set_title('Distribution of ratings across the dataset',fontsize=19)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(2,1,1)\nax =sns.distplot(a_data['Price'],color='red')\nax.set_title('Distribution of prices across the dataset',fontsize=19)\nplt.show()\nplt.subplot(2,1,2)\n#remove outliers\na_data = a_data.query('Price < 60')\nax =sns.distplot(a_data['Price'],color='red')\nax.set_title('Distribution of prices across the dataset (After Outlier Removal)',fontsize=19)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(2,1,1)\nax =sns.distplot(a_data['Reviews'],color='teal')\nax.set_title('Distribution of review counts across the dataset',fontsize=19)\nplt.show()\nplt.subplot(2,1,2)\na_data['Reviews'] =np.log(a_data['Reviews'])\nax =sns.distplot((a_data['Reviews']),color='teal')\nax.set_title('Distribution of review counts across the dataset',fontsize=19)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = ex.pie(a_data,names='Genre',title='Proportion Of Different Geners in Our Dataset',hover_data=['Genre'])\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Year Based Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"pivot = a_data.pivot_table(values='Price',columns='Year',index='Genre')\nplt.title('Prices Of Each Genre Over The Years')\nsns.heatmap(pivot,annot=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see that both genres even though they are in different price ranges they are similarly experiencing a decrease in their price over the years.","metadata":{}},{"cell_type":"code","source":"gby_year = a_data.groupby('Year').mean()\ngby_year_median = a_data.groupby('Year').median()\n\ntr1 = go.Scatter(x=gby_year.index,y=gby_year['Price'],name='Mean')\ntr2 = go.Scatter(x=gby_year_median.index,y=gby_year_median['Price'],name='Median')\n\n\nlayout = dict(title='Average Book Price Over The Years',yaxis_title='Mean Price',xaxis_title='Year')\nfig = go.Figure(data=[tr1,tr2],layout=layout)\n\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There is a clear trend, we can see that the average book price is decreasing with each year","metadata":{}},{"cell_type":"code","source":"tr1 = go.Scatter(x=gby_year.index,y=gby_year['Reviews'],name='Mean')\ntr2 = go.Scatter(x=gby_year_median.index,y=gby_year_median['Reviews'],name='Median')\n\nlayout = dict(title='Average Book Review Count Over The Years',yaxis_title='Mean Number Of Reviews',xaxis_title='Year')\ngo.Figure(data=[tr1,tr2],layout=layout)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There is a clear trend, we can see that the average review count increases with each year","metadata":{}},{"cell_type":"code","source":"tr1 = go.Scatter(x=gby_year.index,y=gby_year['User Rating'],name='Mean')\ntr2 = go.Scatter(x=gby_year_median.index,y=gby_year_median['User Rating'],name='Median')\n\nlayout = dict(title='Average Book User Rating Over The Years',yaxis_title='Mean Rating',xaxis_title='Year')\ngo.Figure(data=[tr1,tr2],layout=layout)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There is a clear trend, we can see that the average rating score increase with each year","metadata":{}},{"cell_type":"code","source":"print('There are -{}- unique number of authors in our dataset!'.format(len(a_data['Author'].unique())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Text Based Analysis</h3>\n","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(y=a_data['Author'].value_counts()[:10].index,x=a_data['Author'].value_counts()[:10].values,palette='mako')\nax.set_title('Top 10 Authors',fontsize=16)\nax.set_xlabel('Number Of Books in Our Dataset',fontsize=14)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(3,1,1)\nsns.scatterplot(x=a_data['Compound Sentiment'],y=a_data['Price'])\nplt.subplot(3,1,2)\nsns.scatterplot(x=a_data['Compound Sentiment'],y=a_data['Reviews'])\nplt.subplot(3,1,3)\nsns.scatterplot(x=a_data['Compound Sentiment'],y=a_data['User Rating'])\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can clearly see that there is no significant correaltion between the sentiment compounds and our numeric features (price, review counts, and rating)","metadata":{}},{"cell_type":"code","source":"plt.subplot(3,1,1)\nax =sns.distplot((a_data[a_data['Compound Sentiment']>0]['Price']),label='Positive Compound')\nax =sns.distplot((a_data[a_data['Compound Sentiment']<0]['Price']),label='Negative Compound')\nplt.legend()\nplt.show()\nplt.subplot(3,1,2)\nax =sns.distplot((a_data[a_data['Compound Sentiment']>0]['Reviews']),label='Positive Compound')\nax =sns.distplot((a_data[a_data['Compound Sentiment']<0]['Reviews']),label='Negative Compound')\nplt.legend()\nplt.show()\nplt.subplot(3,1,3)\nax =sns.distplot((a_data[a_data['Compound Sentiment']>0]['User Rating']),label='Positive Compound')\nax =sns.distplot((a_data[a_data['Compound Sentiment']<0]['User Rating']),label='Negative Compound')\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can clearly see that there is no significant change between the sentiment compounds and our numeric features (price, review counts, and rating) distribution","metadata":{}},{"cell_type":"code","source":"plt.imshow(WordCloud(width=900,height=600,stopwords=STOPWORDS).generate(' '.join(a_data.Name.values)))\nplt.axis('off')\nplt.title('Most Frequent Words In Our Book Names',fontsize=16)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Using The Book Name Feature as A Predictor For Price</h3>\n","metadata":{}},{"cell_type":"code","source":"NUMBER_OF_COMPONENTS = 180\n\nvectorizer = TfidfVectorizer()\nsp_matrix = vectorizer.fit_transform(a_data['Name'])\n\nsvd_truncer = TruncatedSVD(n_components=NUMBER_OF_COMPONENTS)\ndec_mat = svd_truncer.fit_transform(sp_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cu_sum = np.cumsum(svd_truncer.explained_variance_ratio_)\ntr1 = go.Scatter(x=np.arange(0,len(cu_sum)),y=cu_sum)\nlayout=dict(yaxis_title='Explained Variance',xaxis_title='# Of Components',title='Explained Variance Of Name Tfidf Matrix Using {} Components'.format(NUMBER_OF_COMPONENTS))\ngo.Figure(data=[tr1],layout=layout)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x,test_x,train_y,test_y = train_test_split(dec_mat,a_data['Price'],random_state=42)\n\n\nLR_Pipe = Pipeline(steps = [('model',LinearRegression())])\nLR_Pipe.fit(train_x,train_y)    \nRF_Pipe = Pipeline(steps = [('model',RandomForestRegressor(random_state=42))])\nRF_Pipe.fit(train_x,train_y)    \n\n\nlr_predictions = LR_Pipe.predict(test_x)\nrf_predictions = RF_Pipe.predict(test_x)\n\nplt.subplot(2,1,1)\nplt.title('Linear Regression Residual Plot')\nsns.residplot(lr_predictions,test_y)\n\nplt.subplot(2,1,2)\nplt.title('Random Forest Residual Plot')\nsns.residplot(rf_predictions,test_y)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will use the linear regression model for our meta feature, we see that in comparison to the random forest model it looks like the linear regression tends to show less heteroskedasticity.\n","metadata":{}},{"cell_type":"code","source":"L_Encoder = LabelEncoder()\nLR_Pipe.fit(dec_mat,a_data['Price'])\na_data['LR_Pred']  = RF_Pipe.predict(dec_mat)\na_data['Genre'] = L_Encoder.fit_transform(a_data['Genre'] )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Price Prediction Using Stacking And Random Forest Model</h3>\n","metadata":{}},{"cell_type":"code","source":"X = a_data[['Year','Genre','Compound Sentiment','LR_Pred']]\nY = a_data['Price']\nRF_Pipe.fit(X,Y)\n\nplt.title('Final Price Prediction Residuals sing Random Forest')\nax = sns.residplot(RF_Pipe.predict(X),Y)\ntextstr = f'RMSE: {np.round(rmse(RF_Pipe.predict(X),Y),2)}'\nprops = dict(boxstyle='round', facecolor='tab:blue', alpha=0.5)\nax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=29,\n        verticalalignment='top', bbox=props)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({\"Actual\":Y,'Prediction':RF_Pipe.predict(X)})\n\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Actual\"])),\n        y=output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"],\n        mode=\"markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'Book Name':a_data['Name'],'Year':a_data[\"Year\"],'Actual Price':a_data['Price'],'Predicted Price':RF_Pipe.predict(X)})\noutput.to_csv('Price_Prediction.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}