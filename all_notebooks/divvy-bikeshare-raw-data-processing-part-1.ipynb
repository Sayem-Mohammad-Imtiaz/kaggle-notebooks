{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def func_process_datasets(path, to_del, to_datetime, to_int, to_cat, lat_lon):\n    \n    try:\n\n        # Store strating time\n        process_start = datetime.now()\n\n        print('Processing {} ...\\n'.format(path))\n\n        # Load dataset into chuncks --------------------------------------------------------------------------------------------------------------------\n        chunck_reader = pd.read_csv(path, chunksize = 500000)\n\n        df_list = []\n        i = 0\n\n        for df in chunck_reader:\n            df_list.append(df) # Append every chunk into df_list \n            i = i + 1\n            print('Chunk: {} - length = {} - is processed and appended'.format(i, len(df)))\n\n        print('\\nAll chunks were processed and appended')\n        print('All datasets loaded successfully\\n\\n')\n\n        # Concat nested lists --------------------------------------------------------------------------------------------------------------------------\n        main_df = pd.concat(df_list, sort = False)\n        main_df.reset_index(drop = True, inplace = True)\n        print('Main dataframe is created by concatenating all chunks')\n\n        # Delete unwanted object to better memory usage\n        del chunck_reader, df, df_list\n        print('Release objects for better memory usage\\n\\n')\n\n        # Store the length of the dataframe\n        orig_df = len(main_df)  # original dataset length\n        print('Dataset length is: {}'.format(orig_df))\n\n        # Memory usage before optimization\n        bo = round(main_df.memory_usage(deep = True).sum() / 1000**3, 2)   # Convert bytes to GBs\n        print('DataFrame memory usage before optimization: {} GB\\n\\n'.format(bo))\n\n        # Check numeric features missing values ---------------------------------------------------------------------------------------------------------\n        main_df.loc[main_df.gender == -25, 'gender'] = np.nan\n        main_df.loc[main_df.dpcapacity_start == -25, 'dpcapacity_start'] = np.nan\n        main_df.loc[main_df.dpcapacity_end == -25, 'dpcapacity_end'] = np.nan\n        main_df.loc[main_df.windchill == -999.0, 'windchill'] = np.nan\n        main_df.loc[main_df.precipitation == -9999.0, 'precipitation'] = np.nan\n        main_df.loc[main_df.wind_speed == -9999.0, 'wind_speed'] = np.nan\n        main_df.loc[main_df.visibility == -9999.0, 'visibility'] = np.nan\n        main_df.loc[main_df.pressure == -9999.0, 'pressure'] = np.nan\n        main_df.loc[main_df.temperature == -9999.000000, 'temperature'] = np.nan\n        main_df.loc[main_df.dewpoint == -9999.0, 'dewpoint'] = np.nan\n\n        # Check categorical features missing values\n        main_df.loc[main_df.events == 'unknown', 'events'] = np.nan\n        main_df.loc[main_df.conditions == 'unknown', 'conditions'] = np.nan\n\n        # Create list of the columns to test ------------------------------------------------------------------------------------------------------------\n        numeric_cols = ['gender', 'dpcapacity_end', 'windchill', 'wind_speed', 'precipitation', 'visibility', 'pressure', 'temperature', 'dewpoint', 'events', 'conditions']\n\n        for col in numeric_cols:\n            missing_val = main_df[col].isnull().sum()\n            print('{} feature contains: {} missing values. {}% of the dataset'.format(col, missing_val, round((missing_val / orig_df) * 100, 2)))\n\n        # Drop unwanted columns -------------------------------------------------------------------------------------------------------------------------\n        main_df.drop(to_del, axis = 1, inplace = True)\n        print('\\n{} features were droppped successfully\\n\\n'.format(to_del))\n\n        # latitude_start: Assigning the 'Clark St & 9th St (AMLI)' LAT & LON for the missing values in latitude_start & longitude_start features ------------------------\n        main_df.loc[main_df.latitude_start.isnull(), 'latitude_start'] = 41.870815\n        main_df.loc[main_df.longitude_start.isnull(), 'longitude_start'] = -87.631248\n        print('Missing start LAT & LON fixed')\n\n        # latitude_end: Assigning the 'Clark St & 9th St (AMLI)' LAT & LON for the missing values in latitude_start & longitude_start features ------------------------\n        main_df.loc[main_df.latitude_end.isnull(), 'latitude_end'] = 41.870815\n        main_df.loc[main_df.longitude_end.isnull(), 'longitude_end'] = -87.631248\n        print('Missing end LAT & LON fixed\\n\\n')\n\n        # Fill nan values for specific features using the method = 'ffill'\n        #df.loc[:,['temperature', 'pressure', 'pressure']].fillna(method= 'ffill', inplace=True)\n        #print('['temperature', 'pressure', 'pressure'] features were treated\\n\\n')\n\n        # Drop rows where they contain Nan values and reset the indexes\n        main_df.dropna(inplace = True)\n        main_df.reset_index(drop = True, inplace = True)\n        print('Null values were dropped and indexes were reset successfully\\n\\n')\n\n        # Encode classes ------------------------------------------------------------------------------------------------------------------------------------------------\n        main_df.gender.replace({'Male': 0, 'Female': '1'}, inplace= True) \n        print('Gender feature encoded')\n\n        # Process usertype encode classes\n        main_df.usertype.replace({'Subscriber': 0, 'Customer': 1, 'Dependent': 3}, inplace= True)\n        print('UserType feature encoded\\n\\n')\n\n        # Converting datatypes ------------------------------------------------------------------------------------------------------------------------------------------\n        print('Processing Datatype conversion ...\\n')\n\n        # Process int8 columns\n        for col in to_int:\n            main_df[col] = main_df[col].astype('int8')\n            print('{} column processed...'.format(col))\n        print('Int columns processed successfully\\n')\n\n        # Process category columns\n        for col in to_cat:\n            main_df[col] = main_df[col].astype('category')\n            print('{} column processed...'.format(col))\n        print('Category columns processed successfully\\n')\n\n        # Print note\n        print('Processing Datetime features ...\\n')    \n\n        # Process datetime columns\n        for col in to_datetime:\n            main_df[col] = pd.to_datetime(main_df[col])\n            print('{} column processed...'.format(col))\n        print('Datetime columns processed successfully\\n\\n')\n\n        # Process LAT & LON columns and convert them to float32\n        for col in to_round4_latlon:\n            main_df[col] = main_df[col].apply(lambda x: round(x,3))\n            print('{} column processed...'.format(col))\n        print('4 decimal points LAT & LON processed successfully\\n')\n\n        # Create new trip duration feature ------------------------------------------------------------------------------------------------------------------------------\n        main_df['new_tripduration'] = main_df.stoptime - main_df.starttime\n        main_df.new_tripduration = main_df.new_tripduration.astype('timedelta64[s]') # To convert 00:05:00 datetime to seconds\n        main_df.new_tripduration = main_df.new_tripduration.astype('int16') # int8 will convert numbers and give negatove values, instead will use int16\n        main_df.drop('tripduration', axis = 1, inplace= True) # Drop column\n        print('[new_tripduration] feature created based on (stoptime - starttime) and the [tripduration] dropped')\n\n        # Remove records where trip duration is less than 300 seconds - (1,268,968 Rows)\n        main_df = main_df[(main_df.new_tripduration >= 300) & (main_df.new_tripduration <= 3600)] # Keep the data where trip duration is between 5 to 60 mins only.\n        main_df.reset_index(drop= True, inplace= True)\n        print('Data with trip duration between 5 to 60 mins kept\\n\\n')\n\n        # Remove 2013 and 2014 from the dataset \n        main_df = main_df.loc[main_df.starttime.dt.year > 2014]\n        main_df.sort_values(by = 'starttime', inplace = True)\n        main_df.reset_index(drop = True, inplace = True)\n        print('2013 and 2014 records dropped')\n        print('New dataset length is: {} records. {} records dropped\\n\\n'.format(len(main_df), orig_df - len(main_df)))\n\n        # Percentage of removed values ----------------------------------------------------------------------------------------------------------------------------------\n        print('Dropped values percentage: {}%'.format(round(((orig_df - len(main_df)) / orig_df) * 100, 1)))\n\n        # Memory usage before optimization ------------------------------------------------------------------------------------------------------------------------------\n        ao = round(main_df.memory_usage(deep = True).sum() / 1000**3, 2)   # Convert bytes to GBs\n        print('DataFrame memory usage after optimization: {} GB\\n\\n'.format(ao))\n\n        # Store ending time\n        process_end = datetime.now()\n\n        # Finish statement\n        print('Processing time: {}'.format(process_end - process_start))\n\n        return main_df\n\n    except:\n        print('Error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Prepare lists\nto_delete_cols   = ['trip_id', 'windchill', 'precipitation', 'dewpoint', 'from_station_id', 'to_station_id']\nto_datetime_cols = ['starttime','stoptime']\nto_category_cols = ['from_station_name','to_station_name','events','conditions']\nto_round4_latlon = ['latitude_start', 'longitude_start', 'latitude_end', 'longitude_end']\nto_int8_cols     = ['usertype', 'gender', 'dpcapacity_start', 'dpcapacity_end', 'tripduration', 'humidity', 'rain', 'dpcapacity_start', 'dpcapacity_end', 'temperature', 'pressure', 'visibility', 'wind_speed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = func_process_datasets( path = '../input/chicago-divvy-bicycle-sharing-data/data_raw.csv', \n                            to_del = to_delete_cols, \n                            to_datetime = to_datetime_cols,\n                            to_int = to_int8_cols,\n                            to_cat = to_category_cols,\n                            lat_lon = to_round4_latlon )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle('divvy_bikeshare_picklefile.pickle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}