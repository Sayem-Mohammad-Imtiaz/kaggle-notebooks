{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport optuna\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom sklearn import preprocessing\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\nimport sklearn\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-31T16:49:33.716049Z","iopub.execute_input":"2021-08-31T16:49:33.716409Z","iopub.status.idle":"2021-08-31T16:49:37.237751Z","shell.execute_reply.started":"2021-08-31T16:49:33.716333Z","shell.execute_reply":"2021-08-31T16:49:37.236922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing Blended Files Generated from :**\n## https://www.kaggle.com/snikhil17/tuning-blending-of-xgb-catboost-lightgbm/edit","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"../input/fin-stack/train_pred_XGB.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"../input/blending-n/train_pred_XGB1(1).csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"../input/fin-stack/train_pred_XGB3.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\ndf4 = pd.read_csv(\"../input/blending-n/train_pred_LGB(1).csv\")\ndf4.columns = [\"id\", \"pred_4\"]\ndf5 = pd.read_csv(\"../input/catboost-nik/train_pred_CatB.csv\")\ndf5.columns = [\"id\", \"pred_5\"]\n\ndf_test1 = pd.read_csv(\"../input/fin-stack/test_pred_XGB.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"../input/blending-n/test_pred_XGB1(1).csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"../input/fin-stack/test_pred_XGB3.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\ndf_test4 = pd.read_csv(\"../input/blending-n/test_pred_LGB(1).csv\")\ndf_test4.columns = [\"id\", \"pred_4\"]\ndf_test5 = pd.read_csv(\"../input/catboost-nik/test_pred_CatB.csv\")\ndf_test5.columns = [\"id\", \"pred_5\"]\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\ndf = df.merge(df4, on=\"id\", how=\"left\")\ndf = df.merge(df5, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test4, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test5, on=\"id\", how=\"left\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T17:22:01.145173Z","iopub.execute_input":"2021-08-31T17:22:01.145525Z","iopub.status.idle":"2021-08-31T17:22:04.554606Z","shell.execute_reply.started":"2021-08-31T17:22:01.145495Z","shell.execute_reply":"2021-08-31T17:22:04.553723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **HyperParamerter Tuning of XGBRegressor (Model 1)**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\",\"pred_5\"]\ndf_test = df_test[useful_features]\n\n\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n        reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n        reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n        subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n        \n        model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\",\n        n_estimators=9000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth)\n        \n        \n        model.fit(xtrain, ytrain, early_stopping_rounds=300,eval_set=[(xvalid, yvalid)],  verbose=1000)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","metadata":{"execution":{"iopub.status.busy":"2021-08-31T17:22:04.556308Z","iopub.execute_input":"2021-08-31T17:22:04.556692Z","iopub.status.idle":"2021-08-31T17:22:09.619123Z","shell.execute_reply.started":"2021-08-31T17:22:04.55665Z","shell.execute_reply":"2021-08-31T17:22:09.615263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n\n    params = {'learning_rate': 0.031354783240695655,\n 'reg_lambda': 0.004484898434507873,\n 'reg_alpha': 3.429535691214872e-07,\n 'subsample': 0.14371213599791083,\n 'colsample_bytree': 0.562816437109489,\n 'max_depth': 2}\n    \n    model = XGBRegressor(\n        n_jobs=-1,\n        \n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=9000,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"level1_test_pred_1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T17:24:14.755826Z","iopub.execute_input":"2021-08-31T17:24:14.756116Z","iopub.status.idle":"2021-08-31T17:24:22.43124Z","shell.execute_reply.started":"2021-08-31T17:24:14.75609Z","shell.execute_reply":"2021-08-31T17:24:22.430184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model 2 tuning (RandomForestRegressor)**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\n\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        params = {'n_estimators':trial.suggest_int(\"n_estimators\", 100,105),\n              'min_weight_fraction_leaf':trial.suggest_float(\"min_weight_fraction_leaf\", 0.01,0.05),\n                        'min_samples_leaf':trial.suggest_int(\"min_samples_leaf\", 2,7),\n                        'max_depth': trial.suggest_int(\"max_depth\", 55, 65),\n                        'min_samples_split': trial.suggest_int(\"min_samples_split\", 10,15),\n                        'n_jobs':-1,\n                        'max_features':'sqrt',\n                        'oob_score':False,\n                        'verbose':False,\n                        'random_state':7, \n                        'warm_start':False, 'bootstrap':True,\n                        'max_leaf_nodes' : None,\n                        'min_impurity_split':None}\n        \n        \n        \n        \n        model = RandomForestRegressor(**params)\n        model.fit(xtrain, ytrain)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","metadata":{"execution":{"iopub.status.busy":"2021-08-31T16:54:43.14344Z","iopub.execute_input":"2021-08-31T16:54:43.143797Z","iopub.status.idle":"2021-08-31T17:09:44.620409Z","shell.execute_reply.started":"2021-08-31T16:54:43.143756Z","shell.execute_reply":"2021-08-31T17:09:44.619664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model 2: RandomForestRegressor**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    params = {'n_estimators': 102,\n 'min_weight_fraction_leaf': 0.014050550614029426,\n 'min_samples_leaf': 3,\n 'max_depth': 64,\n 'min_samples_split': 13,'n_jobs':-1,\n                        'max_features':'sqrt',\n                        'oob_score':False,\n                        'verbose':False,\n                        'random_state':7, \n                        'warm_start':False, 'bootstrap':True,\n                        'max_leaf_nodes' : None,\n                        'min_impurity_split':None}\n    \n    model = RandomForestRegressor(**params)\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_2.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"level1_test_pred_2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T17:22:22.334571Z","iopub.execute_input":"2021-08-31T17:22:22.3349Z","iopub.status.idle":"2021-08-31T17:24:14.753386Z","shell.execute_reply.started":"2021-08-31T17:22:22.334854Z","shell.execute_reply":"2021-08-31T17:24:14.752546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tuning 3rd Model**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\n\n\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n        model = CatBoostRegressor(**cat_parameters_1,task_type = \"GPU\")\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-31T17:09:44.62199Z","iopub.execute_input":"2021-08-31T17:09:44.622336Z","iopub.status.idle":"2021-08-31T17:14:35.472933Z","shell.execute_reply.started":"2021-08-31T17:09:44.622295Z","shell.execute_reply":"2021-08-31T17:14:35.472185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model 3: CatBoost**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    params = {'iterations': 7126,\n 'learning_rate': 0.2433541796869192,\n 'l2_leaf_reg': 177,\n 'random_strength': 4.029534673251209,'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n    \n    model = CatBoostRegressor(**params, task_type = \"GPU\")    \n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_3\"]\nsample_submission.to_csv(\"level1_test_pred_3.csv\", index=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-31T17:24:22.433033Z","iopub.execute_input":"2021-08-31T17:24:22.433572Z","iopub.status.idle":"2021-08-31T17:24:41.430359Z","shell.execute_reply.started":"2021-08-31T17:24:22.433531Z","shell.execute_reply":"2021-08-31T17:24:41.429256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Model","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"./level1_train_pred_1.csv\")\ndf2 = pd.read_csv(\"./level1_train_pred_2.csv\")\ndf3 = pd.read_csv(\"./level1_train_pred_3.csv\")\n\ndf_test1 = pd.read_csv(\"./level1_test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"./level1_test_pred_2.csv\")\ndf_test3 = pd.read_csv(\"./level1_test_pred_3.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T17:24:41.435231Z","iopub.execute_input":"2021-08-31T17:24:41.435505Z","iopub.status.idle":"2021-08-31T17:24:44.536504Z","shell.execute_reply.started":"2021-08-31T17:24:41.435477Z","shell.execute_reply":"2021-08-31T17:24:44.535617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hpertuning Final Model","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_2\", \"pred_3\", \"pred_1\"]\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        model = CatBoostRegressor(**cat_parameters_1, task_type = \"GPU\")\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-31T17:25:17.568647Z","iopub.execute_input":"2021-08-31T17:25:17.569011Z","iopub.status.idle":"2021-08-31T17:29:36.298141Z","shell.execute_reply.started":"2021-08-31T17:25:17.568976Z","shell.execute_reply":"2021-08-31T17:29:36.297411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    cat_parameters_1 = {'iterations': 5958,\n 'learning_rate': 0.05235006201456179,\n 'l2_leaf_reg': 140,\n 'random_strength': 2.2544196494134883,\n                        'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n    model = CatBoostRegressor(**cat_parameters_1, task_type = \"GPU\")\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-31T17:30:37.69329Z","iopub.execute_input":"2021-08-31T17:30:37.693631Z","iopub.status.idle":"2021-08-31T17:30:59.668744Z","shell.execute_reply.started":"2021-08-31T17:30:37.693601Z","shell.execute_reply":"2021-08-31T17:30:59.66793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T17:32:24.568649Z","iopub.execute_input":"2021-08-31T17:32:24.569012Z","iopub.status.idle":"2021-08-31T17:32:25.192136Z","shell.execute_reply.started":"2021-08-31T17:32:24.568979Z","shell.execute_reply":"2021-08-31T17:32:25.191255Z"},"trusted":true},"execution_count":null,"outputs":[]}]}