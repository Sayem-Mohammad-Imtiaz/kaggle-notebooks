{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Predicting Audi Car Prices**\n<span id=\"0\"></span>\n1. [Overview](#1)\n1. [Importing Modules](#2)\n1. [Defining an Evaluation Table](#3)\n1. [Creating a Model Evaluation Function and Adjusted $R^{2}$ Function](#4)\n1. [Reading the Dataset](#5)\n1. [Take a quick look at the data set](#6)\n1. [Preprocessing and Visualize](#7)\n    * [Handling The Categorical Features](#8)\n    * [Visualizing](#9)\n    * [Scaling](#10)    \n    * [Splitting data into training set and test set](#11)\n1. [Linear Regression](#12)\n1. [Regularized Linear Models](#13)\n    * [Ridge Regression](#14)\n    * [Lasso Regression](#15)\n    * [Elastic Net](#16)\n1. [Polynomial Regression](#17)\n1. [Support Vector Machine (SVM)](#18)  \n1. [Decision Tree Regressor](#19)\n1. [Random Forest Regressor](#20)    \n1. [AdaBoost](#21)\n1. [Gradient Boosting](#22)\n1. [XGBoost (Extreme Gradient Boosting)](#23)\n1. [Voting Regressor](#24)\n1. [Evaluation Table](#25)\n1. [Conclusion](#26)\n1. [Resources](#27)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"1\"></span> **Overview**\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Welcome to my kernel.I'm trying to learn machine learning. I wanted to write a kernel. In this kernel, I used various regression models to predict Audi car prices. Also, I tried to explain briefly the models I used.\n\nIf you have a question or feedback, do not hesitate to write. Thanks ðŸ™‚\n\n<img src=\"https://carfromjapan.com/wp-content/uploads/2016/10/audi-tax-free-cars.jpg\" title=\"source: https://carfromjapan.com/\" />","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"2\"></span> **Importing Modules**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.metrics import r2_score,mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"3\"></span> **Defining an Evaluation Table**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will train many models. So I created a dataframe to better see the model evaluation metrics. This dataframe includes **Root Mean Squared Error (RMSE)**, **R-squared**, **Adjusted R-squared**, **mean of the R-squared values obtained by the k-Fold Cross Validation** and **mean of the Root Mean Squared Error (RMSE) values obtained by the k-Fold Cross Validation**,which are the important metrics to compare different models.Usually having a R-squared value closer to one and smaller RMSE means a better fit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation = pd.DataFrame({\n    \"Model\":[],\n    \"R2 Score (train)\":[],\n    \"Adjusted R2 Score (train)\":[],\n    \"R2 Score (test)\":[],\n    \"Adjusted R2 Score (test)\":[],\n    \"Root Mean Squared Error(RMSE) (train)\":[],\n    \"Root Mean Squared Error(RMSE) (test)\":[],\n    \"R2 Score (5-Fold Cross Validation)\":[],\n    \"Root Mean Squared Error(RMSE) (5-Fold Cross Validation)\":[]\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"4\"></span>**Creating a Model Evaluation Function and Adjusted $R^{2}$ Function**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The $R^{2}$ increases when the number of features increase. Therefore, we may need a stronger evaluation metric to compare different models. We can use Adjusted $R^{2}$ for this purpose. Adjusted $R^{2}$ only increases if adding the feature decreases MSE. It is therefore a better metric than $R^{2}$.\n\nSo,I created a function to calculate the $R^{2}$ score, adjusted $R^{2}$ score, and Root Mean Squared Error in the model's training set and test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate adjusted r2 score:\n# where  n  is the number of instances and  k  is the number of features.\ndef adjustedR2(r2_score,n,k):\n    return 1-(((1-r2_score)*(n-1))/(n-k-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Evaluation Function\n\ndef evaluateModel(model,X_train,X_test,y_train,y_test,model_name):\n    \n    if(model_name==\"Polynomial Regression\"):\n        n_train=X_train.shape[1]\n        n_test=X_test.shape[1]\n    else:\n        n_train=len(X_train.columns)\n        n_test=len(X_test.columns)\n        \n    y_predict_test = model.predict(X_test)\n    y_predict_train = model.predict(X_train)\n    \n    r2_score_train = float(format(r2_score(y_train,y_predict_train),'.3f'))\n    \n    r2_score_test = float(format(r2_score(y_test,y_predict_test),'.3f'))\n    \n    rmse_train = float(format(np.sqrt(mean_squared_error(y_train,y_predict_train)),'.3f'))\n    \n    rmse_test = np.sqrt(mean_squared_error(y_test,y_predict_test))\n    \n    ad_r2_score_train = float(format(adjustedR2(r2_score_train,X_train.shape[0],n_train),'.3f'))\n    \n    ad_r2_score_test = float(format(adjustedR2(r2_score_test,X_test.shape[0],n_test),'.3f'))\n                              \n    r2_score_mean = float(format(cross_val_score(model,X_train,y_train,cv=5).mean(),'.3f'))\n                              \n    rmse_mean = -float(format(cross_val_score(model,X_train,y_train,cv=5,scoring=\"neg_root_mean_squared_error\").mean(),'.3f'))\n    \n    r = evaluation.shape[0]\n    evaluation.loc[r]=[model_name,\n                       r2_score_train,ad_r2_score_train,\n                       r2_score_test,ad_r2_score_test,\n                       rmse_train,rmse_test,\n                      r2_score_mean,\n                       rmse_mean]\n    \n    return evaluation.sort_values(by = 'Root Mean Squared Error(RMSE) (5-Fold Cross Validation)', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"5\"></span> **Reading the Dataset**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# read and load data\ndf = pd.read_csv(\"../input/used-car-dataset-ford-and-mercedes/audi.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"6\"></span> **Take a quick look at the data set**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"7\"></span>**Preprocessing and Visualize**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I've done some visualization and preprocessing in this section.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I created a copy of df so that my operations do not affect the actual data set:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2=df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"8\"></span>**Handling The Categorical Features**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Most machine learning algorithms operate with numbers. Therefore, it is necessary to convert categorical features into numbers. For this purpose, I convert categorical features to numbers using the pd.get_dummies() method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_2=pd.concat((df_2,pd.get_dummies(df_2[\"model\"]),pd.get_dummies(df_2[\"transmission\"]),pd.get_dummies(df_2[\"fuelType\"]))\n               ,axis=1)\n\ndf_2.drop([\"model\",\"transmission\",\"fuelType\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## <span id=\"9\"></span>Visualizing\n #### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# correlation\n\nplt.figure(figsize=(30,30))\nsns.heatmap(df_2.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the correlation matrix, there is a very strong relationship between diesel and petrol, let's remove one of the them:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2.drop(\"Petrol\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#  Count plot on fuel type\n\nfig, ax1 = plt.subplots(figsize=(5,4))\ngraph = sns.countplot(ax=ax1,x='fuelType', data=df)\ngraph.set_xticklabels(graph.get_xticklabels())\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height/2,height ,ha=\"center\",fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the count plot on fuel type,let's delete the hybrid model because it is very few:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2.drop(\"Hybrid\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Count plot on the transmission\n\nfig, ax1 = plt.subplots(figsize=(5,4))\ngraph = sns.countplot(ax=ax1,x='transmission', data=df)\ngraph.set_xticklabels(graph.get_xticklabels())\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height/2,height ,ha=\"center\",fontsize=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Pairplotting\n\nsns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I deleted the values I considered as outlier:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2.drop(index=df_2[df_2[\"mileage\"]>160000].index,inplace=True)\ndf_2.drop(index=df_2[df_2[\"year\"]<2000].index,inplace=True)\n\ndf_2.drop(index=df_2[df_2[\"engineSize\"]==0].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"10\"></span>**Scaling**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If the scales of the features are very different, some features may suppress other features. This can adversely affect the performance of the model. It is therefore necessary to scale the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stdn_scaler = StandardScaler().fit(df_2[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])\n\ndf_2[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]] = stdn_scaler.transform(df_2[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"11\"></span> Splitting data into training set and test set\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = train_test_split(df_2,test_size=0.25,random_state=42)\n\nX_train = train.drop(\"price\",axis=1)\ny_train = train[\"price\"]\n\nX_test = test.drop(\"price\",axis=1)\ny_test = test[\"price\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"12\"></span>Linear Regression\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Examining the linear relationship between 2 or more numerical variables is called linear regression. \n<br>\nInput variables are called independent variables(or features), and result variables are called dependent variables(or target).\n<br>\nUsually inputs are matrix, outputs are vector.\n<br>\nIf linear regression is created using an independent variable, it is called simple linear regression. If linear regression is created using two or more independent variables, it is called multiple linear regression.\n\n<b>Simple Linear Regression Equation</b>\n$$Y=\\beta_{0}+\\beta_{1}x_{1}$$\n\n<b>Multiple Linear Regression Equation</b>\n$$Y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{n}x_{n}$$\n\nLinear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term).\n\n<b>Linear Regression model prediction</b>\n\n$$\\hat{y}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$\n\nIn this equation:\n* $\\hat{y}$ is the predicted value.\n* $n$ is the number of features.\n* $x_{i}$  is the $i^{th}$ feature value\n* $\\theta_{j}$ is the $j^{th}$  model parameter (including the bias term $Î¸_{0}$ and the feature weights  $Î¸_{1}$ ,  $Î¸_{2}$ , â‹¯,  $Î¸_{n}$ ).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thatâ€™s the Linear Regression model but how do we train it? Training a model means setting its parameters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data.Most\ncommon performance measure of a regression model is the Root Mean Square Error (RMSE).Therefore, to train a Linear Regression model, we need to find the value of $\\theta$ that minimizes the RMSE.\n\n<b>Root Mean Square Error (RMSE)</b>\n$$RMSE(X, h) = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(\\theta^{T}x^{(i)}-y^{(i)})^{2}}$$\n\nWhen we find the model parameters that will minimize the RMSE, we train the model.So how do we find the model parameters that minimize the RMSE? Approaches for this purpose(This approachs will not be covered in this kernel.):\n* Normal Equation\n* Singular Value Decomposition (SVD)\n* Gradient Descent","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's create a linear regression model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created the linear regression model. Now let's evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(lin_reg,X_train,X_test,y_train,y_test,\"Linear Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"13\"></span>Regularized Linear Models\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A good way to reduce overfitting is to regularize the model.For a linear model, regularization is typically achieved by constraining the weights of the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"14\"></span>Ridge Regression\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ridge Regression (also called Tikhonov regularization) is a regularized version of Linear Regression:a regularization term equal to $\\alpha\\sum_{i=i}^{n}\\theta_{i}^{2}$  is added to the cost function. \n<br>\nThe purpose of Ridge Regression is to keep model weights as small as possible.\n<br>\nThe hyperparameter $\\alpha$ controls how much you want to regularize the model:\n* if $\\alpha=0$,then Ridge Regression is just Linear Regression.  \n* if $\\alpha$  is very large, then all weights end up very close to zero.\n\n<b>Ridge Regression cost function</b>\n$$J(\\theta)=MSE(\\theta)+\\alpha\\frac{1}{2}\\sum_{i=1}^{n}\\theta_{i}^2$$","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Important Warning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is important to scale the data before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.\n\n-----------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's create a ridge regression model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge_reg=Ridge().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created the model. Now let's evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(ridge_reg,X_train,X_test,y_train,y_test,\"Ridge Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"15\"></span>Lasso Regression\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Least Absolute Shrinkage and Selection Operator Regression (usually simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function.\n\n<b>Lasso regression retularization term</b>\n$$|\\theta_{i}|$$\n\n<b>Lasso Regression cost function</b>\n$$J(\\theta)=MSE(\\theta)+\\alpha\\sum_{i=1}^{n}|\\theta_{i}|$$\n\nAn important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero). ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's create a lasso regression model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created the model. Now let's evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(lasso_reg,X_train,X_test,y_train,y_test,\"Lasso Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"16\"></span>Elastic Net\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lassoâ€™s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression.\n\n<b>Elastic Net cost function</b>\n$$J(\\theta)=MSE(\\theta)+r\\alpha\\sum_{i=1}^{n}|\\theta_{i}|+\\frac{1-r}{2}\\alpha\\sum_{i=1}^{n}\\theta_{i}^2$$","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's create a Elastic Net regression model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nelastic_reg = ElasticNet().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created the model. Now let's evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(elastic_reg,X_train,X_test,y_train,y_test,\"Elastic Net Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try a few parameters for Elastic Net regression with GridSearch and choose the best model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params={\"alpha\":[0.0001,0.001,0.01,0.1,1,10,100,1000,10000],\"max_iter\":[1000,2500,5000,7500,10000],\n       \"l1_ratio\":[0.3,0.4,0.5,0.6,0.7]}\n\nelastic_gs=GridSearchCV(ElasticNet(random_state=42),param_grid=params,cv=5,scoring=\"neg_root_mean_squared_error\")\nelastic_gs.fit(X_train,y_train)\n\nelastic_reg_best=elastic_gs.best_estimator_.fit(X_train,y_train)\n\nevaluateModel(elastic_reg_best,X_train,X_test,y_train,y_test,\"Best Elastic Net Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"17\"></span>Polynomial Regression\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the linear models, the main idea is to fit a straight line to our data.What if your data is more complex than a straight line? Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called **Polynomial Regression**.\nWhile using polynomial transformation and deciding to degree, we should be very careful because it migh cause overfitting. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have 35 features. Since our new feature number for degree = 3 will be 8435 and this process will take a long time, I used the selected features instead of using all the features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Note","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"PolynomialFeatures(degree=d) transforms an array containing n features into an array containing $\\frac{(n+d)!}{d!n!}$ features, where n is the number of samples. \n<br>\nImplementation with scipy:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.special import factorial\nfactorial(38)/(factorial(35)*factorial(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\n\nX_train_poly = poly_features.fit_transform(X_train[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])\nX_test_poly= poly_features.fit_transform(X_test[[\"year\",\"mileage\",\"tax\",\"mpg\",\"engineSize\"]])\n\npoly_reg=LinearRegression()\npoly_reg.fit(X_train_poly,y_train)\n\nevaluateModel(poly_reg,X_train_poly,X_test_poly,y_train,y_test,\"Polynomial Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"18\"></span>Support Vector Machine (SVM)\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection.\n<br>\n<br>\nLinear SVM Regression tries to fit as many instances as possible on the street(represented by the parallel dashed lines) while limiting margin violations (i.e., instances off the street).The width of the street is controlled by a hyperparameter, Ïµ (epsilon). ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![title](https://www.researchgate.net/profile/Hassen_Bouzgou/publication/316351306/figure/fig7/AS:485878301761550@1492853822259/Example-of-linear-SVM-regression-with-tube.png)\n<center><b>Resource</b> :https://www.researchgate.net/figure/Example-of-linear-SVM-regression-with-tube_fig7_316351306</center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Important Warning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before performing SVM Regression the training data should be scaled and centered.\nThis happens automatically because we are using StandartScaller.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's create a linear svm regression model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR()\nsvm_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We created the model. Now let's evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(svm_reg,X_train,X_test,y_train,y_test,\"Linear SVM Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Previously, I tried various parameter values and found the most optimal values and then set the following values to obtain the optimum values I previously found.\n\nLet's try a few parameters for linear SVM regression with GridSearch and choose the best model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params={\"C\":[100,1000,10000],\n       \"dual\":[True,False],\"epsilon\":[1500,4500,7500],\n       \"fit_intercept\":[True,False],\"max_iter\":[5000,7500,10000]}\n\nsvm_gs=GridSearchCV(LinearSVR(),param_grid=params,cv=5,n_jobs=-1,scoring=\"neg_root_mean_squared_error\")\nsvm_gs.fit(X_train,y_train)\n\nbest_linear_svm_reg=svm_gs.best_estimator_\nbest_linear_svm_reg.fit(X_train,y_train)\nevaluateModel(best_linear_svm_reg,X_train,X_test,y_train,y_test,\"Best Linear SVM Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try nonlinear SVM regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nnonlinear_svm_reg=SVR()\nnonlinear_svm_reg.fit(X_train,y_train)\nevaluateModel(nonlinear_svm_reg,X_train,X_test,y_train,y_test,\"Nonlinear SVM Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best Nonlinear SVM Regression:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_nonlinear_svm_reg = SVR(C=100000,degree=2,epsilon=1000,gamma=0.1,kernel=\"rbf\",max_iter=10000)\nbest_nonlinear_svm_reg.fit(X_train,y_train)\nevaluateModel(best_nonlinear_svm_reg,X_train,X_test,y_train,y_test,\"Best Nonlinear SVM Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"19\"></span>**Decision Tree Regressor**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndec_reg=DecisionTreeRegressor()\ndec_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(dec_reg,X_train,X_test,y_train,y_test,\"Decision Tree Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"20\"></span>**Random Forest Regressor**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Random Forest is an ensemble of Decision Trees,that is, a Random Forest contains more than one Decision Tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrand_reg=RandomForestRegressor().fit(X_train,y_train)\nevaluateModel(rand_reg,X_train,X_test,y_train,y_test,\"Random Forest Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"21\"></span>**AdaBoost**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To explain Adaboost with an example: a predictor is trained in the training set, then predictions are made in the training set using this predictor, and more attention is paid to training instances where the predictor is undefitted. The weight of the training instances where the predictor is underfitting is increased. A new predictor is then trained in the training set with updated weights.This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's train a AdaBoost Regressor:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nadaboost_reg=AdaBoostRegressor().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(adaboost_reg,X_train,X_test,y_train,y_test,\"Adaboost Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"22\"></span>**Gradient Boosting**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting tries to fit the new predictor to the residual errors made by the previous predictor.For example, suppose you train a model(call this model model1) using an X training set and y targets. To train the next model(call this model model2.), you need to subtract the predictions made by the previous model(i.e,model1) from y, and our new y target values will be these values(y-model1 predictions).\n\nModel 2 is trained using the X training set and the new y target values(y - model1 predictions), and so on.\n\nTo make a prediction for a new instances,predictions of all models are summed up.\n\nThis is the technique used by Gradient Boosting.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's train a Gradient Boosting Regressor:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngb_reg=GradientBoostingRegressor().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(gb_reg,X_train,X_test,y_train,y_test,\"Gradient Boosting Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting Regressor with optimum n_estimators","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ngbrt=GradientBoostingRegressor(max_depth=2,n_estimators=10000)\ngbrt.fit(X_train,y_train)\n\nerrors=[mean_squared_error(y_test,y_pred) for y_pred in gbrt.staged_predict(X_test)]\n\nbest_n_est=np.argmin(errors)+1\n\ngbrt_best=GradientBoostingRegressor(n_estimators=best_n_est,max_depth=2).fit(X_train,y_train)\nevaluateModel(gbrt_best,X_train,X_test,y_train,y_test,\"Gradient Boosting Regressor with optimum n_estimators\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"23\"></span>XGBoost (Extreme Gradient Boosting)\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"XGBoost is optimized implementation of Gradient Boosting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\n\nxgb_reg=xgboost.XGBRegressor()\nxgb_reg.fit(X_train,y_train)\nevaluateModel(xgb_reg,X_train,X_test,y_train,y_test,\"XGB Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost also offers several nice features, such as automatically taking care of early stopping:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_reg_early_stopping=xgboost.XGBRegressor()\nxgb_reg_early_stopping.fit(X_train,y_train,\n                           eval_set=[(X_test,y_test)],early_stopping_rounds=2)\nevaluateModel(xgb_reg_early_stopping,X_train,X_test,y_train,y_test,\"XGB Regressor with early stopping\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"24\"></span>**Voting Regressor**\n#### [Return Contents](#0)\n<hr/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can use multiple regression models as a single model. The VotingRegressor class can be used for this.The prediction of the VotingRegressor model is the arithmetic mean of the prediction of each regression model. For example, if the prediction of linear regression is 25000 and the prediction of Rasso regression is 15000, the estimate of VotingRegressor is (25000 + 15000) / 2 = 20000.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's create a VotingRegressor using all the models we have created so far(except Polynomial Regression,because the dataset used to train Polynomial Regression is different from the dataset used to train other models.).","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\nvoting_reg=VotingRegressor(\nestimators=[(\"v_lin_reg\",lin_reg),\n            (\"v_rid_reg\",ridge_reg),\n            (\"v_lasso_reg\",lasso_reg),\n            (\"v_elastic_reg_best\",elastic_reg_best),\n            (\"v_elastic_reg\",elastic_reg),\n            (\"v_lin_svm_reg\",svm_reg),\n            (\"v_best_lin_svm_reg\",best_linear_svm_reg),\n            (\"v_nonlinear_svm_reg\",nonlinear_svm_reg),\n            (\"v_best_nonlinear_svm_reg\",best_nonlinear_svm_reg),\n            (\"v_dec_reg\",dec_reg),\n            (\"v_rand_reg\",rand_reg),\n            (\"v_adaboost_reg\",adaboost_reg),\n            (\"v_gb_reg\",gb_reg),\n            (\"v_gb_reg_best\",gbrt_best),\n            (\"v_xgb_reg\",xgb_reg),\n            (\"v_xgb_reg_best\",xgb_reg_early_stopping)\n            ]\n)\n\nvoting_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluateModel(voting_reg,X_train,X_test,y_train,y_test,\"Voting Regression\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"25\"></span>Evaluation Table","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"evaluation.sort_values(by=\"Root Mean Squared Error(RMSE) (5-Fold Cross Validation)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"26\"></span>Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"According to the table, the **\"Gradient Boosting Regressor with optimum n_estimators\"** model is the best model but looks a bit overfitting. It can be tried to solve the overfitting problem by giving more data to the model or by regularization the model.\n\n**Decision Tree Regressor** and **Random Forest Regressor** are severely overfitting in training set.So the generalization performance will be bad.Generalization performance can be improved by regularization models.\n\nAlso, the **Best Nonlinear SVM Regression** model seems to be a good model.\n\n# <center>Thank you for reading my kernel and If you liked it, please do not forget to <font color=\"blue\">UPVOTE </font>ðŸ™‚ </center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"27\"></span>Resources","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Resources I used while writing this kernel:\n* https://www.kaggle.com/burhanykiyakoglu/predicting-house-prices\n* Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition by Aurelien Geron","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}