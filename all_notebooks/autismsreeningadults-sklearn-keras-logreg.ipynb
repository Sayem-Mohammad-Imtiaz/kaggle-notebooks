{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, loading our data and looking into it"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/autism-screening-on-adults/autism_screening.csv')\nprint(data.info())\nprint(data.info)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've noticed that instead of NaN there are question marks when data is missing, we will deal with that.\nAlso, i will drop a few columns that i consider not important from the get go and rename others, for simplicity sake"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.replace('?', np.nan, inplace=True)\ndata = data.drop(columns = ['used_app_before'])\ndata = data.drop(columns = ['age_desc'])\ndata = data.rename(columns={'Class/ASD' : 'classASD'})\ndata = data.rename(columns={'austim' : 'autism'})\ndata = data.rename(columns={'contry_of_res' : 'country_of_res'})\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets take a look at results of test"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['result'].describe())\nsns.displot(data['result'], bins=50, kde = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, all values of result are between 0 and 10. They are a sum of scores from A1 to A10\n\nWe know that this operate on data from adults. Lets take a look at their age ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['age'].describe())\nsns.displot(data['age'], bins=50, kde = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its hard to believe that someone would be 383 years old and still alive, so i will consider it as a typo and change it to 38.\nAlso two records are missing and i would like change result and age data type to integer, so i will fill all blanks with first value."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data.age == 383, 'age'] = 38\ndata['age'].fillna(data['age'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, changing data types and fill all other blanks with first values of their columns.\nThere arent that many missing values, so it wont hurt our models that much."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.age = data.age.astype(int)\ndata.result = data.result.astype(int)\ndata = data.fillna(data.mode().iloc[0])\ndata.info()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As i mentioned earlier, \"result\" is a sum of scores from A1 to A10, so we dont really need all columns for all scores."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data = data.drop(columns = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will try guessing if someone is classified on Autism Spectrum Disorder, which is our \"classASD\" collumn.\nLets see some data about gender, ethnicity and nationality"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 15))\nsns.countplot(x = 'classASD', hue = 'gender', data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like women are more often classified on ASD, but not but a whole lot. Lets look at ethicities next."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 15))\nsns.countplot(x = 'classASD', hue = 'ethnicity', data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like White-European adults are visibly more often classified on ASD\nNow lets take a look at nationality."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (30, 20))\nsns.countplot(x = 'classASD', hue = 'country_of_res', data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, most cases are in USA and UK. But are those two observations real coreallations or pure coincidence?\nWe will take a look at that now.\nFirst, i will convert all strings to integers (unique integer for each of possible strings) using LabelEncoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata['ethnicity'] = le.fit_transform(data['ethnicity'])\ndata['jundice'] = le.fit_transform(data['jundice'])\ndata['autism'] = le.fit_transform(data['autism'])\ndata['country_of_res'] = le.fit_transform(data['country_of_res'])\ndata['relation'] = le.fit_transform(data['relation'])\ndata['classASD'] = le.fit_transform(data['classASD'])\ndata['gender'] = le.fit_transform(data['gender'])\ndata['age'] = le.fit_transform(data['age'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets take a look how our data looks now"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now, we can create correlation matrix for all our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix = data.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"result and classASD have a really strong correlation. Othe columns aren't nearly as correlated as \"result\".\ni shall drop all columns with weaker correlation than 0.1"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns = ['gender', 'country_of_res', 'relation'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a look at classASD histogram. If the proportion was not balanced enough, models could learn to always just guess one anwser, and we want something more complicated than that."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.classASD.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good enough for me, now i will separate data for features (X) and anwser (Y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns = 'classASD')\nY = data['classASD']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will start with scaling features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(X)  \nX = scaler.transform(X)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First model - MLPClassifier from sklearn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(solver='adam', alpha=1e-3, hidden_layer_sizes=(3, 2), random_state=1, max_iter=10000)\nclf.fit(X, Y)\nMLPClassifier(alpha=1e-3, hidden_layer_sizes=(3, 2), random_state=1, solver='adam', max_iter=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"alpha = 0.001, 2 layers with 3 tuples may seem a little too little, but it will be more than enough.\nI will cross-validate that model 5 times."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X, Y, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"%0.4f accuracy with a standard deviation of %0.4f\" % (scores.mean(), scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thats a really good score, a little too good. Its because of such a big correlation of \"result\" and \"classASD\". Either our model is that good, or its overfitting."},{"metadata":{},"cell_type":"markdown","source":"Now lets try with keras Sequential model\njust to be fair, same amount of layers, tuples, and same solver"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(3, input_dim = 5, activation='relu'))\nmodel.add(Dense(3, activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, Y, epochs = 200, batch_size = 10, verbose = 0)\n_, accuracy = model.evaluate(X, Y)\nprint('Accuracy: %.4f' % (accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks like a more propable accuracy. Now lets look how a simple Logistic Regression will solve that problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import check_random_state\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"spliting test and train datasets in 8:2 ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"lx = data.drop(columns = ['classASD'])\nly = data['classASD']\nlx_train, lx_test, ly_train, ly_test = train_test_split(lx, ly, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lx_train = scaler.fit_transform(lx_train)\nlx_test = scaler.fit_transform(lx_test)\nlogReg = LogisticRegression(C = 50. / 10000, penalty='l1', solver='liblinear', tol=0.1)\nlogReg.fit(lx_train, ly_train)\nsparsity = np.mean(logReg.coef_ == 0) * 100\nscore = logReg.score(lx_test, ly_test)\n\nprint(\"Test score with L1 penalty: %.4f\" % score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like its almost the same anwser as for keras model"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}