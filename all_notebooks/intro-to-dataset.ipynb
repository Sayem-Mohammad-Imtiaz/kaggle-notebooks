{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating file out of the whole dataset\n\n##### To ensure faster and more demonstrative work on dataset let's create one *.csv* file containg 10 advertising topics with 1.000 advertisements (or else *lines*) each. I choose such topics by meaning so that they differ from each other as much as possible. For instane, monitors - shoes - watches - food - cars ands so on"},{"metadata":{},"cell_type":"markdown","source":"First find all files in all other directories; write number of files and number of *lines* (or else advertisements) in each file"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import glob\n\nall_scrapped_files = glob.glob('/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/*/*.csv')\narrayOfLines = []\n\nfor i in all_scrapped_files:\n    count = 0\n    with open(i, 'r') as file:\n        for line in file:\n            count += 1\n    # print(i,  '|| LINES: ', count)\n    arrayOfLines.append(count)\nprint('Amount of scrapped files: ', len(all_scrapped_files))\nprint('List of all lines in datasets: ', arrayOfLines)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Writing only files with more than 1000 lines of data"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"new_scrapped_files = []\narrayOfLines_2 = []\n\nfor i in all_scrapped_files:\n    count = 0\n    with open(i, 'r') as file:\n        for line in file:\n            count += 1\n    if count >= 1000: \n        print(i,  '|| LINES: ', count)\n        new_scrapped_files.append(i)\n        arrayOfLines_2.append(count)\n    \n    \nprint('Amount of scrapped files: ', len(new_scrapped_files))\nprint('List of all lines in datasets: ', sorted(arrayOfLines_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of topics that have more than 1.000 lines (by searching mentions in file's name) and their names"},{"metadata":{"trusted":true},"cell_type":"code","source":"skipper = len('/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/')\ncheck = set()\ncounter = 0\n\nfor i in new_scrapped_files:\n    counter += 1\n    stripped_line = i[skipper:]\n    # print('stripped_line: ', stripped_line)\n    indexOfCrossedLine = stripped_line.find('/')\n    j = stripped_line[:indexOfCrossedLine]\n    # print('name of future cluster/topic: ', j)\n    check.add(j)\n    \nprint('Counter/number of files: ', counter)\nprint('number of cluster / topics to be measured: ', len(check))\nfor i in sorted(check):\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For further classification I chose the following topics and assigned corresponding labels to them**\n0. monitors\n1. printers\n2. headphones\n3. hair-care\n4. rings\n5. backpacks\n6. jeans\n7. sneakers\n8. watches\n9. flashlight"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# writing paths to files explicitly\nlist_of_ten = ['/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/computers/amazon_computers_monitors.csv', \n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/computers/amazon_computers_printers.csv', \n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/electronics/amazon_electronics_headphones.csv', \n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/beauty/amazon_beauty_hair-care.csv',\n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/women-fashion__jewelry/amazon_women-fashion_jewelry_rings.csv',\n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/luggage/amazon_luggage_backpacks.csv', \n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/women-fashion__clothing/amazon_women-fashion_clothing_jeans.csv',\n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/women-fashion__shoes/amazon_women-fashion_shoes_sneakers.csv',\n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/women-fashion__watches/amazon_women-fashion_watches_wrist.csv',\n               '/kaggle/input/amazons-advertisements/scrapped_data/scrapped_data/tools-home__safety-security/amazon_tools-home_safety-security_flashlights.csv'\n              ]\n   \nimport random\n\n\nfor i in range(len(list_of_ten)):\n    with open(list_of_ten[i], 'r') as source:\n        data = [ (random.random(), line) for line in source ]\n        print('Reading {n} '.format(n=list_of_ten[i]))\n    data.sort()\n    with open('amazon_ten_topic_data_labeled.csv','a') as target:\n        for _, line in data[:1000]:\n            if line.startswith('\"') and not line.endswith('\"'):\n                line + '\"'\n            _line = line.rstrip('\\n') + ', ' + str(i)\n            target.write( _line )\n            target.write('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.display.max_colwidth = 80\n\nimport numpy as np\nimport spacy\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn import pipeline, ensemble, preprocessing, feature_extraction, metrics\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads = pd.read_csv('amazon_ten_topic_data_labeled.csv', names=['ad', 'label'])\nads.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing NaN values\nads.dropna(inplace=True)\n\n# Shuffling rows\nads = ads.sample(frac=1)\nads['ad'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Top words in dataset before feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count vetorizing object\ncount_vectorizer = CountVectorizer()\n\n# fitting CV\ncount_vectorizer.fit(ads['ad'])\n\n# collecting the vocabulary items used in vectorizer\ndictionary = count_vectorizer.vocabulary_.items()\n\n# Storing vocab and counts in a pandas DF\nvocab = []\ncount = []\n\n# iterating through each vocab and count append the value to designated list\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n    \n# storing the count in pandas DF with vocab as index\nvocab_bef_stem = pd.Series(count, index = vocab)\n\n# sorting the DF\nvocab_bef_stem = vocab_bef_stem.sort_values(ascending=False)\n\ntop_vocab = vocab_bef_stem.head(10)\n\n# Note, that since lines in dataset is always generated randlomly, dataframes may differ\n# So,play with xlim parameter to gain visual representation of the dataset.\ntop_vocab.plot(kind = 'barh', figsize=(20, 15), xlim=(16320, 16355)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the length of each line"},{"metadata":{"trusted":true},"cell_type":"code","source":"def length(text):\n    \"\"\"Function that returns the length of a text\"\"\"\n    return len(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads['length'] = ads['ad'].apply(length)\nads.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sample_length_distribution(sample_texts):\n    \"\"\"Plots the sample length distribution.\n\n    # Arguments\n        samples_texts: list, sample texts.\n    \"\"\"\n    \n    plt.figure(figsize=(20,10))\n    plt.hist([len(s) for s in sample_texts], 100)\n    plt.xlabel('Length of a sample')\n    plt.ylabel('Number of samples')\n    plt.title('Sample length distribution of the documents before FE')\n    plt.show()\n    \n\nplot_sample_length_distribution(ads['ad'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing stopwords, punctuation, numbers and words with numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stopwords(text):\n    \"\"\"\n    Function for removing \n        - stopwords,\n        - punctuation,\n        - numbers / digits\n        - words containing numbers\n    \"\"\"\n    doc = nlp(text)\n    for token in doc:\n        text = [token.text for token in doc if \n                not token.is_stop \n                and not token.is_punct \n                and not token.is_digit]\n        \n        \n    # joining the list of words with space separator\n    joined_text = \" \".join(text)\n    # removing words that contain any sort of numbers, like 'G2420-BK' or 'G1W40A#BGJ '\n    re_text = re.sub(r\"\\S*\\d\\S*\", '', joined_text).strip()\n    \n    return re_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads['NO SW'] = ads['ad'].apply(stopwords)\nads.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### IMPORTANT! \nOn this step check for whitespaces, NaN or missing values and son, because applying *stopwords* method may leave in your dataframe.\nI know that mine has them too on this step and only as example I provide a little solution, but mostly I'm leaving that to you, cause I'm lazy... sorry "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 Getting rid off of two whitespaces¶\nads['NO SW']=ads['NO SW'].str.replace(\"  \",\" \")\n\n# 2 Replacing rows with no entries with NaN to dropna them\nads['NO SW'].replace('', np.nan, inplace=True)\nads.dropna(inplace=True)\nads['NO SW'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Top words in dataset after feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count vetorizing object\ncount_vectorizer = CountVectorizer()\n\n# fitting CV\ncount_vectorizer.fit(ads['NO SW'])\n\n# collecting the vocabulary items used in vectorizer\ndictionary = count_vectorizer.vocabulary_.items()\n\n# Storing vocab and counts in a pandas DF\nvocab = []\ncount = []\n\n# iterating through each vocab and count append the value to designated list\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n    \n# storing the count in pandas DF with vocab as index\nvocab_bef_stem = pd.Series(count, index = vocab)\n\n# sorting the DF\nvocab_bef_stem = vocab_bef_stem.sort_values(ascending=False)\n\ntop_vocab = vocab_bef_stem.head(10)\ntop_vocab.plot(kind = 'barh', figsize=(20, 15), xlim=(10850, 10970))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Engineering in action"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ad is chosen randomly\nprint(ads['ad'][806], ' ====', ads['NO SW'][806])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification with Non-negative matrix factorization (NMF)"},{"metadata":{},"cell_type":"markdown","source":"<p>To be exact, this is what will happen next: after applying TF-IDF on the dataset I’m feeding it to a topic model algorithm, telling it how many topics/models it needs to find. A very important note: the machine learning algorithm won’t know the “correct” topic or “right answer”. It will only know that the documents clustered together share similar topic ideas. It is up to me, as a developer, to identify what these topics represents and assign them to the documents.</p>\n\nWhat are these topics? Technically, they are multinomial distributions over words, which means that they assign a probability to each word in the vocabulary. Words with with high probability are more associated with that topic than words with lower probability.\n\nSimply put, the algorithm will produce 10 groups of words that are linked together by the same meaning, weight or, more simply, a topic. The most interesting thing is that each group will contain the same words in the same amount, but the words will be arranged in a certain order, by value to their topic. So, for example, the words “denim“, “jeans“ and “skinny“ will be located close to each other and far from the words “headphones“ and “ear“.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ads.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course we will use lines not containing stopwords and all other stuff that we got rid off"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_df=0.95, min_df=2)\n\ndtm = tfidf.fit_transform(ads['NO SW'])\n\ndtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nmf_model = NMF(n_components=10,random_state=42)\n\n# This can take awhile, we're dealing with a large amount of documents!\nnmf_model.fit(dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,topic in enumerate(nmf_model.components_):\n    print(f'THE TOP 5 WORDS FOR TOPIC #{index}')\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-5:]])\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When reading through the lists of words, we can clearly see that topics are not just random words, but instead are logical groups.\n\nThen, based on these words and the knowledge of the predefined **true** topics I can manually tell which topics corresponds each cluster. For example, Topic No1 to “Monitors”, Topic No4 to “Rings” and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_results = nmf_model.transform(dtm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Recall labels of each of our topic**\n0. monitors\n1. printers\n2. headphones\n3. hair-care\n4. rings\n5. backpacks\n6. jeans\n7. sneakers\n8. watches\n9. flashlight\n\nBased on NMF output I assign each group number to it's predicted topic by meaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"ads['topic label'] = topic_results.argmax(axis=1)\n\nmy_topic_dictionary = {0: 'Watches', \n                       1: 'Monitors', \n                       2: 'Printers', \n                       3: 'Ring', \n                       4: 'Jeans', \n                       5: 'Headphones', \n                       6: 'Flashlights',\n                       7: 'Backpacks',\n                       8: 'Sneakers',\n                       9: 'Hair-Care'}\n\nads['topic name'] = ads['topic label'].map(my_topic_dictionary)\nads","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I only need to assign predicted topic number to it's supposedly right label to evaluate the model. Yes, it's strange and cubmersome, but it's only an example"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reassign_2_label(topic):\n    switcher = {\n        0: 8,\n        1: 0,\n        2: 1,\n        3: 4,\n        4: 6,\n        5: 2,\n        6: 9,\n        7: 5,\n        8: 7,\n        9: 3\n    }\n    return switcher.get(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ads['predicted label'] = ads['topic label'].apply(reassign_2_label)\nads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix = pd.crosstab(ads['label'], ads['predicted label'])\nconfusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As you see, model predicts alsmost always right. There are some missunderstandings between such words as *sneakers* and *backpacks* or *jeans* and *watches* or *flashlights*, but overall, it's fine in predicting task"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}