{"cells":[{"metadata":{"_uuid":"b23ce6e5-1f00-4b96-9218-37ef1c40b507","_cell_guid":"c3d5cc37-ff45-4006-bff9-d638f0eb330e","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n#importing necessary libraries\nimport csv\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom scipy.stats import norm\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,AdaBoostRegressor)\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy.special import boxcox1p\nfrom sklearn.metrics import  make_scorer\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\ntrain = pd.read_csv('../input/advance-house-price-predicitons/train.csv')\ntest = pd.read_csv('../input/advance-house-price-predicitons/test.csv')\n\n\n#joining train and test data\ndf=pd.concat((train, test)).reset_index(drop=True)\nprint(df.head())\n#sns.heatmap(df)\n\n\n#print((df.dtypes[df.dtypes!='object'].index).shape)\n#print((df.dtypes[df.dtypes=='object'].index).shape)\n\n#from scipy.stats import norm\n#sns.distplot(train['SalePrice'].values,fit=norm);\n\n#from scipy import stats\n#import numpy as np\n#z = np.abs(stats.zscore(df))\n#print(z)\n#print(np.where(z > 3))\n\n#print(df.shape)\n#filtered_entry = (z < 3).all(axis=1)\n##df1=df[filtered_entry]\n#print(df1.head())\ninitial_ytrain_series= train['SalePrice']\nprint(\"---------saleprice----------\")\nsns.distplot(train['SalePrice'] , fit=norm);\nplt.show()\n\ninitial_ytrain= train['SalePrice']\n#df[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nprint(\"---------saleprice with log----------\")\n\n\n#transforming the traget variable to logarithm scale\nytrain=  np.log1p(train[\"SalePrice\"]);\nsns.distplot(ytrain , fit=norm);\nplt.show()\n\n\n#removing ID column from test and train\ntrain_id=train[\"Id\"]\ntest_id=test[\"Id\"]\n\n#removing sale price and ID from dataset \ndf.drop('SalePrice',axis=1,inplace=True)\ndf.drop('Id',axis=1,inplace=True)\n\n\ndf = df.reset_index()\n\n#print(train.shape,test.shape)\n#print(test.describe())\n#print(train.head())\n\n#print(train.describe())\n#print(train.columns)\n#print(test.dtypes)\n\n\n# function to determine the null value percent\ndef nullvalues(df):\n    total_null_values = df.isnull().sum()\n    total_values = len(df)\n    null_values_percent = ((total_null_values*100)/total_values).sort_values(ascending=False)[:15]\n    #table = pd.concat((total_null_values, null_values_percent), axis=1)\n    #print(table[table[1]>0])\n    #print(table.sort_values(by=1,ascending= False))\n    missing_data = pd.DataFrame({'Missing Value Percent' :null_values_percent})\n    f, ax = plt.subplots(figsize=(15,10))\n    plt.xticks(rotation='65')\n    sns.barplot(null_values_percent.index, y=(null_values_percent))\n    sns.set(style=\"whitegrid\")\n    sns.set_color_codes(\"pastel\")\n    plt.xlabel('Features')\n    plt.ylabel('Percent - missing values')\n    plt.title('Percent missing data')\n    plt.show()\n    print(missing_data)\n    \n\nprint(nullvalues(df))\n\n\n#combine few variables into string type\ndf['MSSubClass'] = df['MSSubClass'].astype(str)\ndf['OverallQual'] = df['OverallQual'].astype(str)\ndf['OverallCond'] = df['OverallCond'].astype(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\n\n\n#filling null column with \"None\" \n\nnone_columns=['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageType', 'GarageFinish',\n              'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n              'MasVnrType','MSSubClass']\n\nfor c in none_columns:\n    df[c]=df[c].fillna(\"None\")\n    \n    \n#filling missing value with 0 value\n    \nzero_columns=['GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', \n              'BsmtFullBath', 'BsmtHalfBath','MasVnrArea'] \nfor z in zero_columns:\n    train[z]=train[z].fillna(0)\n    #df.drop(z,inplace=True,axis=1)\n    \n\n#filling null column with \"mode\" \n\nmode_columns= ['MSZoning','Electrical','KitchenQual','Exterior1st','Exterior2nd','SaleType','Functional'\n              ,'LotFrontage']\n\nfor m in mode_columns:\n    df[m]=df[m].fillna(train[m].mode()[0])\n    \n\nprint(df.shape)    \n\n#Label Encoding categorical variable\n\ncategorical_variable=['GarageFinish','BsmtCond','BsmtQual',  'GarageQual', 'GarageCond','BldgType', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'HouseStyle',\n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope','RoofStyle',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n       'YrSold', 'MoSold','MSZoning','LandContour','LotConfig','Neighborhood','Condition1','Condition2',\n                     'RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','HeatingQC',\n                     'CentralAir','Electrical','KitchenQual','Functional','GarageType','FireplaceQu',\n                     'GarageCond','PavedDrive','MiscFeature','SaleType','SaleCondition','OverallQual']\n\nfor c in  categorical_variable:\n    df[c]= LabelEncoder().fit_transform(df[c].values)\n    \n\n#print(df.head())\n    \n\n#dropping the utility column    \ndf.drop('Utilities',axis=1,inplace=True)\n\n\n#finding the skewness of the predictors in the dataset\n\nnumeric_features=df.dtypes[df.dtypes!='object'].index\n#print(numeric_features)\nskewValue = df[numeric_features].skew().sort_values(ascending=False)\nskewness=pd.DataFrame({'Skew':skewValue})\npositive_skew=skewness[skewness['Skew']>1]\nprint(\"-------------\")\nprint(positive_skew)\npostive_skew_index=positive_skew.index\n\n#sns.barplot(postive_skew_index, y=skewValue )\n#print(postive_skew_index)\n#print(positive_skew)\n#print(skewValue.sort_values(ascending=False))\n\n\n#applying boxcox transformation to remove skewness\nlam=0.15\n\nfor s in postive_skew_index:\n    df[s]=boxcox1p(df[s],lam)\n    #print(s,df[s].skew())\n\nprint(\"--------------\")\n\n#finding skewness after applying boxcox transformation\n\n\nskewValue2 = df[numeric_features].skew().sort_values(ascending=False)\n#psotive_skew= skewValue2[skewValue2]\nskewness2=pd.DataFrame({'Skew':skewValue2})\nskewness2_index=skewness2.index\nprint(skewness2)\n\n\n#skewValue2 = df[numeric_features].skew().sort_values(ascending=False)\n#skewness2=pd.DataFrame({'Skew':skewValue})\n#print(skewness2)\n\n\n#Parameter Grid- which would have been otherwise used with GRIDSearchCV or RandomisedSearch CV\n'''\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n#max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nrfrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf\n               }\n\nlearning_rate = [0.1,0.5,1.0]\n#print(random_grid)\n\nadarandom_grid = {'n_estimators': n_estimators,\n                  'learning_rate': learning_rate\n}\n\nxgb_randomgrid = {\n    'learning_rate' :learning_rate,\n    'n_estimators':n_estimators,\n    'max_depth':max_depth\n}\n'''\n\n#np.where(train.values >= np.finfo(np.float64).max)\n\nrf=RandomForestRegressor()\nada=AdaBoostRegressor()\ngb=GradientBoostingRegressor()\nxgb=XGBRegressor()\n\n# Random search of parameters, using 3 fold cross validation,\n# search across 100 different combinations, and use all available cores\n#cv_split = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\ncv_split = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n\n#xgb=XGBRegressor(n_estimators=50,learning_rate=0.05)   \n#xgb_random = RandomizedSearchCV(estimator=xgb,param_distributions=xgb_randomgrid,cv=cv_split)\n#rf_random = RandomizedSearchCV(estimator=rf,param_distributions=rfrandom_grid,cv=cv_split)\n#gb_random=RandomizedSearchCV(estimator=gb,param_distributions=rfrandom_grid,cv=cv_split)\n#ada_random = RandomizedSearchCV(estimator=ada,param_distributions=adarandom_grid,cv=cv_split)\n\nfrom sklearn import linear_model\npoly=PolynomialFeatures(order=2)\nlm= LinearRegression()\nrf=RandomForestRegressor()\nls = linear_model.Lasso(alpha=0.1)\n\n\n#rf=RandomForestRegressor(max_features='auto',min_samples_leaf=2,n_estimators=50)\n#ada=AdaBoostRegressor(n_estimators=50,learning_rate=0.05)\n#gb=GradientBoostingRegressor(n_estimators=250, learning_rate=0.1, max_depth=5, random_state=0, loss='ls')\n#xgb=XGBRegressor(n_estimators=50,learning_rate=0.05)   \n\n\n\nprint(np.where(df.values >= np.finfo(np.float64).max))\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf[:] = np.nan_to_num(df)\n\n\n#caluclating corrleation cofficient by first combining the dataset again\ncorelation_df = pd.concat((train,ytrain), axis=1)\n\n#remvong the duplicate column of SalePrice\ncorelation_df = corelation_df.loc[:,~corelation_df.columns.duplicated()]\n\n#plotting heatmap of all variables- sale price varibale is dropped in it\nsns.heatmap(df)\n\ntrain = df[:train.shape[0]]\ntest = df[train.shape[0]:]\n\n#print(train.shape,ytrain.shape)\n\n#print(corelation_df.head())\ndf.reset_index(inplace=True)\n\n\ncorr=corelation_df.corr()\n#corr_data = pd.DataFrame({'Corr coff' :corr})\n\n#sns.heatmap(corr,cmap=\"YlGnBu\", annot=True)\n\nsns.heatmap(corr,cmap=\"YlGnBu\")\n\n#locking the sales price column for all rows\nnew_corr_df=pd.DataFrame(corr.loc[:,\"SalePrice\"])\n#print(new_corr_df)\n\n\n#plotting correlation coff with just sales price variable\nf, ax = plt.subplots(figsize=(15,10))\nplt.xticks(rotation='65')\nsns.heatmap(new_corr_df)\nplt.show()\n\n\n#putting the columns with correlation cofficient of less than 0.2 and 0.3 in two lists\n\nnon_corr_columns_02= [\"MSSubClass\",\"OverallCond\",\"BsmtFinSF2\",\"LowQualFinSF\",\"BsmtHalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\n                   \"PoolArea\",\"MiscVal\",\"MoSold\",\"YrSold\"]\n\nnon_corr_columns_03= [\"MSSubClass\",\"OverallCond\",\"BsmtFinSF2\",\"LowQualFinSF\",\"BsmtHalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\n                   \"PoolArea\",\"MiscVal\",\"MoSold\",\"YrSold\",\"WoodDeckSF\",\"2ndFlrSF\",\"OpenPorchSF\",\"HalfBath\",\"LotArea\",\"GarageYrBlt\",\"BsmtFullBath\",\"BsmtUnfSF\"]\n\n\n#removing the columns with corellation coff less than 0.2\n\nfor nc in non_corr_columns_02:\n    train.drop(nc,inplace=True,axis=1)\n    test.drop(nc,inplace=True,axis=1)\n\n    \n    \n#print(\"Program has come here\")\nnumeric_features=train.dtypes[df.dtypes!='object'].index\n#print(numeric_features)\n\n#print(train.shape,\"-----------------------------\")\n\n#performing RFE on different models\nrfe_selector1 = RFE(estimator=ls, step=10, verbose=5,n_features_to_select=65)\nrfe_selector_rf = RFE(estimator=rf, step=10, verbose=5,n_features_to_select=65)\nrfe_selector_gb = RFE(estimator=gb, step=10, verbose=5,n_features_to_select=65)\nrfe_selector_xgb = RFE(estimator=xgb, step=10, verbose=5,n_features_to_select=65)\n\nridge= Ridge(alpha=0.1)\n\n\n#poly=PolynomialFeatures(2)\n#train=poly.fit_transform(train)\n\n#removing outliers from the dataset by replacing their values\n\nfor i in train.columns:\n    quartile_one,quartile_three = np.percentile(train[i],[25,75])\n    quartile_first,quartile_last = np.percentile(train[i],[12,85])\n    inter_quartile_range = quartile_three-quartile_one\n    cut_off= 1.5*inter_quartile_range\n    lower_bound = quartile_one - (cut_off)\n    upper_bound = quartile_three + (cut_off)\n    print()\n    train[i].loc[df[i] < lower_bound] = quartile_first\n    train[i].loc[df[i] > upper_bound] = quartile_last\n    print(i,lower_bound,upper_bound,quartile_first,quartile_last )\n\n        \n\n#building voting regressor of different kind \n\n#eclf1 = VotingRegressor(estimators=[('rm',rf_random), ('ada', ada_random),('XGB',xgb_random),('GB',gb_random)])\n\neclf2 = VotingRegressor(estimators=[('rm',rf),('XGB',xgb),('ada', ada),('GB',gb)])\n\neclf4 = VotingRegressor(estimators=[('rm_rfe',rfe_selector_rf),('XGB_rfe',rfe_selector_xgb),('GB_rfe',rfe_selector_gb)])\n\neclf3 = VotingRegressor(estimators=[('rm',rf), ('XGB',xgb),('GB',gb)])\n\neclf4= VotingRegressor(estimators=[('rm',rf),('XGB',xgb),('ada', ada),('GB',gb),('rm_rfe',rfe_selector_rf),('XGB_rfe',rfe_selector_xgb),('GB_rfe',rfe_selector_gb),\n                      (\"Linear Regression\",lm),('Lasso',ls)])\n\neclf5 = VotingRegressor(estimators=[('rm',rf), ('Linear Regression',lm),('XGB',xgb), ('GB',gb)])\n\n\n#to print the test and training sample splitted by the cv\n#for train_index, test_index in cv_split.split(train):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\n\n#print(train.shape,ytrain.shape)\n\n\n#using voting regressor and other models to obtain the accuracy and standar deviation\n\nfor clf, label in zip([lm,rf,ada,ls,gb,ridge,xgb,eclf2,eclf4, eclf3,eclf5], ['Linear Regression','RandomForest','ADAboost', 'Lasso','GB','Ridge','XGB','Combine with ada',\n                                                                      'Combine without ada with rfe','Combine without ada',\"combine with linear\"]):\n                     score=cross_val_score(clf,train,ytrain,scoring='neg_mean_squared_error',cv=cv_split) \n                     clf.fit(train,ytrain)\n                     #train['prediction %s'%label]=clf.predict(train) # this line was a mistake which resulted in additon of a column in dataset and increased the accuracy of the model, but was later corrected\n                     print(\"Accuracy: %0.3f (+/- %0.3f) [%s]\" % (score.mean(), score.std(), label))  \n                     print('----------------')\n    \n    \n#predicting values of traget variable in test\n    \n#yhat3=np.expm1(eclf3.predict(test)) #without ada\n#yhat2=np.expm1(eclf2.predict(test)) #with ada\n#yhat4=np.expm1(eclf4.predict(test)) #with everything\nyhat5=np.expm1(eclf5.predict(test)) #with linear\n#yhat2=(eclf.predict(train))\n#yhat=pd.Series(yhat)\n#print(yhat)\n\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_id\nsubmission['SalePrice'] = yhat5\nsubmission.to_csv('submission.csv',index=False)\n\n#kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"submit\"\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}