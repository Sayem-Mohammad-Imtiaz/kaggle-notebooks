{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# version | Test Loss | n_hidden layers | hidden_dim |   l1   |   l2   | dropout | batch_size | epochs\n# --------|-----------|-----------------|------------|--------|--------|---------|------------|--------\n#    01   |   0.1218  |        2        |     4      |  0.0   |  0.1   |   0.0   |     128    |   50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_hidden_layers = 2 # number of hidden layers\nhidden_dim = 32 # dimensions of hidden layers\nl1 = 0.0 # penalty on output\nl2 = 0.1 # weights l2 regularization parameter\ndropout = 0.2 # dropout parameter [0,1]\nbatch_size = 128 # batch_size\nepochs = 50 # number of epochs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read & Pre-process"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/cmsnewsamples/new-smaples.csv').drop(columns = 'Unnamed: 0')\ndf = df.drop(columns = [i for i in df.columns if '_1' in i])\ndf['non_hits'] = df[[i for i in df.columns if 'mask' in i]].sum(axis=1)\ndf = df[df['non_hits']==0].reset_index(drop=True)\n\ndf['1/pT'] = df['q/pt'].abs()\ndef label(a):\n    if a<=10:\n        return 0\n    if a>10 and a<=30:\n        return 1\n    if a>30 and a<=100:\n        return 2\n    if a>100:\n        return 3\n\ndf['pT'] = 1/df['1/pT']\n    \ndf['pT_classes'] = df['pT'].apply(label)\n\nfeatures = ['emtf_phi_'+str(i) for i in [0,2,3,4]] + ['emtf_theta_'+str(i) for i in [0,2,3,4]] + ['old_emtf_phi_'+str(i) for i in [0,2,3,4]]\n\nnew_features = []\nfor i in range(len(features)-1):\n    for j in range(i+1, (i//4+1)*4):\n        new_features.append('delta_'+'_'.join(features[i].split('_')[:-1])+'_'+str((j)%4)+'_'+str(i%4))\n        df[new_features[-1]]=df[features[j]]-df[features[i]]\n\nfeatures = new_features[:]\n\nlabels_1 = ['1/pT']\nlabels_2 = ['pT_classes']\nlabels_3 = ['vx']\n\nscaler_1 = MinMaxScaler()\ndf[features] = scaler_1.fit_transform(df[features])\n\nscaler_3 = MinMaxScaler()\ndf[labels_3] = scaler_3.fit_transform(df[labels_3])\n\ndf_features = df[features].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_list = []\ncol_f1 = [i for i in df_features.columns if 'delta_emtf_phi' in i]\ncol_f2 = [i for i in df_features.columns if 'delta_emtf_theta' in i]\ncol_f3 = [i for i in df_features.columns if 'delta_old_emtf_phi' in i]\n\ndf['s1'] = ((df[col_f1]>=0).sum(axis=1))>2\ndf['s2'] = (df[col_f2]>=0).sum(axis=1)>2\ndf['s3'] = (df[col_f3]>=0).sum(axis=1)>2\n\nfor (s1,s2,s3), dd in df.groupby(['s1','s2','s3']):\n    print(s1,s2,s3, len(dd), dd['1/pT'].min(), dd['1/pT'].mean(), dd['1/pT'].median(), dd['1/pT'].max())\n    df_list.append(dd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[features+labels_1].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ExU layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExU(torch.nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ExU, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.weight = Parameter(torch.Tensor(out_dim, in_dim))\n        self.bias = Parameter(torch.Tensor(in_dim))\n        self.reset_parameters()\n        \n    def reset_parameters(self):\n        self.weight = torch.nn.init.normal_(self.weight, mean=3.5, std=0.5)\n        self.bias = torch.nn.init.normal_(self.bias, mean=3.5, std=0.5)\n        \n    def forward(self, inp):\n        output = inp-self.bias\n#         output = output.matmul(torch.exp(self.weight.t()))\n        output = output.matmul(self.weight.t())\n        output = F.relu(output)\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NAM(torch.nn.Module):\n    def __init__(self, in_dim, n_hidden_layers, hidden_layer_dim, dropout = 0):\n        super(NAM, self).__init__()\n        self.dropout = dropout\n        self.model = []\n        for i in range(in_dim):\n            if n_hidden_layers==0:\n                layers = [ExU(1, 1)]\n            else:\n                layers = [ExU(1, hidden_layer_dim), torch.nn.Dropout(self.dropout)]\n                for i in range(n_hidden_layers):\n                    layers+=[ExU(hidden_layer_dim, hidden_layer_dim), torch.nn.Dropout(self.dropout)]\n                layers+=[ExU(hidden_layer_dim, 1)]\n            self.model.append(torch.nn.Sequential(*layers))\n            \n        self.model = torch.nn.ModuleList(self.model)\n            \n        self.in_dim = in_dim\n        self.n_hidden_layers = n_hidden_layers\n        self.hidden_layer_dim = hidden_layer_dim\n        \n        self.summation_params = []\n        for i in range(in_dim + 1):\n            self.summation_params.append(torch.nn.init.normal_(Parameter(torch.Tensor(1)), mean=0.5, std=0.5))\n        self.summation_params = torch.nn.ParameterList(self.summation_params)\n            \n    def forward(self, x):\n        \n        output = self.summation_params[0]*self.model[0](x[:,0].reshape(-1,1))\n        for i in range(1,self.in_dim):\n            output += self.summation_params[i]*self.model[i](x[:,i].reshape(-1,1))\n        output += self.summation_params[self.in_dim]\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def criterion(outputs, labels, weights, l1=0):\n    loss0 = torch.sqrt(torch.mean((labels-outputs)**2))\n    loss1 = torch.sqrt(torch.mean(outputs**2))\n    \n    return loss0+loss1*l1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Fn"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_nam(model, X_train, Y_train, X_test, Y_test, l1, l2, fold=0, epochs=50, batch_size=128, results_path='./', progress_bar=False):\n    \n    test_index = list(X_test.index)\n    X_val = torch.Tensor(X_train.reset_index(drop=True).iloc[:int(len(X_train)*0.1)].to_numpy())\n    Y_val = torch.Tensor(Y_train.reset_index(drop=True).iloc[:int(len(Y_train)*0.1)].to_numpy())\n    X_train = torch.Tensor(X_train.reset_index(drop=True).iloc[int(len(X_train)*0.1):].reset_index(drop=True).to_numpy())\n    Y_train = torch.Tensor(Y_train.reset_index(drop=True).iloc[int(len(Y_train)*0.1):].reset_index(drop=True).to_numpy())\n    X_test = torch.Tensor(X_test.reset_index(drop=True).to_numpy())\n    Y_test = torch.Tensor(Y_test.reset_index(drop=True).to_numpy())\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True, num_workers = 4) \n    val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=batch_size) \n    test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=batch_size)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=l2)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=1, factor=0.2)\n    \n    l1=torch.tensor(l1)\n    \n    m_train_loss = []\n    m_val_loss = []\n    m_test_loss = []\n    min_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n      train_loss = 0\n      val_loss = 0\n      if progress_bar:\n          pbar = tqdm(train_loader)\n      else:\n          pbar = train_loader\n      for data in pbar:\n        optimizer.zero_grad()\n        outputs = model(data[0].to(device))\n        labels = data[1].to(device)\n        loss = criterion(outputs, labels, model.parameters(), l1)\n        l2 = criterion(outputs, labels, model.parameters())\n        loss.backward()\n        optimizer.step()\n        if progress_bar:\n          pbar.set_description('Loss: '+str(l2.detach().cpu().numpy()))\n        train_loss += l2.detach().cpu().numpy()/len(train_loader)\n\n      for data in val_loader:\n        optimizer.zero_grad()\n        outputs = model(data[0].to(device))\n        labels = data[1].to(device)\n        loss = criterion(outputs, labels, model.parameters())\n        val_loss += loss.detach().cpu().numpy()/len(val_loader)\n      if val_loss<min_val_loss:\n        min_val_loss = val_loss\n        torch.save(model.state_dict(), 'model.pth')\n      lr_scheduler.step(val_loss)\n      print('Epoch: ', str(epoch+1)+'/'+str(epochs),'| Training Loss: ', train_loss, '| Validation Loss: ', val_loss)\n      m_train_loss.append(train_loss)\n      m_val_loss.append(val_loss)\n\n    model.load_state_dict(torch.load('model.pth'))\n    test_loss = 0\n    true = []\n    preds = []\n    \n    for data in test_loader:\n      optimizer.zero_grad()\n      outputs = model(data[0].to(device))\n      labels = data[1].to(device)\n      true += list(labels.detach().cpu().numpy().flatten())\n      preds += list(outputs.detach().cpu().numpy().flatten())\n      loss = criterion(outputs, labels, model.parameters()).detach().cpu().numpy()\n      test_loss += loss/len(test_loader)\n    \n    print('Test Loss: ', test_loss)\n    \n    OOF_preds = pd.DataFrame()\n    OOF_preds['true_value'] = true\n    OOF_preds['preds'] = preds\n    OOF_preds['row'] = test_index\n    OOF_preds.to_csv(os.path.join(results_path, 'OOF_preds_'+str(fold)+'.csv'), index=False)\n    \n    return m_train_loss, m_val_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold,dfx in enumerate(df_list):\n\n    dfx = dfx.sample(frac=1, random_state=242).reset_index(drop=True)\n\n    model = NAM(len(features), n_hidden_layers, hidden_dim, dropout)\n    X_train = dfx[features].iloc[:int(len(dfx)*0.8)]\n    X_test = dfx[features].iloc[int(len(dfx)*0.8):]\n    Y_train = dfx[labels_1].iloc[:int(len(dfx)*0.8)]\n    Y_test = dfx[labels_1].iloc[int(len(dfx)*0.8):]\n\n    print(len(dfx), len(X_train), len(X_test), len(Y_train), len(Y_test))\n\n    m_train_loss, m_val_loss = train_nam(model, X_train, Y_train, X_test, Y_test, l1, l2, batch_size=batch_size, epochs=epochs, fold=fold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"files = os.listdir('/kaggle/working')\ndf = pd.concat([pd.read_csv('/kaggle/working/'+i) for i in files if 'OOF_preds_' in i])\ndf.to_csv('OOF_preds.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('OOF_preds.csv').drop(columns = ['Unnamed: 0'])\ndf = df.sort_values(by = 'row').reset_index(drop = True)\ndf['True_pT'] = 1/df['true_value']\ndf['Predicted_pT'] = 1/df['preds']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fcnn = pd.read_csv('../input/1-pt-regression-swiss-activation-new-data/OOF_preds.csv').drop(columns = ['Unnamed: 0'])\ndf_fcnn = df_fcnn.sort_values(by = 'row').reset_index(drop = True)\ndf_fcnn['True_pT'] = 1/df_fcnn['true_value']\ndf_fcnn['Predicted_pT'] = 1/df_fcnn['preds']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\n\ndef MAE(df):\n    MAE1 = []\n    dx = 0.5\n    for i in range(int(2/dx),int(150/dx)):\n        P = df[(df['True_pT']>=(i-1)*dx)&(df['True_pT']<=(i+1)*dx)]\n        try:\n            p = mae(P['True_pT'],P['Predicted_pT'])\n        except:\n            p=0\n        MAE1.append(p)\n    MAE1 = MAE1[:196]\n    return MAE1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dx = 0.5\nMAE1 = MAE(df)\nplt.plot([i*dx for i in range(4,200)],MAE1,label = 'NAM')\nplt.plot([i*dx for i in range(4,200)],MAE(df_fcnn),label = 'FCNN')\nplt.xlabel('pT -->')\nplt.ylabel('MAE -->')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Partial Dependence Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NAM(torch.nn.Module):\n    def __init__(self, in_dim, n_hidden_layers, hidden_layer_dim, dropout = 0):\n        super(NAM, self).__init__()\n        self.dropout = dropout\n        self.model = []\n        for i in range(in_dim):\n            if n_hidden_layers==0:\n                layers = [ExU(1, 1)]\n            else:\n                layers = [ExU(1, hidden_layer_dim), torch.nn.Dropout(self.dropout)]\n                for i in range(n_hidden_layers):\n                    layers+=[ExU(hidden_layer_dim, hidden_layer_dim), torch.nn.Dropout(self.dropout)]\n                layers+=[ExU(hidden_layer_dim, 1)]\n            self.model.append(torch.nn.Sequential(*layers))\n            \n        self.model = torch.nn.ModuleList(self.model)\n            \n        self.in_dim = in_dim\n        self.n_hidden_layers = n_hidden_layers\n        self.hidden_layer_dim = hidden_layer_dim\n        \n        self.summation_params = []\n        for i in range(in_dim + 1):\n            self.summation_params.append(torch.nn.init.normal_(Parameter(torch.Tensor(1)), mean=0.5, std=0.5))\n        self.summation_params = torch.nn.ParameterList(self.summation_params)\n            \n    def forward(self, x):\n        \n        partial_output = []\n        output = self.summation_params[0]*self.model[0](x[:,0].reshape(-1,1))\n        partial_output.append((self.summation_params[0]*self.model[0](x[:,0].reshape(-1,1))).detach().cpu().numpy())\n        for i in range(1,self.in_dim):\n            output += self.summation_params[i]*self.model[i](x[:,i].reshape(-1,1))\n            partial_output.append((self.summation_params[i]*self.model[i](x[:,i].reshape(-1,1))).detach().cpu().numpy())\n        output += self.summation_params[self.in_dim]\n        \n        return output, partial_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_values = df_features.max().to_numpy()\nmin_values = df_features.min().to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = NAM(len(features), n_hidden_layers, hidden_dim, dropout)\nmodel.load_state_dict(torch.load('model.pth'))\ninput_to_model = torch.Tensor([[ min_values[j]+(max_values[j]-min_values[j])*i/1000 for i in range(1000)] for j in range(len(features))]).t()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_,partial_output = model(input_to_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(9):\n    plt.plot([min_values[i]+(max_values[i]-min_values[i])*j/1000 for j in range(1000)], partial_output[i].flatten())\n    plt.title('Partial_dependence__of_1/pT_on_'+features[i])\n    plt.xlabel(features[i])\n    plt.ylabel('Componenet in predicted pT')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(9):\n    plt.plot([min_values[i]+(max_values[i]-min_values[i])*j/1000 for j in range(1000)], 1/partial_output[i].flatten())\n    plt.title('Partial_dependence__of_pT_on_'+features[i])\n    plt.xlabel(features[i])\n    plt.ylabel('Componenet in predicted pT')\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}