{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **The highest *Variance Score* I could achieve (0.9848)**\n\nIn this Kernel, I am going to share my thoughts and findings for the Diamonds dataset.  \nI would be greatful if you leave your comments about any sections you like or dislike/disagree.   \n\n**General Steps**:\n1. [**Exploratory Data Analysis (EDA)**](#there_you_go_1)\n2. [**Data Cleaning**](#there_you_go_2)\n3. [**Preprocessing and Feature Engineering**](#there_you_go_3)\n4. [**Training Machine Learning Algorithms**](#there_you_go_4)\n5. [**Hyperparameter Tuning**](#there_you_go_5)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing data visualization libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/diamonds/diamonds.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Unnamed:0 column does not add any useful information. Let's simply delete it.\n\ndf.drop('Unnamed: 0', axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# High level information about the dataset\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**53940 rows and 10 columns (9 features & 1 target).  \nAs you can see above, there are 3 columns with Categorical data type: cut, color and clarity.  \nWe will deal with them in a bit.  \nAlso, it looks like there is no missing value at all (*superficially though*).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get some high level insight about our numerical features\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is something fishy here!  \nThe min for x, y and z are zero which does not make sense.  \nOn top of that, the min for depth (which is multiplication of x, y and z) is not zero!  \nWe need to take care of this issue, for sure.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how many rows (instances) has either x, y or z values equal to zero\n\nlen(df[(df['x']==0) | (df['y']==0) | (df['z']==0)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just 20 rows out of 53,940 rows have either x, y or z values equal to zero.  \nDropping all of them should not be harmful to our final results."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(df['x']!=0) & (df['y']!=0) & (df['z']!=0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's get some high level insight about our categorical features\n\ndf.describe(exclude=[np.number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out all the unique categories of the categorical features\n\ncategory_list = ['cut','color', 'clarity']\nfor cat in category_list:\n    print(f\"Unique values of {cat} column: {df[cat].unique()}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the description of the features, **cut**, **color** and **clarity** are all Oridinal categorical features. There are various ways to convert Ordinal features into Numerical. OrdinalEncoder() is the most famous one. However, I am going to use factorize method (You can check out the following article to figure out why: [https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9](http://)).  \nIf there were any Nominal category, I would have used pd.get_dummies or OneHotEncoding (just a reminder). "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pay attention that the order of categories are from the worst to the best.\n\ncut = pd.Categorical(df['cut'], categories=['Fair','Good','Very Good','Premium','Ideal'], ordered=True)\nlabels_cut, unique = pd.factorize(cut, sort=True)\ndf['cut'] = labels_cut\n\ncolor = pd.Categorical(df['color'], categories=['J','I','H','G','F','E','D'], ordered=True)\nlabels_color, unique = pd.factorize(color, sort=True)\ndf['color'] = labels_color\n\nclarity = pd.Categorical(df['clarity'], categories=['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'], ordered=True)\nlabels_clarity, unique = pd.factorize(clarity, sort=True)\ndf['clarity'] = labels_clarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at our data frame now\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alright way better now. Time to check the corrolations between the features\n\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(),annot=True,square=True,cmap='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable price has the highest correlation with **carat(0.92)**. \n\nprice is highly correlated to **x(0.89)**, **y(0.87)** and **z(0.87)** and also to themselves. At this point, there are two different approaches one can take. Either keep x, y and z as separate features, or drop them all and add ***xyz or volume*** as a new feature to the dataframe. I have tried both methods and it turned out keeping x, y and z, results in better performance.\n\nprice has surprisingly negative correlation with **cut(-0.053)**, **color(-0.17)**, **clarity(-0.15)** and **depth(-0.011)**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# How's the distribution of the target variale\n\nplt.figure(figsize=(12,5))\nsns.distplot(df['price'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seems like there are lots of outliers in the price variable, for better visualization, I will use the boxplot:\n\nplt.figure(figsize=(10,3))\nsns.boxplot(data=df,x=df['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's calculate the max in the boxplot above \n\niqr = df['price'].quantile(q=0.75) - df['price'].quantile(q=0.25)\nmaximum = df['price'].quantile(q=0.75) + 1.5*iqr\nmaximum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of the instances above the maximum\n\nlen(df[df['price']>maximum])/len(df)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6.55%** is relatively a considerable number. Therefore, for now we will keep these instances in the dataframe rather than dropping them.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating features form the target variable\n# Then, spliting the data into train and test datasets prior to feature scaling to avoid data leakage.\n# I chose the test_size=0.1 since the number of instances are big enough.\n\nX = df.drop('price',axis=1)\ny = df['price']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to perform feature scaling.  \nThere are different methods to choose between, **StandardScaler**, **MinMax Scaler**, **MaxAbs Scaler** and **Robust Scaler**.  \nI chose ***StandardScaler*** since it returned better results (you can try other ones and play with them yourself)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing all the required libraries\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instanciating all the models that we are going to apply\n\nlr = LinearRegression()\nlasso = Lasso()\nridge = Ridge()\nsvr = SVR()\nknr = KNeighborsRegressor()\ndtree = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\nabr = AdaBoostRegressor(n_estimators=1000)\nmlpr = MLPRegressor()\nxgb = xgboost.XGBRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the sake of automation, let's create a function to train the model and generate the variance score \n\ndef R2_function(regressor,X_train,y_train,X_test,y_test):\n    regressor.fit(X_train,y_train)\n    predictions = regressor.predict(X_test)\n    return (metrics.explained_variance_score(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_list = [lr, lasso, ridge, svr, knr, dtree, rfr, abr, mlpr, xgb]\n\nfor model in models_list:\n    print(f'{model} R2 score is: {R2_function(model,X_train,y_train,X_test,y_test)} \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top three models are **RandomForestRegressor(R2=0.9846)**, **XGBRegressor(R2=0.9830)** and **KNeighborsRegressor(R2=0.9717)**.  \n\nNow it is time to perform **Hyperparameter Tuning** by using **GridSearchCV**."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest GridSearch\n\nparams_dict = {'n_estimators':[20,40,60,80,100], 'n_jobs':[-1],'max_features':['auto','sqrt','log2']}\nrfr_GridSearch = GridSearchCV(estimator=RandomForestRegressor(), param_grid=params_dict,scoring='r2')\nrfr_GridSearch.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr_GridSearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr_GridSearch_BestParam = RandomForestRegressor(max_features='auto',n_estimators=100,n_jobs=-1)\n\nrfr_GridSearch_BestParam.fit(X_train,y_train)\npredictions = rfr_GridSearch_BestParam.predict(X_test)\nprint(f\"R2 score: {metrics.explained_variance_score(y_test,predictions)}\")\nprint(f\"Mean absolute error: {metrics.mean_absolute_error(y_test,predictions)}\")\nprint(f\"Mean squared error: {metrics.mean_squared_error(y_test,predictions)}\")\nprint(f\"Root Mean squared error: {np.sqrt(metrics.mean_squared_error(y_test,predictions))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Residual Histogram\n\nsns.distplot((y_test-predictions),bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thanks for reading and agian, I would appreciate your comments for improvement purposes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}