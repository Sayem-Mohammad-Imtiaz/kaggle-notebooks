{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Bank Note Authentication - PySpark - StreamLit","metadata":{}},{"cell_type":"markdown","source":"**Description of the data:**\n\nData were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n\n**Class 0 is for Authentic notes and Class 1 is for Fake notes**\n\n**Objective:**\n**To build a classification model that can predict the authenticity of banknotes and deploy it using streamlit library**","metadata":{}},{"cell_type":"code","source":"# Install PySpark\n!pip -q install pyspark flasgger","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing initial libraries","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom pyspark.sql import SparkSession, functions as f\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Exploration","metadata":{}},{"cell_type":"markdown","source":"**Creating a Spark Session**","metadata":{}},{"cell_type":"code","source":"spark = SparkSession.builder.master('local[3]').appName('BankNoteAuthentication').getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading the CSV file**","metadata":{}},{"cell_type":"code","source":"df = spark.read.csv('../input/bank-note-authentication-uci-data/BankNote_Authentication.csv',header=True,inferSchema=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's print the first few elements in the dataset**","metadata":{}},{"cell_type":"code","source":"df.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As shown above, we have 5 columns of which the last column is the target class**","metadata":{}},{"cell_type":"markdown","source":"**Let's print the schema of the data**","metadata":{}},{"cell_type":"code","source":"df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Spark has correctly inferred the schema for this data. Hence type casting is not required.**","metadata":{}},{"cell_type":"markdown","source":"**Searching for null values in df**","metadata":{}},{"cell_type":"code","source":"df.select([f.sum(f.isnan(f.col(c)).cast('int')).alias(f'null_{c}') for c in df.columns]).show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The dataset is very clean and has no null values present.**\n\n**Let's print the shape of the dataset**","metadata":{}},{"cell_type":"code","source":"print(\"(\",df.count(),\",\",len(df.columns),\")\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's have a look at the countplot of the classes available. As of May 26 2021, PySpark doesn't have any plotting functionality. We shall do this using other Python plotting libraries.**","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:56:34.233324Z","iopub.execute_input":"2021-05-26T16:56:34.233739Z","iopub.status.idle":"2021-05-26T16:56:34.240988Z","shell.execute_reply.started":"2021-05-26T16:56:34.233707Z","shell.execute_reply":"2021-05-26T16:56:34.23999Z"}}},{"cell_type":"code","source":"data = df.select('class').toPandas()\nsns.countplot(data=data,x='class')\n_=plt.title(\"Countplot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's separate each feature by class and visualize their distribution.**","metadata":{"execution":{"iopub.status.busy":"2021-05-26T17:55:53.211872Z","iopub.execute_input":"2021-05-26T17:55:53.212226Z","iopub.status.idle":"2021-05-26T17:55:53.24689Z","shell.execute_reply.started":"2021-05-26T17:55:53.212198Z","shell.execute_reply":"2021-05-26T17:55:53.24525Z"}}},{"cell_type":"markdown","source":"**Both the classes have nearly equal count. Hence, the dataset has good balance**","metadata":{}},{"cell_type":"code","source":"#Seperating the dataset according to class for easy plotting\ndata_0=df.where(f.col(\"class\")==0)\ndata_1=df.where(f.col(\"class\")==1)\n\n#KDE plots\ncols = df.columns\ncols.remove('class')\nfig,ax =plt.subplots(2,2,figsize=(8,8))\nfor feature,axes in zip(cols,ax.ravel()):\n    sns.kdeplot(data_0.select(feature).toPandas()[feature],color='blue',ax=axes)\n    sns.kdeplot(data_1.select(feature).toPandas()[feature],color='orange',ax=axes)\n    axes.set_title(feature)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Most of the features nearly follow a normal distribution. From the plots it is understood that 'variance' is a feature that can help distinguish classes the most. 'Entropy' on the other hand exhibit the same distribution for both classes.**","metadata":{}},{"cell_type":"markdown","source":"**Let's visualize a pairplot for better understanding**","metadata":{}},{"cell_type":"code","source":"data = df.toPandas()\nsns.pairplot(data=data,hue='class')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are clear separations shown, especially for pairs of features having 'variance'. The curtosis-entropy scatterplot exhibits the lowest separation**\n\n**Let's perform a PCA analysis to bring down the features to two and plot them. Before that we have to create a single vector of features.**","metadata":{}},{"cell_type":"code","source":"#displaying the dataframe\ndf.show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's create the Feature column using VectorAssembler**","metadata":{}},{"cell_type":"code","source":"#Import VectorAssembler\nfrom pyspark.ml.feature import VectorAssembler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#'cols' is the list of feature names that we have\nprint(cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VecAssembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n#A demo of the output of vector assembler. Later we will assemble all feature transformations in a single pipeline\nVecAssembler.transform(df).select(\"features\",\"class\").show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note that we have a features and a class column now**<br>\n**Let's use the randomSplit method to split the dataframe to train and test sets**","metadata":{}},{"cell_type":"code","source":"#let's choose 0.8 as the training split length, because we have only a few rows of data\ntrain,test = df.randomSplit([0.8,0.2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Earlier we saw in the KDE graphs that the features were nearly following a normal distribution. Hence let' use a StandardScaler to scale the features**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StandardScaler\nfrom pyspark.ml import Pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#specify input col and output col and fit the scaler\nscaler = StandardScaler(inputCol=\"features\",outputCol=\"features_scaled\")\n\n#transform the datasets using a pipeline\nfeature_pipe = Pipeline(stages=[VecAssembler,scaler]).fit(train)\ntrain = feature_pipe.transform(train)\ntest = feature_pipe.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's have a look at the scaled features**","metadata":{}},{"cell_type":"code","source":"train.select(\"features\",\"features_scaled\").show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now that we are ready with the feature vectors, let's perform the PCA decomposition**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import PCA\nfrom pyspark.ml.functions import vector_to_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA decmposition to 2 features\npca = PCA(k=3,inputCol=\"features\",outputCol=\"pca\").fit(train)\ndata = pca.transform(train).select(\"pca\",\"class\")\ndata.show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now the vectors in 'pca' column has to be split to two to plot it**","metadata":{}},{"cell_type":"code","source":"data = data.withColumn(\"pca\",vector_to_array(\"pca\")).select(f.col(\"class\"),f.col(\"pca\")[0],f.col(\"pca\")[1],f.col(\"pca\")[2])\ndata.show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's create a 3D Scatter plot**","metadata":{}},{"cell_type":"code","source":"data = data.toPandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting\nfig = px.scatter_3d(data, x='pca[0]', y='pca[1]', z='pca[2]',color='class',title=\"3D Scatterplot of PCA Features\",color_continuous_scale=px.colors.sequential.Viridis)\nfig.update(layout_coloraxis_showscale=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using PCA, we tried to reduce the number of features to 3 from 4, so that they can be visualized. As you can see, there is an excellent separation between both the classes. Let's use Logistic Classifier or Support Vector Classifier for making the model**","metadata":{}},{"cell_type":"markdown","source":"#### Training - Logistic Regression","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(featuresCol=\"features\",labelCol=\"class\",predictionCol='prediction_lr')\nparams = ParamGridBuilder().addGrid(lr.maxIter,[50,75,100,150,200]).addGrid(lr.regParam,[0,0.0001,0.001,0.01,0.1,0.5]).build()\n\nevaluator = MulticlassClassificationEvaluator(predictionCol='prediction_lr',labelCol='class',metricName='f1')\nCValidator = CrossValidator(estimator=lr,estimatorParamMaps=params,evaluator=evaluator,numFolds=5)\nCValidator_lr = CValidator.fit(train)\n\n#predictions on test data\npredictions_lr= CValidator_lr.transform(test).select('prediction_lr','class')\npredictions_lr.show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**'class' is the original label, 'prediciton' is the predicted label and 'features_scaled' represent the scaled features**\n\n**Let's plot the ROC curve and print the area under it**","metadata":{}},{"cell_type":"code","source":"data = CValidator_lr.bestModel.summary.roc.toPandas()\nsns.lineplot(data=data,x='FPR',y='TPR',color='green')\nprint(\"Area under the ROC curve:\",format(CValidator_lr.bestModel.summary.areaUnderROC,'.4f'))\nprint(\"F1 Score:\",format(evaluator.evaluate(predictions_lr),'.4f'))\nevaluator.setMetricName('accuracy')\nprint(\"Accuracy:\",format(evaluator.evaluate(predictions_lr),'.4f'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wow! we have a nice score!**","metadata":{}},{"cell_type":"markdown","source":"#### Training - Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import LinearSVC","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = LinearSVC(featuresCol='features_scaled',labelCol='class',predictionCol='prediction_svc')\nparams = ParamGridBuilder().addGrid(svc.maxIter,[50,100,200]).addGrid(svc.regParam,[0,0.001,1]).build()\nevaluator.setPredictionCol('prediction_svc')\nevaluator.setMetricName('f1')\n\nCValidator_svc = CrossValidator(estimator=svc,estimatorParamMaps=params,evaluator=evaluator).fit(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_svc = CValidator_svc.transform(test).select(\"prediction_svc\",\"class\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"f1 Score\",format(evaluator.evaluate(predictions_svc),'.4f'))\nevaluator.setMetricName('accuracy')\nprint(\"Accuracy score: \",format(evaluator.evaluate(predictions_svc),'.4f'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Saving the model and feature pipeline**\n\n**Let's save the SVC model**","metadata":{}},{"cell_type":"code","source":"#Save feature pipe\n\nfeature_pipe.save('feature_pipe')\n\n#Save the model\n\nCValidator_svc.bestModel.save('bank_note_model_svc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deployment using StreamLit Library","metadata":{}},{"cell_type":"markdown","source":"#### This is an example code for deployment. Please try it on a local machine.\nThe following code can be saved, say as deploy.py and run as \"streamlit run deploy.py\"","metadata":{}},{"cell_type":"code","source":"!pip -q install streamlit","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:39:55.720085Z","iopub.execute_input":"2021-06-02T18:39:55.720477Z","iopub.status.idle":"2021-06-02T18:40:34.219951Z","shell.execute_reply.started":"2021-06-02T18:39:55.720445Z","shell.execute_reply":"2021-06-02T18:40:34.21867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml.classification import LinearSVCModel\nfrom pyspark.ml.pipeline import PipelineModel\nimport streamlit as st","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:40:46.036171Z","iopub.execute_input":"2021-06-02T18:40:46.037782Z","iopub.status.idle":"2021-06-02T18:40:47.786197Z","shell.execute_reply.started":"2021-06-02T18:40:46.037715Z","shell.execute_reply":"2021-06-02T18:40:47.784419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load feature_pipe and model\n\nfeature_pipe = PipelineModel.load('feature_pipe')\nmodel = LinearSVCModel.load('bank_note_model_svc')\nspark = SparkSession.builder.master('local').appName('deployPyspark').getOrCreate()\n\ndef predict(variance,skewness,curtosis,entropy,spark):\n    schema = \"variance FLOAT, skewness FLOAT, curtosis FLOAT, entropy FLOAT\"\n    data = spark.createDataFrame([[variance,skewness,curtosis,entropy]],schema=schema)\n    data = feature_pipe.transform(data)\n    prediction = model.transform(data).select(\"prediction_svc\").collect()[0][0]\n    return \"Fake Note\" if(prediction) else \"Authentic Note\"\n    \ndef noteAuth():\n    st.title(\"Bank Note Authentication\")\n    st.markdown(\"Application for predicting the authenticity of Bank Notes\")\n\n    variance = float(st.text_input(\"Variance\", 3.6216))\n    skewness = float(st.text_input(\"Skewness\",8.6661))\n    curtosis = float(st.text_input(\"Curtosis\",-2.8073))\n    entropy = float(st.text_input(\"Entropy\",-0.44699))\n\n    if(st.button(\"Predict\")):\n        result = predict(variance,skewness,curtosis,entropy,spark)\n        st.success(f\"Prediction: {result}\")\n\nif __name__ == '__main__':\n    noteAuth()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank You","metadata":{}}]}