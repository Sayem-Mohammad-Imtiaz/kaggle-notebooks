{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Binary Classification for Bike Sales\n\nAlgorithms implemented -\n1. Naive Bayes Classifier\n2. Decision Tree Classifier\n3. Random Forest Classifier\n4. XGBoost Classifier"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bike-buyers/bike_buyers_clean.csv', sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix = df.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = [var for var in df.columns if df[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\nprint('The numerical variables are :', numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numerical].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values in numerical variables\ndf[numerical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))\nprint('The categorical variables are :', categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[categorical].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view frequency counts of values in categorical variables\nfor var in categorical: \n    print(df[var].value_counts())\n    print(df[var].value_counts()/np.float(len(df)))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for cardinality in categorical variables\nfor var in categorical:\n    print(var, ' contains ', len(df[var].unique()), ' labels')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\ndf['Marital Status'] = label_encoder.fit_transform(df['Marital Status'])\ndf['Gender'] = label_encoder.fit_transform(df['Gender'])\ndf['Education'] = label_encoder.fit_transform(df['Education'])\ndf['Occupation'] = label_encoder.fit_transform(df['Occupation'])\ndf['Home Owner'] = label_encoder.fit_transform(df['Home Owner'])\ndf['Commute Distance'] = label_encoder.fit_transform(df['Commute Distance'])\ndf['Region'] = label_encoder.fit_transform(df['Region'])\ndf['Purchased Bike'] = label_encoder.fit_transform(df['Purchased Bike'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorize Continuous Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age'] = pd.cut(x = df['Age'], bins = [0,30,40,50,60,100,150], labels = [0, 1, 2, 3, 4, 5])\ndf['Age'] = df['Age'].astype('int64') \ndf['Age'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Income'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Income'] = pd.cut(x = df['Income'], bins = [0, 30000, 50000, 75000, 100000, 150000, 200000], labels = [1, 2, 3, 4, 5, 6])\ndf['Income'] = df['Income'].astype('int64') \ndf['Income'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Purchased Bike'], axis=1)\ny = df['Purchased Bike']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 999)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gausian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\n\n# instantiate the model\ngnb = GaussianNB()\n\n# fit the model\ngnb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = gnb.predict(X_test)\n\ny_pred[:10]\nlen(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train = gnb.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n', cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                                 index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=999)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n# Predict the response for test dataset\ny_pred2 = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train2 = clf.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\nfn=['ID', 'Marital Status', 'Gender', 'Income', 'Children', 'Education',\n       'Occupation', 'Home Owner', 'Cars', 'Commute Distance', 'Region', 'Age']\ncn=['Bought', 'Not Bought']\n\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(clf, feature_names = fn, \n               class_names=cn, filled=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Improving Accuracy using Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nclf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\ngrid_values = {'criterion': ['gini', 'entropy'], 'max_features': ['auto', 'sqrt', 'log2'], \n               'max_depth':[4,5,6,7,8,9,10], 'min_samples_split': [2,3,4]}\ngrid_clf_acc = GridSearchCV(clf, param_grid = grid_values)\ngrid_clf_acc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_acc = grid_clf_acc.predict(X_test)\n\n# New Model Evaluation metrics \nprint('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))\n\n#Confusion matrix\ncm = confusion_matrix(y_test,y_pred_acc)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive', 'Actual Negative'], \n                                 index=['Predict Positive', 'Predict Negative'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train) \nresult = model.score(X_test, y_test)\n\nprint('Model accuracy score: {0:0.4f}'. format(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer, accuracy_score\n\nrfc = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Run the grid search\ngrid_obj = GridSearchCV(rfc, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrfc = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred4 = grid_clf_acc.predict(X_test)\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Note\n\nIn this kernel, I have analysed the Bike Buyers dataset and performed Binary Classification using various Supervised Learning Classification algorithms. The accuracy is fairly less due the the limitations of the dataset. Let me know how you found this kernel, Happy Kaggling :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}