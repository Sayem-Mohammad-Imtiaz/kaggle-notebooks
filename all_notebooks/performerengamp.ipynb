{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex\n!cd apex; pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install performer-pytorch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performer Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try enwik dataset\nfrom performer_pytorch import PerformerLM\n# Calculates loss\nfrom performer_pytorch.autoregressive_wrapper import AutoregressiveWrapper\n\nimport random\n#import tqdm\nfrom tqdm.notebook import tqdm\nimport gzip\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n# Mixed Precision in PyTorch: - Throws Apex Error (or not?)\nfrom torch.cuda.amp import autocast, GradScaler\n\n# constants\nNUM_BATCHES = 10#int(1e5)\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 1e-4\nVALIDATE_EVERY  = 100\nGENERATE_EVERY  = 500\nGENERATE_LENGTH = 2048\nSEQ_LEN = 4096\n\n# helpers\ndef cycle(loader):\n    while True:\n        for data in loader:\n            yield data\n\ndef decode_token(token):\n    \"\"\"\n    chr: returns character from num; e.g. chr(97)) -> a; chr of <=32 -> whitespace\n    \"\"\"\n    return str(chr(max(32, token)))\n\ndef decode_tokens(tokens):\n    return ''.join(list(map(decode_token, tokens)))\n\n\n# instantiate model\n\nmodel = PerformerLM(\n    num_tokens = 256,\n    dim = 512,\n    depth = 6,\n    max_seq_len = SEQ_LEN,\n    heads = 8,\n    causal = True,\n    reversible = True,\n    nb_features = 256,\n    use_scalenorm = True,\n    local_attn_heads = (8, 8, 8, 6, 4, 2) # Attention Heads per layer\n)\n\nmodel = AutoregressiveWrapper(model)\nmodel.cuda()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare enwik8 data\n\nwith open(\"../input/enwikidataset/enwikinews-dataset.txt\") as file:\n    \n    file = file.read()\n    X = np.fromstring(file, dtype=np.uint8)\n    trX, vaX = np.split(X, [int(len(file)*0.9)])\n    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n    \nclass TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n        return full_seq.cuda()\n\n    def __len__(self):\n        return self.data.size(0) // self.seq_len\n    \ntrain_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\nval_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n\n# optimizer\noptim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscaler = GradScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in tqdm(range(NUM_BATCHES), desc='training'):\n    model.train()\n    \n    ###\n    ##if i % 500 == 0:\n    #    print(i)\n\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n        with autocast():\n            loss = model(next(train_loader), return_loss = True)\n        #loss.backward()\n        scaler.scale(loss).backward()\n        \n    print(f'training loss: {loss.item()}')\n\n    scaler.unscale_(optim)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    scaler.step(optim)\n    scaler.update()\n    #optim.step() # TMP\n    \n    optim.zero_grad()\n\n    if i % VALIDATE_EVERY == 0:\n        model.eval()\n        with torch.no_grad():\n            loss = model(next(val_loader), return_loss = True)\n            print(f'validation loss: {loss.item()}')\n\n    if i % GENERATE_EVERY == 0 and i != 0:\n        model.eval()\n        inp = random.choice(val_dataset)[:-1]\n        prime = decode_tokens(inp)\n        print(f'%s \\n\\n %s', (prime, '*' * 100))\n\n        sample = model.generate(inp, GENERATE_LENGTH)\n        output_str = decode_tokens(sample)\n        print(output_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# W/ AMP, 60secs\n\ntraining loss: 5.719467639923096\nvalidation loss: 5.679171562194824\ntraining loss: 5.688153266906738\ntraining loss: 5.64174747467041\ntraining loss: 5.6018266677856445\ntraining loss: 5.5692853927612305\ntraining loss: 5.528021335601807\ntraining loss: 5.4963812828063965\ntraining loss: 5.4679107666015625\ntraining loss: 5.419412136077881\ntraining loss: 5.399522304534912\n\n\n# W/o AMP, 57 secs\n\ntraining loss: 5.67160701751709\nvalidation loss: 5.638372421264648\ntraining loss: 5.621777057647705\ntraining loss: 5.606544494628906\ntraining loss: 5.550500869750977\ntraining loss: 5.5118408203125\ntraining loss: 5.481593132019043\ntraining loss: 5.444958686828613\ntraining loss: 5.410432815551758\ntraining loss: 5.354680061340332\ntraining loss: 5.319333553314209","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}