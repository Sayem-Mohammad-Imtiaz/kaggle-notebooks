{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Statements","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Imports statements.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n#Data Preprocessing and Feature Engineering\nfrom wordcloud import WordCloud, STOPWORDS \nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n#Model Selection and Validation\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting Insight about the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_all_details(dataframe):\n    print(('='*50)+'DATA'+('='*50))\n    print(('-'*50)+'SHAPE'+('-'*50))\n    print(dataframe.shape)\n    print(('-'*50)+'COLUMNS'+('-'*50))\n    print(dataframe.columns)\n    print(('-'*50)+'DESCRIBE'+('-'*50))\n    print(dataframe.describe())\n    print(('-'*50)+'INFO'+('-'*50))\n    print(dataframe.info())\n    print(('='*50)+'===='+('='*50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data = pd.read_csv('../input/twitterdata/finalSentimentdata2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_all_details(twitter_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking missing values columns\ntwitter_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(twitter_data.sentiment.value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data.sentiment.value_counts().plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a mapping for sentiments\nmapping = {'fear':0,\n          'sad':1,\n          'anger':2,\n          'joy':3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data['sentiment'] = twitter_data['sentiment'].map(mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for tweet in twitter_data.text.head(20):\n    print(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text_column(row):\n    text = row['text'].lower()\n    text = re.sub(r'[^(a-zA-Z\\s)]','',text)\n    text = re.sub(r'\\(','',text)\n    text = re.sub(r'\\)','',text)\n    text = text.replace('\\n',' ')\n    text = text.strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data['cleaned_text'] = twitter_data.apply(clean_text_column,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These are new stopwords which i add after several model runs and found out these are irrelevant words which are created which cleaning process.\nnew_additions=['aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a WordCloud to visualize most frequent words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_string = ''\nstopwords = set(list(STOPWORDS)+new_additions)\nfor val in twitter_data.cleaned_text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    new_string += \" \".join(tokens)+\" \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(new_string) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This seems good and gives a good picture of words frequency in dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for spaced entry which can be created due to cleaning step.\ntwitter_data.cleaned_text.str.isspace().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stopword removal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_sentences = []\nfor tweet in twitter_data.cleaned_text:\n    filtered_sentences.append(remove_stopwords(tweet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_sentence_df = pd.DataFrame(filtered_sentences,columns = ['filter_sentence'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_twitter_data = pd.concat([twitter_data,filter_sentence_df],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_twitter_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing the words in tweets \ndef normalization(tweet):\n    lem = WordNetLemmatizer()\n    normalized_tweet = []\n    for word in tweet['filter_sentence'].split():\n        normalized_text = lem.lemmatize(word,'v')\n        normalized_tweet.append(normalized_text)\n    return normalized_tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_twitter_data['normalised_tweet'] = new_twitter_data.apply(normalization,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_twitter_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msg_train, msg_test, label_train, label_test = train_test_split(new_twitter_data['filter_sentence'],new_twitter_data['sentiment'], test_size=0.1,random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MultinomialNB Pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(msg_train,label_train)\npredictions = pipeline.predict(msg_test)\nprint(classification_report(predictions,label_test))\nprint(confusion_matrix(predictions,label_test))\nprint(accuracy_score(predictions,label_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline2 = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n    ('classifier',LogisticRegression(solver='sag')),  # train on TF-IDF vectors w/ Naive Bayes classifier\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline2.fit(msg_train,label_train)\npredictions2 = pipeline2.predict(msg_test)\nprint(classification_report(predictions2,label_test))\nprint(confusion_matrix(predictions2,label_test))\nprint(accuracy_score(predictions2,label_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline3 = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n                ('clf', OneVsRestClassifier(SVC(), n_jobs=1)),\n            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline3.fit(msg_train,label_train)\npredictions3 = pipeline3.predict(msg_test)\nprint(classification_report(predictions3,label_test))\nprint(confusion_matrix(predictions3,label_test))\nprint(accuracy_score(predictions3,label_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting Classifier pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classifier = VotingClassifier(estimators=[ ('nb', pipeline),('lr', pipeline2), ('svc', pipeline3)], voting='hard')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classifier.fit(msg_train,label_train)\npredictions4 = voting_classifier.predict(msg_test)\nprint(classification_report(predictions4,label_test))\nprint(confusion_matrix(predictions4,label_test))\nprint(accuracy_score(predictions4,label_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Try LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_twitter_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 1000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(new_twitter_data.filter_sentence.values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(new_twitter_data.filter_sentence.values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(new_twitter_data.sentiment).values\nprint('Shape of label tensor:', Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 2)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.1))\nmodel.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 20\nbatch_size = 64\n# I am using EarlyStopping to monitor val_loss upto 3 patience level to prevent the model from overfitting.\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results:\n### * Multinomial naive bayes Accuracy: 71.84%\n### * Logistic Regression Accuracy: 75.00%\n### * Support Vector Machine Accuracy: 74.75%\n### * Voting Classifier Accuracy: 75.00%\n### * LSTM Accuracy: 65.00%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you find this notebook useful, please upvote it!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}