{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport tempfile\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_df = pd.read_csv(r'/kaggle/input/credit-card-customers/BankChurners.csv')\nraw_df = raw_df[raw_df.columns[:-2]]\nraw_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert data types of numerical columns to float32\nint_col = raw_df.dtypes==int\nraw_df.loc[:,int_col[int_col].index.to_list()] = raw_df.loc[:,int_col[int_col].index.to_list()].astype(np.float32)\nfloat_col = raw_df.dtypes==float\nraw_df.loc[:,float_col[float_col].index.to_list()] = raw_df.loc[:,float_col[float_col].index.to_list()].astype(np.float32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check null value of each column\nraw_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_cate={'Existing Customer':1, 'Attrited Customer':0}\nraw_df['Attrition_Flag'] = raw_df['Attrition_Flag'].apply(lambda x:dict_cate[x])\nchurned, existed = np.bincount(raw_df['Attrition_Flag'])\nprint('Existed Customer : {}, Churned Customer : {}'.format(existed, churned))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(raw_df, test_size = 0.2)\ntrain, val = train_test_split(train, test_size = 0.2)\nprint('Train number of rows : {}'.format(train.shape[0]))\nprint('Test number of rows : {}'.format(test.shape[0]))\nprint('Validation number of rows : {}'.format(val.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tf_dataset(df, shuffle=False, batch_size=32):\n    label = df.pop('Attrition_Flag')\n    df = tf.data.Dataset.from_tensor_slices((dict(df), label))\n    if shuffle:\n        df = df.shuffle(train.shape[0])\n    df = df.batch(batch_size)\n    df = df.prefetch(batch_size)\n    return df\ntrain = tf_dataset(train, shuffle=True)\ntest = tf_dataset(test)\nval = tf_dataset(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalization(df, col):\n    normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n    feature = df.map(lambda x, y: x[col])\n    normalizer.adapt(feature)\n    return normalizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def category_encoding(df, col):\n    index = tf.keras.layers.experimental.preprocessing.StringLookup()\n    features = df.map(lambda x, y: x[col])\n    index.adapt(features)\n    encoder = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n    features = features.map(index)\n    encoder.adapt(features)\n    return lambda feature: encoder(index(feature))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_features=[]\ninputs=[]\nfloat_col = raw_df.dtypes==np.float32\nfor col in float_col[float_col].index:\n    numerical_input = tf.keras.Input(shape=(1,), name=col)\n    normalization_layer = normalization(train, col)\n    encoded_col = normalization_layer(numerical_input)\n    inputs.append(numerical_input)\n    encoded_features.append(encoded_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_col = ['Card_Category','Education_Level','Gender','Income_Category','Marital_Status']\nfor col in category_col:\n    categorical_input = tf.keras.Input(shape=(1,), name=col, dtype='string')\n    encoding_layer = category_encoding(train, col)\n    encoded_col = encoding_layer(categorical_input)\n    inputs.append(categorical_input)\n    encoded_features.append(encoded_col)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = tf.keras.layers.concatenate(encoded_features)\nx = tf.keras.layers.Dense(64, activation='relu')(features)\nx = tf.keras.layers.Dropout(0.2)(x)\noutput = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, output)\nmodel.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weight = {0: 1627/(1627+8500), 1:8500/(1627+8500)}\nmodel.fit(train, validation_data=val, epochs=5, class_weight=class_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}