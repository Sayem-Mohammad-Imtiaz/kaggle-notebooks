{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Introduction\n\nIn this notebook I'll use the Medical Cost Personal Datasets from Machine Learning with R by Brett Lantz to follow the Neural Networks and TensorFlow course from Daniel Bourke. \n\n**Can you accurately predict insurance costs?** This dataset shows what the patients medicual insurance price was based on their age, sex, bmi, children, if they smoke and their region.\n\nThis is a **regression problem** with the goal to attempt to determine the strength and character of the relationship between one dependent variable (the *charges*) and a series of the features.","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Data and Packages","metadata":{}},{"cell_type":"code","source":"# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# TensorFlow\nimport tensorflow as tf\n\nprint(\"Setup Complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import data\ndata_filepath = \"/kaggle/input/insurance/insurance.csv\" # Path of the file to read\ndata = pd.read_csv(data_filepath)\n\n# Print the first rows of the training data\ndata.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The number of samples into the dataset is {}.'.format(data.shape[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Manipulation","metadata":{}},{"cell_type":"markdown","source":"## Split the dataframe into features and labels","metadata":{}},{"cell_type":"code","source":"# Creade X & y values\nX = data.drop('charges', axis=1)\ny = data.charges","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-Hot Encode Categorical Variables\nAs we're working with regression, we need to transform the categorical variables into numerical\n\n## Normalize the data (aka Scaling)\nConverts all values to between 0 and 1 whilst preserving the original distribution\n\n## Dividing the datasets\n\n* **Training set.** the model learns from this data, which will be the 80% of the total data available.\n* **Test set.** the model gets evaluated on this data to test what it has learned, this set will be 20% of the data available","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split\n\n# Create a column transformer\nct = make_column_transformer(\n    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # turn all values in these columns between 0 and 1 \n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n)\n\n# Build our train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the column transformer to our training data\nct.fit(X_train)\n\n# Transform training and test data with normalization (MinMaxScaler) and OneHotEncoder\nX_train_normal = ct.transform(X_train)\nX_test_normal = ct.transform(X_test)\n\n# One-Hot Encoding\ndata_one_hot = pd.get_dummies(data)\ndata_one_hot.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The split of the under_sampled data is as follows\")\nprint(\"X_train: \", len(X_train_normal))\nprint(\"X_test: \", len(X_test_normal))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Building and Evaluating the Model\n\n## Steps in modelling with TensorFlow\n\n1. **Creating a model** - define the input and output layers, as well as the hidden layers of a deep learning model.\n\n2. **Compiling a model** - define the loss function (in other words, the function which tells our model how wrong it is) and the optimizer (tells our model how to improve the patterns its learning) and evaluation metrics (what we can use to interpret the performance of our model).\n\n3. **Fitting a model** -letting the model try to find patterns between X & y (features and labels)","metadata":{}},{"cell_type":"code","source":"# set a random seed\ntf.random.set_seed(64)\n\n# 1. create a model using the Sequential API\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(1)\n])\n\n# 2. compile the model\nmodel.compile(loss = tf.keras.losses.mae,\n             optimizer = tf.keras.optimizers.SGD(),\n             metrics=['mae'])\n\n# 3. Fit the model\nmodel.fit(X_train_normal, y_train, epochs=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Evaluating the model\n","metadata":{}},{"cell_type":"code","source":"# Check the results of the insurance model on the test data\nmodel.evaluate(X_test_normal, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.median(), y_train.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Improving the Model\nRight now it looks like our model isn't performing too well... let's try and improve it!\nTo (try) improve our model, we'll run 2 experiments:\n1. Add an extra layer with more hidden units and use the Adam optimizer\n2. Same as above but train for longer (200 epochs)\n3. Add another extra layer with more hidden units","metadata":{}},{"cell_type":"code","source":"# Building model_1\n# set a random seed\ntf.random.set_seed(64)\n\n# 1. create a model using the Sequential API\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(10),\n    tf.keras.layers.Dense(1)\n])\n\n# 2. compile the model\nmodel_1.compile(loss = tf.keras.losses.mae,\n             optimizer = tf.keras.optimizers.Adam(),\n             metrics=['mae'])\n\n# 3. Fit the model\nhistory_1 = model_1.fit(X_train_normal, y_train, epochs=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results of the model_1 on the test data\nmodel_1.evaluate(X_test_normal, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot history (also known as loss curve or a training curve)\npd.DataFrame(history_1.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building model_2\n# set a random seed\ntf.random.set_seed(64)\n\n# 1. create a model using the Sequential API\nmodel_2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(10),\n    tf.keras.layers.Dense(1)\n])\n\n# 2. compile the model\nmodel_2.compile(loss = tf.keras.losses.mae,\n             optimizer = tf.keras.optimizers.Adam(),\n             metrics=['mae'])\n\n# 3. Fit the model\nhistory_2 = model_2.fit(X_train_normal, y_train, epochs=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results of the model_2 on the test data\nmodel_2.evaluate(X_test_normal, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot history (also known as loss curve or a training curve)\npd.DataFrame(history_2.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building model_3\n# set a random seed\ntf.random.set_seed(64)\n\n# 1. create a model using the Sequential API\nmodel_3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(100),\n    tf.keras.layers.Dense(10),\n    tf.keras.layers.Dense(1)\n])\n\n# 2. compile the model\nmodel_3.compile(loss = tf.keras.losses.mae,\n             optimizer = tf.keras.optimizers.Adam(),\n             metrics=['mae'])\n\n# 3. Fit the model\nmodel_3.fit(X_train_normal, y_train, epochs=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results of the model_3 on the test data\nmodel_3.evaluate(X_test_normal, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare the models\n\nWait, which model is actually working better??","metadata":{}},{"cell_type":"code","source":"# Establish the predictions for each model\ny_preds_1 = model_1.predict(X_test_normal)\ny_preds_2 = model_2.predict(X_test_normal)\ny_preds_3 = model_3.predict(X_test_normal)\n\n# Make some functions to reuse MAE and MSE\ndef mae(y_true, y_pred):\n  return tf.metrics.mean_absolute_error(y_true=y_true,\n                                        y_pred=tf.squeeze(y_pred))\n  \ndef mse(y_true, y_pred):\n  return tf.metrics.mean_squared_error(y_true=y_true,\n                                       y_pred=tf.squeeze(y_pred))\n\n# Calculate the models evaluations values\nmae_1 = mae(y_test, y_preds_1)\nmse_1 = mse(y_test, y_preds_1)\nmae_2 = mae(y_test, y_preds_2)\nmse_2 = mse(y_test, y_preds_2)\nmae_3 = mae(y_test, y_preds_3)\nmse_3 = mse(y_test, y_preds_3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's compare our model's results using a pandas DataFrame\nimport pandas as pd\n\nmodel_results = [[\"model_1\", mae_1.numpy(), mse_1.numpy()],\n                 [\"model_2\", mae_2.numpy(), mse_2.numpy()],\n                 [\"model_3\", mae_3.numpy(), mse_3.numpy()]]\n\nall_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"])\nall_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the discussed models, the last one seems like it's working the best with our data, but definitely not perfect... if you are reading this, **what could I do to improve my results?**\n\n:) :)","metadata":{}}]}