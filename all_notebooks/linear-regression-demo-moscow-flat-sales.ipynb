{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# подключение библиотек для работы с данными и визуализации\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# загрузка данных из файла\nfile_path = \"/kaggle/input/price-of-flats-in-moscow/flats_moscow.csv\"\ndata = pd.read_csv(file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Описание данных, позволяет сразу узнать медианные значения, количество записей по каждому полю, максимум, минимум итд\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**price** - цена квартиры в $1000\n\n**totsp** - общая площадь квартиры, кв.м.\n\n**livesp** - жилая площадь квартиры, кв.м.\n\n**kitsp** - площадь кухни, кв.м.\n\n**dist** - расстояние от центра в км.\n\n**metrdist** - расстояние до метро в минутах\n\n**walk** - 1 – пешком от метро, 0 – на транспорте\n\n**brick** - 1 – кирпичный, монолит ж/б, 0 – другой\n\n**floor** - 1 – этаж кроме первого и последнего, 0 – иначе.\n\n**code** - число от 1 до 8, при помощи которого мы группируем наблюдения по подвыборкам: 1. Наблюдения сгруппированы на севере, вокруг Калужско-Рижской линии метрополитена 2. Север, вокруг Серпуховско-Тимирязевской линии метрополитена 3. Северо-запад, вокруг Замоскворецкой линии метрополитена 4. Северо-запад, вокруг Таганско-Краснопресненской линии метрополитена 5. Юго-восток, вокруг Люблинской линии метрополитена 6. Юго-восток, вокруг Таганско-Краснопресненской линии метрополитена 7. Восток, вокруг Калиниской линии метрополитена 8. Восток, вокруг Арбатско-Покровской линии метрополитена"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target - целевая функция\ntarget = data.price\n# строим график, чтобы посмотреть на распределение целевой функции\nsns.distplot(target)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Поскольку распределение оказалось сильно сдвинуто - работаем с логарифмами\nsns.distplot(np.log(target))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём фигуру 10 на 5 дюймов\nfig = plt.figure(figsize=(10, 5))\n# Прикидываем сколько графиков нужно разместить, в моём случае 2, поэтому я выбрал 1 строку и 2 стобца\n# Строим первый график в разметке (1 строка, 2 стобца)\n# График распределения totsp - общей площади квартиры, кв.м.\nfig.add_subplot(1, 2, 1)\nsns.distplot(data.totsp)\n# Строим второй график в разметке (1 строка, 2 стобца)\n# График распределения livetsp - жилой площади квартиры, кв.м.\nfig.add_subplot(1, 2, 2)\nsns.distplot(data.livesp)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# аналогично целевой функции распределение показателей сильно сдвинуты, поэтому работаем с логарифмами\nfig = plt.figure(figsize=(10, 5))\nfig.add_subplot(1, 2, 1)\nsns.distplot(np.log(data.totsp))\nfig.add_subplot(1, 2, 2)\nsns.distplot(np.log(data.livesp))\n    \nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# с помощью numpy для всеого списка площадей вычисляем список прологарифмированных площадей\nmain = np.log(data.livesp)\n# аналогично для всего списка целевых значений вычисляем список прологарифмированных значений\ntarget_log = np.log(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём фигуру 10 на 10 дюймов, бубем использовать разметку 2x2 (2 строки, 2 столбца)\nfig = plt.figure(figsize=(10, 10))\n# На первом графике обе величины без изменений\nfig.add_subplot(2, 2, 1)\nsns.scatterplot(data.livesp, target)\n# На втором графике величина livesp прологарифмирована, целевая функция без изменений\nfig.add_subplot(2, 2, 2)\nsns.scatterplot(main, target)\n# На третьем графике величина livesp без изменений, целевая функция прологарифмирована\nfig.add_subplot(2, 2, 3)\nsns.scatterplot(data.livesp, target_log)\n# На четвёртом графике величина, и livesp, и целевая функция прологарифмированы\nfig.add_subplot(2, 2, 4)\nsns.scatterplot(main, target_log)\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x$"},{"metadata":{},"cell_type":"markdown","source":"## $J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum_{i = 1}^{m} (h_\\theta(x_i) - y_i)^2 $"},{"metadata":{"trusted":true},"cell_type":"code","source":"# будем строить прямую по двум точкам, для этого возьмём минимальное и максимальное значения по x\nmin_x = main.min()\nmax_x = main.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# и в этих точках найдём значение функции h\nt0 = 0\nt1 = 1\nhmn = t0 + t1 * min_x\nhmx = t0 + t1 * max_x\n\n# Построим два графика на одном\nfig, ax = plt.subplots()\n# декартова плоскость с размеченными точками\nsns.scatterplot(main, target_log, ax=ax)\n# прямая, соответствующая функции h\nsns.lineplot(x=[min_x, max_x], y=[hmn, hmx], ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычислим значение J\nJ = 0\nfor i in range(len(data.livesp)):\n    J += (np.exp(t0 + t1 * main[i]) - data.price[i])**2\nJ /= 2 * len(data.livesp)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(main, target_log)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Наивная попытка подбора коэффициентов\nt1_arr = []\nt2_arr = []\nJ_arr = []\nbest_t1 = -1\nbest_t2 = -1\nbest_J = 1000000\nfor i in np.arange(-2.5, 2.5, 0.1):\n    for j in np.arange(0.0, 2.0, 0.1):\n        tmp_J = 0\n        for k in range(len(data.livesp)):\n            tmp_J += (np.exp(i + j * main[k]) - data.price[k])**2\n        tmp_J /= 2 * len(data.livesp)\n        if tmp_J < best_J:\n            best_i = i\n            best_j = j\n            best_J = tmp_J\n        t1_arr.append(i)\n        t2_arr.append(j)\n        J_arr.append(min(tmp_J, 10000))\nprint(best_J)\nprint(i)\nprint(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3D отрисовка подбора коэффициентов.\n# По оси X откладывается коэффициент theta0, по оси Y откладывается коэффициент theta1, а по Z откладывается значение J(theta1, theta)\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\ntrace1 = go.Scatter3d(\n    x=t1_arr,\n    y=t2_arr,\n    z=J_arr,\n    mode='markers',\n    marker=dict(\n        size=5,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.1\n        ),\n        opacity=1\n    ),\n    name = 'price'\n)\nfig = go.Figure(data=trace1)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t0_best = t0\nt1_best = t1\nhmn = t0_best + t1_best * min_x\nhmx = t0_best + t1_best * max_x\n\n# Построим два графика на одном\nfig, ax = plt.subplots()\n# декартова плоскость с размеченными точками\nsns.scatterplot(main, target_log, ax=ax)\n# прямая, соответствующая функции h\nsns.lineplot(x=[min_x, max_x], y=[hmn, hmx], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Градиентный спуск"},{"metadata":{},"cell_type":"markdown","source":"## Важно! Обновлять значения $\\theta_0$ и $\\theta_1$ нужно одновременно"},{"metadata":{},"cell_type":"markdown","source":"##  $\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0}J(\\theta_0, \\theta_1)$\n##  $\\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1}J(\\theta_0, \\theta_1)$"},{"metadata":{},"cell_type":"markdown","source":"##  $\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} (\\frac{1}{2m}\\sum_{i = 1}^{m} (h_\\theta(x_i) - y_i)^2)$\n##  $\\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} (\\frac{1}{2m}\\sum_{i = 1}^{m} (h_\\theta(x_i) - y_i)^2)$"},{"metadata":{},"cell_type":"markdown","source":"##  $\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} (\\frac{1}{2m}\\sum_{i = 1}^{m} (\\theta_0 + \\theta_1 \\cdot x_i - y_i)^2)$\n##  $\\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_1} (\\frac{1}{2m}\\sum_{i = 1}^{m} (\\theta_0 + \\theta_1 \\cdot x_i - y_i)^2)$"},{"metadata":{},"cell_type":"markdown","source":"##  $\\theta_0 = \\theta_0 - \\frac{\\alpha}{m}\\sum_{i = 1}^{m} (\\theta_0 + \\theta_1 \\cdot x_i - y_i)$\n##  $\\theta_1 = \\theta_1 - \\frac{\\alpha}{m}\\sum_{i = 1}^{m} (\\theta_0 + \\theta_1 \\cdot x_i - y_i)\\cdot x_i$"},{"metadata":{"trusted":true},"cell_type":"code","source":"t0_best = t0\nt1_best = t1\nalpha = 0.0001\n# желаемая точность\neps = 0.0001\n# количество шагов, которое понадобилось сделать для достижения желаемой точности\nc = 1\n\n# Высляем частную производную функции J по t_0 во всех точках выборки, суммируем\nJ_t0 = 0\nfor i in range(len(data.livesp)):\n    J_t0+= (t0_best + t1_best * main[i] - target_log[i])\n# делим на количество\nJ_t0 /= len(data.livesp)\n# умножаем на шаг обучения\nJ_t0 *= alpha\n\n# Высляем частную производную функции J по t_1 во всех точках выборки, суммируем\nJ_t1 = 0\nfor i in range(len(data.livesp)):\n    J_t1 += (t0_best + t1_best * main[i] - target_log[i]) * main[i]\n# делим на количество\nJ_t1 /= len(data.livesp)\n# умножаем на шаг обучения\nJ_t1 *= alpha\n\n# обновляем значения t0 и t1\nt0_best = t0_best - J_t0\nt1_best = t1_best - J_t1\nprint(J_t0, J_t1)\n# выводим на экран лучшие значения t0 и t1\nprint(t0_best, t1_best)\n# отображаем график\nhmn = t0_best + t1_best * min_x\nhmx = t0_best + t1_best * max_x\nfig, ax = plt.subplots()\nsns.scatterplot(main, target_log, ax=ax)\nsns.lineplot(x=[min_x, max_x], y=[hmn, hmx], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Возьмём шаг обучения 0.01\nalpha = 0.01\n# желаемая точность\neps = 0.0001\n# количество шагов, которое понадобилось сделать для достижения желаемой точности\nc = 1\n\n# Высляем частную производную функции J по t_0 во всех точках выборки, суммируем\nJ_t0 = 0\nfor i in range(len(data.livesp)):\n    J_t0+= (t0_best + t1_best * main[i] - target_log[i])\n# делим на количество\nJ_t0 /= len(data.livesp)\n# умножаем на шаг обучения\nJ_t0 *= alpha\n\n# Высляем частную производную функции J по t_1 во всех точках выборки, суммируем\nJ_t1 = 0\nfor i in range(len(data.livesp)):\n    J_t1 += (t0_best + t1_best * main[i] - target_log[i]) * main[i]\n# делим на количество\nJ_t1 /= len(data.livesp)\n# умножаем на шаг обучения\nJ_t1 *= alpha\n\n# обновляем значения t0 и t1\nt0_best = t0_best - J_t0\nt1_best = t1_best - J_t1\n\n# запускаем цикл, пока J_t0 и J_t1 больше желаемой точности\n# значения J_t0 и J_t1 показывают, на сколько изменились t0_best и t1_best\n# и если это изменение меньшн eps, то мы достигли нужной точности\nwhile (abs(J_t0) > eps or abs(J_t1) > eps):\n    # Высляем частную производную функции J по t_0 во всех точках выборки, суммируем\n    J_t0 = 0\n    for i in range(len(data.livesp)):\n        J_t0+= (t0_best + t1_best * main[i] - target_log[i])\n    # делим на количество\n    J_t0 /= len(data.livesp)\n    # умножаем на шаг обучения\n    J_t0 *= alpha\n\n    # Высляем частную производную функции J по t_1 во всех точках выборки, суммируем\n    J_t1 = 0\n    for i in range(len(data.livesp)):\n        J_t1 += (t0_best + t1_best * main[i] - target_log[i]) * main[i]\n    # делим на количество\n    J_t1 /= len(data.livesp)\n    # умножаем на шаг обучения\n    J_t1 *= alpha\n    # обновляем значения t0 и t1\n    t0_best = t0_best - J_t0\n    t1_best = t1_best - J_t1\n    c += 1\n# выводим на экран количество шагов\nprint(c)\n# выводим на экрнан значения J_t0 и J_t1, чтобы убедиться в достигнутой точности\nprint(J_t0, J_t1)\n# выводим на экран лучшие значения t0 и t1\nprint(t0_best, t1_best)\n# отображаем график\nhmn = t0_best + t1_best * min_x\nhmx = t0_best + t1_best * max_x\nfig, ax = plt.subplots()\nsns.scatterplot(main, target_log, ax=ax)\nsns.lineplot(x=[min_x, max_x], y=[hmn, hmx], ax=ax)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычислим значение J в точке начального приближения t0, t1 \n# в моём случае (0, 1)\n# если работа шла с логарифмическими величинами, то нужно не забыть взять экспоненту e^(h(x))\n# если вы работали с исходными величинами, то np.exp в вашем случае не будет\nJ = 0\nfor i in range(len(data.livesp)):\n    J += (np.exp(t0 + t1 * main[i]) - data.price[i])**2\nJ /= 2 * len(data.livesp)\nprint(J)\n\n# Вычислим значение J в точке t0_best, t1_best\n# аналогично если работа шла с логарифмическими величинами, то нужно не забыть взять экспоненту e^(h(x))\n# если вы работали с исходными величинами, то np.exp в вашем случае не будет\nJ = 0\nfor i in range(len(data.livesp)):\n    J += (np.exp(t0_best + t1_best * main[i]) - data.price[i])**2\nJ /= 2 * len(data.livesp)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Множественная линейная регрессия"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Построим графики других параметром, чтобы выбрать пригодные для построения регрессии\nattribs = data.select_dtypes(exclude='object').drop('price', axis=1).copy()\n\n# построим все 10 графиков на одной фигуре\nfig = plt.figure(figsize=(20,25))\nfor i in range(len(attribs.columns)):\n    fig.add_subplot(4, 3, i+1)\n    # перед построением удаляем NaN значения\n    sns.distplot(attribs.iloc[:, i].dropna())\n    plt.xlabel(attribs.columns[i])\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + \\dots + \\theta_n \\cdot x_n$"},{"metadata":{},"cell_type":"markdown","source":"## $J(\\theta_0, \\theta_1, \\dots, \\theta_n) = \\frac{1}{2m}\\sum_{i = 1}^{m} (h_\\theta(x_i) - y_i)^2 $"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'code']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Построим таблицу корреляций параметров между собой и с целевой функцией"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_corr = data[features + ['price']]\nX_corr.describe()\n\nx_corr_info =  X_corr.corr()\nf, ax = plt.subplots(figsize=(14, 12))\nsns.heatmap(x_corr_info, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Оставим только те, которые имеют высокую степень корреляции с целевой функцией"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['totsp', 'livesp', 'kitsp', 'dist', 'brick']\nprint(len(features))\ntheta0 = 0\nthetaJ = [1 for i in range(len(features))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вычислим значение J\nthetaJnp = np.array(thetaJ)\nX = data[features]\n#print(X)\nhtheta = thetaJnp.dot(np.transpose(X))\nJ = 0\nfor i in range(len(htheta)):\n    J += (t0 + htheta[i] - data['price'][i])**2\nJ /= 2 * len(htheta)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  $\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0}J(\\theta_0, \\theta_1, \\dots, \\theta_n)$\n##  $\\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1, \\dots, \\theta_n)$"},{"metadata":{},"cell_type":"markdown","source":"##  $\\theta_0 = \\theta_0 - \\frac{\\alpha}{m}\\sum_{i = 1}^{m} (\\theta_0 + \\theta_1 \\cdot x^{(1)}_i + \\dots + \\theta_n \\cdot x^{(n)}_i - y_i)$\n##  $\\theta_j = \\theta_j - \\frac{\\alpha}{m}\\sum_{i = 1}^{m} (\\theta_0 + \\theta_1 \\cdot x^{(1)}_i + \\dots + \\theta_n \\cdot x^{(n)}_i - y_i)\\cdot x^{(j)}_i$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Возьмём шаг обучения 0.0001\nalpha = 0.00022\n# желаемая точность\neps = 0.0001\n# количество шагов, которое понадобилось сделать для достижения желаемой точности\nc = 1\n\nX = data[features]\n\n# задаём начальное приближение\ntheta0_best = 0\ntheta_best = [1 for i in range(len(features))]\nhtheta_best = np.array(theta_best).dot(np.transpose(X))\n\ncont = True\n# запускаем цикл, пока хотя бы одно значение частной производной больше eps\nwhile True:  \n    # Высляем частную производную функции J по t_0 во всех точках выборки, суммируем\n    J_theta0 = 0\n    for i in range(len(htheta_best)):\n        J_theta0 += (theta0_best + htheta_best[i] - data.price[i])\n    # делим на количество\n    J_theta0 /= len(htheta_best)\n    # умножаем на шаг обучения\n    J_theta0 *= alpha\n\n    J_theta = [0 for i in range(len(features))]\n    for j in range(len(features)):\n        # Высляем частную производную функции J по t_j во всех точках выборки, суммируем\n        J_theta[j] = 0\n        for i in range(len(htheta_best)):\n            J_theta[j] += (theta0_best + htheta_best[i] - data.price[i]) * data[features[j]][i]\n        # делим на количество\n        J_theta[j] /= len(htheta_best)\n        # умножаем на шаг обучения\n        J_theta[j] *= alpha\n\n    # обновляем значение коэффициентов тета\n    theta0_best = theta0_best - J_theta0\n    theta_best = np.subtract(theta_best, J_theta)\n    htheta_best = np.array(theta_best).dot(np.transpose(X))\n    # пересчитываем оценочную функцию\n    J = 0\n    for i in range(len(htheta_best)):\n        J += (theta0_best + htheta_best[i] - data['price'][i])**2\n    J /= 2 * len(htheta_best)\n    # Выводим на экран J, чтобы убедиться, что оценочная функция улучшается\n    # Если при выводе значения будут увеличиваться, то цикл нужно остановить и скоректировать alpha\n    print(J)\n    # Для примера делаем отсечение в 200 шагов, иначе слишком долго сходится\n    c += 1\n    if c == 200:\n        break\n    # значения J_theta0 и J_theta показывают, на сколько изменились theta0_best и theta_best\n    # и если это изменение меньшн eps, то мы достигли нужной точности\n    continue_loop = False\n    for i in range(len(J_theta)):\n        if abs(J_theta[i]) > eps:\n            continue_loop = True\n            break\n    continue_loop = continue_loop or abs(J_theta0) > eps\n    if not continue_loop:\n        break   \n\nprint('t0 =', theta0_best)\nfor i in range(len(theta_best)):\n    print('t', i + 1, ' = ', theta_best[i], sep=\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Работа с pandas и кроссвалидация"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = data[features][:len(data[features]) // 2]\nX2 = data[features][len(data[features]) // 2:]\ny1 = data['price'][:len(data[features]) // 2]\ny2 = data['price'][len(data[features]) // 2:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LinearRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Обучаем модель на наборе X1 y1\nlreg = LinearRegression().fit(X1, y1)\nprint(lreg.score(X1, y1))\nprint(lreg.score(X2, y2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Находим значение функции J на наборе X1\ny_pred1 = lreg.predict(X1)\nJ = 0\nfor i in range(len(y_pred1)):\n    J += (y1[i] - y_pred1[i])**2\nJ /= 2 * len(y_pred1)\nprint(J)\n# Находим значение функции J на наборе X2\ny_pred2 = lreg.predict(X2)\nJ = 0\nfor i in range(len(y_pred2)):\n    J += (y2[i + len(y_pred1)] - y_pred2[i])**2\nJ /= 2 * len(y_pred2)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GradientBoostingRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n# Обучаем модель на наборе X1 y1\ngbr = GradientBoostingRegressor(learning_rate=0.02).fit(X1, y1)\nprint(gbr.score(X1, y1))\nprint(gbr.score(X2, y2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Находим значение функции J на наборе X1\ny_pred_gbr1 = gbr.predict(X1)\nJ = 0\nfor i in range(len(y_pred_gbr1)):\n    J += (y1[i] - y_pred_gbr1[i])**2\nJ /= 2 * len(y_pred_gbr1)\nprint(J)\n# Находим значение функции J на наборе X2\ny_pred_gbr2 = gbr.predict(X2)\nJ = 0\nfor i in range(len(y_pred_gbr2)):\n    J += (y2[i + len(y_pred_gbr1)] - y_pred_gbr2[i])**2\nJ /= 2 * len(y_pred_gbr2)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoostRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n# Обучаем модель на наборе X1 y1\nada = AdaBoostRegressor().fit(X1, y1)\nprint(ada.score(X1, y1))\nprint(ada.score(X2, y2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Находим значение функции J на наборе X1\ny_pred_ada1 = ada.predict(X1)\nJ = 0\nfor i in range(len(y_pred_ada1)):\n    J += (y1[i] - y_pred_ada1[i])**2\nJ /= 2 * len(y_pred_ada1)\nprint(J)\n# Находим значение функции J на наборе X2\ny_pred_ada2 = ada.predict(X2)\nJ = 0\nfor i in range(len(y_pred_ada2)):\n    J += (y2[i + len(y_pred_ada1)] - y_pred_ada2[i])**2\nJ /= 2 * len(y_pred_ada2)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}