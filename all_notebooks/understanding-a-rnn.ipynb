{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Understanding a RNN ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Datasets**\n\nCheck: https://www.kaggle.com/ehallmar/daily-historical-stock-prices-1970-2018","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport torch.nn.functional as F\nimport tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/daily-historical-stock-prices-1970-2018/historical_stock_prices.csv',index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## df.shape\nCOLUMN_NAMES = ['open','close','adj_close','low','high']\ndf_ge = df.loc['CAT',COLUMN_NAMES]\ndf_ge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\nprint(df_ge.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\nis_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**tqdm**\n\nInstantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable), and you're done!\n\nCheck: https://github.com/tqdm/tqdm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_timeseries1D(mat, y_col_index, TIME_STEPS):\n    # y_col_index is the index of column that would act as output column\n    # total number of time-series samples would be len(mat) - TIME_STEPS\n    dim_0 = mat.shape[0] - TIME_STEPS\n    dim_1 =1\n    x = np.zeros((dim_0, TIME_STEPS,dim_1))\n    y = np.zeros((dim_0,dim_1))\n    \n    for i in tqdm.notebook.tqdm(range(dim_0)):\n        aux = mat[i:TIME_STEPS+i, y_col_index]\n        x[i] = np.expand_dims(aux,axis = 1)\n        y[i] = mat[TIME_STEPS+i, y_col_index]\n        y[i] = np.expand_dims(y[i],axis = 1)\n    print(\"length of time-series i/o\",x.shape,y.shape)\n    return x, y\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**torch.nn**\n\nBase class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:\n\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n    class Model(nn.Module):\n\n        def __init__(self):\n            super(Model, self).__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nCheck: https://pytorch.org/docs/stable/nn.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MV_LSTM(torch.nn.Module):\n    def __init__(self,n_features,seq_length):\n        super(MV_LSTM, self).__init__()\n        self.n_features = n_features\n        self.seq_len = seq_length\n        self.n_hidden = 256 # number of hidden states\n        self.n_layers = 2 # number of LSTM layers (stacked)\n\n        self.l_lstm = torch.nn.LSTM(n_features, \n                                 self.n_hidden,\n                                 self.n_layers, \n                                 batch_first = True, bidirectional = False, dropout = 0.2)\n        # according to pytorch docs LSTM output is \n        # (batch_size,seq_len, num_directions * hidden_size)\n        # when considering batch_first = True\n        self.l_linear = torch.nn.Linear(in_features = self.n_hidden, out_features = 1)\n        \n\n    def forward(self, x, h):        \n        batch_size, seq_len, _ = x.size()\n        \n        #Dudas sobre esta parte *****\n        #(Darle margen de error en los primeros datos)\n        x, self.hidden = self.l_lstm(x,h)\n        #x = x[:,-1,:]\n        x = x[:,-1]\n        x = F.relu(x)\n        x = self.l_linear(x)\n        \n        return x, self.hidden\n    \n    def init_hidden(self,batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_().to(device),weight.new(self.n_layers,batch_size,self.n_hidden).zero_().to(device))\n        return(hidden)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Se usa esta:\nclass LSTMNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n        super(LSTMNet, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        \n        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x, h):\n        out, h = self.lstm(x, h)\n        out = self.fc(self.relu(out[:,-1]))\n        return out, h\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        return hidden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**fit & fit_transform**\n\n\nTo center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation.\n\n**x′=x−μσ**\n\nYou do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the same two parameters μ and σ (values) that you used for centering the training set.\n\nHence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n\nfit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x , but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the same data.\n\ncheck: https://datascience.stackexchange.com/questions/12321/difference-between-fit-and-fit-transform-in-scikit-learn-models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_ge.values\nX.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain, Xtest = train_test_split(X,train_size=0.9, test_size=0.1, shuffle=False)\nXtr, Xval = train_test_split(Xtrain,train_size=0.9, test_size=0.1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shapes\")\nprint(\"Xtrain:\",Xtrain.shape,\"\\nXtr:\", Xtr.shape)\nprint(\"Xtest:\",Xtest.shape,\"\\nXval:\",Xval.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col_index = 2\nscal_lab = MinMaxScaler()\nscal_lab_fit = scal_lab.fit(Xtr[:,y_col_index].reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scal = MinMaxScaler()\nXtr = scal.fit_transform(Xtr)\nXval = scal.transform(Xval)\nXtest = scal.transform(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create train, validation, and test sets\n#cols = df_ge.columns.values.tolist()\n#cols = cols[1:]\n\nX = df_ge.values\n#scal = MinMaxScaler()\n#X = scal.fit_transform(X)\n\n#df_train, df_test = train_test_split(df_ge, train_size=0.9, test_size=0.1, shuffle=False)\nXtrain, Xtest = train_test_split(X,train_size=0.9, test_size=0.1, shuffle=False)\nXtr, Xval = train_test_split(Xtrain,train_size=0.9, test_size=0.1, shuffle=False)\n\ny_col_index = 2\nscal_lab = MinMaxScaler()\nscal_lab_fit = scal_lab.fit(Xtr[:,y_col_index].reshape(-1, 1))\n\n\nscal = MinMaxScaler()\nXtr = scal.fit_transform(Xtr)\nXval = scal.transform(Xval)\nXtest = scal.transform(Xtest)\n\n\n#del df_ge\n#del df_train\n\n\n#Create time series and batchify\n\ntw = 90\nbatch_size = 64\nXtr, ytr = build_timeseries1D(Xtr,y_col_index,tw)\nXval, yval = build_timeseries1D(Xval,y_col_index,tw)\nXtest, ytest = build_timeseries1D(Xtest,y_col_index,tw)\n\n\ntrain_data = TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(ytr))\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n\nval_data = TensorDataset(torch.from_numpy(Xval), torch.from_numpy(yval))\nval_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ytr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**model.to(device) and model=model.to(device)**\n\n\nNo semantic difference. nn.Module.to function moves the model to the device.\n\nBut be cautious. For tensors:\n\n    # tensor a is in CPU\n    device = torch.device('cuda:0')\n    b = a.to(device)\n    # a is still in CPU!\n    # b is in GPU!\n    # a and b are different \n\nFor models:\n\n    # model a is in CPU\n    device = torch.device('cuda:0')\n    b = a.to(device)\n    # a and b are in GPU\n    # a and b point to the same model \n\n\n\ncheck: https://stackoverflow.com/questions/59560043/what-is-the-difference-between-model-todevice-and-model-model-todevice\n\ncheck: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.to","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**torch.optim**\n\nIs a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.\n\n* Note: If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it.\n  Parameters of a model after .cuda() will be different objects with those before the call. In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\n\n\nCheck: https://pytorch.org/docs/stable/optim.html\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features = 1 # this is number of parallel inputs\nn_timesteps = tw # this is number of timesteps\nhidden_dim = 256\noutput_dim = 1\nn_layers = 2\n# create NN\n#model = MV_LSTM(n_features,n_timesteps)\n\nmodel = LSTMNet(n_features,hidden_dim,output_dim,n_layers)\nmodel.to(device)\n\ncriterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ntrain_episodes = 60","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Not pretty sure about this:**\n\nType Markdown and LaTeX: α2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()\nfor t in range(train_episodes):\n    htr = model.init_hidden(batch_size)\n    for x,y in train_loader:\n        htr = tuple([e.data for e in htr])\n        model.zero_grad()\n        output, htr = model(x.to(device).float(),htr) \n        loss = criterion(output, y.to(device).float())  \n        #Calcular gradiente\n        loss.backward() \n        #Mover los datos\n        optimizer.step()        \n        \n    hval = model.init_hidden(batch_size)    \n    for xval,yval in val_loader:\n        hval = tuple([e.data for e in hval])\n        output_val, hval = model(xval.to(device).float(),htr) \n        loss_val = criterion(output_val, yval.to(device).float())  \n        \n        \n        \n    print('step : ' , t , ' loss_train: ' , loss.item(), ' loss_val: ', loss_val.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate the model\nmodel.eval()\nXtest = torch.from_numpy(Xtest)\nhtest = model.init_hidden(Xtest.shape[0])\nout, htest = model(Xtest.to(device).float(), htest)\n\nout = out.cpu().detach().numpy()\nout = scal_lab_fit.inverse_transform(out)\nytest = scal_lab_fit.inverse_transform(ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\n\nax11 = fig.add_subplot(211)\nax11.plot(out)\nax11.plot(ytest,'r')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}