{"cells":[{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Reclamos ciudadanos en Twitter"},{"metadata":{},"cell_type":"markdown","source":"## Introducción"},{"metadata":{},"cell_type":"markdown","source":"Desde hace ya unos años, las redes sociales se volvieron el medio de comunicación preferido de los ciudadanos para realizar reclamos relacionados a la provisión de servicios públicos e infraestructura (electricidad, agua potable, recolección de basura, reportes de baches, etc.)\n\nSi bien esto produjo un avance importante en la comunicación ciudadanía-autoridades, el exceso y la velocidad de generación de la información impide tener un análisis certero de los reclamos como para reaccionar de manera eficaz, entender la causa raíz y prevenir futuros eventos.\n\nEntender esto nos ayudaría por ejemplo a responder las siguientes preguntas:\n\n- ¿Cuáles son los reclamos más frecuentes realizados por los ciudadanos?\n- ¿Cómo varía la intensidad de estos reclamos en el tiempo?\n- ¿Cuál es el sentimiento de las publicaciones realizadas por los ciudadanos hacia las autoridades?\n\nCon el objetivo de lograr un entendimiento mas profundo que nos permita responder a estas y otras preguntas, utilizo los tweets o publicaciones realizadas en Twitter donde se mencionan a la Municipalidad de Asunción (@AsuncionMuni) y al Intendente (@FerreiroMario1)."},{"metadata":{},"cell_type":"markdown","source":"## Análisis Exploratorio"},{"metadata":{},"cell_type":"markdown","source":"### Librerias\nEn primer lugar, importamos todas las librerias necesarias para el análisis. En este caso, decidí utilizar Pandas para la manipulación de los datos con los conocidos dataframes, seaborn y matplotlib para visualizaciones, NLTK para tokenización, en conjunto con sklearn para implementar técnicas como TF-IDF que son útiles para extraer las combinaciones de palabras más relevantes y como entrada para otras técnicas como LDA, que detallo más adelante."},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime, re, spacy, nltk\nimport calendar\nfrom time import time\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk import ngrams\nimport string\nimport pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.pipeline import Pipeline\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Carga de tweets a *pandas* dataframe\nLos tweets se encuentran contenidos en un archivo cuya estructura se detalla en el apartado de datos en Kaggle. Además de cargar los tweets a un dataframe de *pandas*, derivo otros atributos que serán útiles más adelante. Extraer algunos tweets de ejemplo puede ser útil para entender los tipos de datos en cada columna."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweets_muni = pd.read_csv(\"../input/tweets-municipalidad-asuncion/tweets_municipalidad.csv\")\ndf_tweets_muni['created_at'] = pd.to_datetime(df_tweets_muni['created_at'])\ndf_tweets_muni['date'] = pd.to_datetime(df_tweets_muni['created_at'].dt.date)\ndf_tweets_muni['month'] = df_tweets_muni['date'].dt.month\ndf_tweets_muni['year'] = df_tweets_muni['date'].dt.year\ndf_tweets_muni['tweet'] = df_tweets_muni['tweet'].astype(str)\ndf_tweets_muni['year_month'] = df_tweets_muni['date'].dt.to_period('M')","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":true},"cell_type":"code","source":"df_tweets_muni.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deduplicación y filtrado\n\nEn las siguientes celdas, elimino los tweets duplicados que puedieron aparecer al realizar la extracción o en caso que usuarios hayan publicado el mismo tweet varias veces.\n\nLuego, identifico los usuarios con más cantidad de tweets y excluyo aquellas cuentas relacionadas a la Municipalidad, medios de comunicación u otros entes públicos de tal manera a considerar únicamente las publicaciones de los ciudadanos."},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"# Borrar tweets duplicados (nos quedamos con el primero)\ndf_tweets_muni = df_tweets_muni.drop_duplicates(subset=['tweet'], keep='first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usuarios con mas tweets\ndf_tweets_username = df_tweets_muni.groupby('username').count().reset_index()\ndf_tweets_username.sort_values(by='tweet', ascending=False)[['username','tweet']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluir cuentas de la municipalidad o de medios de comunicacion\nfiltro = ~(df_tweets_muni['username'].isin(['AsuncionMuni','pmtasuncion1','AsuDsu','ABCCardinal',\n                                           'LaUnionAM', '780AM', 'AsuDgrrd', 'Universo970py', \n                                           '1000_am', 'EssapSA', 'Ferreiromario1', 'AbastoAsu',\n                                           'ANDEOficial', 'mopcparaguay']))\n\n# Crear copia de dataframe con filtro aplicado\ndf_tweets_muni_filtro = df_tweets_muni[filtro].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Limpieza\nLos tweets publicados pueden tener información poco relevante para análisis textuales como por ejemplo URLs, emails, referencias a imagenes o simbolos. Además, existen palabras conocidas como stopwords que se repiten frecuentemente y no aportan al entendimiento de la conversación (el, la, y, con, para, mi, etc.). NLTK es una librería de NLP que incluye stopwords en diferentes idiomas, incluyendo español."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cargar stopwords en español\nstopwords_es = stopwords.words('spanish')\n\n\"\"\"\nExcluir menciones, emails, URLs y simbolos\n\"\"\"\ndef clean_tweet(tweet):\n    # Convertir a minusculas\n    tweet = tweet.lower()\n    \n    # Excluir menciones o emails\n    tweet = re.sub(r'\\w*@(\\w+\\.*\\w+\\.*\\w+)',' ', tweet)\n    \n    # Excluir simbolos\n    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n    \n    # Excluir URLs \n    tweet = re.sub(r'(?:www\\.|https?)[^\\s]+', ' ', tweet, flags=re.MULTILINE) \n    \n    # Borrar espacios\n    tweet = tweet.strip()\n    \n    # Considerar solo valores alfa numericos\n    tweet_alfa = re.compile(\"^(?![0-9]*$)[a-zA-Z0-9]+$\") \n    \n    # Eliminar stopwords y palabras con longitud <= 2\n    tokens = tweet.split()\n    text = [token for token in tokens if token not in stopwords_es and len(token)>2 and tweet_alfa.match(token)]\n    return ' '.join(text)\n\n# Aplicar filtro a tweets\ndf_tweets_muni_filtro['tweet_cleaned'] = df_tweets_muni_filtro['tweet'].apply(clean_tweet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bigramas frecuentes por año\nLos bigramas son combinaciones de dos palabras que pueden dar una mejor idea de los temas de conversación. En este caso, me interesa conocer los bigramas que más se repiten y para ellos aplico técnicas de tokenización que separan las palabras del texto y cada par se convierte en una fila. También se puede modificar el tamaño del ngram para formar unigramas, trigramas, etc. El bigrama es una opción intermedia que permite tener algo más de contexto pero tiene suficientes ocurrencias para que sea significativa la muestra (mientras mayor sea el ngram, menor el número de ocurrencias)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tamano del ngram\nngram = 2\ntokenizer = TweetTokenizer()\n\n# Tokenizar y aplicar ngram\ndf_tweets_muni_filtro['tokenize'] = df_tweets_muni_filtro['tweet_cleaned'] \\\n                            .apply(tokenizer.tokenize)\ndf_tweets_muni_filtro['ngram'] = df_tweets_muni_filtro['tokenize'] \\\n                            .apply(lambda x: list(ngrams(x, ngram)))\n\n# Una fila por ngram\ndf_tweets_muni_exploded = df_tweets_muni_filtro \\\n                          .explode('ngram')[['date','year','ngram']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agrupar por cantidad de ocurrencias\ndf_tweets_muni_exploded_grouped = df_tweets_muni_exploded \\\n        .groupby(['year', 'ngram'])  \\\n        .agg({'date':'count'}) \\\n        .reset_index() \\\n        .sort_values(by=['year', 'date'], ascending=False) \\\n        .rename(columns={'date':'count'})\n\n# Top 10 por YYYY\ndf_tweets_muni_top_year = df_tweets_muni_exploded_grouped \\\n    .drop_duplicates(subset=['count','year']) \\\n    .groupby(['year']) \\\n    .head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La siguiente gráfica muestra las ocurrencias de cada bigrama por año. Se pueden identificar por ejemplo bigramas que aparecen en multiples años"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = sns.catplot(x=\"count\",y=\"ngram\",\n                   col=\"year\",\n                   data=df_tweets_muni_top_year, kind=\"bar\",\n                   height=5, aspect=.7);\nax.set(xlabel=\"Frequencia\", ylabel=\"Bigrama\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud\nLas nubes de palabras o word clouds permiten visualizar las palabras más frecuentes de un texto utilizando el tamaño para representar la frecuencia o importancia. En este caso, las palabras se extraen de los tweets filtrados (cerca de 200.000).\n\nPara volverlo un poco más divertido y patriota, utilizo un fondo con nuestra bandera pero se puede adaptar a cualquier tipo de imagen."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generación de un wordcloud paraguayo\ntexto_tweets = ' '.join(df_tweets_muni_filtro['tweet_cleaned'])\n\nmask = np.array(Image.open(\"../input/paraguay-flag/paraguay_flag_.png\"))\nwordcloud_py = WordCloud(background_color=\"white\", mode=\"RGBA\", max_words=1000, mask=mask).generate(texto_tweets)\n\n# Utilización de colores de la imagen \nimage_colors = ImageColorGenerator(mask)\nplt.figure(figsize=[12,8])\nplt.imshow(wordcloud_py.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Usuarios únicos\nLa cantidad de usuarios únicos que publican tweets nos puede ayudar a identificar situaciones que provocaron mayores picos de participación en el tiempo. Si nos fijamos la gráfica de abajo, se observan ciertos picos en el 2019, tanto en Mayo como en Julio. \n¿Qué se mencionaba con frecuencia en estas fechas?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_unique_users = df_tweets_muni_filtro.groupby(['date'])['username'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\n\nsns.lineplot(x='date', y='username', data=df_unique_users.reset_index())\n\nplt.title('Usuarios Unicos por Dia')\nplt.ylabel('Usuarios')\nplt.xlabel('Fecha')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_unique_users.reset_index().sort_values('username', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"gente puerca\", \"basura calle\", \"tira basura\".. estos bigramas dan para pensar que en estas fechas pudo haber llovido con mucha frequencia, lo que pudo haber ocasionado raudales que a su vez movieron las basuras de un lado para otro. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\n\ndf_top_unique_days = df_tweets_muni_exploded[df_tweets_muni_exploded['date'].isin(['2019-05-10','2019-05-11'])] \\\n        .groupby('ngram') \\\n        .count() \\\n        .reset_index() \\\n        .sort_values('date', ascending=False) \\\n        .head(10)\n        \nsns.barplot(x='date', y='ngram', data=df_top_unique_days, orient='h')\n\nplt.title('Top bigramas para 10-11/05/2019')\nplt.xlabel('Menciones')\nplt.ylabel('Bigrama')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Análisis por eventos\nCon lo analizado hasta ahora, tenemos una leve idea de los temas de conversación. Además, existen otros temas que pueden ser interesantes y no se encuentran a simple vista. Para entender un poco mejor como estos temas se mencionan en el tiempo, utilizo expresiones regulares que de acuerdo a ciertos patrones de búsqueda identifiquen tweets relacionados a los temas que nos interesan. En este caso, elegí los siguientes temas que me parecieron los más reclamados por la ciudadanía: **basura, raudales, baches y dengue**. Es importante tener en cuenta que un mismo tweet puede pertenecer a mas de una categoria porque de hecho puede existir correlacion entre varios de estos temas."},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"# Filtrar tweets por categorias de acuerdo a palabras claves\ndef get_df_from_criteria(df, criteria):\n    df = df[df['tweet_cleaned'] \\\n            .str \\\n            .contains(criteria, flags=re.IGNORECASE, regex=True)] \\\n            .groupby(['date','year','month'], as_index=False) \\\n            .agg(['count'])['created_at'].reset_index().rename(columns={'count':'tweets'})\n    return df.copy()\n\nfiltro_baches = r'\\bbache|\\bvache|\\bcrater'\nbaches_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_baches)\n\nfiltro_basura = r'\\bbasura|\\brecicla|\\bdesecho|\\btoxico|\\bvertedero|\\bescombro|\\bsucio|\\basco'\nbasura_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_basura)\n \nfiltro_inundado = r'\\binunda|\\blluvia|\\bllueve|\\braudal|\\bdesagu*'    \ninundado_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_inundado)\n\nfiltro_dengue = r'\\bdengue|\\bmosquito|\\baedes|\\bcriadero|\\bminga|\\bfumiga'\ndengue_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_dengue)   ","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(111)\n\nsns.lineplot(x='date', y='tweets', data=baches_x_dia_df[baches_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\nsns.lineplot(x='date', y='tweets', data=basura_x_dia_df[basura_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\nsns.lineplot(x='date', y='tweets', data=inundado_x_dia_df[inundado_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\nsns.lineplot(x='date', y='tweets', data=dengue_x_dia_df[dengue_x_dia_df['year'].isin([2017,2018,2019])], ax=ax)\n\nplt.title('Cantidad de Menciones por Dia')\nplt.ylabel('Menciones')\nplt.xlabel('Fecha')\n\nax.legend(['Baches', 'Basura', 'Raudales', 'Dengue'], loc='upper left', prop={'size': 12})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En Mayo vemos picos de menciones a raudales que coinciden con los picos de usuarios unicos que vimos mas arriba. También se observa un pico de conversaciones relacionadas al dengue en Marzo de 2018 que coincide con uno de los brotes mas importantes en los últimos años.\n\nPara facilitar comparaciones interanuales, en las próximas gráficas podemos ver para cada tema como cambian las menciones en el tiempo. En el caso de dengue por ejemplo, se observa un aumento importante en Marzo de 2018. Por otro lado, en las menciones de raudales se ve un crecimiento notorio en Mayo de 2019 en comparación con años anteriores."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agrupar por mes\ndengue_x_mes_df = dengue_x_dia_df.groupby(['year','month']) \\\n                    .sum().reset_index()[['month','year','tweets']]\n\n# Pivotear\ndengue_x_mes_pivot = dengue_x_mes_df \\\n                    .pivot(index='month',columns='year', values='tweets')\n    \ndengue_x_mes_pivot.plot(kind='bar', figsize=(15, 5), color=['lightgray', 'gray', 'black'], rot=0)                                       \nplt.title(\"Comparaciones interanuales de menciones de Dengue\")\nplt.xlabel(\"Mes\", labelpad=16)\nplt.ylabel(\"Menciones (Dengue)\", labelpad=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agrupar por mes\nbasura_x_mes_df = basura_x_dia_df.groupby(['year','month']) \\\n                    .sum().reset_index()[['month','year','tweets']]\n    \n# Pivotear\nbasura_x_mes_pivot = basura_x_mes_df \\\n                    .pivot(index='month',columns='year', values='tweets')\n\nbasura_x_mes_pivot.plot(kind='bar', figsize=(15, 5), color=['lightgray', 'gray', 'black'], rot=0)                                       \nplt.title(\"Comparaciones interanuales de menciones de Basura\")\nplt.xlabel(\"Mes\", labelpad=16)\nplt.ylabel(\"Menciones (Basura)\", labelpad=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agrupar por mes\ninundado_x_mes_df = inundado_x_dia_df.groupby(['year','month']) \\\n                    .sum().reset_index()[['month','year','tweets']]\n\n# Pivotear\ninundado_x_mes_pivot = inundado_x_mes_df \\\n                    .pivot(index='month',columns='year', values='tweets')\n    \ninundado_x_mes_pivot.plot(kind='bar', figsize=(15, 5), color=['lightgray', 'gray', 'black'], rot=0)                                       \nplt.title(\"Comparaciones interanuales de menciones de Raudales\")\nplt.xlabel(\"Mes\", labelpad=16)\nplt.ylabel(\"Menciones (Raudales)\", labelpad=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelado de Topicos con LDA\nPara casos como este, dónde tenemos una buena idea de los temas de conversación en los tweets, las expresiones regulares pueden ser suficientes. Sin embargo, existen otros casos dónde se necesitan de técnicas más avanzadas para identificar temas que pueden estar escondidos, o latentes. \n\nExisten diferentes técnicas de identificación de temas o tópicos pero una de las más utilizadas es Latent Dirichlet Allocation o LDA. Se trata de una técnica que genera un modelo probabilístico que asume que cada tema es una combinación de palabras y que cada documento (o tweet en este caso) es una combinación de temas con diferentes probabilidades.\n\nEn las celdas de abajo, creo un Pipeline que primero aplica una técnica conocida como TF-IDF que calcula la frecuencia de palabras en los tweets, y calcula un score para cada palabra dando menos importancia a aquellas que aparecen con demasiada frecuencia y son poco relevantes. En el siguiente paso del pipeline se entrena el modelo y transforma el dataframe original. En este caso, elijo clasificar los temas en 5 diferentes categorias pero el número puede variar de acuerdo al caso. Existen formas de medir la calidad del modelo, calculando lo que se denomina *perplexity* pero no lo voy a utilizar en este caso.\n\nOtro punto importante es que el entrenamiento del modelo puede tomar mucho tiempo y por esto es clave definir ciertos parametros que limiten la busqueda y la cantidad de iteraciones. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cantidad de topicos\nn_topics = 5\n\n# Pipeline con pasos a ejectuar\ntext_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(ngram_range=(2,3), min_df=100, max_df=0.85)),\n    ('lda', LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online'))\n])\nt0 = time()\n# Entrenar y transformar modelo\nlda_model = text_pipeline.fit_transform(df_tweets_muni_filtro['tweet_cleaned'])\nprint(time() - t0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modelos resultantes del pipeline\ntfidf = text_pipeline.steps[0][1]\nlda = text_pipeline.steps[1][1]\nvocabulario = tfidf.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_topics = 5\ntopic_dict = {}\ntopic_scores = []\n\n# Para cada topico, buscamos el top 5 de acuerdo a los scores calculados por el modelo\nfor topic_idx, topic in enumerate(lda.components_):\n    topic_dict[str(topic_idx)] = \",\".join([vocabulario[i] for i in topic.argsort()[:-top_topics - 1:-1]])\n    topic_scores.append([topic[i] for i in topic.argsort()[:-top_topics - 1:-1]])\ndf_topics_lda = pd.DataFrame(topic_dict, index=['bigrams'])\ndf_topics_lda = df_topics_lda.T.reset_index()\n\n# Cargamos las palabras claves por topico en un dataframe\ndf_topics_names = pd.DataFrame(df_topics_lda.bigrams.str.split(',').tolist(), index=df_topics_lda.index) \\\n            .stack() \\\n            .reset_index() \\\n            .drop(['level_1'], axis=1) \\\n            .rename(columns={0:'bigrams', 'level_0':'topic'})\n\n# Cargamos scores por topicos en un dataframe\ndf_topics_scores = pd.DataFrame(topic_scores) \\\n        .stack() \\\n        .reset_index(drop=True)\n\n# Concatenamos palabras claves con scores correspondientes\ndf_topics = pd.concat([df_topics_names, df_topics_scores], axis=1) \\\n        .rename(columns={0:'score'})\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finalmente se observan los bigramas mas frecuentes para cada una de las categorias. No se identifican categorias especificas y esto puede deberse a que no existe una diferenciacion muy clara entre los temas de conversacion. También se pueden obtener mejores resultados realizando validaciones adicionales y excluyendo otras combinaciones poco relevantes."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\n\nfor i in range(1,n_topics+1):    \n    plt.subplot(3,2,i,frameon=True)\n    sns.barplot('score', 'bigrams', data=df_topics[df_topics['topic']==i-1], orient='h') \n    plt.title(\"Topico {}\".format(i))\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Otros análisis que se podrian realizar con estos datos:\n- Análisis de sentimiento\n- Correlación entre temas y visualización con *scattertext*\n- Series temporales para predecir eventos (como brotes de dengue)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}