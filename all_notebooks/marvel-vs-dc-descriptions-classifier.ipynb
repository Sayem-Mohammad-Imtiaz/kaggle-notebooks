{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-12T11:55:44.843602Z","iopub.execute_input":"2021-08-12T11:55:44.843983Z","iopub.status.idle":"2021-08-12T11:55:44.852625Z","shell.execute_reply.started":"2021-08-12T11:55:44.843954Z","shell.execute_reply":"2021-08-12T11:55:44.851383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data import and first analysis\n \nLet's start by import the data and give it a look.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n\n\ndf = pd.read_csv('/kaggle/input/marvel-vs-dc-imdb-dataset/Marvel_DC_imdb.csv')\n\ndel df['Unnamed: 0']\nprint(df.head())\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:44.857365Z","iopub.execute_input":"2021-08-12T11:55:44.857814Z","iopub.status.idle":"2021-08-12T11:55:44.907871Z","shell.execute_reply.started":"2021-08-12T11:55:44.857773Z","shell.execute_reply":"2021-08-12T11:55:44.90686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a lot of nans. We can visualize them easily.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nsns.heatmap(df.isnull())","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:44.909447Z","iopub.execute_input":"2021-08-12T11:55:44.909777Z","iopub.status.idle":"2021-08-12T11:55:45.368613Z","shell.execute_reply.started":"2021-08-12T11:55:44.909738Z","shell.execute_reply":"2021-08-12T11:55:45.367566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The only numeric features are IMBD scores and metascores. I will create another one to distinguish Marvel and DC in a boolean way.","metadata":{}},{"cell_type":"code","source":"df['MvsDC'] = df['Category'] == 'Marvel'\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:45.37044Z","iopub.execute_input":"2021-08-12T11:55:45.370785Z","iopub.status.idle":"2021-08-12T11:55:45.385756Z","shell.execute_reply.started":"2021-08-12T11:55:45.370754Z","shell.execute_reply":"2021-08-12T11:55:45.384324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_marvel = df[df['MvsDC']]\ndf_DC = df[df['MvsDC'] == False]\ndf_marvel['IMDB_Score'].dropna()\ndf_DC['IMDB_Score'].dropna()\nprint(df_DC.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:45.387525Z","iopub.execute_input":"2021-08-12T11:55:45.387892Z","iopub.status.idle":"2021-08-12T11:55:45.41246Z","shell.execute_reply.started":"2021-08-12T11:55:45.387859Z","shell.execute_reply":"2021-08-12T11:55:45.411336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_marvel.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:45.413879Z","iopub.execute_input":"2021-08-12T11:55:45.414174Z","iopub.status.idle":"2021-08-12T11:55:45.438476Z","shell.execute_reply.started":"2021-08-12T11:55:45.414147Z","shell.execute_reply":"2021-08-12T11:55:45.4372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_DC.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:45.440267Z","iopub.execute_input":"2021-08-12T11:55:45.441178Z","iopub.status.idle":"2021-08-12T11:55:45.464695Z","shell.execute_reply.started":"2021-08-12T11:55:45.44113Z","shell.execute_reply":"2021-08-12T11:55:45.463452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_marvel.describe())\nprint(df_DC.describe())","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:45.466053Z","iopub.execute_input":"2021-08-12T11:55:45.466642Z","iopub.status.idle":"2021-08-12T11:55:45.493018Z","shell.execute_reply.started":"2021-08-12T11:55:45.466596Z","shell.execute_reply":"2021-08-12T11:55:45.491766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the average for the two numerical features of the movies. In particular we see that the IMDB score for DC is higher than marvel, while it is the opposite for the metascore.","metadata":{}},{"cell_type":"markdown","source":"# Score classification by description\n\nWe go for a kind of crazy idea. We want to understand if a superhero movie is good only through its description. We will use the spacy library for this, as feature the description column, and as target the IMDB score. ","metadata":{}},{"cell_type":"code","source":"import spacy \nfrom math import floor\n\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:45.495777Z","iopub.execute_input":"2021-08-12T11:55:45.496111Z","iopub.status.idle":"2021-08-12T11:55:46.629331Z","shell.execute_reply.started":"2021-08-12T11:55:45.496078Z","shell.execute_reply":"2021-08-12T11:55:46.628248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"nennenWe need to provide the data in the correct fashion. For a multiclassification problem i will have ten classes, from 0 to 9, which are the floor values of the imdb scores. They will be encoded as one-hot dictionaries in votes.","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True,subset=['IMDB_Score'])\n\ndescr = [nlp(doc) for doc in  df['Description'] ]\nvotes =[]\nscore_keys=['0','1','2','3','5','6','7','8','9']\n#for imdb in df['IMDB_Score']:\n#    votes.append({ sc: floor(imdb) == int(sc) }for sc in score_keys  )\nvotes = [{'0': floor(imdb) == 0,'1': floor(imdb) == 1,'2': floor(imdb) == 2,'3': floor(imdb) == 3,\n         '4': floor(imdb) == 4,'5': floor(imdb) == 5,'6': floor(imdb) == 6,'7': floor(imdb) == 7,\n         '8': floor(imdb) == 8,'9': floor(imdb) == 9} for imdb in df['IMDB_Score']]\n    \nvotes  = [{\"cats\": labels} for labels in votes]\n    \n\nfor ent in descr[30].ents:\n    print(ent.text)\n    \nprint(votes[30])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:55:46.630856Z","iopub.execute_input":"2021-08-12T11:55:46.631171Z","iopub.status.idle":"2021-08-12T11:56:04.445242Z","shell.execute_reply.started":"2021-08-12T11:55:46.631139Z","shell.execute_reply":"2021-08-12T11:56:04.444263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we create the data in a way that spacy likes. A tuple with strings and labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX = descr\nylabels = votes\n\nX_train, X_test,y_train,y_test = train_test_split(X,ylabels,test_size=0.001)\n\n#y_train = np.array(y_train)\n#y_oh = OneHotEncoder(sparse=False)\n#y_oh = y_oh.fit_transform(y_train.reshape(len(y_train),1))\n\nX_sp = [(X_train[i],y_train[i]) for i in range(len(y_train))]\nprint(X_sp[:1])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:56:04.446485Z","iopub.execute_input":"2021-08-12T11:56:04.446811Z","iopub.status.idle":"2021-08-12T11:56:04.455945Z","shell.execute_reply.started":"2021-08-12T11:56:04.446775Z","shell.execute_reply":"2021-08-12T11:56:04.454774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We prepare the model for training.","metadata":{}},{"cell_type":"code","source":"from spacy.util import minibatch\n\n\n#textcat = nlp.create_pipe(\n#              \"textcat_multilabel\")\nn_iter = 10\ntextcat = nlp.create_pipe('textcat',\n              config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"ensemble\"})\n\n\n\n\ntextcat.add_label(\"0\")\ntextcat.add_label(\"1\")\ntextcat.add_label(\"2\")\ntextcat.add_label(\"3\")\ntextcat.add_label(\"4\")\ntextcat.add_label(\"5\")\ntextcat.add_label(\"6\")\ntextcat.add_label(\"7\")\ntextcat.add_label(\"8\")\ntextcat.add_label(\"9\")\n\n\nnlp.add_pipe(textcat)\n\ntextcat.labels\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:56:04.457652Z","iopub.execute_input":"2021-08-12T11:56:04.458102Z","iopub.status.idle":"2021-08-12T11:56:04.473412Z","shell.execute_reply.started":"2021-08-12T11:56:04.458058Z","shell.execute_reply":"2021-08-12T11:56:04.472461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define the training function. ","metadata":{}},{"cell_type":"code","source":"import random\n\ndef train(model, train_data, optimizer):\n    losses = {}\n    random.seed(1)\n    random.shuffle(train_data)\n    \n    batches = minibatch(train_data, size=8)\n    for batch in batches:\n        # train_data is a list of tuples [(text0, label0), (text1, label1), ...]\n        # Split batch into texts and labels\n        texts, labels = zip(*batch)\n        \n        # Update model with texts and labels\n        model.update(texts,labels,sgd=optimizer,losses=losses)\n        \n    return losses","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:56:04.474623Z","iopub.execute_input":"2021-08-12T11:56:04.475163Z","iopub.status.idle":"2021-08-12T11:56:04.491691Z","shell.execute_reply.started":"2021-08-12T11:56:04.47511Z","shell.execute_reply":"2021-08-12T11:56:04.490625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"We then execute the training.","metadata":{}},{"cell_type":"code","source":"optimizer = nlp.begin_training()\ntrain_data = X_sp\nn_iter =5\nfor i in range(n_iter):\n    losses = train(nlp, train_data, optimizer)\n    print(losses['textcat'])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:59:49.791627Z","iopub.execute_input":"2021-08-12T11:59:49.792216Z","iopub.status.idle":"2021-08-12T12:01:21.589806Z","shell.execute_reply.started":"2021-08-12T11:59:49.792164Z","shell.execute_reply":"2021-08-12T12:01:21.588556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a trained model capable of guessing the votes of a superhero movie given its description. lol","metadata":{}},{"cell_type":"code","source":"def predict(nlp, docs): \n    # Use the model's tokenizer to tokenize each input text\n    #docs = [nlp.tokenizer(text) for text in texts]\n    \n    # Use textcat to get the scores for each doc\n    textcat= nlp.get_pipe('textcat')\n    scores,_ = textcat.predict(docs)\n    print(scores)\n    \n    # From the scores, find the class with the highest score/probability\n    predicted_class = scores.argmax(axis=1)\n    \n    return predicted_class\n\ndef find_key(input_dict, value):\n    return next((k for k, v in input_dict.items() if v == value), None)\n\ntexts =X_test\ntrue_sc =[]\nfor y in y_test:\n    # list out keys and values separately\n    true_sc.append(int(find_key(y['cats'],True)))\n\npredictions = predict(nlp, texts)\ntrue_pred = np.array(predictions == true_sc)\n#print(predictions)\n#print(true_sc)\n#print(true_pred)\nacc = sum(true_pred)/len(true_sc)\n\n \nfor p, t,sc in zip(predictions, texts,true_sc):\n    print(f\"{textcat.labels[p]}: {t} , true scores: {sc}  \\n\")\n    \nprint('Accuracy = ', acc)    ","metadata":{"execution":{"iopub.status.busy":"2021-08-12T12:01:36.593478Z","iopub.execute_input":"2021-08-12T12:01:36.594009Z","iopub.status.idle":"2021-08-12T12:01:36.608314Z","shell.execute_reply.started":"2021-08-12T12:01:36.593975Z","shell.execute_reply":"2021-08-12T12:01:36.607457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model seems to work very well!\n\nAn alternative would be to use vector embeddings, which we have already thanks to spacy processing, and use sklearn with a support vector machine.\n\nNow we are ready to give votes to invented superhero movies description.","metadata":{}},{"cell_type":"code","source":"desc_inv = 'Green lantern spends the whole day doing taxes calculations.'\n\ndocs_inv = [nlp.tokenizer(desc_inv)]\n\nprint(docs_inv)\n\npred_inv= predict(nlp,docs_inv)\n\nprint(pred_inv)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T12:03:42.548988Z","iopub.execute_input":"2021-08-12T12:03:42.5494Z","iopub.status.idle":"2021-08-12T12:03:42.561535Z","shell.execute_reply.started":"2021-08-12T12:03:42.549362Z","shell.execute_reply":"2021-08-12T12:03:42.560237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}