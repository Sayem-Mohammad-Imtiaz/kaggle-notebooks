{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Predicting Loan Sanction Status using Neural Network**\n\nI am stepping into Deep Learning for the first time. I thought of building a classifier using Keras. In order to go through the full ML experience, I chose an unpopular dataset from Kaggle Datasets. Handled data viz and preprocessing, then built a Sequential NN model. It may seem unnecessary to use Neural Network for numerical prediction, but I would like to see if Deep Learning can perform well in these prediction tasks."},{"metadata":{},"cell_type":"markdown","source":"Dataset source: Loan_data (https://www.kaggle.com/pallavi31/loan-data)\n\nDataset owner: Pallavi Vibhute (https://www.kaggle.com/pallavi31)\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing necessary modules\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/loan-data/Loan_Training_data.csv')\ntest = pd.read_csv('../input/loan-data/Loan_Test_Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, our training data has shape of (614, 13). The last column, Loan_Status, will be our target label.\n\nIt has two classes: 'Y' for sanctioned loans & 'N' for rejected loan applications\n\nMost of the other columns are self-explanatory. 'Loan_Amount_Term' is the number of months within which the loan should be repaid; 'Credit_History' has values 1.0 if the applicant has credit history and 0.0 if they do not; 'Property_Area tells' us the category of location of the collateral property which is provided as security for the loan."},{"metadata":{},"cell_type":"markdown","source":"# **Data Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Loan_Status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value counts of the label shows that we have an imbalanced dataset. Most ML models do not work well with imbalanced datasets. To tackle this, we can either collect more data or add synthetic data using popular tools like SMOTE or sklearn.utils.resample. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a lot of null values in various columns. Since our training dataset is small, dropping rows is not a viable solution. So, I am gonna replace the null values of a feature with the most frequent value of that feature. We will see that in the preprocessing part."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train.Gender, train.LoanAmount, hue=train.Loan_Status)\nplt.legend(loc='upper right')\nplt.title('Loan Amount vs Gender - grouped based on Loan Status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot tells us that male applicants are lent a larger amount compared to female applicants. But the sanction-to-rejection ratio is higher for female applicants than to that of male applicants."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing categorical names for the sake of easier understanding\ntrain.Married = train.Married.apply(lambda x: 'Married' if x == 'Yes' else 'Unmarried')\n\nsns.barplot(train.Gender, train.LoanAmount, hue=train.Married, hue_order = ['Married', 'Unmarried'])\nplt.legend(loc='upper left')\nplt.title('Loan Amount vs Gender - grouped based on Marital Status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But when it comes to marital status, both men and women can get equal loans when they are married."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(train.ApplicantIncome, train.LoanAmount, hue=train.Loan_Status)\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no discernible correlation between Applicant's Income and the Loan Amount. But the scatter plot shows that most of the applicants are people with low-income and the chance of getting a loan does not depend on the applicant's income."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train.Education, hue=train.Loan_Status)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not so surprisingly, people with graduate degree have a higher chance of getting a loan than the applicant's without one."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train.Property_Area, hue=train.Loan_Status)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprisingly, property in semi-urban areas render a higher chance of loan sanction than others. This might be due to the fact that semi-urban areas call for more reconstructional capabilities than the urban areas. A semi-urban area is a lot easier to remodel and build than an urban area. And as expected, rural areas score the least here."},{"metadata":{},"cell_type":"markdown","source":"Next, for a full scale visualization, I am employing the pairplot from seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train, hue='Loan_Status', palette='Set2', diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some interesting things going on here:\n1. As already stated, Applicant's Income does not determine the loan sanction status (plot - row 1, col 1).\n2. But, applicants having credit history have a higher chance of getting a loan (plot - row 5 col 5).\n3. Applicants with higher income **and** credit history have a higher chance of getting a loan (plot row 1 col 5).\n4. Loan Amount KDE (plot - row 3 col 3) peaks around 150 for both sanctioned and rejected loans, with sanctioned overpowering\n   rejected. Beyond the peak, more loan amount invites more rejected applications than sanctioned ones."},{"metadata":{},"cell_type":"markdown","source":"Bottomline from these plots: You have a higher chance of getting a loan if you are a **Married Male Graduate having a Property in Semi-Urban area**."},{"metadata":{},"cell_type":"markdown","source":"Now, the preprocessing part."},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"Step 1 is to split the dataset into features and target."},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_split(train): \n    train_mod = train[~train['LoanAmount'].isnull()] # Loan Amount has few null values but they should not be imputed\n    train_mod.drop('Loan_ID', axis = 1, inplace=True) # Dropping ID column as it is not relevant to the model\n\n    y = train_mod.Loan_Status\n    train_mod.drop('Loan_Status', axis = 1, inplace=True)\n\n    y = y.apply(lambda x: 1 if x == 'Y' else 0) # Changing categories to numerical values\n    \n    return train_mod, y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next step is to fill the null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute(train):\n    cols = train.columns\n    nan_cols = []\n    for col in cols:\n        if(train[col].isnull().sum() > 0):\n            nan_cols.append(col)\n    # nan_cols contains the list of columns having null values\n    \n    argmax_in_nan = {}\n    for col in nan_cols:\n        argmax_in_nan[col] = None\n        argmax_in_nan[col] = train[col].value_counts().idxmax() # Getting the most frequent value in the column\n        \n        train[col].fillna(argmax_in_nan[col], inplace=True)\n            \n    return train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After filling null values, comes the normalization part. MinMaxScaler maps the values to values in range [0,1]. "},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"def scaler(train):\n    num_cols = [col for col in train.select_dtypes(exclude='object').columns]\n    scaler = MinMaxScaler()\n    for col in num_cols:\n        if (col != 'Credit_History'): # Credit_History belongs to int64 datatype but it is a categorical value. So it should not be scaled.\n            train[col] = scaler.fit_transform(train[[col]])\n            \n    return train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, to handle the categorical variables, I am using get_dummies function from pandas. This will create a column for each categorical variable under each feature"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def cat_enc(train):\n    cat_cols = [col for col in train.select_dtypes(include='object').columns]\n    \n    for col in cat_cols:\n        dummies = pd.get_dummies(train[col], prefix=col)\n        train = pd.concat([train,dummies], axis=1)\n        train.drop([col],axis = 1 , inplace=True)\n    \n    return train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compiling all the above steps, this function renders a ML-ready dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(train):\n    train, y = target_split(train)\n    train = impute(train)\n    train = scaler(train)\n    train = cat_enc(train)\n    \n    return train, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mod, y = preprocess(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mod.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After dropping rows with null LoanAmount and preprocessing, the training data has shape of (592, 20)"},{"metadata":{},"cell_type":"markdown","source":"# **Model development**"},{"metadata":{},"cell_type":"markdown","source":"I'm using keras module to build the neural network. I'm going for a wider, shallow network rather than a narrow deeper one. Adam is currently the most popular optimizer. I'm printing the accuracy score of the model at every epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\n\nmodel = Sequential()\n\nmodel.add(Dense(48, kernel_initializer='normal',input_dim = train_mod.shape[1], activation='relu'))\nmodel.add(Dense(96, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(96, kernel_initializer='normal',activation='relu'))\n\nmodel.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I am using a checkpoint list to store the weights of the model when it performs the best. By this method, I can reuse the best weights for the test data."},{"metadata":{},"cell_type":"markdown","source":"The filename stores the epoch number along with the validation loss at that epoch (it is useful in the coming part). I specified the model to use log_loss\nAfter training the model, the weights file with the least validation loss will be selected for predicting."},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_name = 'Weights-{epoch:02d}--{val_loss:.2f}.hdf5'\ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_mod, y, epochs=50, batch_size=37, validation_split = 0.2, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls .","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ls command shows the checkpoints at which the model has the highest accuracy. The files contain the weights for that particular epoch (the two digit number after *Weights-*). "},{"metadata":{},"cell_type":"markdown","source":"So, in this part, I had a problem. Each time I run this kernel, the name of weight file with best weights change. While committing, the filename changes and throws an error in final kernel. Hence, I could not specify one particular filename when loading weights. So, I wrote a piece of code where I check for the filename with lowest validation loss. I used that file in load_weights function."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nbest_weight_file = str()\nval_loss = 100\nfor filename in os.listdir():\n    if(filename.startswith('W')):\n        name, ext = os.path.splitext(filename)\n        if(int(name[-2:]) < val_loss):\n            val_loss = int(name[-2:])\n            best_weight_file = filename\n            \nprint(best_weight_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(best_weight_file)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nscores = model.evaluate(train_mod, y, verbose=0)\nprint(\"Accuracy of model: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overall model gave 82.09% accuracy on training data, which is pretty decent for a shallow neural network."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nIn this kernel, I chose a random problem and built a decent classifier using a shallow neural network. I was able to tackle the problems I faced. If you have any questions or suggestions to improve the kernel, you are welcome to say it."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}