{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport sys\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import f1_score, recall_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\ncompanies = pd.read_csv('../input/financial-distress/Financial Distress.csv')\ncompanies.rename(index=str, columns={\"Company\": \"company\", \"Time\": \"time\", \"Financial Distress\": \"financial_distress\"}, inplace=True)\n\n\n# Quick validity checks\ntotal_n = len(companies.groupby('company')['company'].nunique())\nprint(total_n)\n\ndistress_companies = companies[companies['financial_distress'] < -0.5]\nu_distress = distress_companies['company'].unique()\nprint(u_distress.shape)\n\nfeature_names = list(companies.columns.values)[3:] # ignore first 3: company, time, financial_distress\nprint(feature_names)\n\nf80 = list(companies.groupby('company')['x80'].agg('mean'))\nf80 = [int(c) for c in f80]\n\n\n# Generate new train/val/test sets.\n# Populate the entire pandas array into a dict for easier processing\n\ndatadict = {}\ndistress_dict = {}\n\nfor i in range (1, total_n+1):\n    datadict[i] = {}\n    distress_dict[i] = {}\n\nprint(\"Populating dictionary...\")\nfor idx, row in companies.iterrows():\n    company = row['company']\n    time = int(row['time'])\n    \n    datadict[company][time] = {}\n    \n    if row['financial_distress'] < -0.5:\n        distress_dict[company][time] = 1\n    else:\n        distress_dict[company][time] = 0\n        \n    for feat_idx, column in enumerate(row[3:]):\n        feat = feature_names[feat_idx]\n        datadict[company][time][feat] = column\n        \n\nprint('We can encode categorical feature 80 as a one-hot vector with this many dimensions:')\nprint(len(list(set(f80))))\n\nlabel_binarizer = LabelBinarizer()\nlabel_binarizer.fit(range(max(f80)))\nf80_oh = label_binarizer.transform(f80)\n\n\n\n# Make new features as np array. We'll even add x80 back!\n\ndef rolling_operation(time, train_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods):\n\n    for company in range(1, total_n+1):\n            \n            all_periods_exist = True\n            for j in range(0, lookback_periods):\n                if not time-j in distress_dict[company]:\n                    all_periods_exist = False\n            if not all_periods_exist:\n                continue\n            \n            distress_at_eop = distress_dict[company][time]\n            new_row = [company]\n\n            for feature in feature_names:\n                if feature == 'x80':\n                    continue\n                feat_sum = 0.0\n                variance_arr = []\n                for j in range(0, lookback_periods):\n                    feat_sum += datadict[company][time-j][feature]\n                    variance_arr.append(datadict[company][time-j][feature])\n                new_row.append(feat_sum)\n                new_row.append(np.var(variance_arr))\n                \n            for j in range(0,len(f80_oh[0])):\n                new_row.append(f80_oh[company-1][j])\n\n            if len(new_row) == ((len(feature_names)-1)*2 + 1 + len(f80_oh[0])) : # we have a complete row\n                new_row.append(distress_at_eop)\n                new_row_np = np.asarray(new_row)\n                train_array.append(new_row_np)\n    \n\ndef custom_timeseries_cv(datadict, distress_dict, feature_names, total_n, val_time, test_time, lookback_periods, total_periods=14):\n\n    # Train data\n    train_array = []\n    for _t in range(1, val_time+1):\n        time = (val_time+1) -_t # Start from time period 10 and work backwards\n        train_array_np = rolling_operation(time, train_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods)\n\n    train_array_np = np.asarray(train_array)\n    print(train_array_np.shape)\n    # print(train_array_np[0])\n    \n    # Val data\n    if val_time != test_time:\n        val_array = []\n        for time in range(val_time+1, test_time+1):\n            val_array_np = rolling_operation(time, val_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods)\n\n        val_array_np = np.asarray(val_array)\n        print(val_array_np.shape)\n        # print(val_array_np[0])\n    else:\n        val_array_np = None\n\n    # Test data\n    test_array = []\n    # start from time period 11 and work forwards\n    for time in range(test_time+1,total_periods+1):\n        test_array_np = rolling_operation(time, test_array, datadict, distress_dict, feature_names, total_n,\n                         lookback_periods)\n\n    test_array_np = np.asarray(test_array)\n    print(test_array_np.shape)\n    # print(test_array_np[0])\n    \n    return train_array_np, val_array_np, test_array_np\n\n# Generate our sets\ntrain_array_np, val_array_np, test_array_np = custom_timeseries_cv(datadict, distress_dict, feature_names, total_n, val_time=9, test_time=12, lookback_periods=3, total_periods=14)\n\n# Pull out last column as labels\nX_fit = train_array_np[:,0:train_array_np.shape[1]-1]\ny_fit = train_array_np[:,-1].astype(int)\n\nX_val = val_array_np[:,0:val_array_np.shape[1]-1]\ny_val = val_array_np[:,-1].astype(int)\n\nX_train = np.concatenate((X_fit, X_val))\ny_train = np.concatenate((y_fit, y_val))\n\nX_test = test_array_np[:,0:test_array_np.shape[1]-1]\ny_test = test_array_np[:,-1].astype(int)\n\nnp.set_printoptions(threshold=sys.maxsize)\nprint(X_fit[0,:])\nprint(y_fit)\n\nprint(X_val[0,:])\nprint(y_val)\n\nprint(X_train[0,:])\nprint(y_train)\n\nprint(X_test[0,:])\nprint(y_test)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend as K\n\nimport os\nimport tempfile\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\nMETRICS = [\n    keras.metrics.TruePositives(name='tp'),\n    keras.metrics.FalsePositives(name='fp'),\n    keras.metrics.TrueNegatives(name='tn'),\n    keras.metrics.FalseNegatives(name='fn'), \n    keras.metrics.AUC(name='auc'),\n    precision_m, \n    recall_m,\n    f1_m,\n]\n\n# Функция создания модели\ndef make_model(metrics=METRICS, output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n        \n    model = keras.Sequential([\n        keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[-1],)),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(16, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),\n    ])\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(lr=1e-3),\n        loss=keras.losses.BinaryCrossentropy(),\n        metrics=metrics\n    )\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Веса классов\nneg, pos = np.bincount(y_train)\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos / total))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_for_0 = (1 / neg)*(total)/2.0 \nweight_for_1 = (1 / pos)*(total)/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создание модели\nmodel = make_model()\n\n# Сохранение весов в файл для повторяемости эксперимента\ninitial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\nmodel.save_weights(initial_weights)\n\nEPOCHS = 300 #\nBATCH_SIZE = 2048 # Большой объём батча, чтобы попали образцы меньшего класса\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=50, # default = 10\n    mode='max',\n    restore_best_weights=True)\n\n# Загрузка весов из файла\nmodel.load_weights(initial_weights)\n\n# Тренировка\nbaseline_history = model.fit(\n    X_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n    validation_data=(X_val, y_val),\n    class_weight=class_weight,\n    verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\ndef plot_metrics(history):\n    metrics = ['loss', 'precision_m', 'recall_m', 'f1_m']\n    for n, metric in enumerate(metrics):\n        name = metric.replace(\"_\",\" \").capitalize()\n        plt.subplot(2,2,n+1)\n        plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n        plt.plot(history.epoch, history.history['val_'+metric],\n                 color=colors[0], linestyle=\"--\", label='Val')\n        plt.xlabel('Epoch')\n        plt.ylabel(name)\n        if metric == 'loss':\n            plt.ylim([0, plt.ylim()[1]])\n        elif metric == 'auc':\n            plt.ylim([0.8,1])\n        else:\n            plt.ylim([0,1])\n\n    plt.legend()\n    \nplot_metrics(baseline_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_cm(labels, predictions, p=0.5):\n    cm = confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\n    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n    \n\nbaseline_results = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, baseline_results):\n    print(name, ': ', value) \n    \nprint()\ntrain_predictions_baseline = model.predict(X_train, batch_size=BATCH_SIZE)\ntest_predictions_baseline = model.predict(X_test, batch_size=BATCH_SIZE)\nplot_cm(y_test, test_predictions_baseline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}