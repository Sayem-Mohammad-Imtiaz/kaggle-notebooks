{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://i.ibb.co/6mBRYRq/heart-p.jpg)\n\nHeart Disease (including Coronary Heart Disease, Hypertension, and Stroke) remains the No. 1\ncause of death in the US.The Heart Disease and Stroke Statistics—2019 Update from the **American Heart Association** indicates that:\n* 116.4 million, or 46% of US adults are estimated to have hypertension. These are findings related to the new 2017 Hypertension Clinical Practice Guidelines.\n* On average, someone dies of CVD every 38 seconds. About 2,303 deaths from CVD each day, based on 2016 data.\n* On average, someone dies of a stroke every 3.70 minutes. About 389.4 deaths from stroke each day, based on 2016 data.\n\nIn this notebook i will try to unleash useful insights using this heart disease datasets and by building stacked ensemble model by combining the power of best performing machine learning algorithms.\n\nThis notebook is divided into 13 major steps which are as follows:\n\n1. [Data description](#data-desc)\n2. [Importing Libraries & setting up environment](#imp-lib)\n3. [Loading dataset](#data-load)\n4. [Data Cleaning & Preprocessing](#data-prep)\n5. [Exploratory Data Analysis](#data-eda)\n6. [OUtlier Detection & Removal](#data-out)\n7. [Training & Test Split](#data-train)\n8. [Cross Validation](#cross-val)\n9. [Model Building](#data-model)\n10. [Model Selection](#selct-model)\n11. [Stacked Ensemble](#stack-ensemble)<br>\n12. [Model evaluation & comparison](#model-eval)<br>\n13. [Model Interpretation](#model-inter)\n14. [Conclusion](#data-conc)\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Dataset description<a id='data-desc'></a>\n\nThis dataset consists of 11 features and a target variable. It has 6 nominal variables and 5 numeric variables. The detailed description of all the features are as follows:\n\n**1. Age:** Patients Age in years (Numeric)<br>\n**2. Sex:** Gender of patient (Male - 1, Female - 0) (Nominal)<br>\n**3. Chest Pain Type:** Type of chest pain experienced by patient categorized into 1 typical, 2 typical angina, 3 non-        anginal pain, 4 asymptomatic (Nominal)<br>\n**4. resting bp s:** Level of blood pressure at resting mode in mm/HG (Numerical)<br>\n**5. cholestrol:** Serum cholestrol in mg/dl (Numeric)<br>\n**6. fasting blood sugar:** Blood sugar levels on fasting > 120 mg/dl represents as 1 in case of true and 0 as false (Nominal)<br>\n**7. resting ecg:** Result of electrocardiogram while at rest are represented in 3 distinct values 0 : Normal 1: Abnormality in ST-T wave 2: Left ventricular hypertrophy (Nominal)<br>\n**8. max heart rate:** Maximum heart rate achieved (Numeric)<br>\n**9. exercise angina:** Angina induced by exercise 0 depicting NO 1 depicting Yes (Nominal)<br>\n**10. oldpeak:** Exercise induced ST-depression in comparison with the state of rest (Numeric)<br>\n**11. ST slope:** ST segment measured in terms of slope during peak exercise 0: Normal 1: Upsloping 2: Flat 3: Downsloping (Nominal)<br>\n\n#### Target variable\n**12. target:** It is the target variable which we have to predict 1 means patient is suffering from heart risk and 0 means patient is normal.\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Importing Libraries<a id='imp-lib'></a>","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import log_loss,roc_auc_score,precision_score,f1_score,recall_score,roc_curve,auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score,fbeta_score,matthews_corrcoef\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom imblearn.metrics import geometric_mean_score\nimport warnings\nimport re\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC \nimport xgboost as xgb\nfrom vecstack import stacking\nfrom scipy import stats\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-04T13:28:15.044114Z","iopub.execute_input":"2021-07-04T13:28:15.044371Z","iopub.status.idle":"2021-07-04T13:28:27.28462Z","shell.execute_reply.started":"2021-07-04T13:28:15.044346Z","shell.execute_reply":"2021-07-04T13:28:27.283486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Loading Dataset<a id='data-load'></a>","metadata":{}},{"cell_type":"code","source":"dt = pd.read_csv('/kaggle/input/heart-statlog-cleveland-hungary-final/heart_statlog_cleveland_hungary_final.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.286812Z","iopub.execute_input":"2021-07-04T13:28:27.287083Z","iopub.status.idle":"2021-07-04T13:28:27.320706Z","shell.execute_reply.started":"2021-07-04T13:28:27.28705Z","shell.execute_reply":"2021-07-04T13:28:27.318985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see some of the sample entries of dataset","metadata":{}},{"cell_type":"code","source":"dt.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-04T13:28:27.323731Z","iopub.execute_input":"2021-07-04T13:28:27.324039Z","iopub.status.idle":"2021-07-04T13:28:27.355782Z","shell.execute_reply.started":"2021-07-04T13:28:27.324005Z","shell.execute_reply":"2021-07-04T13:28:27.354183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above dataset entries some of the features should be nominal and to be encoded as their category type. In the next step we will be encoding features to their respective category as per the dataset description.","metadata":{}},{"cell_type":"markdown","source":"## 4. Data Cleaning & Preprocessing <a id='data-prep'></a>\n In this step we will first change the name of columns as some of the columns have weird naming pattern and then we will encode the features into categorical variables","metadata":{}},{"cell_type":"code","source":"# renaming features to proper name\ndt.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope','target']","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.358617Z","iopub.execute_input":"2021-07-04T13:28:27.358909Z","iopub.status.idle":"2021-07-04T13:28:27.364604Z","shell.execute_reply.started":"2021-07-04T13:28:27.358879Z","shell.execute_reply":"2021-07-04T13:28:27.363684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting features to categorical features \n\ndt['chest_pain_type'][dt['chest_pain_type'] == 1] = 'typical angina'\ndt['chest_pain_type'][dt['chest_pain_type'] == 2] = 'atypical angina'\ndt['chest_pain_type'][dt['chest_pain_type'] == 3] = 'non-anginal pain'\ndt['chest_pain_type'][dt['chest_pain_type'] == 4] = 'asymptomatic'\n\n\n\ndt['rest_ecg'][dt['rest_ecg'] == 0] = 'normal'\ndt['rest_ecg'][dt['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndt['rest_ecg'][dt['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\n\ndt['st_slope'][dt['st_slope'] == 0] = 'normal'\ndt['st_slope'][dt['st_slope'] == 1] = 'upsloping'\ndt['st_slope'][dt['st_slope'] == 2] = 'flat'\ndt['st_slope'][dt['st_slope'] == 3] = 'downsloping'\n\ndt[\"sex\"] = dt.sex.apply(lambda  x:'male' if x==1 else 'female')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.367406Z","iopub.execute_input":"2021-07-04T13:28:27.367825Z","iopub.status.idle":"2021-07-04T13:28:27.4246Z","shell.execute_reply.started":"2021-07-04T13:28:27.367794Z","shell.execute_reply":"2021-07-04T13:28:27.423035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the top 5 entries of dataset after feature encoding\ndt.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.42813Z","iopub.execute_input":"2021-07-04T13:28:27.42836Z","iopub.status.idle":"2021-07-04T13:28:27.443761Z","shell.execute_reply.started":"2021-07-04T13:28:27.428338Z","shell.execute_reply":"2021-07-04T13:28:27.442082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see features are encoded sucessfully to their respective categories. Next we will be checking if there is any missing entry or not ?","metadata":{}},{"cell_type":"code","source":"## Checking missing entries in the dataset columnwise\ndt.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.445209Z","iopub.execute_input":"2021-07-04T13:28:27.445411Z","iopub.status.idle":"2021-07-04T13:28:27.46457Z","shell.execute_reply.started":"2021-07-04T13:28:27.445387Z","shell.execute_reply":"2021-07-04T13:28:27.463132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, there are no missing entries in the dataset thats great. Next we will move towards exploring the dataset by performing detailed EDA","metadata":{}},{"cell_type":"markdown","source":"## 5. Exploratory Data Analysis (EDA) <a id='data-eda'></a>","metadata":{}},{"cell_type":"code","source":"# first checking the shape of the dataset\ndt.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.466339Z","iopub.execute_input":"2021-07-04T13:28:27.466617Z","iopub.status.idle":"2021-07-04T13:28:27.482805Z","shell.execute_reply.started":"2021-07-04T13:28:27.466583Z","shell.execute_reply":"2021-07-04T13:28:27.481454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, there are total 1190 records and 11 features with 1 target variable. Lets check the summary of numerical and categorical features.","metadata":{}},{"cell_type":"code","source":"# summary statistics of numerical columns\ndt.describe(include =[np.number])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.48446Z","iopub.execute_input":"2021-07-04T13:28:27.484733Z","iopub.status.idle":"2021-07-04T13:28:27.536573Z","shell.execute_reply.started":"2021-07-04T13:28:27.484703Z","shell.execute_reply":"2021-07-04T13:28:27.536105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above description resting_blood_pressure and cholestrol have some outliers as they have minimum value of 0 whereas cholestrol has outlier on upper side also having maximum value of 603.","metadata":{}},{"cell_type":"code","source":"# summary statistics of categorical columns\ndt.describe(include =[np.object])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.537603Z","iopub.execute_input":"2021-07-04T13:28:27.537905Z","iopub.status.idle":"2021-07-04T13:28:27.561643Z","shell.execute_reply.started":"2021-07-04T13:28:27.537876Z","shell.execute_reply":"2021-07-04T13:28:27.56088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Heart disease (target variable)","metadata":{}},{"cell_type":"code","source":"# Plotting attrition of employees\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=False, figsize=(14,6))\n\nax1 = dt['target'].value_counts().plot.pie( x=\"Heart disease\" ,y ='no.of patients', \n                   autopct = \"%1.0f%%\",labels=[\"Heart Disease\",\"Normal\"], startangle = 60,ax=ax1);\nax1.set(title = 'Percentage of Heart disease patients in Dataset')\n\nax2 = dt[\"target\"].value_counts().plot(kind=\"barh\" ,ax =ax2)\nfor i,j in enumerate(dt[\"target\"].value_counts().values):\n    ax2.text(.5,i,j,fontsize=12)\nax2.set(title = 'No. of Heart disease patients in Dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.562591Z","iopub.execute_input":"2021-07-04T13:28:27.562865Z","iopub.status.idle":"2021-07-04T13:28:27.763753Z","shell.execute_reply.started":"2021-07-04T13:28:27.562842Z","shell.execute_reply":"2021-07-04T13:28:27.762923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is balanced having 629 heart disease patients and 561 normal patients","metadata":{}},{"cell_type":"markdown","source":"### Checking Gender & Agewise Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,12))\nplt.subplot(221)\ndt[\"sex\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = sns.color_palette(\"prism\",5),startangle = 60,labels=[\"Male\",\"Female\"],\nwedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},explode=[.1,.1],shadow =True)\nplt.title(\"Distribution of Gender\")\nplt.subplot(222)\nax= sns.distplot(dt['age'], rug=True)\nplt.title(\"Age wise distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:27.764746Z","iopub.execute_input":"2021-07-04T13:28:27.765021Z","iopub.status.idle":"2021-07-04T13:28:27.999678Z","shell.execute_reply.started":"2021-07-04T13:28:27.764998Z","shell.execute_reply":"2021-07-04T13:28:27.998888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above plot, in this dataset males percentage is way too higher than females where as average age of patients is around 55.","metadata":{}},{"cell_type":"code","source":"attr_1=dt[dt['target']==1]\nattr_0=dt[dt['target']==0]\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.distplot(attr_0['age'])\nplt.title('AGE DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold')\n\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(attr_0['sex'], palette='viridis')\nplt.title('GENDER DISTRIBUTION OF NORMAL PATIENTS', fontsize=15, weight='bold' )\nplt.show()\n\nfig = plt.figure(figsize=(15,5))\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.distplot(attr_1['age'])\nplt.title('AGE DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold')\n\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(attr_1['sex'], palette='viridis')\nplt.title('GENDER DISTRIBUTION OF HEART DISEASE PATIENTS', fontsize=15, weight='bold' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:28.000707Z","iopub.execute_input":"2021-07-04T13:28:28.000968Z","iopub.status.idle":"2021-07-04T13:28:28.554366Z","shell.execute_reply.started":"2021-07-04T13:28:28.000944Z","shell.execute_reply":"2021-07-04T13:28:28.553171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above plot more patients accounts for heart disease in comparison to females whereas mean age for heart disease patients is around 58 to 60 years","metadata":{}},{"cell_type":"markdown","source":"### Distribution of Chest Pain Type","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[3].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Chest Pain type Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.chest_pain_type.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[3].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Chest Pain type Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:28.55763Z","iopub.execute_input":"2021-07-04T13:28:28.557868Z","iopub.status.idle":"2021-07-04T13:28:29.081159Z","shell.execute_reply.started":"2021-07-04T13:28:28.557845Z","shell.execute_reply":"2021-07-04T13:28:29.079715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the Heart Disease patients based on Chest Pain Type\nplot_criteria= ['chest_pain_type', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:29.083231Z","iopub.execute_input":"2021-07-04T13:28:29.083564Z","iopub.status.idle":"2021-07-04T13:28:29.250033Z","shell.execute_reply.started":"2021-07-04T13:28:29.083507Z","shell.execute_reply":"2021-07-04T13:28:29.247988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above plot **76%** of the chest pain type of the heart disease patients have asymptomatic chest pain. \n\nAsymptomatic heart attacks medically known as **silent myocardial infarction (SMI)** annually accounts for around 45-50% of morbidities due to cardiac ailments and even premature deaths in India. The incidences among middle aged people experiencing SMI is twice likely to develop in males than females. The symptoms of SMI being very mild in comparison to an actual heart attack; it is described as a silent killer. Unlike the symptoms in a normal heart attack which includes extreme chest pain, stabbing pain in the arms, neck & jaw, sudden shortness of breath, sweating and dizziness, the symptoms of SMI are very brief and hence confused with regular discomfort and most often ignored.\n\n[reference](https://www.maxhealthcare.in/blogs/cardiology/rise-cases-asymptomatic-heart-attacks-amongst-middle-aged-people)","metadata":{}},{"cell_type":"markdown","source":"### Distribution of Rest ECG","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.rest_ecg.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[2].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Rest ECG Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.rest_ecg.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[2].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Rest ECG Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:29.251968Z","iopub.execute_input":"2021-07-04T13:28:29.252266Z","iopub.status.idle":"2021-07-04T13:28:29.747008Z","shell.execute_reply.started":"2021-07-04T13:28:29.252223Z","shell.execute_reply":"2021-07-04T13:28:29.745117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the Heart Disease patients based on REST ECG\nplot_criteria= ['rest_ecg', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:29.748608Z","iopub.execute_input":"2021-07-04T13:28:29.748942Z","iopub.status.idle":"2021-07-04T13:28:29.794042Z","shell.execute_reply.started":"2021-07-04T13:28:29.74891Z","shell.execute_reply":"2021-07-04T13:28:29.793102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.ibb.co/R3V4tWC/heart1.png)\nAn electrocardiogram records the electrical signals in your heart. It's a common test used to detect heart problems and monitor the heart's status in many situations. Electrocardiograms — also called ECGs or EKGs. but ECG has limits. It measures heart rate and rhythm—but it doesn’t necessarily show blockages in the arteries.Thats why in this dataset around 52% heart disease patients have normal ECG","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=dt.st_slope.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('lightskyblue')\nmn[3].set_color('crimson')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('ST Slope Distribution',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,4))\n\n# Horizontal Bar Plot\ntitle_cnt=attr_1.st_slope.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1], color='silver')\nmn[0].set_color('red')\nmn[3].set_color('blue')\n\n\n# Remove axes splines\nfor s in ['top','bottom','left','right']:\n    ax.spines[s].set_visible(False)\n\n# Remove x,y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad=5)\nax.yaxis.set_tick_params(pad=10)\n\n# Add x,y gridlines\nax.grid(b=True, color='grey', linestyle='-.', linewidth=1, alpha=0.4)\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('ST Slope Distribution of Heart patients',\n             loc='center', pad=10, fontsize=16)\nplt.yticks(weight='bold')\n\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+10, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n# Show Plot\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-04T13:28:29.795278Z","iopub.execute_input":"2021-07-04T13:28:29.795474Z","iopub.status.idle":"2021-07-04T13:28:30.268855Z","shell.execute_reply.started":"2021-07-04T13:28:29.795451Z","shell.execute_reply":"2021-07-04T13:28:30.267945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exploring the Heart Disease patients based on ST Slope\nplot_criteria= ['st_slope', 'target']\ncm = sns.light_palette(\"red\", as_cmap=True)\n(round(pd.crosstab(dt[plot_criteria[0]], dt[plot_criteria[1]], normalize='columns') * 100,2)).style.background_gradient(cmap = cm)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:30.269865Z","iopub.execute_input":"2021-07-04T13:28:30.270051Z","iopub.status.idle":"2021-07-04T13:28:30.30623Z","shell.execute_reply.started":"2021-07-04T13:28:30.270029Z","shell.execute_reply":"2021-07-04T13:28:30.305626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.ibb.co/jw2P8Nr/ST-segment-depression-upsloping-downsloping-horizontal.png)\n\nThe ST segment /heart rate slope (ST/HR slope), has been proposed as a more accurate ECG criterion for diagnosing significant coronary artery disease (CAD) in most of the research papers. \n\nAs we can see from above plot upsloping is positive sign as 74% of the normal patients have upslope where as 72.97% heart patients have flat sloping.","metadata":{}},{"cell_type":"markdown","source":"### Distribution of Numerical features","metadata":{}},{"cell_type":"code","source":"sns.pairplot(dt, hue = 'target', vars = ['age', 'resting_blood_pressure', 'cholesterol'] )","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:30.307117Z","iopub.execute_input":"2021-07-04T13:28:30.307419Z","iopub.status.idle":"2021-07-04T13:28:33.178514Z","shell.execute_reply.started":"2021-07-04T13:28:30.307387Z","shell.execute_reply":"2021-07-04T13:28:33.177612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot it is clear that as the age increases chances of heart disease increases","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x = 'resting_blood_pressure', y = 'cholesterol', hue = 'target', data = dt)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.179507Z","iopub.execute_input":"2021-07-04T13:28:33.179732Z","iopub.status.idle":"2021-07-04T13:28:33.420306Z","shell.execute_reply.started":"2021-07-04T13:28:33.179709Z","shell.execute_reply":"2021-07-04T13:28:33.419423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can see outliers clearly as for some of the patients cholestrol is 0 whereas for one patient both cholestrol and resting bp is 0 which is may be due to missing entries we will filter these ouliers later","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x = 'resting_blood_pressure', y = 'age', hue = 'target', data = dt)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.421679Z","iopub.execute_input":"2021-07-04T13:28:33.421909Z","iopub.status.idle":"2021-07-04T13:28:33.669082Z","shell.execute_reply.started":"2021-07-04T13:28:33.42188Z","shell.execute_reply":"2021-07-04T13:28:33.667802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Outlier Detection & Removal <a id='data-out'></a>","metadata":{}},{"cell_type":"markdown","source":"### Detecting outlier using z-score\n![](https://i.ibb.co/bgRRWrp/outlier.jpg)\n![](https://i.ibb.co/jDHP7Sj/Z-score-formula.jpg)","metadata":{}},{"cell_type":"code","source":"# filtering numeric features as age , resting bp, cholestrol and max heart rate achieved has outliers as per EDA\n\ndt_numeric = dt[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved']]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.671919Z","iopub.execute_input":"2021-07-04T13:28:33.672244Z","iopub.status.idle":"2021-07-04T13:28:33.678288Z","shell.execute_reply.started":"2021-07-04T13:28:33.672205Z","shell.execute_reply":"2021-07-04T13:28:33.676982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_numeric.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.67993Z","iopub.execute_input":"2021-07-04T13:28:33.680184Z","iopub.status.idle":"2021-07-04T13:28:33.701998Z","shell.execute_reply.started":"2021-07-04T13:28:33.680152Z","shell.execute_reply":"2021-07-04T13:28:33.700552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating zscore of numeric columns in the dataset\nz = np.abs(stats.zscore(dt_numeric))\nprint(z)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.703312Z","iopub.execute_input":"2021-07-04T13:28:33.703492Z","iopub.status.idle":"2021-07-04T13:28:33.719068Z","shell.execute_reply.started":"2021-07-04T13:28:33.70347Z","shell.execute_reply":"2021-07-04T13:28:33.717788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from these points it is diffciult to say which points are outliers so we will now define threshold","metadata":{}},{"cell_type":"code","source":"# Defining threshold for filtering outliers \nthreshold = 3\nprint(np.where(z > 3))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.720428Z","iopub.execute_input":"2021-07-04T13:28:33.72083Z","iopub.status.idle":"2021-07-04T13:28:33.736699Z","shell.execute_reply.started":"2021-07-04T13:28:33.720802Z","shell.execute_reply":"2021-07-04T13:28:33.735489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Don’t be confused by the results. The first array contains the list of row numbers and second array respective column numbers, which mean z[30][2] have a Z-score higher than 3. There are total 17 data points which are outliers.","metadata":{}},{"cell_type":"code","source":"#filtering outliers retaining only those data points which are below threshhold\ndt = dt[(z < 3).all(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.738661Z","iopub.execute_input":"2021-07-04T13:28:33.738957Z","iopub.status.idle":"2021-07-04T13:28:33.756395Z","shell.execute_reply.started":"2021-07-04T13:28:33.738926Z","shell.execute_reply":"2021-07-04T13:28:33.755224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking shape of dataset after outlier removal\ndt.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.75742Z","iopub.execute_input":"2021-07-04T13:28:33.757621Z","iopub.status.idle":"2021-07-04T13:28:33.77985Z","shell.execute_reply.started":"2021-07-04T13:28:33.757598Z","shell.execute_reply":"2021-07-04T13:28:33.778813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great !! all the 17 data points which are outliers are now removed.\n\nNow before splitting dataset into train and test we first encode categorical variables as dummy variables and segregate feature and target variable.","metadata":{}},{"cell_type":"code","source":"## encoding categorical variables\ndt = pd.get_dummies(dt, drop_first=True)\n\ndt.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.781637Z","iopub.execute_input":"2021-07-04T13:28:33.781866Z","iopub.status.idle":"2021-07-04T13:28:33.823911Z","shell.execute_reply.started":"2021-07-04T13:28:33.781842Z","shell.execute_reply":"2021-07-04T13:28:33.823068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the shape of dataset\ndt.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.825716Z","iopub.execute_input":"2021-07-04T13:28:33.825927Z","iopub.status.idle":"2021-07-04T13:28:33.832412Z","shell.execute_reply.started":"2021-07-04T13:28:33.825904Z","shell.execute_reply":"2021-07-04T13:28:33.831272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segregating dataset into features i.e., X and target variables i.e., y\nX = dt.drop(['target'],axis=1)\ny = dt['target']","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.833529Z","iopub.execute_input":"2021-07-04T13:28:33.833728Z","iopub.status.idle":"2021-07-04T13:28:33.855242Z","shell.execute_reply.started":"2021-07-04T13:28:33.833706Z","shell.execute_reply":"2021-07-04T13:28:33.853794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Train Test Split <a id='data-train'></a>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.864016Z","iopub.execute_input":"2021-07-04T13:28:33.864263Z","iopub.status.idle":"2021-07-04T13:28:33.876216Z","shell.execute_reply.started":"2021-07-04T13:28:33.86424Z","shell.execute_reply":"2021-07-04T13:28:33.874591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('------------Training Set------------------')\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint('------------Test Set------------------')\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.877649Z","iopub.execute_input":"2021-07-04T13:28:33.877865Z","iopub.status.idle":"2021-07-04T13:28:33.897323Z","shell.execute_reply.started":"2021-07-04T13:28:33.877841Z","shell.execute_reply":"2021-07-04T13:28:33.896346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### feature normalization\nIn this step we will normalize all the numeric feature in the range of 0 to 1","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.fit_transform(X_train[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.899323Z","iopub.execute_input":"2021-07-04T13:28:33.899605Z","iopub.status.idle":"2021-07-04T13:28:33.932607Z","shell.execute_reply.started":"2021-07-04T13:28:33.899574Z","shell.execute_reply":"2021-07-04T13:28:33.930959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.transform(X_test[['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression']])\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.934083Z","iopub.execute_input":"2021-07-04T13:28:33.93437Z","iopub.status.idle":"2021-07-04T13:28:33.966863Z","shell.execute_reply.started":"2021-07-04T13:28:33.934337Z","shell.execute_reply":"2021-07-04T13:28:33.964911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Cross Validation <a id='cross-val'></a>\n\nIn this step, we will build different baseline models and perform 10-fold cross validation to filter top performing baseline models to be used in level 0 of stacked ensemble method.","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\n# function initializing baseline machine learning models\ndef GetBasedModel():\n    basedModels = []\n    basedModels.append(('LR_L2'   , LogisticRegression(penalty='l2')))\n    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n    basedModels.append(('KNN7'  , KNeighborsClassifier(7)))\n    basedModels.append(('KNN5'  , KNeighborsClassifier(5)))\n    basedModels.append(('KNN9'  , KNeighborsClassifier(9)))\n    basedModels.append(('KNN11'  , KNeighborsClassifier(11)))\n    basedModels.append(('CART' , DecisionTreeClassifier()))\n    basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('SVM Linear'  , SVC(kernel='linear',gamma='auto',probability=True)))\n    basedModels.append(('SVM RBF'  , SVC(kernel='rbf',gamma='auto',probability=True)))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier(n_estimators=100,max_features='sqrt')))\n    basedModels.append(('RF_Ent100'   , RandomForestClassifier(criterion='entropy',n_estimators=100)))\n    basedModels.append(('RF_Gini100'   , RandomForestClassifier(criterion='gini',n_estimators=100)))\n    basedModels.append(('ET100'   , ExtraTreesClassifier(n_estimators= 100)))\n    basedModels.append(('ET500'   , ExtraTreesClassifier(n_estimators= 500)))\n    basedModels.append(('MLP', MLPClassifier()))\n    basedModels.append(('SGD3000', SGDClassifier(max_iter=1000, tol=1e-4)))\n    basedModels.append(('XGB_2000', xgb.XGBClassifier(n_estimators= 2000)))\n    basedModels.append(('XGB_500', xgb.XGBClassifier(n_estimators= 500)))\n    basedModels.append(('XGB_100', xgb.XGBClassifier(n_estimators= 100)))\n    basedModels.append(('XGB_1000', xgb.XGBClassifier(n_estimators= 1000)))\n    basedModels.append(('ET1000'   , ExtraTreesClassifier(n_estimators= 1000)))\n    \n    return basedModels\n\n# function for performing 10-fold cross validation of all the baseline models\ndef BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n    seed = 7\n    results = []\n    names = []\n    for name, model in models:\n        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n        cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n         \n        \n    return results,msg","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.968763Z","iopub.execute_input":"2021-07-04T13:28:33.969035Z","iopub.status.idle":"2021-07-04T13:28:33.989651Z","shell.execute_reply.started":"2021-07-04T13:28:33.969003Z","shell.execute_reply":"2021-07-04T13:28:33.988304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:28:33.990839Z","iopub.execute_input":"2021-07-04T13:28:33.991076Z","iopub.status.idle":"2021-07-04T13:29:34.988297Z","shell.execute_reply.started":"2021-07-04T13:28:33.991047Z","shell.execute_reply":"2021-07-04T13:29:34.987663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Model building <a id='data-model'></a>","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Classifier (criterion = 'entropy')","metadata":{}},{"cell_type":"code","source":"rf_ent = RandomForestClassifier(criterion='entropy',n_estimators=100)\nrf_ent.fit(X_train, y_train)\ny_pred_rfe = rf_ent.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:34.989417Z","iopub.execute_input":"2021-07-04T13:29:34.989812Z","iopub.status.idle":"2021-07-04T13:29:35.247885Z","shell.execute_reply.started":"2021-07-04T13:29:34.98978Z","shell.execute_reply":"2021-07-04T13:29:35.24702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi Layer Perceptron","metadata":{}},{"cell_type":"code","source":"mlp = MLPClassifier()\nmlp.fit(X_train,y_train)\ny_pred_mlp = mlp.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:35.249172Z","iopub.execute_input":"2021-07-04T13:29:35.249417Z","iopub.status.idle":"2021-07-04T13:29:36.288933Z","shell.execute_reply.started":"2021-07-04T13:29:35.249388Z","shell.execute_reply":"2021-07-04T13:29:36.2882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K nearest neighbour (n=9)","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(9)\nknn.fit(X_train,y_train)\ny_pred_knn = knn.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:36.294229Z","iopub.execute_input":"2021-07-04T13:29:36.296455Z","iopub.status.idle":"2021-07-04T13:29:36.326648Z","shell.execute_reply.started":"2021-07-04T13:29:36.296413Z","shell.execute_reply":"2021-07-04T13:29:36.325918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extra Tree Classifier (n_estimators=500)","metadata":{}},{"cell_type":"code","source":"et_500 = ExtraTreesClassifier(n_estimators= 500)\net_500.fit(X_train,y_train)\ny_pred_et500 = et_500.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:36.330801Z","iopub.execute_input":"2021-07-04T13:29:36.332778Z","iopub.status.idle":"2021-07-04T13:29:37.183329Z","shell.execute_reply.started":"2021-07-04T13:29:36.332738Z","shell.execute_reply":"2021-07-04T13:29:37.182528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost (n_estimators=100)","metadata":{}},{"cell_type":"code","source":"xgb = xgb.XGBClassifier(n_estimators= 100)\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.18478Z","iopub.execute_input":"2021-07-04T13:29:37.185064Z","iopub.status.idle":"2021-07-04T13:29:37.283027Z","shell.execute_reply.started":"2021-07-04T13:29:37.185026Z","shell.execute_reply":"2021-07-04T13:29:37.282388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Classifier (kernel='linear')","metadata":{}},{"cell_type":"code","source":"svc = SVC(kernel='linear',gamma='auto',probability=True)\nsvc.fit(X_train,y_train)\ny_pred_svc = svc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.28421Z","iopub.execute_input":"2021-07-04T13:29:37.284566Z","iopub.status.idle":"2021-07-04T13:29:37.348129Z","shell.execute_reply.started":"2021-07-04T13:29:37.284521Z","shell.execute_reply":"2021-07-04T13:29:37.347337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Descent","metadata":{}},{"cell_type":"code","source":"sgd = SGDClassifier(max_iter=1000, tol=1e-4)\nsgd.fit(X_train,y_train)\ny_pred_sgd = sgd.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.349243Z","iopub.execute_input":"2021-07-04T13:29:37.349635Z","iopub.status.idle":"2021-07-04T13:29:37.364574Z","shell.execute_reply.started":"2021-07-04T13:29:37.349603Z","shell.execute_reply":"2021-07-04T13:29:37.363622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adaboost Classifier","metadata":{}},{"cell_type":"code","source":"ada = AdaBoostClassifier()\nada.fit(X_train,y_train)\ny_pred_ada = ada.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.365731Z","iopub.execute_input":"2021-07-04T13:29:37.366044Z","iopub.status.idle":"2021-07-04T13:29:37.485516Z","shell.execute_reply.started":"2021-07-04T13:29:37.366019Z","shell.execute_reply":"2021-07-04T13:29:37.484989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### decision Tree Classifier (CART)","metadata":{}},{"cell_type":"code","source":"decc = DecisionTreeClassifier()\ndecc.fit(X_train,y_train)\ny_pred_decc = decc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.486639Z","iopub.execute_input":"2021-07-04T13:29:37.486966Z","iopub.status.idle":"2021-07-04T13:29:37.496917Z","shell.execute_reply.started":"2021-07-04T13:29:37.486941Z","shell.execute_reply":"2021-07-04T13:29:37.495962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### gradient boosting machine ","metadata":{}},{"cell_type":"code","source":"gbm = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')\ngbm.fit(X_train,y_train)\ny_pred_gbm = gbm.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.498398Z","iopub.execute_input":"2021-07-04T13:29:37.498655Z","iopub.status.idle":"2021-07-04T13:29:37.60436Z","shell.execute_reply.started":"2021-07-04T13:29:37.498629Z","shell.execute_reply":"2021-07-04T13:29:37.603148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.  ## 10. Model Selection <a id='selct-model'></a>","metadata":{}},{"cell_type":"code","source":"import xgboost as xgboost\n# selecting list of top performing models to be used in stacked ensemble method\nmodels = [\n    RandomForestClassifier(criterion='entropy',n_estimators=100),\n    MLPClassifier(),\n    RandomForestClassifier(criterion='gini',n_estimators=100),\n    KNeighborsClassifier(9),\n    ExtraTreesClassifier(n_estimators= 500),\n    ExtraTreesClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 1000),\n    xgboost.XGBClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 500),\n    xgboost.XGBClassifier(n_estimators= 2000),\n    xgboost.XGBClassifier(),\n    SGDClassifier(max_iter=1000, tol=1e-4),\n    \n    SVC(kernel='linear',gamma='auto',probability=True),\n    AdaBoostClassifier(),\n    DecisionTreeClassifier(),\n    LinearDiscriminantAnalysis(),\n    GradientBoostingClassifier(n_estimators=100,max_features='sqrt'),\n    ExtraTreesClassifier(n_estimators= 1000),\n]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.605428Z","iopub.execute_input":"2021-07-04T13:29:37.605648Z","iopub.status.idle":"2021-07-04T13:29:37.614719Z","shell.execute_reply.started":"2021-07-04T13:29:37.605622Z","shell.execute_reply":"2021-07-04T13:29:37.613463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11. Stacked Ensemble (2 Level) <a id='stack-ensemble'></a>\n\nStacking, also called Super Learning or Stacked Regression, is a class of algorithms that involves training a second-level **“metalearner”** to find the optimal combination of the base learners. Unlike bagging and boosting, the goal in stacking is to ensemble strong, diverse sets of learners together.\nReference: [H2O](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html)\n\n### Stacking Algorithm\n\n**Step 1. Set up the ensemble.**<br>\n a. Specify a list of L base algorithms (with a specific set of model parameters)..<br>\n b. Specify a metalearning algorithm..<br>\n \n**Step 2. Train the ensemble.**<br>\n a. Train each of the L base algorithms on the training set..<br>\n b. Perform k-fold cross-validation on each of these learners and collect the cross-validated predicted values from each of the L algorithms..<br>\n c. The N cross-validated predicted values from each of the L algorithms can be combined to form a new N x L matrix. This matrix, along wtih the original response vector, is called the “level-one” data. (N = number of rows in the training set.).<br>\n d. Train the metalearning algorithm on the level-one data. The “ensemble model” consists of the L base learning models and the metalearning model, which can then be used to generate predictions on a test set.<br>\n \n**Step 3. Predict on new data.**<br>\n a. To generate ensemble predictions, first generate predictions from the base learners.<br>\n b. Feed those predictions into the metalearner to generate the ensemble prediction.<br>\n\n![](https://i.ibb.co/3cRfkxK/stacked-ensemble.png)","metadata":{}},{"cell_type":"markdown","source":"Now, as per performance of different baseline models on cross validation accuracy we will be selecting best performing models for level 0 of **stacked ensemble** so that their ensemble will produce higher performance in comparison to individual machine learning model.","metadata":{}},{"cell_type":"code","source":"S_train, S_test = stacking(models,                   \n                           X_train, y_train, X_test,   \n                           regression=False, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric=accuracy_score, \n    \n                           n_folds=5, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:29:37.615996Z","iopub.execute_input":"2021-07-04T13:29:37.616209Z","iopub.status.idle":"2021-07-04T13:30:11.827442Z","shell.execute_reply.started":"2021-07-04T13:29:37.616186Z","shell.execute_reply":"2021-07-04T13:30:11.825917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing generalizer model i.e., MLP classifier in our case\nmodel = MLPClassifier()\n    \nmodel = model.fit(S_train, y_train)\ny_pred = model.predict(S_test)\nprint('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:11.828977Z","iopub.execute_input":"2021-07-04T13:30:11.829441Z","iopub.status.idle":"2021-07-04T13:30:12.967977Z","shell.execute_reply.started":"2021-07-04T13:30:11.829406Z","shell.execute_reply":"2021-07-04T13:30:12.967309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12. Model Evaluation  <a id='model-eval'></a>\n\n In this step we will first define which evaluation metrics we will use to evaluate our model. The most important evaluation metric for this problem domain is **sensitivity, specificity, Precision, F1-measure, Geometric mean and mathew correlation coefficient and finally ROC AUC curve**\n \n### Sensitivity vs Specificity","metadata":{}},{"cell_type":"markdown","source":"![](https://i.ibb.co/d43FVfJ/Sensitivity-and-specificity-svg.png)\n\n### Mathew Correlation coefficient (MCC)\n\nThe Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.\n\n![](https://i.ibb.co/mH6MmG4/mcc.jpg)","metadata":{}},{"cell_type":"markdown","source":"### Log Loss\nLogarithmic loss  measures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of our machine learning models is to minimize this value. A perfect model would have a log loss of 0. Log loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high log loss.\n\nThe graph below shows the range of possible log loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n\n![](https://i.ibb.co/6BdDczW/log-loss.jpg)","metadata":{}},{"cell_type":"markdown","source":"### F1 Score\n\n F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n\n**F1 Score = 2*(Recall * Precision) / (Recall + Precision)**","metadata":{}},{"cell_type":"code","source":"CM=confusion_matrix(y_test,y_pred)\nsns.heatmap(CM, annot=True)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\nspecificity = TN/(TN+FP)\nloss_log = log_loss(y_test, y_pred)\nacc= accuracy_score(y_test, y_pred)\nroc=roc_auc_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmathew = matthews_corrcoef(y_test, y_pred)\nmodel_results =pd.DataFrame([['STacked Classifier',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:12.97118Z","iopub.execute_input":"2021-07-04T13:30:12.971547Z","iopub.status.idle":"2021-07-04T13:30:13.186134Z","shell.execute_reply.started":"2021-07-04T13:30:12.971502Z","shell.execute_reply":"2021-07-04T13:30:13.185308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparison with other Models","metadata":{}},{"cell_type":"code","source":"data = {'Random Forest': y_pred_rfe, \n                'MLP': y_pred_mlp, \n                'KNN': y_pred_knn, \n                'EXtra tree classifier': y_pred_et500,\n                'XGB': y_pred_xgb, \n                'SVC': y_pred_svc, \n                'SGD': y_pred_sgd,\n                'Adaboost': y_pred_ada, \n                'CART': y_pred_decc, \n                'GBM': y_pred_gbm }\n\nmodels = pd.DataFrame(data) \n \nfor column in models:\n    CM=confusion_matrix(y_test,models[column])\n    \n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    specificity = TN/(TN+FP)\n    loss_log = log_loss(y_test, models[column])\n    acc= accuracy_score(y_test, models[column])\n    roc=roc_auc_score(y_test, models[column])\n    prec = precision_score(y_test, models[column])\n    rec = recall_score(y_test, models[column])\n    f1 = f1_score(y_test, models[column])\n    \n    mathew = matthews_corrcoef(y_test, models[column])\n    results =pd.DataFrame([[column,acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n    model_results = model_results.append(results, ignore_index = True)\n\nmodel_results\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:13.187126Z","iopub.execute_input":"2021-07-04T13:30:13.187412Z","iopub.status.idle":"2021-07-04T13:30:13.319972Z","shell.execute_reply.started":"2021-07-04T13:30:13.187388Z","shell.execute_reply":"2021-07-04T13:30:13.31909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Findings\n- AS we can see from above results, Stacked Ensemble Classifier is best performer as it has highest test accuracy of 0.906, sensitivity of 0.943 and specificity of 0.866 and highest f1-score of 0.9133 and lowest Log Loss of 3.23 and highest ROC value of 0.904\n- Random Forest & Extra Tree classifier are second best having same performance measure in every aspect\n- XGboost has third best sensitivity level","metadata":{}},{"cell_type":"markdown","source":"### ROC AUC Curve","metadata":{}},{"cell_type":"code","source":"def roc_auc_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import roc_curve, roc_auc_score\n    fpr, tpr, _ = roc_curve(y_true, y_proba[:,1])\n    ax.plot(fpr, tpr, linestyle=l, linewidth=lw,\n            label=\"%s (area=%.3f)\"%(label,roc_auc_score(y_true, y_proba[:,1])))\n\nf, ax = plt.subplots(figsize=(12,8))\n\nroc_auc_plot(y_test,model.predict_proba(S_test),label='Stacked Classifier ',l='-')\nroc_auc_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nroc_auc_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nroc_auc_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\n\nax.plot([0,1], [0,1], color='k', linewidth=0.5, linestyle='--', \n        )    \nax.legend(loc=\"lower right\")    \nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Receiver Operator Characteristic curves')\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:13.320957Z","iopub.execute_input":"2021-07-04T13:30:13.321243Z","iopub.status.idle":"2021-07-04T13:30:13.644387Z","shell.execute_reply.started":"2021-07-04T13:30:13.321219Z","shell.execute_reply":"2021-07-04T13:30:13.642073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see highest average area under the curve (AUC) of 0.950 is attained by Extra Tree Classifier","metadata":{}},{"cell_type":"markdown","source":"## Precision Recall curve","metadata":{}},{"cell_type":"code","source":"def precision_recall_plot(y_true, y_proba, label=' ', l='-', lw=1.0):\n    from sklearn.metrics import precision_recall_curve, average_precision_score\n    precision, recall, _ = precision_recall_curve(y_test,\n                                                  y_proba[:,1])\n    average_precision = average_precision_score(y_test, y_proba[:,1],\n                                                     average=\"micro\")\n    ax.plot(recall, precision, label='%s (average=%.3f)'%(label,average_precision),\n            linestyle=l, linewidth=lw)\n\nf, ax = plt.subplots(figsize=(14,10))\nprecision_recall_plot(y_test,model.predict_proba(S_test),label='Stacking classifier ',l='-')\nprecision_recall_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')\nprecision_recall_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')\nprecision_recall_plot(y_test,xgb.predict_proba(X_test),label='XGboost',l='-')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.legend(loc=\"lower left\")\nax.grid(True)\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.set_title('Precision-recall curves')\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:13.646184Z","iopub.execute_input":"2021-07-04T13:30:13.646497Z","iopub.status.idle":"2021-07-04T13:30:14.113514Z","shell.execute_reply.started":"2021-07-04T13:30:13.64646Z","shell.execute_reply":"2021-07-04T13:30:14.112233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have seen Randomforest is second best performing model in every performance measure. So we will be plotting SHAP feature importance plot using random forest to know which features are actually contributing in model's prediction.","metadata":{}},{"cell_type":"code","source":"num_feats=11\n\ndef cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.114733Z","iopub.execute_input":"2021-07-04T13:30:14.114966Z","iopub.status.idle":"2021-07-04T13:30:14.134318Z","shell.execute_reply.started":"2021-07-04T13:30:14.114932Z","shell.execute_reply":"2021-07-04T13:30:14.132143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.135774Z","iopub.execute_input":"2021-07-04T13:30:14.135993Z","iopub.status.idle":"2021-07-04T13:30:14.176935Z","shell.execute_reply.started":"2021-07-04T13:30:14.135968Z","shell.execute_reply":"2021-07-04T13:30:14.175151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.178575Z","iopub.execute_input":"2021-07-04T13:30:14.178811Z","iopub.status.idle":"2021-07-04T13:30:14.229972Z","shell.execute_reply.started":"2021-07-04T13:30:14.178785Z","shell.execute_reply":"2021-07-04T13:30:14.22776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\", solver='lbfgs'), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)\n\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.231247Z","iopub.execute_input":"2021-07-04T13:30:14.231471Z","iopub.status.idle":"2021-07-04T13:30:14.267294Z","shell.execute_reply.started":"2021-07-04T13:30:14.231442Z","shell.execute_reply":"2021-07-04T13:30:14.266674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, criterion='entropy'), max_features=num_feats)\nembeded_rf_selector.fit(X, y)\n\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.26843Z","iopub.execute_input":"2021-07-04T13:30:14.268967Z","iopub.status.idle":"2021-07-04T13:30:14.588065Z","shell.execute_reply.started":"2021-07-04T13:30:14.268932Z","shell.execute_reply":"2021-07-04T13:30:14.587491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\nembeded_lgb_selector.fit(X, y)\n\nembeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.588877Z","iopub.execute_input":"2021-07-04T13:30:14.589115Z","iopub.status.idle":"2021-07-04T13:30:14.851932Z","shell.execute_reply.started":"2021-07-04T13:30:14.589092Z","shell.execute_reply":"2021-07-04T13:30:14.850462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# put all selection together\nfeature_name = X.columns\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(num_feats)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.85691Z","iopub.execute_input":"2021-07-04T13:30:14.857826Z","iopub.status.idle":"2021-07-04T13:30:14.882717Z","shell.execute_reply.started":"2021-07-04T13:30:14.857781Z","shell.execute_reply":"2021-07-04T13:30:14.881602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segregating dataset into features i.e., X and target variables i.e., y\nX = dt.drop(['target','resting_blood_pressure'],axis=1)\ny = dt['target']","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.884409Z","iopub.execute_input":"2021-07-04T13:30:14.884703Z","iopub.status.idle":"2021-07-04T13:30:14.894726Z","shell.execute_reply.started":"2021-07-04T13:30:14.884673Z","shell.execute_reply":"2021-07-04T13:30:14.894069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.896045Z","iopub.execute_input":"2021-07-04T13:30:14.89642Z","iopub.status.idle":"2021-07-04T13:30:14.923866Z","shell.execute_reply.started":"2021-07-04T13:30:14.896386Z","shell.execute_reply":"2021-07-04T13:30:14.922992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[['age','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.fit_transform(X_train[['age','cholesterol','max_heart_rate_achieved','st_depression']])\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.924922Z","iopub.execute_input":"2021-07-04T13:30:14.925248Z","iopub.status.idle":"2021-07-04T13:30:14.968771Z","shell.execute_reply.started":"2021-07-04T13:30:14.925204Z","shell.execute_reply":"2021-07-04T13:30:14.967385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[['age','cholesterol','max_heart_rate_achieved','st_depression']] = scaler.transform(X_test[['age','cholesterol','max_heart_rate_achieved','st_depression']])\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:30:14.970431Z","iopub.execute_input":"2021-07-04T13:30:14.970712Z","iopub.status.idle":"2021-07-04T13:30:15.017269Z","shell.execute_reply.started":"2021-07-04T13:30:14.970681Z","shell.execute_reply":"2021-07-04T13:30:15.015619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_ent = RandomForestClassifier(criterion='entropy',n_estimators=100)\nrf_ent.fit(X_train, y_train)\ny_pred_rfe = rf_ent.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:33:24.160661Z","iopub.execute_input":"2021-07-04T13:33:24.160928Z","iopub.status.idle":"2021-07-04T13:33:24.402105Z","shell.execute_reply.started":"2021-07-04T13:33:24.160903Z","shell.execute_reply":"2021-07-04T13:33:24.400846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp = MLPClassifier()\nmlp.fit(X_train,y_train)\ny_pred_mlp = mlp.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:33:27.794306Z","iopub.execute_input":"2021-07-04T13:33:27.794578Z","iopub.status.idle":"2021-07-04T13:33:28.878642Z","shell.execute_reply.started":"2021-07-04T13:33:27.79455Z","shell.execute_reply":"2021-07-04T13:33:28.877998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(9)\nknn.fit(X_train,y_train)\ny_pred_knn = knn.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:33:31.004969Z","iopub.execute_input":"2021-07-04T13:33:31.005338Z","iopub.status.idle":"2021-07-04T13:33:31.026067Z","shell.execute_reply.started":"2021-07-04T13:33:31.005303Z","shell.execute_reply":"2021-07-04T13:33:31.025224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"et_500 = ExtraTreesClassifier(n_estimators= 500)\net_500.fit(X_train,y_train)\ny_pred_et500 = et_500.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:33:34.508221Z","iopub.execute_input":"2021-07-04T13:33:34.508619Z","iopub.status.idle":"2021-07-04T13:33:35.603142Z","shell.execute_reply.started":"2021-07-04T13:33:34.508585Z","shell.execute_reply":"2021-07-04T13:33:35.601467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(kernel='linear',gamma='auto',probability=True)\nsvc.fit(X_train,y_train)\ny_pred_svc = svc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:37:30.276294Z","iopub.execute_input":"2021-07-04T13:37:30.277428Z","iopub.status.idle":"2021-07-04T13:37:30.35249Z","shell.execute_reply.started":"2021-07-04T13:37:30.277363Z","shell.execute_reply":"2021-07-04T13:37:30.350734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd = SGDClassifier(max_iter=1000, tol=1e-4)\nsgd.fit(X_train,y_train)\ny_pred_sgd = sgd.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:37:32.585343Z","iopub.execute_input":"2021-07-04T13:37:32.585805Z","iopub.status.idle":"2021-07-04T13:37:32.599055Z","shell.execute_reply.started":"2021-07-04T13:37:32.585774Z","shell.execute_reply":"2021-07-04T13:37:32.597933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada = AdaBoostClassifier()\nada.fit(X_train,y_train)\ny_pred_ada = ada.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:37:35.841096Z","iopub.execute_input":"2021-07-04T13:37:35.841409Z","iopub.status.idle":"2021-07-04T13:37:35.988772Z","shell.execute_reply.started":"2021-07-04T13:37:35.841375Z","shell.execute_reply":"2021-07-04T13:37:35.987718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decc = DecisionTreeClassifier()\ndecc.fit(X_train,y_train)\ny_pred_decc = decc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:37:38.355168Z","iopub.execute_input":"2021-07-04T13:37:38.355443Z","iopub.status.idle":"2021-07-04T13:37:38.367407Z","shell.execute_reply.started":"2021-07-04T13:37:38.355418Z","shell.execute_reply":"2021-07-04T13:37:38.366374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')\ngbm.fit(X_train,y_train)\ny_pred_gbm = gbm.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:37:41.583309Z","iopub.execute_input":"2021-07-04T13:37:41.583599Z","iopub.status.idle":"2021-07-04T13:37:41.680787Z","shell.execute_reply.started":"2021-07-04T13:37:41.583571Z","shell.execute_reply":"2021-07-04T13:37:41.679156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgboost\n# selecting list of top performing models to be used in stacked ensemble method\nmodels = [\n    RandomForestClassifier(criterion='entropy',n_estimators=100),\n    MLPClassifier(),\n    RandomForestClassifier(criterion='gini',n_estimators=100),\n    KNeighborsClassifier(9),\n    ExtraTreesClassifier(n_estimators= 500),\n    ExtraTreesClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 1000),\n    xgboost.XGBClassifier(n_estimators= 100),\n    xgboost.XGBClassifier(n_estimators= 500),\n    xgboost.XGBClassifier(n_estimators= 2000),\n    xgboost.XGBClassifier(),\n    SGDClassifier(max_iter=1000, tol=1e-4),\n    \n    SVC(kernel='linear',gamma='auto',probability=True),\n    AdaBoostClassifier(),\n    DecisionTreeClassifier(),\n    LinearDiscriminantAnalysis(),\n    GradientBoostingClassifier(n_estimators=100,max_features='sqrt'),\n    ExtraTreesClassifier(n_estimators= 1000),\n]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:38:20.78073Z","iopub.execute_input":"2021-07-04T13:38:20.780974Z","iopub.status.idle":"2021-07-04T13:38:20.789779Z","shell.execute_reply.started":"2021-07-04T13:38:20.780951Z","shell.execute_reply":"2021-07-04T13:38:20.788589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"S_train, S_test = stacking(models,                   \n                           X_train, y_train, X_test,   \n                           regression=False, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric=accuracy_score, \n    \n                           n_folds=5, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:38:28.367987Z","iopub.execute_input":"2021-07-04T13:38:28.368453Z","iopub.status.idle":"2021-07-04T13:39:00.484008Z","shell.execute_reply.started":"2021-07-04T13:38:28.368418Z","shell.execute_reply":"2021-07-04T13:39:00.482737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing generalizer model i.e., MLP classifier in our case\nmodel = MLPClassifier()\n    \nmodel = model.fit(S_train, y_train)\ny_pred = model.predict(S_test)\nprint('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:39:07.219355Z","iopub.execute_input":"2021-07-04T13:39:07.219623Z","iopub.status.idle":"2021-07-04T13:39:08.315855Z","shell.execute_reply.started":"2021-07-04T13:39:07.219597Z","shell.execute_reply":"2021-07-04T13:39:08.315126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CM=confusion_matrix(y_test,y_pred)\nsns.heatmap(CM, annot=True)\n\nTN = CM[0][0]\nFN = CM[1][0]\nTP = CM[1][1]\nFP = CM[0][1]\nspecificity = TN/(TN+FP)\nloss_log = log_loss(y_test, y_pred)\nacc= accuracy_score(y_test, y_pred)\nroc=roc_auc_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmathew = matthews_corrcoef(y_test, y_pred)\nmodel_results =pd.DataFrame([['STacked Classifier2',acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:39:14.452697Z","iopub.execute_input":"2021-07-04T13:39:14.452964Z","iopub.status.idle":"2021-07-04T13:39:14.680629Z","shell.execute_reply.started":"2021-07-04T13:39:14.452937Z","shell.execute_reply":"2021-07-04T13:39:14.679984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'Random Forest2': y_pred_rfe, \n                'MLP2': y_pred_mlp, \n                'KNN2': y_pred_knn, \n                'EXtra tree classifier2': y_pred_et500,\n                'XGB2': y_pred_xgb, \n                'SVC2': y_pred_svc, \n                'SGD2': y_pred_sgd,\n                'Adaboost2': y_pred_ada, \n                'CART2': y_pred_decc, \n                'GBM2': y_pred_gbm }\n\nmodels = pd.DataFrame(data) \n \nfor column in models:\n    CM=confusion_matrix(y_test,models[column])\n    \n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    specificity = TN/(TN+FP)\n    loss_log = log_loss(y_test, models[column])\n    acc= accuracy_score(y_test, models[column])\n    roc=roc_auc_score(y_test, models[column])\n    prec = precision_score(y_test, models[column])\n    rec = recall_score(y_test, models[column])\n    f1 = f1_score(y_test, models[column])\n    \n    mathew = matthews_corrcoef(y_test, models[column])\n    results =pd.DataFrame([[column,acc, prec,rec,specificity, f1,roc, loss_log,mathew]],\n               columns = ['Model', 'Accuracy','Precision', 'Sensitivity','Specificity', 'F1 Score','ROC','Log_Loss','mathew_corrcoef'])\n    model_results = model_results.append(results, ignore_index = True)\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:39:19.775858Z","iopub.execute_input":"2021-07-04T13:39:19.776244Z","iopub.status.idle":"2021-07-04T13:39:19.90863Z","shell.execute_reply.started":"2021-07-04T13:39:19.776216Z","shell.execute_reply":"2021-07-04T13:39:19.907766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13. Model Interpretation  <a id='model-inter'></a>","metadata":{}},{"cell_type":"markdown","source":"### SHAP Feature Importance\nThe global mean(|Tree SHAP|) method applied to the heart disease prediction model. The x-axis is essentially the average magnitude change in model output when a feature is “hidden” from the model (for this model the output has log-odds units). “hidden” means integrating the variable out of the model. Since the impact of hiding a feature changes depending on what other features are also hidden, Shapley values are used to enforce consistency and accuracy.","metadata":{}},{"cell_type":"code","source":"import shap \nexplainer = shap.TreeExplainer(rf_ent)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:40:41.220494Z","iopub.execute_input":"2021-07-04T13:40:41.221008Z","iopub.status.idle":"2021-07-04T13:40:43.332404Z","shell.execute_reply.started":"2021-07-04T13:40:41.220973Z","shell.execute_reply":"2021-07-04T13:40:43.330876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SHAP Summary Plot","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values[1], X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:41:04.974065Z","iopub.execute_input":"2021-07-04T13:41:04.974336Z","iopub.status.idle":"2021-07-04T13:41:05.425965Z","shell.execute_reply.started":"2021-07-04T13:41:04.974307Z","shell.execute_reply":"2021-07-04T13:41:05.424415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The higher the SHAP value of a feature, the higher is the log odds of heart disease in this heart disease prediction model. Every patient in the dataset is run through the model and a dot is created for each feature attribution value, so one patient gets one dot on each feature’s line. Dot’s are colored by the feature’s value for that patient and pile up vertically to show density. In above plot we see that **st_slope_upsloping** is the most important risk factor for heart disease patients. The lower values of **st_slope_upsloping** leads to heart disease, whereas its higher values decreases the chances of heart disease. Higher values of **exercise_induced_angina** increases the risk of heart disease whereas its lower values decreases the chances of heart disease.","metadata":{}},{"cell_type":"markdown","source":"Next we will see how st_slope_upsloping, st_slope_flat and excercise_induced_angina which are top three contributing features affects model prediction by visualizing them using [**Partial Dependence Plots**](https://www.kaggle.com/dansbecker/partial-plots) ","metadata":{}},{"cell_type":"code","source":"features = [c for c in X_test.columns]\n\nfrom pdpbox import pdp, get_dataset, info_plots\n\npdp_thal = pdp.pdp_isolate(model=rf_ent, dataset=X_test, model_features=features, feature='st_slope_upsloping')\n\n# plot it\npdp.pdp_plot(pdp_thal, 'st_slope_upsloping')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:41:13.100076Z","iopub.execute_input":"2021-07-04T13:41:13.100494Z","iopub.status.idle":"2021-07-04T13:41:13.354037Z","shell.execute_reply.started":"2021-07-04T13:41:13.100463Z","shell.execute_reply":"2021-07-04T13:41:13.352223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As we can see from above plot it is evident that higher values of st_slope_upsloping decreases the chances of heart disease which we also find through summary plot.","metadata":{}},{"cell_type":"code","source":"pdp_thal = pdp.pdp_isolate(model=rf_ent, dataset=X_test, model_features=features, feature='st_slope_flat')\n\n# plot it\npdp.pdp_plot(pdp_thal, 'st_slope_flat') \n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:41:19.798371Z","iopub.execute_input":"2021-07-04T13:41:19.798634Z","iopub.status.idle":"2021-07-04T13:41:20.25161Z","shell.execute_reply.started":"2021-07-04T13:41:19.798609Z","shell.execute_reply":"2021-07-04T13:41:20.25068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdp_thal = pdp.pdp_isolate(model=rf_ent, dataset=X_test, model_features=features, feature='exercise_induced_angina')\n\n# plot it\npdp.pdp_plot(pdp_thal, 'exercise_induced_angina')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T13:41:27.468219Z","iopub.execute_input":"2021-07-04T13:41:27.468493Z","iopub.status.idle":"2021-07-04T13:41:27.666906Z","shell.execute_reply.started":"2021-07-04T13:41:27.468465Z","shell.execute_reply":"2021-07-04T13:41:27.665569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- After seeing the above two plots, it is clear now higher values of st_slope_flat and excercise_induced_angina leads to heart disease which is also evident through SHAP summary plots.","metadata":{}},{"cell_type":"markdown","source":"## 14. Conclusion  <a id='data-conc'></a>\n\n- As we have seen, stacked ensemble of power machine learning algorithms resulted in higher performance than any individual machine learning model.\n- We have also interpreted second best performing algo i.e., random forest algorithm\n- The top 5 most contribution features are **st_slope_upsloping, st_slope_flat, excercise_induced_angina, sex** and **cholesterol **\n- Further we have also deep dive into top 3 features to know how their values affecting the final model prediction and find that ***higher value of st_slope_upsloping reduces the risk of heart disease while lower values increases the chances of heart disease where as in case of st_slope_flat and excercise_induced_angina their higher values are riskier for heart disease while lower values are safe***","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}