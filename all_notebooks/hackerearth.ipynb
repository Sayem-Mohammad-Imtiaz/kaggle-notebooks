{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import nltk \nfrom nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk import pos_tag\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nltk.corpus.wordnet.fileids()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lemma = WordNetLemmatizer()\n# pstem = PorterStemmer()\n# lemma.lemmatize('goodness'), pstem.stem('goodness')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for word in nltk.corpus.wordnet.words():\n#     print(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('../input/train.csv')\ndf_test=pd.read_csv('../input/test.csv')\ndf_subm=pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(len(df_train.columns)):\n#     if i in [0,2,3,6,7,8,9]:\n#         pass\n#     else:\n#         print(df_train.iloc[:,i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6']\nfor c in col:\n    df_train[c].fillna(df_train[c].dropna().median(), inplace=True)\n    df_test[c].fillna(df_train[c].dropna().median(), inplace=True)\n\ndf_train['advice_to_mgmt'].fillna('', inplace=True)\ndf_test['advice_to_mgmt'].fillna('', inplace=True)\n\ndf_train.dropna(subset=['negatives','summary'], inplace=True)\ndf_test.dropna(subset=['negatives','summary'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_col = ['ID', 'location', 'date']\ndf_train.drop(columns=drop_col, inplace=True)\ndf_test.drop(columns=drop_col, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Place'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OEncoder = OrdinalEncoder()\nEncoded = OEncoder.fit_transform(df_train[['Place', 'status']])\nEncoded_test = OEncoder.transform(df_test[['Place', 'status']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoded.shape, df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_ENC(df, Enc):\n#   Create empty arrays with random elements with dimensions of the encoded column\n    Place_enc = np.empty((len(Enc),))  \n    Status_enc = np.empty((len(Enc),))\n    for i in range(len(Enc)):\n        Place_enc[i] = Enc[i][0]\n        Status_enc[i] = Enc[i][1]\n    df['place_enc'] = Place_enc\n    df['status_enc'] = Status_enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Create_ENC(df_train, Encoded)\nCreate_ENC(df_test, Encoded_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('overall').Place.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('Place').overall.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.groupby('job_title').overall.count()\n# no information from this","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Review_len(df):\n    df['len_pos'] = df['positives'].str.len()\n    df['len_neg'] = df['negatives'].str.len()\n    df['len_sum'] = df['summary'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Review_len(df_train)\nReview_len(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ChangeToInt(df,col):\n    df[col]=df[col].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label='overall'\nChangeToInt(df_train,label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_wordcloud(data, title = None):\n    V_wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 200,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 7\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize = (20, 20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(V_wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print positive wordcloud\nshow_wordcloud(df_train[\"positives\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print negatives wordcloud\nshow_wordcloud(df_train[\"negatives\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print summary wordcloud\nshow_wordcloud(df_train[\"summary\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def Reviews(df):\n#     df['Reviews']=df['positives']+' '+df['negatives']+' '+df['summary']\n# #     +' '+df['advice_to_mgmt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reviews(df_train)\n# Reviews(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.Reviews[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    # lower text\n    text = text.lower()\n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    # remove stop words\n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    # remove empty and less than 3 length tokens\n    text = [t for t in text if len(t) >= 3]\n    # pos tag text\n    pos_tags = pos_tag(text)\n    # lemmatize text\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    # remove words with less than 3 letters\n    text = [t for t in text if len(t) >= 3]\n    # join all\n    text = \" \".join(text)\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # clean text data\n# df_train[\"Clean_reviews\"] = df_train[\"Reviews\"].apply(lambda x: clean_text(x))\n# df_test[\"Clean_reviews\"] = df_test[\"Reviews\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.drop(columns='Reviews', inplace=True)\n# df_test.drop(columns='Reviews', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Clean_positives\"] = df_train[\"positives\"].apply(lambda x: clean_text(x))\ndf_train[\"Clean_negatives\"] = df_train[\"negatives\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[\"Clean_positives\"] = df_test[\"positives\"].apply(lambda x: clean_text(x))\ndf_test[\"Clean_negatives\"] = df_test[\"negatives\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Clean_summary\"] = df_test[\"summary\"].apply(lambda x: clean_text(x))\ndf_test[\"Clean_summary\"] = df_test[\"summary\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Clean_reviews\"] = df_train[\"Clean_positives\"]+' '+df_train[\"Clean_negatives\"]+' '+df_train[\"Clean_summary\"]\ndf_test[\"Clean_reviews\"] = df_test[\"Clean_positives\"]+' '+df_test[\"Clean_negatives\"]+' '+df_test[\"Clean_summary\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6']\n# for col in scores:\n#     print(df_train[col].value_counts())\n# score_6 column doesn't have uniform values. Ignore this column in analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.isnull().sum()  \n# 1115\n# df_train[df_train['Clean_reviews'].isnull() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train.drop(columns=['num_words_pos', 'num_words_neg'], inplace=True)\n# df_test.drop(columns=['num_words_pos', 'num_words_neg'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dropna(inplace=True)\ndf_test.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_train.loc[0,'Clean_positives'].split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_words(df):\n    df['num_words_pos'] = df['positives'].apply(lambda x: len(x.split()))\n    df['num_words_neg'] = df['negatives'].apply(lambda x: len(x.split()))\n    df['num_words_sum'] = df['summary'].apply(lambda x: len(x.split()))\n#     df['num_words_neg'] = len(df['negatives'].str.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words(df_train)\nnum_words(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n# CVec = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\nCVec = CountVectorizer(stop_words='english', min_df=3)\nfeat_col=['place_enc', 'status_enc', 'len_pos', 'len_neg', 'num_words_pos', 'num_words_neg', 'num_words_sum', \n          'len_sum', 'score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'Clean_reviews']\nX=df_train[feat_col]\nY=df_train['overall']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect=CVec.fit_transform(X_train['Clean_reviews'])\nX_test_vect=CVec.transform(X_test['Clean_reviews'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect.shape, X_test_vect.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(CVec.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_train = pd.DataFrame(X_train_vect.todense(), columns=CVec.get_feature_names())\nreviews_test = pd.DataFrame(X_test_vect.todense(), columns=CVec.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_train.shape, reviews_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(columns='Clean_reviews', inplace=True)\nX_test.drop(columns='Clean_reviews', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape , X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_withvect = pd.concat([X_train, reviews_train], axis=1)\n# X_test_withvect = pd.concat([X_test, reviews_test], axis=1)\n\n# creating a new df is causing RAM to be exhausted.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.reset_index(drop=True, inplace=True)\n# X_train.drop(columns='index', inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n# X_test.drop(columns='index', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_ = pd.concat([X_train, reviews_train], axis=1)\n# X_test_ = pd.concat([X_test, reviews_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_.shape, X_test_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text('aa'), clean_text('abused'), clean_text('abusive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_.dropna(inplace=True)\n# X_test_.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_cols=['place_enc','status_enc']\ntrain_dummies=pd.get_dummies(data=X_train, columns=dummy_cols)\ntest_dummies=pd.get_dummies(data=X_test, columns=dummy_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_dummy_cols = [ col for col in X_train.columns.tolist() if col not in dummy_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies.drop(columns=t_dummy_cols, inplace=True)\ntest_dummies.drop(columns=t_dummy_cols, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies.shape, test_dummies.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat = pd.concat([train_dummies, reviews_train], axis=1)\ntest_cat = pd.concat([test_dummies, reviews_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat.shape, test_cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(columns=['place_enc', 'status_enc'], inplace=True)\nX_test.drop(columns=['place_enc', 'status_enc'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# col_to_scale = ['len_pos', 'len_neg', 'num_words_pos', 'num_words_neg', 'num_words_sum', 'len_sum']\ncol_to_scale = X_train.columns.tolist()\nscaled_tr_values = scaler.fit_transform(X_train[col_to_scale])\nscaled_ts_values = scaler.transform(X_test[col_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[:,col_to_scale] = scaled_tr_values\nX_test.loc[:,col_to_scale] = scaled_ts_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_XGB = XGBClassifier(random_state=5, n_jobs=-1)\nmodel_XGB.fit(train_cat, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# pred_XGB = model_XGB.predict(test_cat)\n# print('XGB_Train_Accuracy: ', model_XGB.score(train_cat, y_train))\n# print('XGB_Test_Accuracy: ', accuracy_score(y_test, pred_XGB))\n# print('XGB_F1_score: ', f1_score(y_test, pred_XGB, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_XGB = model_XGB.predict_proba(test_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_XGB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pb_col=['pb1', 'pb2', 'pb3', 'pb4', 'pb5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_sc = pd.DataFrame(data=pred_test_XGB, columns=pb_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_sc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from random import sample\ndata_tr = pd.concat([prob_sc, X_test], axis = 1)\n# index_trn = sample(list(data_tr.index),round(len(data_tr)*0.8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xf_train, Xf_test, yf_train, yf_test = train_test_split(data_tr, y_test, random_state=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xf_train.shape, yf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logi1 = LogisticRegression('l2',1,.01,.05,1,solver='liblinear',max_iter=500)\nlogi1.fit(Xf_train,yf_train)\nprediction_logi1 = logi1.predict(Xf_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train_Accuracy: ', logi1.score(Xf_train,yf_train))\nprint('Test_Accuracy: ', accuracy_score(yf_test, prediction_logi1))\nprint('F1_score: ', f1_score(yf_test, prediction_logi1, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_NB = MultinomialNB()\n# model_NB.fit(train_cat, y_train)\n\n# prediction_NB = model_NB.predict_proba(test_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # print('AUC: ', roc_auc_score(y_test, prediction))\n# print('NB_Train_Accuracy: ', model_NB.score(X_train_, y_train))\n# print('NB_Test_Accuracy: ', accuracy_score(y_test, prediction_NB))\n# print('NB_F1_score: ', f1_score(y_test, prediction_NB, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_prob_NB = model_NB.predict_proba(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_prob_NB[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MultinomialNB Model: Initial version commits(Not recent)\n> Positives + Negatives:\nAccuracy:  0.36494518557654204\nF1_score:  0.3254225708516727\n> Positives + Negatives + Summary:\nAccuracy:  0.36402060493990224\nF1_score:  0.32921876951906376\n> Positives + Negatives + Summary + advice_to_mgmt:\nAccuracy:  0.36309602430326243\nF1_score:  0.3289207289982767"},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_test[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction_NB[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_DTC = DecisionTreeClassifier(random_state=7)\n# model_DTC.fit(X_train_, y_train)\n\n# prediction_DTC = model_DTC.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('DTC_Train_Accuracy: ', model_DTC.score(X_train_, y_train))\n# print('DTC_Test_Accuracy: ', accuracy_score(y_test, prediction_DTC))\n# print('DTC_F1_score: ', f1_score(y_test, prediction_DTC, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # num = [5 , 7 , 9, 11]\n# # num = [5]\n# # for k in num:\n# k=5\n# model_KNN = KNeighborsClassifier(n_neighbors=k)\n# model_KNN.fit(X_train_, y_train)\n\n# prediction_KNN = model_KNN.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Num_Neighbors: ', k)\n# print('KNN_Train_Accuracy: ', model_KNN.score(X_train_, y_train))\n# print('KNN_Test_Accuracy: ', accuracy_score(y_test, prediction_KNN))\n# print('KNN_F1_score: ', f1_score(y_test, prediction_KNN, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> K = 5 is it. with the least worst F1_score of 0.3013  (during initial testing of the data) using positives, negatives, summary, advice_to_management"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_RDF = RandomForestClassifier(n_estimators=300, random_state=5, n_jobs=-1)\n# model_RDF.fit(X_train_, y_train)\n\n# prediction_RDF = model_RDF.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # print('AUC: ', roc_auc_score(y_test, prediction))\n# print('RDF_Accuracy: ', accuracy_score(y_test, prediction_RDF))\n# print('RDF_F1_score: ', f1_score(y_test, prediction_RDF, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_SGD = SGDClassifier(random_state=5, n_jobs=-1)\n# model_SGD.fit(X_train_, y_train)\n\n# prediction_SGD = model_SGD.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # print('AUC: ', roc_auc_score(y_test, prediction))\n# print('SGD_Train_Accuracy: ', model_SGD.score(X_train_, y_train))\n# print('SGD_Test_Accuracy: ', accuracy_score(y_test, prediction_SGD))\n# print('SGD_F1_score: ', f1_score(y_test, prediction_SGD, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_LGR = LogisticRegression(random_state=5, n_jobs=-1)\n# model_LGR.fit(X_train_, y_train)\n\n# prediction_LGR = model_LGR.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('LGR_Train_Accuracy: ', model_LGR.score(X_train_, y_train))\n# print('LGR_Test_Accuracy: ', accuracy_score(y_test, prediction_LGR))\n# print('LGR_F1_score: ', f1_score(y_test, prediction_LGR, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_ET = ExtraTreeClassifier(random_state=5)\n# model_ET.fit(X_train_, y_train)\n\n# prediction_ET = model_ET.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('ET_Train_Accuracy: ', model_ET.score(X_train_, y_train))\n# print('ET_Test_Accuracy: ', accuracy_score(y_test, prediction_ET))\n# print('ET_F1_score: ', f1_score(y_test, prediction_ET, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Reference: https://towardsdatascience.com/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}