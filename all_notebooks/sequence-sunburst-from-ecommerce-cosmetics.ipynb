{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction\n\nThis notebook allows you to create the data (in the expected format) to elaborate [d3 sunburst visualizations](https://bl.ocks.org/kerryrodden/7090426) like this one:\n\n![Sunburst Visualization](https://analista-digital.com/wp-content/uploads/2021/02/showcase_sunburst.png)\n\nFor this particular case, I only picked sequences including a purchase, because I was interested to see which flows lead to purchases more often."},{"metadata":{},"cell_type":"markdown","source":"### Import needed packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create initial dataFrame from October's data"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Load October Data (keep only needed Columns)\ndf = pd.read_csv('/kaggle/input/ecommerce-events-history-in-cosmetics-shop/2019-Oct.csv',\n                 usecols=[\"event_time\", \"event_type\", \"user_session\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Get a random sample of events\ndf.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Start Data Cleansing & Manipulation"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Make sure columns do not have NAN elements\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# drop rows with NAN user_session\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check validity of user_session. It seems to be a uuid, an therefore every session id should have the same number of characters\ndf['tmp_session_len'] = df['user_session'].apply(len)\ndf['tmp_session_len'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check validity of event_type. It should be factorial with 4 different events\ndf['event_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace remove_from_cart with remove (str too long for good visualization)\ndf['event_type'].replace(['remove_from_cart'], ['remove'], inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Convert event_time in real datetime type\ndf['event_time'] = pd.to_datetime(df['event_time'],infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check event_time. Are all days represented?\ndf[\"event_time\"].groupby(df[\"event_time\"].dt.day).count().plot(kind=\"line\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check hour of the day... are all hours represented?\ndf[\"event_time\"].groupby(df[\"event_time\"].dt.hour).count().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Factorize session id to save some RAM\ndf['user_session'] = pd.factorize(df.user_session)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a new feature, sequences, which will be the base of the final output\n\nIf you are familiar with `SQL`, I am going to do a `collect_list` of all event_types, `partitioned` by the session id and `ordered by` the event_time. This sequences will include all events done in the user flow in chronological order. Imagine, a user's flow in a particular session is composed of (in chronological order, from older to newer):\n\n* A product view\n* Another product view\n* A cart addition\n* Another product view\n* Another cart addition\n* A cart removal\n* A purchase\n\nThe final sequence for this user session will be this array:\n\n`['view', 'view', 'cart', 'view', 'cart', 'remove', 'purchase']`"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Create a collect_list of events, partitioned by user_session & ordered by date_time ascending\ngrouped_df = df.sort_values(['event_time'],ascending=True).groupby('user_session')['event_type'].apply(list).to_frame(name='sequences')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per specification, all sequences should have an \"end\" marker as the last element, unless it has been truncated because it is longer than the maximum sequence length (6, in the example). The purpose of the \"end\" marker is to distinguish a true end point (e.g. the user left the site) from an end point that has been forced by truncation."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Add 'end' to the end of the array for each sequence\n# This is done to finalize sequences with less than 6 diferent touch points\ngrouped_df['sequences'] = grouped_df.apply(lambda x: x['sequences'] + ['end'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As specified previously, the longest sequence length I want to analyze is 6 (I do not want to go deeper with the analysis). You can change it easily within the next code"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Constrain sequences to a maximum of 6 touchpoints\n# Sequences with less than 6 touchpoints will finalize with 'end'\ngrouped_df['sequences'] = grouped_df.apply(lambda x: x['sequences'][:6], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep only sequences with at least one purchase\ngrouped_df = grouped_df[grouped_df.sequences.apply(lambda x: np.any(np.in1d(x, ['purchase'])))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate the final output file"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Transform array into a str (- separated). This is the format expected by d3\ngrouped_df['sequences'] = ['-'.join(map(str, l)) for l in grouped_df['sequences']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Create a df with the top N sequences by count of appearence\ntop_sequences = grouped_df['sequences'].value_counts().nlargest(100).to_frame(name='occurences')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Export resulting dataframe into a csv file\ntop_sequences.to_csv('top_sequences.csv', header=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display final output\ntop_sequences","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}