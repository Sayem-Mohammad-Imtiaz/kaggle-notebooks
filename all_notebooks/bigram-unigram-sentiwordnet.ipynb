{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nThis code uses the bigram model for SentiWordNet\nIt is based off of the unigram model approach\nreference : http://nlpforhackers.io/sentiment-analysis-intro/\n\"\"\"\n\nimport pandas as pd\n\ndata = pd.read_csv(\"../input/imdb-movie-reviews-dataset/movie_data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_data = list(zip(data[\"review\"], data[\"sentiment\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 80% for training\ntrain_X, train_y = zip(*sentiment_data[:40000])\n\n# Keep 20% for testing\ntest_X, test_y = zip(*sentiment_data[40000:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\nfrom nltk.util import ngrams\n\nlemmatizer = WordNetLemmatizer()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def penn_to_wn(tag):\n    \"\"\"\n    Convert between the PennTreebank tags to simple Wordnet tags\n    \"\"\"\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.replace(\"<br />\", \" \")\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def swn_polarity(text):\n    \"\"\"\n    Return a sentiment polarity: 0 = negative, 1 = positive\n    \"\"\"\n\n    sentiment = 0.0\n    tokens_count = 0\n\n    text = clean_text(text)\n\n    raw_sentences = sent_tokenize(text)\n    for raw_sentence in raw_sentences:\n\n        unigramSent = 0.0\n        bigramSent = 0.0\n\n\n        ##############################\n        #       Bigram Model\n        ##############################\n\n\n        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n        tokens, tags = zip(*tagged_sentence)\n        bigramTokens = []\n        bigramTags = []\n        tokenBigrams = ngrams(tokens, 2)\n        tagBigrams = ngrams(tags, 2)\n\n        for t1, t2 in tokenBigrams:\n            bigramTokens.append((t1, t2))\n\n        for t1, t2 in tagBigrams:\n            bigramTags.append((t1, t2))\n\n        for i in range(0, len(bigramTokens)):\n            wn_tag1 = penn_to_wn(bigramTags[i][0])\n            if wn_tag1 not in (wn.NOUN, wn.ADJ, wn.ADV):\n                continue\n\n            wn_tag2 = penn_to_wn(bigramTags[i][1])\n            if wn_tag2 not in (wn.NOUN, wn.ADJ, wn.ADV):\n                continue\n\n            lemma1 = lemmatizer.lemmatize(bigramTokens[i][0], pos=wn_tag1)\n            if not lemma1:\n                continue\n\n            lemma2 = lemmatizer.lemmatize(bigramTokens[i][1], pos=wn_tag2)\n            if not lemma2:\n                continue\n\n            synsets1 = wn.synsets(lemma1, pos=wn_tag1)\n            if not synsets1:\n                continue\n\n            synsets2 = wn.synsets(lemma2, pos=wn_tag2)\n            if not synsets2:\n                continue\n\n            # Take the first sense, the most common\n            synset1 = synsets1[0]\n            synset2 = synsets2[0]\n            swn_synset1 = swn.senti_synset(synset1.name())\n            swn_synset2 = swn.senti_synset(synset2.name())\n\n            tmp1sent = swn_synset1.pos_score() - swn_synset1.neg_score()\n            tmp2sent = swn_synset2.pos_score() - swn_synset2.neg_score()\n\n            if tmp1sent < 0 and tmp1sent < 0:\n                bigramSent += -1 * min(tmp1sent, tmp2sent)\n            elif tmp1sent > 0 and tmp2sent > 0:\n                bigramSent += max(tmp1sent, tmp2sent)\n            else:\n                bigramSent += min(tmp1sent, tmp2sent)\n\n        ###########################\n        #      Unigram Model\n        ###########################\n\n        for word, tag in tagged_sentence:\n            wn_tag = penn_to_wn(tag)\n            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n                continue\n\n            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n            if not lemma:\n                continue\n\n            synsets = wn.synsets(lemma, pos=wn_tag)\n            if not synsets:\n                continue\n\n            # Take the first sense, the most common\n            synset = synsets[0]\n            swn_synset = swn.senti_synset(synset.name())\n\n            unigramSent += swn_synset.pos_score() - swn_synset.neg_score()\n            tokens_count += 1\n\n        if max(abs(bigramSent), abs(unigramSent)) == abs(bigramSent):\n            sentiment += bigramSent\n        else:\n            sentiment += unigramSent\n\n\n    # judgment call ? Default to positive or negative\n    if not tokens_count:\n        return 0\n\n    # sum greater than 0 => positive sentiment\n    if sentiment >= 0:\n        return 1\n\n    # negative sentiment\n    return 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since we're shuffling, you'll get diffrent results\nprint(swn_polarity(train_X[0]), train_y[0])  # 0 1\nprint(swn_polarity(train_X[1]), train_y[1])  # 1 0\nprint(swn_polarity(train_X[2]), train_y[2])  # 0 0\nprint(swn_polarity(train_X[3]), train_y[3])  # 1 1\nprint(swn_polarity(train_X[4]), train_y[4])  # 0 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\npred_y = [swn_polarity(text) for text in test_X]\n\nprint(accuracy_score(test_y, pred_y))  # \nfrom sklearn.metrics import accuracy_score\n\npred_y = [swn_polarity(text) for text in test_X]\n\nprint(accuracy_score(test_y, pred_y))  # 0.6094","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}