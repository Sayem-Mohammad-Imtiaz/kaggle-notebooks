{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Understanding:\ndataset: (row 1_time k = row 2_time k-1 = row 3_time k-2)\n#### TASK: build 2 models (The inputs are without n_k, try and add it to see its contribution?)：\n#### 1. Black-box inverter model: (model of ideal inverter: u_x_k-1= d_x_k-2 * u_dc_k-2)\n#### inputs: d_a/b/c_k-3, d_a/b/c_k-2, i_a/b/c_k-1, i_a/b/c_k, u_dc_k-1,u_dc_k,\n#### targets: u_a/b/c_k-1\n#### 2. Black-box inverter compensation scheme:\n#### inputs: u_a/b/c_k-1, d_a/b/c_k-3, i_a/b/c_k-3, i_a/b/c_k-2, u_dc_k-3, u_dc_k-2,\n#### targets: d_a/b/c_k-2"},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing & Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/inverter-data-set/Inverter Data Set.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#列出所有欄位及其資料類型：全部都是float\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#查看缺失值：\ndef missing_values_table(df):\n    mis_val = df.isnull().sum()\n    #print(mis_val)\n    mis_val_percent = 100*mis_val/len(df) \n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns=mis_val_table.rename(\n    columns = {0: 'Missing Values', 1: '% of total values'})\n    mis_val_table_ren_columns =mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values('% of total values',ascending=False).round(1)\n    print('your selected has' +str(df.shape[1])+'columns.\\n' 'there are' + str(mis_val_table_ren_columns.shape[0])+ ' columns that have missing values')\n    \n    return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_misstb = missing_values_table(df)\n#-->沒有缺失值","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(17,17))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['n_k'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA -Correlation Coefficients:  between columns + between features and targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#顯示所有欄位和target（u_a/b/c_k-1）的相關\n#from kirgson's notebook\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.figure(figsize=(14,14))\n_ = sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#各栏位和目标(target)的相关系数浏览\ntargets = ['u_a_k-1','u_b_k-1','u_c_k-1']\nfor i in targets:\n    df_corr = df.corr()[i].sort_values()\n\n    # print the strongest correlation coefficients(positive & negative)\n    print('strongest positive correlations with target: \\n', df_corr.tail(8))\n    print('strongest negative correlations with target: \\n', df_corr.head(8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization\n#### Features in Phase a (head 5000 samples)"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## rows有按照時間順序排列，phase a的d, i, u隨著時間k的變化图。\n# 2500 samples/switch\np_sample= 5000\n\ncol_iak1=df['i_a_k-1'].head(p_sample)\np_iak1 = plt.subplot(4,1,1)\ncol_iak1.plot(use_index = True, figsize = (20,10), title='phase a')\nplt.setp(p_iak1.get_xticklabels(), visible=False)\np_iak1.set_ylabel('Phase Currents in A')\n\ncol_uak1 = df['u_a_k-1'].head(p_sample)\np_uak1 = plt.subplot(4,1,2)\ncol_uak1.plot(use_index = True)\nplt.setp(p_uak1.get_xticklabels(), visible=False)\np_uak1.set_ylabel('Mean Phase Voltages in V')\n\ncol_udck1 = df['u_dc_k-1'].head(p_sample)\np_udck1 = plt.subplot(4,1,3)\ncol_udck1.plot(use_index = True)\nplt.setp(p_udck1.get_xticklabels(), visible=False)\np_udck1.set_ylabel('DC-link voltage in V')\n\ncol_nk = df['n_k'].head(p_sample)\np_nk = plt.subplot(4,1,4)\ncol_nk.plot(use_index = True)\np_nk.set_ylabel('Speed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#三個相位之間的currents之間的關係圖：\n# p_iabc = df[['i_a_k','i_b_k','i_c_k']].head(5000)\n# pd.plotting.scatter_matrix(p_iabc, alpha=0.2)\n#三個相位之間的voltages之間的關係圖：\n# p_uabc = df[['u_a_k-1','u_b_k-1','u_c_k-1']].head(5000)\n# pd.plotting.scatter_matrix(p_uabc, alpha=0.2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# u, udc, n, d, i 兩兩之間的關係圖\np_5 = df[['u_a_k-1','u_dc_k-1','i_a_k-1','n_k','d_a_k-2']].head(5000)\npd.plotting.scatter_matrix(p_5, alpha=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\ndomain knowledge..."},{"metadata":{},"cell_type":"markdown","source":"## Model 1: Inverter Model \n#### Defined: Supervised - Regression problem\n#### 0. Ideal model(Baseline) 1. Neural Network; 2. Random Forest"},{"metadata":{},"cell_type":"markdown","source":"### Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cutting\ndfx1 = df[['i_a_k','i_a_k-1','i_b_k','i_b_k-1','i_c_k','i_c_k-1','u_dc_k','u_dc_k-1','d_a_k-2','d_a_k-3','d_b_k-2','d_b_k-3','d_c_k-2','d_c_k-3']]\ndfy1 = df[['u_a_k-1','u_b_k-1','u_c_k-1','u_dc_k-2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dfx1.shape\ndfy1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train1, x_test1, y_train1, y_test1 = train_test_split(dfx1, dfy1, test_size = 0.2)\nx_train1, x_val1, y_train1, y_val1 = train_test_split(x_train1, y_train1, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train1.shape, x_val1.shape, x_test1.shape, y_train1.shape, y_val1.shape, y_test1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 targets y\ny_tr1a = y_train1['u_a_k-1']\ny_tr1b = y_train1['u_b_k-1']\ny_tr1c = y_train1['u_c_k-1']\n\ny_va1a = y_val1['u_a_k-1']\ny_va1b = y_val1['u_b_k-1']\ny_va1c = y_val1['u_c_k-1']\n\ny_te1a = y_test1['u_a_k-1']\ny_te1b = y_test1['u_b_k-1']\ny_te1c = y_test1['u_c_k-1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization\nscaler = preprocessing.StandardScaler()\n\nscaler.fit(x_train1)\nx_train1_std = scaler.transform(x_train1)\nx_val1_std = scaler.transform(x_val1)\nx_test1_std = scaler.transform(x_test1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# 檢查確認標準化後的均值\nprint('mean of standardized test dataset', round(x_test1_std[:,5].mean()))\nprint('std of standardized test dataset', round(x_train1_std[:,8].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train1.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 0. The Ideal Inverter Model : u_x_k-1 = d_x_k-2 * u_dc_k-2 (baseline)\n(phase a)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fix\ndf00 = pd.DataFrame(x_test1, columns = x_train1.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df00['udc'] = y_test1['u_dc_k-2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df00","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df00['pred00'] = df00['d_a_k-2']*df00['udc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df00['true'] = pd.DataFrame(y_te1a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nmse1a = mean_squared_error(df00['true'] , df00['pred00'])\nmse1a","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Neural Network\n#### To Be Continued: try different units (10/20/30/40/50) and more hidden layers(2/3/4)"},{"metadata":{},"cell_type":"markdown","source":"#### Build a Neural Network: 3 Layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\n# nn1: 1 hidden layer\nnn1 = models.Sequential()\nnn1.add(layers.Dense(units=30, activation = 'relu', input_shape =(14,)))\nnn1.add(layers.Dense(units=30, activation = 'relu'))\nnn1.add(layers.Dense(units=1))\nnn1.compile(loss='mse', optimizer='Adam', metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nn2: k=5, u=3 (u=50: bad)\nnn2 = models.Sequential()\nnn2.add(layers.Dense(units=30, activation = 'relu', input_shape =(14,)))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=30, activation = 'relu'))\nnn2.add(layers.Dense(units=1))\nnn2.compile(loss='mse', optimizer='Adam', metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (1) 14features --> target1: u_a_k-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnfit1 = nn1.fit(x_train1_std, y_tr1a, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_val1_std, y_va1a))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training history Visualization\ntra_loss = nnfit1.history['loss']\nte_loss = nnfit1.history['val_loss']\n\nepoch_count = range(1, len(tra_loss)+1)\nplt.plot(epoch_count, tra_loss,'r--')\nplt.plot(epoch_count, te_loss, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nn2:\nnn2fit1 = nn2.fit(x_train1_std, y_tr1a, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_val1_std, y_va1a))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training history Visualization\ntra_loss = nnfit2.history['loss']\nte_loss = nnfit2.history['val_loss']\n\nepoch_count = range(1, len(tra_loss)+1)\nplt.plot(epoch_count, tra_loss,'r--')\nplt.plot(epoch_count, te_loss, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (2) 14features --> target2: u_b_k-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnfit2 = nn2.fit(x_train1_std, y_tr1b, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_test1_std, y_te1b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tra_loss2 = nnfit2.history['loss']\nte_loss2 = nnfit2.history['val_loss']\nepoch_count = range(1, len(tra_loss2)+1)\nplt.plot(epoch_count, tra_loss2,'r--')\nplt.plot(epoch_count, te_loss2, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (3) 14features --> target3: u_c_k-1"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"nnfit3 = nn2.fit(x_train1_std, y_tr1c, \n                    epochs=30, batch_size=500,\n                   validation_data=(x_test1_std, y_te1c))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"tra_loss3 = nnfit3.history['loss']\nte_loss3 = nnfit3.history['val_loss']\nepoch_count = range(1, len(tra_loss3)+1)\nplt.plot(epoch_count, tra_loss3,'r--')\nplt.plot(epoch_count, te_loss3, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. RandomForest\n#### 14 features --> target: u_a_k-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# time: 4m50s\nrfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(x_train1_std, y_tr1a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfrpred = rfr.predict(x_test1_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nmserfr = mean_squared_error(y_te1a, rfrpred)\nmserfr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_te1a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfrpred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importance = rfr.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_colname = list(x_train1.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_impor_df = pd.DataFrame({'feature': x_colname, 'importance': feat_importance})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_impor_df.sort_values('importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2: Inverter Compensation Scheme\n#### Defined: Supervised - Regression problem"},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing: Dataset cutting + Standadization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cutting dataset\ndfx2 = df[['u_a_k-1','u_b_k-1','u_c_k-1','d_a_k-3','d_b_k-3','d_c_k-3','i_a_k-3','i_b_k-3','i_c_k-3','i_a_k-2','i_b_k-2','i_c_k-2','u_dc_k-3','u_dc_k-2']]\ndfy2 = df[['d_a_k-2','d_b_k-2','d_c_k-2']]\nx_train2, x_test2, y_train2, y_test2 = train_test_split(dfx2, dfy2, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train2, x_test2, y_train2, y_test2 = train_test_split(dfx2, dfy2, test_size = 0.2)\n# x_train2, x_val2, y_train2, y_val2 = train_test_split(x_train2, y_train2, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train2.shape, x_test2.shape, y_train2.shape, y_test2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 targets y\ny_tr2a = y_train2['d_a_k-2']\ny_tr2b = y_train2['d_b_k-2']\ny_tr2c = y_train2['d_c_k-2']\n\n# y_va1a = y_val1['u_a_k-1']\n# y_va1b = y_val1['u_b_k-1']\n# y_va1c = y_val1['u_c_k-1']\n\ny_te2a = y_test2['d_a_k-2']\ny_te2b = y_test2['d_b_k-2']\ny_te2c = y_test2['d_c_k-2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization\nscaler2 = preprocessing.StandardScaler()\n\nscaler2.fit(x_train2)\nx_train2_std = scaler2.transform(x_train2)\n# x_val2_std = scaler2.transform(x_val2)\nx_test2_std = scaler2.transform(x_test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 檢查確認標準化後的均值\nprint('mean of standardized test dataset', round(x_test2_std[:,5].mean()))\nprint('std of standardized test dataset', round(x_train2_std[:,8].std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Neural Network\n#### To Be Continued: try different units (10/20/30/40/50) & k (1/2/3)"},{"metadata":{},"cell_type":"markdown","source":"#### 1. 14 features --> target 1: d_a_k-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnfit2a = nn2.fit(x_train2_std, y_tr2a,\n                  epochs=30, batch_size=300,\n                   validation_data=(x_test2_std, y_te2a))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"tra_loss2a = nnfit2a.history['loss']\nte_loss2a = nnfit2a.history['val_loss']\nepoch_count = range(1, len(tra_loss2a)+1)\nplt.plot(epoch_count, tra_loss2a,'r--')\nplt.plot(epoch_count, te_loss2a, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. 14 features --> target 2: d_b_k-2 "},{"metadata":{"trusted":true},"cell_type":"code","source":"nnfit2b = nn2.fit(x_train2_std, y_tr2b,\n                  epochs=30, batch_size=300,\n                   validation_data=(x_test2_std, y_te2b))\n# loss\ntra_loss2b = nnfit2b.history['loss']\nte_loss2b = nnfit2b.history['val_loss']\nepoch_count = range(1, len(tra_loss2b)+1)\nplt.plot(epoch_count, tra_loss2b,'r--')\nplt.plot(epoch_count, te_loss2b, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. 14 features --> target 3: d_c_k-2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nnfit2c = nn2.fit(x_train2_std, y_tr2c,\n                  epochs=30, batch_size=300,\n                   validation_data=(x_test2_std, y_te2c))\ntra_loss2c = nnfit2c.history['loss']\nte_loss2c = nnfit2c.history['val_loss']\nepoch_count = range(1, len(tra_loss2c)+1)\nplt.plot(epoch_count, tra_loss2c,'r--')\nplt.plot(epoch_count, te_loss2c, 'b-')\n\nplt.legend(['training loss','test loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}