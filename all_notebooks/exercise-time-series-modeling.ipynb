{"cells":[{"metadata":{"trusted":true,"_uuid":"3144c4e3acf6570cccdee57ec7205ad6140814bd"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pandas import Series\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4b3c73366a1fdfde6860bb3cd93ffe65ee6dc25"},"cell_type":"code","source":"train = pd.read_csv(\"../input/Train_SU63ISt.csv\")\ntest = pd.read_csv(\"../input/Test_0qrQsBZ.csv\")\ntrain_original = train.copy()\ntest_original = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f90fbfaea3ef1777354018241e084e2a2a957c3"},"cell_type":"markdown","source":"# Extracting Features"},{"metadata":{"trusted":true,"_uuid":"4b04f41f3fd4669ea32f2457a104b09c982cf39a"},"cell_type":"code","source":"train['Datetime'] = pd.to_datetime(train.Datetime, format = '%d-%m-%Y %H:%M')\ntest['Datetime'] = pd.to_datetime(test.Datetime, format = '%d-%m-%Y %H:%M')\ntrain_original['Datetime'] = pd.to_datetime(train_original.Datetime, format = '%d-%m-%Y %H:%M')\ntest_original['Datetime'] = pd.to_datetime(test_original.Datetime, format = '%d-%m-%Y %H:%M')\n\nfor i in (train, test, train_original, test_original):\n    i['year'] = i.Datetime.dt.year\n    i['month'] = i.Datetime.dt.month\n    i['day']= i.Datetime.dt.day\n    i['Hour']=i.Datetime.dt.hour\n    \ntrain['Day of week'] = train['Datetime'].dt.dayofweek\ntemp = train['Datetime']\n\ndef applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0\n    \ntemp2 = train['Datetime'].apply(applyer)\ntrain['weekend'] = temp2\n\ntrain.index = train['Datetime']\ndf = train.drop('ID',1)\nts = df['Count']\nplt.figure(figsize = (16,8))\nplt.plot(ts)\nplt.title(\"Time Series\")\nplt.xlabel(\"Time (year-month)\")\nplt.ylabel(\"Passenger Count\")\nplt.legend(loc = 'best')\n\n\ntest.Timestamp = pd.to_datetime(test.Datetime, format='%d-%m-%Y %H:%M')\ntest.index = test.Timestamp\n\n#Converting to Daily mean \ntest = test.resample('D').mean()\n\ntrain.Timestamp = pd.to_datetime(train.Datetime, format='%d-%m-%Y %H:%M')\ntrain.index = train.Timestamp\n\n#Converting to Daily mean\ntrain = train.resample('D').mean()\n\nTrain = train.ix['2012-08-25':'2014-06-24']\nvalid = train.ix['2014-06-25':'2014-09-25']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b83aaa27665d43bf56947c74e02fa618622f4ee5"},"cell_type":"markdown","source":"# Divide data into training and validation"},{"metadata":{"trusted":true,"_uuid":"8cf94d181df8bc29de7b03d1ee0e12d02f91b384"},"cell_type":"code","source":"Train.Count.plot(figsize = (15,8), title = 'Daily Ridership', fontsize = 14, label = 'Train')\nvalid.Count.plot(figsize = (15,8), title = 'Daily Ridership', fontsize =14, label = 'Valid')\nplt.xlabel('Datetime')\nplt.ylabel('Passenger Count')\nplt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96fd3baf36e9c893d4eca4657f12958f4511cbdf"},"cell_type":"markdown","source":"# Predicting Timeseries: Naive Approach\n\nIn this approach you're simply using the last data point to make a prediction.  Try modifying `y_hat['naive']` and see what happens. You could also try making it more complex or maybe even more Naive."},{"metadata":{"trusted":true,"_uuid":"284e39826fe7da226aadbc4ff974d04bb9533318"},"cell_type":"code","source":"dd = np.asarray(Train.Count)\ny_hat =valid.copy()\ny_hat['naive']= dd[len(dd)- 1]\nplt.figure(figsize = (12,8))\nplt.plot(Train.index, Train['Count'],label = 'Train')\nplt.plot(valid.index, valid['Count'], label = 'Validation')\nplt.plot(y_hat.index, y_hat['naive'],  label = 'Naive')\nplt.legend(loc = 'best')\nplt.title('Naive Forecast')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dd07c502432ca73e0d18d09a497d266a3eb434c"},"cell_type":"markdown","source":"### Calculate the RMSE for a Naive Approach"},{"metadata":{"trusted":true,"_uuid":"0a4a0fb6e10e58e4402c466a3d253d44c7da93e4"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrmse = sqrt(mean_squared_error(valid.Count, y_hat.naive))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a13ba71186bf20d8d619fc028ee0f25213b1034"},"cell_type":"markdown","source":"In this section, try increasgin or decreasing the size of the rolling average widow to see how it affects your Naive predictions."},{"metadata":{"trusted":true,"_uuid":"5a4084d95ec0e5229b43fa0e926ca42dd8b070a5"},"cell_type":"code","source":"y_hat_avg = valid.copy()\ny_hat_avg['moving_average_forecast'] = Train['Count'].rolling(10).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(Train['Count'], label = 'Train')\nplt.plot(valid['Count'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = 'Moving Average Forecast with 10 Observations')\nplt.legend(loc = 'best')\nplt.show()\n\ny_hat_avg = valid.copy()\ny_hat_avg['moving_average_forecast'] = Train['Count'].rolling(20).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(Train['Count'], label = 'Train')\nplt.plot(valid['Count'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'],label = 'Moving Average Forecast with 20 Observations')\nplt.legend(loc = 'best')\nplt.show()\n\ny_hat_avg = valid.copy()\ny_hat_avg['moving_average_forecast']= Train['Count'].rolling(50).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(Train['Count'], label = 'Train')\nplt.plot(valid['Count'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = \"Moving Average Forecast with 50 Observations\")\nplt.legend(loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a3d08c495e53b2c760a6818c2a8f68c38374063"},"cell_type":"markdown","source":"### Calculate RMSE for a moving average"},{"metadata":{"trusted":true,"_uuid":"826687f45fe3613552454b8c6fcca0f4f1c800ac"},"cell_type":"code","source":"rmse = sqrt(mean_squared_error(valid['Count'], y_hat_avg['moving_average_forecast']))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55c436a2ff8f8bdc03fcd7a5c9b8c526bb1a2e49"},"cell_type":"markdown","source":"# Predicting Timeseries: Simple Exponential Smoothing\n\nSimple Exponential Smoothing is another fun method. Try out different for `smoothing_level` and `optimized`"},{"metadata":{"trusted":true,"_uuid":"5802001e255f13be3c84bfac5b289cc4de7f9782"},"cell_type":"code","source":"from statsmodels.tsa.api import ExponentialSmoothing,SimpleExpSmoothing, Holt\n\ny_hat = valid.copy()\nfit2 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level = 0.6,optimized = False)\ny_hat['SES'] = fit2.forecast(len(valid))\nplt.figure(figsize =(15,8))\nplt.plot(Train['Count'], label = 'Train')\nplt.plot(valid['Count'], label = 'Validation')\nplt.plot(y_hat['SES'], label = 'Simple Exponential Smoothing')\nplt.legend(loc = 'best')\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"45214d58cead3a838a629086a2cf4f72d7c330df"},"cell_type":"markdown","source":"### Calculate RMSE for Simple Exponential Smoothing"},{"metadata":{"trusted":true,"_uuid":"ff18cf53c3ba294219f7ef855b35ac0eac20aa5e"},"cell_type":"code","source":"rmse = sqrt(mean_squared_error(valid.Count, y_hat['SES']))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3630634f01b68dcb7bf71f951815f4708057ceae"},"cell_type":"markdown","source":"# Predicting Timeseris: Holt's Linear Trend Model\n\nFor this method, modify the `smoothing_level` and `smoothing_slope` parameters to see how it affects your predictions."},{"metadata":{"trusted":true,"_uuid":"25b15d3f0471132dfd5d7f1e87f8f47c8f52706c"},"cell_type":"code","source":"plt.style.use('default')\nplt.figure(figsize = (16,8))\nimport statsmodels.api as sm\nsm.tsa.seasonal_decompose(Train.Count).plot()\nresult = sm.tsa.stattools.adfuller(train.Count)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"511fec7e253db745d083be532e0418435fec0e9d"},"cell_type":"code","source":"y_hat_holt = valid.copy()\nfit1 = Holt(np.asarray(Train['Count'])).fit(smoothing_level = 0.3, smoothing_slope = 0.1)\ny_hat_holt['Holt_linear'] = fit1.forecast(len(valid))\nplt.style.use('fivethirtyeight')\nplt.figure(figsize = (15,8))\nplt.plot(Train.Count, label = 'Train')\nplt.plot(valid.Count, label = 'Validation')\nplt.plot(y_hat_holt['Holt_linear'], label = 'Holt Linear')\nplt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2cfd3bd7a5e2968c2ef1b69bb33e4e0e4d0b2c1"},"cell_type":"code","source":"rmse = sqrt(mean_squared_error(valid.Count, y_hat_holt.Holt_linear))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8aa473bc92b6e8d1d4d4d446b4077bfd5a91fccb"},"cell_type":"code","source":"predict = fit1.forecast(len(test))\ntest['prediction'] = predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61596ea4e2e6cae2ce5ddaac3970abc38404a868"},"cell_type":"code","source":"#Calculating hourly ration of count\ntrain_original['ratio'] = train_original['Count']/train_original['Count'].sum()\n\n#Grouping hourly ratio\ntemp = train_original.groupby(['Hour']) ['ratio'].sum()\n\n#Group by to csv format\npd.DataFrame(temp, columns= ['Hour', 'ratio']).to_csv('Groupby.csv')\ntemp2 = pd.read_csv(\"Groupby.csv\")\ntemp2 =temp2.drop('Hour.1',1)\n#Merge test and test_original on day, month and year\nmerge = pd.merge(test, test_original, on = ('day', 'month','year'), how = 'left')\nmerge['Hour'] = merge['Hour_y']\nmerge = merge.drop(['year','month','day','Hour_x','Datetime','Hour_y'], axis =1)\n\n#Predicting by merging temp2 and merge\nprediction = pd.merge(merge, temp2, on = 'Hour',how = 'left')\n\n#Converting the ration to original scale\nprediction['Count'] = prediction['prediction'] * prediction['ratio'] * 24\nprediction['ID'] = prediction['ID_y']\nprediction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89b0a6ba6252d6ed932bbb74e91d3706b572f0c3"},"cell_type":"code","source":"submission = prediction.drop(['ID_x','ID_y','prediction','Hour','ratio'], axis =1)\npd.DataFrame(submission, columns = ['ID','Count']).to_csv('Holt_Linear.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c194c5e1990d9b25e15ae94ab24008885985f6a9"},"cell_type":"markdown","source":"# Predicting Timeseries: Holt Winter's Model\n\nIn this method, you can change the `ExponentialSmoothing` parameters. How many season_periods gives the best predictions?"},{"metadata":{"trusted":true,"_uuid":"fb06270c7863e22af1cc011875f65312a0054ca5"},"cell_type":"code","source":"y_hat_avg = valid.copy()\nfit1 = ExponentialSmoothing(np.asarray(Train['Count']), seasonal_periods= 7, trend = 'add', seasonal= 'add').fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(valid))\nplt.figure(figsize = (16,8))\nplt.plot(Train['Count'], label = 'Train')\nplt.plot(valid['Count'], label = 'Validation')\nplt.plot(y_hat_avg.Holt_Winter, label = 'Holt Winters')\nplt.legend(loc = 'best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2da66bdd9426862bd3be720be50c55cbac5c127e"},"cell_type":"code","source":"rmse = sqrt(mean_squared_error(valid['Count'], y_hat_avg['Holt_Winter']))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85a664f8ee45b2a5ee8338885adf81665df8a25e"},"cell_type":"code","source":"predict = fit1.forecast(len(test))\ntest['prediction'] = predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dfe9d683e6cfa5551b35f537c24e02dcb5dbf0a"},"cell_type":"code","source":"#Merge test and test_original on day,month and year\nmerge = pd.merge(test, test_original, on = ('day', 'month', 'year'), how = 'left')\nmerge['Hour']= merge['Hour_y']\nmerge.head()\nmerge = merge.drop(['year', 'month', 'Datetime','Hour_x', 'Hour_y'], axis =1)\n\n#Predicting by merge and temp2\nprediction = pd.merge(merge, temp2 , on = 'Hour', how = 'left')\n\n#Converting the ration to original scale\nprediction['Count'] = prediction['prediction'] * prediction['ratio'] *24\nprediction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff3484ebae9aae6698e94c8655c3bb468837f12e"},"cell_type":"code","source":"prediction['ID']= prediction['ID_y']\nsubmission = prediction.drop(['ID_x','ID_y','day','Hour','prediction','ratio'], axis =1)\n\npd.DataFrame(submission, columns = ['ID','Count']).to_csv('Holt winters.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7aa6285a78164bc3640d4ce35d61cee89faa09fb"},"cell_type":"markdown","source":"# Predicting Time Series: ARIMA Model\n\nExperiment with this ARIMA model by adjusting the rolling average and standard deviation windows. How do they affect the results of the Dickey-Fuller test? "},{"metadata":{"trusted":true,"_uuid":"3f16dbe5920b5c6acf59e8658357d4504241cb8b"},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationary(timeseries):\n    #Determine rolling statistics\n    rolmean = timeseries.rolling(24).mean()\n    rolstd = timeseries.rolling(24).std()\n    \n    #Plot rolling Statistics\n    orig = plt.plot(timeseries, color = \"blue\", label = \"Original\")\n    mean = plt.plot(rolmean, color = \"red\", label = \"Rolling Mean\")\n    std = plt.plot(rolstd, color = \"black\", label = \"Rolling Std\")\n    plt.legend(loc = \"best\")\n    plt.title(\"Rolling Mean and Standard Deviation\")\n    plt.show(block = False)\n    \n    #Perform Dickey Fuller test\n    print(\"Results of Dickey Fuller test: \")\n    dftest = adfuller(timeseries, autolag = 'AIC')\n    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistics', 'p-value', '# Lag Used', 'Number of Observations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)' %key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fca38363901ee82aa2d2705ab23eb4ef4dc73e6"},"cell_type":"code","source":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize']=(20,10)\ntest_stationary(train_original['Count'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6adc15ab0baffe09faac7f2eab965a33862a1d0"},"cell_type":"markdown","source":"### Remove trends"},{"metadata":{"trusted":true,"_uuid":"c8b1e0e36f11a1d6e41f7c04bc735b10e7606f46"},"cell_type":"code","source":"Train_log = np.log(Train['Count'])\nvalid_log = np.log(valid['Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b385377a96eee9622ca2f2e9d62d135f8f454061"},"cell_type":"code","source":"moving_avg = Train_log.rolling(24).mean()\nplt.plot(Train_log)\nplt.plot(moving_avg, color = 'red')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c20f04bcb3820d7a7c900f870bef98eea05fa6f"},"cell_type":"code","source":"train_log_moving_diff = Train_log - moving_avg\ntrain_log_moving_diff.dropna(inplace = True)\ntest_stationary(train_log_moving_diff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4546b34fa6fd8c30bb7bfb285b5f217ce15a95ef"},"cell_type":"markdown","source":"### Differencing can help to make the series stable and eliminate trends"},{"metadata":{"trusted":true,"_uuid":"7691022b05bafb82ee7a602feff7c1a7602842ad"},"cell_type":"code","source":"train_log_diff = Train_log - Train_log.shift(1)\ntest_stationary(train_log_diff.dropna())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af98f5d4eb829b09e07e53573a411cdd7ec42997"},"cell_type":"markdown","source":"### Removing Seasonailty"},{"metadata":{"trusted":true,"_uuid":"072d280550d248010be412c5a536188e43482201"},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nplt.figure(figsize = (16,10))\ndecomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values, freq = 24)\nplt.style.use('default')\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(Train_log, label = 'Original')\nplt.legend(loc = 'best')\nplt.subplot(412)\nplt.plot(trend, label = 'Trend')\nplt.legend(loc = 'best')\nplt.subplot(413)\nplt.plot(seasonal, label = 'Seasonal')\nplt.legend(loc = 'best')\nplt.subplot(414)\nplt.plot(residual, label = 'Residuals')\nplt.legend(loc = 'best')\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0c8c8b03d48e528f2e10b28a755dc3846275f148"},"cell_type":"markdown","source":"### Check stationarity of residuals"},{"metadata":{"trusted":true,"_uuid":"367f440768793a2f2c082bd0654758a983cf8f7e"},"cell_type":"code","source":"plt.figure(figsize = (16,8))\ntrain_log_decompose = pd.DataFrame(residual)\ntrain_log_decompose['date'] = Train_log.index\ntrain_log_decompose.set_index('date', inplace = True)\ntrain_log_decompose.dropna(inplace = True)\ntest_stationary(train_log_decompose[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87e2d816a34f8e19ccaad6d2170b031ffed1308f"},"cell_type":"code","source":"from statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(train_log_diff.dropna(), nlags = 25)\nlag_pacf = pacf(train_log_diff.dropna(), nlags = 25, method= \"ols\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d74c1ddd58bd69656225ba4be989a85f36dbf3d"},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nplt.style.use(\"fivethirtyeight\")\nplt.plot(lag_acf)\nplt.axhline( y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline( y= -1.96/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline(y = 1.96 /np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Autocorrelation Function\")\nplt.show()\n# PACF\nplt.figure(figsize = (15,8))\nplt.plot(lag_pacf)\nplt.axhline(y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline(y = -1.96/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline( y = 1.96/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Partial Autocorrelation Function\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b6f8aa10b137871365390c6aaef23fd23066adc"},"cell_type":"markdown","source":"# Predicting Timeseries: AR Model\n\nIf you're unfamiliar with the ARIMA model, it might be helpful to see what parameters and arguments you can use to improve your predictions. Read up on `p,d,q`.<br> \n[https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html)"},{"metadata":{"trusted":true,"_uuid":"136116404e359ae60f0190b11d7a4032f5af6070"},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nplt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (2,1,0))  #here q value is zero since it is just AR Model\nresults_AR = model.fit(disp=-1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_AR.fittedvalues, color = 'red', label = 'Predictions')\nplt.legend(loc = 'best')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b674ac36018574661a2801e976340c350f0511c2"},"cell_type":"code","source":"AR_predict = results_AR.predict(start=\"2014-06-25\", end=\"2014-09-25\")\nAR_predict = AR_predict.cumsum().shift().fillna(0)\nAR_predict1 = pd.Series(np.ones(valid.shape[0])* np.log(valid['Count'])[0], index = valid.index)\nAR_predict1=AR_predict1.add(AR_predict,fill_value=0)\nAR_predict = np.exp(AR_predict1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cf9fe9ba34ef3d76d515c1bce6794b84ef3dc74"},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nplt.plot(valid['Count'], label = \"Validation\")\nplt.plot(AR_predict, color = \"red\", label = \"Predict\")\nplt.legend(loc = \"best\")\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(AR_predict, valid['Count']))/valid.shape[0]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20326483786cc61f08b62ad98fc90e9dd208118e"},"cell_type":"markdown","source":"# Predicting Timeseries: Moving Average Model\n\nFor this model, you can modify the same parameters (p,d,q) because the base model relies on ARIMA."},{"metadata":{"trusted":true,"_uuid":"96e5ed7e83199c7f8ddc528b1a45b459b4f75785"},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (0,1,2)) # here the p value is 0 since it is moving average model\nresults_MA = model.fit(disp = -1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_MA.fittedvalues, color = \"red\", label = \"Prediction\")\nplt.legend(loc = \"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c1ea42d899794463764392cb763e613b6529a9"},"cell_type":"code","source":"MA_predict = results_MA.predict(start=\"2014-06-25\", end=\"2014-09-25\")\nMA_predict=MA_predict.cumsum().shift().fillna(0)\nMA_predict1=pd.Series(np.ones(valid.shape[0]) * np.log(valid['Count'])[0], index = valid.index)\nMA_predict1=MA_predict1.add(MA_predict,fill_value=0)\nMA_predict = np.exp(MA_predict1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b878152fffa01bff5914636a8aa66c47ab737313"},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nplt.plot(valid['Count'], label = \"Valid\")\nplt.plot(MA_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, valid['Count']))/valid.shape[0]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"790469da0d7bb3fd1c3828eda1e63b75ff4e03aa"},"cell_type":"markdown","source":"### Combining Models"},{"metadata":{"trusted":true,"_uuid":"a9ba2862b1027e02852e9687f819bb93e9ec0cf0"},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nmodel = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='Original')\nplt.plot(results_ARIMA.fittedvalues, color='red', label='Predicted')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9252435f4d4860e79bdef6f838c0050b02462240"},"cell_type":"markdown","source":"### Scale model to original scale"},{"metadata":{"trusted":true,"_uuid":"f428106f716c917bfcdb19e395cabf6c55d20aec"},"cell_type":"code","source":"\ndef check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Count'])[0], index = given_set.index)\n    predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))/given_set.shape[0]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"368e8110a2b29a6d0cd62a0b91382f563b751acf"},"cell_type":"code","source":"def check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['Count'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Count']))/given_set.shape[0]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7968788888244c49b7b910fe4a875e91ef381b9"},"cell_type":"code","source":"ARIMA_predict_diff=results_ARIMA.predict(start=\"2014-06-25\", end=\"2014-09-25\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91a7a3fa847c7a3b239e9f73e0dfbb8f599d0700"},"cell_type":"code","source":"plt.figure(figsize = (16,8))\ncheck_prediction_diff(ARIMA_predict_diff, valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd74c9861565749513e454de2d977bd883eedace"},"cell_type":"markdown","source":"# Predicting Timeseries: SARIMAX Model\n\nBy this point, you've probably got a pretty good idea about which parameters you can modify to make improvements to your predictions. Try out what you've learned using the SARIMAX model. "},{"metadata":{"trusted":true,"_uuid":"f56a45c3f85eebfcf4483ae0af9a4f5a2d4eeddb"},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bd1508f578f7bf63daaefd3db4dbd823f2c6422"},"cell_type":"code","source":"y_hat_avg = valid.copy()\nfit1 = sm.tsa.statespace.SARIMAX(Train.Count, order = (2,1,4), seasonal_order =(0,1,1,7)).fit()\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-6-25\", end=\"2014-9-25\", dynamic=True)\nplt.figure(figsize=(16,8))\nplt.plot(Train['Count'], label = \"Train\")\nplt.plot(valid.Count, label = \"Validation\")\nplt.plot(y_hat_avg['SARIMA'], label =\"SARIMA\")\nplt.legend(loc = \"best\")\nplt.title(\"SARIMAX Model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ec2369438ecf5eec22a64b0b8e2bbcefb5d0f09"},"cell_type":"code","source":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SARIMA))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50e3d0ad2675331d44df151cec2eb6ebd9d06285"},"cell_type":"markdown","source":"### Covert to Hourly Predictions"},{"metadata":{"trusted":true,"_uuid":"854f50e8aa413157080f322357c65c585b1a4bb0"},"cell_type":"code","source":"predict = fit1.predict(start=\"2014-9-26\", end=\"2015-4-26\", dynamic=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"646970748036902d0be7479e094505b8ab1ee7f0"},"cell_type":"code","source":"test['prediction']=predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c34a8802998888583942b5303edb63b453aa972e"},"cell_type":"code","source":"#Merge test and test_original on day,month and year\nmerge = pd.merge(test,test_original, on = ('day', 'month', 'year'), how = 'left')\nmerge['Hour'] = merge['Hour_y']\n\n#Predicting by merging merge and temp2\nprediction = pd.merge(merge, temp2, on = 'Hour', how = 'left')\n\n#Converting the ratio to original scale\nprediction['Count'] = prediction['prediction'] * prediction['ratio'] * 24","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16c3cfd3f649383ed43fbbffb667ba299da9f8dc"},"cell_type":"code","source":"prediction['ID']=prediction['ID_y']\nsubmission=prediction.drop(['day','Hour','ratio','prediction', 'ID_x', 'ID_y'],axis=1)\n\n# Converting the final submission to csv format\npd.DataFrame(submission, columns=['ID','Count']).to_csv('SARIMAX.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}