{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Default Prediction Using Classification Models"},{"metadata":{},"cell_type":"markdown","source":"**Author: Rahul Kulkarni**\n"},{"metadata":{},"cell_type":"markdown","source":"**Scope and Problem Statement:** I have built this kernel to explain various classification models and metrics which can be used to evaluate models. I will go through various aspects of machine learning such as data wrangling, exploratory data analysis, model selection, parameter tuning and resampling. We will aim to predict the probablity of deafaulting in the next month using various data related to the customers.\n\nThis kind of task is very useful in risk management. Since banks make most of their revenues thorugh credit card bills of their customers, it's very important to have a good prediction otherwise defaulting causes huge losses to the banks."},{"metadata":{},"cell_type":"markdown","source":"**Description of the dataset:**\n\nThere are 25 variables:\n\n1. ID: ID of each client\n1. LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n1. SEX: Gender (1=male, 2=female)\n1. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n1. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n1. AGE: Age in years\n1. PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n1. PAY_2: Repayment status in August, 2005 (scale same as above)\n1. PAY_3: Repayment status in July, 2005 (scale same as above)\n1. PAY_4: Repayment status in June, 2005 (scale same as above)\n1. PAY_5: Repayment status in May, 2005 (scale same as above)\n1. PAY_6: Repayment status in April, 2005 (scale same as above)\n1. BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n1. BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n1. BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n1. BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n1. BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n1. BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n1. PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n1. PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n1. PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n1. PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n1. PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n1. PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n1. default.payment.next.month: Default payment (1=yes, 0=no)"},{"metadata":{},"cell_type":"markdown","source":"**Flow of process:**\nWith this kernel, I will try to explain the basic steps which I took for various tasks mentioned below. Since this is clearly a classification problem, I will fit various models on the data and try to find the best one.\n\n* Data Wrangling:\nHandling missing values and collecting information about various variables.\n* Exploratory Data Analysis:\nFinding a correlation between various variables through plots.\n* Modelling:\nFitting various models on the train data. Also, we will check the parameters which give us the optimum results for particular models. For example: Varying 'K' in KNearest Neighbors model, Maximum depth in Decision Tree Classifier etc. We will also evaluate the models by predicting the result using test data. Predominantly, we will make use of confusion matrix and f1 scores.\n* Resampling:\nDuring EDA we will get to know if the dataset is imbalanced or not, depending on that we will visit this technique.\n* Re-Modelling:\nWe will again fit the models on the resampled data and check if any better results are obtained."},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling"},{"metadata":{},"cell_type":"markdown","source":"Data wrangling refers to the process of obtaining, cleaning, restructuring and enriching the raw data available into a more usable format. This will help us quicken the process of decision making, and thus get better insights in less time. Certain objectives of this step will be handle missing values, check datatypes of our variables etc."},{"metadata":{},"cell_type":"markdown","source":"**Importing Libraries**\nThese will be used to read data, plotting and transformation of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nloan = pd.read_csv('../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\nloan.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the amount of missing data we have."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"loan.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we have got ourselves a perfect dataset(I hope), but we have completed one important task from our checklist.\nNow let's check the datatypes of the variables to verify that we don't have any other surprises at hand."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the data types seem to be correct."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will attempt to build relations between various attributes of the customer, make sense out of the data and answer questions like How these variables affect the probability of a customer defaulting in the next month? At first, we can't even imagine how variables like Age, Sex etc. affect the chance of defaulting. But this is what EDA is all about, finding intricate details we can't see through our eyes at the first glance. I believe, the more surprises EDA provides us, the better it is!! \n\nLet's look at the various statistical attributes of each variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing that I observed was the huge range of 'LIMIT_BAL'. The reason why I am worried about this is that this might cause us problems while modelling and create a bias towards this variable. Secondly, I have observed certain values which are not mentioned in the data description like '0' in 'MARRIAGE' and '-2' and '-1' in the 'PAY_' variables. Thirdly, there are negative values in 'BILL_AMT' variables, which may mean the bank provides a credit system for bill payment. But we will deal with all of these problems afterwards while preparing the data for the splitting. \n\nLet's check the correlations between the variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = loan.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr,annot=True)\nplt.show()\ncorr = np.array(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we go, the first surprise. None of the variables have a strong correlation with the defaulting chance of the customer. Does this mean this problem is not solvable? Obviously NO! We aren't going to quit until we solve this. \n\nOk, so let's calm down now and take a look at things that might make sense to us. The 'PAY_' variables have a small amount of positive correlation with the target variable and a stronger correlation amongst themselves. Well, if you have or haven't defaulted in the previous months could decide the probability of defaulting in the next month. \n\nSecondly, 'BILL_AMT' variables have a strong positive correlation amongst themselves. This also makes sense as spending patterns of customers tend to remain constant and, E-commerce organizations take advantage of this, but that's a different matter and something to research about but later.\n\nAs expected, 'LIMIT_BAL' has a positive correlation with 'BILL_AMT' and a negative correlation with 'PAY_' variables. \n\nThe strongest negative correlation is between 'AGE' and 'MARRIAGE' which, is expected as younger customers tend to be single and older ones tend to be married."},{"metadata":{},"cell_type":"markdown","source":"Since we saw some unexpected values in certain columns, let's take a closer look at them."},{"metadata":{"trusted":true},"cell_type":"code","source":"sex = loan['SEX'].value_counts()\ned = loan['EDUCATION'].value_counts()\nmg = loan['MARRIAGE'].value_counts()\nage = loan['AGE'].value_counts()\nprint(sex)\nprint('----------')\nprint(ed)\nprint('----------')\nprint(mg)\nprint('----------')\nprint(age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'EDUCATION' variable has an unexplained value of '0' and another thing that bothers me are the values '4','5' and '6'. The description has very ambiguous descriptions for '5' and '6' as they both have 'Unknown' value. 'MARRIAGE' variable has another ambiguous value of '0'. The value '0' in 'EDUCATION' might mean high school dropout and '0' and '3' in 'MARRIAGE' might mean divorced, but we aren't here to assign descriptions. We can club together the ambiguous values since they don't make much sense to us. \n\nThis allows us to use the loc method on the DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.loc[loan['EDUCATION']==5,'EDUCATION']=4     \nloan.loc[loan['EDUCATION']==6,'EDUCATION']=4\nloan.loc[loan['EDUCATION']==0,'EDUCATION']=4\nloan.loc[loan['MARRIAGE']==0,'MARRIAGE']=3\nprint(loan['EDUCATION'].value_counts())\nprint('---------')\nprint(loan['MARRIAGE'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we cleaned certain variables, so let's check how they correlate to our target variable and hope for an improvement."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_corr = loan[['EDUCATION','MARRIAGE','default.payment.next.month']].corr()\nnc = np.array(new_corr)\nnc = nc[2,0:2]\noc = corr[24,3:5]\ndiff = nc-oc\nprint('Improvements in correlation:',diff)\nloan[['EDUCATION','MARRIAGE','default.payment.next.month']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation factors increased for 'AGE' by ~ 0.00584 and decreased for 'MARRIAGE' by ~ 0.00323. Amazing! That's an improvement, a minute one but as Neil Armstrong said 'This is one small step for a man, one giant leap for mankind.' \n\nOk I agree, that was a bit extreme, I guess that's what happens when you try to be creative on your kernel at 3 AM. But that's our aim, improve at every step and learn something new.\n\nLet's look at the 'PAY_' variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"p0 = loan['PAY_0'].value_counts()\np2 = loan['PAY_2'].value_counts()\np3 = loan['PAY_3'].value_counts()\np4 = loan['PAY_4'].value_counts()\np5 = loan['PAY_5'].value_counts()\np6 = loan['PAY_6'].value_counts()\nprint(p0)\nprint('----------')\nprint(p2)\nprint('----------')\nprint(p3)\nprint('----------')\nprint(p4)\nprint('----------')\nprint(p5)\nprint('----------')\nprint(p6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data description mentions '-1' value for the 'PAY_' variables as duly paid but there values such as '0' and'-2', which are ambiguous. Maybe '-2' refers to paid duly 2 months prior the deadline, but it would be easier for our task to club these values together as '0', because all of them mean that the bill was paid before the deadline."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.loc[loan['PAY_0']<=0,'PAY_0']=0          \nloan.loc[loan['PAY_2']<=0,'PAY_2']=0\nloan.loc[loan['PAY_3']<=0,'PAY_3']=0\nloan.loc[loan['PAY_4']<=0,'PAY_4']=0\nloan.loc[loan['PAY_5']<=0,'PAY_5']=0\nloan.loc[loan['PAY_6']<=0,'PAY_6']=0\np0 = loan['PAY_0'].value_counts()\np2 = loan['PAY_2'].value_counts()\np3 = loan['PAY_3'].value_counts()\np4 = loan['PAY_4'].value_counts()\np5 = loan['PAY_5'].value_counts()\np6 = loan['PAY_6'].value_counts()\nprint(p0)\nprint('----------')\nprint(p2)\nprint('----------')\nprint(p3)\nprint('----------')\nprint(p4)\nprint('----------')\nprint(p5)\nprint('----------')\nprint(p6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nc = loan[['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','default.payment.next.month']].corr()\nnc = np.array(nc)\noc = np.array(corr)\nnew_corr = nc[6,0:6]\nold_corr = oc[24,6:12]\ndiff = new_corr-old_corr\nprint('Improvements in correlation:',diff)\nloan[['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','default.payment.next.month']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like our replacement technique will certainly benefit our models. We have been able to achieve small improvements and are moving in the positive direction.\n\nThere are certain variables which I believe would give a better correlation when clubbed together like 'BILL_AMT_' and 'PAY_AMT_'. We can add them up which would represent the total bill and payment amount for the mentioned time period."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan['total_bill'] = loan['BILL_AMT1']+loan['BILL_AMT2']+loan['BILL_AMT3']+loan['BILL_AMT4']+loan['BILL_AMT5']+loan['BILL_AMT6']\nloan['total_pay'] = loan['PAY_AMT1']+loan['PAY_AMT2']+loan['PAY_AMT3']+loan['PAY_AMT4']+loan['PAY_AMT5']+loan['PAY_AMT6']\nloan[['total_bill','total_pay']].sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan[['total_bill','total_pay','default.payment.next.month']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the categorical variables and try to find trends within them by using plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"s_df = loan.groupby(['SEX','default.payment.next.month'])['default.payment.next.month'].count().unstack()\ns_df['percentage'] = round((s_df[1]/(s_df[0]+s_df[1]))*100,2)\ns_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_df[[0,1]].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot, it's evident that males have a higher probability of defaulting than females, since ~24% of males default compared to ~20% of females.\n\nAlso, according to the data, a higher number of females have defaulted (3763) than males (2873) in the previous months."},{"metadata":{"trusted":true},"cell_type":"code","source":"ed_df = loan.groupby(['EDUCATION','default.payment.next.month'])['default.payment.next.month'].count().unstack()\ned_df['percentage'] = round((ed_df[1]/(ed_df[0]+ed_df[1]))*100,2)\ned_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ed_df[[0,1]].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, it's visible that a greater amount of education tends to reduce the probability of default. This makes sense because lower education may mean a lower salary and limit. Let's check if our assumption is correct."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.groupby(['EDUCATION'])['LIMIT_BAL'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our assumption was perfect customers with graduate school degrees have the highest limits whereas high school pass outs have the least amongst all the categories.\n\nBut category '4' customers have their limits between graduate school and university pass-outs, so our initial assumption of '4' being high school dropouts may be wrong. Also, that won't affect our modelling, so over thinking about that won't be effective and useful.\n\nLet's check our marriage status affects the probability of default."},{"metadata":{"trusted":true},"cell_type":"code","source":"m_df = loan.groupby(['MARRIAGE','default.payment.next.month'])['default.payment.next.month'].count().unstack()\nm_df['percentage'] = round((m_df[1]/(m_df[0]+m_df[1]))*100,2)\nm_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_df[[0,1]].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Married customers have a higher chance of defaulting than single customers. The other category has the highest probability of default, but that is numerically irrelevant.\n\nLet's bin the ages into groups which would make it easier to look at the plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan['AGE'] = pd.cut(loan['AGE'],bins=[20,30,40,50,60,70,80],labels=[1,2,3,4,5,6])\nloan['AGE'].sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a_df = loan.groupby(['AGE','default.payment.next.month'])['default.payment.next.month'].count().unstack()\na_df['percentage'] = round((a_df[1]/(a_df[0]+a_df[1]))*100,2)\na_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a_df[[0,1]].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Younger customers have fewer chances of defaulting and older customers have a higher probability. But numerically category '6' and '5' are quite irrelevant.\n\nLet's take a look at the limits provided for various age groups, maybe this can clear out or give some sense to the results we are observing."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan.groupby(['AGE'])['LIMIT_BAL'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is surprising, category '6' has the highest mean limit but has the highest probability of default. Even though it's numerically irrelevant, I'm curious about this.\n\nAnother interesting observation is that even though younger customers have lower limits, they don't default as much as the older customers"},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"Now we will attempt to build various models, fit the training data and predict on the test data. We will aim to find the best model and check how the accuracy varies with the parameters. \n\nWe will use metrics such as Precision, Recall, F1 Scores and Logarithmic Loss to compare the models. To visualize the results we will make use of line plots and confusion matrix. The plots will help us choose the optimum parameter and confusion matrix will help us to calculate the F1 score. The confusion matrix will also give us the number of predictions with their respective true and predicted label.\n\nLet's split our data into training and test sets. The test set will be used to see how our model performs on the 'new' data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = loan[['LIMIT_BAL','SEX','MARRIAGE','EDUCATION','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','total_bill','total_pay']]\ny = loan['default.payment.next.month']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)\nfrom sklearn.preprocessing import MinMaxScaler\nmms_xtrain = MinMaxScaler().fit_transform(x_train)\nmms_xtest = MinMaxScaler().fit_transform(x_test)\n\nMetrics = pd.DataFrame({'F1':[],'Recall':[]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are certain variables with large ranges such as 'LIMIT_BAL', 'TOTAL_BILL' and 'TOTAL_PAY', we will use MinMaxScaler function to convert the ranges to 0-1. Since the ranges for all variables are transformed to the same values, the degree to which they will affect the target variable might become equal and avoid biases towards variables with a larger range.\n\nTo avoid repeating the code for creating plots and confusion matrix, I will create a function which will reduce the burden on us."},{"metadata":{"trusted":true},"cell_type":"code","source":"def accplots(param,predict,train,xlabel):\n    plt.plot(param,predict,'g')\n    plt.plot(param,train,'b')\n    plt.title('Variation Of F1 Score')\n    plt.xlabel(xlabel)\n    plt.ylabel('F1 Score')\n    plt.legend(('Prediction','Train'))\ndef cfmatrix(ytest,ypredict,title):\n    cfm = confusion_matrix(ytest,ypredict,labels=[0,1])\n    sns.heatmap(cfm,annot=True,fmt='d')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will predominantly make use of recall and F1 scores for evaluating the model. The reason why I'm focusing on these metrics is that they are of the most importance to us. Let me elaborate by connecting this with our problem.\n\nWhen banks predict the probability of default, they need the predictions to be very accurate. What I mean by this is, if a customer actually defaults, the model should accurately predict this outcome. So, we need the True positive(actual:1,predicted:1) predictions to be high in number and False negatives(actual:1,predicted:0) to be low in number. Since defaults cost highly to the banks and cause a high level of risk. Since recall is the ratio of true positives and the sum of true positives and false negatives, we need it to be as large as possible.\n\nBut if our model predicts some '0' labels as '1', that is allowable. Since this wrong prediction isn't as costly to the banks. Therefore a high precision of the model isn't our top priority. Precision is the ratio of true positives and the sum of true positives and false positives."},{"metadata":{},"cell_type":"markdown","source":"**K Nearest Neighbours**"},{"metadata":{},"cell_type":"markdown","source":"K-Nearest Neighbours is one of the most basic yet essential classification algorithms in Machine Learning. It is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data. If we plot the points (training data) on a graph, we may be able to locate some clusters or groups. Now, given an unclassified point, we can assign it to a group by observing what group its nearest neighbours belong to.\n\nOne of the parameters of the K-Nearest neighbours algorithm is the 'n_neighbors' value. It decides the number of neighbours which have to taken into consideration to classify a point (test data). We will vary this parameter and try to find the optimum number."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nneighbors,score_predict,score_train = [3,4,5,6,7,8,9],[],[]\nfor n in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n)\n    knn.fit(mms_xtrain,y_train)\n    y_ptrain = knn.predict(mms_xtrain)\n    y_predict = knn.predict(mms_xtest)\n    score_predict.append(metrics.f1_score(y_test,y_predict))\n    score_train.append(metrics.f1_score(y_train,y_ptrain))\naccplots(neighbors,score_predict,score_train,'Neighbours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot, it's evident that when we increase the number of neighbours, F1 scores of training data reduces and that of test data increases. Another trend which is observed is, F1 scores for even neighbours are less than odd numbers before and after itself. This is because when we set neighbours to an even number there is a possibility of a tie occurring. For example when neighbours = 4, there are chances of 2 of them belonging to '0' and other 2 belonging to '1'. In this situation, the algorithm chooses the label which came first in the training data.\n\nTherefore we will choose neighbours = 5, for our further predictions. Even though higher values have larger F1 scores, I'm pretty sure choosing these values will cause overfitting on the training data. Now we will predict the labels for the test data and analyze the confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(mms_xtrain,y_train)\ny_predict = knn.predict(mms_xtest)\nfrom sklearn.metrics import confusion_matrix\ncfmatrix(y_test,y_predict,'Confusion Matrix K Nearest Neighbors')\nprint(metrics.classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The precision and recall for '0' label are very high but it's the complete opposite for '1' label. This may be due to an imbalanced dataset. But we will tackle this problem after finding a couple of models which will give us a decent result."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.41,'Recall':0.33},name='KNN')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables. But instead of giving the exact value of the label, it gives the probabilistic values which lie between 0 and 1. Logistic Regression is quite similar to Linear Regression except that how they are used. \n\nIn Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1). The curve from the logistic function indicates the likelihood of something, in our case whether the customer will default(1) or not(0).\n\nFor this algorithm, we will vary the regularization factor. Regularizations also known as ‘shrinkage’ methods, reduce or shrink the coefficients in the resulting regression. This reduces the variance in the model: as input variables are changed, the model’s prediction changes less than it would have without the regularization. We use this parameter to avoid overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nreg,score_train,score_predict = [0.001,0.01,0.1,0.5,1.0],[],[]\n\nfor r in reg:\n    lr = LogisticRegression(C=r, solver='liblinear',penalty='l1')\n    lr.fit(mms_xtrain,y_train)\n    y_ptrain = lr.predict(mms_xtrain)\n    y_predict = lr.predict(mms_xtest)\n    score_train.append(round(metrics.f1_score(y_train,y_ptrain),4))\n    score_predict.append(round(metrics.f1_score(y_test,y_predict),4))\naccplots(reg,score_predict,score_train,'Regularization Factor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's visible from the plot that the F1 score suddenly increases in 0 to 0.1 range and stays constant for the remaining values. To avoid any overfitting, we will keep the value of the regularization factor as 0.1.\n\nNow let's predict the labels for the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.1, solver='liblinear')\nlr.fit(mms_xtrain,y_train)\ny_predict = lr.predict(mms_xtest)\nprobab = lr.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Logistic Regression')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log-Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recall for the positive label is very low and that is not a good sign. Since predicting an actual default as not a default, maybe very costly for the bank. But the model has a decent precision. "},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.39,'Recall':0.27},name='LR')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"Decision tree analysis is a predictive modelling tool that can be applied across many areas. Decision trees can be constructed by an algorithmic approach that can split the dataset in different ways based on different conditions. Decisions trees are the most powerful algorithms that fall under the category of supervised algorithms. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome. A decision tree simply asks a question and based on the answer (Yes/No), it will further split the tree into subtrees.\n\nWe will vary the depth parameter for the model. It specifies the depth up to which the tree must produce leaf nodes/ prediction nodes. A low depth will cause underfitting and very high will cause overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\ndepths,score_train,score_predict = [3,4,5,6,8,10,12,14,16,18,20],[],[]\nfor depth in depths:\n    dtc = DecisionTreeClassifier(criterion='entropy',max_depth=depth)\n    dtc.fit(mms_xtrain,y_train)\n    y_ptrain = dtc.predict(mms_xtrain)\n    y_predict = dtc.predict(mms_xtest)\n    score_train.append(round(metrics.f1_score(y_train,y_ptrain),4))\n    score_predict.append(round(metrics.f1_score(y_test,y_predict),4))\naccplots(depths,score_predict,score_train,'Depth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot clearly shows a case of overfitting when the depth increases. As the F1 score for training data increases but the F1 score for the test data decreases. We will set the depth value to 5, to avoid overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)\ndtc.fit(mms_xtrain,y_train)\ny_predict = dtc.predict(mms_xtest)\nprobab = dtc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Decision Tree')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model has a better recall for the positive label, sign of improvement from previous models."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.46,'Recall':0.36},name='DTC')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the tree that has been generated by the algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,20))\nfeatures = ['LIMIT_BAL','SEX','MARRIAGE','EDUCATION','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','total_bill','total_pay']\n_ = tree.plot_tree(dtc, \n                   feature_names=features,  \n                   class_names=['0','1'],\n                   filled=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"The objective of the support vector machine algorithm is to find a hyperplane in N-dimensional space(N — the number of features) that distinctly classifies the data points. To separate the two classes of data points, many possible hyperplanes could be chosen. Our objective is to find a plane that has the maximum margin. Margin is the maximum distance between data points of both classes. Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane.\n\nWe will vary the kernel function for this algorithm. Kernel Function is a method used to take data as input and transform into the required form of processing data. So, Kernel Function generally transforms the training set of data so that a non-linear decision surface can be transformed into a linear equation in a higher number of dimension spaces."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nkernels,score_train,score_predict = ['rbf','linear','poly','sigmoid'],[],[]\nfor kernel in kernels:\n    svc = SVC(kernel=kernel)\n    svc.fit(mms_xtrain,y_train)\n    y_ptrain = svc.predict(mms_xtrain)\n    y_predict = svc.predict(mms_xtest)\n    score_train.append(round(metrics.f1_score(y_train,y_ptrain),4))\n    score_predict.append(round(metrics.f1_score(y_test,y_predict),4))\naccplots(kernels,score_predict,score_train,'Kernels')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's evident that linear kernel function gives us the optimum F1 score, therefore we will use this function for further modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nsvc.fit(mms_xtrain,y_train)\ny_predict = svc.predict(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix SVM')\nprint(metrics.classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the largest F1 score and recall we have obtained till now. I was expecting this since SVM has an effective logic and doesn't assume anything based on the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.51,'Recall':0.50},name='SVM')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.\n\nBayes’ Theorem finds the probability of an event occurring given the probability of another event that has already occurred. Bayes’ theorem is stated mathematically as the following equation: P(A|B) = (P(B|A)xP(B))/P(A)"},{"metadata":{},"cell_type":"markdown","source":"Gaussian NB"},{"metadata":{},"cell_type":"markdown","source":"GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The distribution of the features is assumed to be Gaussian/Normal.\n\nWe will not vary any parameters,since there are none required for this algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(mms_xtrain,y_train)\ny_predict = gnb.predict(mms_xtest)\nprobab = gnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Gaussian NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:', metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recall for this model is also good, since it's quite close to the recall obtained from the SVM model."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.51,'Recall':0.49},name='GNB')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multinomial NB"},{"metadata":{},"cell_type":"markdown","source":"MultinomialB implements the Multinomial Naive Bayes algorithm for classification. The distribution of the features is assumed to be Multinomial.\n\nWe will vary the smoothing parameter( alpha) for the model. Additive smoothing is a technique used to smooth categorical data. It is a type of shrinkage estimator, as the resulting estimate will be between the experimental probability and the uniform probability. This parameter will be varied for all the remaining naive Bayes classifiers as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nalphas,score_train,score_predict = [1,5,10,25,40,80,100,150],[],[]\nfor alpha in alphas:\n    mnb = MultinomialNB(alpha=alpha)\n    mnb.fit(mms_xtrain,y_train)\n    y_ptrain = mnb.predict(mms_xtrain)\n    y_predict = mnb.predict(mms_xtest)\n    score_train.append(round(metrics.f1_score(y_train,y_ptrain),4))\n    score_predict.append(round(metrics.f1_score(y_test,y_predict),4))\naccplots(alphas,score_predict,score_train,'Alpha')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's visible that increasing the value of the smoothing parameter will decrease the F1 score. Therefore we will set the alpha value to 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB(alpha=5)\nmnb.fit(mms_xtrain,y_train)\ny_predict = mnb.predict(mms_xtest)\nprobab = mnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Multinomial NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I don't think we can consider this model for further analysis, since the recall is very low."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.29,'Recall':0.18},name='MNB')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Complement NB"},{"metadata":{},"cell_type":"markdown","source":"Complement NB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Specifically, CNB uses statistics from the complement of each class to compute the model’s weights. CNB regularly outperforms MNB (often by a considerable margin).\n\nWe will vary the smoothing parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import ComplementNB\nalphas,score_train,score_predict = [1,5,10,25,40,80,100,150],[],[]\nfor alpha in alphas:\n    cnb = ComplementNB(alpha=alpha)\n    cnb.fit(mms_xtrain,y_train)\n    y_ptrain = cnb.predict(mms_xtrain)\n    y_predict = cnb.predict(mms_xtest)\n    score_train.append(round(metrics.f1_score(y_train,y_ptrain),4))\n    score_predict.append(round(metrics.f1_score(y_test,y_predict),4))\naccplots(alphas,score_predict,score_train,'Alpha')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we increase the alpha value, F1 score for training data decreases but for test data it increases first then decreases. Therefore we will choose the point at which sudden gradient change is observed, that is alpha equal to 40. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cnb = ComplementNB(alpha=40)\ncnb.fit(mms_xtrain,y_train)\ny_predict = cnb.predict(mms_xtest)\nprobab = cnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Complement NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected CNB performed better than MNB. The model generated a decent recall similar to SVM."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.50,'Recall':0.50},name='CNB')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical NB"},{"metadata":{},"cell_type":"markdown","source":"CategoricalNB implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index, has its own categorical distribution. For each feature in the training set, CategoricalNB estimates a categorical distribution for each feature, conditioned on the labels.\n\nWe will vary the smoothing parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import CategoricalNB\nalphas,score_train,score_predict = [1,5,10,25,40,80,100,150],[],[]\nfor alpha in alphas:\n    catnb = CategoricalNB(alpha=alpha)\n    catnb.fit(mms_xtrain,y_train)\n    y_ptrain = catnb.predict(mms_xtrain)\n    y_predict = catnb.predict(mms_xtest)\n    score_train.append(round(metrics.f1_score(y_train,y_ptrain),4))\n    score_predict.append(round(metrics.f1_score(y_test,y_predict),4))\naccplots(alphas,score_predict,score_train,'Alpha')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The F1 score doesn't change much after altering the alpha values. But the F1 scores are very less so this model won't be of much use to us."},{"metadata":{"trusted":true},"cell_type":"code","source":"catnb = CategoricalNB(alpha=40)\ncatnb.fit(mms_xtrain,y_train)\ny_predict = catnb.predict(mms_xtest)\nprobab = catnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Categorical NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recall for the model is very low and is heavily affected by the fact that the data is imbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.02,'Recall':0.01},name='CatNB')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Random forest is an ensemble machine learning model. An ensemble machine learning model is a model which is a collection of several smaller models. The Random Forest model of machine learning is nothing but a collection of several decision trees. These trees come together to a combined decision to give the output. The average of all the trees is selected as the output.\n\nWe will vary the estimator parameter, which is the number of decision trees under consideration. Optimum depth for the trees was found to be 5 in the Decision Tree Model, therefore we will use the same for this model as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nestimators,score_train,score_predict = [10,50,80,100,150,200,250,300,500],[],[]\nfor est in estimators:\n    rfc = RandomForestClassifier(n_estimators=est,criterion='entropy',max_depth=5,random_state=10)\n    rfc.fit(mms_xtrain,y_train)\n    y_ptrain = rfc.predict(mms_xtrain)\n    y_predict = rfc.predict(mms_xtest)\n    score_train.append(round(metrics.f1_score(y_train,y_ptrain),4))\n    score_predict.append(round(metrics.f1_score(y_test,y_predict),4))\n\naccplots(estimators,score_predict,score_train,'Estimator')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot it's evident that F1 scores first increase, then decrease when estimators is set as 100 and increase again. We will use estimators set to 300 for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=300,criterion='entropy',max_depth=5,random_state=10)\nrfc.fit(mms_xtrain,y_train)\ny_predict = rfc.predict(mms_xtest)\nprobab = rfc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Random Forests')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model generates a decent F1 score but less recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.45,'Recall':0.33},name='RFC')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's comapare all the models we have tested till now. We will create a dataframe using pandas and sort them by F1 score and Recall. "},{"metadata":{"trusted":true},"cell_type":"code","source":"Metrics.sort_values(by=['F1','Recall'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best models which we have built till now are Support Vector Machine, Gaussian Naive Bayes and Complement Naive Bayes. We will now address the problem of imbalanced dataset, with the help of resampling."},{"metadata":{},"cell_type":"markdown","source":"# Resampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(loan['default.payment.next.month'].value_counts())\nprint(y_train.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, we have an imbalanced dataset, since the data with '1' label or default is minuscule as compared to '0' label or not default. This will cause issues with certain algorithms. To avoid this dilemma we will make use of resampling. Resampling is a method that consists of drawing repeated samples from the original data samples. Resampling involves the selection of randomized cases with replacement from the original data sample in such a manner that each number of the sample drawn has several cases that are similar to the original data sample. Due to replacement, the drawn number of samples that are used by the method of resampling consists of repetitive cases. \n\nAlgorithms such as Decision Tree and Logistic Regression have a bias towards the majority class, and they tend to ignore the minority class. They tend only to predict the majority class, hence, having major misclassification of the minority class in comparison with the majority class.\n\nSo we will use techniques such as OverSampling, UnderSampling, SMOTE and NearMiss Algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\nconc = pd.DataFrame(x_train)\nconc['default.payment.next.month'] = y_train\ndf_maj = conc[conc['default.payment.next.month']==0]\ndf_min = conc[conc['default.payment.next.month']==1]\nprint(df_maj['default.payment.next.month'].value_counts())\nprint(df_min['default.payment.next.month'].value_counts())\nconc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Under Sampling"},{"metadata":{},"cell_type":"markdown","source":"In this technique we will under sample the majority class. It aims to balance class distribution by randomly eliminating majority class examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_maj_ds = resample(df_maj,replace=False,n_samples=5299,random_state=1234)\ndf_ds = pd.concat([df_maj_ds,df_min])\nprint(df_ds['default.payment.next.month'].value_counts())\ndf_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over Sampling"},{"metadata":{},"cell_type":"markdown","source":"Oversampling involves supplementing the training data with multiple copies of some of the minority classes. It aims to balance the dataset by randomly duplicating the minority class."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_min_us = resample(df_min,replace=True,n_samples=18701,random_state=1234)\ndf_us = pd.concat([df_maj,df_min_us])\nprint(df_us['default.payment.next.month'].value_counts())\ndf_us","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SMOTE"},{"metadata":{},"cell_type":"markdown","source":"Synthetic Minority Oversampling Technique or SMOTE, is a type of data augmentation for the minority class. It involves synthesizing of new samples from the existing samples. A random sample from the minority class is first chosen. Then k of the nearest neighbors for that sample are found (typically k=5). A randomly selected neighbor is chosen and a synthetic sample is created at a randomly selected point between the two examples in feature space. Basically, in oversampling duplicates are produced, but in SMOTE samples are generated(by mixing and matching) with the help of minority labels in the training data.\n\nTo implement SMOTE algorithm we will make use of the Imbalanced-Learn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\ndf_smote,smote_ytrain = SMOTE(random_state=1234).fit_sample(x_train,y_train)\nprint(df_smote['default.payment.next.month'].value_counts())\ndf_smote['AGE'] = df_smote['AGE'].fillna(df_smote['AGE'].mode()[0])\ndf_smote","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Near Miss"},{"metadata":{},"cell_type":"markdown","source":"NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process. To prevent problem of information loss in most under-sampling techniques, near-neighbor methods are widely used.\n\nTo implement Near Miss algorithm we will make use of the Imbalanced-Learn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import NearMiss\ndf_nm,nm_ytrain = NearMiss().fit_sample(x_train,y_train)\nprint(df_nm['default.payment.next.month'].value_counts())\ndf_nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will seperate the data into features and target variable. Since we transformed our data using MinMaxScaler for intial modelling we will use the same here as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_xtrain = df_ds[['LIMIT_BAL','SEX','MARRIAGE','EDUCATION','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','total_bill','total_pay']]\nds_ytrain = df_ds['default.payment.next.month']\nus_xtrain = df_us[['LIMIT_BAL','SEX','MARRIAGE','EDUCATION','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','total_bill','total_pay']]\nus_ytrain = df_us['default.payment.next.month']\nsmote_xtrain = df_smote[['LIMIT_BAL','SEX','MARRIAGE','EDUCATION','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','total_bill','total_pay']]\nnm_xtrain = df_nm[['LIMIT_BAL','SEX','MARRIAGE','EDUCATION','AGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','total_bill','total_pay']]\nds_xtrain = MinMaxScaler().fit_transform(ds_xtrain)\nus_xtrain = MinMaxScaler().fit_transform(us_xtrain)\nsmote_xtrain = MinMaxScaler().fit_transform(smote_xtrain)\nnm_xtrain = MinMaxScaler().fit_transform(nm_xtrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Re-Modelling"},{"metadata":{},"cell_type":"markdown","source":"Here we will fit the newly generated datasets on the models which performed impressively before. "},{"metadata":{},"cell_type":"markdown","source":"SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nsvc.fit(ds_xtrain,ds_ytrain)\ny_predict = svc.predict(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix SVM')\nprint(metrics.classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nsvc.fit(us_xtrain,us_ytrain)\ny_predict = svc.predict(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix SVM')\nprint(metrics.classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nsvc.fit(smote_xtrain,smote_ytrain)\ny_predict = svc.predict(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix SVM')\nprint(metrics.classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nsvc.fit(nm_xtrain,nm_ytrain)\ny_predict = svc.predict(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix SVM')\nprint(metrics.classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best results were obtained for undersampled and oversampled datasets. But one unique observation is that the results for both the techniques were exactly the same. This may be due to the fact that both the techniques contain duplicates, therefore nothing 'new' was provided to the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.51,'Recall':0.51},name='SVM_US')\nMetrics = Metrics.append(new_row, ignore_index=False)\nnew_row = pd.Series(data={'F1':0.51,'Recall':0.51},name='SVM_OS')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gaussian NB"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(ds_xtrain,ds_ytrain)\ny_predict = gnb.predict(mms_xtest)\nprobab = gnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Gaussian NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:', metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(us_xtrain,us_ytrain)\ny_predict = gnb.predict(mms_xtest)\nprobab = gnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Gaussian NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:', metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(smote_xtrain,smote_ytrain)\ny_predict = gnb.predict(mms_xtest)\nprobab = gnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Gaussian NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:', metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(nm_xtrain,nm_ytrain)\ny_predict = gnb.predict(mms_xtest)\nprobab = gnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Gaussian NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:', metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best result was obatained after making use of the SMOTE and under sampled training data. We have generated the highest recall till now of 0.53."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.51,'Recall':0.53},name='GNB-SMT')\nMetrics = Metrics.append(new_row, ignore_index=False)\nnew_row = pd.Series(data={'F1':0.51,'Recall':0.53},name='GNB-DS')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Complement NB"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnb = ComplementNB(alpha=40)\ncnb.fit(ds_xtrain,ds_ytrain)\ny_predict = cnb.predict(mms_xtest)\nprobab = cnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Complement NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnb = ComplementNB(alpha=40)\ncnb.fit(us_xtrain,us_ytrain)\ny_predict = cnb.predict(mms_xtest)\nprobab = cnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Complement NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnb = ComplementNB(alpha=40)\ncnb.fit(smote_xtrain,smote_ytrain)\ny_predict = cnb.predict(mms_xtest)\nprobab = cnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Complement NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnb = ComplementNB(alpha=40)\ncnb.fit(nm_xtrain,nm_ytrain)\ny_predict = cnb.predict(mms_xtest)\nprobab = cnb.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Complement NB')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We haven't obtained a better model than before, so we won't be considering any of the above ones for further analysis. One of the reasons we weren't able to generate a better model than before is because Complement NB model is suited for imbalanced datasets. The Complement Naive Bayes classifier was designed to correct the “severe assumptions” made by the standard Multinomial Naive Bayes classifier, which makes it suitable for imbalanced datasets. "},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)\ndtc.fit(ds_xtrain,ds_ytrain)\ny_predict = dtc.predict(mms_xtest)\nprobab = dtc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Decision Tree')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)\ndtc.fit(us_xtrain,us_ytrain)\ny_predict = dtc.predict(mms_xtest)\nprobab = dtc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Decision Tree')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)\ndtc.fit(smote_xtrain,smote_ytrain)\ny_predict = dtc.predict(mms_xtest)\nprobab = dtc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Decision Tree')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)\ndtc.fit(nm_xtrain,nm_ytrain)\ny_predict = dtc.predict(mms_xtest)\nprobab = dtc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Decision Tree')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! We just achieved our highest recall till now, of 0.63!"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.50,'Recall':0.63},name='DTC_DS')\nMetrics = Metrics.append(new_row, ignore_index=False)\nnew_row = pd.Series(data={'F1':0.52,'Recall':0.52},name='DTC_US')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=300,criterion='entropy',max_depth=5,random_state=10)\nrfc.fit(ds_xtrain,ds_ytrain)\ny_predict = rfc.predict(mms_xtest)\nprobab = rfc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Random Forests')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=300,criterion='entropy',max_depth=5,random_state=10)\nrfc.fit(us_xtrain,us_ytrain)\ny_predict = rfc.predict(mms_xtest)\nprobab = rfc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Random Forests')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=300,criterion='entropy',max_depth=5,random_state=10)\nrfc.fit(smote_xtrain,smote_ytrain)\ny_predict = rfc.predict(mms_xtest)\nprobab = rfc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Random Forests')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=300,criterion='entropy',max_depth=5,random_state=10)\nrfc.fit(nm_xtrain,nm_ytrain)\ny_predict = rfc.predict(mms_xtest)\nprobab = rfc.predict_proba(mms_xtest)\ncfmatrix(y_test,y_predict,'Confusion Matrix Random Forests')\nprint(metrics.classification_report(y_test,y_predict))\nprint('Log Loss:',metrics.log_loss(y_test,probab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of our new models performed better than the orignal model. Therefore one inference that can be made is that RFC model is highly affected due to imbalanced dataset. This is due to the fact that DTC/RFC algorithms tend to reduce the error and equal importance to the minority class isn't given."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.54,'Recall':0.60},name='RFC_DS')\nMetrics = Metrics.append(new_row, ignore_index=False)\nnew_row = pd.Series(data={'F1':0.53,'Recall':0.54},name='RFC_US')\nMetrics = Metrics.append(new_row, ignore_index=False)\nnew_row = pd.Series(data={'F1':0.39,'Recall':0.55},name='RFC_NM')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSince the down sampled data and SMOTE data have been producing good results, let's check it with one last model. Let's make use of the dataset with the k nearest neighbours model. Hoping for a new surprise!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(ds_xtrain,ds_ytrain)\ny_predict = knn.predict(mms_xtest)\nfrom sklearn.metrics import confusion_matrix\ncfmatrix(y_test,y_predict,'Confusion Matrix K Nearest Neighbors')\nprint(metrics.classification_report(y_test,y_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Brilliant!! We have again obtained our highest recall score. This is amazing the down sampled data has performed very well on various models."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_row = pd.Series(data={'F1':0.47,'Recall':0.64},name='KNN_DS')\nMetrics = Metrics.append(new_row, ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"Metrics.sort_values(by=['Recall','F1'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(Metrics.index,Metrics['Recall'],color='b')\nplt.scatter(Metrics.index,Metrics['F1'],color='g')\nplt.xticks(rotation= 90)\nplt.legend(('Recall','F1'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The various resampling techniques defintely helped us to refine our models.\n\nI hope this kernel helped you guys to gain a bit of knowledge on various classification models, metrics used for evaluation of models and most importantly how to tackle imbalanced datasets with the help of resampling."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}