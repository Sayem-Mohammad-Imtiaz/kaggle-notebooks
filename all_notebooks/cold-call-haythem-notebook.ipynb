{"cells":[{"metadata":{"_uuid":"38f9e75171f2014682cedd1209e0700e7d925e30"},"cell_type":"markdown","source":"# Cold Calls"},{"metadata":{"_uuid":"272a8f45fb0ea9c512e981ffe2cde615b8bd7543"},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"trusted":false,"_uuid":"04433183c5220914bb5af40c71af74c71b2cd086"},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,GradientBoostingClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d55026c108c1a5edef060aa04b1f5903049d529"},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":false,"_uuid":"627616f046f4ef45271c88bff8a71d515ee0462c"},"cell_type":"code","source":"train=pd.read_csv('../input/carInsurance_train.csv')\ntest=pd.read_csv('../input/carInsurance_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8581645f51a22bb2cf66ae256c8dac96b6c52e93"},"cell_type":"code","source":"print('The train dataset has %d observations and %d features' % (train.shape[0], train.shape[1]))\nprint('The test dataset has %d observations and %d features' % (test.shape[0], test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"196bc5d03bfd2f2a076f35bc6280b4f05ef52ea7"},"cell_type":"markdown","source":"# Data Exploration & VisualizationÂ¶"},{"metadata":{"trusted":false,"_uuid":"393db729cd0fc83388640c128bb6ca4935fed1fb"},"cell_type":"code","source":"# First check out correlations among numeric features\n# Heatmap is a useful tool to get a quick understanding of which variables are important\ncolormap = plt.cm.viridis\ncor = train.corr()\ncor = cor.drop(['Id'],axis=1).drop(['Id'],axis=0)\nplt.figure(figsize=(12,12))\nsns.heatmap(cor,vmax=0.8,cmap=colormap,annot=True,fmt='.2f',square=True,annot_kws={'size':10},linecolor='white',linewidths=0.1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f122a876137fa207b932ef5b74f54a1fd7a780f5"},"cell_type":"markdown","source":"Features are fairly independent, except DaysPassed and PreAttempts. Cold call success is positively correlated with PreAttemps,DaysPassed,Age and Balance, and negatively correlated with default, HHInsurance, CarLoan, LastContactDay and NoOfContacts."},{"metadata":{"trusted":false,"_uuid":"535e9c049647dbf12f5b0b2f5c79d214fda18046"},"cell_type":"code","source":"imp_feats = ['CarInsurance','Age','Balance','HHInsurance', 'CarLoan','NoOfContacts','DaysPassed','PrevAttempts']\nsns.pairplot(train[imp_feats],hue='CarInsurance',palette='viridis',size=2.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d43084acdc10b00cbe5f8b65bb106a2cb7bb9d0"},"cell_type":"markdown","source":"Age: It's interesting to see that seniors are more likely to buy car insurance.\n\nBalance: For balance, the data point at the upper right corner might be an outlier \n\nHHInsurance: Households insured are less likely to buy car insurance \n\nCarLoan: People with car loan are less likely to buy \n\nNoOfContacts: Too many contacts causes customer attrition \n\nDaysPassed: It looks like the more day passed since the last contact, the better \n\nPrevAttempts: Also, more previous attempts, less likely to buy. There is a potential outlier here"},{"metadata":{"trusted":false,"_uuid":"efa0a1a5eb06d0ceadb39f440d8defc48e14ee53"},"cell_type":"code","source":"# Next check out categorical features\ncat_feats = train.select_dtypes(include=['object']).columns\nplt_feats = cat_feats[(cat_feats!= 'CallStart') & (cat_feats!='CallEnd')]\n\nfor feature in plt_feats:\n    plt.figure(figsize=(10,6))\n    sns.barplot(feature,'CarInsurance', data=train,palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2e7fb6b1ddfe363f7cf380052e890fb21aedcd8"},"cell_type":"markdown","source":"Job: Student are most likely to buy insurance, followed by retired and unemployed folks.This is aligned with the age distribution. There might be some promotion targeting students? \n\nMarital status: Married people are least likely to buy car insurance. Opportunities for developing family insurance business\n\nEducation: People with higher education are more likely to buy \n\nCommunication: No big difference between cellular and telephone \n\nOutcome in previous campaign: Success in previous marketing campaign is largely associated with success in this campaign \n\nContact Month: Mar, Sep, Oct, and Dec are the hot months. It might be associated with school season?"},{"metadata":{"trusted":false,"_uuid":"aba177d6c34b4ff72de22a373cf267d682d1ba79"},"cell_type":"code","source":"# Check outliers\n# From the pairplot, we can see there is an outlier with extreme high balance. Drop that obs here.\ntrain[train['Balance']>80000]\ntrain = train.drop(train[train.index==1742].index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ab5178ae4d962a4ad4b92e787903ee466a47e0"},"cell_type":"markdown","source":"### Handling Miss Data\n"},{"metadata":{"trusted":false,"_uuid":"436ae56c49c07a070d72f5e76e1d58184bbb4d1a"},"cell_type":"code","source":"# merge train and test data here in order to impute missing values all at once\nall=pd.concat([train,test],keys=('train','test'))\nall.drop(['CarInsurance','Id'],axis=1,inplace=True)\nprint(all.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a2514ee655df3cf76009b73a5d55a1bf5158a97b"},"cell_type":"code","source":"total = all.isnull().sum()\npct = total/all.isnull().count()\nNAs = pd.concat([total,pct],axis=1,keys=('Total','Pct'))\nNAs[NAs.Total>0].sort_values(by='Total',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"388f62ff16be64cb9b61d9ce5a1793cfb9498026"},"cell_type":"code","source":"all_df = all.copy()\n\n# Fill missing outcome as not in previous campaign\nall_df[all_df['DaysPassed']==-1].count()\nall_df.loc[all_df['DaysPassed']==-1,'Outcome']='NoPrev'\n\n# Fill missing communication with none \nall_df['Communication'].value_counts()\nall_df['Communication'].fillna('None',inplace=True)\n\nall_df['Education'].value_counts()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c7a80b09bc5396a0af78c592efe4cdce90317926"},"cell_type":"code","source":"# Create job-education level mode mapping\nedu_mode=[]\njob_types = all_df.Job.value_counts().index\nfor job in job_types:\n    mode = all_df[all_df.Job==job]['Education'].value_counts().nlargest(1).index\n    edu_mode = np.append(edu_mode,mode)\nedu_map=pd.Series(edu_mode,index=all_df.Job.value_counts().index)\nedu_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"17edf4791e8020f3227e61cf50bbbd62a555d9cf"},"cell_type":"code","source":"#  Apply the mapping to missing eductaion obs\nfor j in job_types:\n    all_df.loc[(all_df['Education'].isnull()) & (all_df['Job']==j),'Education'] = edu_map.loc[edu_map.index==j][0]\nall_df['Education'].fillna('None',inplace=True)\n\n# Fill missing job with none\nall_df['Job'].fillna('None',inplace=True)\n\n# Double check if there is still any missing value\nall_df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a41d39b6541b994e14310bb56b4e9dc6f6a25d1"},"cell_type":"markdown","source":"# Feature Engineering\n"},{"metadata":{"_uuid":"832d00d4d20edbcbe5a263d9f4d94b2047ad8da6"},"cell_type":"markdown","source":"There are three types of features: \n\nClient features: Age, Job, Marital, Education, Default, Balance, HHInsurance, CarLoan \n\nCommunication features: LastContactDay, LastContactMonth, CallStart, CallEnd, Communication, NoOfContacts, DaysPassed \n\nPrevious campaign features: PrevAttempts, Outcome"},{"metadata":{"trusted":false,"_uuid":"291f58deda96096590b4299606188a940dc2cdc7"},"cell_type":"code","source":"# First simplify some client features\n\n# Create age group based on age bands\nall_df['AgeBand']=pd.cut(all_df['Age'],5)\nprint(all_df['AgeBand'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"758808551ee046002454b5d6bd3a3eac8a0ccc35"},"cell_type":"code","source":"all_df.loc[(all_df['Age']>=17) & (all_df['Age']<34),'AgeBin'] = 1\nall_df.loc[(all_df['Age']>=34) & (all_df['Age']<49),'AgeBin'] = 2\nall_df.loc[(all_df['Age']>=49) & (all_df['Age']<65),'AgeBin'] = 3\nall_df.loc[(all_df['Age']>=65) & (all_df['Age']<80),'AgeBin'] = 4\nall_df.loc[(all_df['Age']>=80) & (all_df['Age']<96),'AgeBin'] = 5\nall_df['AgeBin'] = all_df['AgeBin'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e41b02a0246af232742095e4c8dd583e305980fe"},"cell_type":"code","source":"# Create balance groups\nall_df['BalanceBand']=pd.cut(all_df['Balance'],5)\nprint(all_df['BalanceBand'].value_counts())\nall_df.loc[(all_df['Balance']>=-3200) & (all_df['Balance']<17237),'BalanceBin'] = 1\nall_df.loc[(all_df['Balance']>=17237) & (all_df['Balance']<37532),'BalanceBin'] = 2\nall_df.loc[(all_df['Balance']>=37532) & (all_df['Balance']<57827),'BalanceBin'] = 3\nall_df.loc[(all_df['Balance']>=57827) & (all_df['Balance']<78122),'BalanceBin'] = 4\nall_df.loc[(all_df['Balance']>=78122) & (all_df['Balance']<98418),'BalanceBin'] = 5\nall_df['BalanceBin'] = all_df['BalanceBin'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"343b48b52ed9754dcd338c956b0f95740a938f86"},"cell_type":"code","source":"all_df = all_df.drop(['AgeBand','BalanceBand','Age','Balance'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0d5fd7fe93b202f91503c2031b42c65b508b190e"},"cell_type":"code","source":"#  Convert education level to numeric \nall_df['Education'] = all_df['Education'].replace({'None':0,'primary':1,'secondary':2,'tertiary':3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f635d6ff2e64141aa864f7a78c2735c969658a45"},"cell_type":"code","source":"# Next create some new communication Features. This is the place feature engineering coming into play\n\n# Get call length\nall_df['CallEnd'] = pd.to_datetime(all_df['CallEnd'])\nall_df['CallStart'] = pd.to_datetime(all_df['CallStart'])\nall_df['CallLength'] = ((all_df['CallEnd'] - all_df['CallStart'])/np.timedelta64(1,'m')).astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a84dbeb9cd4a3606042c217b366059c447bbe6d1"},"cell_type":"code","source":"all_df['CallLenBand']=pd.cut(all_df['CallLength'],5)\nprint(all_df['CallLenBand'].value_counts())\n\n# Create call length bins\nall_df.loc[(all_df['CallLength']>= 0) & (all_df['CallLength']<11),'CallLengthBin'] = 1\nall_df.loc[(all_df['CallLength']>=11) & (all_df['CallLength']<22),'CallLengthBin'] = 2\nall_df.loc[(all_df['CallLength']>=22) & (all_df['CallLength']<33),'CallLengthBin'] = 3\nall_df.loc[(all_df['CallLength']>=33) & (all_df['CallLength']<44),'CallLengthBin'] = 4\nall_df.loc[(all_df['CallLength']>=44) & (all_df['CallLength']<55),'CallLengthBin'] = 5\nall_df['CallLengthBin'] = all_df['CallLengthBin'].astype(int)\nall_df = all_df.drop('CallLenBand',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6178c5704ffe8a4fd4963cd7b4b4513b547d697f"},"cell_type":"code","source":"# Get call start hour\nall_df['CallStartHour'] = all_df['CallStart'].dt.hour\nall_df[['CallStart','CallEnd','CallLength','CallStartHour']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"988009919243cb4e2f604909d19919d3721e711b"},"cell_type":"code","source":"# Get workday of last contact based on call day and month, assuming the year is 2016\nall_df['LastContactDate'] = all_df.apply(lambda x:datetime.datetime.strptime(\"%s %s %s\" %(2018,x['LastContactMonth'],x['LastContactDay']),\"%Y %b %d\"),axis=1)\nall_df['LastContactWkd'] = all_df['LastContactDate'].dt.weekday\nall_df['LastContactWkd'].value_counts()\nall_df['LastContactMon'] = all_df['LastContactDate'].dt.month\nall_df = all_df.drop('LastContactMonth',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f69f20780d20a63c8901029cd9519ee499785df1"},"cell_type":"code","source":"# Get week of last contact\nall_df['LastContactWk'] = all_df['LastContactDate'].dt.week\nMonWk = all_df.groupby(['LastContactWk','LastContactMon'])['Education'].count().reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c0a074b34abaf1a5e0ac3369f788f673acb8076"},"cell_type":"code","source":"MonWk = MonWk.drop('Education',axis=1)\nMonWk['LastContactWkNum']=0\nfor m in range(1,13):\n    k=0\n    for i,row in MonWk.iterrows():\n        if row['LastContactMon']== m:\n            k=k+1\n            row['LastContactWkNum']=k","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b3c9cd698f7005e6f258af6c0e778335d86650d5"},"cell_type":"code","source":"def get_num_of_week(df):\n    for i,row in MonWk.iterrows():\n        if (df['LastContactWk']== row['LastContactWk']) & (df['LastContactMon']== row['LastContactMon']):\n            return row['LastContactWkNum']\n\nall_df['LastContactWkNum'] = all_df.apply(lambda x: get_num_of_week(x),axis=1)\nall_df[['LastContactWkNum','LastContactWk','LastContactMon']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b27136efc3ef36bf58de17ba656061ecf3fbffe7"},"cell_type":"code","source":"# Spilt numeric and categorical features\ncat_feats = all_df.select_dtypes(include=['object']).columns\nnum_feats = all_df.select_dtypes(include=['float64','int64']).columns\nnum_df = all_df[num_feats]\ncat_df = all_df[cat_feats]\nprint('There are %d numeric features and %d categorical features\\n' %(len(num_feats),len(cat_feats)))\nprint('Numeric features:\\n',num_feats.values)\nprint('Categorical features:\\n',cat_feats.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a0db12cd31711e4aed1147c9549652738afc86b0"},"cell_type":"code","source":"cat_df = pd.get_dummies(cat_df)\nall_data = pd.concat([num_df,cat_df],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0bed4f9e7cbcfa6bf07ef698914c6ba6247c5dea"},"cell_type":"code","source":"# Split train and test\nidx=pd.IndexSlice\ntrain_df=all_data.loc[idx[['train',],:]]\ntest_df=all_data.loc[idx[['test',],:]]\ntrain_label=train['CarInsurance']\nprint(train_df.shape)\nprint(len(train_label))\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6870c62cd1b11e34693fa50cdfca76d11e50b077"},"cell_type":"code","source":"# Train test split\nx_train, x_test, y_train, y_test = train_test_split(train_df,train_label,test_size = 0.005,random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"19b8cf072db8dc7c4b1e2d113c86e53b87b582b7"},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98121849a6ced8987a77efeca355f3a317ea7862"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":false,"_uuid":"701476d46f34c4304aaef2dd80766e5f000f4430"},"cell_type":"code","source":"# Create a cross validation function \ndef get_best_model(estimator, params_grid={}):\n    \n    model = GridSearchCV(estimator = estimator,param_grid = params_grid,cv=3, scoring=\"accuracy\", n_jobs= -1)\n    model.fit(x_train,y_train)\n    print('\\n--- Best Parameters -----------------------------')\n    print(model.best_params_)\n    print('\\n--- Best Model -----------------------------')\n    best_model = model.best_estimator_\n    print(best_model)\n    return best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1d8fed24b9426a9623684a30dde5de8b7f054fe6"},"cell_type":"code","source":"# Create a model fitting function\ndef model_fit(model,feature_imp=True,cv=5):\n\n    # model fit   \n    clf = model.fit(x_train,y_train)\n    \n    # model prediction     \n    y_pred = clf.predict(x_test)\n    \n    # model report     \n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix')\n\n    print('\\n--- Train Set -----------------------------')\n    print('Accuracy: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv)),np.std(cross_val_score(clf,x_train,y_train,cv=cv))))\n    print('AUC: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc'))))\n    print('\\n--- Validation Set -----------------------------')    \n    print('Accuracy: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv)),np.std(cross_val_score(clf,x_test,y_test,cv=cv))))\n    print('AUC: %.5f +/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc'))))\n    print('-----------------------------------------------') \n\n    # feature importance \n    if feature_imp:\n        feat_imp = pd.Series(clf.feature_importances_,index=all_data.columns)\n        feat_imp = feat_imp.nlargest(15).sort_values()\n        plt.figure()\n        feat_imp.plot(kind=\"barh\",figsize=(6,8),title=\"Most Important Features\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ac9cad9d98e18c1ae4e54dea5064dd150cc5936c"},"cell_type":"code","source":"# The confusion matrix plotting function is from the sklearn documentation below:\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = ['Success','Failure']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b71bd959a6b9c4210330765395705632d5095e7f"},"cell_type":"markdown","source":"## k-Nearest Neighbors (KNN)"},{"metadata":{"trusted":false,"_uuid":"e6c145ee5885773114b0813e870edc3bdc987f1b"},"cell_type":"code","source":"# Let's start with KNN. An accuracy of 0.76 is not very impressive. I will just take this as the model benchmark. \nknn = KNeighborsClassifier()\nparameters = {'n_neighbors':[5,6,7], \n              'p':[1,2],\n              'weights':['uniform','distance']}\nclf_knn = get_best_model(knn,parameters)\nmodel_fit(model=clf_knn, feature_imp=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e48ae56b5bb6892ebe21bf32f117b6912f02f853"},"cell_type":"markdown","source":"## Naive Bayes Classifier"},{"metadata":{"trusted":false,"_uuid":"be67a50f71bbb116930f4480254f140a26cc8d5f"},"cell_type":"code","source":"# As expected, Naive Bayes classifier doesn't perform well here. \n# There are multiple reasons. Some of the numeric features are not normally distributed, which is a strong assemption hold by Naive Bayes. \n# Also, features are definitely not independent.  \nclf_nb = GaussianNB()\nmodel_fit(model=clf_nb,feature_imp=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"544dec212dddd491ee55cf874cd02213b2122c34"},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":false,"_uuid":"b58d0a96437f7874c183063aa39d64a95f96351b"},"cell_type":"code","source":"# We're making progress here. Logistic regression performs better than KNN. \nlg = LogisticRegression(random_state=3)\nparameters = {'C':[0.8,0.9,1], \n              'penalty':['l1','l2']}\nclf_lg = get_best_model(lg,parameters)\nmodel_fit(model=clf_lg, feature_imp=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e6dcf3bc96d94f0aa8300501d910f4744836f74"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":false,"_uuid":"ba82c252302f5c5a63484ef015f4ccba80d0d751"},"cell_type":"code","source":"# I did some manual parameter tuning here. This is the best model so far. \n# Based on the feature importance report, call length, last contact week, and previous success are strong predictors of cold call success\nrf = RandomForestClassifier(random_state=3)\nparameters={'n_estimators':[100],\n            'max_depth':[10],\n            'max_features':[13,14],\n            'min_samples_split':[11]}\nclf_rf= get_best_model(rf,parameters)\nmodel_fit(model=clf_rf, feature_imp=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb27527445e7be0d5095289a41b8ccba6382210c"},"cell_type":"markdown","source":"## Support Vector Machines"},{"metadata":{"trusted":false,"_uuid":"42aa985834641d66ae82b5130f7f4e86a31405f4"},"cell_type":"code","source":"# try a SVM RBF model \nsvc = svm.SVC(kernel='rbf', probability=True, random_state=3)\nparameters = {'gamma': [0.005,0.01,0.02],\n              'C': [0.5,1,5]}\nclf_svc = get_best_model(svc, parameters)\nmodel_fit(model=clf_svc,feature_imp=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41be8d047f0ac56127651b4a8ea931ec99f95f44"},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":false,"_uuid":"1ff096f52b2fd2fd355aaa3a67cad42f291402a2"},"cell_type":"code","source":"# Finally let's try out XBGoost. As expected, it outperforms all other algorithms. \n# Also, based on feature importances, some of the newly created features such as call start hour, last contact week and weekday \n# have been picked as top features. \n\nimport xgboost as xgb\nxgb = xgb.XGBClassifier()\nparameters={'n_estimators':[900,1000,1100],\n            'learning_rate':[0.01],\n            'max_depth':[8],\n            'min_child_weight':[1],\n            'subsample':[0.8],\n            'colsample_bytree':[0.3,0.4,0.5]}\nclf_xgb= get_best_model(xgb,parameters)\nmodel_fit(model=clf_xgb, feature_imp=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"446c33511fbbb43c9872ed0db30a6f5cce432b90"},"cell_type":"markdown","source":"# Model EvaluationÂ¶\n"},{"metadata":{"trusted":false,"_uuid":"a43153c19ebeeb9dfcb78100b40980ef0105f676"},"cell_type":"code","source":"# Compare model performance\nclfs= [clf_knn, clf_nb, clf_lg, clf_rf, clf_svc, clf_xgb]\nindex =['K-Nearest Neighbors','Naive Bayes','Logistic Regression','Random Forest','Support Vector Machines','XGBoost']\nscores=[]\nfor clf in clfs:\n    score = np.mean(cross_val_score(clf,x_test,y_test,cv=5,scoring = 'accuracy'))\n    scores = np.append(scores,score)\nmodels = pd.Series(scores,index=index)\nmodels.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"643dced280c218c0bcdbc19c2b8497092a97458b"},"cell_type":"markdown","source":"# Ensemble Voting"},{"metadata":{"trusted":false,"_uuid":"05cc1d56dfd8de2fd287ddbaf477ebd15745269f"},"cell_type":"code","source":"#XGBoost and Random Forest show different important features, implying that those models are capturing different aspects of the data\n# To get the final model, I ensembled different classifiers based on majority voting.\n# XGBoost and Random Forest are given larger weights due to their better performance. \n\nclf_vc = VotingClassifier(estimators=[('xgb', clf_xgb),                                       \n                                      ('rf', clf_rf),\n                                      ('lg', clf_lg), \n                                      ('svc', clf_svc)], \n                          voting='hard',\n                          weights=[4,4,1,1])\nclf_vc = clf_vc.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a7ac42382c94ee7cb409c8c646bef3f4e00fc973"},"cell_type":"code","source":"clf_xgb = clf_xgb.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"13e4ef160d26764f45a1291873c858e24ce3c017"},"cell_type":"code","source":"print('Final Model Accuracy: %.5f'%(accuracy_score(y_test, clf_xgb.predict(x_test))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"943276116ec03bdf2f5f9c96213ea13e4bb60926"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}