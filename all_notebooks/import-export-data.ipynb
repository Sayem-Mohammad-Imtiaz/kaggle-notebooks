{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Indian Trade Data regression\nBy : Hesham Asem\n\n____\n\nso we have here 2 data files , for import & export for a specific indian company , between 2010 & 2018\n\nlets use them to build a regression model , so we can expect the value using other features . \n\ndata file : \nhttps://www.kaggle.com/lakshyaag/india-trade-data#2018-2010_export.csv\n\n_____\n\nfirst to import the libraries \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then to read the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"idata = pd.read_csv('../input/india-trade-data/2018-2010_import.csv') \nedata = pd.read_csv('../input/india-trade-data/2018-2010_export.csv') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"since we have 2 files , we'll need to handle them separately "},{"metadata":{},"cell_type":"markdown","source":"____\n\n# Needed Functions . \n\nas usual , we'll build here important funcstion which will be helpful in data processing . \n\nand you can see that some functions have the arg (data) , since we'll have to repeat our steps on idata & edata"},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_counts( feature , number, data, return_rest = False ) : \n    counts = data[feature].value_counts()\n    values_list = list(counts[:number].values)\n    rest_value =  sum(counts.values) - sum (values_list)\n    index_list = list(counts[:number].index)\n    \n    if return_rest : \n        values_list.append(rest_value )\n        index_list.append('rest items')\n    \n    result = pd.Series(values_list, index=index_list)\n    if len(data[feature]) <= number : \n        result = None\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def series_pie(series) : \n    plt.pie(series.values,labels=list(series.index),autopct ='%1.2f%%',labeldistance = 1.1,explode = [0.05 for i in range(len(series.values))] )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def series_bar(series) : \n    plt.bar(list(series.index),series.values )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_label_encoder(original_feature , new_feature,data) : \n    enc  = LabelEncoder()\n    enc.fit(data[original_feature])\n    data[new_feature] = enc.transform(data[original_feature])\n    data.drop([original_feature],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\n# Import Data\n\nlet's have a look to import data"},{"metadata":{"trusted":true},"cell_type":"code","source":"idata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"few features , what is the dimension ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"idata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we'll need to start in data processing"},{"metadata":{},"cell_type":"markdown","source":"_____\n\n# Data Processing\n"},{"metadata":{},"cell_type":"markdown","source":"let's start with HSCode feature , how many unique values in it"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(idata['HSCode'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's have a look to top values for it"},{"metadata":{"trusted":true},"cell_type":"code","source":"idata['HSCode'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"______\n\nso it looks that this feature is very correlated to the other feature Commodity . \n\nso how many are them "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(idata['HSCode'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"& how many unqiue values for Commodity ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(idata['Commodity'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cool , lets get the Commodity data , for only specific HSCode value"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = idata[idata['HSCode']==5]['Commodity']\nnew_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"what are the unique values for it ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and to be more sure , we'll mak for block for HSCode values , to get unique values for Commodity data "},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in range(idata['HSCode'].max()): \n    new_data = idata[idata['HSCode']==x]['Commodity']\n    print(len(new_data.unique()))\n    print('---------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so it's clear that those two features are very correlated , so we can drop one of them , let's drop commodity"},{"metadata":{"trusted":true},"cell_type":"code","source":"idata.drop(['Commodity'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\nhow data looks like now ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"idata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's work with country feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"idata['country'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how many country we have here "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(idata['country'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"what are the most repeated countries "},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = idata['country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we'll need to get the max countries to graph them , let's say we'll get max 8 countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_countries = max_counts('country',8,idata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_countries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\nlet's pie chart it"},{"metadata":{"trusted":true},"cell_type":"code","source":"series_pie(max_countries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also we can bar them "},{"metadata":{"trusted":true},"cell_type":"code","source":"series_bar(max_countries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we can apply labelencoder to them"},{"metadata":{"trusted":true},"cell_type":"code","source":"make_label_encoder('country','country_code',idata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\nnow how data looks like "},{"metadata":{"trusted":true},"cell_type":"code","source":"idata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we ned to check if we have any nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"idata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so value feature got more than 14K null value , & we cannot fill them with mean or median , since this is the output , so it will mislead the training , we'll have to drop all these rows with any value less than or equal 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data  = idata[idata['value'] > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how fata looks like "},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cool , now we ready for training\n\n________\n\n"},{"metadata":{},"cell_type":"markdown","source":"\n_____\n\n\n\n# Building the Model\n\n\nlet's first define X & y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_data.drop(['value'], axis=1, inplace=False)\ny = new_data['value']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"what are the dimensions ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"split it"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_____\n\nlet's  use Gradient Boosting Regressor , with 1000 estimators & 10 depth & 0.1 learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"GBRModel = GradientBoostingRegressor(n_estimators=1000,max_depth=10,learning_rate = 0.1 ,random_state=33)\nGBRModel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how is accuracy ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GBRModel Train Score is : ' , GBRModel.score(X_train, y_train))\nprint('GBRModel Test Score is : ' , GBRModel.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great work , & even we avoided OF , since test accuracy is high , now let's make prediction "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = GBRModel.predict(X_test)\nprint('Predicted Value for GBRModel is : ' , y_pred[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"______\n\n______\n\n# Export Data\n\n\nnow , we'll have to repeat almost all steps to export data , instead of import data"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(edata['HSCode'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edata['HSCode'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(edata['HSCode'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edata.drop(['Commodity'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edata['country'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(edata['country'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = edata['country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(counts[:10].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_countries = max_counts('country',8,edata)\nmax_countries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_pie(max_countries)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_bar(max_countries)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_label_encoder('country','country_code',edata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data  = edata[edata['value'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_data.drop(['value'], axis=1, inplace=False)\ny = new_data['value']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBRModel = GradientBoostingRegressor(n_estimators=1000,max_depth=10,learning_rate = 0.1 ,random_state=33)\nGBRModel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GBRModel Train Score is : ' , GBRModel.score(X_train, y_train))\nprint('GBRModel Test Score is : ' , GBRModel.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = GBRModel.predict(X_test)\nprint('Predicted Value for GBRModel is : ' , y_pred[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}