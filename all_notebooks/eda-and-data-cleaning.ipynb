{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div align=\"center\"><font size=6 color=\"red\">Spam </font><font size=6 color=\"green\"> & Ham</font><font size=6 color=\"black\"> Detection</font></div>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\n#impore re for regular expression\nimport re\n\n# import punctuations\nfrom string import punctuation\n\n#importing stopwords\nfrom nltk.corpus import stopwords\n\n#import the tokenizer\nfrom nltk.tokenize import RegexpTokenizer\n\n#importing the Stemmer\nfrom nltk.stem import PorterStemmer,SnowballStemmer\n\n#importing the Lemmatizer\nfrom nltk.stem import WordNetLemmatizer\n\n# import wordcloud for text visualization\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import chardet\nwith open(\"../input/sms-spam-collection-dataset/spam.csv\" , \"rb\") as f:\n    enc = chardet.detect(f.read(1000000))\n    \nprint(enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\" , encoding=\"Windows-1252\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of Data: {data.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the colums with lot's of missing values","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# P.S: The display keyword only works in the jupyter notebook enviroment and would raise an error in an actual python file\ndisplay(data[data[\"Unnamed: 2\"].notnull()].head())\ndisplay(data[data[\"Unnamed: 3\"].notnull()].head())\ndisplay(data[data[\"Unnamed: 4\"].notnull()].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text in the \"v2\" column and the other colums are all of the same text, this could have been caused by commas(,) in the text since we're reading as csv(comma seperated values), we can just add all the columns back together to make a single text","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#for every line where feature \"Unnamed: 2\" is not_null, we add Feature \"Unnamed: 2\" to \"Text\" feature\ndata[\"v2\"][data[\"Unnamed: 2\"].notna()] = data[\"v2\"][data[\"Unnamed: 2\"].notna()] + data[\"Unnamed: 2\"][data[\"Unnamed: 2\"].notna()]\n#for every line where feature \"Unnamed: 3\" is not_null, we add Feature \"Unnamed: 3\" to \"Text\" feature\ndata[\"v2\"][data[\"Unnamed: 3\"].notna()] = data[\"v2\"][data[\"Unnamed: 3\"].notna()] + data[\"Unnamed: 3\"][data[\"Unnamed: 3\"].notna()]\n#for every line where feature \"Unnamed: 4\" is not_null, we add Feature \"Unnamed: 4\" to \"Text\" feature\ndata[\"v2\"][data[\"Unnamed: 4\"].notna()] = data[\"v2\"][data[\"Unnamed: 4\"].notna()] + data[\"Unnamed: 4\"][data[\"Unnamed: 4\"].notna()]\n#drop feature \"A\",\"B\", and \"C\"\ndata.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"] , axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.columns = [\"label\",\"text\"]#rename columns\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The problem of missing value has been solved with that","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data[\"label\"].value_counts().plot(kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a very HUGE imbalance in our dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Let's take a look at the distribution in our Dataset","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"fig , ax = plt.subplots(nrows=6 , ncols=1,figsize=(10,50))\ndef word_length_count(text):\n    return len(text.split())\ndata[\"word_length\"] = data[\"text\"].apply(word_length_count)\nsns.kdeplot(data[\"word_length\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[0])\nsns.kdeplot(data[\"word_length\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" , ax=ax[0])\nax[0].set_title(\"Distribution of Words Count\", fontsize=14)\nax[0].set_xlabel(\"Number of Words in Text\" , fontsize=14)\nax[0].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef char_length_count(text):\n    return len(text)\ndata[\"characters_count\"] = data[\"text\"].apply(char_length_count)\nsns.kdeplot(data[\"characters_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[1])\nsns.kdeplot(data[\"characters_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" , ax=ax[1])\nax[1].set_title(\"Distribution of Characters Count\", fontsize=14)\nax[1].set_xlabel(\"Number of Characters in Text\" , fontsize=14)\nax[1].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef punct_count(text):\n    return len([char for char in text if char in punctuation])\ndata[\"punctuation_count\"] = data[\"text\"].apply(punct_count)\nsns.kdeplot(data[\"punctuation_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[2])\nsns.kdeplot(data[\"punctuation_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" ,ax = ax[2])\nax[2].set_title(\"Distribution of Punctuations Count\", fontsize=14)\nax[2].set_xlabel(\"Number of Punctuations in Text\" , fontsize=14)\nax[2].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef numbers_count(text):\n    return len([char for char in text if char.isnumeric()])\ndata[\"numbers_count\"] = data[\"text\"].apply(numbers_count)\nsns.kdeplot(data[\"numbers_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[3])\nsns.kdeplot(data[\"numbers_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\",ax = ax[3])\nax[3].set_title(\"Distribution of Numerical Characters Count\", fontsize=14)\nax[3].set_xlabel(\"Number of Numerical String in Text\" , fontsize=14)\nax[3].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef stopwords_counts(text):\n    return len([word for word in text.split() if word in stopwords.words(\"english\")])\ndata[\"stopwords_count\"] = data[\"text\"].apply(stopwords_counts)\nsns.kdeplot(data[\"stopwords_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[4])\nsns.kdeplot(data[\"stopwords_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" , ax = ax[4])\nax[4].set_title(\"Distribution of Stopwords Count\", fontsize=14)\nax[4].set_xlabel(\"Number of Stopwords in Text\" , fontsize=14)\nax[4].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef non_stopwords_counts(text):\n    return len([word for word in text.split() if word not in stopwords.words(\"english\")])\ndata[\"non_stopwords_count\"] = data[\"text\"].apply(non_stopwords_counts)\nsns.kdeplot(data[\"non_stopwords_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[5])\nsns.kdeplot(data[\"non_stopwords_count\"][data[\"label\"] == \"spam\"] ,  shade=True , label=\"spam\" , ax = ax[5])\nax[5].set_title(\"Distibution of Non-Stopwords Count\", fontsize=14)\nax[5].set_xlabel(\"Number of Non-Stopwords in Text\" , fontsize=14)\nax[5].set_ylabel(\"Frequency Rate\" , fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OBSERVATIONS:**\n1. Spam Text has alot of numbers in them while Ham Text has comaparably low numbers in them, this is very logical\n2. Spam Text has an average of more words than Ham Text, this is also very logical since scammers with try to convince you and convey extra amount of information to intice unsuspecting people","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"for _ in data[\"text\"]:\n    print(_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#change text to lowercase\ndata[\"text\"] = data[\"text\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's check is our text containg web links","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"display(data[data[\"text\"].str.contains(\"http\")].head())\ndisplay(data[data[\"text\"].str.contains(\"www\")].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look well, we'll see there are some datapoints with \"https\" | \"http\" and some with \"www\", we'll have to deal with this","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# write a function to remove web links\ndef remove_html(text):\n    text = re.sub(r\"http://.+\\.com\" , \"\" , text)\n    text = re.sub(r\"http://\\S+\" , \"\" , text)\n    text = re.sub(r\"http://[A-Za-z0-9./?= *&:-]+\" , \"\" , text)\n    text = re.sub(r\"http.+\\.com\" , \"\" , text)\n    text = re.sub(r\"http//[A-Za-z0-9./?= *&:-]+\" , \"\" ,text)\n    text = re.sub(r\"www[.A-Za-z0-9/+-]+\" , \"\" , text)\n    return text\ndata[\"text\"] = data[\"text\"].apply(remove_html)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Remove numbers and words which are joined with numbers such that the number removal makes them useless\ndef remove_numbers(text):\n    text = re.sub(r\"[å£A-Za-z0-9]*\\d+[A-Za-z0-9]*\" , \"\" , text)\n    return text\ndata[\"text\"] = data[\"text\"].apply(remove_numbers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are alot of instances with text like \"å£\". For example:\n\"winner!! as a valued network customer you have been selected to receivea å£900 prize reward! to claim call. claim code. valid hours only\"\n\n**Let's deal with that**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# to view some datapoints with charcters such as \"å\"\ndata[data[\"text\"].str.contains(\"å\")].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_fancy_text(word):\n    if word.isascii() == False:\n        return \"\"\n    else:\n        return word\ndata[\"text\"] = data[\"text\"].apply(lambda x: \" \".join([remove_fancy_text(word) for word in x.split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for _ in data[\"text\"]:\n    print(_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's get rid of email addresses","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data[data[\"text\"].str.contains(\"@\")].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_mails(text):\n    text = re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+(\\.[a-zA-Z]{2,5})\" , \"\" , text)\n    return text\ndata[\"text\"] = data[\"text\"].apply(remove_mails)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# view amils with \"@\" in them \ndata[data[\"text\"].str.contains(\"@\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After cleaning, we still have some text with \"@\" in them, we'll deal with this in punctattion removel","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Tokenization","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S=+')\ndata[\"text\"] = data[\"text\"].apply(lambda x : tokenizer.tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stopwords Removal","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_stopwords(text):\n    return [word for word in text if word not in stopwords.words(\"english\")]\ndata[\"text\"] = data[\"text\"].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove Punctautions","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_puncts(text):\n    return [word for word in text if word not in punctuation]\ndata[\"text\"] = data[\"text\"].apply(remove_puncts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data[\"text_length\"] = data[\"text\"].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"text_length\" column should be less than or equal to \"non_stopwords_count\" and also similiar to it and if not so it should should only be greater than \"non_stopwords_counts\" by 1","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data[data[\"text_length\"] > data[\"non_stopwords_count\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove standalone text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"removing standalone words like \"k\",\"a\",\"m\" and the likes","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data[\"text\"] = data[\"text\"].apply(lambda x: [word for word in x if len(word) >= 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"lemma = WordNetLemmatizer()\ndata[\"text\"] = data[\"text\"].apply(lambda x: \" \".join([lemma.lemmatize(word) for word in x]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stemming","execution_count":null},{"metadata":{},"cell_type":"raw","source":"Not intrested in stemming but the cell below will do stemming if uncommented","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# stemmer = SnowballStemmer(language=\"english\")\n# data[\"text\"].apply(lambda x: \" \".join([stemmer.stem(word) for word in x]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"spam_corpus = \"\"\nfor text in data[data[\"label\"] == \"spam\"][\"text\"]:\n    spam_corpus += \" \" + text\nham_corpus = \"\"\nfor text in data[data[\"label\"] == \"ham\"][\"text\"]:\n    ham_corpus += \" \" + text","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"spam_wordcloud = WordCloud(background_color=\"white\").generate(spam_corpus)\nham_wordcloud = WordCloud(background_color=\"white\").generate(ham_corpus)\nfig,ax= plt.subplots(nrows=1,ncols=2,figsize=(20,100))\nax[0].imshow(spam_wordcloud)\nax[0].set_title(\"Spam Text\" , fontsize=20)\nax[0].axis(False)\nax[0].grid(False)\nplt.savefig(\"Spam Text\")\nax[1].imshow(ham_wordcloud)\nax[1].set_title(\"Ham Text\" , fontsize=20)\nax[1].axis(False)\nax[1].grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}