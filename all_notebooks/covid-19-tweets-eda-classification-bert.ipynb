{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction<a name=\"introduction\"></a>\n### With this notebook, we will attempt to find the best model to classify these tweets. We'll start with some data cleaning and some vizualizations to get a little more familiar with the data, and from there we'll explore some more traditional classification methods.  After that, we'll train BERT on the tweets and compare the results with other models. "},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install chart_studio","execution_count":null,"outputs":[]},{"metadata":{"id":"w1PmDg0caQZU","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re \nimport string\nimport requests\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom wordcloud import WordCloud\nimport chart_studio.plotly as py\nimport plotly.graph_objects as go\nfrom plotly.offline import download_plotlyjs, plot, iplot\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport os\nfrom os import path","execution_count":null,"outputs":[]},{"metadata":{"id":"FenhG8YTaQZd","outputId":"650e8d01-e444-42c7-a8d9-f3c3f207608a","trusted":true,"collapsed":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"Awl5ToXsaQZo","outputId":"9c4c4900-2053-4643-b0be-c0631dfc6855","trusted":true,"collapsed":true},"cell_type":"code","source":"import nltk\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"id":"bZLeYfvvappH","outputId":"37ce9fbd-0a2e-4558-9352-9e08b0ba2c27","trusted":true},"cell_type":"code","source":"data_dict = {}\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data_dict[filename] = os.path.join(dirname,filename)\n        print(os.path.join(dirname, filename))\nprint(data_dict)","execution_count":null,"outputs":[]},{"metadata":{"id":"jv-Bvu1XaQZu","outputId":"7cdd1559-9912-453f-8e04-a4e3e9b182f7","trusted":true},"cell_type":"code","source":"# Load in training data\ntrain = pd.read_csv(data_dict['Corona_NLP_train.csv'], encoding = 'latin1')\n# Copy training data\ndf = train.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"wTbPtILGaQZ1","outputId":"5513cc5a-9914-452e-a98e-4a17dd8cabd9","trusted":true},"cell_type":"code","source":"# Load in test data\ntest_df = pd.read_csv(data_dict['Corona_NLP_test.csv'], encoding = 'latin1')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"BZvL6hzdaQZ7"},"cell_type":"markdown","source":"# 2. Data Cleaning <a name=\"data_cleaning\"></a>"},{"metadata":{"id":"wSKBPJu1aQZ8","outputId":"47b90944-338c-4d44-b591-16adb71de9dc","trusted":true},"cell_type":"code","source":"# Check for nulls\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"W0Swnf6CaQaB","outputId":"bf9ba9ae-26d4-4881-c741-ffe3a452c218","trusted":true},"cell_type":"code","source":"# Replace na with 'None'\ndf['Location'].fillna('None', inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"ee6gxsE-aQaG","trusted":true},"cell_type":"code","source":"# Join stopwords together and set them for use in cleaning function.\n\", \".join(stopwords.words('english'))\nstops = set(stopwords.words('english'))\n\n# Function that cleans tweets for classification. \ndef clean_tweet(tweet):\n    # Remove hyperlinks.\n    tweet= re.sub(r'https?://\\S+|www\\.\\S+','',tweet)\n    # Remove html\n    tweet = re.sub(r'<.*?>','',tweet)\n    # Remove numbers (Do we want to remove numbers? Death toll?)\n    tweet = re.sub(r'\\d+','',tweet)\n    # Remove mentions\n    tweet = re.sub(r'@\\w+','',tweet)\n    # Remove punctuation\n    tweet = re.sub(r'[^\\w\\s\\d]','',tweet)\n    # Remove whitespace\n    tweet = re.sub(r'\\s+',' ',tweet).strip()\n    # Remove stopwords\n    tweet = \" \".join([word for word in str(tweet).split() if word not in stops])\n    \n    return tweet.lower()","execution_count":null,"outputs":[]},{"metadata":{"id":"EAoloGXKaQaK","outputId":"66110840-03d7-494b-e8b1-91aa1e5ae6e3","trusted":true},"cell_type":"code","source":"# Check function\nexample2 = df['OriginalTweet'][1]\nclean_tweet(example2)","execution_count":null,"outputs":[]},{"metadata":{"id":"h2dAhrlMaQaO","outputId":"054f4a37-39bf-4f21-adac-57ce9d406ebe","trusted":true},"cell_type":"code","source":"# Apply text cleaning function to training and test dataframes.\ndf['newTweet'] = df['OriginalTweet'].apply(lambda x: clean_tweet(x))\ntest_df['newTweet'] = test_df['OriginalTweet'].apply(lambda x: clean_tweet(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here, we'll define a couple functions to either stem or lemmatize the tweets. These methods will be compared during classification to see which one gives us the model with the greatest accuracy. "},{"metadata":{"id":"8R548iBgaQaU","trusted":true},"cell_type":"code","source":"def token_stem(tweet):\n    tk = TweetTokenizer()\n    stemmer = PorterStemmer()\n    tweet = tk.tokenize(tweet)\n    tweet = [stemmer.stem(word) for word in tweet]\n    tweet =  tweet = \" \".join([word for word in tweet])\n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"id":"9WsA1achaQaY","trusted":true},"cell_type":"code","source":"def token_lemma(tweet):\n    tk = TweetTokenizer()\n    lemma = WordNetLemmatizer()\n    tweet = tk.tokenize(tweet)\n    tweet = [lemma.lemmatize(word) for word in tweet]\n    tweet = \" \".join([word for word in tweet])\n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"id":"61Byipz6aQad","outputId":"5b12afcd-ba5a-4804-ca69-f8b10f1433c5","trusted":true},"cell_type":"code","source":"tweet = df['newTweet'][1]\ntweet","execution_count":null,"outputs":[]},{"metadata":{"id":"UGow3_nraQai","outputId":"33088bc2-cb19-401b-9ed5-7246c073aa12","trusted":true},"cell_type":"code","source":"print(token_stem(tweet))\nprint('\\n')\nprint(token_lemma(tweet))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### See the differences in these techniques? Stemming converts words to their 'stems', while lemmatizing brings the words to their 'lemmas', or dictionary forms. "},{"metadata":{"id":"qouCXKncaQam","outputId":"a1c8d3d5-ae3a-499f-b721-c7ff693526b3","trusted":true},"cell_type":"code","source":"df['stemTweet'] = df['newTweet'].apply(lambda x: token_stem(x))\ndf['lemmaTweet'] = df['newTweet'].apply(lambda x: token_lemma(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"axLuYS8QaQar","trusted":true},"cell_type":"code","source":"# Create more useful labels for classification.\n# We will take the original 5 possibilites and\n# reduce them to 3, removing the \"extremelys\".\ndef make_label(sentiment):\n    \n    label = ''\n    if 'Positive' in sentiment: \n        label = 1\n    if 'Negative' in sentiment:\n        label = -1\n    if 'Neutral' in sentiment:\n        label = 0\n    return label","execution_count":null,"outputs":[]},{"metadata":{"id":"Y9hCBNCpaQau","outputId":"51d0f7ae-dd0f-4473-ed0d-e4ef52d6182a","trusted":true},"cell_type":"code","source":"# Apply make_label funtion to training and test dataframes.\ndf['label'] = df['Sentiment'].apply(lambda x: make_label(x))\ntest_df['label'] = test_df['Sentiment'].apply(lambda x: make_label(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below are some of the common locations found in the tweets that will help us properly map more tweets to a particular country."},{"metadata":{"id":"_wyPR2NYaQay","trusted":true},"cell_type":"code","source":"# Some frequent US locations\nus_filters = ('New York', 'New York, NY', 'NYC', 'NY', 'Washington, DC', 'Los Angeles, CA',\n             'Seattle, Washington', 'Chicago', 'Chicago, IL', 'California, USA', 'Atlanta, GA',\n             'San Francisco, CA', 'Boston, MA', 'New York, USA', 'Texas, USA', 'Austin, TX',\n              'Houston, TX', 'New York City', 'Philadelphia, PA', 'Florida, USA', 'Seattle, WA',\n             'Washington, D.C.', 'San Diego, CA', 'Las Vegas, NV', 'Dallas, TX', 'Denver, CO',\n             'New Jersey, USA', 'Brooklyn, NY', 'California', 'Michigan, USA', 'Minneapolis, MN',\n             'Virginia, USA', 'Miami, FL', 'Texas', 'Los Angeles', 'United States', 'San Francisco',\n             'Indianapolis, IN', 'Pennsylvania, USA', 'Phoenix, AZ', 'New Jersey', 'Baltimore, MD',\n             'CA', 'FL', 'DC', 'TX', 'IL', 'MA', 'PA', 'GA', 'NC', 'NJ', 'WA', 'VA', 'PAK', 'MI', 'OH',\n             'CO', 'AZ', 'D.C.', 'WI', 'MD', 'MO', 'TN', 'Florida', 'IN', 'NV', 'MN', 'OR','LA', 'Michigan',\n             'CT', 'SC', 'OK', 'Illinois', 'Ohio', 'UT', 'KY', 'Arizona', 'Colorado')\n\n# Various nation's frequent locations\nuk_filters = ('England', 'London', 'london', 'United Kingdom', 'united kingdom',\n              'England, United Kingdom', 'London, UK', 'London, England',\n              'Manchester, England', 'Scotland, UK', 'Scotland', 'Scotland, United Kingdom',\n              'Birmingham, England', 'UK', 'Wales')\nindia_filters = ('New Delhi, India', 'Mumbai', 'Mumbai, India', 'New Delhi', 'India', \n                 'Bengaluru, India')\naustralia_filters = ('Sydney, Australia', 'New South Wales', 'Melbourne, Australia', 'Sydney',\n                     'Sydney, New South Wales', 'Melbourne, Victoria', 'Melbourne', 'Australia')\ncanada_filters = ('Toronto, Ontario', 'Toronto', 'Ontario, Canada', 'Toronto, Canada', 'Canada',\n                  'Vancouver, British Columbia', 'Ontario', 'Victoria', 'British Columbia', 'Alberta',)\nsouth_africa_filters = ('Johannesburg, South Africa', 'Cape Town, South Africa', 'South Africa')\nnigeria_filters = ('Lagos, Nigeria')\nkenya_filters = ('Nairobi, Kenya')\nfrance_filters = ('Paris, France')\nireland_filters = ('Ireland')\nnew_zealand_filters = ('New Zealand')\npakistan_filters = ('Pakistan')\nmalaysia_filters = ('Malaysia')\nuganda_filters = ('Kampala, Uganda', 'Uganda')\nsingapore_filters = ('Singapore')\ngermany_filters = ('Germany', 'Deutschland')\nswitz_filters = ('Switzerland')\nuae_filters = ('United Arab Emirates', 'Dubai')\nspain_filters = ('Spain')\nbelg_filters = ('Belgium')\nphil_filters = ('Philippines')\nhk_filters = ('Hong Kong')\nghana_filters = ('Ghana')\n# These all have large counts. Need to be removed from rest of data\nother_filters = ('None', 'Worldwide', 'Global', 'Earth', '??')","execution_count":null,"outputs":[]},{"metadata":{"id":"0ulJ3GFraQa1","trusted":true},"cell_type":"code","source":"df['country'] = df['Location'].apply(lambda x: x.split(\",\")[-1].strip() if (\",\" in x) else x)","execution_count":null,"outputs":[]},{"metadata":{"id":"8-d0d338aQa5","outputId":"1ab9eb50-be1b-4d19-b79b-8ea1c5e638cb","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"A7LDuoTEaQa8","trusted":true},"cell_type":"code","source":"# Changing strings found with filters into 3 digit codes\ndf['country'] = df['country'].apply(lambda x: 'USA' if x in us_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'GBR' if x in uk_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'IND' if x in india_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'AUS' if x in australia_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'CAN' if x in canada_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'ZAF' if x in south_africa_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'KEN' if x in kenya_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'NGA' if x in nigeria_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'SGP' if x in singapore_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'FRA' if x in france_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'NZL' if x in new_zealand_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'PAK' if x in pakistan_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'MYS' if x in malaysia_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'IRL' if x in ireland_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'UGA' if x in uganda_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'DEU' if x in germany_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'CHE' if x in switz_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'ARE' if x in uae_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'ESP' if x in spain_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'BEL' if x in belg_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'PHL' if x in phil_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'GHA' if x in ghana_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'HKG' if x in hk_filters else x)\ndf['country'] = df['country'].apply(lambda x: 'None' if x in other_filters else x)","execution_count":null,"outputs":[]},{"metadata":{"id":"bPxmj94xaQbC","outputId":"42848ee7-96cd-4856-a1b0-e713535466d4","trusted":true},"cell_type":"code","source":"df['country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Vizualizations <a name=\"viz\"></a>"},{"metadata":{"id":"AjMdLWmqaQbF","trusted":true},"cell_type":"code","source":"# 0:30 because that's where the labeled countries end\nplaces_df = pd.DataFrame(df['country'].value_counts()[0:30])\nplaces_df.reset_index(inplace = True)\nplaces_df.rename(columns = {'index':'Country', 'country':'Tweets'}, inplace = True)\n# Remove 'None' location\nplaces_df = places_df[places_df['Country'] != 'None']","execution_count":null,"outputs":[]},{"metadata":{"id":"fxtHZ7cGaQbJ","trusted":true},"cell_type":"code","source":"data = dict(type='choropleth',\n            colorscale = 'inferno',\n            locations = places_df['Country'],\n            z = places_df['Tweets'],\n            #locationmode = 'USA-states',\n            text = places_df['Tweets'],\n            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),\n            colorbar = {'title':\"Number of Tweets\"}\n            ) \n\nlayout = dict(title = 'Number of Tweets By Country',\n              geo = dict(#scope='usa',\n                         showlakes = False,\n                         lakecolor = 'rgb(85,173,240)',\n                         projection_type='equirectangular')\n             )\n\nchoromap = go.Figure(data = [data],layout = layout)","execution_count":null,"outputs":[]},{"metadata":{"id":"0zOaY7DhaQbL","outputId":"ef704a37-91ba-49ed-ffad-e2eb03cde4ee","trusted":true},"cell_type":"code","source":"iplot(choromap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The vast majority of tweets come from English speaking countries, which makes sense, since these tweets are all in English. Largest contributer is the USA followed by the UK and Canada.  \n"},{"metadata":{"id":"e9ipqnZsaQbO","outputId":"dbf70a36-e9dd-4e40-a38d-ab27eeead17b","trusted":true},"cell_type":"code","source":"# image courtesy of https://tse2.mm.bing.net/th?id=OIP.VLv_PpEOc8TDwuTNvj5hWQHaHa&pid=Api\n#img = Image.open(data_dict['rona4.jpeg'])\nmask = np.array(Image.open(data_dict['rona4.jpeg']))\n\n# Positive WordCloud\npos_df = df[df['label'] == 1]\npos_text = pos_df['newTweet'].to_string(index = False)\npos_text = re.sub(r'\\n','',pos_text)\npos_cloud = WordCloud(colormap = 'Greens', mask = mask).generate(pos_text)\n\n# Neutral WordCloud\nneut_df = df[df['label'] == 0]\nneut_text = neut_df['newTweet'].to_string(index = False)\nneut_text = re.sub(r'\\n','', neut_text)\nneut_cloud = WordCloud(colormap = 'Blues', mask = mask).generate(neut_text)\n\n# Negative wordcloud\nneg_df = df[df['label'] == -1]\nneg_text = neg_df['newTweet'].to_string(index = False)\nneg_text = re.sub(r'\\n','', neg_text)\nneg_cloud = WordCloud(colormap = 'Reds', mask = mask).generate(neg_text)","execution_count":null,"outputs":[]},{"metadata":{"id":"7SNoTZKBaQbS","outputId":"fce0ab8e-6ce1-40e0-ca1b-a0dce16283ac","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = [30,20])\nax1.imshow(pos_cloud)\nax1.set_title('Positive Cloud', fontsize = 30)\nax1.axis('off')\nax2.imshow(neut_cloud)\nax2.set_title('Neutral Cloud', fontsize = 30)\nax2.axis('off')\nax3.imshow(neg_cloud)\nax3.set_title('Negative Cloud', fontsize = 30)\nax3.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"id":"BOM1mAUnaQbW"},"cell_type":"markdown","source":"### Tried to use an image of the coronavirus for the mask, it certainly could have turned out better...\n### 'Grocery store', 'price', 'supermarket', and 'online shopping' being frequent in positive, neutral, and negative tweets is interesting.  Some stand-out negative terms are 'panic buying' and 'toilet paper'. For positive, 'hand sanitizer' catches my attention. "},{"metadata":{"id":"Ca2164oKaQbW","trusted":true},"cell_type":"code","source":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"Gs3mSg2PaQbZ","trusted":true},"cell_type":"code","source":"unigram_df = ngram_df(df['newTweet'],(1,1),20)\nbigram_df = ngram_df(df['newTweet'],(2,2),20)\ntrigram_df = ngram_df(df['newTweet'],(3,3),20)","execution_count":null,"outputs":[]},{"metadata":{"id":"LmzloFDqaQbb","outputId":"725e00cc-4ffc-427e-d99f-8f5356c6ec73","trusted":true},"cell_type":"code","source":"unigram_df['text'][::-1]","execution_count":null,"outputs":[]},{"metadata":{"id":"xGPn83oUaQbf","outputId":"5d93a74b-20f4-4262-ae84-b9d731a8a91a","trusted":true},"cell_type":"code","source":"sns.set(font_scale = 1.3)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.barplot(data = unigram_df, y = 'text', x = 'count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 'Prices' being the most frequent unigram after covid/coronavirus may be due to rising food prices and other various shortages."},{"metadata":{"id":"NUrL8oCdaQbi","outputId":"f958d54d-327b-48a0-d881-2a3eed862617","trusted":true},"cell_type":"code","source":"sns.set(font_scale = 1.3)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.barplot(data = bigram_df, y = 'text', x = 'count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grocery store way outpacing covid bigrams is pretty interesting. Online shopping, hand sanitizer, toilet paper, and panic buying are all within the realm of expectation. "},{"metadata":{"id":"iOQ_GzrNaQbl","outputId":"c2652e4b-230b-4829-c10a-cb3e22e1099f","trusted":true},"cell_type":"code","source":"sns.set(font_scale = 1.3)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.barplot(data = trigram_df, y = 'text', x = 'count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grocery store dominates these trigrams. People may be concerned about the safety of grocery shopping during a pandemic, and the health of the grocery store workers. "},{"metadata":{"id":"2b0WAl_naQbo"},"cell_type":"markdown","source":"# 4. Classification <a name=\"classification\"></a>"},{"metadata":{"id":"Sw_FN142aQbs","trusted":true},"cell_type":"code","source":"# Set X and y.\nX = df['newTweet']\ny = df['label']\n\n# Split data into training and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We'll try 4 different classifiers here: SVC, Logisitic Regression, Naive Bayes, and Random Forest. Furthermore, we'll also be testing whether these models perform better using Term Frequency Inverse Document Frequency or just a simple Count for the vectors we feed into the model. TFIDF increases every time a word appears in a document(tweet), but is then offset for every document(tweet) that word appears. This can help pick out the more important words for classification. Additionally, we'll be using cross validation to help gauge each model's accuracy and variance across multiple splits of the data. "},{"metadata":{"id":"nue-W28paQbv","trusted":true},"cell_type":"code","source":"clf = dict({'SVC': LinearSVC(max_iter = 5000),\n            'Logisitc': LogisticRegression(max_iter = 5000),\n            'NaiveBayes': MultinomialNB(),\n            'RandomForest': RandomForestClassifier(),\n           })","execution_count":null,"outputs":[]},{"metadata":{"id":"fOkZbBWdaQby","trusted":true},"cell_type":"code","source":"def make_models(clf, vectorizer, X_train, y_train, cv = 5):\n    \n    acc_df = pd.DataFrame(index=range(cv * len(clf)))\n    results = []\n    for classifier in clf.keys():\n        model = Pipeline([('vectorizer',vectorizer),\n                   ('clf', clf[classifier])])\n        model.fit(X_train, y_train)\n        scores = cross_val_score(model, X_train , y_train, cv = cv)\n        model_name = classifier\n        for fold, score in enumerate(scores):\n            results.append((model_name, fold, score))\n    \n    acc_df = pd.DataFrame(results, columns=['model_name', 'fold', 'accuracy'])\n    \n    return acc_df","execution_count":null,"outputs":[]},{"metadata":{"id":"KW-812DpaQb0","trusted":true},"cell_type":"code","source":"# Number of folds for K-fold cross validation\ncv = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Takes a good bit to run (over 30 minutes)...10 fold cross validation on 4 separate classifiers will take a while.\n### Results are saved to 'pipe_results.csv' if you want to save time. \n### Skip down a few cells to see where I load the results if you don't want to run each model.\n### Logistic and RandomForest take much longer than SVC and NaiveBayes."},{"metadata":{"id":"enWEIUTAaQb5","outputId":"0b55be8f-5e90-48b3-d038-f11b3c895fca","trusted":true},"cell_type":"code","source":"tfidf_df = make_models(clf, TfidfVectorizer(), X_train, y_train, cv)\ncount_df = make_models(clf, CountVectorizer(), X_train, y_train, cv)\ntfidf_df['vectorizer'] = 'tfidf'\ncount_df['vectorizer'] = 'count'\ncombined_df = tfidf_df.append(count_df)\ncombined_df['method'] = 'none'\ncombined_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"bYHhXlAzaQb-","trusted":true},"cell_type":"code","source":"# Set X and y.\nX = df['stemTweet']\ny = df['label']\n\n# Split data into training and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nstem_tfidf_df = make_models(clf, TfidfVectorizer(), X_train, y_train, cv)\nstem_count_df = make_models(clf, CountVectorizer(), X_train, y_train, cv)\n\nstem_tfidf_df['method'] = 'stem'\nstem_tfidf_df['vectorizer'] = 'tfidf'\nstem_count_df['method'] = 'stem'\nstem_count_df['vectorizer'] = 'count'\nstem_df = stem_tfidf_df.append(stem_count_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"1D9pkC0jaQcB","trusted":true},"cell_type":"code","source":"# Set X and y.\nX = df['lemmaTweet']\ny = df['label']\n\n# Split data into training and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nlemma_tfidf_df = make_models(clf, TfidfVectorizer(), X_train, y_train, cv)\nlemma_count_df = make_models(clf, CountVectorizer(), X_train, y_train, cv)\n\nlemma_tfidf_df['vectorizer'] = 'tfidf'\nlemma_tfidf_df['method'] = 'lemma'\nlemma_count_df['vectorizer'] = 'count'\nlemma_count_df['method'] = 'lemma'\nlemma_df = lemma_tfidf_df.append(lemma_count_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"T84TklGWaQcD","trusted":true},"cell_type":"code","source":"all_df = lemma_df.append(stem_df)\nall_df = all_df.append(combined_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skip to here to avoid running the models\nall_df = pd.read_csv(data_dict['pipe_results.csv'])","execution_count":null,"outputs":[]},{"metadata":{"id":"WhDsw2neaQcL","trusted":true},"cell_type":"code","source":"sns.set(font_scale = 1.4)\nsns.catplot(x = 'model_name', y = 'accuracy', hue = 'method', height = 7,\n            data = all_df, kind = 'box', col = 'vectorizer', palette = 'rainbow')","execution_count":null,"outputs":[]},{"metadata":{"id":"Mq6rJToXaQcO"},"cell_type":"markdown","source":"### Naive Bayes and RandomForest do much worse than Logistic and SVC, and make the boxplots fairly hard to look at. Let's drop them for better visuals. "},{"metadata":{"id":"ybtZhohQaQcR","trusted":true},"cell_type":"code","source":"no_nb = all_df[all_df['model_name'] != 'NaiveBayes']\nno_nb_rf = no_nb[no_nb['model_name'] != 'RandomForest']\nsns.set(font_scale = 1.4)\nsns.catplot(x = 'model_name', y = 'accuracy', hue = 'method', height = 7,\n            data = no_nb_rf, kind = 'box', col = 'vectorizer', palette = 'rainbow')","execution_count":null,"outputs":[]},{"metadata":{"id":"ix82xJx9aQcT"},"cell_type":"markdown","source":"### SVC does better when using tfidf, and Logistic Regression does better when using count. Stemming seems to do worse than lemmatization accuracy wise, although lemmatization has more outliers. The best results tend to come from using neither lemmatization nor stemming on the tweets. \n### SVC using tfidf and Logistic with count have approximately the same median, but the SVC has less variance and a slightly more even distribution. \n### It should be noted that the differences in accuracies between the best performing models are very small, and are probably due to the random splits more than methodology.  Bearing that in mind, I would select the LinearSVC using tfidf and no lemma/stem because it takes MUCH less time to run than the logistic regression, and based on these results, it has less variance. "},{"metadata":{"id":"jJpHVPZPaQcX","trusted":true},"cell_type":"code","source":"accuracies = all_df.groupby(['model_name', 'method', 'vectorizer']).accuracy.mean()\nstdDev = all_df.groupby(['model_name', 'method', 'vectorizer']).accuracy.std()\nmetrics_df = pd.concat([accuracies, stdDev], axis = 1, ignore_index = True)\nmetrics_df.columns = ['mean_acc', 'mean_std']","execution_count":null,"outputs":[]},{"metadata":{"id":"JD4ahqVpaQcZ","trusted":true},"cell_type":"code","source":"metrics_df.sort_values(by = ['mean_acc','method'], ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"0LpIq8wxaQce"},"cell_type":"markdown","source":"## Again, this displays just how small the accuracy differences are between the best models. For the sake of efficiency, an SVC using tfidf vectors is recommended. Let's fit one and explore the results more thorouhgly. "},{"metadata":{"id":"VpFTeoxYaQcf","outputId":"5a2b8a26-25ab-4034-df58-87b24e57a80b","trusted":true},"cell_type":"code","source":"# Set X and y.\nX = df['newTweet']\ny = df['label']\n\n# Set vectorizer for feature extraction.\nvectorizer = TfidfVectorizer()\n\n# Split data into training and test sets to fit the model.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Define model for predictions\nmodel = Pipeline([('vectorizer',vectorizer),\n                  ('clf', LinearSVC(max_iter = 5000))])\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"KZdT4tsbaQcg","outputId":"e721ccc8-3f31-4840-ef99-a2c9341c14a4","trusted":true},"cell_type":"code","source":"train_preds = model.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test, train_preds))\nprint('\\n')\nprint(classification_report(y_test, train_preds))","execution_count":null,"outputs":[]},{"metadata":{"id":"o3FBwcZhaQck"},"cell_type":"markdown","source":"### ~80% accuracy on the training data, not too bad. Precision and recall are significantly lower for neutral tweets than positive or negative, possibly due to the lower support, but it could also be that neutral tweets are harder to classify. This model appears to be slightly better at predicting positive tweets than negative tweets. \n### Now, we'll see how the model performs on the test data."},{"metadata":{"id":"uXZ7gY-aaQck","outputId":"97fcdc89-38c1-464f-a818-f429b66f066c","trusted":true},"cell_type":"code","source":"# Set X and y.\nX2 = test_df['newTweet']\ny2 = test_df['label']\n\n\ntest_preds = model.predict(X2)\nprint('Accuracy:', accuracy_score(y2, test_preds))\nprint('\\n')\nprint(classification_report(y2, test_preds))","execution_count":null,"outputs":[]},{"metadata":{"id":"rNHko_mQaQcn"},"cell_type":"markdown","source":"### Model does a little bit worse on test data than on training data. Let's see if we can improve the accuracy by tuning some parameters"},{"metadata":{"id":"oBIZFSGzaQcn","outputId":"b252b5af-3bbb-4106-b7b4-7362f942d106","trusted":true},"cell_type":"code","source":"# Dictionary of parameters that can be tuned\nmodel.get_params()","execution_count":null,"outputs":[]},{"metadata":{"id":"BEGZGO-3aQc3","outputId":"b9537cbd-6022-42c9-837c-06842475f9c8","trusted":true},"cell_type":"code","source":"# GridSearchCV goes through specified parameter values and finds the best ones. \nfrom sklearn.model_selection import GridSearchCV\n\n# We'll try a few different options here.\nhyperparameters = { 'vectorizer__max_df': [1, 0.9, 0.95, .85],\n                    'vectorizer__ngram_range': [(1,1), (1,2), (2,2),(2,3)],\n                  }\nmodel_tune = GridSearchCV(model, hyperparameters, cv=5)\n\n# Fit and tune model\nmodel_tune.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"qYkTjGH_aQc5","outputId":"b5236826-038b-4e34-b4a0-423d2f71ae9a","trusted":true},"cell_type":"code","source":"# These are the best parameters according to the GridSearch\nmodel_tune.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"rbOUyWG7aQc7","outputId":"7be80411-ae4a-4133-fbac-5d0e8b072ad2","trusted":true},"cell_type":"code","source":"# Gridsearch will refit the model on the best settings\nmodel_tune.refit","execution_count":null,"outputs":[]},{"metadata":{"id":"kxKAUAPhaQc-","outputId":"86b49b35-15c0-4eb6-b1d1-33f552f8f112","trusted":true},"cell_type":"code","source":"preds = model_tune.predict(X2)\nprint('Accuracy:', accuracy_score(y2, preds))\nprint('\\n')\nprint(classification_report(y2, preds))","execution_count":null,"outputs":[]},{"metadata":{"id":"BeEjQ4HZaQc_"},"cell_type":"markdown","source":"### Looks like our tuning didn't improve accuracy at all.  Let's take a look at some of the mislabled tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['pred_label'] = test_preds\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mislabel_df = test_df[test_df['label'] != test_preds]\nmislabel_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"mVu_EgIRaQdC","trusted":true,"collapsed":true},"cell_type":"code","source":"mislabel_df['Location'].fillna('None', inplace = True)\nmislabel_df['country'] = mislabel_df['Location'].apply(lambda x: x.split(\",\")[-1].strip() if (\",\" in x) else x)\n\n# Changing strings found with filters into 3 digit codes\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'USA' if x in us_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'GBR' if x in uk_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'IND' if x in india_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'AUS' if x in australia_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'CAN' if x in canada_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'ZAF' if x in south_africa_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'KEN' if x in kenya_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'NGA' if x in nigeria_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'SGP' if x in singapore_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'FRA' if x in france_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'NZL' if x in new_zealand_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'PAK' if x in pakistan_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'MYS' if x in malaysia_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'IRL' if x in ireland_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'UGA' if x in uganda_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'DEU' if x in germany_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'CHE' if x in switz_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'ARE' if x in uae_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'ESP' if x in spain_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'BEL' if x in belg_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'PHL' if x in phil_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'GHA' if x in ghana_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'HKG' if x in hk_filters else x)\nmislabel_df['country'] = mislabel_df['country'].apply(lambda x: 'None' if x in other_filters else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mislabel_df['country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The mislabels mostly come from the USA, which is where the majority of the tweets are from anyway. Let's take a look at a few of the tweets themselves."},{"metadata":{"trusted":true},"cell_type":"code","source":"mislabel_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mislabel_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neutral tweet\nprint(mislabel_df['OriginalTweet'][7])\nprint('\\n')\nprint(mislabel_df['newTweet'][7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wonder how that mark above the 'A' affected the predicted label, and other marked words.  Words that may have contributed to the model predicting positive might be 'surgical' and 'healthworkers'. "},{"metadata":{"id":"-0BsoemKaQdG","trusted":true},"cell_type":"code","source":"# Negative tweet\nprint(mislabel_df['OriginalTweet'][15])\nprint('\\n')\nprint(mislabel_df['newTweet'][15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here the model predicted positive, and the 'Â' showed up again in a mislabel. The model may have seen 'free' and 'rights' and labeled it as positive, even though to a human reader, this is quite clearly a negative tweet."},{"metadata":{"id":"HdlxlPlOaQdI","trusted":true},"cell_type":"code","source":"# Positive tweet\nprint(mislabel_df['OriginalTweet'][3779])\nprint('\\n')\nprint(mislabel_df['newTweet'][3779])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The model predicted negative for this tweet, while the true label is positive. Lower prices is indeed a positive thing for consumers. Perhaps the model took 'stuck', 'coronavirus', and 'covid' to be more negative than the other words in the tweet."},{"metadata":{},"cell_type":"markdown","source":"### A possible update to improve accuracy of this model may involve handling the accented letters in a better way. \n### However, if we want better accuracy, we should try BERT. We'll fit a BERT model and see how well it does."},{"metadata":{},"cell_type":"markdown","source":"# 5. BERT <a name=\"bert\"></a>"},{"metadata":{"id":"aBmHpmxWaQdN","trusted":true,"collapsed":true},"cell_type":"code","source":"import torch\nfrom tqdm.notebook import tqdm\n\nfrom transformers import BertTokenizer\n\nfrom torch.utils.data import TensorDataset\n\nimport transformers\nfrom transformers import BertForSequenceClassification\n\n#import numpy as np\n#import pandas as pd\n#import re","execution_count":null,"outputs":[]},{"metadata":{"id":"-vgwf4ZWaQdP","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode the classes for BERT. We'll keep using the 3 labels we made earlier.  \nencoder = LabelEncoder()\ndf['encoded_sentiment'] = encoder.fit_transform(df['label'])","execution_count":null,"outputs":[]},{"metadata":{"id":"OFQdATPMaQdS","trusted":true},"cell_type":"code","source":"# Set X and y.\nX = df['newTweet']\ny = df['encoded_sentiment']\n\n# Split data into training and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"N2yQejgiaQdV","outputId":"2d2d9c51-24f4-416f-cd41-2fa099a64d06","trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"YWyYJCIZaQdb","outputId":"90d329ad-9362-439d-e146-40364609385a","trusted":true},"cell_type":"code","source":"# Encoding the words in the training data into vectors.\nencoded_data_train = tokenizer.batch_encode_plus(\n    X_train, \n    truncation = True,\n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=50, \n    return_tensors='pt'\n)\n\n# Encoding the words in the test data into vectors.\nencoded_data_test = tokenizer.batch_encode_plus(\n    X_test, \n    truncation = True,\n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=50, \n    return_tensors='pt'\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"bpTiXmaUaQde","trusted":true},"cell_type":"code","source":"# Get inputs and attention masks from previously encoded data. \ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(y_train.values)\n\ninput_ids_test = encoded_data_test['input_ids']\nattention_masks_test = encoded_data_test['attention_mask']\nlabels_test = torch.tensor(y_test.values)\n\n# Instantiate TensorDataset\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"nLTjXXz2aQdg","outputId":"075e05a8-e048-4061-f79f-865e358c8926","trusted":true},"cell_type":"code","source":"# Initialize the model. \nmodel = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=3,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"h8w2dJ8IaQdi","trusted":true},"cell_type":"code","source":"# DataLoaders for running the model\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=128)\n\ndataloader_test = DataLoader(dataset_test, \n                                   sampler=SequentialSampler(dataset_test), \n                                   batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"id":"hYoa1XM8aQdk","trusted":true},"cell_type":"code","source":"# Setting hyperparameters\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\noptimizer = AdamW(model.parameters(),\n                  lr=1e-5, \n                  eps=1e-8)\n                  \nepochs = 10\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*epochs)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"iU6Z8A03aQdn","trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{"id":"sjLpCzaPaQdo","trusted":true},"cell_type":"code","source":"import random\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"id":"Y95m4ApUaQdr","outputId":"3685cce0-c629-4102-d1a6-6544deef5e18","trusted":true},"cell_type":"code","source":"model.to(device)\n\nfor epoch in tqdm(range(1, epochs+1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0].to(device),\n                  'attention_mask': batch[1].to(device),\n                  'labels':         batch[2].to(device),\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7aCc9RlfaQdt","trusted":true},"cell_type":"code","source":"def evaluate(dataloader_test):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_test:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_test) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","execution_count":null,"outputs":[]},{"metadata":{"id":"TThGC7JPkfuS","trusted":true},"cell_type":"code","source":"val_loss, predictions, true_vals = evaluate(dataloader_test)\nval_f1 = f1_score_func(predictions, true_vals)","execution_count":null,"outputs":[]},{"metadata":{"id":"ePsKJm1kkrGY","outputId":"53339d35-25ad-445e-84bb-177d38251e02","trusted":true},"cell_type":"code","source":"print('Val Loss = ', val_loss)\nprint('Val F1 = ', val_f1)","execution_count":null,"outputs":[]},{"metadata":{"id":"1H8uGE0mks-P","trusted":true},"cell_type":"code","source":"encoded_classes = encoder.classes_\npredicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\ntrue_category = [encoded_classes[x] for x in true_vals]","execution_count":null,"outputs":[]},{"metadata":{"id":"p_q4Hg6tlILn","outputId":"fd554e89-d269-437f-bf50-e90edfa61c99","trusted":true},"cell_type":"code","source":"x = 0\nfor i in range(len(true_category)):\n    if true_category[i] == predicted_category[i]:\n        x += 1\n        \nprint('Accuracy Score = ', x / len(true_category))","execution_count":null,"outputs":[]},{"metadata":{"id":"8LrgPngEnnzP","outputId":"fcafcf6e-3abe-485b-dba0-18db5023411f","trusted":true},"cell_type":"code","source":"print(classification_report(true_category, predicted_category))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 87% accuracy is about 7% better than what we get using an SVC for training. "},{"metadata":{"id":"QJXD5vlPnr4F"},"cell_type":"markdown","source":"## Now, we'll use the test dataset to evaluate BERT."},{"metadata":{"id":"gSi-Q2edlPWy","outputId":"4e9067a3-34fb-4f00-98ec-5a435b38cf07","trusted":true},"cell_type":"code","source":"test_df['encoded_sentiment'] = encoder.fit_transform(test_df['label'])\n\n# Set X and y.\nX = test_df['newTweet']\ny = test_df['encoded_sentiment']\n\nencoded_data_test = tokenizer.batch_encode_plus(\n    X, \n    truncation = True,\n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=50, \n    return_tensors='pt'\n)\n\ninput_ids_test = encoded_data_test['input_ids']\nattention_masks_test = encoded_data_test['attention_mask']\nlabels_test = torch.tensor(y.values)\n\n# Pytorch TensorDataset Instance\ndataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n\ndataloader_test = DataLoader(dataset_test, \n                                   sampler=SequentialSampler(dataset_test), \n                                   batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"id":"3GLrztMPoFmO","trusted":true},"cell_type":"code","source":"val_loss, predictions, true_vals = evaluate(dataloader_test)\nval_f1 = f1_score_func(predictions, true_vals)","execution_count":null,"outputs":[]},{"metadata":{"id":"yca-88ISos7q","outputId":"753de962-6d2c-4430-f210-56dc00426f50","trusted":true},"cell_type":"code","source":"encoded_classes = encoder.classes_\npredicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\ntrue_category = [encoded_classes[x] for x in true_vals]\n\nx = 0\nfor i in range(len(true_category)):\n    if true_category[i] == predicted_category[i]:\n        x += 1\n        \nprint('Accuracy Score = ', x / len(true_category))\nprint('\\n')\nprint(classification_report(true_category, predicted_category))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### On the actual test data, the model scores an 85%, which is ~6% better than the LinearSVC performed on this data. This BERT model could possibly squeeze out some more accuracy with additional hyperparameter tuning, as I did not play around with the learning rate. Also, you could try feeding the lemmatized or stemmed tweets into it, as for this run, I went with the cleaned tweets instead of the stemmed/lemmatized. "},{"metadata":{},"cell_type":"markdown","source":"# 6. Conclusion <a name=\"conclusion\"></a>\n\n### Unsuprisingly, BERT performs better than an SVC or logistic regression. However, it was fairly shocking to see lemmatization and stemming perform a bit worse than just leaving the words alone. It was also a bit curious how the Random Forest Classifier lagged a bit behind the SVC and the logistic regression. We chose the SVC using TFIDF amongst the traditional classifiers because it was the most accurate and runs much faster than the logisitic regression on this data. But when it comes to raw accuracy, BERT is decidedly better than an SVC. \n\n### To further increase prediction accuracy, one should try tuning the hyperparameters of BERT, or testing other pretrained HuggingFace transformers on this dataset. "},{"metadata":{},"cell_type":"markdown","source":"### A big thank you to all these notebooks:\n\nhttps://www.kaggle.com/immvab/transformers-covid-19-tweets-sentiment-analysis/comments\n\nhttps://www.kaggle.com/arushi2/covid19-tweets-geo-and-sentiment-analysis#data\n\nhttps://www.kaggle.com/datatattle/battle-of-ml-classification-models\n\nhttps://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}