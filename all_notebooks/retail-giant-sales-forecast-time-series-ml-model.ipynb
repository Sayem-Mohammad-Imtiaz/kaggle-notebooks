{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"#importing libraries\nimport warnings\nwarnings.filterwarning='ignore'\n\nimport pandas as pd,numpy as np,matplotlib.pyplot as plt,seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Read data\ndata=pd.read_csv('../input/globalsuperstoredata/GlobalSuperstoreData.csv')\ndata['Order Date']=pd.to_datetime(data['Order Date'],format='%d-%m-%Y')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#checking  columns details and data types\ndata.info()\n#no null values present. order date data type needs to be changed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#checking column and rows \ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Preparation"},{"metadata":{"trusted":false},"cell_type":"code","source":"#creating a new column Market Segments by combining two columnsmarkets and segments seperated by '-'\ndata['Market Segments']=data['Segment']+'-'+data['Market']\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#droping the old columns \ndata.drop(['Segment','Market'],axis=1,inplace =True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Convert Order date column into year month format\ndata['Order Date'] = pd.to_datetime(data['Order Date']).dt.to_period('m')\n#data= data.sort_values(by=['Order Date'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Aggregate data as per month\ndata_profit=data.pivot_table(index='Order Date',columns='Market Segments',aggfunc='sum',values='Profit')\ndata_profit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#check shape rows/columns\ndata_profit.shape\n#48 months and 21 market segments present ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Train testdata split - 42 months for train data and 6 months for test data\ntrain_len=42\ntrain=data_profit[0:train_len]\ntest=data_profit[train_len:]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#checking most consistent market segment by checkingthe COV on profit. \nmean=np.mean(train)\nstd= np.std(train)\n\nCoV_df= pd.DataFrame(std/mean)\nCoV_df= CoV_df.reset_index()\nCoV_df.columns= ['Market_Segment','CoV']\nCoV_df.sort_values(by='CoV', ascending= True, inplace = True)\nCoV_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comments: \n - Consumer-APAC has the least COV. which means this has least variance and has more stability. Hence we select this most profitable market segment for sales forcasting. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Change date format into timestamps\ndata['Order Date'] = data['Order Date'].astype(str)\ndata['Order Date']=pd.to_datetime(data['Order Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#filter the main data set to Consumer-APAC which is the most profitable market segment.\ndata1=data[data['Market Segments']=='Consumer-APAC']\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#group data for using it for forecasting and applying timeseries modelling\ndata1=data1[['Order Date','Sales']].groupby('Order Date').sum()\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#plot time series data\ndata1.plot(figsize=(12, 4))\nplt.legend(loc='best')\nplt.title('Retail Giant Sales')\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Split data Train-Test"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_len=42\ntrain=data1[0:train_len]\ntest=data1[train_len:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Time Series Decomposition"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Additive method\n\nfrom pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 12, 8\ndecomposition = sm.tsa.seasonal_decompose(data1.Sales, model='additive') \nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##Multiplicative method\ndecomposition = sm.tsa.seasonal_decompose(data1.Sales, model='multiplicative') \nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Build and Evaluate Time Series forecasting model"},{"metadata":{},"cell_type":"markdown","source":"### Simple Time series method"},{"metadata":{},"cell_type":"markdown","source":"### 1. Naive method\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_naive = test.copy()\ny_hat_naive['naive_forecast'] = train['Sales'][train_len-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_naive['naive_forecast'], label='Naive forecast')\nplt.legend(loc='best')\nplt.title('Naive Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n    - As seen in the plot,the naive method uses the last or previous month data which is 2014-06. We can see that the forecast for the next six months is the same value as the last observation ."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_naive['naive_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_naive['naive_forecast'])/test['Sales'])*100,2)\n\nresults = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Simple Average method"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_avg = test.copy()\ny_hat_avg['avg_forecast'] = train['Sales'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Simple average forecast')\nplt.legend(loc='best')\nplt.title('Simple Average Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n- Forecast = Average of past months’ sales i.e average of  the 42 months sales data which is not showing any trend or seasonality while train and test data has both trend and seasonality."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['avg_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_avg['avg_forecast'])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple average method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Simple moving average method"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_sma = data1.copy()\nma_window = 3\ny_hat_sma['sma_forecast'] = data1['Sales'].rolling(ma_window).mean()\ny_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_sma['sma_forecast'], label='Simple moving average forecast')\nplt.legend(loc='best')\nplt.title('Simple Moving Average Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n- Forecast = Rolling Average of past 3 months’ sales. This method works well if the number of observations is fewer and the data is noisy since it  is able to predict the forecast better as it takes the variation of very few data points. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_sma['sma_forecast'][train_len:])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_sma['sma_forecast'][train_len:])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Simple exponential smoothing"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\nmodel = SimpleExpSmoothing(train['Sales'])\nmodel_fit = model.fit(optimized=True)\nmodel_fit.params\ny_hat_ses = test.copy()\ny_hat_ses['ses_forecast'] = model_fit.forecast(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_ses['ses_forecast'], label='Simple exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Simple Exponential Smoothing Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n- The model captures the level of a time series which it does as seen in this graph."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ses['ses_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ses['ses_forecast'])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple exponential smoothing forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Holt's Exponential Smoothing"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='additive', seasonal=None)\nmodel_fit = model.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\nprint(model_fit.params)\ny_hat_holt = test.copy()\ny_hat_holt['holt_forecast'] = model_fit.forecast(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_holt['holt_forecast'], label='Holt\\'s exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Holt\\'s Exponential Smoothing Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n- This model captures the trend component along with the level component but still does not capture the seasonality component."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RSME and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_holt['holt_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_holt['holt_forecast'])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Holt Winters' additive method"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_hwa = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='add')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwa['hw_forecast'] = model_fit.forecast(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s additive forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Additive Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comment:\n- This model captures the level and the trend along with the seasonality very well. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_hwa['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_hwa['hw_forecast'])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Holt Winter's multiplicative method"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_hat_hwm = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='mul')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwm['hw_forecast'] = model_fit.forecast(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_hwm['hw_forecast'], label='Holt Winters\\'s mulitplicative forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Mulitplicative Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_hwm['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_hwm['hw_forecast'])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n- Although this model captures level , trend and seasonality , the additive model has a better MAPE value. If the difference between subsequent troughs of the time series data does not increase as you progress in the graph, then the Holt-Winters' additive method works best"},{"metadata":{},"cell_type":"markdown","source":"## Auto Regressive methods"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Stationarity vs non-stationary time series\ndata1['Sales'].plot(figsize=(12, 4))\nplt.legend(loc='best')\nplt.title('Retail Giant Sales')\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Augmented Dickey-Fuller (ADF) test\nfrom statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(data1['Sales'])\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments: \n - P value <0.5 hence we rejct the null hypothesis . The series is stationary."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\nfrom statsmodels.tsa.stattools import kpss\nkpss_test = kpss(data1['Sales'])\n\nprint('KPSS Statistic: %f' % kpss_test[0])\nprint('Critical Values @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments: \n - P value <0.5 hence we rejct the null hypothesis . The series is not stationary. so lets make non-stationary series into stationary series"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Box Cox transformation to make variance constant\nfrom scipy.stats import boxcox\ndata_boxcox = pd.Series(boxcox(data1['Sales'], lmbda=0), index = data1.index)\n\nplt.figure(figsize=(12,4))\nplt.plot(data_boxcox, label='After Box Cox tranformation')\nplt.legend(loc='best')\nplt.title('After Box Cox transform')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Differencing to remove trend\ndata_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), data1.index)\nplt.figure(figsize=(12,4))\nplt.plot(data_boxcox_diff, label='After Box Cox tranformation and differencing')\nplt.legend(loc='best')\nplt.title('After Box Cox transform and differencing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_boxcox_diff.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Augmented Dickey-Fuller (ADF) test\nfrom statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(data_boxcox_diff)\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments: \n - P value <0.5 hence we rejct the null hypothesis . The series is stationary."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\nfrom statsmodels.tsa.stattools import kpss\nkpss_test = kpss(data_boxcox_diff)\n\nprint('KPSS Statistic: %f' % kpss_test[0])\nprint('Critical Values @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments: \n - P value >0.5 hence we fail to rejct the null hypothesis . The series is stationary."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data_boxcox = data_boxcox[:train_len]\ntest_data_boxcox = data_boxcox[train_len:]\ntrain_data_boxcox_diff = data_boxcox_diff[:train_len-1]\ntest_data_boxcox_diff = data_boxcox_diff[train_len-1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Auto regression method (AR)"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(train_data_boxcox_diff, order=(1, 0, 0)) \nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Recover original time series\ny_hat_ar = data_boxcox_diff.copy()\ny_hat_ar['ar_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox_diff'].cumsum()\ny_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox'].add(data_boxcox[0])\ny_hat_ar['ar_forecast'] = np.exp(y_hat_ar['ar_forecast_boxcox'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_ar['ar_forecast'][test.index.min():], label='Auto regression forecast')\nplt.legend(loc='best')\nplt.title('Auto Regression Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n    - From the plot we can see that we are able to capture trend in the forecast but could not cature the seasonality."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ar['ar_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ar['ar_forecast'][test.index.min():])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive (AR) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Moving average method (MA)\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = ARIMA(train_data_boxcox_diff, order=(0, 0, 1)) \nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Recover original time series\ny_hat_ma = data_boxcox_diff.copy()\ny_hat_ma['ma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ma['ma_forecast_boxcox'] = y_hat_ma['ma_forecast_boxcox_diff'].cumsum()\ny_hat_ma['ma_forecast_boxcox'] = y_hat_ma['ma_forecast_boxcox'].add(data_boxcox[0])\ny_hat_ma['ma_forecast'] = np.exp(y_hat_ma['ma_forecast_boxcox'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(data1['Sales'][:train_len], label='Train')\nplt.plot(data1['Sales'][train_len:], label='Test')\nplt.plot(y_hat_ma['ma_forecast'][test.index.min():], label='Moving average forecast')\nplt.legend(loc='best')\nplt.title('Moving Average Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"### Comment:\n    - Here we are able to capture under forecasting trend but not seasonality in the forecast"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ma['ma_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ma['ma_forecast'][test.index.min():])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Moving Average (MA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Auto regression moving average method (ARMA)"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = ARIMA(train_data_boxcox_diff, order=(1, 0, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Recover original time series\ny_hat_arma = data_boxcox_diff.copy()\ny_hat_arma['arma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arma['arma_forecast_boxcox'] = y_hat_arma['arma_forecast_boxcox_diff'].cumsum()\ny_hat_arma['arma_forecast_boxcox'] = y_hat_arma['arma_forecast_boxcox'].add(data_boxcox[0])\ny_hat_arma['arma_forecast'] = np.exp(y_hat_arma['arma_forecast_boxcox'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( data1['Sales'][:train_len-1], label='Train')\nplt.plot(data1['Sales'][train_len-1:], label='Test')\nplt.plot(y_hat_arma['arma_forecast'][test.index.min():], label='ARMA forecast')\nplt.legend(loc='best')\nplt.title('ARMA Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n-ARMA model captured Trend which is under forecasting and also  no seasonality"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_arma['arma_forecast'][train_len-1:])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_arma['arma_forecast'][train_len-1:])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive moving average (ARMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Auto regressive integrated moving average (ARIMA)"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = ARIMA(train_data_boxcox, order=(1, 1, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Recover original time series forecast\ny_hat_arima = data_boxcox_diff.copy()\ny_hat_arima['arima_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arima['arima_forecast_boxcox'] = y_hat_arima['arima_forecast_boxcox_diff'].cumsum()\ny_hat_arima['arima_forecast_boxcox'] = y_hat_arima['arima_forecast_boxcox'].add(data_boxcox[0])\ny_hat_arima['arima_forecast'] = np.exp(y_hat_arima['arima_forecast_boxcox'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_arima['arima_forecast'][test.index.min():], label='ARIMA forecast')\nplt.legend(loc='best')\nplt.title('Autoregressive integrated moving average (ARIMA) method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n-The trend captured is under forecasting. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_arima['arima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_arima['arima_forecast'][test.index.min():])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive integrated moving average (ARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Seasonal auto regressive integrated moving average (SARIMA)"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(train_data_boxcox, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)) \nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Recover original time series forecast\ny_hat_sarima = data_boxcox_diff.copy()\ny_hat_sarima['sarima_forecast_boxcox'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_sarima['sarima_forecast'] = np.exp(y_hat_sarima['sarima_forecast_boxcox'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_sarima['sarima_forecast'][test.index.min():], label='SARIMA forecast')\nplt.legend(loc='best')\nplt.title('Seasonal autoregressive integrated moving average (SARIMA) method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments:\n- The forecast captured both trend and seasonality very well. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_sarima['sarima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_sarima['sarima_forecast'][test.index.min():])/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Seasonal autoregressive integrated moving average (SARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n- Thus we can conclude that, Holt Winters additive method is the best forecasting method in the smoothing technique\n- And SARIMA - Seasonal Autoregressive Integrated moving average is the best method in ARIMA set of techniques.\n- Both these techniques could capture the trend very well and also the seasonality which the rest of the models could not. "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}