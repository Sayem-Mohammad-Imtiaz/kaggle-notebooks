{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nrandom_seed = np.random.RandomState(12)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generating a set of normal observation to be used as training data","metadata":{}},{"cell_type":"code","source":"X_train = 0.5 * random_seed.randn(500,2)\nX_train = np.r_[X_train + 3, X_train]\nX_train = pd.DataFrame(X_train, columns=[\"x\",\"y\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = 0.5 * random_seed.randn(500,2)\nX_test = np.r_[X_test + 3, X_test]\nX_test = pd.DataFrame(X_test, columns=[\"x\",\"y\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate a set of outliers observation","metadata":{}},{"cell_type":"code","source":"X_outliers = random_seed.uniform(low=-5, high=5, size=(50,2))\nX_outliers = pd.DataFrame(X_outliers, columns = [\"x\", \"y\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets take a look at what we have generated","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\np1 = plt.scatter(X_train.x, X_train.y, c=\"white\", s=50, edgecolor=\"black\")\np2 = plt.scatter(X_test.x, X_test.y, c=\"green\", s=50, edgecolor=\"black\")\np3 = plt.scatter(X_outliers.x, X_outliers.y, c=\"blue\", s=50, edgecolor=\"black\")\n\nplt.xlim((-6,6))\nplt.ylim((-6,6))\nplt.legend([p1,p2,p3], [\"training set\", \"normal testing set\", \"anomalous testing set\"], loc=\"lower right\",)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training an isolation forest model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nclf = IsolationForest()\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_outliers = X_outliers.assign(pred=y_pred_outliers)\nX_outliers.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets plot the isolation forest prediction and overlay it on top of the graph","metadata":{}},{"cell_type":"code","source":"def printOutliers(X_train, X_outliers):\n    p1 = plt.scatter(X_train.x, X_train.y, c=\"white\", s=50, edgecolor=\"black\")\n    p2 = plt.scatter(X_outliers.loc[X_outliers.pred == -1, [\"x\"]],\n                     X_outliers.loc[X_outliers.pred == -1, [\"y\"]],\n                     c = \"blue\",\n                     s=50,\n                     edgecolor=\"black\")\n    p3 = plt.scatter(X_outliers.loc[X_outliers.pred == 1, [\"x\"]],\n                     X_outliers.loc[X_outliers.pred == 1, [\"y\"]],\n                     c = \"red\",\n                     s=50,\n                     edgecolor=\"black\")\n    plt.xlim((-6,6))\n    plt.ylim((-6,6))\n    plt.legend(\n        [p1,p2,p3],\n        [\"training observations\",\"detected outliers\", \"incorrectly labeled outliers\"],\n        loc = \"lower right\"\n    )\n\n    plt.show()\n\nprintOutliers(X_train, X_outliers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.assign(pred=y_pred_test)\nX_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets plot the testing data","metadata":{}},{"cell_type":"code","source":"printOutliers(X_train, X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first step involves simply loading the necessary libraries that will allow us to\nmanipulate data quickly and easily. In steps 2 and 3, we generate a training and testing set\nconsisting of normal observations. These have the same distributions. In step 4, on the other\nhand, we generate the remainder of our testing set by creating outliers. This anomalous\ndataset has a different distribution from the training data and the rest of the testing data.\nPlotting our data, we see that some outlier points look indistinguishable from normal\npoints (step 5). This guarantees that our classifier will have a significant percentage of\nmisclassifications, due to the nature of the data, and we must keep this in mind when\nevaluating its performance. In step 6, we fit an instance of Isolation Forest with default\nparameters to the training data.\nNote that the algorithm is fed no information about the anomalous data. We use our\ntrained instance of Isolation Forest to predict whether the testing data is normal or\nanomalous, and similarly to predict whether the anomalous data is normal or anomalous.\nTo examine how the algorithm performs, we append the predicted labels to X_outliers\n(step 7) and then plot the predictions of the Isolation Forest instance on the outliers (step 8).\nWe see that it was able to capture most of the anomalies. Those that were incorrectly\nlabeled were indistinguishable from normal observations. Next, in step 9, we append the\npredicted label to X_test in preparation for analysis and then plot the predictions of the\nIsolation Forest instance on the normal testing data (step 10). We see that it correctly labeled\nthe majority of normal observations. At the same time, there was a significant number of\nincorrectly classified normal observations (shown in red).\nDepending on how many false alarms we are willing to tolerate, we may need to fine-tune\nour classifier to reduce the number of false positives","metadata":{}}]}