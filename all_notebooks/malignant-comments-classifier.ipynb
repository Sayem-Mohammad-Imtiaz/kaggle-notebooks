{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Malignant Comments Project","metadata":{}},{"cell_type":"code","source":"# import useful libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:13.071717Z","iopub.execute_input":"2021-09-11T04:30:13.072662Z","iopub.status.idle":"2021-09-11T04:30:13.424073Z","shell.execute_reply.started":"2021-09-11T04:30:13.072557Z","shell.execute_reply":"2021-09-11T04:30:13.423169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Reading and Understanding","metadata":{}},{"cell_type":"code","source":"# import train dataset\ndf_train = pd.read_csv(\"../input/malignant-comments/Malignant_train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:13.425569Z","iopub.execute_input":"2021-09-11T04:30:13.425801Z","iopub.status.idle":"2021-09-11T04:30:14.550601Z","shell.execute_reply.started":"2021-09-11T04:30:13.425764Z","shell.execute_reply":"2021-09-11T04:30:14.549601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import test dataset\ndf_test = pd.read_csv(\"../input/malignant-comments/Malignant_test.csv\")\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:14.552258Z","iopub.execute_input":"2021-09-11T04:30:14.552526Z","iopub.status.idle":"2021-09-11T04:30:15.4617Z","shell.execute_reply.started":"2021-09-11T04:30:14.55249Z","shell.execute_reply":"2021-09-11T04:30:15.46087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check shape of the train and test dataset\nprint(df_train.shape)\nprint(df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:15.464084Z","iopub.execute_input":"2021-09-11T04:30:15.464681Z","iopub.status.idle":"2021-09-11T04:30:15.470103Z","shell.execute_reply.started":"2021-09-11T04:30:15.464639Z","shell.execute_reply":"2021-09-11T04:30:15.469432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our train dataset we have 159571 rows and 8 features, while in test dataset 153164 rows and 2 features are present.","metadata":{}},{"cell_type":"code","source":"# check information of train data\nprint(df_train.info())\n\n# check information of test data\nprint(df_test.info())","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:15.470978Z","iopub.execute_input":"2021-09-11T04:30:15.471195Z","iopub.status.idle":"2021-09-11T04:30:15.581208Z","shell.execute_reply.started":"2021-09-11T04:30:15.471169Z","shell.execute_reply":"2021-09-11T04:30:15.580137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't have any null value in our train and test dataset. ","metadata":{}},{"cell_type":"code","source":"# check null values of train data using heatmap\nsns.heatmap(df_train.isnull())","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:15.582492Z","iopub.execute_input":"2021-09-11T04:30:15.582815Z","iopub.status.idle":"2021-09-11T04:30:17.569682Z","shell.execute_reply.started":"2021-09-11T04:30:15.582776Z","shell.execute_reply":"2021-09-11T04:30:17.56868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmap, we can clearly see that there is no null value found in our dataset.","metadata":{}},{"cell_type":"code","source":"# check discriptive statistics of the train dataset\ndf_train.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:17.570898Z","iopub.execute_input":"2021-09-11T04:30:17.571457Z","iopub.status.idle":"2021-09-11T04:30:17.964055Z","shell.execute_reply.started":"2021-09-11T04:30:17.571413Z","shell.execute_reply":"2021-09-11T04:30:17.963191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. From the above table, we can see that there no duplicate data present in comment_text column.\n2. All numerical columns have only two values i.e. 0 and 1.","metadata":{}},{"cell_type":"code","source":"# check correlation of numerical features using heatmap\nsns.heatmap(df_train.corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:17.966108Z","iopub.execute_input":"2021-09-11T04:30:17.966694Z","iopub.status.idle":"2021-09-11T04:30:18.482759Z","shell.execute_reply.started":"2021-09-11T04:30:17.966649Z","shell.execute_reply":"2021-09-11T04:30:18.481793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analysis","metadata":{}},{"cell_type":"code","source":"# check count plot of all target features. \ncolumn = ['malignant','highly_malignant','loathe','rude','abuse','threat']\nfor i in column:\n    print(i)\n    print('\\n')\n    print(df_train[i].value_counts())\n    sns.countplot(df_train[i])\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-09-11T04:30:18.483903Z","iopub.execute_input":"2021-09-11T04:30:18.484144Z","iopub.status.idle":"2021-09-11T04:30:19.641338Z","shell.execute_reply.started":"2021-09-11T04:30:18.484117Z","shell.execute_reply":"2021-09-11T04:30:19.640307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above all count plot is our target feature and we can see that there is imbalanced classification and this issue affect our final result. So, we will use oversampling method in further process.","metadata":{}},{"cell_type":"code","source":"# create a label feature, which is combination of all target columns.\nall_labels = ['malignant','highly_malignant','rude','threat','abuse','loathe']\ndf_train['Label'] = df_train[all_labels].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:19.64519Z","iopub.execute_input":"2021-09-11T04:30:19.645463Z","iopub.status.idle":"2021-09-11T04:30:19.656004Z","shell.execute_reply.started":"2021-09-11T04:30:19.645433Z","shell.execute_reply":"2021-09-11T04:30:19.65499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(8)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:19.657172Z","iopub.execute_input":"2021-09-11T04:30:19.657482Z","iopub.status.idle":"2021-09-11T04:30:19.67604Z","shell.execute_reply.started":"2021-09-11T04:30:19.657443Z","shell.execute_reply":"2021-09-11T04:30:19.675014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot label column count\nplt.figure(figsize=(9,5))\nsns.countplot(df_train['Label'])\nplt.title(\"Label Count\",fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:19.677891Z","iopub.execute_input":"2021-09-11T04:30:19.678151Z","iopub.status.idle":"2021-09-11T04:30:19.909738Z","shell.execute_reply.started":"2021-09-11T04:30:19.678117Z","shell.execute_reply":"2021-09-11T04:30:19.908639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can say that, most of the comments are good and very less numbers of comments is bad.(0=good comments and others are bad comments.)","metadata":{}},{"cell_type":"code","source":"# Here, we convert label column in form of 0 and 1 (scaling).\n# 0 = good comments and 1 = bad comments\ndf_train['Label'] = df_train['Label']>0\ndf_train['Label'] = df_train['Label'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:19.913026Z","iopub.execute_input":"2021-09-11T04:30:19.913264Z","iopub.status.idle":"2021-09-11T04:30:19.92018Z","shell.execute_reply.started":"2021-09-11T04:30:19.913237Z","shell.execute_reply":"2021-09-11T04:30:19.919505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(8)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:19.921694Z","iopub.execute_input":"2021-09-11T04:30:19.922475Z","iopub.status.idle":"2021-09-11T04:30:19.942057Z","shell.execute_reply.started":"2021-09-11T04:30:19.922424Z","shell.execute_reply":"2021-09-11T04:30:19.941034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, we plot our label column\nsns.countplot(df_train['Label'])\nplt.show()\n\ndf_train['Label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:19.943656Z","iopub.execute_input":"2021-09-11T04:30:19.944266Z","iopub.status.idle":"2021-09-11T04:30:20.127954Z","shell.execute_reply.started":"2021-09-11T04:30:19.944222Z","shell.execute_reply":"2021-09-11T04:30:20.127135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above count plot we can see that, even after merge all target columns, it is still imbalanced. So, to solve this issue we use oversampling method in further process. ","metadata":{}},{"cell_type":"code","source":"# Now, we plot wordcloud of malignant comments and see which type word is most used in malignant comments.\nfrom wordcloud import WordCloud\nhams = df_train['comment_text'][df_train['malignant']==1]\nspam_cloud = WordCloud(width=750,height=500,background_color='black',max_words=45).generate(' '.join(hams))\nplt.figure(figsize=(10,8),facecolor='k')\nplt.imshow(spam_cloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:20.129424Z","iopub.execute_input":"2021-09-11T04:30:20.130211Z","iopub.status.idle":"2021-09-11T04:30:23.801092Z","shell.execute_reply.started":"2021-09-11T04:30:20.130171Z","shell.execute_reply":"2021-09-11T04:30:23.799999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, we plot wordcloud of abuse comments and see which type word is most used in abuse comments.\nhams = df_train['comment_text'][df_train['abuse']==1]\nspam_cloud = WordCloud(width=750,height=500,background_color='black',max_words=45).generate(' '.join(hams))\nplt.figure(figsize=(10,8),facecolor='k')\nplt.imshow(spam_cloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:23.80258Z","iopub.execute_input":"2021-09-11T04:30:23.8029Z","iopub.status.idle":"2021-09-11T04:30:25.846809Z","shell.execute_reply.started":"2021-09-11T04:30:23.802862Z","shell.execute_reply":"2021-09-11T04:30:25.846166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# import useful libraries\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:25.847737Z","iopub.execute_input":"2021-09-11T04:30:25.84848Z","iopub.status.idle":"2021-09-11T04:30:26.147957Z","shell.execute_reply.started":"2021-09-11T04:30:25.848445Z","shell.execute_reply":"2021-09-11T04:30:26.146913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating comments length\ndf_train[\"comment_length\"] = df_train[\"comment_text\"].str.len()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:26.149341Z","iopub.execute_input":"2021-09-11T04:30:26.149971Z","iopub.status.idle":"2021-09-11T04:30:26.306239Z","shell.execute_reply.started":"2021-09-11T04:30:26.149936Z","shell.execute_reply":"2021-09-11T04:30:26.30527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert all comments text into lower case\ndf_train['Cleaned_comment_text'] = df_train['comment_text'].str.lower()\ndf_test['cleaned_comment_text'] = df_test['comment_text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:26.307539Z","iopub.execute_input":"2021-09-11T04:30:26.307767Z","iopub.status.idle":"2021-09-11T04:30:26.743306Z","shell.execute_reply.started":"2021-09-11T04:30:26.30774Z","shell.execute_reply":"2021-09-11T04:30:26.742397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove punctuation from cleaned comment text column\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].str.replace('[^\\w\\s]','')\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].str.replace('[^\\w\\s]','')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:26.744649Z","iopub.execute_input":"2021-09-11T04:30:26.744934Z","iopub.status.idle":"2021-09-11T04:30:32.953529Z","shell.execute_reply.started":"2021-09-11T04:30:26.744896Z","shell.execute_reply":"2021-09-11T04:30:32.95204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing stopwords from cleaned comment text column\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:30:32.956381Z","iopub.execute_input":"2021-09-11T04:30:32.956873Z","iopub.status.idle":"2021-09-11T04:31:13.198153Z","shell.execute_reply.started":"2021-09-11T04:30:32.956839Z","shell.execute_reply":"2021-09-11T04:31:13.197103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, let's remove digits from the cleaned comment text column\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].str.replace('\\d+', '')\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].str.replace('\\d+', '')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:31:13.199615Z","iopub.execute_input":"2021-09-11T04:31:13.199984Z","iopub.status.idle":"2021-09-11T04:31:16.233412Z","shell.execute_reply.started":"2021-09-11T04:31:13.199943Z","shell.execute_reply":"2021-09-11T04:31:16.232623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, we use Lemmatizing. Lemmatization is the process of converting a word to its base form.\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(x) for x in x.split()))\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(x) for x in x.split()))\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:31:16.235297Z","iopub.execute_input":"2021-09-11T04:31:16.235604Z","iopub.status.idle":"2021-09-11T04:32:15.967185Z","shell.execute_reply.started":"2021-09-11T04:31:16.235564Z","shell.execute_reply":"2021-09-11T04:32:15.9663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating cleaned comments length\ndf_train[\"Cleaned_comment_length\"] = df_train[\"Cleaned_comment_text\"].str.len()\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:15.968344Z","iopub.execute_input":"2021-09-11T04:32:15.968577Z","iopub.status.idle":"2021-09-11T04:32:16.136667Z","shell.execute_reply.started":"2021-09-11T04:32:15.968551Z","shell.execute_reply":"2021-09-11T04:32:16.135855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total reduced length\nprint(\"Original Length: \",df_train['comment_length'].sum())\nprint(\"Cleaned Length: \",df_train['Cleaned_comment_length'].sum())","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:16.137923Z","iopub.execute_input":"2021-09-11T04:32:16.138193Z","iopub.status.idle":"2021-09-11T04:32:16.145341Z","shell.execute_reply.started":"2021-09-11T04:32:16.138154Z","shell.execute_reply":"2021-09-11T04:32:16.144385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert text into vectors using TF-IDF\ntf_vec = TfidfVectorizer(max_features=8000, stop_words='english')\nfeature = tf_vec.fit_transform(df_train['Cleaned_comment_text'])\n\n# split the target column.\n# here, our target column is label and it is a classification problem.\nx = feature\ny = df_train['Label']","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:16.146749Z","iopub.execute_input":"2021-09-11T04:32:16.147096Z","iopub.status.idle":"2021-09-11T04:32:24.93031Z","shell.execute_reply.started":"2021-09-11T04:32:16.147067Z","shell.execute_reply":"2021-09-11T04:32:24.929432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert test data's text into vectors using TF-IDF\ntf_vec = TfidfVectorizer(max_features=8000, stop_words='english')\nfeature_test = tf_vec.fit_transform(df_test['cleaned_comment_text'])","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:24.934292Z","iopub.execute_input":"2021-09-11T04:32:24.93452Z","iopub.status.idle":"2021-09-11T04:32:33.151728Z","shell.execute_reply.started":"2021-09-11T04:32:24.934495Z","shell.execute_reply":"2021-09-11T04:32:33.15067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hear, we use SMOTE(resampling) method to cop up with imbalanced classification of target variable.\n# SMOTE(Synthetic Minority Oversampling Technique) algorithm generates synthetic samples of minority class.\nx_smote,y_smote = smote.fit_resample(x,y)\n\n# check shape of the train dataset before oversampling\nprint(x.shape)\nprint(y.shape)\n\n# check shape of the train dataset after oversampling \nprint(x_smote.shape)\nprint(y_smote.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:33.152981Z","iopub.execute_input":"2021-09-11T04:32:33.153218Z","iopub.status.idle":"2021-09-11T04:32:41.855057Z","shell.execute_reply.started":"2021-09-11T04:32:33.153191Z","shell.execute_reply":"2021-09-11T04:32:41.854066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the counts of original target column\ny.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:41.856648Z","iopub.execute_input":"2021-09-11T04:32:41.857061Z","iopub.status.idle":"2021-09-11T04:32:41.867964Z","shell.execute_reply.started":"2021-09-11T04:32:41.857018Z","shell.execute_reply":"2021-09-11T04:32:41.867056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the count of target column after oversampling \ny_smote.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:41.869407Z","iopub.execute_input":"2021-09-11T04:32:41.869646Z","iopub.status.idle":"2021-09-11T04:32:41.881756Z","shell.execute_reply.started":"2021-09-11T04:32:41.869621Z","shell.execute_reply":"2021-09-11T04:32:41.880977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split train and test data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size=0.20,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:41.88289Z","iopub.execute_input":"2021-09-11T04:32:41.883764Z","iopub.status.idle":"2021-09-11T04:32:41.940274Z","shell.execute_reply.started":"2021-09-11T04:32:41.883731Z","shell.execute_reply":"2021-09-11T04:32:41.939233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Best Model","metadata":{}},{"cell_type":"code","source":"# our problem is classification type of problem.\n# import useful libraries for machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nmodel = [LogisticRegression(solver='liblinear'),DecisionTreeClassifier(),MultinomialNB()]\n\nfor m in model:\n    m.fit(x_train,y_train)\n    train = m.score(x_train,y_train)\n    predm = m.predict(x_test)\n    print(\"Accuracy of\",m,\"is:\")\n    print(\"Accuracy of training model is:\",train)\n    print(\"Accuracy Score:\",accuracy_score(y_test,predm))\n    print(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predm))\n    print(\"Classification report:\",\"\\n\",classification_report(y_test,predm))\n    print(\"************************************************************\")\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:32:41.941427Z","iopub.execute_input":"2021-09-11T04:32:41.941648Z","iopub.status.idle":"2021-09-11T04:35:56.97898Z","shell.execute_reply.started":"2021-09-11T04:32:41.941622Z","shell.execute_reply":"2021-09-11T04:35:56.978007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bagging and Boosting methods","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train)\ntrain = rf.score(x_train,y_train)\npred_rf=rf.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,pred_rf))\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,pred_rf))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,pred_rf))","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:35:56.980464Z","iopub.execute_input":"2021-09-11T04:35:56.980805Z","iopub.status.idle":"2021-09-11T04:52:16.182353Z","shell.execute_reply.started":"2021-09-11T04:35:56.980747Z","shell.execute_reply":"2021-09-11T04:52:16.181414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\nparameters = {'learning_rate':[0.01,0.1]}\nclf = GridSearchCV(gbc,parameters)\nclf.fit(x_train,y_train)\nprint(clf.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T04:52:16.183826Z","iopub.execute_input":"2021-09-11T04:52:16.184172Z","iopub.status.idle":"2021-09-11T05:20:54.418903Z","shell.execute_reply.started":"2021-09-11T04:52:16.184133Z","shell.execute_reply":"2021-09-11T05:20:54.417975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc = GradientBoostingClassifier(learning_rate=0.1)\ngbc.fit(x_train,y_train)\ntrain = gbc.score(x_train,y_train)\npredgbc = gbc.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,predgbc)*100)\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predgbc))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,predgbc))","metadata":{"execution":{"iopub.status.busy":"2021-09-11T05:20:54.420238Z","iopub.execute_input":"2021-09-11T05:20:54.420484Z","iopub.status.idle":"2021-09-11T05:24:16.200732Z","shell.execute_reply.started":"2021-09-11T05:20:54.420456Z","shell.execute_reply":"2021-09-11T05:24:16.199738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxg = XGBClassifier()\nxg.fit(x_train,y_train)\ntrain = xg.score(x_train,y_train)\npredxg = xg.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,predxg))\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predxg))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,predxg))","metadata":{"execution":{"iopub.status.busy":"2021-09-11T05:24:16.202102Z","iopub.execute_input":"2021-09-11T05:24:16.202402Z","iopub.status.idle":"2021-09-11T05:25:47.48658Z","shell.execute_reply.started":"2021-09-11T05:24:16.202364Z","shell.execute_reply":"2021-09-11T05:25:47.485589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\ncb = CatBoostClassifier()\ncb.fit(x_train,y_train)\ntrain = cb.score(x_train,y_train)\npredcb = cb.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,predcb))\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predcb))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,predcb))  ","metadata":{"execution":{"iopub.status.busy":"2021-09-11T05:25:47.487972Z","iopub.execute_input":"2021-09-11T05:25:47.488278Z","iopub.status.idle":"2021-09-11T05:42:13.986389Z","shell.execute_reply.started":"2021-09-11T05:25:47.488239Z","shell.execute_reply":"2021-09-11T05:42:13.98538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. We use some algorithms and we find randomforest classifier as best model. It gives 99% training model accuracy and 97% testing accuracy. Randomforest classifier also gives good precision and recall score along with f1 score.\n2. Here, we don't use hyperparameter tuning because it takes too much time as well as some algorithm also gives memory error.","metadata":{}},{"cell_type":"code","source":"# check auc_roc curve and auc score of best model\nfrom sklearn.metrics import roc_curve,auc\nfpr,tpr,thresholds = roc_curve(pred_rf,y_test)\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nplt.plot(fpr,tpr,color=\"orange\", lw=3, label=(\"ROC curve (area = %0.2f)\" % roc_auc))\nplt.plot([0,1],[0,1],color = \"navy\",lw=3,linestyle=\"--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"RandomForest Classifier\")\nplt.legend(loc = \"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T05:42:13.98771Z","iopub.execute_input":"2021-09-11T05:42:13.988579Z","iopub.status.idle":"2021-09-11T05:42:14.225748Z","shell.execute_reply.started":"2021-09-11T05:42:13.98853Z","shell.execute_reply":"2021-09-11T05:42:14.224891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can see that, we get best area under the curve for randomforest classifier, which is 98%.","metadata":{}},{"cell_type":"code","source":"#save best result\ndf1 = pd.DataFrame(pred_rf)\ndf1.to_csv(\"rf_malignant.csv\")\n#save best model\nimport joblib\njoblib.dump(rf,\"rf_malignant.obj\")","metadata":{"execution":{"iopub.status.busy":"2021-09-11T05:42:14.227306Z","iopub.execute_input":"2021-09-11T05:42:14.227888Z","iopub.status.idle":"2021-09-11T05:42:14.795265Z","shell.execute_reply.started":"2021-09-11T05:42:14.227847Z","shell.execute_reply":"2021-09-11T05:42:14.794618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check our test dataset with best model\ntest_dataset = rf.predict(feature_test)\nprint(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T05:42:14.796296Z","iopub.execute_input":"2021-09-11T05:42:14.797045Z","iopub.status.idle":"2021-09-11T05:43:05.142573Z","shell.execute_reply.started":"2021-09-11T05:42:14.797011Z","shell.execute_reply":"2021-09-11T05:43:05.141861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save test dataset result\ndf2 = pd.DataFrame(test_dataset)\ndf2.to_csv(\"rf_malignant_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-11T05:43:05.143554Z","iopub.execute_input":"2021-09-11T05:43:05.144447Z","iopub.status.idle":"2021-09-11T05:43:05.48683Z","shell.execute_reply.started":"2021-09-11T05:43:05.144401Z","shell.execute_reply":"2021-09-11T05:43:05.486168Z"},"trusted":true},"execution_count":null,"outputs":[]}]}