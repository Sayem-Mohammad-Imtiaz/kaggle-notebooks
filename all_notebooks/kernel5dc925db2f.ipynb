{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        dataset =os.path.join(dirname, filename)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nmovies_=pd.read_csv(dataset)\nmovies_.tail()\n#Here we wil try to find out the genre of the movie from the plot of the movie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_=movies_[['Title','Plot','Release Year','Origin/Ethnicity']]\nis_indian=desc_['Origin/Ethnicity']==\"Bollywood\" \nindian =desc_[is_indian]\nindian.shape\nindian.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we have 2931 bollywood movies till 2017 as per this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose some words to be stemmed\n#stemming cuts off the end of the word or the beginning of the word.\n#lemmatization takes into consideration into morphological analysis of the words.\n#below is the demonstration\n\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nlemmatizer = WordNetLemmatizer() \nps = PorterStemmer()   \nwords= [\"finally\",\"called\",\"tries\",'marriage','siblings','seduce','escape']\nfor w in words:\n    print(w,ps.stem(w),\"<<< by stemming\")\n    print(w, lemmatizer.lemmatize(w),\" <<< by lemmatizing\")\n    #for experiment try to fist stem and then lemmatize\n    stemmed=ps.stem(w)\n    print(stemmed,lemmatizer.lemmatize(w),\" <<< lemmatizing after stemming\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We can hereby decide not to stem the words as in most cases the words become meaningless thus adding noise.\n2. Buf if we lemmatize a stemmed word , it gives us the actual result."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import sent_tokenize, word_tokenize   \nfrom nltk.tokenize import WordPunctTokenizer\nimport gensim\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\n\npunc = WordPunctTokenizer()\nlemmatizer = WordNetLemmatizer() \nstop = stopwords.words('english')\nexclude = set(string.punctuation)\nextra_stop=['the','he','she','they','charlie','jekyll','jack','film','tom','steve','andrews','jeff',\n            'miranda','jonathan','alicia','john','one','jim','alice','henriette','richard','sylvia','andrew'\n           ,'molly','pollyanna','mrs\"','mrs','mr','louise','keaton','angela','mary','robert','paul','ann\"','joe','bob',\n           'however','sally','judy','jerry','jimmy','also','harold','david','marcus','margaret','nicki','harry',\n           'brice','geoffrey','betty','morgan','juan','kelly','sebastian','cesar','maria','karl','egan','finds'\n           ,'norma','another','tells','two','dorothy','vivian','billie','kitty','vivian','dan','young','man',\n            'christine','eddie','nancy','davidson','david','ann','sadie','ronald','alvin','patricia','kiki','girl',\n           'woman','takes','take','tells','tell','get','gets','have','stan','later','men','ivan','nick','anjali',\n           'raja','killed','sonia','viktor','chris','ben','krishna','michael','adam','johnny','larry','duke','new',\n           'mike','pete','elmer','bill','george','sam','susan','raj','sonali','anna','julia','oleg','joseph',\n           'sergei','raju','kumar','vicky','henry','tony','boby','bobby','lily','raju','salim','kills',\n           'sonia','oleg','julia','conan','peter','kiran','maya','james','singh','olga','philip','shiva',\n           'singh','anton','abhi','arjun','alex','eric','billy','simon','rama','find','jackie','tina',\n           'chandu','next','hari','kate','turn','first','leave','make','fall','soon','tries','try','final',\n           'return','back','meet','want','come','here','call','called','leave','arrive','reach','away','far',\n            'raja','sanjana','amar','vinay','vijay','rahul','rohit','shyam','prem','anand','aarti','ravi','priya',\n           'radha','kiran','karan','vikram','suraj','ajay','ask','turn']\n#here i have included names as stopwords becuase these names do not contirbute anything,\n#towards finding the genre/topic of a movie\nstop.extend(extra_stop)\nstop=set(stop)\nfrom nltk.stem import PorterStemmer\nlemmatizer = WordNetLemmatizer() \nps = PorterStemmer()\n\ndef clean_text(text):\n    word_tokens = (word_tokenize(text))\n    remove_stop=[w.lower() for w in word_tokens if w.lower() not in stop]\n    remove_punct=[c for c in remove_stop if c not in exclude and len(c)>3]\n    clean =[re.sub(r'[^a-zA-Z0-9]','',i) for i in remove_punct ]\n    stemmer=[ps.stem(words) for words in clean]\n    lemma= \" \".join([lemmatizer.lemmatize(wr) for wr in stemmer])\n    print (clean,stemmer,lemma)\n    return lemma\n\nindian['clean_plot'] = indian['Plot'].map(clean_text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the clean text now in 'clean_plot' now. We will find out how many words are there and their respective frequency in the corups"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a word_freq dict\n\nwords_arr =indian['clean_plot'].values\nword_freq={}\nfor sent in words_arr:\n    for wr in word_tokenize(sent):\n        if wr not in word_freq.keys():\n            word_freq[wr]=1\n        else:\n            word_freq[wr]=word_freq[wr]+1\nprint (len(word_freq))            \nlists = sorted(word_freq.items())\npd.DataFrame(lists,columns=['words','frequency']).sort_values(by='frequency',ascending=False).set_index('words')[:20].plot(kind='bar',figsize=(20,10),title='Frequency Dist For Top 20 Words');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TOtal no of words with only lemmatizing =70134\ntotal no of words with lemmaztizing after stemming=53829"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntext = \" \".join(text_ for text_ in indian['clean_plot'])\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATION :\nWe can see terms *\"love,family,father,freind,help,life*\" are important here.\nI have taken the data"},{"metadata":{},"cell_type":"markdown","source":"We will build a topic model on top of the text corpus we have, we will use the probability obtained from the topic obtained and the exam scores to find the correlation.\n\nSTEP 1: Generate BoW model and corresponding TFIDF model\n\nSTEP 2: Generate a topic model based on TFIDF model\n\nSTEP3: Obtain for each sentence from \"clean_plot\" the corresponding topic number and its highest probabilty\n\nSTEP 4: Create an umbrella for the topic words, For ex: [\"love\",\"father\"] will fall under the umbrella of family.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets first build a Bag of Words model\nprocessed_docs = [s.split(' ') for s in indian['clean_plot'].tolist()]\ndictionary = gensim.corpora.Dictionary(processed_docs)\ndictionary.filter_extremes(no_below=50, no_above=0.3)\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe will build a tfidf model from bow_corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will build a tfidf corpus from the Bag of Words Model\nfrom gensim import corpora, models\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets generate tpoics from lda model using tfidf corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets generate topics from the obtained tfidf corpus\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import ldamodel\nimport gensim.corpora;\nimport pickle\n\narray_text =[v for v in tokens_list_doc2bow]\nid2word=gensim.corpora.Dictionary(array_text)\ncorpus = [id2word.doc2bow(text) for text in array_text]\nlda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, \n                        num_topics=10)\nlda.save('model5.gensim')\ntopics = lda.print_topics(num_words=5)\nfor topic in topics:\n    print(topic)\nprint('\\nPerplexity: ', lda.log_perplexity(corpus))  \n# a measure of how good the model is. the lower the better.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These topics are clearly not giving us the good picture as they all contain similar words. need a solution for this"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}