{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Run once per session\n!pip install fastai -q --upgrade\nfrom fastai.vision.all import *","metadata":{"id":"0b55MJA5oB1u","outputId":"0112e628-8c96-4505-c615-a86b1519b0df","execution":{"iopub.status.busy":"2021-06-19T14:18:44.89581Z","iopub.execute_input":"2021-06-19T14:18:44.896448Z","iopub.status.idle":"2021-06-19T14:18:53.988381Z","shell.execute_reply.started":"2021-06-19T14:18:44.896398Z","shell.execute_reply":"2021-06-19T14:18:53.987369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = untar_data(URLs.PASCAL_2007)","metadata":{"id":"NxEIggy-oB10","outputId":"aaba606e-c9cf-44f3-bfd6-adbb74e84b2c","execution":{"iopub.status.busy":"2021-06-19T14:18:38.153386Z","iopub.execute_input":"2021-06-19T14:18:38.154284Z","iopub.status.idle":"2021-06-19T14:18:38.238416Z","shell.execute_reply.started":"2021-06-19T14:18:38.154231Z","shell.execute_reply":"2021-06-19T14:18:38.237258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now how do we get our labels? `fastai` has a `get_annotations` function that we can use to grab the image and their bounding box. The one-line documentation states:\n\"Open a COCO style json in `fname` and returns the list of filenames (with mabye `prefix`) and labelled bounding boxes.\"","metadata":{"id":"FM9FeIxboB13"}},{"cell_type":"code","source":"path.ls()","metadata":{"id":"VHmSV1cMoB13","outputId":"1830e30b-a225-47dc-8e10-fa1160d501dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll want to read out of the `train.json`","metadata":{"id":"9E5vCutmoB15"}},{"cell_type":"code","source":"imgs, lbl_bbox = get_annotations(path/'train.json')","metadata":{"id":"BLfU8hv8oB15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs[0]","metadata":{"id":"_zmQRxdzoB17","outputId":"e50f826d-5331-4455-fb85-97eb1125d2f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lbl_bbox[0]","metadata":{"id":"t0AprSfooB1-","outputId":"3fd5c0c5-375e-454b-de3c-7f8ea7897f1c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we want to be able to quickly look up a corresponding image to it's label. We'll use a dictionary","metadata":{"id":"wJHvjVsWoB2A"}},{"cell_type":"code","source":"img2bbox = dict(zip(imgs, lbl_bbox))","metadata":{"id":"tSWMCLRZoB2A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the first item","metadata":{"id":"aP2IcI1BoB2C"}},{"cell_type":"code","source":"first = {k: img2bbox[k] for k in list(img2bbox)[:1]}; first","metadata":{"id":"J96xwN-6oB2D","outputId":"baee7bb3-1772-4f2f-d1d5-de15e11465fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! Now let's build our `DataBlock`. We'll have two outputs, the bounding box itself and a label, with one input.","metadata":{"id":"CENSeAMVoB2E"}},{"cell_type":"code","source":"getters = [lambda o: path/'train'/o, lambda o: img2bbox[o][0], lambda o: img2bbox[o][1]]","metadata":{"id":"pw3xrPPyoB2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For our transforms, we'll use some of the ones we defined earlier","metadata":{"id":"7VBzZ1jAoB2H"}},{"cell_type":"code","source":"item_tfms = [Resize(128, method='pad'),]\nbatch_tfms = [Rotate(), Flip(), Dihedral(), Normalize.from_stats(*imagenet_stats)]","metadata":{"id":"V0ZPlZuRoB2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why do we need a custom `get_images`? Because we want our **images** that came back to us, not the entire folder","metadata":{"id":"E61v1CQoqjNS"}},{"cell_type":"code","source":"def get_train_imgs(noop):  return imgs","metadata":{"id":"tDX1YnuLqmTY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll now make our `DataBlock`. We want to adjust `n_inp` as we expect two outputs","metadata":{"id":"fHefiSB3oB2L"}},{"cell_type":"code","source":"pascal = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n                 splitter=RandomSplitter(),\n                 get_items=get_train_imgs,\n                 getters=getters,\n                 item_tfms=item_tfms,\n                 batch_tfms=batch_tfms,\n                 n_inp=1)","metadata":{"id":"2bY3RjdNoB2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = pascal.dataloaders(path/'train')","metadata":{"id":"QvI6b6aioB2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.c = 20","metadata":{"id":"-Es7TtheoB2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"id":"VPa2W_x1oB2R","outputId":"5cca2505-0a66-4961-ee7c-904744c34db3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model\n\nThe architecture we are going to use is called `RetinaNet`. I've exported this all myself for you guys to use quickly, if you want to explore what's going on in the code I'd recommend the Object Detection lesson [here](https://www.youtube.com/watch?v=Z0ssNAbe81M&t=4496s)","metadata":{"id":"qBn8v8sgoB2T"}},{"cell_type":"markdown","source":"Let's clone my repo and work out of it","metadata":{"id":"lcZDuQ1uoB2T"}},{"cell_type":"code","source":"!git clone https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0.git\n%cd \"Practical-Deep-Learning-for-Coders-2.0/Computer Vision\"","metadata":{"id":"iMxjDbNvoB2U","outputId":"79c9be5e-1959-495a-b769-f5e5ed072fbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imports import *","metadata":{"id":"nUsSJvE8oB2W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're still going to use transfer learning here by creating an `encoder` (body) of our model and a head","metadata":{"id":"m9y7IZP2oB2X"}},{"cell_type":"code","source":"encoder = create_body(resnet34, pretrained=True)","metadata":{"id":"i0CrU5CkoB2Y","outputId":"03f0935e-a5ce-4ac7-972f-12f86cbbd1ce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have our encoder, we can call the `RetinaNet` architecture. We'll pass in the encoder, the number of classes, and what we want our final bias to be on the last convolutional layer (how we initialize our model). Jeremy has his example at -4 so let's use this","metadata":{"id":"plv1hyUGoB2Z"}},{"cell_type":"code","source":"get_c(dls)","metadata":{"id":"PtPgStQtoB2a","outputId":"96520d75-e28d-448d-9036-dd66eea7a55f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arch = RetinaNet(encoder, get_c(dls), final_bias=-4)","metadata":{"id":"g00phNe2oB2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another big difference is the head of our model. Instead of our linear layers with pooling layers:","metadata":{"id":"YQzm_ni7oB2d"}},{"cell_type":"code","source":"create_head(124, 4)","metadata":{"id":"ECAKT4quoB2d","outputId":"d59662c8-9dd5-460b-8c6f-037842f9ef7c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have one with a smoother, a classifer, and a `box_regressor` (to get our points)","metadata":{"id":"GHpWUYCsoB2f"}},{"cell_type":"code","source":"arch.smoothers","metadata":{"id":"fORf5ZcPoB2f","outputId":"19e76f92-7860-406c-c089-0369a9eaf8b6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arch.classifier","metadata":{"id":"Zw9ypuWYoB2h","outputId":"79b3645b-5ec6-4d03-defb-ad4cdac4d08f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arch.box_regressor","metadata":{"id":"VOllk6w9oB2j","outputId":"e6eccb71-36c5-43ad-e7e6-22078f0c339e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function\nNow we can move onto our loss function. For RetinaNet to work, we need to define what the aspect ratio's and scales of our image should be. The paper used [1,2**(1/3), 2**(2/3)], but they also used an image size of 600 pixels, so even the largest feature map (box) gave anchors that covered less than the image. But for us it would go over. As such we will use -1/3 and -2/3 instead. We will need these for inference later!","metadata":{"id":"ePvMYCh4oB2n"}},{"cell_type":"code","source":"ratios = [1/2,1,2]\nscales = [1,2**(-1/3), 2**(-2/3)]","metadata":{"id":"ICN19VOnoB2n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make our loss function, which is `RetinaNetFocalLoss`","metadata":{"id":"xOukn6h4oB2p"}},{"cell_type":"code","source":"crit = RetinaNetFocalLoss(scales=scales, ratios=ratios)","metadata":{"id":"w1EgEMYQoB2p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's make our `Learner`!","metadata":{"id":"Z0A6DCu3oB2r"}},{"cell_type":"markdown","source":"We want to freeze our `encoder` and keep everything else unfrozen to start","metadata":{"id":"HBTmB2eUoB2r"}},{"cell_type":"code","source":"def _retinanet_split(m): return L(m.encoder,nn.Sequential(m.c5top6, m.p6top7, m.merges, m.smoothers, m.classifier, m.box_regressor)).map(params)","metadata":{"id":"xMYuwnR0oB2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, arch, loss_func=crit, splitter=_retinanet_split)","metadata":{"id":"E8kochl-oB2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.freeze()","metadata":{"id":"IDUhW4yAoB2v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train!","metadata":{"id":"vDZWhZlgoB2y"}},{"cell_type":"code","source":"learn.fit_one_cycle(10, slice(1e-5, 1e-4))","metadata":{"id":"yHBmZg-CoB21","outputId":"18e95aa9-0715-4b2e-91fd-7fad8f8d2e0e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ouxRYNyuoB23"},"execution_count":null,"outputs":[]}]}