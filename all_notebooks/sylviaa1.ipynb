{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Customer Churn at a Bank"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n\n# For the predictive models\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as GBSklearn\nfrom xgboost import XGBClassifier as XGB\nimport lightgbm as lgb\n\n# Removing annoying warnings\nimport sys\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.cross_validation import train_test_split # For splitting the data into training and testing\nfrom sklearn.neighbors import KNeighborsClassifier # K neighbors classification model\nfrom sklearn.naive_bayes import GaussianNB # Gaussian Naive bayes classification model\nfrom sklearn.svm import SVC # Support Vector Classifier model\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree Classifier model\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression model\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest Classifier model\nfrom sklearn.metrics import accuracy_score # For checking the accuracy of the model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the dataset\n\nReading the Churn Modelling dataset to get more insights on the data provided"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/predicting-churn-for-bank-customers/Churn_Modelling.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()  #To view the first rows inorder to understand the data provided","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the data provided and visualization."},{"metadata":{},"cell_type":"markdown","source":"### Some of the key questions from the data view\n1. Some customers exited with balance still on their account, what does this mean?\n2. What is the meaning of active customers? \n3. Does exiting mean exiting a product or?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() # Viewing columns and there data types","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the Exited column, I want to understand the customers that exited and the ones that didn't exit."},{"metadata":{"trusted":true},"cell_type":"code","source":"exited = len(data[data['Exited'] == 1]['Exited'])\nnot_exited = len(data[data['Exited'] == 0]['Exited'])\nexited_perc = round(exited/len(data)*100,1)\nnot_exited_perc = round(not_exited/len(data)*100,1)\n\nprint('Number of clients that have churned: {} ({}%)'.format(exited, exited_perc))\nprint('Number of clients that haven\\'t churned: {} ({}%)'.format(not_exited, not_exited_perc))\n\nlabels = 'Exited', 'Retained'\nsizes = [exited, not_exited]\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots(figsize=(8, 6))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')\nplt.title(\"Proportion of customer churned and retained\", size = 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From the above plotting, there are around 20% customers that exited from the provided dataset"},{"metadata":{},"cell_type":"markdown","source":"#### We need to understand the gender and location of those who exited and if they had a credit card or they were active members compared to those who didn't exit by visualizing the data in a bar graph\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"    fig, axarr = plt.subplots(2, 2, figsize=(20, 12))\n    sns.countplot(x='Geography', hue = 'Exited',data = data, ax=axarr[0][0])\n    sns.countplot(x='Gender', hue = 'Exited',data = data, ax=axarr[0][1])\n    sns.countplot(x='HasCrCard', hue = 'Exited',data = data, ax=axarr[1][0])\n    sns.countplot(x='IsActiveMember', hue = 'Exited',data = data, ax=axarr[1][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the visuals, most customers are from France though most churned customers are from German, also most customers are male but the most churned customers are female. Members who exit most are not active members even though some of the active members exit too."},{"metadata":{},"cell_type":"markdown","source":"### Age count for males and females\n#### The data represents different age groups, therefore I did a count on the age to see the age range where most of the customers fit****"},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = data.groupby(['Age','Gender']).count()\ncounts = counts.RowNumber\nprint(counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Histogram for the Age for both customers who exited or didn't"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stacked histogram: Age\nfigure = plt.figure(figsize=(15,8))\nplt.hist([\n        data[(data.Exited==0)]['Age'],\n        data[(data.Exited==1)]['Age']\n        ], \n         stacked=True, color = ['blue','r'],\n         bins = 'auto',label = ['Stayed','Exited'],\n         edgecolor='black', linewidth=1.2)\nplt.xlabel('Age (years)')\nplt.ylabel('Number of customers')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the histogram shown above, the highest number of customers are between their late 20's and 40 and the most customer's to exit are between their late 30's and 50. This might give you insights on what age group exit the program inorder to help you decide what is the reason for them to exit."},{"metadata":{},"cell_type":"markdown","source":"### Visuals for the remaining features in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (15,15))\nfig.subplots_adjust(left=0.2, wspace=0.6)\nax0, ax1, ax2, ax3 = axes.flatten()\n\nax0.hist([\n        data[(data.Exited==0)]['CreditScore'],\n        data[(data.Exited==1)]['CreditScore']\n        ], \n         stacked=True, color = ['blue','r'],\n         bins = 'auto',label = ['Stayed','Exited'],\n         edgecolor='black', linewidth=1.2)\nax0.legend()\nax0.set_title('Credit Score')\n\nax1.hist([\n        data[(data.Exited==0)]['Tenure'],\n        data[(data.Exited==1)]['Tenure']\n        ], \n         stacked=True, color = ['blue','r'],\n         bins = 'auto',label = ['Stayed','Exited'],\n         edgecolor='black', linewidth=1.2)\nax1.legend()\nax1.set_title('Tenure')\n\nax2.hist([\n        data[(data.Exited==0)]['Balance'],\n        data[(data.Exited==1)]['Balance']\n        ], \n         stacked=True, color = ['blue','r'],\n         bins = 'auto',label = ['Stayed','Exited'],\n         edgecolor='black', linewidth=1.2)\nax2.legend()\nax2.set_title('Balance')\n\nax3.hist([\n        data[(data.Exited==0)]['EstimatedSalary'],\n        data[(data.Exited==1)]['EstimatedSalary']\n        ], \n         stacked=True, color = ['blue','r'],\n         bins = 'auto',label = ['Stayed','Exited'],\n         edgecolor='black', linewidth=1.2)\nax3.legend()\nax3.set_title('Estimated Salary')\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estimated Salary does not seem to affect the churn rate and the customers who churn the most are between 600 and 700 credit score points"},{"metadata":{},"cell_type":"markdown","source":"### Dropping the irrelevant data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding correlation \nFinding how features correlates well with the target variable which is Exited."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr().T,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\nAge, Balance, NumOfProducts, IsActiveMember, CreditScore are the features with significant correlation but all features have a weak or strong correlation with the target therefore we will be using all of them to build a model."},{"metadata":{},"cell_type":"markdown","source":"## Building a Machine learning model"},{"metadata":{},"cell_type":"markdown","source":"### Preparing the data"},{"metadata":{},"cell_type":"markdown","source":"To convert categorical columns to numeric columns is by using one-hot encoding where we take our categories (France, Germany, Spain, male, female) and represent them with columns. In each column, we use a 1 to designate that the category exists for the current row, and a 0 otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot encoding our categorical attributes\nlist_cat = ['Geography', 'Gender']\ndata = pd.get_dummies(data, columns = list_cat, prefix = list_cat)\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing\nBefore we predict, we need to remove the target column from the dataset, features taking all the other feautures and target equating to the Exited column."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(data.drop(['Exited'], axis = 1))\ntarget = 'Exited'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the dataset into the Training set and Test set\nDividing the data into a training and test set,The train set will be used to train our machine learning model. The test set will evaluate how good our model is. I will use 20% of the data for the test set and the remaining 80% for the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(data, test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test[target])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choosing the best predictive model:"},{"metadata":{},"cell_type":"markdown","source":"To decide the best predictive model I will use Gaussian Naive bayes,K-nearest neighbors,Support vector classifier,Decision tree classifier,Random Forest and Logistic Regression to find out the most accurate model. The reason I am using the mentioned models is because\n1. Gaussian Naive bayes is a classification algorithm for binary and multi-class classification problems. The technique is easiest to understand when described using binary or categorical input values.\n2. K-nearest neighbors is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\n3. Support vector classifier can also be used for both classification or regression challenges.\n4. Decision tree classifier is a supervised Machine Learning where the data is continuously split according to a certain parameter.\n5. Random Forest consists of a large number of individual decision trees that operate as an ensemble\n6. Logistic Regression is as simple as plugging in numbers into the logistic regression equation and calculating a result when making a prediction.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a python list containing all defined models\nmodel = [GaussianNB(), KNeighborsClassifier(), SVC(), DecisionTreeClassifier(), RandomForestClassifier(n_estimators=5, random_state=0), LogisticRegression()]\nmodel_names = [\"Gaussian Naive bayes\", \"K-nearest neighbors\", \"Support vector classifier\", \"Decision tree classifier\", \"Random Forest\", \"Logistic Regression\",]\nfor i in range(0, 6):\n    y_pred = model[i].fit(train[features], train[target]).predict(test[features])\n    accuracy = accuracy_score(y_pred, test[target])*100\n    print(model_names[i], \":\", accuracy, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best ML algorithm with the highest accuracy is Random forest with 83.85%, therefore we will use this algorithm to make presictions.\nTo train this algorithm, we call the fit method and pass in the feature set and the corresponding target set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Working with the Random Forest model\nmodel = RandomForestClassifier(n_estimators = 100, random_state = 0)\ny_pred = model.fit(train[features], train[target]).predict(test[features])\nprint(\"The accuracy is:\", accuracy_score(y_pred, test[target])*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How well it works?\nEvaluating how well the above model works using F1-score, precision, recall, and accuracy\n\nAccuracy is used predicting actual positives and false positives, precision is true positives divided by the sum of true positives and false positives, recall is true positives divided by the sum of true positives and false negatives and finally F1-score is needed when you want to seek a balance between Precision and Recall.","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test[target],y_pred ))  \nprint(accuracy_score(test[target], y_pred ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This explains that our algorithm successfully predicts customer churn 86.3%. "},{"metadata":{},"cell_type":"markdown","source":"#### Feature Evaluation:\nTo conclude we want to see which features plays a great role in identifying customer exit."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(model.feature_importances_, index=features)\nfeat_importances.nlargest(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age plays a big role in customer's exit followed by estimated salary, credit score, balance and number of products."},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\nIn conclusion, The model created above is 86.3% accurate to predict the customer's churn, and the feature that plays a big role in customers churn is age. Although I believe the accuracy of this model can be improved adding more different features or collecting relevant data from more customers."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}