{"cells":[{"outputs":[],"source":"I am using the essemble method to classify the mushrooms given the data set. I will be using a selection of Bagging and Boosting methods and each method will be tested using a 10 fold validation. \n\nDo vote for this if you like this analysis :)","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"7c765f3b2e871ccc11528612fd6a8ccfd88a1a72"}},{"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport time","cell_type":"code","execution_count":null,"metadata":{"_uuid":"6043a3a942c7026ac898ccf8222e607d89f7f523","collapsed":true}},{"outputs":[],"source":"## Exploratory analysis and data pre-processing\n\nLoad the dataset and do some quick exploratory analysis. It seems that many of the entries are encoded in alphabets. To make the data useful for machine learning, we need to convert them into integers.","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"2b42488d77c0940618459f33000b720982365890"}},{"outputs":[],"source":"data = pd.read_csv('../input/mushrooms.csv', index_col=False)\ndata.head(5)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"2401312491c387285715f5150b6b6688179d0ee7"}},{"outputs":[],"source":"print(data.shape)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"fbbf4fd11b7aa7542f9cdc7701c015d2801fecef"}},{"outputs":[],"source":"data.describe()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"8faefdedc92b1fa74fec66374f839b479b108e56"}},{"outputs":[],"source":"encoder = LabelEncoder()\n\nfor col in data.columns:\n    data[col] = encoder.fit_transform(data[col])\n \ndata.head()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"176ab554675be75fd096a87026738d0b1df89363"}},{"outputs":[],"source":"Let's take a look at the number of Poisonous (1) and Edible (0) cases from the dataset. From the output shown below, majority of the cases are Edible (0).","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"8b114f291e2200130fb1fcbb5fed430ea1719d67"}},{"outputs":[],"source":"print(data.groupby('class').size())","cell_type":"code","execution_count":null,"metadata":{"_uuid":"a1f8d413b1ba4bc33a34d3fd068bf0df4a83009b"}},{"outputs":[],"source":"Finally, we'll split the data into predictor variables and target variable, following by breaking them into train and test sets. We will use 30% of the data as test set.","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"2dabb0528ca2485024d86bbaa577ab6f420acf71"}},{"outputs":[],"source":"Y = data['class'].values\nX = data.drop('class', axis=1).values\n\nX_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.30, random_state=21)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"e4f492715f520cd10e8d6c6fefc990db66e802c0","collapsed":true}},{"outputs":[],"source":"## Essemble algorithm checking\n\n\nI evaluate four different ensemble machine learning algorithms, two Boosting and two Bagging methods:\n **Boosting Methods**: AdaBoost (AB) and Gradient Boosting (GBM).\n **Bagging Methods**: Random Forests (RF) and Extra Trees (ET).\n\nI did not standardize the training data (use the data as it is), and do a 10-fold cross-validation for each algorithm.","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"6db517e677941cac64ad96ad47c7cb21f7fe90c6"}},{"outputs":[],"source":"# ensembles\nensembles = []\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\nensembles.append(('RF', RandomForestClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))","cell_type":"code","execution_count":null,"metadata":{"_uuid":"0ad5b0c766a55b98cf39b914eddb43b57ab7aa55"}},{"outputs":[],"source":"import warnings\n\nresults = []\nnames = []\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for name, model in ensembles:\n        kfold = KFold(n_splits=10, random_state=21)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"22b0fa8c9dd407556cbd1e53bbe1a407800db045"}},{"outputs":[],"source":"Interestingly, all the algorithms hit 100% accuracy in the test! It would be interesting to observe the performance in detail. But for now, I will just pick RandomForestClassifier to validate with the test set.","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"bcd82a2af205c1712d7545531fb020ad064afdfc"}},{"outputs":[],"source":"# prepare the model\nmodel = RandomForestClassifier(random_state=21, n_estimators=100) \nmodel.fit(X_train, Y_train)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"e72c55608b0e1d2cf19fd8a5b80b7e3b5f77ca80"}},{"outputs":[],"source":"predictions = model.predict(X_test)\nprint(\"Accuracy score %f\" % accuracy_score(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","cell_type":"code","execution_count":null,"metadata":{"_uuid":"03ba9400db8e1abffb7a892732a419450e9458ed"}},{"outputs":[],"source":"print(confusion_matrix(Y_test, predictions))","cell_type":"code","execution_count":null,"metadata":{"_uuid":"cbf7e8d5d66b9700d553e1aa833daefa84ade973"}},{"outputs":[],"source":"## Conclusion\n\nIt looks like the essemble method is able to achieve 100% accuracy for this data set. From the confusion matrix shown, there is no single misclassification observed. Personally, I am a little skeptical of this analysis. I will want to investigate this further. Meanwhile, let me know if you have any comments :)","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"081f15e2407373717689757218d901ee777cebac"}},{"outputs":[],"source":"","cell_type":"code","execution_count":null,"metadata":{"_uuid":"9867e471caddf8523ce1c6ac14592c2e100f1976","collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"coursera":{"launcher_item_id":"oxndk","graded_item_id":"f9SY5","course_slug":"python-machine-learning","part_id":"mh1Vo"},"language_info":{"file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}