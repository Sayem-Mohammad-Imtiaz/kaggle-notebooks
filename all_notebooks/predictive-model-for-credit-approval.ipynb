{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predictive model for credit approval\nCredit score models calculate the probability of default and are one of the most main tools used by several companies to approve or deny credit.\n\nDescription:\n\nEach row represents a customer and the columns represent the data (information) for those customers.\nThe response variable is the defaulting column, which indicates whether the customer has become defaulting (1) or not (0).\nThe variables are described below:\n\n\n- ```ìdade```: The age of the customer\n- ```numero_de_dependentes```: The number of people dependent on the customer.\n- ```salario_mensal```: Monthly salary of the client.\n- ```numero_emprestimos_imobiliarios```: Number of real estate loans that the customer has open.\n- ```numero_vezes_passou_90_dias```: Number of times the policyholder spent more 90 days overdue.\n- ```util_linhas_inseguras```: How much the customer is using in relation to their credit limit, on lines that are not secured by personal assets, such as real estate and cars.\n- ```vezes_passou_de_30_59_dias```:  Number of times the customer delayed the payment of a loan, (between 30 and 59 days).\n- ```razao_debito```: Ratio between debts and the borrower's equity. debt ratio = Debts / Equity\n- ```numero_linhas_crdto_aberto```: Number of loans outstanding by the customer.\n- ```number_of_ numero_de_vezes_que_passou_60_89_dias```: Number of times the customer delayed the payment of a loan, (between 60 and 89 days).\n"},{"metadata":{},"cell_type":"markdown","source":"Acknowledgments\n\nLigthGBM Simple fe by [@caesarlupum](https://www.kaggle.com/caesarlupum/ashrae-ligthgbm-simple-fe), Brazil against the advance of Covid-19 by [@caesarlupum](https://www.kaggle.com/caesarlupum/brazil-against-the-advance-of-covid-19), eda and prediction by [@gpreda Introduction](https://www.kaggle.com/gpreda/santander-eda-and-prediction)."},{"metadata":{},"cell_type":"markdown","source":"### Loading Required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.metrics import roc_auc_score,precision_recall_curve,roc_curve\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\nimport itertools\nfrom datetime import datetime\nfrom scipy import interp\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\n\npd.set_option('max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Training data\ntrain = pd.read_csv('/kaggle/input/risco-de-credito/treino.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data\ntest = pd.read_csv('/kaggle/input/risco-de-credito/teste.csv')\nprint('Test data shape: ', test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lgb_ = train.copy()\ntarget = train['inadimplente']\ndf_lgb = train.drop(['inadimplente'], axis=1)\ntrain_df = train.copy()\nx = train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Examine the Distribution of the Target Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train['inadimplente'].value_counts().values\nsns.barplot([0,1],x)\nplt.title('Target variable count')\n\nprint(\"There are {}% target values with 1\".format(100 * train['inadimplente'].value_counts()[1]/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is unbalanced with respect with target value."},{"metadata":{},"cell_type":"markdown","source":"### Checking missing data in train\n\nnumber and percentage of missing values in each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"total = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()/train.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"missing values: salario_mensal\t```19.78%``` and numero_de_dependentes ```2.61%```"},{"metadata":{},"cell_type":"markdown","source":"### Column Types"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"numeric variables 7 ```int64``` and 4 ```float64``` (which can be either discrete or continuous)."},{"metadata":{},"cell_type":"markdown","source":"### Duplicate values\nLet's now check how many duplicate values exists per columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train.columns.values[1:11]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same columns in train and test set have very close number of duplicates of same or very close values. \nThis is an interesting pattern that we might be able to use in the future"},{"metadata":{},"cell_type":"markdown","source":"### Correlations\n\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n\n- .00-.19 “very weak”\n- .20-.39 “weak”\n- .40-.59 “moderate”\n- .60-.79 “strong”\n- .80-1.0 “very strong”"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = train.corr()['inadimplente'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(5))\nprint('\\nMost Negative Correlations:\\n', correlations.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"more significant correlations: the **vezes_passou_de_30_59_dias** is the most positive correlation;\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_train = train.corr()\nplt.figure(figsize = (14, 10))\n# Heatmap of correlations\nsns.heatmap(corr_train, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```numero_vezes_passou_90_dias```, ```vezes_passou_de_30_59_dias```, ````numero_de_vezes_que_passou_60_89_dias``` are very correlated. expected correlation ;)\n\n```numero_emprestimos_imobiliarios``` has medium correlation with ```salario_mensal:``` indicate that people with a high salary have more loans."},{"metadata":{},"cell_type":"markdown","source":"##### Age informative plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n# Plot the distribution of ages in years\nplt.hist(train['idade'], edgecolor = 'k')\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');\nprint('min age {} max age {}'.format(train['idade'].min(), train['idade'].max()))\nprint('age <20 {}, age >99 {}'.format(len(train[train['idade']>99]),len(train[train['idade']<20])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have some inconsisent values. age equal 0 for example. We need drop these rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\n# KDE plot of loans that were repaid on time\nsns.kdeplot(train.loc[train['inadimplente'] == 0, 'idade'], label = 'target == 0')\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(train.loc[train['inadimplente'] == 1, 'idade'], label = 'target == 1')\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Average failure to repay loans by age bracket."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = train[['inadimplente', 'idade']]\n# Bin the age data\nage_data['age_binned'] = pd.cut(age_data['idade'], bins = np.linspace(20, 60, num = 6))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('age_binned').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['inadimplente'])\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest two age groups 20-28, 28-36.\n\n\nThis is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.kdeplot(train.loc[train['inadimplente'] == 0, 'numero_linhas_crdto_aberto'], label = 'target == 0')\nsns.kdeplot(train.loc[train['inadimplente'] == 1, 'numero_linhas_crdto_aberto'], label = 'target == 1')\nplt.xlabel('number of open credit lines'); plt.ylabel('Density'); plt.title('Distribution of number of open credit lines');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.kdeplot(train.loc[train['inadimplente'] == 0, 'numero_emprestimos_imobiliarios'], label = 'target == 0')\nsns.kdeplot(train.loc[train['inadimplente'] == 1, 'numero_emprestimos_imobiliarios'], label = 'target == 1')\nplt.xlabel('number real estate loans'); plt.ylabel('Density'); plt.title('Distribution of number real estate loans');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Distribution Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dist_col(column, train, test):\n    '''plot dist curves for train and test  data for the given column name'''\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.distplot(train[column].dropna(), color='green', ax=ax).set_title(column, fontsize=10)\n    sns.distplot(test[column].dropna(), color='purple', ax=ax).set_title(column, fontsize=10)\n    plt.xlabel(column, fontsize=12)\n    plt.legend(['train', 'test'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist_col('util_linhas_inseguras', train, test)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist_col('idade', train, test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist_col('razao_debito', train, test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot_dist_col('salario_mensal', train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist_col('numero_emprestimos_imobiliarios', train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist_col('salario_mensal', train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist_col('numero_de_dependentes', train, test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general we have similar distribution in dataset, but **util_linhas_inseguras**, **numero_emprestimos_imobiliarios**,**salario_mensal**  have higher values in test data."},{"metadata":{},"cell_type":"markdown","source":"# Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns = ['inadimplente'])\n# Feature names\nfeatures = list(train.columns)\n# Median imputation of missing values\nimputer = SimpleImputer(strategy = 'median')\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auc_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Area Under ROC Curve (AUC)\n    \"\"\"\n    return roc_auc_score(y_true, y_pred)\ndef plot_curve(y_true_train, y_pred_train, y_true_val, y_pred_val, model_name):\n    \"\"\"\n    Plots the ROC Curve given predictions and labels\n    \"\"\"\n    fpr_train, tpr_train, _ = roc_curve(y_true_train, y_pred_train, pos_label=1)\n    fpr_val, tpr_val, _ = roc_curve(y_true_val, y_pred_val, pos_label=1)\n    plt.figure(figsize=(8, 8))\n    plt.plot(fpr_train, tpr_train, color='black',\n             lw=2, label=f\"ROC train curve (AUC = {round(roc_auc_score(y_true_train, y_pred_train), 4)})\")\n    plt.plot(fpr_val, tpr_val, color='darkorange',\n             lw=2, label=f\"ROC validation curve (AUC = {round(roc_auc_score(y_true_val, y_pred_val), 4)})\")\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.title(f'ROC Plot for {model_name}', weight=\"bold\", fontsize=20)\n    plt.legend(loc=\"lower right\", fontsize=16)\ndef plot_pre_curve(y_test,probs):\n    precision, recall, thresholds = precision_recall_curve(y_test, probs)\n    plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n    # plot the precision-recall curve for the model\n    plt.plot(recall, precision, marker='.')\n    plt.title(\"precision recall curve\")\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # show the plot\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train, target,\n                                                  test_size=0.30, \n                                                  random_state=2020, \n                                                  stratify=target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n# Train on the training data\nlog_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get score on training set and validation set for Logistic Regression\ntrain_preds = log_reg.predict_proba(X_train)[:, 1]\nval_preds = log_reg.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Evaluation Logistic Regression ROC_AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"Logistic Regression Baseline\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pre_curve(y_val ,val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification report train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Improved Model: Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 2020, verbose = 1, n_jobs = -1)\n# Train on the training data\nrandom_forest.fit(X_train,y_train)\n    # Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Get score on training set and validation set for random forest\ntrain_preds = random_forest.predict_proba(X_train)[:, 1]\nval_preds = random_forest.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"Random Forest Baseline\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pre_curve(y_val ,val_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classification report train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is great, the model is accurate! just 5 FP of 71863, 48 FN of 5084."},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lgbm\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(df_lgb.shape[1])\ntrain_weight = 1-y_train.replace(y_train.value_counts()/len(y_train))\npositive_weight = train_weight[y_train==1].values[0]\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 5000, class_weight = 'balanced', scale_pos_weight= positive_weight)\n    \n# Train using early stopping\nmodel.fit(X_train, y_train, early_stopping_rounds=100, eval_set = [(X_val, y_val)], \n              eval_metric = 'auc', verbose = 200)\n    \n    # Record the feature importances\nfeature_importances += model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get score on training set and validation set for random forest\ntrain_preds = model.predict_proba(X_train)[:, 1]\nval_preds = model.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"LGBM Classifier\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average feature importances!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make sure to average feature importances! \nfeature_importances = feature_importances / 2\nfeature_importances = pd.DataFrame({'feature': list(df_lgb.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**razao_debito** and **util_linhas_inseguras** are the most important features for our model !"},{"metadata":{},"cell_type":"markdown","source":"Find the features with zero importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the features with zero importance\nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we not have features that have zero importance. Nice !\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 10 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for prining information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 15\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:10]))), \n            df['importance_normalized'].head(10), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:10]))))\n    ax.set_yticklabels(df['feature'].head(10))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_feature_importances = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nfrom scipy import interp\ndef gradient_boosting_model(params, folds, test_df, model='LGB',stack = False):    \n    print(str(model)+' modeling...')\n    start_time = timer(None)\n    plt.rcParams[\"axes.grid\"] = True\n    nfold = folds\n    skf = StratifiedKFold(n_splits=nfold, shuffle=False, random_state=44400)\n\n    oof = np.zeros(len(train_df))\n    mean_fpr = np.linspace(0,1,100)\n    cms= []\n    tprs = []\n    aucs = []\n    y_real = []\n    y_proba = []\n    recalls = []\n    roc_aucs = []\n    f1_scores = []\n    accuracies = []\n    precisions = []\n    feature_importance_df = pd.DataFrame()\n    predictions = np.zeros(len(test_df))\n\n    i = 1\n    for train_idx, valid_idx in skf.split(train_df, train_df['inadimplente'].values):\n        print(\"\\nfold {}\".format(i))\n        \n        if model == 'LGB':\n        \n            trn_data = lgb.Dataset(train_df.iloc[train_idx][features].values,\n                                   label=train_df.iloc[train_idx]['inadimplente'].values\n                                   )\n            val_data = lgb.Dataset(train_df.iloc[valid_idx][features].values,\n                                   label=train_df.iloc[valid_idx]['inadimplente'].values\n                                   )   \n\n            clf = lgb.train(param_lgb, trn_data, num_boost_round=1000,  valid_sets = [trn_data, val_data], verbose_eval=800, early_stopping_rounds = 10000)\n            oof[valid_idx] = clf.predict(train_df.iloc[valid_idx][features].values) \n  \n            predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / skf.n_splits\n    \n        if model == 'XGB':\n\n            trn_data = xgb.DMatrix(train_df.iloc[train_idx][features], \n                                   label=train_df.iloc[train_idx]['inadimplente'].values)\n            val_data = xgb.DMatrix(train_df.iloc[valid_idx][features], \n                                   label=train_df.iloc[valid_idx]['inadimplente'].values)\n\n            watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n\n            clf = xgb.train(params, dtrain = trn_data, evals=watchlist, early_stopping_rounds=1000, maximize=True, verbose_eval=800)\n            oof[valid_idx] = clf.predict(val_data, ntree_limit=clf.best_ntree_limit)\n            \n            test_xgb = xgb.DMatrix(test_df[features])\n            predictions += clf.predict(test_xgb, ntree_limit=clf.best_ntree_limit) / skf.n_splits\n        \n        # Scores \n        roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx]))\n        accuracies.append(accuracy_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        recalls.append(recall_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        precisions.append(precision_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        f1_scores.append(f1_score(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n\n        # Roc curve by folds\n        f = plt.figure(1)\n        fpr, tpr, t = roc_curve(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n\n        # Precion recall by folds\n        g = plt.figure(2)\n        precision, recall, _ = precision_recall_curve(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx])\n        y_real.append(train_df.iloc[valid_idx]['inadimplente'].values)\n        y_proba.append(oof[valid_idx])\n        plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n\n        i= i+1\n        \n        # Confusion matrix by folds\n        cms.append(confusion_matrix(train_df.iloc[valid_idx]['inadimplente'].values, oof[valid_idx].round()))\n        \n        # Features imp\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"Feature\"] = features\n        if model == 'LGB':\n            fold_importance_df[\"importance\"] = clf.feature_importance()\n        fold_importance_df[\"fold\"] = nfold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)    \n\n    # Metrics\n    print(\n            '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n            '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n            '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n            '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n            '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n    )\n    \n    # Roc plt\n    f = plt.figure(1)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(str(model)+' ROC curve by folds')\n    plt.legend(loc=\"lower right\")\n    \n    # PR plt\n    g = plt.figure(2)\n    plt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\n    y_real = np.concatenate(y_real)\n    y_proba = np.concatenate(y_proba)\n    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n    plt.plot(recall, precision, color='blue',\n             label=r'Mean P|R')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(str(model)+' P|R curve by folds')\n    plt.legend(loc=\"lower left\")\n\n    # Confusion maxtrix\n    plt.rcParams[\"axes.grid\"] = False\n    cm = np.average(cms, axis=0)\n    class_names = [0,1]\n    plt.figure()\n    plot_confusion_matrix(cm, \n                          classes=class_names, \n                          title= str(model).title()+' Confusion matrix [averaged/folds]')\n    \n    # Feat imp plt\n    if model != 'XGB':\n        cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:30].index)\n        best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n        plt.figure(figsize=(10,10))\n        sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n                edgecolor=('white'), linewidth=2, palette=\"rocket\")\n        plt.title(str(model)+' Features importance (averaged/folds)', fontsize=18)\n        plt.tight_layout()\n        \n    # Timer end    \n    timer(start_time)\n    \n    return predictions\n    \n#Timer\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken for Modeling: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nparam_xgb = {\n            'n_jobs' : -1, 'n_estimators' : 500, 'seed' : 4040,\n            'random_state':404, 'eval_metric':'auc' }\n# Test data\nsub_df = pd.read_csv('/kaggle/input/risco-de-credito/teste.csv')\npreds_xgb = gradient_boosting_model(param_xgb, 10, sub_df, 'XGB')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting Model function"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_lgb = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'class_weight':'unbalanced',\n    'scale_pos_weight':positive_weight,    \n    'objective': 'binary', \n    'verbosity': 1\n}\n\n# Test data\nsub_df = pd.read_csv('/kaggle/input/risco-de-credito/teste.csv')\n\npredictions = gradient_boosting_model(param_lgb, 10, sub_df, 'LGB')\nsub_df[\"inadimplente\"] = predictions.round()\nsub_df[\"inadimplente score\"] = predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creation final test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"final_test.csv\", index=False)\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defaulting-Score for customers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def severity_validation(df):\n    df['defaulting-score'] = \"None\"\n    for i, row in df.iterrows():\n        if row['inadimplente'] <0.5:\n            df['defaulting-score'][i] = \"low-defaulting-score\"\n        elif row['inadimplente'] <=0.75:\n            df['defaulting-score'][i] = \"medium-defaulting-score\" \n        else:\n            df['defaulting-score'][i] = \"high-defaulting-score\" \n    return df\n\ncustomer_df= pd.DataFrame(predictions, columns=['inadimplente'])\n\ncustomer_score=severity_validation(customer_df)\ncustomer_score['inadimplente']\nq50, q25 = np.percentile(customer_score['inadimplente'], [50 ,25])\nq75, q25 = np.percentile(customer_score['inadimplente'], [75 ,25])\nq100, q75 = np.percentile(customer_score['inadimplente'], [100 ,75])\n\niqr_50 = q50 - q25\niqr_75 = q75 - q50\niqr_100 = q100 - q75\niqr_75_25 = q75 - q25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_score['inadimplente result'] = predictions.round()\nprint(\"minimum defaulting prob \",customer_score[customer_score['defaulting-score']=='high-defaulting-score']['inadimplente'].min())\nprint(\"maximum defaulting prob \",customer_score[customer_score['defaulting-score']=='high-defaulting-score']['inadimplente'].max())\n\ncustomer_score[customer_score['defaulting-score']=='high-defaulting-score'].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have how to identify high-defaulting customers, the customer can be assigned a \"defaulting-score\" based on the predicted label such that:\n\n- Low-defaulting-score for Customers with label < 0.50\n- Medium-defaulting-score Score for Customers with label between 0.5 and 0.75\n- High-defaulting-score Score for Customers with label > 0.75"},{"metadata":{},"cell_type":"markdown","source":"### Factory Analysis in defaulting customers\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install library \n!pip install factor_analyzer\n# import factor analyzer library\nfrom factor_analyzer import FactorAnalyzer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fa = FactorAnalyzer()\nfa.fit(sub_df, 10)\n\nev, v = fa.get_eigenvalues()\n\n# Create scree plot using matplotlib\nplt.figure(figsize=(25,10))\nplt.scatter(range(1,sub_df.shape[1]+1),ev)\nplt.plot(range(1,sub_df.shape[1]+1),ev)\nplt.hlines(1, 0, sub_df.shape[1], colors='r')\nplt.title('Scree Plot')\nplt.xlabel('Factors')\nplt.ylabel('Eigenvalue')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 factors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform Factor Analysis\nfa = FactorAnalyzer(n_factors=3, rotation='varimax')\nfa.fit(sub_df)\nloads = fa.loadings_\nloads = pd.DataFrame(loads, index=sub_df.T.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap of loadings\nplt.figure(figsize=(30,25))\nsns.heatmap(loads, annot=True, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Factors \nThe factors represents high correlated variables. We just considered factor loadings >35 in each factor. \nLet's analyze defaulting customers and group by the highest scores(factor loadings) for each factor.\n\n- factor 1 (**customer delaying payment.**) - vezes_passou_de_30_59_dias, numero_vezes_passou_90_dias, numero_de_vezes_que_passou_60_89_dias.\n- factor 2 (**customer with many open loans**) - numero_linhas_crdto_aberto,numero_emprestimos_imobiliarios.\n- factor 3 (**young customer with few dependents**) - idade, numero_de_dependentes.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_factor1(df, df_factor_analysis, target, name_target):\n    df['score_factor1_target'] = df[df['inadimplente']==target]['vezes_passou_de_30_59_dias'] * df_factor_analysis.T['vezes_passou_de_30_59_dias'][0] \\\n    + df[df[name_target]==target]['numero_vezes_passou_90_dias'] * df_factor_analysis.T['numero_vezes_passou_90_dias'][0] \\\n    + df[df[name_target]==target]['numero_de_vezes_que_passou_60_89_dias'] * df_factor_analysis.T['numero_de_vezes_que_passou_60_89_dias'][0]        \n\nscore_factor1(sub_df, loads, 0, 'inadimplente')       \nscore_factor1(sub_df, loads, 1, 'inadimplente')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_factor2(df, df_factor_analysis, target, name_target):\n    df['score_factor2_target'] = df[df['inadimplente']==target]['numero_linhas_crdto_aberto'] * df_factor_analysis.T['numero_linhas_crdto_aberto'][1] \\\n    + df[df[name_target]==target]['numero_emprestimos_imobiliarios'] * df_factor_analysis.T['numero_emprestimos_imobiliarios'][1]  \n\nscore_factor2(sub_df, loads, 0, 'inadimplente')       \nscore_factor2(sub_df, loads, 1, 'inadimplente')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_factor3(df, df_factor_analysis, target, name_target):\n    df['score_factor3_target'] = df[df['inadimplente']==target]['idade'] * df_factor_analysis.T['idade'][2] \\\n    + df[df[name_target]==target]['numero_de_dependentes'] * df_factor_analysis.T['numero_de_dependentes'][2]   \n\nscore_factor3(sub_df, loads, 0, 'inadimplente')       \nscore_factor3(sub_df, loads, 1, 'inadimplente')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"customers with high 'inadimplente score' and high score for the Factor 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[['score_factor1_target', 'inadimplente score' ]].sort_values(by=['score_factor1_target', 'inadimplente score'],ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"customers with high 'inadimplente score' and high score for the Factor 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[['score_factor2_target', 'inadimplente score' ]].sort_values(by=['score_factor2_target', 'inadimplente score'],ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"customers with high 'inadimplente score' and high score for the Factor 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[['score_factor3_target', 'inadimplente score' ]].sort_values(by=['score_factor3_target', 'inadimplente score'],ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End Notebook"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}