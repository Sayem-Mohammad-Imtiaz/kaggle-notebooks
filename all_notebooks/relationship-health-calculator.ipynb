{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing required packages"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport scipy as sp\nimport math\n\nfrom sklearn.decomposition import PCA\nfrom scipy.linalg import svd\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_samples","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load data and EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"divorce = pd.read_csv(\"../input/divorce-predictors-data-set-csv/divorce-csv.csv\")\ndivorce.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"divorce_data = divorce.values[:,0:54]\nY = divorce.values[:,54]\ndivorce_df = DataFrame(divorce_data)\ncov = divorce_df.cov()\ncorr = divorce_df.corr()\n\ncmap=sns.diverging_palette(250, 5, as_cmap=True)\ncorr.style.background_gradient(cmap, axis=1)\\\n.set_properties(**{'max-width': '80px', 'font-size': '10pt'}).set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most attributes are very highly correlated to each other which could mean they all are conveying\nthe similar information. For example, Atr 5, Atr 8 and Atr 9 have a strong correlation value of 0.92.\n\nAtr 5: The time I spent with my wife is special for us. Atr 8: I enjoy our holidays with my wife. Atr 9: I\nenjoy traveling with my wife.\nIt's clear these are all providing very similar information.\n\nAttributes which are in blue shades i.e. not strongly correlated to other attributes. But between\nthese attributes, we can notice strong correlation. For example, Atr 42 and 43 are not correlated to\nother attributes but between them there is fairly strong correlation of 0.72\n\nA 42: When I argue with my spouse, Ä± only go out and I don't say a word. A 43: I mostly stay silent\nto calm the environment a little bit. A 44: Sometimes I think it's good for me to leave home for a\nwhile.\n\nThis is a strong evidence of presence of principal components and we can transform this dataset\nto much smaller dimension"},{"metadata":{"trusted":true},"cell_type":"code","source":"w,v = np.linalg.eig(corr)\nplt.bar(np.arange(54),np.cumsum(w)/np.sum(w))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first 2 PC capture around 80% of the variance and individual contributions from every other\ncomponent is very small and hence not considered."},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LinearDiscriminantAnalysis(n_components = 2)\nplt.figure(figsize=(20,10))\nax = sns.heatmap(divorce_data,cmap='BuPu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_r1 = lda.fit(divorce_data[:,0:30],Y).transform(divorce_data[:,0:30])\nsns.heatmap(X_r1,cmap = 'BuPu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = np.arange(170), y = X_r1[:,0],hue = Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proj_data_1 = np.dot(divorce_data,v[:,0:2])\nsns.scatterplot(proj_data_1[:,0],proj_data_1[:,1],hue = Y)\n\ndivorced = proj_data_1[Y==1]\ntogether = proj_data_1[Y!=1]\nprint(divorced.shape)\nprint(together.shape)\nsns.scatterplot(divorced[:,0],divorced[:,1])\nsns.scatterplot(together[:,0],together[:,1])\n\nplt.figure(figsize=(8,4))\nplt.scatter(proj_data_1[:,0],proj_data_1[:,1],c = Y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors \nfrom random import sample \nfrom numpy.random import uniform \nfrom math import isnan\ndef hopkins(X):\n    n=X.shape[0]#rows\n    d=X.shape[1]#cols\n    p=int(0.1*n)#considering 10% of points\n    nbrs=NearestNeighbors(n_neighbors=1).fit(X)\n    \n    rand_X=sample(range(0,n),p)\n    uj=[]\n    wj=[]\n    for j in range(0,p):\n        u_dist,_=nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1,-1),2,return_distance=True)\n        uj.append(u_dist[0][1])#distances to nearest neighbors in random data\n        w_dist,_=nbrs.kneighbors(X[rand_X[j]].reshape(1,-1),2,return_distance=True)\n        wj.append(w_dist[0][1])#distances to nearest neighbors in real data\n    H=sum(uj)/(sum(uj)+sum(wj))\n    if isnan(H):\n        print(uj,wj)\n        H=0\n        \n    return H\n\nprint(hopkins(proj_data_1))\nprint(hopkins(divorced))\nprint(hopkins(together))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 0\nscore = np.zeros(20);\nfor i in range(2,20):\n    kmeans = KMeans(n_clusters=i, random_state=random_state)\n    kmeans.fit_predict(divorced)  \n    score[i] = -kmeans.score(divorced) \n    print(\"SSE Score for k=\",i,\":\", round(score[i],2))\n\nplt.plot(range(2,20),score[2:20])\nplt.scatter(range(2,20),score[2:20])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a bend at k = 4 which means the drop in SSE is minimal after k = 4. Hence we use k = 4 as number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 4\nrandom_state = 3\nkmeans = KMeans(n_clusters=n_clusters, random_state=random_state);\ny_pred = kmeans.fit_predict(divorced)\nplt.scatter(divorced[:, 0], divorced[:, 1], c=y_pred)  # KMeans clusters\nplt.title('K-Means cluster labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For both Divorced and Together datasets, clusters isn't looking distinct enough. I will try other clustering techniques or reduce cluster count to improve this.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"complete_linkage = AgglomerativeClustering(linkage=\"complete\", n_clusters=n_clusters)\ny_pred = complete_linkage.fit_predict(divorced)\n\nplt.scatter(divorced[:, 0], divorced[:, 1], c=y_pred)\nplt.title('Complete link cluster labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_linkage = AgglomerativeClustering(linkage=\"average\", n_clusters=n_clusters)\ny_pred = average_linkage.fit_predict(divorced)\n\nplt.scatter(divorced[:, 0], divorced[:, 1], c=y_pred)\nplt.title('Average link cluster labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spectral = SpectralClustering(n_clusters=4,affinity = 'rbf',n_neighbors = 10,random_state=random_state)\ny_pred = spectral.fit_predict(divorced)\nplt.scatter(divorced[:, 0], divorced[:, 1], c=y_pred)\nplt.title('Spectral link cluster labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def silhouette(X,labels):\n    n_clusters=np.size(np.unique(labels));\n    sample_silhouette_values=silhouette_samples(X,labels)\n    y_lower=10\n    for i in range(n_clusters):\n        ith_cluster_silhouette_values=sample_silhouette_values[labels==i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i=ith_cluster_silhouette_values.shape[0]\n        y_upper=y_lower+size_cluster_i\n        color=cm.nipy_spectral(float(i)/n_clusters)\n        plt.fill_betweenx(np.arange(y_lower,y_upper),0,ith_cluster_silhouette_values,facecolor=color,edgecolor=color,alpha=0.7)# Label the silhouette plots with their cluster numbers at the middle\n        plt.text(-0.05,y_lower+0.5*size_cluster_i,str(i))#Compute the new y_lower for next cluster\n        y_lower=y_upper+10# 10 for the 0 samples\n    plt.title(\"Silhouette plot for the various clusters.\")\n    plt.xlabel(\"Silhouette coefficient values\")\n    plt.ylabel(\"Cluster label\")\n    plt.show()\n    \nsilhouette(divorced,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 4\n\ncomplete_linkage = AgglomerativeClustering(linkage=\"complete\", n_clusters=n_clusters)\ny_pred = complete_linkage.fit_predict(together)\n\nplt.scatter(together[:, 0], together[:, 1], c=y_pred)\nplt.title('Complete link cluster labels')\nplt.show()\n\naverage_linkage = AgglomerativeClustering(linkage=\"average\", n_clusters=n_clusters)\ny_pred = average_linkage.fit_predict(together)\n\nplt.scatter(together[:, 0], together[:, 1], c=y_pred, label=y_pred)\nplt.title('Complete link cluster labels')\nplt.show()\n\nsilhouette(together,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spectral = SpectralClustering(n_clusters=4,affinity = 'rbf',n_neighbors = 10,random_state=random_state)\ny_pred = spectral.fit_predict(together)\nplt.scatter(together[:, 0], together[:, 1]\n            , c=y_pred)\nplt.title('Spectral link cluster labels')\nplt.show()\n\nsilhouette(together,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn_scores = cross_val_score(knn,together,y_pred,cv=3,scoring='accuracy')\nknn_scores.mean(),knn_scores.std()\n\nscatter_kwargs={'s':120,'edgecolor':None,'alpha':0.7}\ncontourf_kwargs={'alpha':0.2}\nscatter_highlight_kwargs = {'s':120,'label':'Test data','alpha':0.7}\n\nknn.fit(divorced,y_pred)\n\nplot_decision_regions(X=divorced,y=y_pred,clf=knn,legend=2,\n                      scatter_kwargs = scatter_kwargs,\n                      contourf_kwargs = contourf_kwargs,\n                     scatter_highlight_kwargs = scatter_highlight_kwargs)\n\nplt.xlabel('Attribute 1')\nplt.ylabel('Attribute 2')\nplt.title('k-NN')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(together,y_pred)\n\nplot_decision_regions(X=together,y=y_pred,clf=knn,legend=2,\n                      scatter_kwargs = scatter_kwargs,\n                      contourf_kwargs = contourf_kwargs,\n                     scatter_highlight_kwargs = scatter_highlight_kwargs)\n\nplt.xlabel('Attribute 1')\nplt.ylabel('Attribute 2')\nplt.title('k-NN')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}