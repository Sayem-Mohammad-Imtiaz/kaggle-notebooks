{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Bank Churners using Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"The model has been built using Logistic regression and using SMOTE(Synthetic Minority Over-Sampling technique) to address the problem of data imbalance.The up sampling has been done in the training dataset to increase the churn rate from **16%** to **50%**. However, these synthetic samples have not been generated for training in order to determine the true accuracy.\n\nThe precision and recall obtained on training set is **79%** & **78%** respectively. However, when the model is ran on test data the precision falls to **44%** while recall is **75%**\nThe reason for the drop in precision is due to **17%** churn rate in testing data. The cut off determined in training is low thus mis classifying the non churn customers as churn. This could be fixed by finding optimum cut off (higher than 0.5) using precision/recall curve."},{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Predicting-Bank-Churners-using-Logistic-Regression\" data-toc-modified-id=\"Predicting-Bank-Churners-using-Logistic-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Predicting Bank Churners using Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Reading-and-Understanding-the-Data\" data-toc-modified-id=\"Step-1:-Reading-and-Understanding-the-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Step 1: Reading and Understanding the Data</a></span></li><li><span><a href=\"#Step-2:-Data-Preparation\" data-toc-modified-id=\"Step-2:-Data-Preparation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Step 2: Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-columns-which-do-not-have-business-relevance\" data-toc-modified-id=\"Dropping-columns-which-do-not-have-business-relevance-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Dropping columns which do not have business relevance</a></span></li><li><span><a href=\"#Data-Balance\" data-toc-modified-id=\"Data-Balance-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Data Balance</a></span></li><li><span><a href=\"#Inspecting-categorical-variables\" data-toc-modified-id=\"Inspecting-categorical-variables-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Inspecting categorical variables</a></span></li><li><span><a href=\"#Inspecting-numerical-variables\" data-toc-modified-id=\"Inspecting-numerical-variables-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Inspecting numerical variables</a></span></li><li><span><a href=\"#One-hot-encoding-the-categorical-variables\" data-toc-modified-id=\"One-hot-encoding-the-categorical-variables-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>One hot encoding the categorical variables</a></span></li><li><span><a href=\"#Test-train-split-and-Feature-scalling\" data-toc-modified-id=\"Test-train-split-and-Feature-scalling-1.2.6\"><span class=\"toc-item-num\">1.2.6&nbsp;&nbsp;</span>Test-train split and Feature scalling</a></span></li><li><span><a href=\"#Feature-Scaling-in-Training-set\" data-toc-modified-id=\"Feature-Scaling-in-Training-set-1.2.7\"><span class=\"toc-item-num\">1.2.7&nbsp;&nbsp;</span>Feature Scaling in Training set</a></span></li><li><span><a href=\"#SMOTE-technique-to-increase-samples-of-churned-customers-compared-to-not-churned\" data-toc-modified-id=\"SMOTE-technique-to-increase-samples-of-churned-customers-compared-to-not-churned-1.2.8\"><span class=\"toc-item-num\">1.2.8&nbsp;&nbsp;</span>SMOTE technique to increase samples of churned customers compared to not churned</a></span></li></ul></li><li><span><a href=\"#Step-3-Model-Building-and-Feature-Selections\" data-toc-modified-id=\"Step-3-Model-Building-and-Feature-Selections-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Step 3 Model Building and Feature Selections</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-selection-using-RFE\" data-toc-modified-id=\"Feature-selection-using-RFE-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Feature selection using RFE</a></span></li><li><span><a href=\"#Reassesing-the-model-using-the-above-selected-features\" data-toc-modified-id=\"Reassesing-the-model-using-the-above-selected-features-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Reassesing the model using the above selected features</a></span></li><li><span><a href=\"#Checking-th-VIFs-value-in-order-to-remove-multicolinearity\" data-toc-modified-id=\"Checking-th-VIFs-value-in-order-to-remove-multicolinearity-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Checking th VIFs value in order to remove multicolinearity</a></span></li><li><span><a href=\"#Calculating-the-churn-probability\" data-toc-modified-id=\"Calculating-the-churn-probability-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Calculating the churn probability</a></span></li></ul></li><li><span><a href=\"#Step-4:-Finding-Optimal-Cutoff-point\" data-toc-modified-id=\"Step-4:-Finding-Optimal-Cutoff-point-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Step 4: Finding Optimal Cutoff point</a></span></li><li><span><a href=\"#Step-5:-Model-evaluation-using-the-training-dataset\" data-toc-modified-id=\"Step-5:-Model-evaluation-using-the-training-dataset-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Step 5: Model evaluation using the training dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-the-precision-and-recall-score\" data-toc-modified-id=\"Checking-the-precision-and-recall-score-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Checking the precision and recall score</a></span></li></ul></li><li><span><a href=\"#Step-6-Model-Evaluation-using-testing-data\" data-toc-modified-id=\"Step-6-Model-Evaluation-using-testing-data-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Step 6 Model Evaluation using testing data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scaling-the-features-of-test-data\" data-toc-modified-id=\"Scaling-the-features-of-test-data-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Scaling the features of test data</a></span></li><li><span><a href=\"#Making-predictions-on-the-test-set\" data-toc-modified-id=\"Making-predictions-on-the-test-set-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Making predictions on the test set</a></span></li><li><span><a href=\"#Checking-Accuracy,-Precision-and-Recall-on-the-testing-data\" data-toc-modified-id=\"Checking-Accuracy,-Precision-and-Recall-on-the-testing-data-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>Checking Accuracy, Precision and Recall on the testing data</a></span></li></ul></li></ul></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Reading and Understanding the Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing relevant libararies\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/credit-card-customers/BankChurners.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the head of the dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the number of variables with object type\ndf_categorical = df.select_dtypes(exclude=['float64','int64'])\nlen(df_categorical.columns)\ndf_categorical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the number of variables with float & int type\ndf_numerical = df.select_dtypes(exclude=['object'])\nlen(df_numerical.columns)\ndf_numerical.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights/Observation**\n\n1. There doesn't seem to be any null values in any of the columns except for last two which can be dropped during data preparation\n2. Clientnumber can be dropped as it doesn't have any business significance\n3. We can check for outliers in next step\n"},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"### Dropping columns which do not have business relevance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping following columns as they don't look useful\n\ncol = ['CLIENTNUM','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']\n\ndf.drop(col,axis=1,inplace=True)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Balance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Attrition_Flag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the data imbalance\ntemp = df['Attrition_Flag'].value_counts()\ndf_1 = pd.DataFrame({'labels': temp.index,'values': temp.values})\ndf_1.iplot(kind='pie',labels='labels',values='values', title=\"% Data Imbalance\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the target variable to binary\nbinary_map = {'Attrited Customer': 1, \"Existing Customer\": 0}\n\ndf['Attrition_Flag'] = df['Attrition_Flag'].map(binary_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Attrition_Flag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nThere is disbalance in data with only 16% Attrition customer against 84%. We will be using SMOTE technique to oversample and get a better model"},{"metadata":{},"cell_type":"markdown","source":"### Inspecting categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating two different set of dataframes - for churners & non churners\n\ndf_churn = df[df.Attrition_Flag==1]\ndf_nochurn = df[df.Attrition_Flag==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pltfunction(var):\n    plt.figure(figsize=(15,5))\n    plt.subplot(1, 2, 1)\n    temp = df_churn[var].value_counts(normalize = True).mul(100)\n    df_1 = pd.DataFrame({'labels': temp.index,'values': temp.values})\n    #print(\"df_1 :{}\".format(df_1))\n    sns.barplot(x=\"labels\",y=\"values\",data=df_1)\n    plt.title('Percent of '+ '%s' %var +' for Churners', fontsize=14)\n    plt.xlabel(var)\n    plt.xticks(rotation=90)\n    plt.ylabel('% Churners')\n    plt.subplot(1, 2, 2)\n    temp_1 = df_nochurn[var].value_counts(normalize = True).mul(100)\n    df_2 = pd.DataFrame({'labels': temp_1.index,'values': temp_1.values})\n    #print(\"df_2 :{}\".format(df_2))\n    sns.barplot(x=\"labels\",y=\"values\",data=df_2)\n    plt.title('Percent of '+ '%s' %var +' for Non-Churners', fontsize=14)\n    plt.xlabel(var)\n    plt.xticks(rotation=90)\n    plt.ylabel('% Non-Churners')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refreshing the categorical variables\ndf_categorical = df.select_dtypes(exclude=['float64','int64'])\ndf_categorical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Gender\npltfunction('Gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nFemales churn more compared to males"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Plotting the Education level\npltfunction('Education_Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nGraduates have a higher chance to churn compared to other education level. There is a category of unknown which needs to be classified"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Education level\npltfunction('Marital_Status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nMarried people tend to churn more compared to other groups"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Plotting the Income level\npltfunction('Income_Category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nMost of the churners have less than 40K dollars"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Income level\npltfunction('Card_Category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nBlue card holders seems to dominate in the dataset so nothing very conclusive can be concluded\n"},{"metadata":{},"cell_type":"markdown","source":"### Inspecting numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Refreshing the numerical variables post removing irrelavant columns\ndf_numerical = df.select_dtypes(exclude=['object'])\nlen(df_numerical.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nFollowing columns have high correlation and one of them can be removed as they may skew the analysis\n1. Customer Age and Months on Book\n2. Credit Limit and Average Open to Buy\n3. Total Trans amt and Total Trans Ct"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the columns having high correlation\ncol = ['Months_on_book','Avg_Open_To_Buy','Total_Trans_Ct']\ndf.drop(col,axis=1,inplace=True)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for outliers\ndf_numerical.describe(percentiles=[.25, .5, .75, .90, .95, .99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights/Obervations**\n\nThere seems to be significant difference between mean and max values for the following variables:\nCredit_Limit,Total_Amt_Chng_Q4_Q1,Total_Ct_Chng_Q4_Q1\nWe inspect it further using boxplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting boxplot\nplt.figure(figsize=(12, 12))\nplt.subplot(2,2,1)\nsns.boxplot(y = 'Credit_Limit', data = df)\nplt.subplot(2,2,2)\nsns.boxplot(y = 'Total_Amt_Chng_Q4_Q1', data = df)\nplt.subplot(2,2,3)\nsns.boxplot(y = 'Total_Ct_Chng_Q4_Q1', data = df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights/Obervations**\n\nTotal_Amt_Chng_Q4_Q1 - There seems to be some outliers but don't think it would impact the analysis"},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_categorical.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df[['Gender', 'Education_Level', 'Marital_Status', 'Income_Category','Card_Category']], drop_first=True)\n\n# Adding the results to the master dataframe\ndf_refined = pd.concat([df, dummy1], axis=1)\n\ndf_refined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Dropping the columns for whom dummy variables have been created\n\ncol = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category','Card_Category']\ndf_refined.drop(col,axis=1,inplace=True)\ndf_refined.columns\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test-train split and Feature scalling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting feature variable to X\nX = df_refined.drop(['Attrition_Flag'], axis=1)\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Putting response variable to y\ny = df_refined['Attrition_Flag']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling in Training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\n\nX_train[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']] = scaler.fit_transform(X_train[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMOTE technique to increase samples of churned customers compared to not churned"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmt = SMOTE(random_state=0)\nX_train_SMOTE, y_train_SMOTE = smt.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the churn rate before and after applying SMOTE\nchurn_1 = (sum(y_train)/len(y_train))*100\nchurn_2 = (sum(y_train_SMOTE)/len(y_train_SMOTE))*100\nprint(\"Before smote the number of records {} and the churn rate is {}\".format(len(y_train),round(churn_1,2)))\nprint(\"After smote the number of records {} and the churn rate is {}\".format(len(y_train_SMOTE),round(churn_2,2)))\nprint('Number of records in X_train before SMOTE {}'.format(len(X_train)))\nprint('Number of records in X_train after SMOTE {}'.format(len(X_train_SMOTE)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nThe up sampling has been done such that the number of churn cases increases from 15% to 50% and now the model can be build using the new training sets"},{"metadata":{},"cell_type":"markdown","source":"## Step 3 Model Building and Feature Selections"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression model\nlogm1 = sm.GLM(y_train_SMOTE,(sm.add_constant(X_train_SMOTE)), family = sm.families.Binomial())\nlogm1.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nFew of the variables have higher p value (>0.05), thus to select the relevant predictors - using the RFE technique\n"},{"metadata":{},"cell_type":"markdown","source":"### Feature selection using RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 12)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train_SMOTE, y_train_SMOTE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe.support_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"list(zip(X_train_SMOTE.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns to be retained\ncol = X_train_SMOTE.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Columns to not be considered\nX_train_SMOTE.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reassesing the model using the above selected features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train_SMOTE[col])\nlogm2 = sm.GLM(y_train_SMOTE,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nThe p values are all 0 except for Total_Amt_Chng_Q4_Q1 - lets check the VIF value"},{"metadata":{},"cell_type":"markdown","source":"### Checking th VIFs value in order to remove multicolinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_SMOTE[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_SMOTE[col].values, i) for i in range(X_train_SMOTE[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nTotal_Amt_Chng_Q4_Q1 has high VIF as well as p value - thus removing it"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Total_Amt_Chng_Q4_Q1 and updating the new defination of col\ncol = ['Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Total_Revolving_Bal',\n       'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio',\n       'Gender_M', 'Income_Category_$40K - $60K',\n       'Income_Category_Less than $40K', 'Income_Category_Unknown']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Total_Amt_Chng_Q4_Q1 and running the model again\nX_train_sm = sm.add_constant(X_train_sm[col])\nlogm3 = sm.GLM(y_train_SMOTE,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_SMOTE[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_SMOTE[col].values, i) for i in range(X_train_SMOTE[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nAll variables have a good value of VIF. So we need not drop any variables and we can proceed with making predictions using this model only"},{"metadata":{},"cell_type":"markdown","source":"### Calculating the churn probability"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Churn':y_train_SMOTE.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train_SMOTE.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Finding Optimal Cutoff point"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nSensitivity and Specificity balance each other at 0.5 probability which could be used as cut off to determine the churn & non churn cases"},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Model evaluation using the training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.5 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the confusion matrix\ncm2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted)\n\nplt.subplots(figsize=(10,10))\nax = sns.heatmap(cm2,annot=True,cmap='coolwarm',fmt='d')\nax.set_title('Prediction on training data using logistic regression',fontsize=18)\nax.set_xticklabels(['Predicted Not Churn','Predicted Churn'],fontsize=15)\nax.set_yticklabels(['Actual Not Churn','Actual Churn'],fontsize=15)\n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\na = round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted),2)*100\nprint(\"Accuracy is {}%\".format(a))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the precision and recall score"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = round(precision_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted),2)*100 \nprint(\"Precision is {}%\".format(p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = round(recall_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted),2)*100\nprint(\"Recall is {}%\".format(r))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observations**\n\nThe precision and recall score of the model is 79% & 78% which is pretty decent result using logistic regression"},{"metadata":{},"cell_type":"markdown","source":"## Step 6 Model Evaluation using testing data"},{"metadata":{},"cell_type":"markdown","source":"### Scaling the features of test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']] = scaler.transform(X_test[['Customer_Age', 'Dependent_count','Total_Relationship_Count', 'Months_Inactive_12_mon',\n       'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']])\n\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the col which were identified during RFE\n\nX_test = X_test[col]\nprint(len(X_test.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding constant\nX_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe which was Pandas series\ny_pred_1 = pd.DataFrame(y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearranging the columns\ny_pred_final = y_pred_final[['CustID','Attrition_Flag','Churn_Prob']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of y_pred_final\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the optimum threshold\ny_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Accuracy, Precision and Recall on the testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the confusion matrix\n\ncm3 = metrics.confusion_matrix(y_pred_final.Attrition_Flag, y_pred_final.final_predicted)\n\nplt.subplots(figsize=(10,10))\nax = sns.heatmap(cm3,annot=True,cmap='coolwarm',fmt='d')\nax.set_title('Prediction on testing data using logistic regression',fontsize=18)\nax.set_xticklabels(['Predicted Not Churn','Predicted Churn'],fontsize=15)\nax.set_yticklabels(['Actual Not Churn','Actual Churn'],fontsize=15)\n\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\na1 = round(metrics.accuracy_score(y_pred_final.Attrition_Flag, y_pred_final.final_predicted),2)*100\nprint(\"Accuracy is {}%\".format(a1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p1 = round(precision_score(y_pred_final.Attrition_Flag, y_pred_final.final_predicted),2)*100 \nprint(\"Precision is {}%\".format(p1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r1 = round(recall_score(y_pred_final.Attrition_Flag, y_pred_final.final_predicted),2)*100\nprint(\"Recall is {}%\".format(r1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observation**\n\nThere is a hige drop in the precision and recall score when the model is applied on test data. The scores drop nearly by half. The plausible reason could be the data imbalance in test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check the churn rate in testing data\n\nchurn_test = round((sum(y_test)/len(y_test)),2)*100\nprint(\"Churn rate in test data {}%\".format(churn_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights and Observation**\n\nThere is only 17% churned customers in test data. This could be the plausible reason"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}