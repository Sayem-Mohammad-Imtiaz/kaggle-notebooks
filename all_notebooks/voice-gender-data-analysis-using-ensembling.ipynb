{"cells":[{"metadata":{"_cell_guid":"72c319f2-13c1-44b3-8d2f-8e692ea610c6","_uuid":"c2902469ac384e49c9d6337dac4c9d76e6604738"},"cell_type":"markdown","source":"This notebook shows a simple approach to analyzing voice samples to detect gender using different classifiers and use an ensembling technique to combine. Overall, the best classifier gives an accuracy of 98.4% on the test set.\n\nWe will first use well-known classifiers like *Logistic Regression, Random Forest, Multilayer Perceptron (Neural Network)* and *Gradient Boosting*.\nWe will then use the outputs of the last three classifiers into an *Ensemble* classifier to see if can get slightly better performance compared to the individual classifiers. Overall, this data set seemed well behaved and relatively easy to work with, although it took several hours to tune the hyper-parameters for each of these classifiers to achieve the advertised performance.\n\n## Environment Setup\nFirst, we set up the environment by importing relevant libs, data and pre-processing the data."},{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"../input/\" directory.","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Read data into pandas data frame\ndf = pd.read_csv('../input/voice.csv')\n\n# Convert categorical label data to integers\ndf['label'] = df['label'].map({'male': 0, 'female': 1})\n# Split data into training and test samples\nx, y = df, df.pop('label')\n# setting random_state to fixed value to replicate same results in each run\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"4f293aaf-fb32-4264-8dec-93b9ce377db3","_uuid":"e490502a1b01a30f0ea48199ac1eab798a15418e"},"cell_type":"markdown","source":"## Logistic Regression\n\nWe will use a ***Logistic Regression*** classifier to get a quick idea on how to proceed with the analysis. We will tune the hyper-parameters using *Grid Search*. However, I soon found out that the classifier accuracy was not improving beyond a certain point."},{"metadata":{"_cell_guid":"89ca375d-bdae-42ab-81ad-d5cadb8e3ae6","_uuid":"65394c2b50c4e14efc8b6280908234387fb1ad31","trusted":true},"cell_type":"code","source":"logr = LogisticRegression()\n\nlogr.fit(X_train, y_train)\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"8c35a4bb-9ea6-467f-bc1a-f2404b1c2b45","_uuid":"eade2228925498744ed8e85f5796e80dfa99c9d1","trusted":true},"cell_type":"code","source":"logr.score(X_train, y_train)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"52e9675a-5f41-4835-a07f-62c7a7e551bf","_uuid":"5f6384ef607481bb97063f34c7671f6e2a4e8e87"},"cell_type":"markdown","source":"This looks ok for a first-cut, but let's see if the accuracy improves with a hyperparameter search."},{"metadata":{"_cell_guid":"5d3bda02-6de4-40d3-9322-0a48b76fe6ec","_uuid":"5b6a40e2731cbaa5cafacfa4da2c257903c968a3","scrolled":false,"trusted":true},"cell_type":"code","source":"    grid_params = {'C': [0.01, 0.1, 1, 2, 5],\n                  'solver': ['newton-cg', 'lbfgs']}\n    grid = GridSearchCV(logr, grid_params, cv=10)\n    grid.fit(X_train, y_train)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"d44ec482-1764-4471-b21e-d0eb5b014846","_uuid":"7b998521c45f015d5f8207b1bda1ac6c1ae66978","trusted":true},"cell_type":"code","source":"grid.score(X_train, y_train)","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"c0a2850a-67e7-4153-b498-f91b2b177d2a","_uuid":"bb672adac7d96854030f0cd9655c4855e07f1cce","trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"1d35bf23-a20f-425a-98fc-e9d44ca10d9f","_uuid":"925574955993b55f9af424c2e76e86ef50beba6a","trusted":true},"cell_type":"code","source":"y_pred_logr = grid.predict(X_test)\naccuracy_score(y_test, y_pred_logr)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"56ff8793-8cc9-4329-8a5d-780d72f42342","_uuid":"3a8bf7f0d0183a2787ac38ff7245b82642c3d2ef"},"cell_type":"markdown","source":"We will try to refine the classifier around these best parameters.  I did not see significant change either in the score or accuracy of the predictions."},{"metadata":{"_cell_guid":"c7346fc2-df2a-40c2-98b6-f01e9ab92d4e","_uuid":"71453f33eeb028623eab6587888b0dae85b09b06","trusted":true},"cell_type":"code","source":"grid_params = {'C': [4.9, 5.0, 5.1, 5.5, 6.0], 'solver': ['newton-cg', 'lbfgs'], 'max_iter':[100, 300, 600]}\ngrid = GridSearchCV(logr, grid_params, cv=10)\ngrid.fit(X_train, y_train)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"a48206e9-9eec-4544-83cb-c6c7bedb5d02","_uuid":"fcfd02b6c30d1ad77b691ce788fbe8a24924201c","trusted":true},"cell_type":"code","source":"grid.score(X_train, y_train)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"69031725-b667-410e-b2a3-69e73fa68c5e","_uuid":"d6a515ef63a521c57a236f1943d890a3423503cb","trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"9aeb7fde-360b-4ad0-aa70-078bd94ec758","_uuid":"30b0e1c746d08dba54a0cd31a3dba98025fccd00"},"cell_type":"markdown","source":"## Gradient Boosting\n\nWe now turn to ***Gradient Boosting (XGBoost)***. Even with default parameters, we get good results compared to *Logistic Regression*, as expected."},{"metadata":{"_cell_guid":"1bf74e19-cb42-4507-a67c-e44e640e6880","_uuid":"79031f1ccde69505c336e01a17062ed434102e1c","trusted":true},"cell_type":"code","source":"gbm = xgb.XGBClassifier()\ngbm.fit(X_train, y_train)\ngbm.score(X_train, y_train)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"f18e92e6-c66d-4076-82cf-3837c72dd12b","_uuid":"51ef44ff47d5053c2e1e3e95abd426149e868cb5"},"cell_type":"markdown","source":"Starting with the default parameters and increasing the number of estimators to 300, we get perfect training!"},{"metadata":{"_cell_guid":"a12cbb59-e175-4a65-a8f3-7f14cbe58208","_uuid":"7ee7c872908f6d6f04b4449b73d172fedb397722","trusted":true},"cell_type":"code","source":"gbm = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=300, n_jobs=1, nthread=None, objective='binary:logistic', random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1)\ngbm.fit(X_train, y_train)\ngbm.score(X_train, y_train)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72e1c47e5749c98ad0dd2986a0bd3807f37b3927"},"cell_type":"code","source":"accuracy_score(y_test, gbm.predict(X_test))","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"b06a8d9d-19a1-4ae8-8a64-13a5efc1419f","_uuid":"3fb260d9d0d4efc7cdcc8700c575840510dede45"},"cell_type":"markdown","source":"## Random Forest\nFollowing a similar strategy of hyper-parameter tuning with *Random Forest* classifier, we get the following score and prediction results. Increasing the number of estimators all the way up to 1000 gave the best performance (at which point, hyperparameter tuning fatigue set in)."},{"metadata":{"_cell_guid":"e347a81b-e1a8-4af2-b061-9791d62db068","_uuid":"7034cfca354a0ae96cf277ad9ad67fd0a94d7680","trusted":true},"cell_type":"code","source":"forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=42, verbose=0, warm_start=False)\nforest.fit(X_train, y_train)\nforest.score(X_train, y_train)","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"94d60853-9ee4-4c8b-8411-ac63dad184bc","_uuid":"473d7245c481d727e56d29604724144523dd6441","trusted":true},"cell_type":"code","source":"accuracy_score(y_test, forest.predict(X_test))","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"3d78f5dc-862c-404a-8b16-6ea5209e61f6","_uuid":"fa45bf4557de1cf78d81272742c70afe209e5617","trusted":true},"cell_type":"code","source":"# increase n_estimators to 1000\nforest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n            oob_score=False, random_state=42, verbose=0, warm_start=False)\nforest.fit(X_train, y_train)\nforest.score(X_train, y_train)","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"20c33d1d-0b51-4d3e-8d48-1d895073122a","_uuid":"f7eeda6921b802e6073bffc9eda4019775893ba4","scrolled":true,"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, forest.predict(X_test))","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"8362007e-4a3b-4bee-a5f5-6721b69e09da","_uuid":"1fd7066bf0300bb9f4efafec1d6efc4b9d75c9c1"},"cell_type":"markdown","source":"## Feature Importances\nAt this point I wanted to find out if there were any features that can be removed.  We can compute and plot the feature importances for this forest."},{"metadata":{"_cell_guid":"6c33967b-8ac6-448b-b621-999b65795b76","_uuid":"069d5cb9445a0aba6e1721b30a1650de96ae6d34","trusted":true},"cell_type":"code","source":"## plot feature importances\nimports = forest.feature_importances_\nindices = np.argsort(imports)[::-1]\n# stddev of each feature in the forest\nstd = np.std([t.feature_importances_ for t in forest.estimators_], axis=0)\n\nplt.figure()\nplt.bar(range(X_train.shape[1]), imports[indices], yerr=std[indices], color='r', align='center')\nplt.xticks(range(X_train.shape[1]), indices)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()\n","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"48c8e623-fb81-443f-8bea-bd640b029245","_uuid":"d9f723f480d184b4a30a0e5a077211b9d539705c"},"cell_type":"markdown","source":"As we can see most of the features have a non-zero value. However, we can still try to find out if there is any impact or improvement by removing some features. I decided to remove the least important feature \"***maxfun***\"."},{"metadata":{"_cell_guid":"36f9e585-0e04-4d88-8e49-7c693a20333d","_uuid":"cd64a2ff40e1b9d1a91df87425daab41e178de99","scrolled":false,"trusted":true},"cell_type":"code","source":"df.columns[indices]","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"7740bf0e-f1ad-467b-852f-94256dd0eb6e","_uuid":"ed52f979bd27b595cd93d1b95d7fbdacc57401e4","trusted":true},"cell_type":"code","source":"# Dropping the least important feature...\n\nforest.fit(X_train.drop('maxfun', axis=1), y_train)\nforest.score(X_train.drop('maxfun', axis=1), y_train)","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"0e245874-be77-4742-97d3-8254eb52214e","_uuid":"7951c2b1985a573a2686e8632ce0086909c7b130","trusted":true},"cell_type":"code","source":"accuracy_score(y_test, forest.predict(X_test.drop('maxfun', axis=1)))","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"57f76a2a-305e-46b1-bd53-f01f6a3c67ae","_uuid":"00447c7405edacf2d0fe1509f731b9c427deaaff"},"cell_type":"markdown","source":"That slightly improved the performance! However, further dropping one and two more least important features did not help."},{"metadata":{"_cell_guid":"96cca750-5e72-41ec-a2e2-30dcbe99a2bc","_uuid":"39ad3e344812f1e7f17a0226dbffc13dc7ce36fb"},"cell_type":"markdown","source":"## Multilayer Perceptron (MLP)\nWe now turn our attention to MLP. Again, the approach was to start with the default settings and work my way up to better performing settings. The parameters that really worked were the number of iterations, the solver type and changing the activation from default *relu* to *tanh*."},{"metadata":{"_cell_guid":"842a000b-e689-482f-8b16-ca31e0687e94","_uuid":"c250438e9d74b8158c7ed49819d53bf92557ec9a","scrolled":true,"trusted":true},"cell_type":"code","source":"mlp = MLPClassifier(activation='tanh', alpha=0.1, hidden_layer_sizes=(100, 100, 100), max_iter=3000, random_state=42, solver='lbfgs', tol=0.0001)\nmlp.fit(X_train, y_train)\nmlp.score(X_train, y_train)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"2fc7ac1b-ba26-47d2-83a3-e5fa483a0261","_uuid":"970a3eaa49a542ca9e83d7b0de53ea153641e138","trusted":true},"cell_type":"code","source":"accuracy_score(y_test, mlp.predict(X_test))","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"a3cc4660-1903-45fb-9d8b-812984b5fc8f","_uuid":"402c4551b5ba4b4adf8ab97e01b6fb118f0f4b1b"},"cell_type":"markdown","source":"## Ensemble of classifiers\nWe now take the 3 best performing classifiers (*Random Forest, XGBoost and MLP*) and then combine their outputs to see if performance improves even more. We will use the *VotingClassifier* to use (1) a majority vote (hard decision thresholding) or (2) the average predicted probabilities (soft decision thresholding) to predict the target classes."},{"metadata":{"_cell_guid":"1d450102-2c05-4c27-a204-60aeb32be0f5","_uuid":"6bc7e7eb6939705075684aace3aae80d9f57ce3e","trusted":true},"cell_type":"code","source":"vclf = VotingClassifier(estimators=[('forest', forest), ('mlp', mlp), ('gbm', gbm)], voting = 'hard')\nvclf.fit(X_train, y_train)\naccuracy_score(y_test, vclf.predict(X_test))","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"c9f758d7-8d36-4393-98d6-ef26709ae5e2","_uuid":"b1be1fdec42e42208a2c4bd6003f8a1901b81fa8","trusted":true},"cell_type":"code","source":"vclf = VotingClassifier(estimators=[('forest', forest), ('mlp', mlp), ('gbm', gbm)], voting = 'soft')\nvclf.fit(X_train, y_train)\naccuracy_score(y_test, vclf.predict(X_test))","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"a900b32f6385df3f9923120a7c334db43c332725"},"cell_type":"markdown","source":"Bummer :). Ensembling did not help. VotingClassifier is not guaranteed to be better always. So for now, *Random Forest* at 98.4% it is."}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}