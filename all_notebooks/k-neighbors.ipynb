{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Genre classification using Spotify dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib.colors import ListedColormap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get data"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"genre = pd.read_csv('../input/dataset-of-songs-in-spotify/genres_v2.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean dataset\n1. Notice that some sonngs have names but others do not. The ones that do not have a name actually has a reference of where they are found under the **title** column. We merge those with the name column.\n\n2. For purposes of classification, we separate the text columns (type, id, etc.) and create another dataframe containing only numeric values \n\n3. Standardize and remove NaN values"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Unify song name\nfor i in range(len(genre)):\n    if genre['song_name'][i] != genre['song_name'][i]:\n        genre['song_name'][i:] = genre['title'][i:]\n        break\ngenre.drop(['Unnamed: 0', 'title'], axis=1, inplace=True)\n# Drop NaN\ngenre.dropna(inplace=True)\n# Produce a numerical version dataset\ngenre_data = genre.drop(['type', 'id', 'uri', 'track_href', 'analysis_url', 'song_name', 'genre'], axis=1)\n# Standardize \nnum_column = genre_data.columns\ngenre_data = StandardScaler().fit_transform(genre_data)\ngenre_data = pd.DataFrame(genre_data)\ngenre_data.columns = num_column\npca = PCA(n_components=2).fit(genre_data).transform(genre_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# View dataset using pairplot"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.pairplot(genre_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using K-NN to build model"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_accuracies = []\ntrain_accuracies = []\nKNN_models = []\nX_train, X_test, Y_train, Y_test = train_test_split(genre_data, genre['genre'], test_size=0.2)\nX_train, X_val, Y_train, Y_val = train_test_split(genre_data, genre['genre'], test_size=0.2)\n# We try neighbors from 1 to 11\nfor i in range(1,11):\n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(X_train, Y_train)\n    total = 0\n    for actual,pred in zip(neigh.predict(X_val), Y_val):\n        total += actual == pred\n    val_accuracies.append(total/len(X_val))\n    total = 0\n    for actual,pred in zip(neigh.predict(X_train), Y_train):\n        total += actual == pred\n    train_accuracies.append(total/len(X_train))\n    KNN_models.append(neigh)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# View results and choose which model to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.arange(10) + 1, val_accuracies, c='y')\nplt.plot(np.arange(10) + 1, train_accuracies, c='b')\nplt.legend(['Validation set', 'Train set'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From the plot, we can see that 1-3 neighbors are ideal value to use. Both gives good validation results as well as training results."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(3):\n    select_model = KNN_models[i]\n    total = 0\n    for actual,pred in zip(select_model.predict(X_test), Y_test):\n        total += actual == pred\n    print('Model {} accuracy:'.format(i+1), total/len(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot the first two pcs and their explained variances"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(PCA(n_components=2).fit(genre_data).explained_variance_ratio_)\ngenre['pc1'] = pca[:, 0]\ngenre['pc2'] = pca[:, 1]\nsns.set(rc={'figure.figsize': (20,15)})\nsns.scatterplot(data=genre, x='pc1', y='pc2', hue='genre', alpha=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We choose the 1-neighbor model and plot decision bounadries"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"h = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['orange', 'blue', 'red', 'brown', 'yellow', 'green', 'aqua', 'purple', 'pink', 'silver', 'black', 'dimgray', 'darkred', 'linen', 'lawngreen'])\ncmap_bold = ['orange', 'blue', 'red', 'brown', 'yellow', 'green', 'aqua', 'purple', 'pink', 'silver', 'black', 'dimgray', 'darkred', 'linen', 'lawngreen']\n\n# We train the model under pca to plot 2d boundaries\nclf = KNeighborsClassifier(n_neighbors=5).fit(pca, genre['genre'])\n\ncodemap = {}\nfor i,genre_name in enumerate(genre['genre']):\n    codemap[genre_name] = i\n    \n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = pca[:, 0].min() - 1, pca[:, 0].max() + 1\ny_min, y_max = pca[:, 1].min() - 1, pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = np.array([codemap[genre_name] for genre_name in Z])\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(20,16))\nplt.contourf(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\npts = sns.scatterplot(x=pca[:, 0], y=pca[:, 1], hue=genre['genre'],\n                palette=cmap_bold, alpha=0, edgecolor=\"black\", size=1)\npts.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Limitation  \n\n1. Genres are generally not independent of one another. We have names that implies relationships (Dark Trap VS. Trap). This is also shown from the plot of the first two pc.  \n\n2. There are significant differences between number of samples of each genre as shown in the cell below."},{"metadata":{"trusted":true},"cell_type":"code","source":"count = {}\nfor name in genre['genre']:\n    if name not in count:\n        count[name] = 1\n    else:\n        count[name] += 1\nfor name in count:\n    print(name, count[name])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further questions"},{"metadata":{},"cell_type":"markdown","source":"What is the relationship between these genres? \nPrimary idea: Usign K-Means to generate clusters and analyze the elements of those clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}