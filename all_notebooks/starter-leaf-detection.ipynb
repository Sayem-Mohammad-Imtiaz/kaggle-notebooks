{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nGot help from this notebook: https://www.kaggle.com/pestipeti/pytorch-starter-fasterrcnn-train\n\nThis is a notebook which can be used to detect the leaves in this dataset. At the end there are 6 test images, and as you can see even with a small dataset, it performs pretty well\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to pip install these two versions because it gives me some errors otherwise\n\n# This can take some time\n\n!pip install \"torch==1.7\" \"torchvision==0.8.0\"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\n\nfrom PIL import Image\n\nimport time\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations.pytorch import transforms\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\nDIR_INPUT = '/kaggle/input/leaf-detection'\nDIR_TRAIN = f'{DIR_INPUT}/train' \nDIR_TEST = f'../input/leaftest/mangoplant'\n\n# Loading the device now\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"# Reading and parsing the CSV"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(DIR_INPUT,\"train.csv\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"image_ids = train_df['image_id'].unique()\nvalid_ids = image_ids[-4:]\nvalid_ids = np.append(valid_ids,image_ids[:4])\ntrain_ids = image_ids[4:-4]\n\nvalid_df = train_df[train_df['image_id'].isin(valid_ids)]\ntrain_df = train_df[train_df['image_id'].isin(train_ids)]\n\nvalid_df.shape, train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LeafDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}/{image_id}', cv2.IMREAD_COLOR)                \n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).astype(np.float32)\n        image = np.reshape(image,image.shape+(1,))\n        image /= 255.0\n\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# This Albumentation for now it is empty.\ndef transform():\n    return A.Compose([        \n#         A.Cutout(num_holes=10,max_h_size=15,max_w_size=15,p=1),\n            \n#         A.OneOf([\n#             A.RandomSunFlare(src_radius=200,num_flare_circles_lower=6,num_flare_circles_upper=8,p=1),\n#             A.RandomRain(slant_lower=-10,slant_upper=10,drop_length=20,drop_width=1,p=1),\n#             A.RandomFog(fog_coef_lower=0.05, fog_coef_upper=0.1, alpha_coef=0.08, p=1),  \n#         ], p=1),\n#         A.OneOf([\n#             A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, brightness_by_max=True, p=1),\n#             A.RandomGamma(gamma_limit=(80,165),p=1),  \n#         ], p=1),      \n        ToTensorV2(p=1.0),\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = LeafDataset(train_df, DIR_TRAIN, transform())\nvalid_dataset = LeafDataset(valid_df, DIR_TRAIN, transform())\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HELPER FUNCTIONS FOR VIZUALISING / PREDICTING\n\ndef get_boxes(tensor,index,score=0.5):\n\n    if index >= len(tensor)  or index<0:\n        return 0\n    \n    temp_boxes = []\n    for i in range(len(tensor[index]['boxes'])):\n        if tensor[index]['scores'][i] > score:\n            temp_boxes.append(tensor[index]['boxes'][i].cpu().detach().numpy().astype(np.int32))    \n        \n    return temp_boxes    \ndef get_sample_image(itr):\n    images, targets, image_ids = next(it)\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n    boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[0].permute(1,2,0).cpu().numpy()\n    sample = np.reshape(sample,(sample.shape[1],sample.shape[1]))\n    \n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n    \n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      0, 2)\n\n    ax.set_axis_off()\n    ax.imshow(sample,cmap='gray')\ndef get_validation_image(itr):\n    images, targets, image_ids = next(itr)\n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[0].permute(1,2,0).cpu().numpy()\n    \n    model.eval()\n\n    outputs = model(images)\n    outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n    boxes = get_boxes(outputs,0)\n\n\n    # boxes = outputs[1]['boxes'].cpu().detach().numpy().astype(np.int32)\n\n\n    sample = images[0].permute(1,2,0).cpu().numpy()\n    \n    boxes = get_boxes(outputs,0)\n\n    sample = images[0].permute(1,2,0).cpu().numpy()\n    sample = np.reshape(sample,(sample.shape[1],sample.shape[1]))\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n    \n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      0, 2)\n\n\n    ax.set_axis_off()\n    ax.imshow(sample,cmap='gray')\n    \ndef load_test_dataset():\n    data_path = DIR_TEST\n    test_dataset = torchvision.datasets.ImageFolder(\n        root=data_path,\n        \n        transform=torchvision.transforms.Compose([\n            torchvision.transforms.Grayscale(num_output_channels=1),\n            torchvision.transforms.ToTensor(),]\n    ))\n    \n    test_loader = torch.utils.data.DataLoader(    \n        test_dataset,\n        batch_size=1,\n        num_workers=1,\n        shuffle=False\n    )\n    return test_loader\n\ndef get_test_image(itr,score = 0.5):\n    image, targets= next(itr)\n    sample = image\n    \n    image = image.to(device)\n    model.eval()\n    outputs = model(image)\n    \n    outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]   \n    \n    boxes = get_boxes(outputs,0,score)\n    \n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n    print(sample.shape)\n    img = sample[0].permute(1,2,0).cpu().numpy()\n    print(img.shape)\n    \n    \n    img = np.array(img)\n    img = np.reshape(img,(img.shape[1],img.shape[1]))\n    print(img.shape)\n    for box in boxes:\n        x,y,w,h = box\n        \n        cv2.rectangle(np.float32(img),\n                      (int(box[0]), int(box[1])),\n                      (int(box[2]), int(box[3])),\n                      0, 2)\n    ax.set_axis_off()\n    ax.imshow(img,cmap='gray')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample of training data augumented\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"it = iter(train_data_loader)\nget_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sample_image(it)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading ResNet50 trained on COCO"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\nnum_classes = 2  # 1 class (leaf) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.to(device)\nprint(\"Model loaded\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nlr_scheduler = None\n\nnum_epochs = 15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_hist = Averager()\nitr = 1\n\nprevious_epoch = 1000\nes_rate = 0\n\nes_threshold = 2 # How many epochs without improvement to early stop\n\n\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    min_loss = 1000\n    \n    for images, targets, image_ids in train_data_loader:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n        \n        itr += 1\n                \n            \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    min_loss = loss_hist.value\n    \n    if min_loss < previous_epoch:\n        previous_epoch = min_loss\n        es_rate = 0\n        \n    else:\n        if es_rate < es_threshold:\n            es_rate += 1\n        elif es_rate >= es_threshold:\n            break\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation (On data from Training)"},{"metadata":{"trusted":true},"cell_type":"code","source":"it = iter(valid_data_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_validation_image(it)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_list = os.listdir(DIR_TEST+\"/leaf\")\nprint(image_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"it = iter(load_test_dataset())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nget_test_image(it,0.5)\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nget_test_image(it,0.5)\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nget_test_image(it,0.5)\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nget_test_image(it,0.5)\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nget_test_image(it,0.5)\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nget_test_image(it,0.50)\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nget_test_image(it,0.50)\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model, 'leaves_fasterrcnn_model.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}