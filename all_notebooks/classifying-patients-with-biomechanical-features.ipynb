{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The task consists in classifying patients as belonging to one out of three categories: ```Normal```, ```Disk Hernia``` or ```Spondylolisthesis```.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Load CSV file with 3 classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/biomechanical-features-of-orthopedic-patients/column_3C_weka.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 310 patients. Each patient is represented with 6 biomechanical attributes:\n* pelvic tilt\n* sacral slope: angle between the S1 upper vertebra and a horizontal line.\n* pelvic incidence: sum of pelvic tilt and sacral slope.\n* lumbar lordosis angle \n* pelvic radius \n* degree spondylolisthesis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Number of patients in each class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['class'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_melted = data.melt(id_vars=['class'])\nordered_class = data_melted[\"class\"].value_counts().index\nfacet = sns.FacetGrid(data_melted, col=\"variable\", sharey=False, col_wrap=2, aspect=1.2)\nfacet.map(sns.boxplot, \"class\", \"value\", data=data_melted, palette=[\"#e1812c\", \"#3a923a\", \"#3274a1\"], order=ordered_class)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We remove the outlier for ```degree spondylolisthesis``` higher than 200","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.degree_spondylolisthesis<200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data distribution 2 by 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, hue=\"class\", height=2.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nsns.heatmap(corr)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From all the previous plots, we can see that ```pelvic radius``` is independent from the other variables. We can also notice a high corrolation between ```sacral slope``` and\n```pelvic incidence```, as well as between ```pelvic incidence``` and ```lumbar lordosis angle```.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# PCA\nIn order to better visualize the data we can try to use the Principal Component Analysis (PCA).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=6)\nx_data = data.iloc[:,0:-1]\npcs = pca.fit_transform(x_data)\nplt.plot(np.arange(1,7),pca.explained_variance_ratio_ * 100)\nplt.bar(np.arange(1,7),pca.explained_variance_ratio_ * 100)\nplt.ylabel(\"Inertia (%)\")\nplt.xlabel(\"Dimension\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About 80% of the total inertia is explained by the first two dimensions. We can then try to plot the data in these two dimensions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pca = np.hstack((data[['class']].to_numpy(), pcs))\ndata_pca = pd.DataFrame(data_pca)\ndata_pca = data_pca.rename(columns={i:f'Dim {i}' for i in range(1,7)}).rename(columns={0:'class'})\n\nsns.scatterplot(x='Dim 1', y='Dim 2', hue='class', data= data_pca)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that ```Spondylolisthesis``` is separated from the two other classes. However ```Hernia``` and ```Normal``` are mixed. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.plotting import plot_pca_correlation_graph\n\nfigure, correlation_matrix = plot_pca_correlation_graph(x_data, \n                                                        x_data.columns, \n                                                        dimensions=(1, 2), \n                                                        figure_axis_size=7, \n                                                        X_pca=pcs[:,0:2], \n                                                        explained_variance=pca.explained_variance_[0:2])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-means","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\ndata_pca_quant = data_pca.drop(columns=\"class\")\nkm3 = KMeans (n_clusters=3, init=\"random\")\nkm3.fit(data_pca_quant)\nsns.scatterplot(x='Dim 1', y='Dim 2', data= data_pca_quant, hue=km3.labels_, style=data_pca['class'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With K=3, we obtain a partition that is very different from the real partition.```Spondylolisthesis``` is divided in two clusters but ```Normal``` and ```Hernia```are in the same one. \n\nThis is confirmed with the Adjusted rand score. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Adjusted rand score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import adjusted_rand_score\n\nprint(f\"Adjusted rand score: {round(adjusted_rand_score(data_pca['class'], km3.labels_),2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Supervised learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Train/test split\nWe start by splitting our dataset into a train (2/3) and test (1/3) dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Original data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = data.iloc[:, 0:-1]\ny = data['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data without pelvic incidence\nAs said before, pelvic incidence is the sum of pelvic tilt and sacral slope. This can add redundancy. So it's interesting to see what happens when we drop this variable from our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_5c = X_train.drop(columns=['pelvic_incidence'])\nX_test_5c = X_test.drop(columns=['pelvic_incidence'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search function\nWe can apply some transformations to our dataset before training. For example:\n* Neighborhood Components Analysis (NCA), with ```n_components``` equal to 2 or ```n_features```\n* Standard Scaler\n\nWe can even combine these transformations using pipes.\n\nWe also want to test different parameters for each algorithm\n\nAll this is done with the ```grid_search``` function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import NeighborhoodComponentsAnalysis\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\ndef grid_search(model, parameters):\n    def one_grid_search(X, y, model, parameters):\n        clf = GridSearchCV(model, parameters, scoring=\"accuracy\", cv=5, n_jobs=-1, refit=True)\n        clf.fit(X, y)\n        scores = clf.cv_results_['mean_test_score']\n        return clf.best_params_, clf.best_score_\n\n    pipe1 = Pipeline([('standard', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis())])\n    pipe2 = Pipeline([('standard', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis(2))])\n\n    Xs = [ \n        ('original',X_train),\n        ('without pelvic incidence',X_train_5c),\n\n        ('NCA', NeighborhoodComponentsAnalysis().fit_transform(X_train,y_train)),\n        ('without pelvic incidence + NCA', NeighborhoodComponentsAnalysis().fit_transform(X_train_5c,y_train)),\n        \n        ('NCA(2)', NeighborhoodComponentsAnalysis(2).fit_transform(X_train,y_train)),\n        ('without pelvic incidence + NCA(2)', NeighborhoodComponentsAnalysis(2).fit_transform(X_train_5c,y_train)),\n \n        ('standard scaler', StandardScaler().fit_transform(X_train)),\n        ('without pelvic incidence + standard scaler', StandardScaler().fit_transform(X_train_5c)),\n        \n        ('standard scaler + NCA', pipe1.fit_transform(X_train, y_train)),\n        ('without pelvic incidence + standard scaler + NCA', pipe1.fit_transform(X_train_5c, y_train)),\n\n        ('standard scaler + NCA(2)', pipe2.fit_transform(X_train, y_train)),\n        ('without pelvic incidence + standard scaler + NCA(2)', pipe2.fit_transform(X_train_5c, y_train))\n\n    ]\n    print(f'Parameters to test: {str(list(parameters.keys())).strip(\"[]\")}\\n')\n    print(f\"{'Method':55} {'Score':10} Parameters\")\n    for X in Xs:\n        method, X = X\n        best_params, best_score = one_grid_search(X, y_train, model, parameters)\n        print(f'- {method:52}: ({round(100*best_score,2):5} %)  {str(list(best_params.values())).strip(\"[]\")}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision boundary function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n\ndef plot_decision_boundary(clf, X, y):\n    labels = ['Hernia', 'Spondylolisthesis', 'Normal']\n    \n    # Transfom y to categorical [1,2,3]\n    y_cat = y.copy()\n    y_cat[y_cat=='Hernia']=0\n    y_cat[y_cat=='Spondylolisthesis']=1\n    y_cat[y_cat=='Normal']=2\n    y_cat = y_cat.to_numpy()\n    y_cat = y_cat.astype(int)\n\n    h = 20  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['tab:blue','tab:orange' , 'tab:green'])\n    cmap_bold = ListedColormap(['blue', 'darkorange','darkgreen' ])\n\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf.fit(X, y_cat)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 100, X[:, 0].max() + 100\n    y_min, y_max = X[:, 1].min() - 100, X[:, 1].max() + 100\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(10,5))\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y_cat, cmap=cmap_bold,\n                edgecolor='k', s=20, label=labels)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    legend = plt.legend(*scatter.legend_elements(), title=\"Class\")\n    for i, label in enumerate(labels):\n        legend.get_texts()[i].set_text(label)\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Knn\n","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nparameters = {'n_neighbors':list(range(1, 100)), 'weights':['uniform', 'distance']}\ngrid_search(KNeighborsClassifier(n_jobs=-1), parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best validation score (90.74%) is obtained with the standard scaler and NCA(2) transformations, and with ```n_neighbors=14``` and ```weights='distance```.\nWe can apply this model into the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier(n_neighbors=14, weights='distance')\npipe = Pipeline([('standard', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis(2))])\nX_train_transformed = pipe.fit_transform(X_train, y_train)\nX_test_transformed = pipe.transform(X_test)\n\nknn.fit(X_train_transformed, y_train)\ny_pred = knn.predict(X_test_transformed)\nprint(f'Test score: {round(100*accuracy_score(y_pred, y_test),2)} %')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(knn, X_test_transformed, y_test, normalize='true')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=14, weights='distance')\n\nplot_decision_boundary(knn, X_train_transformed, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nparameters = {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}\ngrid_search(LogisticRegression(random_state=42, multi_class=\"auto\", n_jobs=-1, C=1), parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best validation score (84.43%) is obtained with the NCA(2) transformation, and with ```solver=newton-cg```.\nWe can apply this model into the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nca = NeighborhoodComponentsAnalysis(2)\nX_train_transformed = nca.fit_transform(X_train_5c, y_train)\nX_test_transformed = nca.transform(X_test_5c)\n\nlr = LogisticRegression(random_state=42, multi_class=\"auto\", n_jobs=-1, C=1, solver='newton-cg')\nlr = lr.fit(X_train_transformed, y_train)\n\ny_pred = lr.predict(X_test_transformed) \nprint(f'Test score: {round(100*accuracy_score(y_pred, y_test),2)} %')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(lr, X_test_transformed, y_test, normalize='true')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=42, multi_class=\"auto\", n_jobs=-1, C=1, solver='newton-cg')\n\nplot_decision_boundary(lr, X_train_transformed, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random forest\n","execution_count":null},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nparameters = {\n    'bootstrap':[True, False],            \n    'criterion':['gini', 'entropy'], \n    'max_features':[2, 3, 4, 5, None],\n    'n_estimators':[10, 100, 200],        \n            }\ngrid_search(RandomForestClassifier(n_jobs=-1, random_state=42), parameters)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best validation score (88.82%) is obtained with the NCA transformation, and with ```bootstrap=False, criterion='gini', max_features=4```  and```n_estimators=100```.\nWe can apply this model into the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nca = NeighborhoodComponentsAnalysis()\nX_train_transformed = nca.fit_transform(X_train, y_train)\nX_test_transformed = nca.transform(X_test)\n\nrf = RandomForestClassifier(bootstrap=False,criterion='gini', max_features=4, n_estimators=100,  random_state=42)\nrf = rf.fit(X_train_transformed, y_train)\n\ny_pred = rf.predict(X_test_transformed)\nprint(f'Test score: {round(100*accuracy_score(y_pred, y_test),2)} %')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(rf, X_test_transformed, y_test, normalize='true')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe obtain the best test score (84.47%) with the logistic regression. Some improvements can be applied to this work:\n* Other algorithms could be tested in order to improve the score (SVM, neural networks for example).\n* We can also add a higher weight for wrong classifications, especially when we classify a patient with a disease as ```Normal```.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}