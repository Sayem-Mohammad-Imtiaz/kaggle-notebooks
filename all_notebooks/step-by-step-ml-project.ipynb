{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"text-align:center;\"> <u>STROKE RISK PREDICTION</u> </p>","metadata":{}},{"cell_type":"markdown","source":"### AIM: OUR AIM IS TO PREDICT WHETHER A PERSON HAD A STROKE OR NOT BASED ON THE FOLLOWING FEATURES:\n\n* id: unique identifier\n* gender: \"Male\", \"Female\" or \"Other\"\n- age: age of the patient\n- hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n- heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n- ever_married: \"No\" or \"Yes\"\n- work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n- Residence_type: \"Rural\" or \"Urban\"\n- avg_glucose_level: average glucose level in blood\n- bmi: body mass index\n- smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"","metadata":{}},{"cell_type":"markdown","source":"-----------","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"text-align:center;\"> <u>STEPS ONE SHOULD FOLLOW WHILE WORKING ON ML DATASET</u></p>\n\n\n* <span style=\"font-size:20px;\"> In this kernel, I have worked on the Exploratory Data Analysis or EDA of the stroke risk dataset. Exploratory Data Analysis or EDA is a first step in analysing a new dataset. The primary objective of EDA is to analyse the data for distribution, outliers and anomalies in the dataset. It includes analysing the data to find the distribution of data, its main characteristics, identifying patterns and visualizations. It also provides tools for hypothesis generation by visualizing and understanding the data through graphical representation.</span>\n\n* <span style=\"font-size:20px;\">Feature Engineering is the Second Step one should follow while doing any Machine Learning Project. In this step we perform the following things:\n          \n          Handle the Missing Values if any.\n          Handling the Outliers.\n          Handling the Categorical Data.\n          Normalizing the Data for further Model building.\n    \n    \n* <span style=\"font-size:20px;\">The third Step is Feature Selection. In this step we use some techniques to identify the important and unnecessary features to feed to our model.</span>\n    \n* <span style=\"font-size:20px;\">The Final step I performed is Building Machine Learning Models: Random Forest, XGBOOST </span>\n\n* <span style=\"font-size:20px;\">If the Accuracy is NOT SATISFYING, then we perform Hyper-Parameter Tuning to enhance the performance of our Model</span>\n\n","metadata":{}},{"cell_type":"markdown","source":"------","metadata":{}},{"cell_type":"markdown","source":"------","metadata":{}},{"cell_type":"markdown","source":"## IMPORT REQUIRED LIBRARIES:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (5, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GET THE FIRST 5 ROWS:\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GET THE LIST OF COLUMNS IN DATASET:\ndf.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GET THE STATISTICS OF DATA:\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* HERE WE CAN SEE THAT THE COUNT OF \"BMI\" IS LESS COMPARED TO OTHER FEATURES. THAT MEANS BMI HAS SOME NULL VALUES.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* HERE THE CATEGORICAL FEATURES ARE - \"GENDER\", \"EVER_MARRIED\", \"WORK_TYPE\", \"RESIDENCE_TYPE\", \"SMOKING_STATUS\".\n* WE NEED TO HANDLE THESE COLUMNS. (CONVERT TO NUMERIC DATA)","metadata":{}},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"## EDA & FEATURE ENGINEERING","metadata":{}},{"cell_type":"markdown","source":"-----------","metadata":{}},{"cell_type":"markdown","source":"## EDA\n\n* <span style=\"font-size:20px;\">Exploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. (source: Wikipedia)</span>\n\n* <span style=\"font-size:20px;\">In summary, EDA can show us hidden relationships and attributes present in our data even before we throw it at a machine learning model.</span>\n\n## FEATURE ENGINEERING\n    \n*  <span style=\"font-size:20px;\">Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning and is both difficult and expensive. (source: Wikipedia)</span>\n    \n*  <span style=\"font-size:20px;\">In summary, FE is simply using your existing knowledge of the dataset to create new features that can help a machine learning model perform better.</span>","metadata":{}},{"cell_type":"code","source":"# Plot histograms of each parameter \n\ndf.hist(figsize = (20, 20))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualize the correlation\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True, cmap = 'Wistia')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(15,5))\nsns.distplot(df['bmi'], color = 'cyan')\nplt.title('Distribution of bmi', fontsize = 20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(15,5))\nsns.distplot(df['avg_glucose_level'], color = 'cyan')\nplt.title('Distribution of avg_glucose_level', fontsize = 20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df, hue='stroke')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['stroke'].value_counts(dropna = False).plot.bar(color = 'cyan')\nplt.title('Comparison of stroke feature')\nplt.xlabel('zero & one')\nplt.ylabel('count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* HERE WE CAN SEE THE DATA IS COMPLETELY IMBALANCE. WE'LL HANDLE THIS BELOW.","metadata":{}},{"cell_type":"markdown","source":"------","metadata":{}},{"cell_type":"markdown","source":"## 1. HANDLING MISSING VALUES:\n\n### WHAT ARE MISSING VALUES ?\n\n* <span style=\"font-size:20px;\">Many real-world datasets may contain missing values for various reasons. They are often encoded as NaNs, blanks or any other placeholders. Training a model with a dataset that has a lot of missing values can drastically impact the machine learning model's quality.</span>\n\n### WAYS TO HANDLE MISSING VALUES:\n\n* 1. Deletion\n* 2. Impute missing values with Mean/Median\n* 3. Prediction Model\n* 4. KNN Imputer\n\n","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#WE ARE REPLACING THE NULL VALUES WITH MEAN OF THAT FEATURE.\ndf['bmi'].fillna(df['bmi'].mean(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* HERE WE CAN SEE THAT NOW OUR DATA HAVE ZERO NULL VALUES.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 2. CHECK FOR OUTLIERS IN OUR DATA\n\n\n### What is an Outlier ?\n\n* <span style=\"font-size:20px;\">Outlier is a commonly used terminology by analysts and data scientists as it needs close attention else it can result in wildly wrong estimations. Simply speaking, Outlier is an observation that appears far away and diverges from an overall pattern in a sample.(SOURCE:Analytics Vidhya)</span>\n\n![](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Outlier.png)\n\n### What is the impact of Outliers on a dataset?\n\n<span style=\"font-size:20px;\">Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:\n\n* It increases the error variance and reduces the power of statistical tests\n* If the outliers are non-randomly distributed, they can decrease normality\n* They can bias or influence estimates that may be of substantive interest\n* They can also impact the basic assumption of Regression, ANOVA and other statistical model assumptions.\n    \n    ","metadata":{}},{"cell_type":"code","source":"#BMI FEATURE:\ndf.boxplot(column='bmi')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AVG_GLUCOSE_LEVEL:\ndf.boxplot(column='avg_glucose_level')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NO NEED TO WORRY ABOUT THESE OUTLIERS. THESE WILL BE HANDLED AUTOMATICALLY BY THE MODEL WE'LL BE USING i.e., XGBOOST.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 3. HANDLE THE CATEGORICAL VARIABLES USING LABEL ENCODER","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nenc=LabelEncoder()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gender=enc.fit_transform(df['gender'])\nsmoking_status=enc.fit_transform(df['smoking_status'])\nwork_type=enc.fit_transform(df['work_type'])\nResidence_type=enc.fit_transform(df['Residence_type'])\never_married=enc.fit_transform(df['ever_married'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['ever_married']=ever_married\ndf['Residence_type']=Residence_type\ndf['smoking_status']=smoking_status\ndf['gender']=gender\ndf['work_type']=work_type","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['ever_married', 'Residence_type', 'smoking_status', 'gender', 'work_type']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### THE CATEGORICAL FEATURES ARE HANDLED.","metadata":{}},{"cell_type":"markdown","source":"-------","metadata":{}},{"cell_type":"markdown","source":"## 4. REVOME UNNECESSARY COLUMNS IF ANY","metadata":{}},{"cell_type":"code","source":"#ID COLUMN IS NOT REQUIRED.\ndf = df.drop('id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"## SPLIT THE DATASET INTO X & Y","metadata":{}},{"cell_type":"code","source":"X = df.drop('stroke', axis=1)\ny = df['stroke']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"## 5. HANDLING IMBALANCED DATA\n\n* WE WILL BE USING SMOTE TECHNIQUE TO HANDLE THE IMBALANCED DATA.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## THE DATA IS NOW BALANCED !!!!!!!!!","metadata":{}},{"cell_type":"markdown","source":"-----------","metadata":{}},{"cell_type":"markdown","source":"## 6. MODEL BUILDING USING THE PRE-PROCESSED / BALANCED DATASET.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nrdf_model = RandomForestClassifier()\nrdf_model.fit(X_train_res, y_train_res)\nprint('Training Score: {}'.format(rdf_model.score(X_train_res, y_train_res)))\nprint('Test Score: {}'.format(rdf_model.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = XGBClassifier()\nxgb_model.fit(X_train_res, y_train_res)\nprint('Training Score: {}'.format(xgb_model.score(X_train_res, y_train_res)))\n\nprint('Test Score: {}'.format(xgb_model.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## HERE, WE HAVE TRAINED TWO MODELS - RANDOM FOREST & XGBOOST. IT GAVE US VERY GOOD RESULT WITH OUT ANY HYPER-PARAMETERE TUNING.","metadata":{}},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"markdown","source":"## CONCLUSION\n\n-----\n\n### THIS IS HOW YOU HAVE TO APPROACH A MACHINE LEARNING REGRESSION OR CLASSIFICATION PROBLEM. EACH THING SHOULD BE FOLLOWED STEP-WISE.\n\n* ## BEFORE STARTING TO CODE - FIRST UNDERSTAND THE PROBLEM, THEN MAKE A MIND MAP OF HOW TO START AND APPROACH THIS.\n\n----\n\n### I HOPE THIS HELPS YOU TO START YOUR JOURNEY IN THIS FIELD.\n\n### IF YOU LIKE THIS, PLEASE GIVE ME AN UPVOTE.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}