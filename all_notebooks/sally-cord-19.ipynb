{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing relevant libraries\nimport numpy as np # linear algebra\nimport pandas as pd\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.corpus import stopwords\nfrom allennlp.commands.elmo import ElmoEmbedder\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nimport time\nimport random\n\n\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading our data\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# having a quick peek at our data\nmeta_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting only the abstracts from the metadata\nabs_text = meta_df[\"abstract\"].to_list()\n#abs_text[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess text by taking out regular stop words\ndef preprocess(text, remove_stopwords=True):\n   \n    text = re.sub(\"[^a-zA-Z]\",\" \", str(text))\n   \n    words = text.lower().split()\n    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n\n    cleaned_abstract=[]\n    \n    for word in words:\n        cleaned_abstract.append(word)\n\n    \n    return(cleaned_abstract)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a list of my preprocessed text\ncleaned_abstract_body = []\nfor text in abs_text:\n    cleaned_abstract_body.append( \" \".join(preprocess(text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am working with 57k abstract\n#cleaned_abstract_body[0:5]\nabstracts = [i.split() for i in cleaned_abstract_body]\n#print(abstracts) # r is a list of lists. Each list contained in the bigger list is the abstract(each) with its tokenised words\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embeddings = []\nelmo = ElmoEmbedder()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vectorising the entire abstract\n# vectorising 50 abstracts becos it takes infinity to run like 1000\ntext_vector = [elmo.embed_sentence(reviews)[0] for reviews in abstracts[0:2]]\n# text_vector is a list containing the word vectors for the words in each abstract\ntext_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=(np.concatenate(text_vector))\n\n# Now we cluster our vector X using sklearn DBSCAN\neps = 0.3\nmin_samples = 4\n   \nX = StandardScaler().fit_transform(np.asarray(X))\nplt.rcParams.update({'figure.max_open_warning': 0})\n\n   \ndb = DBSCAN(eps, min_samples,algorithm=\"kd_tree\").fit(X)\n\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nprint(n_clusters_)\nn_noise_ = list(labels).count(-1)\n\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\nfig = plt.figure()\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\n\nfig.savefig('cluster_image.png', bbox_inches='tight')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the abstratcs from the clusters\n\nindex2 = []\ncounter = 0\nfor i in labels:\n    if i != -1: # remove the noise labels\n        index2.append(counter)\n    counter+=1\n\nlabel2 = []\nfor j in index2:\n    k = labels[j]\n    label2.append(k) # these are the labels for points in each cluster\n\n\nd = {\"Index\":index2, \"Labels\":label2}\n\ndf = pd.DataFrame(d)\n\ns = pd.Series([\"Labels\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the indices of the labels in the cluster\nindex_kk = []\n\nfor i in range(len(label2)):\n    p= df[df['Labels']==i]\n    \n    index_kk.append(p.Index.tolist())\n#print(\"index for each label:\",index_kk )\nindex_labels = [x for x in index_kk if x != []]\n#print (\"These are the indices for each label\",index_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the reviews in each cluster\nempty_lst = []\nfor i in index_labels:\n    empty_lst.append([cleaned_abstract_body[j] for j in i])\n\n#print (\"these are the abstracts found in each cluster\",empty_lst)\t\n\n#for i in range(n_clusters_):\n    #print(\"Cluster %d:\" % i,' %s' % empty_lst[i])\n    #print(\"Cluster %d:\" % i,' %s' % len(empty_lst[i]))\n\n# all abstracts in a cluster is now a single token\nclust = []\nfor i in range(len(empty_lst)):\n    t= ' '.join(empty_lst[i])\n    clust.append(t)\n#print(\"These are the abstracts in a cluster\",clust)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building word-cluster count matrix\n\ndef fn_tdm_df(docs, xColNames = None):\n    ''' create a term document matrix as pandas DataFrame\n    with **kwargs you can pass arguments of CountVectorizer\n    if xColNames is given the dataframe gets columns Names'''\n\n    #initialize the  vectorizer\n    vectorizer = CountVectorizer()\n    x1 = vectorizer.fit_transform(clust)\n    #create dataFrame\n    df = pd.DataFrame(x1.toarray().transpose(), index = vectorizer.get_feature_names()) \n    \n    if xColNames is not None:\n        df.columns =  xColNames\n\n    return df\n\nfor i in range(len(clust[:5])):\n        \n    count_matrix=fn_tdm_df(docs=clust, xColNames =None)\n#print(\"This is the count matrix\",count_matrix.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building word-cluster PMI matrix\n\ndef pmi(df):\n    '''\n    Calculate the positive pointwise mutal information score for each entry\n    https://en.wikipedia.org/wiki/Pointwise_mutual_information\n    We use the log( p(y|x)/p(y) ), y being the column, x being the row\n    '''\n    # Get numpy array from pandas df\n    arr = df.as_matrix()\n\n    # p(y|x) probability of each t1 overlap within the row\n    row_totals = arr.sum(axis=1).astype(float)\n    prob_cols_given_row = (arr.T / row_totals).T\n\n    # p(y) probability of each t1 in the total set\n    col_totals = arr.sum(axis=0).astype(float)\n    prob_of_cols = col_totals / sum(col_totals)\n\n    # PMI: log( p(y|x) / p(y) )\n    # This is the same data, normalized\n    ratio = prob_cols_given_row / prob_of_cols\n    ratio[ratio==0] = 0.00001\n    _pmi = np.log(ratio)\n    _pmi[_pmi < 0] = 0\n\n    return _pmi\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PMI matrix into a data frame\nc=pmi(count_matrix)\nvectorizer = CountVectorizer()\nx1 = vectorizer.fit_transform(clust)\nPMI_df = pd.DataFrame(c, index = vectorizer.get_feature_names()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the labels of each cluster with a positive pmi score into a list\nList = []\nfor i in range(\nlen(empty_lst)):\n    col=PMI_df[i]\n    List.append(col[col>0].sort_values(ascending=False).to_dict())\n#print('These are the label vectors for each cluster')\n\nfor i in range(n_clusters_):\n    print(\"Cluster_labels %d:\\n\" % i,' %s' % List[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These are the keywords with their corresponding PMI scores\nfor i in range(n_clusters_):\n    print(\"Cluster_labels %d:\\n\" % i,' %s' % List[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(List[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# these are the words that DBSCAN clustered\nempty_lst[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# APPLY LDA TO EXTRACT KEY WORDS\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}