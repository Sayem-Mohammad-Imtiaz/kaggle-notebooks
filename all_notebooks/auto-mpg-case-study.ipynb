{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1 : Importing and Understanding Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing libraries and shuffling the data\ndf = pd.read_csv('/kaggle/input/autompg-dataset/auto-mpg.csv')\ndf = df.sample(frac = 1,random_state = 3)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the unqiue values\ndf['origin'].unique()\n\n#here we see there is '?' here.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To check the shape of Dataframe\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To check the data-type of the dataframe\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To check descriptive statistics of data frame\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()*100/398","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No mising values"},{"metadata":{},"cell_type":"markdown","source":"# Cleaning The Data"},{"metadata":{},"cell_type":"markdown","source":"# Removing ? from hp and converting its data type to INT"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing ?, replacing with mean value and conver the datatype to integer\ndf['horsepower'] = df['horsepower'].replace('?',np.nan)\ndf['horsepower'] = df['horsepower'].astype(float)\ndf['horsepower'] = df['horsepower'].replace(np.nan,df['horsepower'].mean())\ndf['horsepower'] = df['horsepower'].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"? removed and converted into int type"},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"# Bar Plot for car name"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['car name'].value_counts().nlargest(15).plot(kind='bar', figsize=(15,5))\nplt.title(\"Number of vehicles by car brand name\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('car name');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ford Pinto is the car name which has most number of vehicles and Ford Marverick holds the second poisition**"},{"metadata":{},"cell_type":"markdown","source":"# Histogram for mpg\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mpg'].hist(bins = 5,color = 'red')\nplt.title(\"Miles per gallon of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Miles per gallon');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Histogram for cylinders"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cylinders'].hist(bins = 5,color = 'red')\nplt.title(\"Number of Cylinders in a vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Numer of Cylinder');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Maximum number of cars are having cylinder between 4 to 5**"},{"metadata":{},"cell_type":"markdown","source":"# Histogram for displacement"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['displacement'].hist(bins = 5,color = 'red')\nplt.title(\"Displacement of the vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Displacement');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Maximum number of cars are having dispalcement between 70 to 140**"},{"metadata":{},"cell_type":"markdown","source":"# Histogram for weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['weight'].hist(bins = 5,color = 'red')\nplt.title(\"Weight of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Weight');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Max weight of car are between 1600 to 3000  **"},{"metadata":{},"cell_type":"markdown","source":"# Histogram for acceleration"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['acceleration'].hist(bins = 5,color = 'red')\nplt.title(\"Acceleration of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Acceleration');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**General acceleration of cars are between 15.0 to 18.0**"},{"metadata":{},"cell_type":"markdown","source":"# Histogram for model year"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['model year'].hist(bins = 5,color = 'red')\nplt.title(\"Model year of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Year');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can say that most of the cars were made during the year 1970-72, 1975-77 and 1980-82 **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['mpg','cylinders','displacement','weight','acceleration','model year','origin']].hist(figsize=(10,8),bins=6,color='Y')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"# Box plot btw mpg and weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"mpg\", y=\"weight\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see that as the weight of the car increases, the mileage decreases which is obvious because they are inversely proportional to each other**"},{"metadata":{},"cell_type":"markdown","source":" # Box plot btw mpg and displacement"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"mpg\", y=\"displacement\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see that as the displacement increases,mileage decreases.**"},{"metadata":{},"cell_type":"markdown","source":"# Box plot btw horsepower and weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"horsepower\", y=\"weight\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can clearly see that as the weight increases,the horsepower increases.Otherwise car with heavy weight won't run without greater horsepower. So they are directly proportional**"},{"metadata":{},"cell_type":"markdown","source":"# Scatter plot for weight of car and acceleration"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = 'weight',y = 'acceleration',data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It is showing that lesser the weight of car,more is the acceleration.**"},{"metadata":{},"cell_type":"markdown","source":"# Correlation Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(13,7))\na = sns.heatmap(corr, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Statistical Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import library\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop car name\ndf = df.drop('car name',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking target variable and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing vif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([vif(Xc.values,i) for i in range(Xc.shape[1])],index = Xc.columns,columns=['VIF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting model\nmodel = sm.OLS(y,Xc).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here wee see that cylindes,horsepower,acceleration all three are not significant they are above 0.05 so we will drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping cylinders,horsepower,acceleration and fiting into model\nXc = Xc.drop(['cylinders','horsepower','acceleration'],axis = 1)\nmodel = sm.OLS(y,Xc).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test of Assumptions"},{"metadata":{},"cell_type":"markdown","source":"# 1. Test of Normality"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library and creating distplot\nfrom scipy.stats import norm\nnorm.fit(model.resid)\nsns.distplot(model.resid,fit = norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here wee se its almost normal."},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing library and creating probplot\nimport scipy.stats as st\nst.probplot(model.resid,plot = plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ho: Data is normal\n\nH1: Data is not normal"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Jarque Bera Test to check normality\nst.jarque_bera(model.resid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"p-value very close to zero so reject null hypothesis. Hence JB Test indicate that residuals are not normal."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Transformation\nly = np.log(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building model\nmodel = sm.OLS(ly,Xc).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Again creating distplot after applying log\nfrom scipy.stats import norm\nnorm.fit(model.resid)\nsns.distplot(model.resid,fit = norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Again creating probplot after applying log\nimport scipy.stats as st\nst.probplot(model.resid,plot = plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Again performing JB Test\nst.jarque_bera(model.resid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"p-value again closed to zero, so reject Ho. Hence Residuals are not normal"},{"metadata":{},"cell_type":"markdown","source":"# 2.Test of Homoscedasticity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting X and y and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sm.OLS(y,Xc).fit()\ny_pred = model.predict(Xc)\nresids = model.resid\n\nsns.regplot(x = y_pred,y = resids,lowess=True,line_kws={'color':'red'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Goldfeld Test for Checking Homoscedasticity\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\nname = ['F-Statistics','p-value']\ntest = sms.het_goldfeldquandt(model.resid,model.model.exog)\nprint(lzip(name,test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ho: Variance of residuals is constant across the range of data.\nH1: Variance of residuals is not constant across the range of data.\n    \nSince p-value is more than 0.05 we will accept  Ho to conclude that residuals is constatnt across the range of data."},{"metadata":{},"cell_type":"markdown","source":"# 3.Test of Autocorrelation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting X and y and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitiing in model and applying autocorrleation\nmodel = sm.OLS(y,Xc).fit()\nimport statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(model.resid,lags = 30,alpha = 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Durbin-Watson statistic for any model will be between 0 and 4. To conclude that there is auto-correlation DW-statistic should be around 2. The DW statistic for the model is 2.028 and this indicate moderate level of autocorrelation."},{"metadata":{},"cell_type":"markdown","source":"# 4.Test of Linearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting X and y and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting to model and applying linearity\nmodel = sm.OLS(y,Xc).fit()\ny_pred = model.predict(Xc)\nresids = model.resid\n\nsns.regplot(x = y_pred,y = resids,lowess=True,line_kws={'color':'red'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting to model and applying linearity\nmodel = sm.OLS(y,Xc).fit()\ny_pred = model.predict(Xc)\nresids = model.resid\n\nsns.regplot(x = y,y = y_pred,lowess=True,line_kws={'color':'red'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pattern expected when residuals are plotted against y_pred is a horizontal line around zero(0).\nThe pattern in the graph suggests that model design need to improve(we may need to add square and multiplicative terms to the model).\n\nThe expected pattern when y and y_pred is plotted is a line passing through origin with a slope of 45 degrees.\n\n((#if model is perfect then price = price_predicted))\n\nHowever the model shows a pattern away from expected."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rainbow test for Linearity\nimport statsmodels.api as sm\nsm.stats.diagnostic.linear_rainbow(res = model,frac = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ho: residuals exhibit linearity\nH1: residuals exhibit non-linearity\n\nBased on p-value , we say that residuals exhibhit linearity OR there is no non-linearity."},{"metadata":{},"cell_type":"markdown","source":"# Building Linear Regression Model"},{"metadata":{},"cell_type":"markdown","source":"### Splitting data into training and testing sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying in target variables\ny = df['mpg']\nX = df.drop('mpg',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataframe in 70:30(train and test)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the shape of train and test\nprint(X_train.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Performing Linear Regression and calculating error terms"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the library and building the model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n\ny_test_pred = lr.predict(X_test)\ny_train_pred = lr.predict(X_train)\n\nprint('r-square of Train :',r2_score(y_train,y_train_pred))\nprint('rmse for Train: ',np.sqrt(mean_squared_error(y_train,y_train_pred)))\n\nprint('\\n')\nprint('r-square of Test :',r2_score(y_test,y_test_pred))\nprint('rmse for Test: ',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a 4% difference in train and test."},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"# 1. Recursive Feature Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import r2_score,mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting lr to rfe\nlr = LinearRegression()\nrfe = RFE(lr,n_features_to_select = 8)\nrfe.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the rfe support function\nrfe.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(rfe.ranking_,index = X.columns,columns=['SELECT'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This will give 8 best features with rank 1 and rest features with subsequent ranks."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying train test split method and splitting the data in 70:30 ratio\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_of_cols = X_train.shape[1]\nr2score = []\nrmse = []\n\nlr = LinearRegression()  #estimator\nfor i in range(no_of_cols):\n    rfe = RFE(lr,n_features_to_select=i+1)\n    rfe.fit(X_train,y_train)\n    y_test_pred = rfe.predict(X_test)\n    \n    #for r2score\n    r2 = r2_score(y_test,y_test_pred)\n    r2score.append(r2)\n    \n    #for rmse\n    rms = np.sqrt(mean_squared_error(y_test,y_test_pred))\n    rmse.append(rms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot for r2score\nplt.plot(range(1,8),r2score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing r2score\nr2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot for rmse\nplt.plot(range(1,8),rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing rmse\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparamter Tuning to find the optimal number of features to keep"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold,GridSearchCV\n\nparams = {'n_features_to_select':list(range(1,8))}\nlr = LinearRegression()\nrfe = RFE(lr)  #estimator\n\nkf = KFold(n_splits=3,random_state=3)\n\ngsearch = GridSearchCV(rfe,param_grid=params,scoring = 'r2',cv = kf,return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to find the best parameter for our data\ngsearch.get_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forward selection to decide the best number of features to keep in the model(Step Forward Selection)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the SFS\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nsfs1 = sfs(lr,k_features=7,scoring='r2',cv = 3,verbose = 2)\nsfs1.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the dataframe as subset and transforming it\nsf = pd.DataFrame(sfs1.subsets_).T\nsf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(sf['avg_score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(sfs1.k_feature_names_)\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X[cols], y, test_size=0.3, random_state=3)\n\nlr.fit(X_train, y_train)\ny_test_pred = lr.predict(X_test)\n\nplt.scatter(y_test, y_test_pred)\nplt.plot(y_test, y_test,'r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all the point are scattered close to the line."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}