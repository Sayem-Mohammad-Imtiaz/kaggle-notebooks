{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Insurance Churn Prediction\n\nInsurance companies around the world operate in a very competitive environment. With various aspects of data collected from millions of customers, it is painstakingly hard to analyze and understand the reason for a customerâ€™s decision to switch to a different insurance provider.\n\nFor an industry where customer acquisition and retention are equally important, and the former being a more expensive process, insurance companies rely on data to understand customer behavior to prevent retention. Thus knowing whether a customer is possibly going to switch beforehand gives Insurance companies an opportunity to come up with strategies to prevent it from actually happening.\n\n# Task\n\nGiven are 16 distinguishing factors that can help in understanding the customer churn, your objective as a data scientist is to build a Machine Learning model that can predict whether the insurance company will lose a customer or not using these factors.\n\nYou are provided with 16 anonymized factors (feature_0 to feature 15) that influence the churn of customers in the insurance industry.\n\n*Build a Machine Learning model that can predict whether the insurance company will lose a customer or not using these factors.*"},{"metadata":{},"cell_type":"markdown","source":"# 1. Read and import all data files"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfigure = plt.figure()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#read data\ntrain = pd.read_csv(\"/kaggle/input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/Train.csv\")\ntest = pd.read_csv(\"/kaggle/input/insurance-churn-prediction-weekend-hackathon/Insurance_Churn_ParticipantsData/Test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Features Analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 **Check Categorical features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it is clearly seen there is no categorical feature so there is no need of categorical mapping"},{"metadata":{},"cell_type":"markdown","source":"### 2.2 **Check missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing value."},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Univirate Analysis\n\nThis analysis helps us in removing the outliers present in the dataset that may lead to overfit the model with noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    plot = plt.boxplot(train[col])\n    print(f'plot of feature {col} is {plot}')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the help of plots, lets check the index of main outliers so we can delete them."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['feature_1']>24].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['feature_3']>15].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['feature_4']>16].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['feature_6']>20].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us drop these outliers from dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train.drop([5445, 5606, 29608, 20042, 17893, 20894, 32159, 7705])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Bi-variate Analysis\nCheck which feature is relevant or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Split the features and lables from data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[[col for col in train.columns if not col == 'labels']]\nX = X.set_index('feature_0')\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['labels']\ny.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Splitting the dataset into the Training set and Test set\n\nWe divide the data here into 80% train set and 20% test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,train_size=0.8, test_size=0.2,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Create Model\n\nHere I'm using XGBOOST for classification.\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way."},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Import model\n\nHere the tuning of parameters is alraedy done by me."},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel_xgb  = XGBClassifier(n_estimators = 178,\n                       eta = 0.17,\n                       booster_pram = 'dart',\n                       tree_method = 'hist',\n                       scale_pos_weight= 5,\n                       max_bin=215,\n                       random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(train_X,\n          train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Make the Prediction on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model_xgb.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Evaluate these predictions using F1 score metric\n\nThe F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n\nF1 = 2 * (precision * recall) / (precision + recall)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_score(val_y,predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.63 is pretty good score on this dataset. It is improved by doing some feature engineering, etc. \nYou can try different algorithms too."},{"metadata":{},"cell_type":"markdown","source":"### 4.5  Confusion Matrix\n\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion matrix \\n\",confusion_matrix(val_y,predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keep supporting!\n\n\n## Any advice would be appreciated."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}