{"cells":[{"metadata":{},"cell_type":"markdown","source":"Objective: Create an AI system that can tell the latitude and longitude coordinates of a satellite image, given data of satellite images and their respective coordinates\n\nCategory: Supervised Learning, Regression Problem, Image Recognition and Processing, Convolutional Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"First, we shall import the relevant modules.\n\nnumpy: linear algebra  \npandas: data processing  \nPIL, cv2: image manipulation  \nre (regex): natural sorting  \nmatplotlib: data visualization  \nmath: calculate distance  \nrandom: randomization\n\nWe will be using the Keras library for our convolutional neural network."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport PIL.Image\nimport cv2\nimport re\nimport matplotlib.pyplot as plt\nimport random\nimport math\n\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers import MaxPool2D\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras.callbacks import EarlyStopping\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will be defining functions to help with the code."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#this function orders the images using natural sort\n#normal sort order: img01, img135, img296, img37\n#natural sorted order: img01, img37, img296, img135\ndef natural_key(astr):\n    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', astr)]\n\n#helper function to get an ndarray from image files\ndef load_image_array(directory):\n    dfa = []\n    filelist = sorted(os.listdir(directory), key=natural_key)\n    for filename in filelist:\n        path = directory + '/' + filename\n        image = PIL.Image.open(path) # open colour image\n        image = np.array(image)\n        image = image[:, :, :3] #remove transparency layer (alpha)\n        dimension=(200,200) #specify number of pixels\n        rimage = cv2.resize(image, dimension, interpolation=cv2.INTER_AREA) #resize images to specified size\n        dfa.append(rimage)\n    dfnew = np.array(dfa) #convert list to ndarray\n    \n    return dfnew\n\n# get loss for training set and validation set\n# plot for each epoch\ndef plot_loss(num, history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model ' + str(num) + ' loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n\n#haversine function to calculate distance\n#shortest distance above earth's surface\ndef haversine(n1, w1, n2, w2):\n    earthrad = 6371000\n    phi_1 = math.radians(n1)\n    phi_2 = math.radians(n2)\n    phi_change = math.radians(n2 - n1)\n    lambda_change = math.radians(w1 - w2) #(reversed because west is negative)\n    \n    a = math.sin(phi_change/2) ** 2 + math.cos(phi_1) * math.cos(phi_2) * math.sin(lambda_change/2) ** 2\n    c = 2 * math.atan2(a ** 0.5, (1 - a) ** 0.5)\n    d = earthrad * c\n    return d\n\n#distance between top left and bottom right coordinates of area covered (maximum distance)\nprint(haversine(40.57770678, 73.74077701, 40.90365244, 74.03151652))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#start actual code\ntrainDirectory = '../input/train-images/trainimg'\ntestDirectory = '../input/test-images/imagestest'\ntrainX = load_image_array(trainDirectory)\ntestX = load_image_array(testDirectory)\ntrainY = pd.read_csv('../input/train-coordinates/traincoord.txt', sep=\" \", header=None)\ntrainY.columns = ['North', 'West']\ntestY = pd.read_csv('../input/test-coordinates/testcoord.txt', sep=\" \", header=None)\ntestY.columns = ['North', 'West']\n\nmaxNorth = trainY['North'].max()\nminNorth = trainY['North'].min()\nmaxWest = trainY['West'].max()\nminWest = trainY['West'].min()\nprint(maxNorth)\nprint(minNorth)\nprint(maxWest)\nprint(minWest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset is made of satellite images, and their respective coordinates. The satellite images are taken from Google Earth, and they cover the coordinates of New York. They are square images that cover a side length of approximately 1700 metres. The coordinate recorded is that of the center of the satellite image.\n\nTrain Images: 400 images (200px by 200px)  \nTest Images: 100 images (200px by 200px)\n\nSince the scope of our problem description requires the model to find the image coordinates, we need the model to see other images surrounding the area to identify the features in the area. Therefore, the train images will cover a predefined area of New York, while the test images need to be chosen within that area, so that the model will be accurate.\n\nHere is a sample training instance (image and its respective coordinates)."},{"metadata":{"trusted":true},"cell_type":"code","source":"randnum = random.randrange(400)\nif randnum < 10:\n    sample_image_path = trainDirectory + '/img0' + str(randnum) + '.PNG'\nelse:\n    sample_image_path = trainDirectory + '/img' + str(randnum) + '.PNG'\nsample_image = PIL.Image.open(sample_image_path)\nplt.figure()\nplt.imshow(sample_image) \nplt.show()  # display it\nprint(trainY.iloc[randnum])\n\n#note that we have used the original image to show here, so the pixel size is different from (200,200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using a Convolutional Neural Network to train the model. Since this is a regression problem, we will be adding a fully connected layer with linear activation (ReLU: rectified linear unit).  \nWe will be looking at 3 possible models, and we will find the best model to train."},{"metadata":{},"cell_type":"markdown","source":"Model 1: Conv2D (7x7) -> Conv2D (3x3) -> BatchNormalization -> MaxPool2D (2x2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\nfilters = [16, 32, 64]\n\nfor f in filters:\n    model1.add(Conv2D(f, kernel_size=7, activation='relu', input_shape=(200,200,3), padding='same'))\n    model1.add(Conv2D(f, kernel_size=3, activation='relu', input_shape=(200,200,3), padding='same'))\n    model1.add(BatchNormalization())\n    model1.add(MaxPool2D((2,2)))\n\nmodel1.add(Flatten())\nmodel1.add(Dense(3, activation='relu'))\nmodel1.add(BatchNormalization())\nmodel1.add(Dropout(0.25))\nmodel1.add(Dense(2, activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model 2: Conv2D (3x3) -> BatchNormalization -> MaxPool2D (2x2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\nfilters = [16, 32, 64]\n\nfor f in filters:\n    model2.add(Conv2D(f, kernel_size=3, activation='relu', input_shape=(200,200,3), padding='same'))\n    model2.add(BatchNormalization())\n    model2.add(MaxPool2D((2,2)))\n\nmodel2.add(Flatten())\nmodel2.add(Dense(3, activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.25))\nmodel2.add(Dense(2, activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model 3: Conv2D (3x3) -> BatchNormalization -> Conv2D (3x3, strides=(2,2))"},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = Sequential()\nfilters = [16, 32, 64]\n\nfor f in filters:\n    model3.add(Conv2D(f, kernel_size=3, activation='relu', input_shape=(200,200,3), padding='same'))\n    model3.add(BatchNormalization())\n    model3.add(Conv2D(f, kernel_size=3, strides=(2,2), activation='relu', input_shape=(200,200,3), padding='same'))\n\nmodel3.add(Flatten())\nmodel3.add(Dense(3, activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(Dropout(0.25))\nmodel3.add(Dense(2, activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling coordinates from zero to one for better performance\nmaxNorth = trainY['North'].max()\nminNorth = trainY['North'].min()\ndiffNorth = maxNorth - minNorth\nmaxWest = trainY['West'].max()\nminWest = trainY['West'].min()\ndiffWest = maxWest - minWest\n\ntrainY['North'] = (trainY['North'] - minNorth) / diffNorth\ntrainY['West'] = (trainY['West'] - minWest) / diffWest\ntestY['North'] = (testY['North'] - minNorth) / diffNorth\ntestY['West'] = (testY['West'] - minWest) / diffWest\n\nes1 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nes2 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nes3 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n\nmodel1.compile(loss='mean_squared_error', optimizer='adam')\nmodel2.compile(loss='mean_squared_error', optimizer='adam')\nmodel3.compile(loss='mean_squared_error', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model1.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=10, callbacks=[es1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model2.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=10, callbacks=[es2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history3 = model3.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=10, callbacks=[es3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get loss for training set and validation set\n# plot for each epoch    \nplot_loss(1, history1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(2, history2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(3, history3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred1 = model1.predict(testX)\npred2 = model2.predict(testX)\npred3 = model3.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred1[:,0] = pred1[:,0] * diffNorth + minNorth\npred1[:,1] = pred1[:,1] * diffWest + minWest\npred2[:,0] = pred2[:,0] * diffNorth + minNorth\npred2[:,1] = pred2[:,1] * diffWest + minWest\npred3[:,0] = pred3[:,0] * diffNorth + minNorth\npred3[:,1] = pred3[:,1] * diffWest + minWest\ntestY['North'] = testY['North'] * diffNorth + minNorth\ntestY['West'] = testY['West'] * diffWest + minWest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use haversine formula to calculate distance\ndist1 = np.empty(100)\ndist2 = np.empty(100)\ndist3 = np.empty(100)\nfor count in range(100):\n    dist1[count] = haversine(pred1[count,0], pred1[count,1], testY['North'].iloc[count], testY['West'].iloc[count])\n    dist2[count] = haversine(pred2[count,0], pred2[count,1], testY['North'].iloc[count], testY['West'].iloc[count])\n    dist3[count] = haversine(pred3[count,0], pred3[count,1], testY['North'].iloc[count], testY['West'].iloc[count])\n\nprint(dist1[:5])\nprint(dist2[:5])\nprint(dist3[:5])\n\navg1 = dist1.mean()\navg2 = dist2.mean()\navg3 = dist3.mean()\n\nstd1 = dist1.std()\nstd2 = dist2.std()\nstd3 = dist3.std()\n\nprint(str(avg1) + ' ' + str(std1))\nprint(str(avg2) + ' ' + str(std2))\nprint(str(avg3) + ' ' + str(std3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, we found out that Model 1 has an average runtime of 38 seconds per epoch."},{"metadata":{},"cell_type":"markdown","source":"The average distance between the predicted point and actual coordinates is about 8000 metres. The distance of the side length of the image is 1700 metres, while the maximum distance possible is 40000 metres. The average distance between two random points within the area is about 10 to 15 kilometres.  \nAlthough the error was quite big, it was due to the fact that the data given to the model is less, and the model will improve its accuracy given more data. It is successful, as it is a few times better than purely random guessing.\n\nLimitations: There was no data of images labelled with their coordinates, so we had to manually extract the data from Google Earth using Snipping Tool. Moreover, all the images had slightly different number of pixels, so resizing them may have caused the pixels to be different, affecting our results.  \nThe area covered was quite small (about 20km * 35km), so the most difference between coordinates was about 0.5 degrees for latitude and 0.3 degrees for longitude. Therefore, the errors between the predicted output and actual output were magnified, and the accuracy of the model decreased.\n\nConclusions and Recommendations: The model was reasonably accurate for the data that we had, and it will be able to perform with a better accuracy when given more data.\n\nFuture Work: This project can be extended further, when we use photos taken from smartphones as the images. This will increase the usability of the model, as it is more applicable in the real world where billions of photos are present. Some real time applications include finding where a person is, when asking them to take a picture of their whereabouts. In this case, we can have another output attribute be the landmark nearest to the coordinates, so that people can easily track the landmark while also having the coordinates. This application will be very useful, due to its high usability."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}