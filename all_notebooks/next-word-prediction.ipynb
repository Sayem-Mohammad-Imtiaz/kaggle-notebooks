{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import necessary libraries and packages\n\nimport pandas as pd\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data = pd.read_csv(r\"C:\\Users\\parit\\OneDrive\\Desktop\\AI for ALL\\medium_data.csv\\medium_data.csv\")\nmedium_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Here, we have a 10 different fields and 6508 records but we will only use title field for predicting next word.\nprint(\"Number of records: \", medium_data.shape[0])\nprint(\"Number of fields: \", medium_data.shape[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display titles of various articles and preprocess them\nmedium_data['title']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing unwanted characters and words in titles","metadata":{}},{"cell_type":"markdown","source":"Looking at titles, we can see there are some of unwanted characters and words in it which can not be useful for us to predict infact it might decrease our model accuracy so we have to remove it.","metadata":{}},{"cell_type":"code","source":"medium_data['title'] = medium_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\nmedium_data['title'] = medium_data['title'].apply(lambda x: x.replace('\\u200a',' '))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenzation\n","metadata":{}},{"cell_type":"markdown","source":"Tokenzaion is the process in which we provide an unique id to all the words and make a word index or we can say vocabulary.","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\ntokenizer.fit_on_texts(medium_data['title'])\ntotal_words = len(tokenizer.word_index) + 1\n\nprint(\"Total number of words: \", total_words)\nprint(\"Word: ID\")\nprint(\"------------\")\nprint(\"<oov>: \", tokenizer.word_index['<oov>'])\nprint(\"Strong: \", tokenizer.word_index['strong'])\nprint(\"And: \", tokenizer.word_index['and'])\nprint(\"Consumption: \", tokenizer.word_index['consumption'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Titles text into sequences and make n_gram model\nsuppose, we have sentence like \"I am Paritosh\" and this will convert into a sequence with their respective tokens {'I': 1,'am': 2,'Paritosh': 3}. Thus, output will be [ '1' ,'2' ,'3' ]\n\nLikewise, our all titles will be converted into sequences.\n\nThen, we will make a n_gram model for good prediction.","metadata":{}},{"cell_type":"code","source":"input_sequences = []\nfor line in medium_data['title']:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    #print(token_list)\n    \n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n# print(input_sequences)\nprint(\"Total input sequences: \", len(input_sequences))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Make all titles with same length by using padding\nThe length of every title has to be the same. To make it, we need to find a title that has a maximum length, and based on that length, we have to pad rest of titles.","metadata":{}},{"cell_type":"code","source":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\ninput_sequences[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare features and labels","metadata":{}},{"cell_type":"markdown","source":"Here, we consider last element of all sequences as a label.Then, We need to perform onehot encoding on labels corresponding to total_words.","metadata":{}},{"cell_type":"code","source":"# create features and label\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes=total_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(xs[5])\nprint(labels[5])\nprint(ys[5][14])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bi- LSTM Neural Network Model training","metadata":{}},{"cell_type":"raw","source":"Long Short-Term Memory (LSTM) networks is an advance recurrent neural network which is apable to store order states by using its cell state feature.","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150)))\nmodel.add(Dense(total_words, activation='softmax'))\nadam = Adam(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(xs, ys, epochs=50, verbose=1)\n#print model.summary()\nprint(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting model accuracy and loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(history, 'accuracy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(history, 'loss')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting next word of title","metadata":{}},{"cell_type":"code","source":"seed_text = \"implementation of\"\nnext_words = 2\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}