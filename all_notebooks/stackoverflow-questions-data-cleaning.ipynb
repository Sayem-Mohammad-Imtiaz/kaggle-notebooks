{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align:center;\"><img src=\"http://www.mf-data-science.fr/images/projects/intro.jpg\" style='width:100%; margin-left: auto; margin-right: auto; display: block;' /></div>","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color: #641E16\">Contexte</span>\nNous allons ici développer un algorithme de Machine Learning destiné à assigner automatiquement plusieurs tags pertinents à une question posée sur le célébre site Stack overflow.     \nCe programme s'adresse principalement aux nouveaux utilisateurs, afin de leur suggérer quelques tags relatifs à la question qu'ils souhaitent poser.\n\n### Les données sources\nLes données ont été captées via l'outil d’export de données ***stackexchange explorer***, qui recense un grand nombre de données authentiques de la plateforme d’entraide.     \nElles portent sur la période 2009 / 2020 et **uniquement sur les posts \"de qualité\"** ayant au minimum 1 réponse, 5 commentaires, 20 vues et un score supérieur à 5.\n\n### Objectif de ce Notebook\nDans ce Notebook, nous allons traiter la partie **data cleaning et exploration des données**. Un second notebook traitera ensuite les approches supervisées et non supervisées pour traiter la création de Tags à partir des données textuelles.     \n\nTous les Notebooks du projet seront **versionnés dans Kaggle mais également dans un repo GitHub** disponible à l'adresse https://github.com/MikaData57/Analyses-donnees-textuelles-Stackoverflow","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#641E16\">Sommaire</span>\n1. [Importation et description des données](#section_1)\n2. [Data exploration](#section_2)\n3. [Nettoyage des questions](#section_3)\n4. [Nettoyage des titres](#section_4)\n5. [Export du dataset nettoyé](#section_5)","metadata":{}},{"cell_type":"code","source":"# Install package for PEP8 verification\n!pip install pycodestyle\n!pip install --index-url https://test.pypi.org/simple/ nbpep8","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:05.690181Z","iopub.execute_input":"2021-06-25T08:15:05.690621Z","iopub.status.idle":"2021-06-25T08:15:18.957206Z","shell.execute_reply.started":"2021-06-25T08:15:05.690523Z","shell.execute_reply":"2021-06-25T08:15:18.956046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Beautifulsoup4\n!pip install beautifulsoup4","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:18.958879Z","iopub.execute_input":"2021-06-25T08:15:18.959158Z","iopub.status.idle":"2021-06-25T08:15:24.957623Z","shell.execute_reply.started":"2021-06-25T08:15:18.959128Z","shell.execute_reply":"2021-06-25T08:15:24.956513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install langdetect\n!pip install langdetect","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:24.959938Z","iopub.execute_input":"2021-06-25T08:15:24.960375Z","iopub.status.idle":"2021-06-25T08:15:32.53544Z","shell.execute_reply.started":"2021-06-25T08:15:24.960325Z","shell.execute_reply":"2021-06-25T08:15:32.533632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Python libraries\nimport os\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nfrom langdetect import detect\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.snowball import EnglishStemmer\nimport spacy\nfrom spacy import displacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n# Library for PEP8 standard\nfrom nbpep8.nbpep8 import pep8","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-25T08:15:32.537034Z","iopub.execute_input":"2021-06-25T08:15:32.537319Z","iopub.status.idle":"2021-06-25T08:15:35.237321Z","shell.execute_reply.started":"2021-06-25T08:15:32.537273Z","shell.execute_reply":"2021-06-25T08:15:35.236163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nsns.set_style(\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:35.240791Z","iopub.execute_input":"2021-06-25T08:15:35.241122Z","iopub.status.idle":"2021-06-25T08:15:35.246273Z","shell.execute_reply.started":"2021-06-25T08:15:35.241091Z","shell.execute_reply":"2021-06-25T08:15:35.245136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('popular')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:35.247564Z","iopub.execute_input":"2021-06-25T08:15:35.247921Z","iopub.status.idle":"2021-06-25T08:15:35.988896Z","shell.execute_reply.started":"2021-06-25T08:15:35.247889Z","shell.execute_reply":"2021-06-25T08:15:35.987679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color: #641E16\" id=\"section_1\">Importation et description des données</span>","metadata":{}},{"cell_type":"code","source":"# Define path to data\npath = '../input/stackoverflow-questions-filtered-2011-2021/'\n\n# Concat all CSV datasets in one Pandas DataFrame\ndf_columns = pd.read_csv(path+'StackOverflow_questions_2009.csv').columns\ndata = pd.DataFrame(columns=df_columns)\nfor f in os.listdir(path):\n    if(\"cleaned\" not in f):\n        temp = pd.read_csv(path+f)\n        data = pd.concat([data, temp], \n                         axis=0,\n                         ignore_index=True)\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:35.99055Z","iopub.execute_input":"2021-06-25T08:15:35.990845Z","iopub.status.idle":"2021-06-25T08:15:41.169559Z","shell.execute_reply.started":"2021-06-25T08:15:35.990817Z","shell.execute_reply":"2021-06-25T08:15:41.168526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print full dataset infos\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:41.171926Z","iopub.execute_input":"2021-06-25T08:15:41.172234Z","iopub.status.idle":"2021-06-25T08:15:41.238661Z","shell.execute_reply.started":"2021-06-25T08:15:41.172206Z","shell.execute_reply":"2021-06-25T08:15:41.237382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe data\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:41.240546Z","iopub.execute_input":"2021-06-25T08:15:41.240877Z","iopub.status.idle":"2021-06-25T08:15:42.00011Z","shell.execute_reply.started":"2021-06-25T08:15:41.240845Z","shell.execute_reply":"2021-06-25T08:15:41.998881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le jeu de données ne compte pas de valeurs nulles. La variable Id ne compte que des valeurs uniques, nous pouvons donc l'utiliser en index :","metadata":{}},{"cell_type":"code","source":"data.set_index('Id', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:42.002563Z","iopub.execute_input":"2021-06-25T08:15:42.003392Z","iopub.status.idle":"2021-06-25T08:15:42.039728Z","shell.execute_reply.started":"2021-06-25T08:15:42.003267Z","shell.execute_reply":"2021-06-25T08:15:42.038871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color: #641E16\" id=\"section_2\">Data exploration</span>\n\nDans un premier temps, nous allons regarder l'**évolution du nombre de questions par année** dans notre jeu de données.","metadata":{}},{"cell_type":"code","source":"# Convert CreationDate to datetime format\ndata['CreationDate'] = pd.to_datetime(data['CreationDate'])\n\n# Grouper with 1 year delta\npost_year = data.groupby(pd.Grouper(key='CreationDate',\n                                    freq='1Y')).agg({'Title': 'count'})\n\n# Plot evolution\nfig = plt.figure(figsize=(15,6))\nsns.lineplot(data=post_year, x=post_year.index, y='Title')\nplt.axhline(post_year.Title.mean(), \n            color=\"r\", linestyle='--',\n            label=\"Mean of question per year : {:04d}\"\\\n                   .format(int(post_year.Title.mean())))\nplt.xlabel(\"Date of questions\")\nplt.ylabel(\"Number of questions\")\nplt.title(\"Number of questions evolution from 2009 to 2020\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:42.04077Z","iopub.execute_input":"2021-06-25T08:15:42.04118Z","iopub.status.idle":"2021-06-25T08:15:42.521508Z","shell.execute_reply.started":"2021-06-25T08:15:42.04115Z","shell.execute_reply":"2021-06-25T08:15:42.520367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque ici que sur nos critères de sélection, le nombre de questions posées a tendance à diminuer de manière constante depuis 2014.\n\nNous allons à présent vérifier la **longeur des différents titres** de la base :","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=data.Title.str.len())\nstart, end = ax.get_xlim()\nax.xaxis.set_ticks(np.arange(0, end, 5))\nplt.axvline(data.Title.str.len().median() - data.Title.str.len().min(),\n            color=\"r\", linestyle='--',\n            label=\"Title Lenght median : \"+str(data.Title.str.len().median()))\nax.set_xlabel(\"Lenght of title\")\nplt.title(\"Title lenght of Stackoverflow questions\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:42.522862Z","iopub.execute_input":"2021-06-25T08:15:42.523181Z","iopub.status.idle":"2021-06-25T08:15:43.576851Z","shell.execute_reply.started":"2021-06-25T08:15:42.523152Z","shell.execute_reply":"2021-06-25T08:15:43.575613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons également ploter la répartition des **longueurs de la variable** `Body` *(les corps de texte des questions)*. L'étendue étant très importante, nous allons dans un premier temps **discrétiser ces longueur** pour ne pas surcharger les temps de calculs de projection graphique :","metadata":{}},{"cell_type":"code","source":"# Discretizer for Body characters lenght\nX = pd.DataFrame(data.Body.str.len())\n\n# Sklearn discretizer with 200 bins\ndiscretizer = KBinsDiscretizer(n_bins=200,\n                               encode='ordinal',\n                               strategy='uniform')\nbody_lenght = discretizer.fit_transform(X)\nbody_lenght = discretizer.inverse_transform(body_lenght)\nbody_lenght = pd.Series(body_lenght.reshape(-1))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:43.57813Z","iopub.execute_input":"2021-06-25T08:15:43.578478Z","iopub.status.idle":"2021-06-25T08:15:43.680919Z","shell.execute_reply.started":"2021-06-25T08:15:43.578445Z","shell.execute_reply":"2021-06-25T08:15:43.679775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=body_lenght)\nstart, end = ax.get_xlim()\nax.xaxis.set_ticks(np.arange(0, end, 25))\nax.set_xlabel(\"Lenght of Body (after discretization)\")\nplt.title(\"Body lenght of Stackoverflow questions\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:43.682175Z","iopub.execute_input":"2021-06-25T08:15:43.682516Z","iopub.status.idle":"2021-06-25T08:15:44.632045Z","shell.execute_reply.started":"2021-06-25T08:15:43.682486Z","shell.execute_reply":"2021-06-25T08:15:44.631032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque que la majeur partie des questions compte moins de 4000 caractères *(balises HTML compris)* mais certains posts dépassent les 31 000 caractères. Nous allons **filtrer notre jeu de données pour conserver uniquement les questions de moins de 4 000 caractères** afin de ne pas compliquer le NLP plus que nécessaire.","metadata":{}},{"cell_type":"code","source":"# Filter data on body lenght\ndata = data[data.Body.str.len() < 4000]\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:44.633433Z","iopub.execute_input":"2021-06-25T08:15:44.633739Z","iopub.status.idle":"2021-06-25T08:15:44.706438Z","shell.execute_reply.started":"2021-06-25T08:15:44.6337Z","shell.execute_reply":"2021-06-25T08:15:44.705345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyse des tags\nNous allons faire une rapide analyse exploratoire sur les tags du jeu de données.","metadata":{}},{"cell_type":"code","source":"data['Tags'].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:44.707754Z","iopub.execute_input":"2021-06-25T08:15:44.708082Z","iopub.status.idle":"2021-06-25T08:15:44.716284Z","shell.execute_reply.started":"2021-06-25T08:15:44.708053Z","shell.execute_reply":"2021-06-25T08:15:44.714677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons modifier les séparateurs de Tags pour favoriser les extractions :","metadata":{}},{"cell_type":"code","source":"# Replace open and close balise between tags\ndata['Tags'] = data['Tags'].str.translate(str.maketrans({'<': '', '>': ','}))\n\n# Delete last \",\" for each row\ndata['Tags'] = data['Tags'].str[:-1]\ndata['Tags'].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:44.718221Z","iopub.execute_input":"2021-06-25T08:15:44.718658Z","iopub.status.idle":"2021-06-25T08:15:45.014247Z","shell.execute_reply.started":"2021-06-25T08:15:44.718615Z","shell.execute_reply":"2021-06-25T08:15:45.012852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les tags contenus dans la variable `Tags` sont ensuite splités et ajoutés dans une liste pour ensuite les classer :","metadata":{}},{"cell_type":"code","source":"def count_split_tags(df, column, separator):\n    \"\"\"This function allows you to split the different words contained\n    in a Pandas Series cell and to inject them separately into a list.\n    This makes it possible, for example, to count the occurrences of words.\n\n    Parameters\n    ----------------------------------------\n    df : Pandas Dataframe\n        Dataframe to use.\n    column : string\n        Column of the dataframe to use\n    separator : string\n        Separator character for str.split.\n    ----------------------------------------\n    \"\"\"\n    list_words = []\n    for word in df[column].str.split(separator):\n        list_words.extend(word)\n    df_list_words = pd.DataFrame(list_words, columns=[\"Tag\"])\n    df_list_words = df_list_words.groupby(\"Tag\")\\\n        .agg(tag_count=pd.NamedAgg(column=\"Tag\", aggfunc=\"count\"))\n    df_list_words.sort_values(\"tag_count\", ascending=False, inplace=True)\n    return df_list_words","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:45.016572Z","iopub.execute_input":"2021-06-25T08:15:45.017028Z","iopub.status.idle":"2021-06-25T08:15:45.024202Z","shell.execute_reply.started":"2021-06-25T08:15:45.016972Z","shell.execute_reply":"2021-06-25T08:15:45.022916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags_list = count_split_tags(df=data, column='Tags', separator=',')\nprint(\"Le jeu de données compte {} tags.\".format(tags_list.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:45.026097Z","iopub.execute_input":"2021-06-25T08:15:45.026413Z","iopub.status.idle":"2021-06-25T08:15:45.402468Z","shell.execute_reply.started":"2021-06-25T08:15:45.026379Z","shell.execute_reply":"2021-06-25T08:15:45.401042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the results of splits\nfig = plt.figure(figsize=(15, 8))\nsns.barplot(data=tags_list.iloc[0:40, :],\n            x=tags_list.iloc[0:40, :].index,\n            y=\"tag_count\", color=\"#f48023\")\nplt.xticks(rotation=90)\nplt.title(\"40 most popular tags in Stackoverflow (2009 - 2020)\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:45.404777Z","iopub.execute_input":"2021-06-25T08:15:45.405177Z","iopub.status.idle":"2021-06-25T08:15:45.923218Z","shell.execute_reply.started":"2021-06-25T08:15:45.405134Z","shell.execute_reply":"2021-06-25T08:15:45.922475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dans les 40 tags les plus populaires sur StackOverflow, les tags **C++**, **C#** et **java** sont sans surprise dans le top 3. Le dataset compte **plus de 16 800 tags** différents pour la période 2009 - 2020. \n\nNous pouvons également **visualiser les 500 premières catégories dans un nuage de mots** :","metadata":{}},{"cell_type":"code","source":"# Plot word cloud with tags_list (frequencies)\nfig = plt.figure(1, figsize=(17, 12))\nax = fig.add_subplot(1, 1, 1)\nwordcloud = WordCloud(width=900, height=500,\n                      background_color=\"black\",\n                      max_words=500, relative_scaling=1,\n                      normalize_plurals=False)\\\n    .generate_from_frequencies(tags_list.to_dict()['tag_count'])\n\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nplt.title(\"Word Cloud of 500 best Tags on StackOverflow (2009 - 2020)\\n\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:45.924416Z","iopub.execute_input":"2021-06-25T08:15:45.924847Z","iopub.status.idle":"2021-06-25T08:15:48.36678Z","shell.execute_reply.started":"2021-06-25T08:15:45.924805Z","shell.execute_reply":"2021-06-25T08:15:48.366041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Il peut être intéressant de regarder si ces tags populaires ont évolués au fil du temps. Prenons par exemple les années 2009, 2012, 2016 et 2020 pour vérifier.","metadata":{}},{"cell_type":"code","source":"# Subplots parameters\nyears = {0: 2009, 1: 2012, 2: 2016, 3: 2020}\ncolors = {0: \"#f48023\", 1: \"#d16e1e\",\n          2: \"#b25d19\", 3: \"#904b14\"}\nsubplots = 4\ncols = 2\nrows = subplots // cols\nrows += subplots % cols\nposition = range(1, subplots + 1)\n\n# Plot popular tags for each year\nfig = plt.figure(1, figsize=(20, 16))\nfor k in range(subplots):\n    subset = data[data[\"CreationDate\"].dt.year == years[k]]\n    temp_list = count_split_tags(df=subset, column='Tags', separator=',')\n    ax = fig.add_subplot(rows, cols, position[k])\n    sns.barplot(data=temp_list.iloc[0:20, :],\n            x=temp_list.iloc[0:20, :].index,\n            y=\"tag_count\", color=colors[k])\n    plt.xticks(rotation=90)\n    ax.set_title(\"20 most popular tags for {}\".format(years[k]),\n                 fontsize=18, color=\"#641E16\")\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:48.367863Z","iopub.execute_input":"2021-06-25T08:15:48.368307Z","iopub.status.idle":"2021-06-25T08:15:50.077958Z","shell.execute_reply.started":"2021-06-25T08:15:48.36825Z","shell.execute_reply":"2021-06-25T08:15:50.076738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque en effet que les centres d'intérêt évoluent en fonction des années. Cependant, on retrouve les principaux languages et framework informatiques dans les premières places.     \n\nNous allons a présent regarder le **nombre de Tags par question** :","metadata":{}},{"cell_type":"code","source":"# Create a list of Tags and count the number\ndata['Tags_list'] = data['Tags'].str.split(',')\ndata['Tags_count'] = data['Tags_list'].apply(lambda x: len(x))\n\n# Plot the result\nfig = plt.figure(figsize=(12, 8))\nax = sns.countplot(x=data.Tags_count, color=\"#f48023\")\nax.set_xlabel(\"Tags\")\nplt.title(\"Number of tags used per question\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:50.081196Z","iopub.execute_input":"2021-06-25T08:15:50.081562Z","iopub.status.idle":"2021-06-25T08:15:50.52021Z","shell.execute_reply.started":"2021-06-25T08:15:50.08153Z","shell.execute_reply":"2021-06-25T08:15:50.519544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pour la majorité des questions StackOverflow analysées, 3 tags sont utilisés. Cela nous donne déjà une indication sur le type de modélisation à mettre en oeuvre.\n\n### Filtrage du jeu de données avec les meilleurs Tags : \nLes process de NLP sont des algorithmes assez lents compte tenu de la quantité de données à traiter. Pour filtrer notre jeu de données, nous allons **sélectionner toutes les questions qui comportent au minimum un des 50 meilleurs tags et supprimer les autres tags** :","metadata":{}},{"cell_type":"code","source":"def filter_tag(x, top_list):\n    \"\"\"Comparison of the elements of 2 lists to \n    check if all the tags are found in a list of top tags.\n\n    Parameters\n    ----------------------------------------\n    x : list\n        List of tags to test.\n    ----------------------------------------\n    \"\"\"\n    temp_list = []\n    for item in x:\n        if (item in top_list):\n            #x.remove(item)\n            temp_list.append(item)\n    return temp_list","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:50.521703Z","iopub.execute_input":"2021-06-25T08:15:50.522107Z","iopub.status.idle":"2021-06-25T08:15:50.526577Z","shell.execute_reply.started":"2021-06-25T08:15:50.522064Z","shell.execute_reply":"2021-06-25T08:15:50.525532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_tags = list(tags_list.iloc[0:50].index)\ndata['Tags_list'] = data['Tags_list']\\\n                    .apply(lambda x: filter_tag(x, top_tags))\ndata['number_of_tags'] = data['Tags_list'].apply(lambda x : len(x))\ndata = data[data.number_of_tags > 0]\nprint(\"New size of dataset : {} questions.\".format(data.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:50.527834Z","iopub.execute_input":"2021-06-25T08:15:50.528118Z","iopub.status.idle":"2021-06-25T08:15:50.983457Z","shell.execute_reply.started":"2021-06-25T08:15:50.528091Z","shell.execute_reply":"2021-06-25T08:15:50.982384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color: #641E16\" id=\"section_3\">Nettoyage des questions</span>\n\nAfin de traiter au mieux les données textuelles du `Body`, il est nécessaire de réaliser plusieurs tâches de data cleaning. Par exemple, le texte stocké dans cette variable est au format HTML. Ces balises vont polluer notre analyse. Nous allons donc **supprimer toutes les balises HTML** avec la librairie `BeautifulSoup` pour ne conserver que le texte brut.\n\nMais avant cette opération,nous allons supprimer tout le contenu placé entre 2 balises html `<code></code>`, cela nous permettra de supprimer tout le code brut souvent copié dans les questions Stackoverflow et qui pourrait avoir un fort impact pour la suite.","metadata":{}},{"cell_type":"code","source":"def remove_code(x):\n    \"\"\"Function based on the Beautifulsoup library intended to replace \n    the content of all the <code> </code> tags of a text specified as a parameter.\n\n    Parameters\n    ----------------------------------------\n    x : string\n        Sequence of characters to modify.\n    ----------------------------------------\n    \"\"\"\n    soup = BeautifulSoup(x,\"lxml\")\n    code_to_remove = soup.findAll(\"code\")\n    for code in code_to_remove:\n        code.replace_with(\" \")\n    return str(soup)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:50.984707Z","iopub.execute_input":"2021-06-25T08:15:50.984995Z","iopub.status.idle":"2021-06-25T08:15:50.989199Z","shell.execute_reply.started":"2021-06-25T08:15:50.984965Z","shell.execute_reply":"2021-06-25T08:15:50.988443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n# Delete <code> in Body text\ndata['Body'] = data['Body'].apply(remove_code)\n# Delete all html tags\ndata['Body'] = [BeautifulSoup(text,\"lxml\").get_text() for text in data['Body']]\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body'].head(3))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:15:50.990136Z","iopub.execute_input":"2021-06-25T08:15:50.990414Z","iopub.status.idle":"2021-06-25T08:17:34.532799Z","shell.execute_reply.started":"2021-06-25T08:15:50.990385Z","shell.execute_reply":"2021-06-25T08:17:34.531513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A présent, nous devons **vérifier si les textes des questions sont rédigés en diverses langues**. Cela nous permettra de définir la liste des stop words à éliminer :","metadata":{}},{"cell_type":"code","source":"# Create feature \"lang\" with langdetect library\ndef detect_lang(x):\n    try:\n        return detect(x)\n    except:\n        pass\n\nstart_time = time.time()\ndata['short_body'] = data['Body'].apply(lambda x: x[0:100])\ndata['lang'] = data.short_body.apply(detect_lang)\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:17:34.537681Z","iopub.execute_input":"2021-06-25T08:17:34.538043Z","iopub.status.idle":"2021-06-25T08:22:46.003186Z","shell.execute_reply.started":"2021-06-25T08:17:34.538011Z","shell.execute_reply":"2021-06-25T08:22:46.001942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count titles for each language\npd.DataFrame(data.lang.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:22:46.005014Z","iopub.execute_input":"2021-06-25T08:22:46.00534Z","iopub.status.idle":"2021-06-25T08:22:46.025114Z","shell.execute_reply.started":"2021-06-25T08:22:46.005307Z","shell.execute_reply":"2021-06-25T08:22:46.024019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La langue Anglaise est très majoritairement représentée dans notre dataset. Nous allons donc **supprimer de notre jeu de données tous les post dans une autre langue que l'anglais**.","metadata":{}},{"cell_type":"code","source":"# Deletion of data that is not in the English language\ndata = data[data['lang']=='en']","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:22:46.026705Z","iopub.execute_input":"2021-06-25T08:22:46.027085Z","iopub.status.idle":"2021-06-25T08:22:46.115675Z","shell.execute_reply.started":"2021-06-25T08:22:46.027043Z","shell.execute_reply":"2021-06-25T08:22:46.114589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maintenant que nous avons un texte brut débarassé de ses balises HTML et du code, nous allons utiliser `nltk.pos_tag` pour **identifier la nature de chaque mot du corpus afin de pouvoir ensuite conserver uniquement les noms**. Nous allons ici créer une function qui sera appliquée ensuite dans un cleaner plus complet *(tout aurait pu être réalisé avec SpaCy, mais pour l'exercice et la méthode, les autres étapes sont détaillées)*.","metadata":{}},{"cell_type":"code","source":"def remove_pos(nlp, x, pos_list):\n    doc = nlp(x)\n    list_text_row = []\n    for token in doc:\n        if(token.pos_ in pos_list):\n            list_text_row.append(token.text)\n    join_text_row = \" \".join(list_text_row)\n    join_text_row = join_text_row.lower().replace(\"c #\", \"c#\")\n    return join_text_row","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:22:46.117428Z","iopub.execute_input":"2021-06-25T08:22:46.117707Z","iopub.status.idle":"2021-06-25T08:22:46.123327Z","shell.execute_reply.started":"2021-06-25T08:22:46.117681Z","shell.execute_reply":"2021-06-25T08:22:46.122046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons à présent réaliser plusieurs opérations de Text cleaning pour que nos données soient exploitables par les algorithmes de NLP :\n- Suppression de tous les mots autres que les noms\n- Mettre tout le texte en **minuscules**\n- Supprimer les **caractères Unicode** (comme les Emojis par exemple)\n- Suppression des **espaces supplémentaires**\n- Suppression de la **ponctuation**\n- Suppression des **liens**\n- Supprimer les **nombres**","metadata":{}},{"cell_type":"code","source":"def text_cleaner(x, nlp, pos_list):\n    \"\"\"Function allowing to carry out the preprossessing on the textual data. \n        It allows you to remove extra spaces, unicode characters, \n        English contractions, links, punctuation and numbers.\n        \n        The re library for using regular expressions must be loaded beforehand.\n\n    Parameters\n    ----------------------------------------\n    x : string\n        Sequence of characters to modify.\n    ----------------------------------------\n    \"\"\"\n    # Remove POS not in \"NOUN\", \"PROPN\"\n    x = remove_pos(nlp, x, pos_list)\n    # Case normalization\n    x = x.lower()\n    # Remove unicode characters\n    x = x.encode(\"ascii\", \"ignore\").decode()\n    # Remove English contractions\n    x = re.sub(\"\\'\\w+\", '', x)\n    # Remove ponctuation but not # (for C# for example)\n    x = re.sub('[^\\\\w\\\\s#]', '', x)\n    # Remove links\n    x = re.sub(r'http*\\S+', '', x)\n    # Remove numbers\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    # Remove extra spaces\n    x = re.sub('\\s+', ' ', x)\n    \n    # Return cleaned text\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:22:46.12497Z","iopub.execute_input":"2021-06-25T08:22:46.12538Z","iopub.status.idle":"2021-06-25T08:22:46.137749Z","shell.execute_reply.started":"2021-06-25T08:22:46.125336Z","shell.execute_reply":"2021-06-25T08:22:46.136798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply cleaner on Body\n# Spacy features\nnlp = spacy.load('en', exclude=['tok2vec', 'ner', 'parser', \n                                'attribute_ruler', 'lemmatizer'])\npos_list = [\"NOUN\",\"PROPN\"]\n\nstart_time = time.time()\nprint('-' * 50)\nprint(\"Start Body cleaning ...\")\nprint('-' * 50)\n\ntqdm.pandas()\ndata['Body_cleaned'] = data.Body.progress_apply(lambda x : text_cleaner(x, nlp, pos_list))\n\nexec_time = time.time() - start_time\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body_cleaned'].head(3))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:22:46.138933Z","iopub.execute_input":"2021-06-25T08:22:46.139209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous pouvons à présent **supprimer tous les stop words en langue Anglaise** grâce à la librairie `NLTK`. Avant cette étape, nous allons réaliser une **tockenisation** c'est à dire découper les phrase en mots et création d'une liste *(chaque phrase est une liste de mots)*","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n# Tockenization\ndata['Body_cleaned'] = data.Body_cleaned.apply(nltk.tokenize.word_tokenize)\n\n# List of stop words in \"EN\" from NLTK\nstop_words = stopwords.words(\"english\")\n\n# Remove stop words\ndata['Body_cleaned'] = data.Body_cleaned\\\n    .apply(lambda x : [word for word in x\n                       if word not in stop_words\n                       and len(word)>2])\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body_cleaned'].head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A présent, nous avons des listes de mots débarrassées des mots courants (stop words), de la ponctuation, des liens et des nombres. Une dernière étape que nous pouvons effectuer est la **Lemmatisation**. Ce procédé consiste à prend le mot à sa forme racine appelée Lemme. Cela nous permet d'amener les mots à leur forme \"dictionnaire\". Nous allons pour cela utiliser à nouveau la librairie `NLTK`.","metadata":{}},{"cell_type":"code","source":"# Apply lemmatizer on Body\nstart_time = time.time()\nwn = WordNetLemmatizer()\ndata['Body_cleaned'] = data.Body_cleaned\\\n    .apply(lambda x : [wn.lemmatize(word) for word in x])\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body_cleaned'].head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Le body étant à présant cleané, nous allons regarder la répartition de la **taille des corpus** dans le jeu de donné nettoyé :","metadata":{}},{"cell_type":"code","source":"# Calculate lenght of each list in Body\ndata['body_tokens_count'] = [len(_) for _ in data.Body_cleaned]\n\n# Countplot of body lenght\nfig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=data.body_tokens_count)\nstart, end = ax.get_xlim()\nax.xaxis.set_ticks(np.arange(0, end, 25))\nplot_median = data.body_tokens_count.median()\nplt.axvline(plot_median - data.body_tokens_count.min(),\n            color=\"r\", linestyle='--',\n            label=\"Body tokens Lenght median : \"+str(plot_median))\nax.set_xlabel(\"Lenght of body tokens\")\nplt.title(\"Body tokens lenght of Stackoverflow questions after cleaning\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons également regarder les **fréquences de chaque mots de la variable Body** pour visualiser les plus représentés :","metadata":{}},{"cell_type":"code","source":"# Create a list of all tokens for Body\nfull_corpus = []\nfor i in data['Body_cleaned']:\n    full_corpus.extend(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate distribition of words in Body token list\nbody_dist = nltk.FreqDist(full_corpus)\nbody_dist = pd.DataFrame(body_dist.most_common(2000),\n                         columns=['Word', 'Frequency'])\nbody_dist.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On remarque que 75% des 2000 mots les plus communs apparaissent 69 fois *(3ème quartile)*. Or, le maximum se situe à plus de 5700.","metadata":{}},{"cell_type":"code","source":"# Plot word cloud with tags_list (frequencies)\nfig = plt.figure(1, figsize=(17, 12))\nax = fig.add_subplot(1, 1, 1)\nwordcloud = WordCloud(width=900, height=500,\n                      background_color=\"black\",\n                      max_words=500, relative_scaling=1,\n                      normalize_plurals=False)\\\n    .generate_from_frequencies(body_dist.set_index('Word').to_dict()['Frequency'])\n\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nplt.title(\"Word Cloud of 500 most popular words on Body feature\\n\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color: #641E16\" id=\"section_4\">Nettoyage des titres</span>\nNous avons préalablement défini une fonction pour notre cleaning des Body. Nous allons la modifier pour y intégrer la tokenisation, les stop words et la lemmanisation afin d'obtenir un processus complet à appliquer aux titres des posts.","metadata":{}},{"cell_type":"code","source":"def text_cleaner(x, nlp, pos_list, lang=\"english\"):\n    \"\"\"Function allowing to carry out the preprossessing on the textual data. \n        It allows you to remove extra spaces, unicode characters, \n        English contractions, links, punctuation and numbers.\n        \n        The re library for using regular expressions must be loaded beforehand.\n        The SpaCy and NLTK librairies must be loaded too. \n\n    Parameters\n    ----------------------------------------\n    x : string\n        Sequence of characters to modify.\n    ----------------------------------------\n    \"\"\"\n    # Remove POS not in \"NOUN\", \"PROPN\"\n    x = remove_pos(nlp, x, pos_list)\n    # Case normalization\n    x = x.lower()\n    # Remove unicode characters\n    x = x.encode(\"ascii\", \"ignore\").decode()\n    # Remove English contractions\n    x = re.sub(\"\\'\\w+\", '', x)\n    # Remove ponctuation but not # (for C# for example)\n    x = re.sub('[^\\\\w\\\\s#]', '', x)\n    # Remove links\n    x = re.sub(r'http*\\S+', '', x)\n    # Remove numbers\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    # Remove extra spaces\n    x = re.sub('\\s+', ' ', x)\n        \n    # Tokenization\n    x = nltk.tokenize.word_tokenize(x)\n    # List of stop words in select language from NLTK\n    stop_words = stopwords.words(lang)\n    # Remove stop words\n    x = [word for word in x if word not in stop_words \n         and len(word)>2]\n    # Lemmatizer\n    wn = nltk.WordNetLemmatizer()\n    x = [wn.lemmatize(word) for word in x]\n    \n    # Return cleaned text\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spacy features\nnlp = spacy.load('en', exclude=['tok2vec', 'ner', 'parser', \n                                'attribute_ruler', 'lemmatizer'])\npos_list = [\"NOUN\",\"PROPN\"]\n# Apply full cleaner on Title\nprint('-' * 50)\nprint(\"Start Title cleaning ...\")\nprint('-' * 50)\nstart_time = time.time()\ndata['Title_cleaned'] = data.Title\\\n                            .progress_apply(lambda x: \n                                            text_cleaner(x,\n                                                         nlp,\n                                                         pos_list,\n                                                         \"english\"))\nexec_time = time.time() - start_time\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Title_cleaned'].head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous pouvons à présent projeter la distribution de la taille des tokens Title et le nuage de mots correspondant aux 500 meilleurs apparitions : ","metadata":{}},{"cell_type":"code","source":"# Calculate lenght of each list in Body\ndata['Title_tokens_count'] = [len(_) for _ in data.Title_cleaned]\n\n# Countplot of body lenght\nfig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=data.Title_tokens_count)\nmedian_plot = data.Title_tokens_count.median()\nplt.axvline(median_plot - data.Title_tokens_count.min(),\n            color=\"r\", linestyle='--',\n            label=\"Title tokens Lenght median : \"+str(median_plot))\nax.set_xlabel(\"Lenght of body tokens\")\nplt.title(\"Title tokens lenght of Stackoverflow questions after cleaning\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of all tokens for Title\nfull_corpus_t = []\nfor i in data['Title_cleaned']:\n    full_corpus_t.extend(i)\n\n# Calculate distribition of words in Title token list\ntitle_dist = nltk.FreqDist(full_corpus_t)\ntitle_dist = pd.DataFrame(title_dist.most_common(500),\n                          columns=['Word', 'Frequency'])\n\n# Plot word cloud with tags_list (frequencies)\nfig = plt.figure(1, figsize=(17, 12))\nax = fig.add_subplot(1, 1, 1)\nwordcloud = WordCloud(width=900, height=500,\n                      background_color=\"black\",\n                      max_words=500, relative_scaling=1,\n                      normalize_plurals=False)\\\n    .generate_from_frequencies(title_dist.set_index('Word').to_dict()['Frequency'])\n\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nplt.title(\"Word Cloud of 500 most popular words on Title feature\\n\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color: #641E16\" id=\"section_5\">Export du dataset nettoyé</span>\nNous pouvons maintenant supprimer les variables créées pour l'analyse exploratoire, qui ne nous seront plus utiles, et exporter le dataset pour nos modélisations supervisées et non supervisées disponible dans le Notebook [Stackoverflow questions - tag generator](https://www.kaggle.com/michaelfumery/stackoverflow-questions-tag-generator)","metadata":{}},{"cell_type":"markdown","source":"Enfin, afin d'avoir suffisement de \"matière\" pour alimenter nos algorithmes de prédiction de Tags, nous allons **conserver uniquement les questions qui comptent au minimum 5 tokens dans la variable Body**.","metadata":{}},{"cell_type":"code","source":"# Delete items with number of Body tokens < 5\ndata = data[(data.body_tokens_count >= 5) & (data.Title_tokens_count > 0)]\n# Remove calculated features\ndata = data[['Title_cleaned',\n             'Body_cleaned',\n             'Score',\n             'Tags_list']]\n# Rename columns\ndata = data.rename(columns={'Title_cleaned': 'Title',\n                            'Body_cleaned': 'Body',\n                            'Tags_list': 'Tags'})\ndata.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export to CSV\ndata.to_csv(\"StackOverflow_questions_2009_2020_cleaned.csv\", sep=\";\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}