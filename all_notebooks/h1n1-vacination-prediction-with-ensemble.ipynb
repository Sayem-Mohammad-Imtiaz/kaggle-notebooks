{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import RFECV,RFE\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading the dataset\ndata = pd.read_csv(r'../input/h1n1-vaccination/h1n1_vaccine_prediction.csv')\ndata","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns',100)\ndata.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now lets look out for missing values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum().sort_values(ascending=False)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing Percentage\nmiss_percent=((data.isna().sum()/len(data))*100).sort_values(ascending=False)\nmiss_percent.plot.bar()\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# droping columns which has missing percentage greater than 10 percent\nmiss_cols=list(miss_percent[miss_percent>10].index)\ndata1=data.drop(data[miss_cols],axis=1)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.isna().sum().sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hence we can see that large proportion of the column have value zero in valuecounts\n# we are imputing the null value with the mode of the column have lesser that 1000 null values\nmiss_row = data1.isna().sum().sort_values(ascending=False)\nmiss_row = miss_row[(miss_row<1000) & (miss_row!=0)].index\nmiss_row","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2= data1.copy()\ndata2[miss_row] = data2[miss_row].apply(lambda x: x.fillna(x.mode()[0]))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# droping rest of the na values\ndata3 = data2.dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing some of categorical values into numbers to analyse it\nclean = {'age_bracket':{'18 - 34 Years':1,'35 - 44 Years':2,'45 - 54 Years':3,'55 - 64 Years':4,'65+ Years':5},\n        'qualification':{'< 12 Years':1,'12 Years':2,'College Graduate':3,'Some College':4}}\ndata3=data3.replace(clean)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets analyse all feature with corresponding with the target variable by creating a function\ndef analysis(df,graph_per_row,max_graphs):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col]>1 and nunique[col]<50]]\n    nrow, ncol = df.shape\n    colname = list(df)\n    graph_row = (ncol+graph_per_row-1)/graph_per_row\n    plt.figure(figsize=(12*graph_per_row,8*graph_row))\n    for i in range(min(ncol,max_graphs)):\n        plt.subplot(graph_row,graph_per_row,i+1)\n        coltype = df.iloc[:,i]\n        if (not np.issubdtype(type(coltype.iloc[0]),np.str)):\n            sns.countplot(colname[i], hue='h1n1_vaccine',data=df)\n        else:\n            coltype.hist()\n        plt.title(f'{colname[i]}')\n        plt.xticks(rotation=60)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analysis(data3,5,25)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3.isna().sum().sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for Multicollinearity\nobj = data3.select_dtypes(include='object').columns\n[print(i,'-->',data3[i].unique()) for i in obj]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean = {'sex':{'Female':0 ,'Male':1},\n        'employment':{'Not in Labor Force':1,'Employed':2,'Unemployed':3},\n        'census_msa':{'Non-MSA':1,'MSA, Not Principle  City':2,'MSA, Principle City':3},\n         'housing_status':{'Own':1,'Rent':0},\n        'marital_status':{'Not Married':0, 'Married':1}\n}\ndata4=data3.replace(clean)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,24))\ncorr = data4.corr()\nsns.heatmap(corr,annot=True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can more multicollinearity from the heat map \n# so we can check using Variance Influencing Factor\ndef vif_scores(df):\n    VIF_Scores = pd.DataFrame()\n    VIF_Scores[\"Independent Features\"] = df.columns\n    VIF_Scores[\"VIF Scores\"] = [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n    return VIF_Scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=data4.drop(data4[['unique_id','race','is_h1n1_vacc_effective','is_seas_vacc_effective','qualification']],axis=1)#'sex','marital_status','is_h1n1_vacc_effective','is_seas_vacc_effective'\ndf2 = df1.iloc[:,:-1]\nvif_scores(df2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So, Except 'race','is_h1n1_vacc_effective','is_seas_vacc_effective','qualification' no other columns have Multicolinearity","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One Hot Encoding Race\none_hot = data4[['race']]\none_hot= pd.get_dummies(one_hot)\none_hot.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5= data4.drop(data4[['unique_id','race']],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5 = pd.concat([data5,one_hot],axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5.info()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Models","metadata":{}},{"cell_type":"code","source":"# firt Creating model with all the features\nx = data5.drop(['h1n1_vaccine'],axis=1)\ny= data5['h1n1_vaccine']\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20, random_state=1)\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train)\ny_pred= model.predict(x_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Function for viewing Result of the predicted\ndef res(y_valid):\n    cm1 = confusion_matrix(y_test,y_valid)\n    ConfusionMatrixDisplay(cm1).plot().ax_.set(ylabel = 'Actual value', xlabel ='Predicted value')\n    print('Accuracy',accuracy_score(y_test,y_valid))\n    print(classification_report(y_test,y_valid))\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res(y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(model,x_train,y_train,response_method='predict_proba')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prob = model.predict_proba(x_test)\ny_prob = y_prob[:,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict using custom thershold\nThersold = 0.2\ny_pred1 =  np.where(y_prob>Thersold,1,0)\nres(y_pred1)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can use Thershold based on our recuriment of the model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DecisionTree Model\ndec = DecisionTreeClassifier()\ndec.fit(x_train,y_train)\ny_pred_dec = dec.predict(x_test)\nres(y_pred_dec)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Model\nrand = RandomForestClassifier()\nrand.fit(x_train,y_train)\ny_pred_rand = rand.predict(x_test)\nres(y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN Model\nerror = []\nfor i in range(1,20,2):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    y_pred_knn = knn.predict(x_test)\n    error.append(np.mean(y_test!=y_pred_knn))\nplt.plot(range(1,20,2), error, marker='o')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\nres(y_pred_knn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting best Features out of all of them Using\n#Recursive Feature Engineering\nrfe = RFE(rand)\nrfe.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"select = []\nfeatures = rfe.support_\ncols = x.columns\nfor i,j in enumerate(features):\n    if j==True:\n        select.append(cols[i])\nselect","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now from the best Features, Creating logistice Model \nx = data5[['h1n1_worry',\n 'h1n1_awareness',\n 'dr_recc_h1n1_vacc',\n 'is_h1n1_vacc_effective',\n 'is_h1n1_risky',\n 'sick_from_h1n1_vacc',\n 'is_seas_vacc_effective',\n 'is_seas_risky',\n 'sick_from_seas_vacc',\n 'age_bracket',\n 'qualification',\n 'sex',\n 'employment',\n 'census_msa',\n 'no_of_adults',\n 'no_of_children']]\ny= data5['h1n1_vaccine']\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20, random_state=1)\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train)\ny_pred_features= model.predict(x_test)\nres(y_pred_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can see from the Results that 16 features give the same accuracy as 32 features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter tunning\n# first geting best parameter to use in Random Forest model and predict with it\nparameters = {'n_estimators':[10,20,30,40,50],'max_depth':[3,4,5,6,7], 'criterion':('entropy', 'gini'),'max_leaf_nodes':[5,10,15,20]}\nclf = GridSearchCV(rand, parameters)\nclf.fit(x_train,y_train)\nclf.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_GS = clf.predict(x_test)\nres(y_pred_GS)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Models","metadata":{}},{"cell_type":"code","source":"# !pip install xgboost\n# %pip install lightgbm\n# %pip install catboost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGB \nxgb_model = xgb.XGBClassifier()\nxgb_model.fit(x_train,y_train)\ny_pred_xgb = xgb_model.predict(x_test)\nres(y_pred_xgb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cat Boost\nmodel_cat = CatBoostClassifier()\nmodel_cat.fit(x_train,y_train,verbose=False)\ny_pred_cat = model_cat.predict(x_test)\nres(y_pred_cat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LightGB\ntrain_data = lgb.Dataset(x_train,y_train)\nparams = {'learning_rate':0.001}\nmodel_lgb = lgb.train(params,train_data)\ny_pred_lgb=model.predict(x_test)\nres(y_pred_lgb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So from the above model we can conclude that the XGB give some good recall and precision when compared to all\n# More over we cannot depend on accuracy on Classification Problem \n# We can change the True Positive Rate or False Positive Rate depending on our problem statement","metadata":{},"execution_count":null,"outputs":[]}]}