{"cells":[{"metadata":{"_cell_guid":"bed4faa5-7807-4a4d-990d-d4786045d7cc","_uuid":"61899170fd87bbf443842e370b157e2b1fb31bea"},"cell_type":"markdown","source":"**Overview: **This Ultimate goal of this kernal is designed to be a guide for analytical method selection; rational; and include graphic,  programming and explanation of each approach. <br/>\n<b>Using the Notebook</b> simply edit hashtags to activate and deactivate visual functions.\n\n**Analytical Libraries:** Pandas, Numpy, Matplotlib, and Seaborn <br/>\n**Research Sources:** [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf)<br/>\n UDemy.com:  [Python for Data Science and Machine Learning BootCamp](https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp/)<br/>\n Seaborn: [Color Palettes](http://seaborn.pydata.org/tutorial/color_palettes.html)\n \n \n \n\n<b>Exploring the data:</b><br>\n(1) heatmap null values.<br/>\nFindings: Cabin has the largest number of omitted values and should be removed as a feature\n\n(2) Countplot<br/>\nFindings: 3rd class had highest incident of deaths of the classes; Males had the highest incident of death among gender. \n\nMethods:<br/>\n**Logistic Regression:** - Use: Classification Problem<br/>\nLogistic regression (not to be confused with Linear Regression) could be used when the output is binary (e.g. Pass/Fail, Win/Lose, Healthy/Sick, Survived/Died); Said another way, the output is a Binary Dependent Variable and can have several input variables (or Independent variables). Alternatively, if the output had more than 2 results, you could use multi-nominal Logistic Regression (e.g. outcomes being poor, fair, good, great). In the case of Titanic, Logistic Regression could be well suited as survival is a binary output based on several input variables. <br/>\n\n**Additional Notes**<br/>\nHigh Level; Logistic Regression is a Linear Regression that has been re-shaped into a Logistic (sigmoidal function) whereby the outputs will always reside between 0 and 1. Additionally, it would be incorrect to use a Linear Regression when the outputs are noncontinuous / discrete. \n\nTitanic Test results:\nTest 001 - Simple excel model whereby gender was used as the baseline; results .7655\nTest 002 - Logistic Regression using Gender and Age; results .7655. \nLogistic Regression model used: \n<br/> (i) Survival (Dependent Variable)<br/>(ii) Gender and (iiI) Age as (Independent Variables); 3 Coefficients (often referred to as b0, b1 and b2) that I arbitrarily set to .10 (solver adjusted); Eulers Number (2.718281828459045); Used to solve MLL (Maximum Log Likelyhood).\n\n**Linear Regression**<br/>\n(Continuous output)\n\nSearch\nDummy variable trap\nCo-Linearity\n\n"},{"metadata":{"_cell_guid":"49c95f04-8e59-466e-ac3a-60cc761345f2","_uuid":"92a7c98eea9b645e2334cd7d349a8036cae515d5","collapsed":true,"trusted":true},"cell_type":"code","source":"#import the libraries & file(s)\nimport numpy as np\nimport pandas as pd\nimport seaborn as sms\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Import the file\ndata = pd.read_csv('../input/train.csv')","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"f824cef3-41be-4ae0-9884-1923448ce8d4","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"2a6ae0f8b68910349838a1654fbb3e5b5c4d9fa7","trusted":true},"cell_type":"code","source":"#reading data contents.\n#data.head()\n#data.tail()\n\n#function List\n#data.isnull()\n#data.info()\n#data.drop('Cabin',axis=1,inplace=True)\n#data.dropna(inplace=True) \n#pd.get_dummies()\n#pd.get_dummies(data['Sex'],drop_first=True)\n#pd.get_dummies(data['Embarked'], drop_first=True)\n#pd.concat([data,sex,embark], axis=1)\n\nplt.figure(figsize=(10,7))\n\n##################################################################\n#Exploring the data; single line commands\n##################################################################\n\n#sms.distplot(data['Age'].dropna(),kde=False,bins=25)\n\n#Countplots\n#sms.countplot(x='Survived', hue='Sex', data=data)\n#sms.countplot(x='Survived', hue='Pclass', data=data)\n#sms.countplot(x = \"Sibsp\", hue=\"sex\", data=data)\n\n#Jointplots\n#sms.jointplot(x='Age', y='Fare', data=data, kind='reg')\n#sms.jointplot(x='Age', y='Fare', data=data, kind='kde')\n\n#sms.pairplot(data,)\n#sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='plasma')\n\n#BoxPlots\n\nsms.boxplot(x='Pclass', y='Age',data=data)\n","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"efabc73c-ee95-4437-8973-0780357f4ee9","_uuid":"3c12ea55aff9d360fde9a3105d9e67bb0024312e","collapsed":true,"trusted":true},"cell_type":"code","source":"##################################################################\n#Data Cleaning, Purifying, and Imputing\n##################################################################\n#Here we're going to impute the average age for each class using the boxplot above to determine the values for a function we will build below. \n#this script is credited to Jose Portilla and his training book, which I highly recommend Python for Datascience and Machine Learning BootCamp (linked above)\n\ndef impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n\n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"df105e7c-a048-41ee-8e05-73551f43a7b6","_uuid":"0d1e446afb91af3959c799ce85d0551ff39fdc43","collapsed":true,"trusted":true},"cell_type":"code","source":"#using the function above the ages, 37, 29 and 24 were imputed (i.e. replaced the null values in the titanic dataset)\ndata['Age'] = data[['Age','Pclass']].apply(impute_age,axis=1)","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"42959dce-e9ba-4d37-8bb2-dd17b3aa6479","_uuid":"cb123f778eb64b9ec65c04dc07072ce387a826fb","trusted":true},"cell_type":"code","source":"#test that the function worked correctly. \n#drop rows that are missing values\n#drop column Cabin\ndata.dropna(inplace=True)\ndata.drop('Cabin', axis=1, inplace=True)\nsms.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='plasma')\n#data.info()\n","execution_count":31,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b046b8dc6c6703a14519204f04f36d067c804181"},"cell_type":"code","source":"#################################################################\n#converting data to usable format; Essentially we need to convert Categorical values Male & Female to (1 or 0) for computational purposes.\n#################################################################\nsex = pd.get_dummies(data['Sex'],drop_first=True)\nembark = pd.get_dummies(data['Embarked'], drop_first=True)","execution_count":32,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"843da8ee8fcd431566bb88be820431cb580f049e"},"cell_type":"code","source":"##adding the new binary results for sex and adding the departure points QS from the newly created variables above into the data dataframe.\ndata = pd.concat([data,sex,embark], axis=1)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"601cd4a4a00126c012c0a3dcb69b464f525c23fb"},"cell_type":"code","source":"data.head()","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b0e77a3d0ac192dfcf691471cca7abf9f475829"},"cell_type":"code","source":"data.drop(['Sex','Embarked','Name','Ticket','PassengerId'], axis=1, inplace=True)\ndata.head()","execution_count":36,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"637dbc51ef58c453d633499853fb981fe3a0c28c"},"cell_type":"code","source":"X = data.drop('Survived', axis=1)\ny = data['Survived']","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d3ec7bd6556f8e16e144d357e873c47799dcfb9"},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bdc4de50e8d8c90cc5f5ab7ab8480be84f87f379"},"cell_type":"code","source":"X_train, X_test, y_train, y_test, = train_test_split(X,y, test_size=0.30)","execution_count":100,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"83d0b62512063e57068494387356a4cbee02b15c"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":101,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ee920dbee9e5e17760d4463989938ae37c97cdbe"},"cell_type":"code","source":"logisticmodel = LogisticRegression()","execution_count":102,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc8c716c66a858a3c4362df872c3730d723227fe"},"cell_type":"code","source":"logisticmodel.fit(X_train,y_train)","execution_count":103,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e05ef9dca5b90b12f88f2143d1b6b98fafeea825"},"cell_type":"code","source":"predictions = logisticmodel.predict(X_test)","execution_count":104,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"198ad10f4fdcfb1f855268169c2f42c8a6ece576"},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":105,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3200b1a46ff9636e241f7a5aae89c0bca397ee7d"},"cell_type":"code","source":"print(classification_report(y_test,predictions))","execution_count":106,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25b7d79f5aedfbbff7ad7d6f864189c60245ace1"},"cell_type":"code","source":"X_train","execution_count":110,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}