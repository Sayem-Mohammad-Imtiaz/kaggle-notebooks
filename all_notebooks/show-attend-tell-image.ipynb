{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Show-attend-tell image \nAdaptation from https://www.tensorflow.org/tutorials/text/image_captioning"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download necessary packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kulc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/ahmadelsallab/MultiCheXNet.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport cv2\nimport numpy as np\nimport os\nfrom glob import glob\nimport math\nimport matplotlib.pyplot as plt\n\nimport re\nimport html\nimport string\nimport unicodedata\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  =pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = pd.read_csv('../input/chest-xrays-indiana-university/indiana_projections.csv')\ndff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['findings'].iloc[0:10].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['impression'].unique().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MeSH'].unique().tolist()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('/kaggle/input/chest-xrays-indiana-university/images/images_normalized/1_IM-0001-3001.dcm.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\")\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.projection.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build vocab"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text cleaner**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    #words = text2words(text)\n    #stop_words = stopwords.words('english')\n    #words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    #words = lemmatize_words(words)\n    #words = lemmatize_verbs(words)\n\n    return text\n  \ndef normalize_corpus(corpus):\n    return [normalize_text(t) for t in corpus]\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.dropna(subset=['findings'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install swifter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import swifter \ndf['findings_cleaned'] = df['findings'].swifter.apply(normalize_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['findings_cleaned'] = df['findings'].apply(normalize_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['findings_cleaned'] = 'startseq '+df['findings_cleaned']+' endseq'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = []\nfor row in df['findings_cleaned'].tolist():\n    num_words.append(len(word_tokenize(row)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words= np.array(num_words)\nprint(\"min length             : \", num_words.min())\nprint(\"max length             : \", num_words.max())\nprint(\"50th percentile length : \", np.percentile(num_words,50))\nprint(\"75th percentile length : \", np.percentile(num_words,75))\nprint(\"90th percentile length : \", np.percentile(num_words,90))\nprint(\"95th percentile length : \", np.percentile(num_words,95))\nprint(\"98th percentile length : \", np.percentile(num_words,98))\nprint(\"98th percentile length : \", np.percentile(num_words,99))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nmax_len = 100\n\ntok = Tokenizer(num_words=vocab_size,  oov_token='UNK' )\ntok.fit_on_texts(df['findings_cleaned'].tolist())\n\nvocab_size = len(tok.word_index) + 1\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loader from Repo"},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.data_loader.indiana_dataloader import get_train_validation_generator\nfrom tensorflow.keras.applications.densenet import preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_vocab_size=10000\nmax_len=100\n\ncsv_path1  =\"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\"\ncsv_path2  =\"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\"\nimg_path   =\"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/\"\nbatch_sz = 8\nvalidation_split = 0.2\n\ntrain_dataloader, val_dataloader, vocab_size, tok, df = get_train_validation_generator(csv_path1,csv_path2,img_path, max_vocab_size,max_len,preprocess=vgg_preprocess_input, batch_size=batch_sz, validation_split=validation_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch, (X,Y)  = next(enumerate(train_dataloader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X,Y = next(enumerate(train_dataloader))[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X[0]))# images\nprint(X[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sample img\nsmpl_idx = 0\nplt.imshow(X[0][smpl_idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X[1]))# Input Text w/o endseq\nprint(X[1].shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape# Target Text w/o startseq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X[1][smpl_idx])\nprint(Y[smpl_idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tok2txt(tokens, tok):\n    return \" \".join([tok.index_word[token] for token in tokens if token!=0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tok2txt(X[1][smpl_idx], tok))\nprint(tok2txt(Y[smpl_idx], tok))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom data gen"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndef data_gen(df):\n    for i in range(len(df)):\n        \n        yield img, caption\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wrap into tf.Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nimport tensorflow as tf\ndef gen():\n    #return next(enumerate(train_dataloader))[1]\n    return train_dataloader\ndataset = tf.data.Dataset.from_generator(gen, output_types=(tf.float32, tf.float32))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X1,Y1 = next(enumerate(dataset))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Show-Attend-Tell Model\n[Ref TF2](https://www.tensorflow.org/tutorials/text/image_captioning)\n\nFun fact: the decoder below is identical to the one in the example for [Neural Machine Translation with Attention](../sequences/nmt_with_attention.ipynb).\n\nThe model architecture is inspired by the [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf) paper.\n\n* In this example, you extract the features from the lower convolutional layer of InceptionV3 giving us a vector of shape (8, 8, 2048).\n* You squash that to a shape of (64, 2048).\n* This vector is then passed through the CNN Encoder (which consists of a single Fully connected layer).\n* The RNN (here GRU) attends over the image to predict the next word."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # attention_hidden_layer shape == (batch_size, 64, units)\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                         self.W2(hidden_with_time_axis)))\n\n    # score shape == (batch_size, 64, 1)\n    # This gives you an unnormalized score for each image feature.\n    score = self.V(attention_hidden_layer)\n\n    # attention_weights shape == (batch_size, 64, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            #weights=\"imagenet\",\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\nencoder_output = layers.Dropout(0.2)(encoder_output)\nencoder_output = layers.Dense(512,activation='relu')(encoder_output)\n'''\nclass CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        self.feat_ext = tf.keras.applications.DenseNet121(\n            include_top=False,\n            #weights=\"imagenet\",\n            input_shape=(256,256,3),\n        )\n        \n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.feat_ext(x)\n        #x = tf.keras.layers.Flatten()(x)\n        #x = tf.keras.layers.Dropout(0.2)(x)\n        #batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n        # shape after fc == (batch_size, 64, embedding_dim)\n        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n        # shape after fc == (batch_size, 64, embedding_dim)\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 256\nunits = 512\nencoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 0.0001#0.001\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\nloss_plot = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([tok.word_index['startseq']] * target.shape[0], 1)\n\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, trainable_variables)\n\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nEPOCHS = 20\nnum_steps = len(df)*validation_split // batch_sz\n\nfor epoch in range(start_epoch, EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    #for (batch, (img_tensor, target)) in enumerate(dataset):\n    for (batch, (X,Y)) in enumerate(train_dataloader):\n        img_tensor = X[0]\n        target = Y\n        #print(target.shape)\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n        if batch % 100 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n            #print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / num_steps)\n\n    if epoch % 5 == 0:\n      ckpt_manager.save()\n\n    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n                                         total_loss/num_steps))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Caption!\n- The evaluate function is similar to the training loop, except you don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n- Stop predicting when the model predicts the end token.\n- And store the attention weights for every time step."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\n# Shape of the vector extracted from DenseNet121 is (64, 1024)\n# These two variables represent that vector shape\nfeatures_shape = 1024\nattention_features_shape = 64\nencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_len, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(image, 0)\n    #img_tensor_val = image_features_extract_model(temp_input)\n    #img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(temp_input)\n\n    dec_input = tf.expand_dims([tok.word_index['startseq']], 0)\n    result = []\n\n    for i in range(max_len):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tok.index_word[predicted_id])\n\n        if tok.index_word[predicted_id] == 'endseq':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\ndef plot_attention(image, result, attention_plot):\n    temp_image = image#np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (8, 8))\n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch, (X,Y)  = next(enumerate(val_dataloader))\n#Sample img\nsmpl_idx = 0\n\n# Caption\n#print(tok2txt(X[1][smpl_idx], tok))\nreal_caption = tok2txt(Y[smpl_idx], tok)\nprint(real_caption)\n\n# Image\nimage = X[0][smpl_idx]\nplt.imshow(image)\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# captions on the validation set\n#rid = np.random.randint(0, len(img_name_val))\n#image = img_name_val[rid]\n\n#real_caption = ' '.join([tok.index_word[i] for i in cap_val[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image, result, attention_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End of show-attend-tell image \nAdaptation from https://www.tensorflow.org/tutorials/text/image_captioning"},{"metadata":{},"cell_type":"markdown","source":"# BLEU evaluation (TBD)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.evaluation.report_gen_evaluation import get_predictions_from_data_loader\nfrom copy import deepcopy\n\nval_dataloader_tmp = deepcopy(val_dataloader)\nval_dataloader_tmp.nb_iteration  = 3\nGT , preds = get_predictions_from_data_loader(val_dataloader_tmp,tok,encoder, decoder,max_len,decoder_type='GRU')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=0\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=1\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=2\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=3\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=4 \nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_from_dataloader(val_dataloader,tok,encoder_model,decoder_model,max_len,decoder_type='GRU')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_from_dataloader(val_dataloader,tok,encoder_model, decoder_model,max_len,decoder_type=\"LSTM\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}