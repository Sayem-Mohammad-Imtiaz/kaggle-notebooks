{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/creditdata/train.csv',parse_dates = ['X1','X2'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_col = [col for col in df.columns if df[col].dtype == 'object']\nlist_uniq_val = []\nfor i in categorical_col:\n    list_uniq_val.append(df[i].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unique_value = {}\nunique_value = dict(zip(categorical_col,list_uniq_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode(df,categorical_col, nan_as_cat = True):\n    ori_col = list(df.columns)\n    df = pd.get_dummies(df, columns = categorical_col, dummy_na = nan_as_cat)\n    categorical_col = [col for col in df.columns if col not in ori_col]\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def label_enco(df, categorical_col):\n#     for i in categorical_col:\n#         df[i], uniques = pd.factorize(df[i])\n#     return df[i], categorical_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_mean(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n        columns={counted: agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df\n\n\ndef do_median(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n        columns={counted: agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df\n\n\ndef do_std(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n        columns={counted: agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df\n\n\ndef do_sum(df, group_cols, counted, agg_name):\n    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n        columns={counted: agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    del gp\n    gc.collect()\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group = ['X21','X24','X28','X39','X42']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def creat_val(df,group):\n    \n    numerical_col = [col for col in df.columns if (df[col].dtypes =='float64' or df[col].dtypes =='int' )]\n    numerical_col.remove('id')\n    numerical_col.remove('label')\n    for i in numerical_col:\n        df = do_mean(df,group,i,i+'_mean')\n        df = do_median(df, group, i, i+'_median')\n        df = do_std(df, group, i, i+'_std')\n        df = do_sum(df, group,i, i+'_sum')\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = creat_val(df=df,group = group)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = one_hot_encode(df=df_new, categorical_col= ['X3','X21','X24','X28','X39','X42'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\ndf_new['time_interval'] = (df_new['X1']-df_new['X2']).dt.days","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.drop(['X1','X2','X22','X33','X34'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_new.drop(['label','id'], axis=1)\ny = df_new['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install pandas --upgrade","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=1000, learning_rate=0.05, output_process=False):\n    \n#     train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    \n#     def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n#         params = {'application':'binary','num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':100, 'metric':'auc'}\n#         params[\"num_leaves\"] = int(round(num_leaves))\n#         params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n#         params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n#         params['max_depth'] = int(round(max_depth))\n#         params['lambda_l1'] = max(lambda_l1, 0)\n#         params['lambda_l2'] = max(lambda_l2, 0)\n#         params['min_split_gain'] = min_split_gain\n#         params['min_child_weight'] = min_child_weight\n#         cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n#         return max(cv_result['auc-mean'])\n    \n#     lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n#                                             'feature_fraction': (0.1, 0.9),\n#                                             'bagging_fraction': (0.8, 1),\n#                                             'max_depth': (5, 8.99),\n#                                             'lambda_l1': (0, 5),\n#                                             'lambda_l2': (0, 3),\n#                                             'min_split_gain': (0.001, 0.1),\n#                                             'min_child_weight': (5, 50)}, random_state=0)\n    \n#     lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n#     return lgbBO.res['max']['max_params']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_param = opt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, \n#                                                   n_folds=3, random_seed=6, n_estimators=100, learning_rate=0.05)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.25, random_state =42)\nparam_grid = {'num_leaves': (24, 45),\n            'feature_fraction': (0.1, 0.9),\n            'bagging_fraction': (0.8, 1),\n            'max_depth': (5, 8.99),\n            'lambda_l1': (0, 5),\n            'lambda_l2': (0, 3),\n            'min_split_gain': (0.001, 0.1),\n            'min_child_weight': (5, 50)}\ngkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=y_train)\nclf = lgb.LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=20,\n                         learning_rate=0.01, metric='auc')\n# grid_search = GridSearchCV(clf, param_grid)\n# grid_search.fit(X_train,y_train)\ngsearch = GridSearchCV(estimator=clf, param_grid=param_grid, cv=gkf)\nlgb_model = gsearch.fit(X=X_train, y=y_train)\n\ny_pred = gsearch.predict(X_test)\n\nprint(\"The best possible combination of hyperparameters: \",gsearch.best_params_)\nprint(\"Model accuracy: {:.2f}%\".format(accuracy_score(y_pred, y_test)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}