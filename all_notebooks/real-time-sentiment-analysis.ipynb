{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.contrib import rnn\nimport numpy as np\nimport random\nimport pickle\nfrom collections import Counter\nimport tensorflow as tf\nimport pandas as pd\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kaggle datasets download -d krabhi07/positive-and-negative-sentence-only-ascii\npos_filepath= \"../input/positive-and-negative-sentences/positive.txt\"\nneg_filepath= \"../input/positive-and-negative-sentences/negative.txt\"\n#pos_data=pd.read_csv(pos_filepath, sep=\" \", header=None)\n#neg_data=pd.read_clipboard(neg_filepath)\n\nlemmatizer=WordNetLemmatizer()\nhm_lines=10000000\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n        \n\n\ndef create_lexicon(pos,neg):\n    lexicon=[]\n    for fi in [pos,neg]:\n        with open(fi,'r',encoding='utf-8',errors='ignore') as f:\n            contents = f.readlines()\n            for l in contents[:hm_lines]:\n                all_words=word_tokenize(l.lower())\n                lexicon +=list(all_words)\n                \n                \n    lexicon=[lemmatizer.lemmatize(i) for i in lexicon]\n    w_counts = Counter(lexicon)\n    l2=[]\n    for w in w_counts:\n        if 1000 >w_counts[w]> 50:\n            l2.append(w)\n            \n            \n    print(len(l2))      \n    return l2\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_handling(sample,lexicon,classification):\n    featureset=[]\n    with open(sample,'r',encoding='utf-8',errors='ignore') as f:\n        contents=f.readlines()#.decode('windows-1252')    here utf-8 encoding error\n        for l in contents[:hm_lines]:\n            current_words=word_tokenize(l.lower())\n            current_words=[lemmatizer.lemmatize(i) for i in current_words]\n            features=np.zeros(len(lexicon))\n            for word in current_words:\n                if word.lower() in lexicon:\n                    index_value=lexicon.index(word.lower())\n                    features[index_value] += 1\n                    \n            features= list(features)\n            featureset.append([features,classification])\n            \n    return featureset       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_feature_sets_and_labels(pos,neg,test_size=0.1):\n    lexicon = create_lexicon(pos_filepath,neg_filepath)\n    features=[]\n    features += sample_handling(pos_filepath,lexicon,[1,0])\n    features += sample_handling(neg_filepath,lexicon,[0,1])\n    random.shuffle(features)\n    features=np.array(features)\n    testing_size=int(test_size*len(features))\n    \n    train_x=list(features[:,0][:-testing_size])\n    train_y=list(features[:,1][:-testing_size])\n    \n    test_x=list(features[:,0][-testing_size:])\n    test_y=list(features[:,1][-testing_size:])\n    \n\n    return train_x,train_y,test_x,test_y,lexicon    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" if __name__ == '__main__' :\n    train_x,train_y,test_x,test_y,lexicon=create_feature_sets_and_labels(pos_filepath,neg_filepath)\n     #with open('sentiment_set.pickle','wb') as f:\n        #pickle.dump([train_x,train_y,test_x,test_y],f)\n       \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_nodes_hl1=500\nn_nodes_hl2=500\nn_nodes_hl3=500\n\nn_classes=2\nbatch_size=100\n\nx=tf.placeholder('float',shape=[None,len(train_x[0])])\ny=tf.placeholder('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network(data):\n\n    hidden_layer_1={'Weights':tf.Variable(tf.random_normal([len(train_x[0]),n_nodes_hl1])),\n                    'Biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n    hidden_layer_2={'Weights':tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n                    'Biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n    hidden_layer_3={'Weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n                    'Biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n    output_layer={'Weights':tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n                  'Biases':tf.Variable(tf.random_normal([n_classes]))}\n    \n    l1=tf.add(tf.matmul(data,hidden_layer_1['Weights']), hidden_layer_1['Biases'])\n    l1=tf.nn.relu(l1)\n    \n    l2=tf.add(tf.matmul(l1,hidden_layer_2['Weights']), hidden_layer_2['Biases'])\n    l2=tf.nn.relu(l2)\n    \n    l3=tf.add(tf.matmul(l2,hidden_layer_3['Weights']), hidden_layer_3['Biases'])\n    l3=tf.nn.relu(l3)\n    \n    output=tf.add(tf.matmul(l3,output_layer['Weights']), output_layer['Biases'])\n    \n    \n    return output\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_neural_network(x):\n    prediction=recurrent_neural_network(x)                       \n    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( logits= prediction, labels= y,name=None))\n    optimizer=tf.train.AdamOptimizer().minimize(cost)\n    hm_epoch=3\n    print(hm_epoch)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        \n        for epoch in range(hm_epoch):\n            epoch_loss=0\n            \n            i=0\n            while i < len(train_x):\n                start=i\n                end=i+batch_size\n                batch_x=np.array(train_x[start:end])\n                batch_y=np.array(train_y[start:end])\n            \n                _,c=sess.run([optimizer,cost], feed_dict = {x:batch_x,y: batch_y})\n                epoch_loss+=c\n                \n                i += batch_size\n            print('Epoch :',epoch,'completed out of :',hm_epoch,'loss :',epoch_loss)   \n           \n        \n        correct=tf.equal(tf.argmax(prediction,1), tf.argmax(y ,1))\n        accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n        print('Accuracy :',accuracy.eval({x:test_x, y:test_y}))\n        input_data=input(\"enter the test sentence : \")\n        features = get_features_for_input(input_data,lexicon)\n        result = (sess.run(tf.argmax(prediction.eval(feed_dict={x:features}),1)))\n        if result[0] == 0:\n            print('Positive:',input_data)\n            f1=open('../input/positive.txt','r')\n            \n            if f1.mode == 'r':\n                    \n                    contents=f1.read()\n                    print(contents)\n                \n                \n       \n            \n        elif result[0] == 1:\n            print('Negative:',input_data)\n            f2=open('../input/negative.txt','r')\n            if f2.mode =='r':\n                \n                contents=f2.read()\n                print(contents)\n            \n                \n                \n        \n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_neural_network(x):\n    prediction=neural_network(x)                       \n    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( logits= prediction, labels= y,name=None))\n    optimizer=tf.train.AdamOptimizer().minimize(cost)\n    hm_epoch=7\n    print(hm_epoch)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        \n        for epoch in range(hm_epoch):\n            epoch_loss=0\n            \n            i=0\n            while i < len(train_x):\n                start=i\n                end=i+batch_size\n                batch_x=np.array(train_x[start:end])\n                batch_y=np.array(train_y[start:end])\n            \n                _,c=sess.run([optimizer,cost], feed_dict = {x:batch_x,y: batch_y})\n                epoch_loss+=c\n                \n                i += batch_size\n            print('Epoch :',epoch,'completed out of :',hm_epoch,'loss :',epoch_loss)   \n           \n        \n        correct=tf.equal(tf.argmax(prediction,1), tf.argmax(y ,1))\n        accuracy=tf.reduce_mean(tf.cast(correct,'float'))\n        print('Accuracy :',accuracy.eval({x:test_x, y:test_y}))\n        input_data=input(\"enter the test sentence :  \")\n        features = get_features_for_input(input_data,lexicon)\n        result = (sess.run(tf.argmax(prediction.eval(feed_dict={x:features}),1)))\n        if result[0] == 0:\n            print('Positive:',input_data)\n            '''' f1=open('../input/positive.txt','r')\n            \n            if f1.mode == 'r':\n                    \n                    contents=f1.read()\n                    print(contents)'''\n                \n                \n       \n            \n        elif result[0] == 1:\n            print('Negative:',input_data)\n            '''f2=open('../input/negative.txt','r')\n             if f2.mode =='r':\n                \n                contents=f2.read()\n                print(contents)'''\n            \n                \n                \n        \n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_features_for_input(input,lexicon):\n    featureset = []\n    lemmatizer=WordNetLemmatizer()\n    current_words = word_tokenize(input.lower())\n    current_words = [lemmatizer.lemmatize(i) for i in current_words]\n    features = np.zeros(len(lexicon))\n    for word in current_words:\n        if word.lower() in lexicon:\n            index_value = lexicon.index(word.lower())\n            features[index_value] += 1\n    featureset.append(features)\n    return np.asarray(featureset)\n\ntrain_neural_network(x)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}