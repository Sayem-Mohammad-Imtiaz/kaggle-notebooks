{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Reference Links - \n\n* https://jalammar.github.io/illustrated-word2vec/\n* https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/\n* https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n* https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n* https://bio.nlplab.org/\n* https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n* https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can not feed words directly into most of the models, we need numeric values for each text. Earlier we used vectorization techniques like - Tfifd and CountVectorize to represent each text. In this notebook we learn about word and sentence embeddings are how they can be used for downstream tasks like text classification.\n\n## Table of Contents\n\n* Embeddings\n    * Word2Vec\n    * FastText\n    * Doc2Vec\n* Visualizing Representations\n* Evaluating Representations on Text Classification with Logistic Regression","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-specialization-data/Cleaned_POS_Medical_Notes.csv') #for excel file use read_excel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='emb'></a>\n\n### Embeddings\n\nWord embedding is one of the most important techniques in natural language processing(NLP), where words are mapped to vectors of real numbers. Word embedding is capable of capturing the meaning of a word in a document, semantic and syntactic similarity, relation with other words. It also has been widely used for recommender systems and text classification. This tutorial will show a brief introduction of genism word2vec model with an example of generating word embedding for review texts.\n\nEmbedding techniques have been used on different entities - character, word, document, paragraph etc. Apart from word embeddings, we also discuss document embedding in this section.","metadata":{}},{"cell_type":"markdown","source":"<a id='w2v'></a>\n\n#### Word2Vec\n\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one cluster. Another interesting property of word2vec is, it preserves distance between similar items.\n\n<img src=https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/06062705/Word-Vectors.png>\n\nLet us build Word2Vec embeddings using our own dataset.","metadata":{}},{"cell_type":"code","source":"from gensim.models.word2vec import Word2Vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_word2vec = df.clean_text.tolist()\ninput_word2vec = [i.split() for i in input_word2vec]\n\n#phrases = Phrases(input_word2vec, min_count=30, progress_per=10000)\n#bigram = Phraser(phrases)\n#input_word2vec = bigram[input_word2vec]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (len(input_word2vec))\nprint (input_word2vec[0][:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gensim word2vec requires texts in the above format for training.","metadata":{}},{"cell_type":"code","source":"model = Word2Vec(min_count=3)\nmodel.build_vocab(input_word2vec)\nmodel.train(input_word2vec,total_examples = model.corpus_count,epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (model.wv['diabetes'].shape)\nmodel.wv['diabetes']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (len(model.wv.key_to_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, using W2V we are able to calculate 100 dimensional embeddings for each of 6305 words. Let us check few similar word pairs.","metadata":{}},{"cell_type":"code","source":"model.wv.most_similar('diabetes')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('bladder')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('gait')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence, hypertension is very close to diabetes. Similarly, bowel is very similar to the word bladder.","metadata":{}},{"cell_type":"code","source":"def get_document_vector(x,model,vector_size=100):\n    vec = np.zeros(vector_size)\n    count = 0\n    for i in x.split():\n        try:\n            vec += model[i]\n            count += 1\n        except:\n            pass\n    if count > 0:\n        vec /= count\n    \n    return vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec_features = np.array([get_document_vector(i,model) for i in df.clean_text.tolist()])\nprint (word2vec_features.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We calculate average embedding score for each of the texts.","metadata":{}},{"cell_type":"code","source":"model.wv.save_word2vec_format(\"w2v.txt\",binary=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='ft'></a>\n\n#### FastText\n\nFastText is a technique developed by Facebook research for sub-word level representation learning. Fasttext is very fast and efficient for out of vocabulary word and sub-word level embeddings.","metadata":{}},{"cell_type":"code","source":"from gensim.models.fasttext import FastText","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastText(min_count=3,vector_size=100)\nmodel.build_vocab(input_word2vec)\nmodel.train(input_word2vec,total_examples = model.corpus_count,epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('diabetes')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('bladder')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar('gait')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (len(model.wv.key_to_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fasttext_features = np.array([get_document_vector(i,model) for i in df.clean_text.tolist()])\nprint (fasttext_features.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.wv.save_word2vec_format(\"ft.txt\",binary=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pubmed pretrained embeddings\n\nThis embedding was trained on huge biomedical corpus. We load the trained model and do some exploration. The bin file can be downloaded from https://bio.nlplab.org/","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pubmed_model = KeyedVectors.load_word2vec_format('/kaggle/input/nlp-specialization-data/pubmed2018_w2v_200D/pubmed2018_w2v_200D/pubmed2018_w2v_200D.bin',binary=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pubmed_model.most_similar('diabetes')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pubmed_model.most_similar('bladder')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pubmed_model.most_similar('gait')[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (len(pubmed_model.index_to_key))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pubmed_model['gait'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the pubmed pretrained model, we have 200 dimensional embeddings for 2.6 million words, mostly from biomedical domain.","metadata":{}},{"cell_type":"code","source":"pubmed_features = np.array([get_document_vector(i,pubmed_model,vector_size=200) for i in df.clean_text.tolist()])\nprint (pubmed_features.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pubmed_model.save_word2vec_format('pubmed_wv.txt',binary=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='d2v'></a>\n\n#### Doc2Vec\n\nAfter understanding what word2vec is, it will be easier to understand how doc2vec works.\n\nAs said, the goal of doc2vec is to create a numeric representation of a document, regardless of its length. But unlike words, documents do not come in logical structures such as words, so the another method has to be found.\n\n<img src=https://miro.medium.com/max/608/0*x-gtU4UlO8FAsRvL.>\n\nUsing Gensim, we can learn Doc2Vec representation as well for our dataset.","metadata":{}},{"cell_type":"code","source":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df.clean_text.tolist())]\nmodel = Doc2Vec(vector_size=100)\nmodel.build_vocab(documents)\nmodel.train(documents,total_examples = model.corpus_count,epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.docvecs[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.docvecs.most_similar(0)[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Under Doc2Vec, the 1st text is most similar to the 340th text.","metadata":{}},{"cell_type":"code","source":"doc2vec_features = np.array([model.docvecs[i] for i in range(len(df))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='viz'></a>\n\n### Visualizing Representations\n\nWe can use quantitative methods for evaluating representations learned from different model. However, as the task is unsupervised, most of the evaluations are based on similarity based metrics. As we see below, some of the representations preserve local properties i.e - similar texts have similar representation and cluster together. On the other hand, some methods look for global structures. Usually, people evaluate representations using downstream task specific metrics.\n\nAs we can not visualize 100-dim vectors, we use t-SNE embeddings to reduce the dimensions into 2. t-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional dfsets.","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_w2v = tsne_model.fit_transform(word2vec_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_ft = tsne_model.fit_transform(fasttext_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_pm = tsne_model.fit_transform(pubmed_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_d2v = tsne_model.fit_transform(doc2vec_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom IPython.display import HTML, Image\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\nimport plotly.express as px\n#init_notebook_mode(connected=True)\n\nall_colors = px.colors.qualitative.Plotly\nprint (all_colors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_df = pd.DataFrame()\nvis_df['x'] = tsne_w2v[:,0]\nvis_df['y'] = tsne_w2v[:,1]\nvis_df['speciality'] = df.label\n\nfig = px.scatter(vis_df, x=\"x\", y=\"y\", color=vis_df.speciality, title=\"Projection of Clinical Texts (based on Word2Vec)\", height=600, width=1000) #hover_df=['top_words']\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_df = pd.DataFrame()\nvis_df['x'] = tsne_ft[:,0]\nvis_df['y'] = tsne_ft[:,1]\nvis_df['speciality'] = df.label\n\nfig = px.scatter(vis_df, x=\"x\", y=\"y\", color=vis_df.speciality, title=\"Projection of Clinical Texts (based on FastText)\", height=600, width=1000) #hover_df=['top_words']\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_df = pd.DataFrame()\nvis_df['x'] = tsne_pm[:,0]\nvis_df['y'] = tsne_pm[:,1]\nvis_df['speciality'] = df.label\n\nfig = px.scatter(vis_df, x=\"x\", y=\"y\", color=vis_df.speciality, title=\"Projection of Clinical Texts (based on PubMed)\", height=600, width=1000) #hover_df=['top_words']\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vis_df = pd.DataFrame()\nvis_df['x'] = tsne_d2v[:,0]\nvis_df['y'] = tsne_d2v[:,1]\nvis_df['speciality'] = df.label\n\nfig = px.scatter(vis_df, x=\"x\", y=\"y\", color=vis_df.speciality, title=\"Projection of Clinical Texts (based on Doc2Vec)\", height=600, width=1000) #hover_df=['top_words']\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Embeddings with Logistic Regression\n\nIn this section we fit simple logistic regression with embeddings as features to classify the clinial notes into the specialities.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = LogisticRegression(multi_class='auto',solver='lbfgs')\ncv_scores = cross_val_score(X=doc2vec_features,y=df.label,cv=StratifiedKFold(n_splits=5,random_state=42,shuffle=True),estimator=lg)\nprint (cv_scores, np.mean(cv_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = LogisticRegression(multi_class='auto',solver='lbfgs')\ncv_scores = cross_val_score(X=word2vec_features,y=df.label,cv=StratifiedKFold(n_splits=5,random_state=42,shuffle=True),estimator=lg)\nprint (cv_scores, np.mean(cv_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = LogisticRegression(multi_class='auto',solver='lbfgs')\ncv_scores = cross_val_score(X=fasttext_features,y=df.label,cv=StratifiedKFold(n_splits=5,random_state=42,shuffle=True),estimator=lg)\nprint (cv_scores, np.mean(cv_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = LogisticRegression(multi_class='auto',solver='lbfgs')\ncv_scores = cross_val_score(X=pubmed_features,y=df.label,cv=StratifiedKFold(n_splits=5,random_state=42,shuffle=True),estimator=lg)\nprint (cv_scores, np.mean(cv_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using FastText and PubMed embeddings as features, we can achieve good performance using a simple logistic regression model.","metadata":{}}]}