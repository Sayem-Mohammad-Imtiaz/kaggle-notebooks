{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-02T10:27:29.019329Z","iopub.execute_input":"2021-07-02T10:27:29.020066Z","iopub.status.idle":"2021-07-02T10:27:29.040495Z","shell.execute_reply.started":"2021-07-02T10:27:29.019966Z","shell.execute_reply":"2021-07-02T10:27:29.039392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LIBRARIES**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:29.042126Z","iopub.execute_input":"2021-07-02T10:27:29.042466Z","iopub.status.idle":"2021-07-02T10:27:29.939782Z","shell.execute_reply.started":"2021-07-02T10:27:29.042434Z","shell.execute_reply":"2021-07-02T10:27:29.938931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset import**","metadata":{}},{"cell_type":"code","source":"water_potability_filepath = '../input/water-potability/water_potability.csv'\n\nfull_data = pd.read_csv(water_potability_filepath)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:29.941161Z","iopub.execute_input":"2021-07-02T10:27:29.941604Z","iopub.status.idle":"2021-07-02T10:27:29.978951Z","shell.execute_reply.started":"2021-07-02T10:27:29.941573Z","shell.execute_reply":"2021-07-02T10:27:29.977905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:29.980514Z","iopub.execute_input":"2021-07-02T10:27:29.980821Z","iopub.status.idle":"2021-07-02T10:27:29.989181Z","shell.execute_reply.started":"2021-07-02T10:27:29.98079Z","shell.execute_reply":"2021-07-02T10:27:29.988115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a total of *3276* training examples, having 9 features and 1 target, i.e. *Potability*.","metadata":{}},{"cell_type":"code","source":"full_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:29.990671Z","iopub.execute_input":"2021-07-02T10:27:29.991034Z","iopub.status.idle":"2021-07-02T10:27:30.021911Z","shell.execute_reply.started":"2021-07-02T10:27:29.991004Z","shell.execute_reply":"2021-07-02T10:27:30.020725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values in *ph*, *Sulfate*, and *Trihalomethanes*. We will deal with these later.","metadata":{}},{"cell_type":"markdown","source":"# FEATURE DESCRIPTIONS\n\n* **ph** - Indicates acidic/alkaline behaviour of the water sample. Recommended pH value as per WHO guidelines is 6.5 to 8.5.\n\n* **Hardness** - The amount of dissolved magnesium and calcium salts in water. According to WHO, hardness should not exceed 120 to 170 mg/L.\n\n* **Solids** - Given by Total Dissolved Solids (TDS), or the amount of solids dissolved in water. TDS value of 50 to 250 ppm is deemed safe.\n\n* **Chloramines** - Formed when ammonia is added to chlorine to treat drinkning water. Chlorine levels up to 4 mg/L (or 4 parts per million (ppm)) are considered safe in drinking water.\n\n* **Sulfates** - These are a part of naturally occuring minerals and are dissolved into groundwater. Sulfate levels above 250 mg/L in water are considered unsafe.\n\n* **Conductivity** - It is the measure of tendancy of water to conduct electricity. According to WHO, Electrical Conductivity (EC) should not exceed 400 μS/cm.\n\n* **Organic_carbon** - It is a measure of the total amount of carbon in organic compounds in pure water. Total Organic Carbon (TOC) below 25 ppm is considered safe.\n\n* **Trihalomethanes** - These may be found in water treated with chlorine. THM values upto 100 μg/L are considered fit for drinking water.\n\n* **Turbidity** - It is a measure of light emitting properties of water. According to WHO, turbidity of drinking water shouldn't be more than 5 NTU, and should ideally be below 1 NTU.\n\n* **Potability** - Indicates if water is safe for human consumption where 1 means Potable and 0 means Not potable.","metadata":{}},{"cell_type":"code","source":"full_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.023546Z","iopub.execute_input":"2021-07-02T10:27:30.023961Z","iopub.status.idle":"2021-07-02T10:27:30.087396Z","shell.execute_reply.started":"2021-07-02T10:27:30.023931Z","shell.execute_reply":"2021-07-02T10:27:30.086462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.089545Z","iopub.execute_input":"2021-07-02T10:27:30.08985Z","iopub.status.idle":"2021-07-02T10:27:30.111222Z","shell.execute_reply.started":"2021-07-02T10:27:30.08982Z","shell.execute_reply":"2021-07-02T10:27:30.110259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data seems to be grouped by *Potability*, as we only see 0 values in head.\nTherefore, it is best to shuffle this dataset before proceeding.","metadata":{}},{"cell_type":"code","source":"full_data = full_data.sample(frac = 1).reset_index(drop = True)\n\nfull_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.112819Z","iopub.execute_input":"2021-07-02T10:27:30.113096Z","iopub.status.idle":"2021-07-02T10:27:30.136257Z","shell.execute_reply.started":"2021-07-02T10:27:30.113068Z","shell.execute_reply":"2021-07-02T10:27:30.135454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"full_data.Potability.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.137591Z","iopub.execute_input":"2021-07-02T10:27:30.138184Z","iopub.status.idle":"2021-07-02T10:27:30.147717Z","shell.execute_reply.started":"2021-07-02T10:27:30.138138Z","shell.execute_reply":"2021-07-02T10:27:30.146458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,6))\nfull_data.Potability.value_counts().plot.pie(autopct = \"%.1f%%\")\nplt.title('Potability distribution pie chart', pad = 20,\n         fontdict = {'size' : 15, 'color' : 'darkblue', 'weight' : 'bold'})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.148974Z","iopub.execute_input":"2021-07-02T10:27:30.149317Z","iopub.status.idle":"2021-07-02T10:27:30.344816Z","shell.execute_reply.started":"2021-07-02T10:27:30.149286Z","shell.execute_reply":"2021-07-02T10:27:30.344094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be a slight imbalance in target value. However, nothing major to be worried about (**yet**).","metadata":{}},{"cell_type":"markdown","source":"Now we will classify the features into numerical and categorical. This will help us to preprocess the data in an appropriate manner.","metadata":{}},{"cell_type":"code","source":"def col_type(col):\n    if (col.nunique() <= 10) | (col.dtype == 'object'):\n        return 'cat'\n    else:\n        return 'num'\n\nnumerical_col = [col for col in full_data.columns\n                if col_type(full_data[col]) == 'num']\ncategorical_col = [col for col in full_data.columns\n                  if col_type(full_data[col]) == 'cat']\n\nprint('Numerical features: {}'.format(numerical_col))\nprint('Categorical features: {}'.format(categorical_col))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.345937Z","iopub.execute_input":"2021-07-02T10:27:30.346399Z","iopub.status.idle":"2021-07-02T10:27:30.366056Z","shell.execute_reply.started":"2021-07-02T10:27:30.34634Z","shell.execute_reply":"2021-07-02T10:27:30.365105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like the only categorical column we have is the target feature. Therefore, the entire feature set will consist of numerical type features.","metadata":{}},{"cell_type":"code","source":"features = numerical_col","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.367347Z","iopub.execute_input":"2021-07-02T10:27:30.367686Z","iopub.status.idle":"2021-07-02T10:27:30.371599Z","shell.execute_reply.started":"2021-07-02T10:27:30.367656Z","shell.execute_reply":"2021-07-02T10:27:30.370367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(hue = 'Potability', data = full_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:30.372791Z","iopub.execute_input":"2021-07-02T10:27:30.373082Z","iopub.status.idle":"2021-07-02T10:28:03.347057Z","shell.execute_reply.started":"2021-07-02T10:27:30.373054Z","shell.execute_reply":"2021-07-02T10:28:03.345603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There doesn't seem to be any pattern in any of these pair plots. Almost all the points corresponding to 0 Potability and 1 Potabilitiy seem to be jumbled together randomly. This might prove to be a big problem because there doesn't appear any room for classification based on these plots.","metadata":{}},{"cell_type":"markdown","source":"* Let's try finding correlation based on a correlation matrix.","metadata":{}},{"cell_type":"code","source":"corrMat = full_data.corr()\n\nplt.figure(figsize = (15,10))\nsns.heatmap(corrMat, square = True, annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:03.349115Z","iopub.execute_input":"2021-07-02T10:28:03.3496Z","iopub.status.idle":"2021-07-02T10:28:04.160042Z","shell.execute_reply.started":"2021-07-02T10:28:03.349551Z","shell.execute_reply":"2021-07-02T10:28:04.158992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No significant relation between any two features or between a feature and target. \n\nThis makes me question the credibility of the dataset since water quality is a sensitive topic, and there are known borderline values for these features which assign them to safe/unsafe categories. So this kind of randomness just doesn't seem genuine. \nLike for example, we have some examples where water bodies with pH exceeding 10.0 (which is alkaline and way over the safety threshold) is deemed potable. ","metadata":{}},{"cell_type":"code","source":"full_data.sort_values(by = 'ph', ascending = False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:04.161243Z","iopub.execute_input":"2021-07-02T10:28:04.16174Z","iopub.status.idle":"2021-07-02T10:28:04.184933Z","shell.execute_reply.started":"2021-07-02T10:28:04.161697Z","shell.execute_reply":"2021-07-02T10:28:04.184193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fourth row, *ph* = **13.175402**, *chloramines* = **8.9**, *Sulfate* = **375**, *Conductivity* = **500**, and still deemed Potable. This just does not make sense.","metadata":{}},{"cell_type":"markdown","source":"Another questionable feature is *Solids*, which I'll elaborate below:","metadata":{}},{"cell_type":"code","source":"print('Mean value of TDS (ppm):',full_data.Solids.mean())\n\nplt.figure(figsize = (8,5))\nsns.distplot(full_data.Solids)\nplt.xlabel('Solids (ppm)')\nplt.title('Distribution of Total Dissolved Solids (TDS)')\nprint()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:04.186254Z","iopub.execute_input":"2021-07-02T10:28:04.186767Z","iopub.status.idle":"2021-07-02T10:28:04.462142Z","shell.execute_reply.started":"2021-07-02T10:28:04.186733Z","shell.execute_reply":"2021-07-02T10:28:04.461442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the above distribution, mean value of TDS in the dataset is about 22000 ppm. However, safe values for TDS ranges from about 50 to 250 ppm. Thus, about every sample from this dataset should be unfit for consumption.\n\nFor this reason, it seems highly likely that either this feature was incorrectly extracted or that the unit is messed up. Hence, we could get rid of this column from the dataset.","metadata":{}},{"cell_type":"code","source":"#full_data.drop('Solids', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:04.463167Z","iopub.execute_input":"2021-07-02T10:28:04.463578Z","iopub.status.idle":"2021-07-02T10:28:04.466964Z","shell.execute_reply.started":"2021-07-02T10:28:04.463547Z","shell.execute_reply":"2021-07-02T10:28:04.465989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, on inspecting the correlation matrix, it appears that *Potability* has relatively the highest correlation coefficient with *Solids*, however small it may be. Thus, it may not be wise to remove your most significant feature.","metadata":{}},{"cell_type":"code","source":"corrMat.Potability.abs().sort_values(ascending = False)[1:]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:04.468374Z","iopub.execute_input":"2021-07-02T10:28:04.468809Z","iopub.status.idle":"2021-07-02T10:28:04.485955Z","shell.execute_reply.started":"2021-07-02T10:28:04.468763Z","shell.execute_reply":"2021-07-02T10:28:04.484643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, we will divide the entire column *Solids* by 100 so that the values at least seem believable.","metadata":{}},{"cell_type":"code","source":"full_data.Solids = full_data.Solids / 100\n\nprint('Mean value of TDS (ppm):',full_data.Solids.mean())\n\nplt.figure(figsize = (8,5))\nsns.distplot(full_data.Solids)\nplt.xlabel('Solids (ppm)')\nplt.title('Distribution of Total Dissolved Solids (TDS)')\nprint()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:04.49087Z","iopub.execute_input":"2021-07-02T10:28:04.491317Z","iopub.status.idle":"2021-07-02T10:28:04.989653Z","shell.execute_reply.started":"2021-07-02T10:28:04.491285Z","shell.execute_reply":"2021-07-02T10:28:04.988825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import norm\n\nf, axes = plt.subplots(3, 3, figsize = (10, 10))\n\nsns.distplot(full_data.ph, fit = norm, ax = axes[0,0])\nsns.distplot(full_data.Hardness, fit = norm, ax = axes[0,1])\nsns.distplot(full_data.Solids, fit = norm, ax = axes[0,2])\nsns.distplot(full_data.Chloramines, fit = norm, ax = axes[1,0])\nsns.distplot(full_data.Sulfate, fit = norm, ax = axes[1,1])\nsns.distplot(full_data.Conductivity, fit = norm, ax = axes[1,2])\nsns.distplot(full_data.Organic_carbon, fit = norm, ax = axes[2,0])\nsns.distplot(full_data.Trihalomethanes, fit = norm, ax = axes[2,1])\nsns.distplot(full_data.Turbidity, fit = norm, ax = axes[2,2])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:04.991199Z","iopub.execute_input":"2021-07-02T10:28:04.991657Z","iopub.status.idle":"2021-07-02T10:28:07.176044Z","shell.execute_reply.started":"2021-07-02T10:28:04.991623Z","shell.execute_reply":"2021-07-02T10:28:07.175262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All features seem to be distributed as a normal distribution. Funny how the features are all balanced, while the dataset is not.","metadata":{}},{"cell_type":"markdown","source":"# **Data Preprocessing**\n\nNow we will handle the missing values as well as scale the features of our dataset.","metadata":{}},{"cell_type":"code","source":"X = full_data[features]\ny = full_data.Potability","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:07.17703Z","iopub.execute_input":"2021-07-02T10:28:07.177462Z","iopub.status.idle":"2021-07-02T10:28:07.182696Z","shell.execute_reply.started":"2021-07-02T10:28:07.177414Z","shell.execute_reply":"2021-07-02T10:28:07.181469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is important to always split the data into train-test sets before applying any transformations on it. Not doing so could result in data leakage in our model and consequently, inaccurate model predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:07.183761Z","iopub.execute_input":"2021-07-02T10:28:07.184169Z","iopub.status.idle":"2021-07-02T10:28:07.393118Z","shell.execute_reply.started":"2021-07-02T10:28:07.184138Z","shell.execute_reply":"2021-07-02T10:28:07.392068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape, sep = '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:07.394358Z","iopub.execute_input":"2021-07-02T10:28:07.394698Z","iopub.status.idle":"2021-07-02T10:28:07.40032Z","shell.execute_reply.started":"2021-07-02T10:28:07.394667Z","shell.execute_reply":"2021-07-02T10:28:07.399266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_train = pd.concat([X_train, y_train], axis = 1)\nfull_val = pd.concat([X_val, y_val], axis = 1)\n\nfull_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:07.401522Z","iopub.execute_input":"2021-07-02T10:28:07.40196Z","iopub.status.idle":"2021-07-02T10:28:07.42726Z","shell.execute_reply.started":"2021-07-02T10:28:07.401924Z","shell.execute_reply":"2021-07-02T10:28:07.426514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values\n\nThe problem starts by identifying the type and count of missing values. Then we will adopt a suitable algorithm to fill the blanks.","metadata":{}},{"cell_type":"code","source":"missing_val_col = full_train.isnull().sum().sort_values(ascending = False)\nmissing_val_col = missing_val_col[missing_val_col > 0]\nmissing_ratio_col = missing_val_col / full_train.shape[0]\n\nmissing = pd.concat([missing_val_col, missing_ratio_col * 100], axis = 1,\n                   keys = ['total', '%'])\nmissing","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:07.428238Z","iopub.execute_input":"2021-07-02T10:28:07.428702Z","iopub.status.idle":"2021-07-02T10:28:07.444823Z","shell.execute_reply.started":"2021-07-02T10:28:07.428671Z","shell.execute_reply":"2021-07-02T10:28:07.443925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in missing_val_col.index:\n    plt.figure(figsize = (15,10))\n    sns.boxplot(x = 'Potability', y = col, data = full_train)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:07.446143Z","iopub.execute_input":"2021-07-02T10:28:07.446443Z","iopub.status.idle":"2021-07-02T10:28:07.94985Z","shell.execute_reply.started":"2021-07-02T10:28:07.446415Z","shell.execute_reply":"2021-07-02T10:28:07.948791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Medians for Potability = 0 box and Potability = 1 box seem to overlap in all 3 cases, meaning there isn't any significant relation between Potability and the features in debate. This is congruent to our earlier correlation matrix analysis. \n\nImputing the missing values with mean seems okay, as it won't disturb this dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy = 'median')\nimputed_X_train = pd.DataFrame(imputer.fit_transform(full_train))\nimputed_X_val = pd.DataFrame(imputer.transform(full_val))\n\nimputed_X_train.columns = full_train.columns\nimputed_X_val.columns = full_val.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:07.951103Z","iopub.execute_input":"2021-07-02T10:28:07.951423Z","iopub.status.idle":"2021-07-02T10:28:08.1537Z","shell.execute_reply.started":"2021-07-02T10:28:07.951395Z","shell.execute_reply":"2021-07-02T10:28:08.152713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_X_train.isnull().any().any()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:08.155241Z","iopub.execute_input":"2021-07-02T10:28:08.155691Z","iopub.status.idle":"2021-07-02T10:28:08.16425Z","shell.execute_reply.started":"2021-07-02T10:28:08.155648Z","shell.execute_reply":"2021-07-02T10:28:08.163456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All missing values have now been dealt with. ","metadata":{}},{"cell_type":"markdown","source":"Before moving onto feature scale, it is wise to first take a look at the outliers in our dataset, as their presense might lead to glitchy data transformations.","metadata":{}},{"cell_type":"code","source":"for col in features:\n    plt.figure(figsize = (10,8))\n    sns.boxplot(y = col, data = imputed_X_train)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:08.165426Z","iopub.execute_input":"2021-07-02T10:28:08.16575Z","iopub.status.idle":"2021-07-02T10:28:09.271681Z","shell.execute_reply.started":"2021-07-02T10:28:08.16572Z","shell.execute_reply":"2021-07-02T10:28:09.270649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seem to be just a few major outliers, and I've written the filter code for them in the code cell below. However, for such few outliers, using *RobustScaler()* should be enough.","metadata":{}},{"cell_type":"markdown","source":"> the centering and scaling statistics of RobustScaler is based on percentiles and are therefore not influenced by a few number of very large marginal outliers.","metadata":{}},{"cell_type":"markdown","source":"> ","metadata":{}},{"cell_type":"code","source":"#filter = (imputed_X_train['Organic_carbon'] < 25) \n#imputed_X_train = imputed_X_train.loc[filter]\n\n#filter = (imputed_X_train['Hardness'] > 50)\n#imputed_X_train =imputed_X_train.loc[filter]\n\n#filter = (imputed_X_train['Conductivity'] < 700)\n#imputed_X_train = imputed_X_train.loc[filter]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.272881Z","iopub.execute_input":"2021-07-02T10:28:09.273153Z","iopub.status.idle":"2021-07-02T10:28:09.277126Z","shell.execute_reply.started":"2021-07-02T10:28:09.273126Z","shell.execute_reply":"2021-07-02T10:28:09.276027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = imputed_X_train.Potability.round()\nimputed_X_train.drop('Potability', axis = 1, inplace = True)\n\ny_val = imputed_X_val.Potability.round()\nimputed_X_val.drop('Potability', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.278722Z","iopub.execute_input":"2021-07-02T10:28:09.279111Z","iopub.status.idle":"2021-07-02T10:28:09.291974Z","shell.execute_reply.started":"2021-07-02T10:28:09.27907Z","shell.execute_reply":"2021-07-02T10:28:09.290754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(imputed_X_train.shape, y_train.shape)\nprint(imputed_X_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.293159Z","iopub.execute_input":"2021-07-02T10:28:09.29345Z","iopub.status.idle":"2021-07-02T10:28:09.304608Z","shell.execute_reply.started":"2021-07-02T10:28:09.293422Z","shell.execute_reply":"2021-07-02T10:28:09.303458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling\n\nAlthough using just the *StandardScaler()* should do the job, the abovementioned outliers might throw off the calculations of the scaler. Hence, a better approach would be to first use *RobustScaler()*, which will handle the outliers before passing the data to the *StandardScaler()*","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nscaler = RobustScaler()\nrobust_X_train = scaler.fit_transform(imputed_X_train)\nrobust_X_val = scaler.transform(imputed_X_val)\n\nsc = StandardScaler()\nscaled_X_train = sc.fit_transform(robust_X_train)\nscaled_X_val = sc.transform(robust_X_val)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.30597Z","iopub.execute_input":"2021-07-02T10:28:09.306276Z","iopub.status.idle":"2021-07-02T10:28:09.326818Z","shell.execute_reply.started":"2021-07-02T10:28:09.306246Z","shell.execute_reply":"2021-07-02T10:28:09.325878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On passing through the Scalers, our Data Frame has now been converted to a numpy array. So, for convention, we will convert the array back to a Data Frame.","metadata":{}},{"cell_type":"code","source":"final_X_train = pd.DataFrame(scaled_X_train, index = imputed_X_train.index, columns = imputed_X_train.columns)\nfinal_X_val = pd.DataFrame(scaled_X_val, index = imputed_X_val.index,\n                            columns = imputed_X_val.columns)\n\nfinal_X_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.328058Z","iopub.execute_input":"2021-07-02T10:28:09.32837Z","iopub.status.idle":"2021-07-02T10:28:09.377298Z","shell.execute_reply.started":"2021-07-02T10:28:09.328342Z","shell.execute_reply":"2021-07-02T10:28:09.376215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train and test set are now ready for model fitting and evaluation!","metadata":{}},{"cell_type":"markdown","source":"# **MODEL**\n\nNow comes the main part. As mentioned earlier, we have a dataset that contains randomness to a very high degree. Thus, we cannot pick a single model and expect it to work the best with this dataset. For this reason, I'm going to use *GridSearchCV* to enable parameter tuning over several Machine Learning models. Hopefully, by the end of this, we will have a model that works decently on this dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.378764Z","iopub.execute_input":"2021-07-02T10:28:09.379196Z","iopub.status.idle":"2021-07-02T10:28:09.522491Z","shell.execute_reply.started":"2021-07-02T10:28:09.37915Z","shell.execute_reply":"2021-07-02T10:28:09.521463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nxgb = XGBClassifier(eval_metric = 'logloss')\ndt = DecisionTreeClassifier()\nrfc = RandomForestClassifier()\nada = AdaBoostClassifier()\nknn = KNeighborsClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.525182Z","iopub.execute_input":"2021-07-02T10:28:09.525526Z","iopub.status.idle":"2021-07-02T10:28:09.533553Z","shell.execute_reply.started":"2021-07-02T10:28:09.525491Z","shell.execute_reply":"2021-07-02T10:28:09.532416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choosing the scoring metric for such a dataset can be tricky. Let's first emphasise on what we expect from our predictions:\n\n**1.** Given a number of water bodies, we don't want a large number of impotable bodies to be classified as potable, as it could lead to a widespread of diseases. In other words, we don't want a large number of False Positives. \n\n**2.** At the same time, we'd want to classify a fair number of potable water bodies as potable, because that is what we're mostly concerned about. \n\n**3.** But the given dataset is skewed towards 0 class, i.e. impotable water bodies have majority over potable. This could result in a model that classifies most of the water bodies as impotable. Such a model will, in fact, yield a high accuracy on test sets, but would have very little advantage in real-world applications.\n\n**Conclusion:** We are looking for a model having a high TP/FP ratio, in other words, high precision. This is because, practically speaking, the penalty for classifying an impotaple sample as potable should be high, and precision serves the purpose for such situations. However, with a dataset skewed in favor of negative class, high precision could result in low accuracy, or also a large FP.\nKeeping all this in mind, Matthews Correlation Coefficient seems suitable.","metadata":{}},{"cell_type":"markdown","source":"> The Matthews correlation coefficient (MCC) is a highly reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import make_scorer, precision_score, matthews_corrcoef, accuracy_score, confusion_matrix, recall_score, roc_auc_score, f1_score\nfrom scipy.stats import uniform, randint\n\nscorer = make_scorer(matthews_corrcoef)\n\nlr_param = {'C' : uniform(0.01,10),\n           'penalty' : ['l2'], 'class_weight' : ['balanced']}\nlr_rs = RandomizedSearchCV(lr, param_distributions = lr_param, scoring = scorer, cv = 5)\n\n\nxgb_param = {'n_estimators' : [50, 100, 200, 300, 400, 500],\n            'learning_rate' : uniform(0.03,1.0)}\nxgb_rs = RandomizedSearchCV(xgb, param_distributions = xgb_param, scoring = scorer, cv = 5)\n\ndt_param = {'criterion' : ['gini','entropy'], \n            'max_depth' : np.arange(1,50), \n            'min_samples_leaf' : [2, 3, 4, 5, 10, 20, 30, 40, 50]}\ndt_rs = RandomizedSearchCV(dt, param_distributions = dt_param, scoring = scorer, cv=5)\n\n\nrfc_param = {'n_estimators' : [50, 100, 200, 400, 500],\n            'max_depth' : np.arange(1,50),\n            'min_samples_leaf' : [2, 3, 4, 5, 10, 20, 30, 40, 50],\n            'criterion' : ['entropy', 'gini']}\nrfc_rs = RandomizedSearchCV(rfc, param_distributions = rfc_param, scoring = scorer, cv = 5)\n\nada_param = {'n_estimators' : [50, 100, 200, 400, 500],\n            'learning_rate' : uniform(0.01, 1.0)}\nada_rs = RandomizedSearchCV(ada, param_distributions = ada_param, scoring = scorer, cv = 5)\n\n\nknn_param = {'n_neighbors' : np.arange(1,50),\n            'weights' : ['uniform', 'distance']}\nknn_rs = RandomizedSearchCV(knn, param_distributions = knn_param, scoring = scorer, cv = 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.534661Z","iopub.execute_input":"2021-07-02T10:28:09.535042Z","iopub.status.idle":"2021-07-02T10:28:09.553996Z","shell.execute_reply.started":"2021-07-02T10:28:09.53501Z","shell.execute_reply":"2021-07-02T10:28:09.552893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_rs.fit(final_X_train, y_train)\nxgb_rs.fit(final_X_train, y_train)\ndt_rs.fit(final_X_train, y_train)\nrfc_rs.fit(final_X_train, y_train)\nada_rs.fit(final_X_train, y_train)\nknn_rs.fit(final_X_train, y_train)\n\nprint('Logistic Regression best parameters:', lr_rs.best_params_)\nprint('XGB best parameters:', xgb_rs.best_params_)\nprint('Decision Tree best parameters:', dt_rs.best_params_)\nprint('RFC best parameters:', rfc_rs.best_params_)\nprint('Ada Boost best parameters:', ada_rs.best_params_)\nprint('KNN best parameters:', knn_rs.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:28:09.555578Z","iopub.execute_input":"2021-07-02T10:28:09.556035Z","iopub.status.idle":"2021-07-02T10:32:41.115804Z","shell.execute_reply.started":"2021-07-02T10:28:09.555984Z","shell.execute_reply":"2021-07-02T10:32:41.114813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have the best set of parameters for each model.","metadata":{}},{"cell_type":"markdown","source":"Tuning these parameters on the repsective models - ","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(C = lr_rs.best_params_['C'], penalty = lr_rs.best_params_['penalty'], \n                        class_weight = lr_rs.best_params_['class_weight'], \n                        random_state = 42)\n\nxgb = XGBClassifier(n_estimators = xgb_rs.best_params_['n_estimators'], \n                    learning_rate = xgb_rs.best_params_['learning_rate'], \n                    random_state = 42, eval_metric = 'logloss')\n\ndt = DecisionTreeClassifier(min_samples_leaf = dt_rs.best_params_['min_samples_leaf'], \n                            max_depth = dt_rs.best_params_['max_depth'], \n                            criterion = dt_rs.best_params_['criterion'], \n                            random_state = 42)\n\nrfc = RandomForestClassifier(n_estimators = rfc_rs.best_params_['n_estimators'], \n                             criterion = rfc_rs.best_params_['criterion'], \n                             max_depth = rfc_rs.best_params_['max_depth'], \n                             min_samples_leaf = rfc_rs.best_params_['min_samples_leaf'], \n                             random_state = 42)\n\nada = AdaBoostClassifier(n_estimators = ada_rs.best_params_['n_estimators'], \n                         learning_rate = ada_rs.best_params_['learning_rate'], \n                         random_state = 42)\n\nknn = KNeighborsClassifier(n_neighbors=  knn_rs.best_params_['n_neighbors'], \n                           weights = knn_rs.best_params_['weights'])\n\nmodels = [(lr, 'Logistic Regression'), (xgb, 'XG Boost'), (dt, 'Decision Tree'), \n          (rfc, 'Random Forest'), (ada, 'Ada Boost'), (knn, 'K Neighbors')]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:35:44.834234Z","iopub.execute_input":"2021-07-02T10:35:44.834621Z","iopub.status.idle":"2021-07-02T10:35:44.845091Z","shell.execute_reply.started":"2021-07-02T10:35:44.83459Z","shell.execute_reply":"2021-07-02T10:35:44.843968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataframe to keep track of scores of various models\nevaluations = pd.DataFrame({'Model' : [], 'F1' : [], 'MCC' : [], 'Precision' : [],\n                            'Accuracy' : [], 'Recall' : [], 'AUC' : []})\n\n#function that evaluates and returns different scores obtained by a model\ndef evaluate(actual, preds):\n    f1 = f1_score(actual, preds, average = 'binary')\n    mcc = matthews_corrcoef(actual, preds)\n    precision  = precision_score(actual, preds)\n    accuracy = accuracy_score(actual, preds)\n    recall = recall_score(actual, preds)\n    auc = roc_auc_score(actual, preds)\n    confusion = confusion_matrix(actual, preds)\n    return (f1, mcc, precision, accuracy, recall, auc, confusion)\n\nfor model, model_name in models:\n    model.fit(final_X_train, y_train)\n    preds = model.predict(final_X_val)\n    f1, mcc, precision, accuracy, recall, auc, confusion = evaluate(y_val, preds)\n    cur_model = {'Model' : model_name, 'F1' : f1, 'MCC' : mcc, 'Precision' : precision,\n                 'Accuracy' : accuracy, 'Recall' : recall, 'AUC' : auc}\n    evaluations = evaluations.append(cur_model, ignore_index = True)\n    print(model_name, 'Confusion Matrix:')\n    print(confusion)\n    #print('Model: {} f1: {:.3f} accuracy: {:.3f}'.format(model_name, f1, accuracy))\n    \nevaluations.set_index('Model', inplace = True)\nevaluations","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:35:51.438007Z","iopub.execute_input":"2021-07-02T10:35:51.438435Z","iopub.status.idle":"2021-07-02T10:36:02.249275Z","shell.execute_reply.started":"2021-07-02T10:35:51.438401Z","shell.execute_reply":"2021-07-02T10:36:02.248298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1.** Except for Logistic regression, the MCC score for the others turned out to be okayish. \n\n**2.** Even though Decision Tree has a good F1 score comparatively, but it didn't account for the large number of False Positives.\n\n**3.** Ada Boost gave better than random results, but just not good enough.\n\nBased on these results, XG Boost Classifier, Random Forest Classifier, and K Nearest Neighbors Classifier have performed slightly better than the rest of the models. Both these models gave similar results, so both are suitable to pass as our final model.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nAlthough the stats are not very impressive (from a general perspective), but they came out to be better than expected for this particular dataset. \n\nFinal Model: **Random Forest Classifier, XGB Classifier, or K Neighbors Classifier**\n\nExpected test set MCC: **~0.20-0.25**\n\nExpected test set Accuracy: **~65-70%**","metadata":{}}]}