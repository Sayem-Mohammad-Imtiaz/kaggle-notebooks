{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Deep Learning: Computer Vision**\n\nThis project builds artificial intelligence algorithms to label satellite image chips with different atmospheric conditions and the different classes of land cover/land use. This is a multi-label classification problem and has labels from the following categories: cloud cover (clear, partly, cloudy, haze), primary rainforest, water (rivers, lakes), habitation (large city, small homes), agriculture, roads etc.\n\nThis project titled “Planet: Understanding the Amazon from Space” was completed on kaggle as it is considerably computationally expensive.\n\nThe algorithms will enable in understanding where, how and why deforestation happens in the Amazon Rainforests. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm    # To read in images in batches and see progress\nimport pathlib\nimport scipy\nimport subprocess\nimport gc   # Garbage collector module for memory management\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport cv2    #OpenCV for image manipulation\n\nfrom tensorflow import keras  #We need keras library\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import SGD\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator  #Used for Data augmentation\nfrom keras import backend as K   #For specialized and optimized tensor manipulation\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.model_selection import train_test_split  # For the creation of training and validation sets\n\n#from six import string_types\n#from IPython.display import display\n#from keras.preprocessing import image as image_utils\n#from keras import applications","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/planets-dataset/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory data analysis\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using DataFrame to check the shape of the training set, and their tags (labels that may be assigned) for each image\ntrain_df = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\ntrain_df.columns = [\"image_name\", \"tags\"]\ntrain_df\n\n# We can see that there are indeed 40,479 training images mapped to tags.\n# The second column of the sample.csv mapped each image to a tag of possible labels (separated by a space for each), that can be assigned to each image.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/planets-dataset/planet/planet/sample_submission.csv')\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAG SPLITTING: Creating a list of all known tags to be assigned to the images by looping through each row in \n# the “tags” column of the train set, splitting the tags by space, and storing them in a set\nlabel_list = []\nfor tag_str in train_df.tags.values:\n    labels = tag_str.split(' ')\n    for label in labels:\n        if label not in label_list:\n            label_list.append(label)\n\n            \n# Display label list and number of labels in the dataset\nprint(f'The number of data samples is {len(train_df)}. And there are {len(label_list)} unique possible classes.', '\\n' \n      f'The Label list includes {label_list}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dictionary to map tags to integer so we encode and use them for modeling\n# Assign a unique and consistent integer to each tag to be used to develop a target vector for each image with a One-hot encoding.\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in train_df['tags'].values])))\n\n\n# Creating a label map\nlabel_map = {l: i for i, l in enumerate(labels)}\n\nprint(f'label_map = {label_map},\\n length = {len(label_map)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a target vector by applying one hot encoding to the unique labels --- e.g [0 0 0 1 0 0 0 0 0 0] for \"bare_ground\" tag.\ntrain_tag_data = train_df.copy()\nfor label in label_list:\n    train_tag_data[label] = train_tag_data['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n\n# Display head\ntrain_tag_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Printing decreasing frequenciy of instances for each category\ncategory_counts = {}\n\nfor column in train_tag_data.columns[2:]:\n     category_counts[column] = train_tag_data[column].value_counts()[1]\n\nfor w in sorted(category_counts, key=category_counts.get, reverse=True):\n    print(category_counts[w] , w )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print all unique tags\nfrom itertools import chain\nlabels_list = list(chain.from_iterable([labels.split(\" \") for labels in train_tag_data['tags'].values]))\nlabels_set = set(labels_list)\nprint(\"There is {} unique labels including {}\".format(len(labels_set), labels_set))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Visualizing the dataset to compare frequency of occurence**, Using:\n+ bar plot\n+ coocurence matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a Histogram of label instances\ntag_labels = pd.Series(labels_list).value_counts() # To sort them by count\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x=tag_labels, y=tag_labels.index, orient='h')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for cooocurence matrix plotting\ndef make_cooccurence_matrix(labels):\n    numeric_data = train_tag_data[labels]; \n    c_matrix = numeric_data.T.dot(numeric_data)\n    sns.heatmap(c_matrix)\n    return c_matrix\n    \n# Compute the co-ocurrence matrix\nmake_cooccurence_matrix(label_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each image should have exactly one weather label:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot land-use element classes cooccurence matrix\nland_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation']\nmake_cooccurence_matrix(land_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading and visualizing one image in each category (or label) of the  training dataset using matplotlib\nimages = [train_df[train_df['tags'].str.contains(label)].iloc[i]['image_name'] + '.jpg' \n                for i, label in enumerate(labels_set)]\n\nplt.rc('axes', grid=False)\n_, axs = plt.subplots(5, 4, sharex='col', sharey='row', figsize=(15, 20))\naxs = axs.ravel()\n\n# /kaggle/input/planets-dataset/planet/planet/train_classes.csv\n# /kaggle/input/planets-dataset/planet/planet/train-jpg\n\nfor i, (image_name, label) in enumerate(zip(images, labels_set)):\n    img = mpimg.imread('../input/planets-dataset/planet/planet/train-jpg' + '/' + image_name)\n    axs[i].imshow(img)\n    axs[i].set_title('{} - {}'.format(image_name, label))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determining if the length of the train and test dataset csv file equals the actual number of images in the folder\n\n# Assign train and the two test dataset paths\n# train path\ntrain_img_dir = pathlib.Path('../input/planets-dataset/planet/planet/train-jpg')\ntrain_img_path = sorted(list(train_img_dir.glob('*.jpg')))\n\n\n# Let's read in the test image dataset and merge the test_additional jpg file to give an output of 61191 rows\n# test path\ntest_img_dir = pathlib.Path('../input/planets-dataset/planet/planet/test-jpg')\ntest_img_path = sorted(list(test_img_dir.glob('*.jpg')))\n\n# additional test path\ntest_add_img_dir = pathlib.Path('../input/planets-dataset/test-jpg-additional')\ntest_add_img_path = sorted(list(test_add_img_dir.glob('*/*.jpg')))\n\n# Length Confirmation\nassert len(train_img_path) == len(train_df)\nprint(len(test_img_path)+len(test_add_img_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## IMAGE PREPROCESSING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input size. Data Length Check (or checking smapple size)\ninput_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = []\ny_train = []\n\nfor f, tags in tqdm(train_df.values, miniters=1000):\n    img = cv2.imread('../input/planets-dataset/planet/planet/train-jpg/{}.jpg'.format(f))\n    img = cv2.resize(img, (input_size, input_size))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[label_map[t]] = 1\n    x_train.append(img)\n    y_train.append(targets)\n        \nx_train = np.array(x_train, np.float32)\ny_train = np.array(y_train, np.uint8)\n\nprint(x_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating x_test\nx_test = []\n\ntest_jpg_dir = '../input/planets-dataset/planet/planet/test-jpg'\ntest_image_names = os.listdir(test_jpg_dir)\n\nn_test = len(test_image_names)\ntest_classes = test_df.iloc[:n_test, :]\nadd_classes = test_df.iloc[n_test:, :]\n\n\ntest_jpg_add_dir = '../input/planets-dataset/test-jpg-additional/test-jpg-additional'\ntest_add_image_names = os.listdir(test_jpg_add_dir)\n\nfor img_name, _ in tqdm(test_classes.values, miniters=1000):\n    img = cv2.imread(test_jpg_dir + '/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n    \nfor img_name, _ in tqdm(add_classes.values, miniters=1000):\n    img = cv2.imread(test_jpg_add_dir + '/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n\nx_test = np.array(x_test, np.float32)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the train data into train and validation data sets\nX_train = x_train[ :35000]\nY_train = y_train[ :35000]\n\nX_valid = x_train[35000: ]\nY_valid = y_train[35000: ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the Model Architecture\n+ A combination of a custom deep CNN architecture is combined with a pre-trained CNN architecture(VGG16) will be implemented in Keras with Tensorflow backend"},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify sizes (batch and model input) and number of input channels\ninput_size = 64\ninput_channels = 3\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n# Input layer\nmodel.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n\n# CCM_1\nmodel.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n#CCM_2\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n \n#CCM_3\nmodel.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n \n#CCM_4\nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n\n# Create a feature vector from the CCM_4 final layer\nmodel.add(Flatten())\n\n# Fully Connected (FC) Layer\nmodel.add(Dense(512, activation='relu'))\nmodel .add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n# Output layer\nmodel.add(Dense(17, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras as keras\n# Loading the pre-trained VGG16 architecture module\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\n\n\n# Extract the pre - trained architecture\nbase_model = VGG16(input_shape =(input_size,input_size,3),include_top =False,weights ='imagenet')\nbase_model.summary()\n\n# Get the output of the base_model formed above\nx = base_model.output\n# Flatten to obtain a feature vector\nx = Flatten()(x)\n# Connect the feature vector to to the fully connected (FC) layer\nx = Dense (512 , activation ='relu')(x)\n# Form the output label predictions\npredictions = Dense (17 , activation ='sigmoid')(x)\nmodel = Model(inputs= base_model.input,outputs = predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implementing ImageDataGenerator for data augmentation. This is an important technique which reduces \n# overfitting as it generates extra images by flipping, cropping, zooming e,t.c the images. This makes \n# the model have more images to learn from.\n\ndatagen = ImageDataGenerator ( horizontal_flip =True ,\nvertical_flip =True ,\nzoom_range =0.2,\nrotation_range =90 ,\nfill_mode ='reflect')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining other parameters\nepochs=20 # An epoch is one complete pass through the training data, Here, epoch is set equals 20\n\noptimizer = keras.optimizers.Adam(learning_rate=0.0001) # Defining our Adam optimizer and learning rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the fbeta metric\ndef fbeta(y_true, y_pred, threshold_shift=0):\n    beta = 2\n \n    # just in case of hipster activation at the final layer\n    y_pred = K.clip(y_pred, 0, 1)\n \n    # shifting the prediction threshold from .5 if needed\n    y_pred_bin = K.round(y_pred + threshold_shift)\n \n    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n \n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n \n    beta_squared = beta ** 2\n    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=[fbeta])\n\n\ncallbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n                ModelCheckpoint(filepath='weights/best_weights',\n                                 save_best_only=True,\n                                 save_weights_only=True)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training: Fit the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The code below fits the model while generating extra images with the Imagedatagenerator, and then fit them. \nmodel.fit_generator(datagen.flow(X_train,\nY_train,\nbatch_size =24),\nsteps_per_epoch =len(X_train)/32 ,\nvalidation_data = datagen.flow ( X_valid,\nY_valid,\nbatch_size =24),\nvalidation_steps =len(X_valid)/32 ,\nepochs =epochs ,\ncallbacks = callbacks ,\nverbose =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction with the trained model using the test data\ntest_1 =[]\ntest_1.append (model.predict (x_test , batch_size = 128 , verbose =2) ) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After prediction, we compile the results in a pandas dataframe form\nresult = np.array (test_1[0])\nfor i in range (1,len(test_1) ):\n result += np. array (test_1)\nresult = pd.DataFrame (result,columns = labels )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nfor i in tqdm(range(result.shape[0]), miniters=1000):\n    a = result.loc[[i]]\n    a = a.apply(lambda x: x > 0.2, axis=1)\n    a = a.transpose()\n    a = a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The submission csv\ntest_df['tags'] = preds\ntest_df.to_csv('amazon_submission11.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}