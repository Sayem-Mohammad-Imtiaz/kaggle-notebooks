{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(10,10)})\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw = pd.read_csv(\"/kaggle/input/chilled-eda/chilled_water_cleaned.csv\", index_col = \"timestamp\", parse_dates = True)\nraw.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw = raw.resample(\"W\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"col = raw.columns\nprint(col[400:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dataframe for a single site\n\npeacock = pd.DataFrame()\nP = [col for col in raw.columns if 'Peacock' in col]\npeacock[P] = raw[P]\n\nmoose = pd.DataFrame()\nM = [col for col in raw.columns if 'Moose' in col]\nmoose[M] = raw[M]\n \nbull = pd.DataFrame()\nB = [col for col in raw.columns if 'Bull' in col]\nbull[B] = raw[B]\n\nhog = pd.DataFrame()\nH = [col for col in raw.columns if 'Hog' in col]\nhog[H] = raw[H]\n\neagle = pd.DataFrame()\nE = [col for col in raw.columns if 'Eagle' in col]\neagle[E] = raw[E]\n\ncockatoo = pd.DataFrame()\nC = [col for col in raw.columns if 'Cockatoo' in col]\ncockatoo[C] = raw[C]\n\npanther = pd.DataFrame()\npan = [col for col in raw.columns if 'Panther' in col]\npanther[pan] = raw[pan]\n\nfox = pd.DataFrame()\nf = [col for col in raw.columns if 'Fox' in col]\nfox[f] = raw[f]\n\nbobcat = pd.DataFrame()\nbob = [col for col in raw.columns if 'Bobcat' in col]\nbobcat[bob] = raw[bob]\n\ncrow = pd.DataFrame()\ncr = [col for col in raw.columns if 'Crow' in col]\ncrow[cr] = raw[cr]\n\nsites = [peacock, moose, bull, hog, eagle, cockatoo, panther, fox, bobcat, crow]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"testing some of the site df's","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"panther.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crow.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eagle.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bobcat.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"panther.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"eagle.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#summing the total chilled water consumption per week\nfor site in sites:\n    site[\"Chilled_sum\"] = site.sum(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#checking sum correct\neagle.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bull.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import SCORERS\n\nimport datetime as dt\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting df for a site into train and test sets\nmodel_panther = panther.copy()\n\ntrain = model_panther.iloc[0:(len(model_panther)-30)]\ntest = model_panther.iloc[len(train):(len(model_panther)-1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking if stationary or not \nfrom statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(panther[\"Chilled_sum\"])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P-value bigger than .05 threshold, so data is non-stationary. It has some time dependent structure.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"so do we differentiate so p< .05?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"endog = train[\"Chilled_sum\"]\n\nmod = sm.tsa.statespace.SARIMAX(endog=endog, simple_differencing = False)\nmodel_fit = mod.fit()\nmodel_fit.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Chilled_sum'].plot(figsize=(25,10))\nmodel_fit.fittedvalues.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model_fit.predict(start = len(train),end = len(train)+len(test)-1)\ntest['predicted'] = predict.values\ntest.tail(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['predicted'].plot(color = 'red')\ntest[\"Chilled_sum\"].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\nfrom multiprocessing import cpu_count\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom warnings import catch_warnings\nfrom warnings import filterwarnings\nfrom sklearn.metrics import mean_squared_error\n\n \n# one-step sarima forecast\ndef sarima_forecast(history, config):\n    order, sorder, trend = config\n    # define model\n    model = SARIMAX(history, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False, enforce_invertibility=False)\n    # fit model\n    model_fit = model.fit(disp=False)\n    # make one step forecast\n    yhat = model_fit.predict(len(history), len(history))\n    return yhat[0]\n \n# root mean squared error or rmse\ndef measure_rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n \n# split a univariate dataset into train/test sets\ndef train_test_split(data, n_test):\n    return data[:-n_test], data[-n_test:]\n \n# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test, cfg):\n    predictions = list()\n    # split dataset\n    train, test = train_test_split(data, n_test)\n    # seed history with training dataset\n    history = [x for x in train]\n    # step over each time-step in the test set\n    for i in range(len(test)):\n        # fit model and make forecast for history\n        yhat = sarima_forecast(history, cfg)\n        # store forecast in list of predictions\n        predictions.append(yhat)\n        # add actual observation to history for the next loop\n        history.append(test[i])\n    # estimate prediction error\n    error = measure_rmse(test, predictions)\n    return error\n \n# score a model, return None on failure\ndef score_model(data, n_test, cfg, debug=False):\n    result = None\n    # convert config to a key\n    key = str(cfg)\n    # show all warnings and fail on exception if debugging\n    if debug:\n        result = walk_forward_validation(data, n_test, cfg)\n    else:\n        # one failure during model validation suggests an unstable config\n        try:\n            # never show warnings when grid searching, too noisy\n            with catch_warnings():\n                filterwarnings(\"ignore\")\n                result = walk_forward_validation(data, n_test, cfg)\n        except:\n            error = None\n    # check for an interesting result\n    if result is not None:\n        print(' > Model[%s] %.3f' % (key, result))\n    return (key, result)\n \n# grid search configs\ndef grid_search(data, cfg_list, n_test, parallel=True):\n    scores = None\n    if parallel:\n        # execute configs in parallel\n        executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n        tasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)\n        scores = executor(tasks)\n    else:\n        scores = [score_model(data, n_test, cfg) for cfg in cfg_list]\n    # remove empty results\n    scores = [r for r in scores if r[1] != None]\n    # sort configs by error, asc\n    scores.sort(key=lambda tup: tup[1])\n    return scores\n \n# create a set of sarima configs to try\ndef sarima_configs(seasonal=[0]):\n    models = list()\n    # define config lists\n    p_params = [0, 1, 2]\n    d_params = [0, 1]\n    q_params = [0, 1, 2]\n    t_params = ['n','c','t','ct']\n    P_params = [0, 1, 2]\n    D_params = [0, 1]\n    Q_params = [0, 1, 2]\n    m_params = seasonal\n    # create config instances\n    for p in p_params:\n        for d in d_params:\n            for q in q_params:\n                for t in t_params:\n                    for P in P_params:\n                        for D in D_params:\n                            for Q in Q_params:\n                                for m in m_params:\n                                    cfg = [(p,d,q), (P,D,Q,m), t]\n                                    models.append(cfg)\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = panther.values\n\n# data split\nn_test = 4\n# model configs\ncfg_list = sarima_configs(seasonal=[0, 4])\n# grid search\nscores = grid_search(data, cfg_list, n_test)\nprint('done')\n# list top 3 configs\nfor cfg, error in scores[:3]:\n    print(cfg, error)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}