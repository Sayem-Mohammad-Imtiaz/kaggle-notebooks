{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the data and first info"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cars = pd.read_csv('/kaggle/input/craigslist-carstrucks-data/vehicles.csv')\nprint('Columns:',cars.columns.tolist())\ncars.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"## Removing unnecessary data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.drop(columns=['url', 'id', 'size', 'county', 'region_url', 'image_url', 'vin', 'description', 'state', 'lat', 'long', 'region', 'title_status'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars['year'].fillna(cars.year.median(), inplace=True)\ncars['year']= cars.year.astype('int32')\ncars['odometer'].fillna(cars.odometer.median(), inplace=True)\ncars['paint_color'].fillna('Unknown', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop offers before 1960"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars = cars[cars['year']>=1960]\ncars.year.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sales per year"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(30,15))\nsns.countplot(x='year', data=cars)\nplt.xticks(rotation=90)\nplt.xlabel('Year')\nplt.ylabel('Number of offers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2016 was the year with the most offers."},{"metadata":{},"cell_type":"markdown","source":"## Sales per manufacturer"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nsns.countplot(y='manufacturer', data=cars, order=cars['manufacturer'].value_counts().index)\nplt.xlabel('Manufacturer')\nplt.ylabel('Number of offers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ford is the manufacturer with the most offers, followed by Chevrolet and Toyota."},{"metadata":{},"cell_type":"markdown","source":"## Evolution of type with year"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nsns.boxplot(x='year', y='type', data=cars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evolution of paint color with years"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.paint_color.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(x='paint_color', order=cars.paint_color.value_counts().index, data=cars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can explore the evolution of car's color with year:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.groupby('year').paint_color.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can create a pivot table with the paint_color and year columns, aggregated with the count:"},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_cars_year_color=cars[['paint_color', 'year']]\ntable2=pd.pivot_table(reduced_cars_year_color, values='paint_color',index='year', columns='paint_color', aggfunc=len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmap using seaborn for the year and paint_color:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(table2, annot=True, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the car's condition"},{"metadata":{},"cell_type":"markdown","source":"The list of the different conditions for the cars dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars['condition'].fillna('Unknown', inplace=True)\ncars.condition.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the list cars manufacturers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars['manufacturer'].fillna('Unknown', inplace=True)\ncars.manufacturer.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot an histogram for the car's condition:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(x='condition', order=cars.condition.value_counts().index, data=cars)\nplt.xlabel('Condition')\nplt.ylabel('Number of cars')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a pivot table with the condition and manufacturer columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_cars=cars[['condition', 'manufacturer']]\ntable=pd.pivot_table(reduced_cars, values='condition',index='manufacturer', columns='condition', aggfunc=len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using seaborn's heatmap function, create a heatmap for the condition and manufacturer features with the annotated count in each box:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(table, annot=True, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML"},{"metadata":{},"cell_type":"markdown","source":"## Preparing the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop rows with a NaN value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.dropna(inplace=True)\ncars.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we encode target columns ( year, drive, odometer, manufacturer, model, condition, cylinders, fuel, type, paint_color, transmission) using OrdinalEncoder, to be able to feed the ML algorithms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\ncategorical_columns=['year', 'drive', 'odometer', 'manufacturer', 'model', 'condition', 'cylinders', 'fuel', 'type', 'paint_color', 'transmission']\ncars[categorical_columns] = ordinal_encoder.fit_transform(cars[categorical_columns])\ncars.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and look for the standard correlation coefficient (Pearsons r) for every pair of attributes, and especifically for the 'price' column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = cars.corr()\ncorr_matrix['price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to create the train and test sets. We first select the y ('price') and the X(the categorical encoded columns) sets, and create a target and label sets for both the train and test process, using train_test_split from sklearn:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ncars_y = cars['price']\ncars_X = cars[categorical_columns]\ncars_X_train, cars_X_test, cars_y_train, cars_y_test = train_test_split(cars_X, cars_y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the input numerical features (categorical encoded columns) have very different scales, we use StandardScaler to get all attributes to have the same scale. Standardization works by substracting the mean value (so standardizated values always have a zero mean) and then dividing by the standard deviation, so the resulting distribution has a unit variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncars_X_train = pd.DataFrame(scaler.fit_transform(cars_X_train), columns = cars_X_train.columns)\ncars_X_test = pd.DataFrame(scaler.fit_transform(cars_X_test), columns = cars_X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cars_X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression model"},{"metadata":{},"cell_type":"markdown","source":"We will start by training a Linear Regression model, feeding it with the train set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlin_reg=LinearRegression()\nlin_reg.fit(cars_X_train, cars_y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the model is trained with the train set, let's predict the labels of the X_train set, compute the RMSE of the model and display it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = lin_reg.predict(cars_X_train)\nfrom sklearn.metrics import mean_squared_error\nlin_mse=mean_squared_error(cars_y_train, predictions)\nlin_rmse=np.sqrt(lin_mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decission Tree Regressor model"},{"metadata":{},"cell_type":"markdown","source":"Now we try with a Decission Tree Regressor model, which is powerful model capable of finding complex nonlinear relationships in the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(cars_X_train, cars_y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"again, we predict the labels values from the X_train dataset and compute the RMSE:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_predictions = tree_reg.predict(cars_X_train)\nfrom sklearn.metrics import mean_squared_error\ntree_mse=mean_squared_error(cars_y_train, tree_predictions)\ntree_rmse=np.sqrt(tree_mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-Validation with Decission Tree Regressor model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, cars_X_train, cars_y_train, scoring='neg_mean_squared_error', cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_score(scores):\n    print('Scores:', scores)\n    print('Mean:', scores.mean())\n    print('Standard deviation:', scores.std())\ndisplay_score(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-Validation with Linear Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(lin_reg, cars_X_train, cars_y_train, scoring='neg_mean_squared_error', cv=30)\nlin_rmse_scores = np.sqrt(-scores)\ndisplay_score(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(cars_X_train, cars_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_predictions = forest_reg.predict(cars_X_train)\nforest_mse=mean_squared_error(cars_y_train, forest_predictions)\nforest_rmse=np.sqrt(forest_mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing the RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Computed RMSE's for the different models:\")\nprint('Linear Regression Model:', lin_rmse)\nprint('Decision Tree Regressor Model:', tree_rmse)\nprint('Random Forest Regressor Model:', forest_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning the Decission Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid= [\n    {'max_depth': [2,4,6,8,10], 'max_features': [2,3,4]}\n]\ntree_reg = DecisionTreeRegressor()\ngrid_search = GridSearchCV(tree_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(cars_X_train, cars_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_predictions = tree_reg.predict(cars_X_train)\nfrom sklearn.metrics import mean_squared_error\ntree_mse=mean_squared_error(cars_y_train, tree_predictions)\ntree_rmse=np.sqrt(tree_mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This kernel is still WIP. Any comment aimed to improve it will be very helpful."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}