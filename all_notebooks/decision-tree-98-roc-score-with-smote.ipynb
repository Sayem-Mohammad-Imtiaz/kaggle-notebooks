{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/company-bankruptcy-prediction/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Oversampling Method\n[Oversampling](https://imbalanced-learn.org/stable/over_sampling.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\ny = data[\"Bankrupt?\"]\nX = data.drop(columns=[\"Bankrupt?\"])\n\nover_sample=SMOTE()\nX_ros, y_ros=over_sample.fit_resample(X,y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.3)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef KNC(X_train, X_test, y_train, y_test, n_neighbors=2):\n    KNC = KNeighborsClassifier(n_neighbors=n_neighbors)\n    KNC.fit(X_train, y_train)\n    \n    y_prediction = KNC.predict(X_test)\n    \n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction = KNC(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\n\ndef SVClassifier(X_train, X_test, y_train, y_test):\n    svc = SVC()\n    svc.fit(X_train, y_train)\n    \n    y_prediction = svc.predict(X_test)\n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction = SVClassifier(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ndef Logistic(X_train, X_test, y_train, y_test):\n    LR = LogisticRegression()\n    LR.fit(X_train, y_train)\n    \n    y_prediction = LR.predict(X_test)\n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction = Logistic(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n\ndef RandomForest(X_train, X_test, y_train, y_test):\n    RFC = RandomForestClassifier()\n    RFC.fit(X_train, y_train)\n    \n    y_prediction = RFC.predict(X_test)\n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    \n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_prediction = RandomForest(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}