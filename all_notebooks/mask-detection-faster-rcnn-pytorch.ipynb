{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"ha_p89EWkLLu"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport gc\nimport os\nimport re\nfrom tqdm import tqdm\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensor\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"tXlm06_ZkLL9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"bQlGpV1JkLMJ","outputId":"d6e5b84a-78de-49ae-e611-0a6c09796974"},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/face-mask-detection-dataset/train.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"asMS54klkLMU","outputId":"9e0d98f2-a011-4c0d-b3e2-104588cc3dc6"},"cell_type":"code","source":"len(df.classname.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"DC2_3gNVkLMb","outputId":"6cdd73df-bdef-4369-9267-5e2f0e798472"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"FdtZ6845kLMm","outputId":"1c482e49-24eb-45b8-df77-0d8ca39bf8d1"},"cell_type":"code","source":"### Headers Of CSV File Has Error, So Correcting it\n\ndf.rename(columns = {'x2' : 'y1', 'y1' : 'x2'}, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"v39xVV6xm7rB","outputId":"877646d4-8bad-45ab-dc30-97778ff42d28","trusted":true},"cell_type":"code","source":"## Converting The Classname to Respective Integer Label using Sklearn's LabelEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nle.fit(df['classname'])\ndf['classname']=(le.transform(df['classname'])+1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"CByRaU2dJO3I","outputId":"d9c21d20-4792-4587-aa97-b4e2020f773b","trusted":true},"cell_type":"code","source":"## Checking Whether There Is Any NAN or Missing Values In The DataFrame\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_DnOgpl7kLM6"},"cell_type":"code","source":"## Splitting The Dataset\n### For Final Training Purpose, Use Whole Dataset For Training Rather Than Splitting It Into Valid Set\n\nimage_ids = df['name'].unique()\nimage_ids.sort()\nvalid_ids=image_ids[:0] \ntrain_ids=image_ids[:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"kUVkYPVckLNH"},"cell_type":"code","source":"valid_df = df[df['name'].isin(valid_ids)]\ntrain_df = df[df['name'].isin(train_ids)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"XhVR6-uEkLNO","outputId":"ed0074f7-b308-4ed2-e1e6-6216a19d58ad"},"cell_type":"code","source":"valid_df.shape, train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"GmSnehHknoB4","outputId":"4b0f4516-86fe-49ad-ab83-16fa28739151","trusted":true},"cell_type":"code","source":"df.classname.unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"Pz3afKAIpFUV","trusted":true},"cell_type":"code","source":"class Maskdataset(Dataset):\n\n    def __init__(self, dataframe, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['name'].unique()\n        self.df = dataframe\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['name'] == image_id]\n\n        image = cv2.imread('../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/'+f'{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n\n        boxes = torch.as_tensor(records[['x1', 'y1', 'x2', 'y2']].values, dtype=torch.float32)\n        \n        \n\n        # there are 21 classes\n        labels = torch.as_tensor(records.classname.values,dtype=torch.int64)\n        \n\n        keep = (boxes[:, 3]>boxes[:, 1]) & (boxes[:, 2]>boxes[:, 0]) ## To Handle NAN LOSS Cases \n        boxes = boxes[keep]\n        labels = labels[keep]\n\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        # target['area'] = area\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kaBueVB3oe3s","trusted":true},"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        ToTensor()\n    ])\n        \n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensor()\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## UTILS And Model ","execution_count":null},{"metadata":{"id":"L73lAhXi7uw4","trusted":true},"cell_type":"code","source":"## USING SWISH Activation Rather Than The Regular ReLU\n\nimport torch.nn as nn\n\nclass Swish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n    \n    \ndef convert_relu_to_swish(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, nn.ReLU):\n            setattr(model, child_name, Swish())\n        else:\n            convert_relu_to_swish(child)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"YtoKoR28kLOA"},"cell_type":"code","source":"## Using Pytorch Faster-RCNN Resnt50 Pretrained Model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2HDzJypRkLOF","outputId":"63d011ac-095f-4b64-bf66-2a8af140654d"},"cell_type":"code","source":"num_classes = 21  # 20 class (masks) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nconvert_relu_to_swish(model) # converting ReLU to SWISH Activation\n\n## Loading The Trained Model Weights\nmodel.load_state_dict(torch.load('../input/face-mask-detection-weights/model-epoch7.pth'))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5_VEucHL6xPQ","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ZIS7zotWkLOK"},"cell_type":"code","source":"## To Count The Loss During Training\n\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"t0_qaMltkLON"},"cell_type":"code","source":"## Defining DataLoaders\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n\n\ntrain_dataset = Maskdataset(train_df,get_train_transform())\n# valid_dataset = Maskdataset(valid_df, get_valid_transform()) ## No need for Final Training Purpose\n\n\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\n# valid_data_loader = DataLoader(\n#     valid_dataset,\n#     batch_size=8,\n#     shuffle=False,\n#     num_workers=2,\n#     collate_fn=collate_fn\n# )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"qrcUS-dKkLOX"},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets, Check Some Images","execution_count":null},{"metadata":{"trusted":true,"id":"zimmH2d7kLOb"},"cell_type":"code","source":"images, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"kg20pLU8kLOg"},"cell_type":"code","source":"boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[2].permute(1,2,0).cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"4jc0XvVLkLOl","outputId":"919641f3-8809-4b8a-fc79-bcd7eddb6a42"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\nax.imshow(sample)\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Over9000 Optimizer","execution_count":null},{"metadata":{"id":"0FyG0MCBrKnv","trusted":true},"cell_type":"code","source":"from torch.optim.optimizer import Optimizer\nclass Ralamb(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(Ralamb, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Ralamb, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Ralamb does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n\n                if state['step'] == buffered[0]:\n                    N_sma, radam_step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n                    buffered[2] = radam_step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                radam_step = p_data_fp32.clone()\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n                else:\n                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n\n                radam_norm = radam_step.pow(2).sum().sqrt()\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n                if weight_norm == 0 or radam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / radam_norm\n\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = radam_norm\n                state['trust_ratio'] = trust_ratio\n\n                if N_sma >= 5:\n                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\n\"\"\" Lookahead Optimizer Wrapper.\nImplementation modified from: https://github.com/alphadl/lookahead.pytorch\nPaper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n\"\"\"\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom collections import defaultdict\nimport math\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        # manually add our defaults to the param groups\n        for name, default in defaults.items():\n            for group in self.param_groups:\n                group.setdefault(name, default)\n\n    def update_slow(self, group):\n        for fast_p in group[\"params\"]:\n            if fast_p.grad is None:\n                continue\n            param_state = self.state[fast_p]\n            if 'slow_buffer' not in param_state:\n                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n                param_state['slow_buffer'].copy_(fast_p.data)\n            slow = param_state['slow_buffer']\n            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n            fast_p.data.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self.param_groups:\n            self.update_slow(group)\n\n    def step(self, closure=None):\n        # print(self.k)\n        # assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n        loss = self.base_optimizer.step(closure)\n        for group in self.param_groups:\n            group['lookahead_step'] += 1\n            if group['lookahead_step'] % group['lookahead_k'] == 0:\n                self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.base_optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict['state']\n        param_groups = fast_state_dict['param_groups']\n        return {\n            'state': fast_state,\n            'slow_state': slow_state,\n            'param_groups': param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        fast_state_dict = {\n            'state': state_dict['state'],\n            'param_groups': state_dict['param_groups'],\n        }\n        self.base_optimizer.load_state_dict(fast_state_dict)\n\n        # We want to restore the slow state, but share param_groups reference\n        # with base_optimizer. This is a bit redundant but least code\n        slow_state_new = False\n        if 'slow_state' not in state_dict:\n            print('Loading state_dict from optimizer without Lookahead applied.')\n            state_dict['slow_state'] = defaultdict(dict)\n            slow_state_new = True\n        slow_state_dict = {\n            'state': state_dict['slow_state'],\n            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n        if slow_state_new:\n            # reapply defaults to catch missing lookahead specific ones\n            for name, default in self.defaults.items():\n                for group in self.param_groups:\n                    group.setdefault(name, default)\n\n\ndef LookaheadAdam(params, alpha=0.5, k=6, *args, **kwargs):\n    adam = Adam(params, *args, **kwargs)\n    return Lookahead(adam, alpha, k)\n\ndef Over9000(params, alpha=0.5, k=6, *args, **kwargs):\n    ralamb = Ralamb(params, *args, **kwargs)\n    return Lookahead(ralamb, alpha, k)\n\n\nRangerLars = Over9000\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"777Yz_CFkLOq"},"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer=Over9000(params,lr=0.0001) ##Over9000 Optimizer (LARS + LookAhead + Ralamb)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.00017, div_factor=2 ,steps_per_epoch=len(train_data_loader), epochs=5)\n\nnum_epochs = 25","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TRAINING","execution_count":null},{"metadata":{"trusted":true,"id":"i4Rol_g6kLOu","outputId":"ae2097d1-feaa-4264-c6ef-ddbe41c53840"},"cell_type":"code","source":"##COMMENTED FOR SUBMISSION PURPOSE\n\n# import gc\n# loss_hist = Averager()\n\n\n# for epoch in range(num_epochs):\n    \n#     z=tqdm(train_data_loader)\n\n#     loss_hist.reset()\n\n#     for itr,(images, targets, image_ids) in enumerate(z):\n#         torch.cuda.empty_cache()\n#         gc.collect()\n        \n#         images = list(image.to(device).float() for image in images)\n#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n#         loss_dict = model(images, targets)\n\n#         losses = sum(loss for loss in loss_dict.values())\n#         loss_value = losses.item()\n\n#         loss_hist.send(loss_value)\n#         z.set_description(f'Epoch {epoch+1}/{num_epochs}, LR: %6f, Loss: %.6f'%(optimizer.state_dict()['param_groups'][0]['lr'],loss_value))\n#         optimizer.zero_grad()\n#         losses.backward()\n#         optimizer.step()\n#         scheduler.step() ## Since We are using 1-Cycle LR Policy, LR update step has to be taken after every batch\n\n\n#     print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\")\n#     torch.save(model.state_dict(), f'/content/drive/My Drive/internshala round 1/model-epoch{epoch+1}.pth') \n#     print()\n#     print('Saving Model.......')\n#     # print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"AhFBYyFwkLO0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"GfpwVseGkLO5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"RdWBFH9J07tY"},"cell_type":"markdown","source":"# Inference","execution_count":null},{"metadata":{"id":"3nrgW1990-nd","trusted":true},"cell_type":"code","source":"class MaskTestDataset(Dataset):\n\n    def __init__(self, dataframe, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['name'].unique()\n        self.df = dataframe\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['name'] == image_id]\n\n        image = cv2.imread('../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/'+f'{image_id}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n\n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"k8Bn2SgO1A3l","trusted":true},"cell_type":"code","source":"def get_test_transform():\n    return A.Compose([\n        ToTensor()\n    ])","execution_count":null,"outputs":[]},{"metadata":{"id":"lGiGQrpd1C70","outputId":"bafe0c46-b5db-418a-834a-662d5897b06f","trusted":true},"cell_type":"code","source":"test_df=pd.read_csv('../input/face-mask-detection-dataset/submission.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"u0anvqyg1GIC","trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = MaskTestDataset(test_df, get_test_transform())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"fNNy-YPo1I_H","outputId":"50095bf8-b1ed-492b-9fa6-2a25adb90407","trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"EC4h5RaC1LOG","outputId":"51d3608f-d998-4d7e-fa9e-3f7abe63beb9","trusted":true},"cell_type":"code","source":"%%time\n\ndetection_threshold = 0.60\nresults = []\nmodel.eval()\nfor images, image_ids in test_data_loader:\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        labels = outputs[i]['labels'].data.cpu().numpy()\n\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        \n        result = {\n            'image_id': image_id,\n            'labels': labels,\n            'scores': scores,\n            'boxes': boxes\n        }\n\n        \n        results.append(result)","execution_count":null,"outputs":[]},{"metadata":{"id":"jxnFRdot1cYa","trusted":true},"cell_type":"code","source":"## Using Dictionary is Fastest Way to Create SUBMISSION DATASET.\nnew=pd.DataFrame(columns=['image_id', 'boxes', 'label'])\nrows=[]\nfor j in range(len(results)):\n    for i in range(len(results[j]['boxes'])):\n        dict1 = {}\n        dict1={\"image_id\" : results[j]['image_id'],\n                  'x1': results[j]['boxes'][i,0],\n                  'x2': results[j]['boxes'][i,2],\n                  'y1': results[j]['boxes'][i,1],\n                  'y2': results[j]['boxes'][i,3],\n                  'classname':results[j]['labels'][i].item()}\n        rows.append(dict1)\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"adqXTKI72xqG","trusted":true},"cell_type":"code","source":"sub=pd.DataFrame(rows)\nsub['classname']=le.inverse_transform(sub.classname.values - 1) ## Converting Back Labels To Original Names ","execution_count":null,"outputs":[]},{"metadata":{"id":"oOUvrt693OE8","outputId":"f87064bb-637d-4d88-8b5f-fd1fe810f7bf","trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot Some Results Of The SUBMISSION","execution_count":null},{"metadata":{"id":"Q6qyU8Q720Of","trusted":true},"cell_type":"code","source":"sample = images[1].permute(1,2,0).cpu().numpy()\nboxes = outputs[1]['boxes'].data.cpu().numpy()\nscores = outputs[1]['scores'].data.cpu().numpy()\nboxes = boxes[scores >= 0.6].astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"id":"FKarjgF325Df","outputId":"7a2d681a-5583-4f85-d027-6bf0347a3d5c","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finally Create the SUBMISSION File","execution_count":null},{"metadata":{"id":"pKzXrSOM28Rn","trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}