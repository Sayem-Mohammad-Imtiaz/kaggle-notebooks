{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom google.cloud import bigquery","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.) List and explain the steps in a typical data science project workflow. "},{"metadata":{},"cell_type":"markdown","source":"1. Meet with your employer to understand what the questions you are solving will be.\n2. Get or collect the data that will answer the above questions.\n3. Look at the data to try to find any anomolies and either adjust or toss the bad data out.\n4. Create a writup intended for your employer that states what data you are using as well as what questions you will answer\n5. Meet with your employer to discuss the data and reaffirm what the purpose of the project is.\n6. Fully clean the data so that it can be used for model fitting.\n7. Perform a descriptive analysis on the data. This includes things like mean, median and mode for data columns.\n8. Determine if the problem requires machine learning techniques or if the problem requires simple visualisation using plots.\n9. if the problem can be solved through simple plotting you would create the plots and create a story or analysis to backup why these plots solve the  problem.\n10. If the problem requires machine learning techniques to solve decide which ML techniques would be appropriate to use on what portions of the data.\n11. Split data into train and test sets. Randomly split data\n12. See if the method solved the target problem. If not go back and try a different model.\n13. Present to your employer.\n14. If your employer is satisfied your job is done.\n15. If your employer is not satisfied you may need to collect more data or try using different models to answer the desired questions."},{"metadata":{},"cell_type":"markdown","source":"# 2.) List at least three common problems that a data scientist might encounter when working with an employer and potential work-arounds for these problems. "},{"metadata":{},"cell_type":"markdown","source":"1. Often times the lingo that a business executive uses is very different from the lingo that a data scientist might use. This leads to data scientists misunderstanding what an employer may want out of a project. This problem is best solved by having many \"check in\" points during a project. These check in points would involve the data scientist telling their employeer what they are doing and why they are doing it. \n2. Sometimes in projects a company will want you to provide insight using data but you either will not have enough data or the data will not provide the insight the employer wants. The best way to avoid this problem is to determine if you have enough and the right data early on in the process.\n3. Often times companies will use data that the data scientist determines to be unethical. For example, a college may want a professor to perform an analysis on how students should be placed in math courses. Some of the predictors in a dataset for something like this could contain gender and race. It seems like there could be an ethical problem with placing a student in a math class based off of either of those predictors. If this happens it is best for the data scientist to say early on that they will not be using the data. It is also important that the data scientist comes prepared to defend their viewpoints. "},{"metadata":{},"cell_type":"markdown","source":"# 3.) What is the major difference between a database and a flat file and why are databases more useful in many situations?"},{"metadata":{},"cell_type":"markdown","source":"The major difference between a database and a flat file is that flat files contain one table of data whereas databases can have many tables.\n\n"},{"metadata":{},"cell_type":"markdown","source":"Databases excel at keeping company data organized. There are lots of types of data that can't be represented well in just one table. For example, if a company wanted to track both who was buying a product as well as detailed descriptions on what the products are it would be very difficult to have a well organized table with all that data. You'd probobly want a table that contained product data, a table that contained customer data and a table that contained product sale data. Each of these tables would then have primary and secondary keys linking them together. "},{"metadata":{},"cell_type":"markdown","source":"# 4). a). Start by reading in the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"client = bigquery.Client()\n# Refernece to san fran dataset\ndataset_ref = client.dataset(\"san_francisco\", project = 'bigquery-public-data')\n# Stores dataset into dataset\ndataset = client.get_dataset(dataset_ref)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# b.) Produce a list of the tables within this dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stores dataset tables into tables\ntables = list(client.list_tables(dataset))\n# for loop listing table names\nfor table in tables:\n    print(table.table_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# c.) Determine the size of data required to import the film_locations table without actually loading it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all data in film_locations\nquery1 = \"\"\"\n        SELECT *\n        from bigquery-public-data.san_francisco.film_locations\n\n\"\"\"\ndry_run_config = bigquery.QueryJobConfig(dry_run = True)\ndry_run_query_job = client.query(query1, job_config = dry_run_config)\n\n\nprint(\"This query will process {} bytes.\" .format(dry_run_query_job.total_bytes_processed))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# d.) Read this table and produce a data frame that counts the number of movies produced by each production company from before 1950 and listed from most movies to least. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#  READING THE TABLE\n# Create reference to film_locations\ntable_ref_film = dataset_ref.table('film_locations')\n# Create dataset for film_locations\ndfFilm = client.get_table(table_ref_film)\n\nclient.list_rows(dfFilm, max_results = 6).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  PRODUCING THE DATAFRAME\nquery2 = \"\"\"\n        SELECT \n            production_company,\n            COUNT(production_company) as num_movies\n        from bigquery-public-data.san_francisco.film_locations\n        WHERE release_year < 1950\n        GROUP BY production_company\n        ORDER BY num_movies desc\n\n\"\"\"\n# put a cap on byte limit\nOneHundMB = 1000*1000*100  # one hundred megabytes\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed = OneHundMB)\n# view the query\ntreeCensus2015 = client.query(query2, job_config = safe_config)\ntreeCensus2015.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# e.) From the table sfpd_incidents, produce a table of the most common police incidents in the category of LARCENY/THEFT in the year 2016 ordered from most to least common. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#  READING THE TABLE\n# Create reference to film_locations\ntable_ref_film = dataset_ref.table('sfpd_incidents')\n# Create dataset for film_locations\ndfIncidents = client.get_table(table_ref_film)\n\nclient.list_rows(dfIncidents, max_results = 6).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  PRODUCING THE DATAFRAME\nquery2 = \"\"\"\n        SELECT \n            descript,\n            COUNT(descript) as num_occurances\n        FROM bigquery-public-data.san_francisco.sfpd_incidents\n        WHERE EXTRACT(YEAR FROM timestamp) = 2016\n        AND category = \"LARCENY/THEFT\"\n        GROUP BY descript\n        ORDER BY num_occurances desc\n\n\"\"\"\n\n# put a cap on byte limit\nOneThousandMB = 1000*1000*1000  # one thousand megabytes\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed = OneThousandMB)\n# view the query\ntreeCensus2015 = client.query(query2, job_config = safe_config)\ntreeCensus2015.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 5). a.) Using the Kaggle dataset on Melborne_housing_FULL.csv, start by reading and describing this dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/melbourne-houses/Melbourne_housing_FULL.csv')\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See number of rows and columns\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# b.) Using the iloc method, produce the last 10 rows of the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[-10:,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# c.) There are columns with missing data in this dataset. Clean the dataset by removing all rows with missing values. (This is a BAD idea, but it is where we start). "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# d.) Using python code, produce rows where the Suburb is in Albert Park and the house has 3 or more bedrooms and sold in 2017. Print the 5 observations with lowest price where the price is not null. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# This data set does not describe what each collumn is \n# so I decided to interperate the column \n# Bedroom2 as the number of bedrooms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the date datatype\ndf.Date.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Date column is not a time stamp data type so I need to split date into month, day and year\n\ndf[['Month','Day','Year']] = df['Date'].str.split('/',expand=True)\ndf.head()\n# Month, Day, Year is at the end of the ouput","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new dataframe that only has the Albert Park Suburb\ndf2 = df[df['Suburb'] == 'Albert Park']\n# Take the new df and create another df that looks at houses that have 3 or more bedrooms\ndf3 = df2.loc[df2['Bedroom2'] >= 3]\n# Do the same as above but set Year = 2017\ndf4 = df3[df3['Year'] == '2017']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort by price\ndf4 = df4.sort_values('Price', ascending = True)\ndf4.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# e.) Produce a list of all the unique Sellers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SellerG.unique()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}