{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the third part of a series of notebooks about practical time series methods:\n* [Part 1: the basics](https://www.kaggle.com/konradb/practical-time-series-part-1-the-basics)\n* [Part 2: smoothing methods](https://www.kaggle.com/konradb/practical-time-series-part-2-vintage-methods) \n* [Part 3: ARMA and friends](https://www.kaggle.com/konradb/practical-time-series-pt-3-arma-and-friends) - this notebook\n* [Part 4: Prophet](https://www.kaggle.com/konradb/practical-time-series-pt-4-prophet)\n\nThe notebook is split into three sections: we introduce the basic framework of linear processes, then present extensions and finally demonstrate how to solve a prediction problem from scratch. \n\n* [Basic linear processes](#section-one)\n* [Beyond ARMA](#section-two)\n* [Full pipeline](#section-three)\n\n\nAs before, we begin by importing the required packages","metadata":{}},{"cell_type":"code","source":"import itertools\nimport pandas as pd\nimport numpy as np\nfrom random import gauss\n\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\nfrom pandas.plotting import autocorrelation_plot\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport warnings\nimport itertools\nimport statsmodels.api as sm\nfrom random import gauss\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n\nwarnings.simplefilter(action='ignore', category= FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:32.933496Z","iopub.execute_input":"2021-08-24T18:05:32.933966Z","iopub.status.idle":"2021-08-24T18:05:34.762827Z","shell.execute_reply.started":"2021-08-24T18:05:32.933853Z","shell.execute_reply":"2021-08-24T18:05:34.762091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Whatever my personal feelings on the matter, Excel is still used as a data source in a lot of places - so we need to be able to read it \n!pip install xlrd","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:34.764341Z","iopub.execute_input":"2021-08-24T18:05:34.764911Z","iopub.status.idle":"2021-08-24T18:05:44.190856Z","shell.execute_reply.started":"2021-08-24T18:05:34.764864Z","shell.execute_reply":"2021-08-24T18:05:44.190049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general settings\nclass CFG:\n    data_folder = '../input/tsdata-1/'\n    img_dim1 = 20\n    img_dim2 = 10\n        \n# adjust the parameters for displayed figures    \nplt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})    ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:44.193484Z","iopub.execute_input":"2021-08-24T18:05:44.193939Z","iopub.status.idle":"2021-08-24T18:05:44.19954Z","shell.execute_reply.started":"2021-08-24T18:05:44.193876Z","shell.execute_reply":"2021-08-24T18:05:44.198324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Basic linear processes\n\nJust to indulge my inner nerd, I will go top-down in this one and start from a high level definition - because why not :-) A linear process is a time series $X_t$ defined by:\n\\begin{equation}\n    X_t = \\sum_{u = -\\infty}^{\\infty} \\psi_u \\epsilon_{t-u}\n\\end{equation}\nwhere $\\epsilon_t$ is a white noise series and \n\\begin{equation}\n\\sum_{u = -\\infty}^{\\infty} \\left|\\psi_u \\right|^2 < \\infty\n\\end{equation}\n\n\nTranslated to plain English:\n* (in our context) white noise a serially uncorrelated sequence of random variables with zero mean and finite variance. For a general definition (and the origin of the color convention) I can recommend the Wikipedia entry: https://en.wikipedia.org/wiki/White_noise\n\n* it is a linear combination - potentially infinite and depending on both past and present - of a white noise series\n\n* in the real world it has to be causal $\\implies$ depending on a finite number of past values\n\n* it is a unified framework for handling different types of data generating processes\n\n* for the mathematically inclided person reading this: huge parts of the theory behind ARMA processes pop up as special cases / corrolaries from results in functional analysis, specifically the [Hilbert projection theorem](https://en.wikipedia.org/wiki/Hilbert_projection_theorem) and [spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem); the requirement on sum of squared values being finite correspond to the $l_2$ space for our sequence: https://en.wikipedia.org/wiki/Lp_space\n\n* only needs second order statistics (variance) $\\rightarrow$ works best for elliptical distributions (e.g. Gaussian)\n","metadata":{}},{"cell_type":"markdown","source":"\n## AR processes","metadata":{}},{"cell_type":"markdown","source":"The simplest (non-trivial) example of a linear process is an autoregressive process of order $p$:\n    \\begin{equation}\n    X_t = \\sum_{i= 1}^p \\phi_i X_{t-i} + \\epsilon_t\n    \\end{equation}\n    \nUnpacking the formula:\n1. Natural extension of a multiple linear regression model: we forecast the variable of interest using a linear combination of past values of the variable (lagged values act as predictors). The term \\textit{auto}regression reflects the fact that we are regressing the variable on (version of) itself\n2. The single exponential smoothing method (described [here](https://www.kaggle.com/konradb/practical-time-series-pt-2-smoothing-methods?scriptVersionId=68281131) ) can be viewed as a special case of an autoregressive process of order 1\n3. Easy to estimate parameters and forecast \n\n\nNow that we have defined (and hopefully understood) what an AR process is, let's have a look at how to identify one. This is relevant if we need to decide whether an autoregressive process is a right kind of model for a particular dataset. For the sake of demonstration, we will use simulated data, where the data generating process is:\n    \\begin{equation}\n    X_t = 0.9 X_{t-1} + \\epsilon_t\n    \\end{equation}\n    \nObserve in the codeblock below that the coefficient changes sign - this is because of the notational convention in statsmodels, where we read the coefficients in order defined by the characteristic polynomial (more on that below):\n    \\begin{equation}\n    X_t - 0.9 X_{t-1} = \\epsilon_t\n    \\end{equation}\n","metadata":{}},{"cell_type":"code","source":"ar1 = np.array([1, -0.9])\nma1 = np.array([1])\nAR_object1 = ArmaProcess(ar1, ma1)\nsimulated_data_1 = AR_object1.generate_sample(nsample=1000)\nplt.plot(simulated_data_1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:44.20108Z","iopub.execute_input":"2021-08-24T18:05:44.20136Z","iopub.status.idle":"2021-08-24T18:05:44.464392Z","shell.execute_reply.started":"2021-08-24T18:05:44.201334Z","shell.execute_reply":"2021-08-24T18:05:44.463196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A fast technique for deciding if AR is the right kind of model is to examine the autocorrelation and partial autocorrelation functions (described [here](https://www.kaggle.com/konradb/practical-time-series-pt-1-the-basics#Dependence)):","metadata":{}},{"cell_type":"code","source":"plot_acf(simulated_data_1, lags = 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:44.466161Z","iopub.execute_input":"2021-08-24T18:05:44.466595Z","iopub.status.idle":"2021-08-24T18:05:44.719217Z","shell.execute_reply.started":"2021-08-24T18:05:44.466551Z","shell.execute_reply":"2021-08-24T18:05:44.718173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(simulated_data_1, lags = 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:44.720428Z","iopub.execute_input":"2021-08-24T18:05:44.720733Z","iopub.status.idle":"2021-08-24T18:05:45.083172Z","shell.execute_reply.started":"2021-08-24T18:05:44.7207Z","shell.execute_reply":"2021-08-24T18:05:45.082086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Intuition for identification:\n* process memory: direct and indirect dependence information\n* ACF for AR(p): strong until lag p, trailing off afterwards\n* PACF only describes the direct relationship between an observation and its lag. This would suggest that there would be no correlation for lag values beyond k.\n\n","metadata":{}},{"cell_type":"markdown","source":"## MA processes","metadata":{}},{"cell_type":"markdown","source":"An autoregressive model expresses the forecast variable as a linear combination of past realizations of itself and the same idea can be applied to past forecast errors: a moving average process of order $q$ is defined by the relationship:\n    \n\\begin{equation}\nX_t = \\epsilon_t + \\sum_{i = 1}^q \\theta_i \\epsilon_{t-i}\n\\end{equation}\n\nwhere $\\epsilon_t$ is a white noise series.\n  \nLet's repeat the identification exercise from before:\n    \\begin{equation}\n    X_t = \\epsilon_t + 0.9  \\epsilon_{t-1}\n    \\end{equation}\n","metadata":{}},{"cell_type":"code","source":"ar1 = np.array([1])\nma1 = np.array([1, -0.9])\nMA_object1 = ArmaProcess(ar1, ma1)\nsimulated_data_1 = MA_object1.generate_sample(nsample=1000)\nplt.plot(simulated_data_1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:45.085039Z","iopub.execute_input":"2021-08-24T18:05:45.085466Z","iopub.status.idle":"2021-08-24T18:05:45.29706Z","shell.execute_reply.started":"2021-08-24T18:05:45.085419Z","shell.execute_reply":"2021-08-24T18:05:45.296014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acf(simulated_data_1, lags = 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:45.299601Z","iopub.execute_input":"2021-08-24T18:05:45.299893Z","iopub.status.idle":"2021-08-24T18:05:45.53727Z","shell.execute_reply.started":"2021-08-24T18:05:45.299864Z","shell.execute_reply":"2021-08-24T18:05:45.536283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(simulated_data_1, lags = 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:45.539055Z","iopub.execute_input":"2021-08-24T18:05:45.539349Z","iopub.status.idle":"2021-08-24T18:05:45.784572Z","shell.execute_reply.started":"2021-08-24T18:05:45.539319Z","shell.execute_reply":"2021-08-24T18:05:45.783525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Intuition for identification:\n* MA model corrects future forecasts based on errors made on recent forecasts.\n* ACF for MA(k) series: to show a strong correlation with recent values up to the lag of k, then a sharp decline \n* PACF: strong relationship to the lag k and a trailing off of correlation from the lag onwards.","metadata":{}},{"cell_type":"markdown","source":"## ARMA","metadata":{}},{"cell_type":"markdown","source":"We have an autoregressive component and a moving average one, so it is quite natural to combine those two types of dynamics into a single model: ARMA(p,q) series satisfies the relationship:\n\n\\begin{equation}\nX_t = \\sum_{i=1}^p \\phi_i X_{t-i} + \\sum_{j=1}^q \\theta_j \\epsilon_{t-j} + \\epsilon_t\n\\end{equation}\n\nwhere our predictors on the right hand side include both lagged values of the series and lagged errors, $p$ is the order the autoregressive part and $q$ is the order of the moving average component.\n\nSome observations:\n1. if an ARMA(p,q) model is stationary (see [module 1](https://www.kaggle.com/konradb/practical-time-series-pt-1-the-basics#Stationarity) for a refresher), it can be represented as an infinite AR series\n\\begin{equation}    \n    X_t = \\sum_{u=1}^\\infty \\pi_u X_{t-u} + \\epsilon_t\n\\end{equation}\n\n2. this allows for estimation via maximum likelihood and a simple recursive forecast:\n\\begin{equation}\n    \\hat{X}_{T+1} = \\sum_{u=1}^\\infty \\hat{\\pi}_u X_{T+1 - u}\n\\end{equation}\n\nIn practice: Kalman filter (incoming in subsequent modules).\n\n3. Stationarity of the ARMA(p,q) process is established by analyzing the characteristic polynomial. There is quite a substantial body of theoretical work behind this idea, but the word \"practical\" is there for a reason in the title of this series :-) so a crash-course argument would go like this:\n\n    * given ARMA(p,q) series defined by the equation above, we can write its characteristic polynomial \n    \n    \\begin{equation}\n    P(z) = 1 - \\phi_1 z - \\ldots - \\phi_p z^p\n    \\end{equation}\n    \n    * if we look for solutions in complex (as opposed to real numbers) domain, the equation P(z) = 0 has $p$ solutions $z_1, \\ldots, z_p$\n    \n    * if $|z_i| >1$ for all $i$ then the underlying model is stationary\n    ","metadata":{}},{"cell_type":"markdown","source":"## Forecasting with ARMA\n\nLet's put our newly introduced framework to the test and actually predict something.","metadata":{}},{"cell_type":"markdown","source":"### Case 1\n\nThe first dataset we look is the quarterly change in the aggregate savings level in the United States - the data is sourced from FRED: https://fred.stlouisfed.org/series/A191RP1Q027SBEA.","metadata":{}},{"cell_type":"code","source":"xdat = pd.read_csv(CFG.data_folder + 'savings_change.csv')\n\nxdat.columns = ['date', 'value']\nxdat['date'] = pd.to_datetime(xdat['date'])\n\nxdat.set_index('date').plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:45.785771Z","iopub.execute_input":"2021-08-24T18:05:45.78607Z","iopub.status.idle":"2021-08-24T18:05:46.264758Z","shell.execute_reply.started":"2021-08-24T18:05:45.78604Z","shell.execute_reply":"2021-08-24T18:05:46.263726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start with a quick EDA: first, a seasonal decomposition","metadata":{}},{"cell_type":"markdown","source":"There is *a* seasonal component, but it is very small in magnitude; trend does not seem significant either - those are strong clues we can suspect the process is stationary and move to testing this formally (using our old acquaintance, the ADF test):","metadata":{}},{"cell_type":"code","source":"result = adfuller(xdat['value'])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:46.266296Z","iopub.execute_input":"2021-08-24T18:05:46.266601Z","iopub.status.idle":"2021-08-24T18:05:46.305817Z","shell.execute_reply.started":"2021-08-24T18:05:46.266571Z","shell.execute_reply":"2021-08-24T18:05:46.304881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No red flags stopping us at this stage (reminder: this means we don't see an overall linear trend during the sample period) - we can proceed to model the series as ARMA(p,q). What about the values of p and q?","metadata":{}},{"cell_type":"code","source":"plot_acf(xdat['value'], lags = 25); print()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:46.307693Z","iopub.execute_input":"2021-08-24T18:05:46.308139Z","iopub.status.idle":"2021-08-24T18:05:46.585172Z","shell.execute_reply.started":"2021-08-24T18:05:46.308092Z","shell.execute_reply":"2021-08-24T18:05:46.58421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(xdat['value'], lags = 25); print()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:46.586538Z","iopub.execute_input":"2021-08-24T18:05:46.586841Z","iopub.status.idle":"2021-08-24T18:05:46.857902Z","shell.execute_reply.started":"2021-08-24T18:05:46.58681Z","shell.execute_reply":"2021-08-24T18:05:46.856929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the behavior of the ACF and PACF, ARMA(3,0) does seem like a reasonable first pass. Let's split the data into training and validation:","metadata":{}},{"cell_type":"code","source":"x0,x1 = xdat.loc[xdat.date < '2014-01-01']['value'],  xdat.loc[xdat.date >= '2014-01-01']['value']","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:46.85922Z","iopub.execute_input":"2021-08-24T18:05:46.859515Z","iopub.status.idle":"2021-08-24T18:05:46.870656Z","shell.execute_reply.started":"2021-08-24T18:05:46.859485Z","shell.execute_reply":"2021-08-24T18:05:46.869899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next step is specifying the functional form of the model: for now, we focus on two arguments - endog (the data series we are fitting the model on) and order (specifying the $p$ and $q$ parameters). Stationarity and invertibility enforcement are related the discussion above, i.e. the conditions under which models can be flipped between representations, and can be left at default settings in most situations. Once instantiated, we simply call a $.fit$ method.","metadata":{}},{"cell_type":"code","source":"# We specify the model \nmod = sm.tsa.statespace.SARIMAX(endog = x0, order=(3, 0, 0),\n                                seasonal_order=(0, 0, 0, 0),\n                                enforce_stationarity= False,\n                                enforce_invertibility=False\n                               )\n\n# fit\nresults = mod.fit()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:46.872061Z","iopub.execute_input":"2021-08-24T18:05:46.872514Z","iopub.status.idle":"2021-08-24T18:05:47.083757Z","shell.execute_reply.started":"2021-08-24T18:05:46.872472Z","shell.execute_reply":"2021-08-24T18:05:47.082502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Statsmodels provides us with a rich set of descriptive measures characterizing the fitted model:\n* the top table summarizes sample size, likelihood and the information criteria - this is helpful if we want to select models on parsimony. For those unfamiliar with the topic, information criteria (AIC, BIC, HQIC and others) quantify the basic idea of balancing model complexity (number of parameters) and likelihood, so that we end up selecting a model which has the minimal number of parameters necessary to capture the details of our DGP, but not more than that:https://en.wikipedia.org/wiki/Akaike_information_criterion. An information criterion measures how well a model fits the data while taking into account the overall complexity - model that fits the data very well while using large number of parameters will be assigned a larger AIC score than a model that uses fewer features to achieve the same goodness-of-fit. \n\n* the middle table gives the estimated coefficients of the model: $ar.L_{i}$ corresponds to the coefficient of the AR part at lag $i$, which $sigma2$ is the variance of the noise component. The $Z$ statistic and the endpoints of a confidence interval allow for a quick assessment of statistical significance\n\n* finally, the box at the bottom summarizes some diagnostic test: Jarque-Bera is used for testing if the residuals of the model have Gaussian distribution, while Ljung-Box checks whether the results are serially independent. In our case the former indicates no problem, while L-B indicates there is some serial dependence in the residuals of our model, indicating there is some component of the dynamics we are not capturing. ","metadata":{}},{"cell_type":"code","source":"print(results.summary())","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:47.08515Z","iopub.execute_input":"2021-08-24T18:05:47.085491Z","iopub.status.idle":"2021-08-24T18:05:47.100498Z","shell.execute_reply.started":"2021-08-24T18:05:47.085448Z","shell.execute_reply":"2021-08-24T18:05:47.099514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The diagnostics can also be summarized visually - the graphs are pretty self-explanatory ('correlogram' is simply ACF in signal processing parlance):","metadata":{}},{"cell_type":"code","source":"results.plot_diagnostics(); print()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:47.101854Z","iopub.execute_input":"2021-08-24T18:05:47.102208Z","iopub.status.idle":"2021-08-24T18:05:47.885755Z","shell.execute_reply.started":"2021-08-24T18:05:47.102174Z","shell.execute_reply":"2021-08-24T18:05:47.884784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = results.predict(start = len(x0) + 1, end  = len(x0) + len(x1) )\nxpred = xdat.loc[xdat.date >= '2014-01-01'].copy()\nxpred['forecast'] = predictions\nxpred['date'] = pd.to_datetime(xpred['date'])\nxpred.set_index('date').plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:47.887131Z","iopub.execute_input":"2021-08-24T18:05:47.887438Z","iopub.status.idle":"2021-08-24T18:05:48.186091Z","shell.execute_reply.started":"2021-08-24T18:05:47.887407Z","shell.execute_reply":"2021-08-24T18:05:48.185018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In terms of forecast quality this model can be useful as a baseline, but overall it is clearly nothing to write home about:\n1. there is a predictable lag: one period after an increase in the original series the forecast goes up as well; this is due to autoregressive nature of the model\n2. the model exhibits dynamics in the first few periods of the sample and flatlines afterward - recursive nature of the forecast means that for longer time horizons, predictions are used as if they were actual observations (just as we saw with [exponential smoothing models](https://www.kaggle.com/konradb/practical-time-series-pt-2-smoothing-methods?scriptVersionId=68281131)). \n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Case 2\n\nLet's try another example: the passenger dataset we first encountered in the context of exponential smoothing. ","metadata":{}},{"cell_type":"code","source":"xdat = pd.read_csv(CFG.data_folder + 'passengers.csv')\nxdat.columns = ['date', 'value']\n# we will move to the log scale to avoid issues around the changing scale - recall that the definition of an ARMA model assumes \n# the variance of the noise component is constant\nxdat['value'] = np.log1p(xdat['value'])\nxdat['date'] = pd.to_datetime(xdat['date'])\nxdat.set_index('date').plot()\n\n\ndecomposition = sm.tsa.seasonal_decompose(xdat[\"value\"],period = 12) \nfigure = decomposition.plot()\nplt.show()\n\n\n# prepare the train / test split\ncutoff = '1958-01-01'\nx0,x1 = xdat.loc[xdat.date < cutoff]['value'],  xdat.loc[xdat.date >= cutoff]['value']\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:48.187519Z","iopub.execute_input":"2021-08-24T18:05:48.187827Z","iopub.status.idle":"2021-08-24T18:05:49.24617Z","shell.execute_reply.started":"2021-08-24T18:05:48.18779Z","shell.execute_reply":"2021-08-24T18:05:49.245163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us begin by fitting a model as-is - ignoring the obvious fact that the model has a clear trend component, so most certainly will not be stationary.","metadata":{}},{"cell_type":"code","source":"\nmod = sm.tsa.statespace.SARIMAX(endog = x0, order=(3, 0, 1), seasonal_order=(0, 0, 0, 0),\n                                enforce_stationarity= False,  enforce_invertibility=False\n                               )\n\n# fit\nresults = mod.fit()\n\n# plot the predictions\npredictions = results.predict(len(x0), len(xdat)  - 1)\nxpred = xdat.loc[xdat.date >= cutoff].copy()\nxpred['forecast'] = predictions\nxpred['date'] = pd.to_datetime(xpred['date'])\nxpred.set_index('date').plot()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:49.247712Z","iopub.execute_input":"2021-08-24T18:05:49.248151Z","iopub.status.idle":"2021-08-24T18:05:49.800429Z","shell.execute_reply.started":"2021-08-24T18:05:49.248106Z","shell.execute_reply":"2021-08-24T18:05:49.799563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The general direction of the forecast seems in line with the change in the data, but this directional correctness is the most positive thing to say. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Beyond ARMA\n\nAs we can see from the examples above, there are issues when applying the baseline $ARMA(p,q)$ model to real life time series:\n* the models are only efficient if the series is stationary - peformance deteriorates rapidly otherwise \n* does not allow for trend\n* does not allow for seasonality\n\nThe last two issues can be handled by differencing the series at appropriate lags, and it is this idea that allows us to extend ARMA - first, we move towards ARIMA. \n\n### ARIMA\n\nA process $X_t$ is ARIMA(p,d,q) $\\iff$ $\\nabla^d X_t$ is ARMA(p,q)\n\nUnpacking the (slightly cryptic :-) formulation: \n* a series becomes a stationary ARMA(p,q) after differencing it $d$ times\n\n* discrete version of differentiation: polynomial of order $d$ becomes a constant after taking $d$th derivative\n\n* non-seasonal ARIMA encapsulates other models as special cases\n  - ARIMA(0,0,0) is white noise\n  - ARIMA(0,1,0) is random walk\n  - ARIMA(p,0,0) is AR(p)\n  - ARIMA(0,0,q) is MA(q)\n  \n  \nThe basic idea is that if we are dealing with a series that can be represented as a stationary process with polynomial trend, we can model it jointly (instead of manually differencing $d$ times and then flipping back). \n\n### SARIMA\n\nThe \"I\" in ARIMA corresponds to the \"integrated\" component, which is a formal way of saying we are incorporating the trend into our setup starting with a stationary $ARMA(p,q)$ process. Following a similar logic, we can incorporate the seasonal component and allow it to follow the same type of dynamic: a seasonal ARIMA (SARIMA) model can be denoted as ARIMA(p,d,q)(P,D,Q)m, where:\n- m refers to the number of periods in each season\n- (lowercase) p,d,q refer to the definition of the ARIMA part\n- (uppercase) P,D,Q refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model.\n\nWhile not obvious on an intuitive level, the SARIMA model is easier to understand if we make use of the backshift (lag) operator (https://en.wikipedia.org/wiki/Lag_operator). If we have a model for quarterly data, \n\n\nThe seasonal part of the model consists of terms that are similar to the non-seasonal components of the model, but involve backshifts of the seasonal period. For example, an $ARIMA(1,1,1)(1,1,1)_4$ model  is for quarterly data (m = 4) series $y_t$ can be specified as \n    \\begin{equation}       \n    (1 - \\phi_1 B)(1- \\Phi_1 B^4)(1- B)(1-B^4)y_t = (1 + \\theta_1 B)(1 + \\Theta_1 B)\\epsilon_t\n    \\end{equation}\n\nFor a proper exposition, with examples in R, the reader is encouraged to read Rob Hyndman's online book: https://otexts.com/fpp2/seasonal-arima.html\n\n### SARIMAX\n\nWhile not a very common situation, we do have data available for the forecast horizon (for example long term economic forecasts, which are available in advance). This means we can use **exogenous** variables to improve the quality of the forecast - hence SARIMA**X**.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Full pipeline\n\nWe are in a position to combine all the building blocks and solve a real-life problem in full generality. We will the Superstore dataset (a version can be found here: https://www.kaggle.com/bravehart101/sample-supermarket-dataset). ","metadata":{}},{"cell_type":"code","source":"df = pd.read_excel(CFG.data_folder + \"Sample - Superstore.xls\")\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:49.801734Z","iopub.execute_input":"2021-08-24T18:05:49.802269Z","iopub.status.idle":"2021-08-24T18:05:51.037246Z","shell.execute_reply.started":"2021-08-24T18:05:49.802232Z","shell.execute_reply":"2021-08-24T18:05:51.036144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is quite a lot of information here, most of which is not needed for our purposes - so let's clean it up a bit. ","metadata":{}},{"cell_type":"code","source":"# we pick only a single category of purchases\ndf = df.loc[df['Category'] == 'Furniture']\n# remove the unnecessary columns\ncols = ['Row ID', 'Order ID', 'Ship Date', 'Ship Mode', 'Customer ID',\n        'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region',\n        'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Quantity', 'Discount', 'Profit']\ndf.drop(cols, axis=1, inplace=True)\ndf = df.sort_values('Order Date')\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:51.038645Z","iopub.execute_input":"2021-08-24T18:05:51.038973Z","iopub.status.idle":"2021-08-24T18:05:51.060862Z","shell.execute_reply.started":"2021-08-24T18:05:51.038925Z","shell.execute_reply":"2021-08-24T18:05:51.059556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the time range we are looking at\ndf['Order Date'].min(), df['Order Date'].max()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:51.063897Z","iopub.execute_input":"2021-08-24T18:05:51.064238Z","iopub.status.idle":"2021-08-24T18:05:51.073321Z","shell.execute_reply.started":"2021-08-24T18:05:51.064207Z","shell.execute_reply":"2021-08-24T18:05:51.072109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With four years worth of data, we can think about capturing seasonal patterns (always worth remembering: modeling seasonal effects makes sense if you have at least two complete data cycles in your dataset).\n\nUnlike some other models we will discuss in subsequent parts of this series (Kalman filter being a prominent example), ARIMA models require regularly sampled observations and do not work with missing values. Let's ensure our dataset satisfies those requirements:","metadata":{}},{"cell_type":"code","source":"\n# we have multiple transactions per day => we need to aggregate the data\ndf = df.groupby('Order Date')['Sales'].sum().reset_index()\n\n# set index for easier manipulation\ndf = df.set_index('Order Date')\n\n# as shown above, some days are missing => we resample the data to a monthly frequency\ndf = df['Sales'].resample('MS').mean()\n\n# examine the results\ndf.plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:51.075213Z","iopub.execute_input":"2021-08-24T18:05:51.07568Z","iopub.status.idle":"2021-08-24T18:05:51.490878Z","shell.execute_reply.started":"2021-08-24T18:05:51.075631Z","shell.execute_reply":"2021-08-24T18:05:51.490097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's definitely some seasonality, possibly trend? We can have a closer look using the seasonal decomposition:","metadata":{}},{"cell_type":"code","source":"\n# basic decomposition \ndecomposition = sm.tsa.seasonal_decompose(df, model='additive')\nfig = decomposition.plot()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:51.492378Z","iopub.execute_input":"2021-08-24T18:05:51.492672Z","iopub.status.idle":"2021-08-24T18:05:52.486257Z","shell.execute_reply.started":"2021-08-24T18:05:51.492641Z","shell.execute_reply":"2021-08-24T18:05:52.485074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use start of 2017 as cutoff, so we use the period up until the end of 2016 as training data:","metadata":{}},{"cell_type":"code","source":"x0, x1 = df[:'2016'], df['2017':]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:05:57.675347Z","iopub.execute_input":"2021-08-24T18:05:57.675747Z","iopub.status.idle":"2021-08-24T18:05:57.683985Z","shell.execute_reply.started":"2021-08-24T18:05:57.675715Z","shell.execute_reply":"2021-08-24T18:05:57.682858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check stationarity\nresult = adfuller(x0)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:06:05.130116Z","iopub.execute_input":"2021-08-24T18:06:05.13067Z","iopub.status.idle":"2021-08-24T18:06:05.142561Z","shell.execute_reply.started":"2021-08-24T18:06:05.130636Z","shell.execute_reply":"2021-08-24T18:06:05.141783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# examine autocorrelation\nplot_acf(x0, lags = 12); print()\nplot_pacf(x0, lags = 12); print()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:06:32.001272Z","iopub.execute_input":"2021-08-24T18:06:32.001637Z","iopub.status.idle":"2021-08-24T18:06:32.518675Z","shell.execute_reply.started":"2021-08-24T18:06:32.001607Z","shell.execute_reply":"2021-08-24T18:06:32.517576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nothing stands out (pun intended) except lag 12 - what about the version differenced thus? If we are talking about monthly sales data, it reasonable for such serial dependence to persist in raw data, but disappear with seasonal differencing. ","metadata":{}},{"cell_type":"code","source":"\nplot_acf(x0.diff(12)[13:], lags = 12); print()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:14:45.198912Z","iopub.execute_input":"2021-08-24T18:14:45.199279Z","iopub.status.idle":"2021-08-24T18:14:45.472918Z","shell.execute_reply.started":"2021-08-24T18:14:45.199248Z","shell.execute_reply":"2021-08-24T18:14:45.472012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed it does. We could use this guesstimate for our model, but we can do something else: evaluate different possible combinations of parameters (effectively replicating the functionality from R wrapped in the `auto.arima` function - I copied this chunk from https://medium.com/mlearning-ai/sarima-vs-prophet-forecasting-time-series-b121e1e2bd37):","metadata":{}},{"cell_type":"code","source":"# Define the p, d and q parameters to take any value between 0 and 3\np = d = q = range(0, 2)\n# Generate all different combinations of p, q and q triplets\nsimple_pdq = list(itertools.product(p, d, q))\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(i[0], i[1], i[2], 12) for i in list(itertools.product(p, d, q))]\nprint('Parameter combinations for Seasonal ARIMA...')\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n\nfor param in simple_pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(x0, order=param,)\n            results = mod.fit()\n\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:20:40.547087Z","iopub.execute_input":"2021-08-24T18:20:40.547441Z","iopub.status.idle":"2021-08-24T18:20:42.774628Z","shell.execute_reply.started":"2021-08-24T18:20:40.547409Z","shell.execute_reply":"2021-08-24T18:20:42.773705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit the model with optimal parameters:","metadata":{}},{"cell_type":"code","source":"# Fit model\nmod = sm.tsa.statespace.SARIMAX(x0,\n                                order=(0,1,1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n\nresults = mod.fit()\n\nprint(results.summary().tables[1])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:22:26.293113Z","iopub.execute_input":"2021-08-24T18:22:26.293503Z","iopub.status.idle":"2021-08-24T18:22:26.545361Z","shell.execute_reply.started":"2021-08-24T18:22:26.293472Z","shell.execute_reply":"2021-08-24T18:22:26.544311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the forecast\nyhat = results.predict(x0.shape[0],x0.shape[0] + x1.shape[0] - 1)\nx1.plot()\nyhat.plot(color = 'red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:25:55.059618Z","iopub.execute_input":"2021-08-24T18:25:55.06018Z","iopub.status.idle":"2021-08-24T18:25:55.301604Z","shell.execute_reply.started":"2021-08-24T18:25:55.060145Z","shell.execute_reply":"2021-08-24T18:25:55.300501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This part took a while to land - going on vacation and coding do not exactly match, at least for me :-) In the next installment, we will talk about state space models: a general class of time series models which combine the speed of exponential smoothing with the generalization of ARMA. Stay tuned!\n","metadata":{}}]}