{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Name: SP Tian \n* Date: May 5, 2019 \n\n#            Sentiment Analysis on Anime Reviews \n\n* ## 0 Introduction \n    * ### 0.1 Import Libraries\n    * ### 0.2 Loading the Database \n\n* ## 1 Exploratory Data Analysis \n    * ### 1.1 Data Exploration \n        * #### 1.1.1 Rating Frequency Table\n        * #### 1.1.2 Word Cloud \n    * ### 1.2 Data Cleaning \n    * ### 1.3 Data Split - only get rTrain\n    * ### 1.4 Word Frequency Table \n    * ### 1.5  Import Sentiment Weights \n\n* ## 2 Train Models \n    * ### 2.1 Logistic Regression \n    * ### 2.2 Gaussian Naive Bayes \n    * ### 2.3 Random Forests \n\n* ## 3 Model Selection \n        * Cross Validatioin \n        * print skm_conf_mat\n\n* ## 4 Prediction on Random Forests "},{"metadata":{},"cell_type":"markdown","source":"## 0. Introduction \n\n* Question: \nHow we can use Sentiment Analysis on comments to further predict viewers' ratings? \n\n* Source: \nA Japanese anime, from Chinese viewing website called bilibili.com, which went IPO in NY Exchange as ticker (BILI). The reviews are scrapped from the website using JSON and till the end of the date of May 6, 2019. \n\n* Deliverables: cvs.file on predicting Ratings \n\n## Note: \nTest data is split half (train/test) and then 70-30, containing 2258 comments. "},{"metadata":{},"cell_type":"markdown","source":"### 0.1 Import Libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nimport sys \nimport re\n\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport jieba.analyse\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\n# import sklearn modules \nimport sklearn.metrics as skm\nimport sklearn.model_selection\nimport sklearn.preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix as skm_conf_mat\nfrom collections import Counter\nfrom collections import defaultdict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 0.2 Loading the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"datas = pd.read_csv(\"../input/bilibilib_gongzuoxibao.csv\", sep = \",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis \n### 1.1. Data Exploration "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"colnames = datas.columns\nprint(colnames) # author, score, disliked, likes, liked, ctime, score.1, content, last_ex_index, cursor, date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datas.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datas.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.1.1 Rating Frequency Table"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"datas['score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(sorted(datas['score'].unique()))\ny = list(datas['score'].value_counts())[::-1]\nplt.bar(x,y, color='orange')\nplt.xlabel('Score')\nplt.ylabel('')\nplt.title('Rating Frequencies')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Content Analysis \ntexts = ';'.join(datas['content'].tolist())\ncut_text = \" \".join(jieba.cut(texts))\n# TF_IDF\nkeywords = jieba.analyse.extract_tags(cut_text, topK=100, withWeight=True, allowPOS=('a','e','n','nr','ns'))\ntext_cloud = dict(keywords)\n###pd.DataFrame(keywords).to_excel('TF_IDF关键词前100.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Remove all punctuation and expression marks \ntemp =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### cannot open Word Cloud picture "},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Data Cleaning "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"del datas['ctime']\ndel datas['cursor']\ndel datas['liked']\ndel datas['disliked']\ndel datas['likes']\ndel datas['last_ep_index']\npd.isnull(datas).astype(int).aggregate(sum, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3. Data Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"perfect = datas[datas.score == 10]\nimperfect = datas[datas.score != 10]\nperfect_sample = perfect.sample(n = 1583, random_state = 1 )\nnew_data = pd.concat([perfect_sample, imperfect], axis = 0)\n\nfeatures = new_data['content']\nlabels = new_data['score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rTrain, rTest, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=42)\n# let's understand up a bit the data\n## print out the shapes of  resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(rTrain.shape), \n      #\"\\nValidation set: \\t{}\".format(rValidation.shape),\n      \"\\nTest set: \\t\\t{}\".format(rTest.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4. Frequency Table for Top 100 "},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = '\\n'.join(rTrain.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.5. Import Sentiment Weights \n> X_rTrain"},{"metadata":{"trusted":true},"cell_type":"code","source":"grade1 = np.array([0.1\n,0\n,0\n,0.7\n,0.8\n,0.1\n,0\n,0.3\n,0\n,0\n,0\n,0\n,0.6\n,0.1\n,-1\n,0\n,0\n,1\n,0\n,0\n,0\n,0.5\n,-0.3\n,-0.1\n,0.8\n,0\n,0.4\n,0\n,0\n,0\n,0.6\n,0.6\n,0.8\n,0\n,0.6\n,0.4\n,0.6\n,1\n,0\n,-0.7\n,0\n,0.9\n,0\n,-0.2\n,0\n,0\n,0\n,0\n,0\n,0.7\n,0\n,1\n,0\n,0\n,0\n,0\n,-0.2\n,0\n,0\n,0.6\n,0.1\n,0\n,0.6\n,0.3\n,0\n,0.7\n,0.7\n,0\n,0\n,0\n,0\n,0\n,0\n,0\n,0\n,0.4\n,0\n,0.6\n,0\n,1\n,0.6\n,0\n,0\n,1\n,0.4\n,0.2\n,-1\n,0.8\n,-1\n,0\n,1\n,0\n,0.9\n,0.7\n,-0.3\n,0\n,0.2\n,0\n,0\n,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(matrix) * grade1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Train Model \n### 2.1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Logistic model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\n\nclf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X, y_train)\nclf.score(X, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(clf.predict(X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Library of Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n\ngnb.fit(X, y_train)\ngnb.score(X,y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"np.unique(gnb.predict(X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Random Forests "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RFClass\nmodel_rf = RFClass(n_estimators = 100, max_depth=5, random_state=2019)\nmodel_rf.fit(X, y_train)\nmodel_rf.score(X, y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"np.unique(gnb.predict(X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Model Selection\n> Import rTest"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"texts = '\\n'.join(rTest.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grade2 = np.array([0.1\n,0\n,0\n,0.7\n,0.3\n,0\n,0\n,0.8\n,0.5\n,0\n,0.1\n,0.1\n,0\n,0\n,1\n,-1\n,0\n,0\n,0\n,0.4\n,0\n,0.6\n,0\n,0.6\n,0\n,0\n,1\n,0\n,0.8\n,-0.1\n,0\n,0\n,0.4\n,0\n,0\n,0\n,0.6\n,0.6\n,-0.4\n,0\n,0\n,0\n,0\n,0\n,0.4\n,1\n,-0.6\n,0\n,-0.7\n,0.9\n,-1\n,0.4\n,0.1\n,-0.2\n,-0.3\n,0.6\n,0\n,0.2\n,0\n,0\n,0\n,0\n,0.2\n,0\n,0.6\n,0\n,0.5\n,-1\n,0\n,0\n,0.9\n,0\n,0\n,-0.6\n,0.1\n,0\n,0.4\n,-0.8\n,0\n,0\n,-0.3\n,0\n,0.7\n,0.5\n,0\n,0.8\n,0\n,0\n,0\n,0\n,-0.2\n,0.6\n,0.5\n,0.7\n,0\n,0\n,0.8\n,0.5\n,0.7\n,-0.4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest = np.array(matrix) * grade2\nxTest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> function confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.show()\n\nnp.set_printoptions(precision = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Log prediction"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"clf_proba = clf.predict_proba(xTest)   # predict probability \nclf_pred = clf.predict(xTest)   # prediction result\nclf.score(xTest, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_cm = skm_conf_mat(y_test, clf_pred)\nplot_confusion_matrix(clf_cm, classes = list(sorted(y_train.unique())), title = 'Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"clfcv = LogisticRegressionCV(cv=5, random_state=0, multi_class='multinomial').fit(X, y_train)\nclfcv.score(X, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfcv_proba = clfcv.predict_proba(xTest)\nclfcv_pred = clfcv.predict(xTest)\nclfcv.score(xTest, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfcv_cm = skm_conf_mat(y_test, clf_pred)\nplot_confusion_matrix(clfcv_cm, classes = list(sorted(datas['score'].unique())), title = 'Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_proba = model_rf.predict_proba(xTest)\nrf_pred = model_rf.predict(xTest)\nmodel_rf.score(xTest, y_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Tree Plot\nfrom graphviz import Source\nfrom sklearn import tree as treemodule\nSource(treemodule.export_graphviz(\n        model_rf.estimators_[1]\n        , out_file=None\n        , filled = True\n        , proportion = True #@@ try False and understand the differences\n        )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_cm = skm_conf_mat(y_test, rf_pred)\nplot_confusion_matrix(rf_cm, classes = list(sorted(datas['score'].unique())), title = 'Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Prediction Print on RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred = pd.DataFrame(rf_pred)\nrf_pred.to_csv(\"Predictions on Ratings.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## 3 Model Selection REVISED\n    * ### 3.1 Group Ratings by very high(10), high(8), and others(2-6) TO (4) "},{"metadata":{"trusted":true},"cell_type":"code","source":"#score = (new_data.score == 2)|(new_data.score == 6)\nnew_data.loc[new_data.score == 6, 'score'] = 4\nnew_data.loc[new_data.score == 2, 'score'] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = new_data['content']\nlabels = new_data['score']\n\nnew_data['score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rTrain, rTest, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=42)\n# let's understand up a bit the data\n## print out the shapes of  resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(rTrain.shape), \n      #\"\\nValidation set: \\t{}\".format(rValidation.shape),\n      \"\\nTest set: \\t\\t{}\".format(rTest.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* rTrain"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = '\\n'.join(rTrain.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(matrix) * grade1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* rTest "},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = '\\n'.join(rTest.tolist())\n#cut_text = jieba.lcut(texts)\ncut_text = \"\".join(jieba.cut(texts))\ncut_text = re.sub(pattern = temp, repl = \"\", string = cut_text)\n\nkeyword = jieba.analyse.extract_tags(cut_text, topK=100, allowPOS=('a','e','n','nr','ns'))  # list\ncut_text = cut_text.split('\\n')\nkeyword","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutlist = []\n\nfor i in range(0, len(cut_text)):\n    cut_dic = defaultdict(int) \n    comment = cut_text[i]\n    comment_cut = jieba.lcut(comment)\n    for word in comment_cut: # word freq for every comment \n        if word in keyword:\n            cut_dic[word] += 1  \n    order = sorted(cut_dic.items(),key = lambda x:x[1],reverse = True) # word freq in descending order\n    #print(order)\n \n    myresult = \"\" \n    for j in range(0,len(order)): \n        result = order[j][0]+ \"-\" + str(order[j][1])\n        myresult = myresult + \" \" + result  \n    cutlist.append(myresult)\n#print(cutlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freqs = []\nfor raw in cutlist:\n    word_freq = {}\n    for word_freq_raw in raw.split():\n        index = word_freq_raw.find('-')\n        word = word_freq_raw[:index]\n        freq = int(word_freq_raw[index + 1])\n        word_freq[word] = freq\n    word_freqs.append(word_freq)\n    \nmatrix = []\nfor word_freq in word_freqs:\n    row = []\n    for word in keyword:\n        if word in word_freq:\n            row.append(word_freq[word])\n        else:\n            row.append(0)\n    matrix.append(row)\n#print(matrix)\nmatrix = np.array(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest = np.array(matrix) * grade2\nxTest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X, y_train)\nclf.score(xTest, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfcv = LogisticRegressionCV(cv=5, random_state=0, multi_class='multinomial').fit(X, y_train)\nclfcv.score(xTest, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb.fit(X, y_train)\ngnb.score(xTest, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf.fit(X, y_train)\nprint(model_rf.score(X, y_train))\nprint(model_rf.score(xTest, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Until now, we attempted multiple approaches to improve the accuracy rate of predicting corresponding scores on sentiment analysis. \n\n> 1. Importing different sets of sentiment weights\n> 2. Lowering score dimensions to [4,8,10] rather [2,4,6,8,10] "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}