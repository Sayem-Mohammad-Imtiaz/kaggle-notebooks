{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np \nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/insurance/insurance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# exploratory data analysis :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['age'],bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(df['age'],df['charges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nsns.scatterplot(x=df['age'],y=df['charges'],hue=df['region'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nsns.scatterplot(x=df['age'],y=df['charges'],hue=df['smoker'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*smokers seem to have higher charges for any age, hence it needs to be converted from a categorical to numerical variable(binary-encoding { 1: yes, 0: no})*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['children'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nsns.scatterplot(x=df['age'],y=df['charges'],hue=df['children'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*if the family size is large, the chances of contracting an ailment increases, hence the expenditure rises, so we have to include the number of children as one of the factors !*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nsns.kdeplot(df['bmi'],shade=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*the distribution of the BMI in the population seems to be centred around 30, in a gaussian normal distribution, very few people are at the extreme sides !*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*seeing the data, it seems clear that age, number of children, BMI, smoker or not are closely linked with the insurance column so our aim is whether to or not one-hot-encode the region column ? it may be that people in some region of the state tend to fall ill due to poor conditions and facilities maybe ?*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*proposed solution, we will try to use random forests also polynomial regression to study the data once with the region column and once without !*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# building up the model and preprocessing of the data !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[df.columns[:len(df.columns)-1]]\ny = df[df.columns[-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encoder_1(x):\n    if x == 'yes':\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encoder_2(x):\n    if x == 'male':\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['region'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['smoker'] = X['smoker'].apply(one_hot_encoder_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['sex'] = X['sex'].apply(one_hot_encoder_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.concat([X,pd.get_dummies(X['region'],drop_first=True)],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop('region',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_train = scaler.fit_transform(X_train)\nscaled_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# using linear regression :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr_model = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**how does linear regression work ?**\n\n* well if we have data having features ${x_{1}, x_{2} ......, x_{n}}$ ,we model it as a vector $ \\vec{x} = <x_{1},x_{2}....,x_{n}>$ , and fit the data into a hypothesis function $f(\\,\\vec{x}) = \\theta^{T}\\,. \\vec{x} +\\, b $ and then fine tune for the best fit ! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model.fit(scaled_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = lr_model.predict(scaled_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_error(y_test,preds)**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_absolute_error(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Polynomial Regression :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer = PolynomialFeatures(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_poly_train,X_poly_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_poly_train = poly_transformer.fit_transform(X_poly_train)\nX_poly_test = poly_transformer.transform(X_poly_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_model = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_model.fit(X_poly_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = poly_model.predict(X_poly_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_model.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_error(y_test,preds)**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_absolute_error(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**how does polynomial regression work ?**\n* *it is very similar to the linear regression, we just aim to introduce higher degree interaction terms in the data set ! as you know sometimes the presence of two or more features simultaneously has a greater effect than them alone, hence adding interaction terms (of higher degrees, by default degree = 2) helps us fit better !*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Using Random Forests :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = []\ni_vals = []\nfor i in range(1,70):\n    decision_forest = RandomForestRegressor(n_estimators = i)\n    \n    decision_forest.fit(scaled_train,y_train)\n    \n    pred = decision_forest.predict(scaled_test)\n    \n    i_vals.append(i)\n    \n    losses.append(mean_squared_error(y_test,pred)**0.5)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.plot(i_vals,losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_forest = RandomForestRegressor(n_estimators = 50)\n\ndecision_forest.fit(scaled_train,y_train)\n\npred = decision_forest.predict(scaled_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_error(y_test,pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_absolute_error(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_forest.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**using the above line of code, you can see the related importance of the features ! hence i avoid further discussion you can explore which to include and which to avoided !**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**conclusion : the region column is of importance hence it cannot be ignored blatently !**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}