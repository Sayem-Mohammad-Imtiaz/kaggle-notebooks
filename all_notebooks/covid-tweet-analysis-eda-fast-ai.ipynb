{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:aqua;text-align:center\">COVID-19 Tweet EDA + Fast.ai Classification</h1>\n\n\n<strong style=\"color:red\">If you like my notebook, please leave an upvote!</strong>\n<hr>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"! pip install --quiet chart-studio","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nfrom tqdm.notebook import tqdm\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nimport torch\nimport fastai\nfrom fastai import *\nfrom fastai.text import *\n\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('fivethirtyeight')\ntorch.device(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", encoding='latin-1')\ntest_data = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\", encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll join both Datasets, shuffle them and them divide them."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = pd.concat([train_data, test_data])\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want this classification to be 3-way so changing `Extremely Positive` to `Positive` and `Extremely Negative` to `Negative`.\nFor the moment, we only need the tweet text and the sentiment of it."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data['Sentiment'] = data['Sentiment'].map({'Extremely Positive':'Positive', 'Extremely Negative':'Negative', 'Negative':'Negative', 'Positive':'Positive', 'Neutral':'Neutral'})\ntrain_data = data[['OriginalTweet', 'Sentiment']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"color:blue;text-align:center\">Exploratory Data Analysis</h2>\n<hr>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color:green;text-align:center\">Target Value Distribution</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"vals = [len(train_data[train_data['Sentiment']=='Negative']['Sentiment']), len(train_data[train_data['Sentiment']=='Positive']['Sentiment']), len(train_data[train_data['Sentiment']=='Neutral']['Sentiment'])]\nidx = ['Negative', 'Positive', 'Neutral']\nfig = px.pie(\n    train_data,\n    names='Sentiment',\n    title='Target Value Distribution Chart',\n    color_discrete_sequence=px.colors.sequential.Agsunset\n)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color:green;text-align:center\">Character Frequency Count</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].str.len()\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].str.len()\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].str.len()\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\n\nfig.update_layout(title_text=\"Character Count\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color:green;text-align:center\">Word Count Distribution</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].str.split().map(lambda x: len(x))\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].str.split().map(lambda x: len(x))\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].str.split().map(lambda x: len(x))\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\nfig.update_layout(title_text=\"Word Count\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color:green;text-align:center\">Unique Word Count</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].apply(lambda x: len(set(str(x).split()))).to_list()\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].apply(lambda x: len(set(str(x).split()))).to_list()\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([neg, pos, neu], ['Negative', 'Positive', 'Neutral'])\nfig.update_layout(title_text=\"Unique Word Count Distribution\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color:green;text-align:center\">URL Count</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\nfig.update_layout(title_text=\"URL Count\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color:green;text-align:center\">Word Cloud</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"negative = \" \".join(train_data[train_data['Sentiment'] == 'Negative']['OriginalTweet'].to_list())\npositive = \" \".join(train_data[train_data['Sentiment'] == 'Positive']['OriginalTweet'].to_list())\nneutral = \" \".join(train_data[train_data['Sentiment'] == 'Neutral']['OriginalTweet'].to_list())\n\nfig, ax = plt.subplots(1, 3, figsize=(15,15))\nng_wlc = WordCloud(width=256, height=256, collocations=False).generate(negative)\nps_wlc = WordCloud(width=256, height=256, collocations=False).generate(positive)\nne_wlc = WordCloud(width=256, height=256, collocations=False).generate(neutral)\nwcs = [ng_wlc, ps_wlc, ne_wlc]\ntitls = [\"Negative Tweets\", \"Positive Tweets\", \"Neutral Tweets\"]\n\nfor num, el in enumerate(wcs):\n    ax[num].imshow(el)\n    ax[num].axis('off')\n    ax[num].set_title(titls[num])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"color:blue;text-align:center\">Text Cleaning</h2>\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove everything except basic text characters\ntrain_data['OriginalTweet'] = train_data['OriginalTweet'].str.replace(\"[^a-zA-Z]\", \" \").str.lower()\ntrain_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change the column name and encode the labels\ntrain_data = train_data.rename(columns={'Sentiment':'label'})\ntrain_data['label'] = train_data['label'].apply(lambda x: 0 if x=='Negative' else (1 if x=='Positive' else 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now split the dataset into training and validation sets\nsplit_pcent = 0.20  # How much percent of data should go into testing set\nsplit = int(split_pcent * len(train_data))\n\nshuffled_set = train_data.sample(frac=1).reset_index(drop=True)   # Shuffle the data\nvalid_set = shuffled_set[:split]   # Get everything till split number\ntrain_set = shuffled_set[split:]   # Get everything after split number\n\ntrain_set = train_set[['label', 'OriginalTweet']]\nvalid_set = valid_set[['label', 'OriginalTweet']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"color:blue;text-align:center\">Modelling</h2>\n<hr>"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Make a Language Model Data Bunch from our train set\ndata_bunch = TextLMDataBunch.from_df(train_df=train_set, valid_df=valid_set, path=\"\")\n\n# Make the data classifier\ndata_clf = TextClasDataBunch.from_df(path=\"\", train_df=train_set, valid_df=valid_set, vocab=data_bunch.train_ds.vocab, bs=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just train the learner as-is."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the language learner model and fit for one epoch\nlearner = language_model_learner(data_bunch, arch=AWD_LSTM, drop_mult=0.5)\n\nlearner.fit_one_cycle(1, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now unfreeze the hidden layers and train the learner."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try unfreezing last 3 layers first\nlayers_to_unfreeze = [1, 2, 3]\nfor i in layers_to_unfreeze:\n    learner.freeze_to(-i)\n    learner.fit_one_cycle(1, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now unfreeze all layers and then train them."},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.unfreeze()\nlearner.fit_one_cycle(1, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the encoder\nlearner.save_encoder('learn_encoder')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we train the classifier using the encoder above."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = text_classifier_learner(data_clf, arch=AWD_LSTM, drop_mult=0.5)\nclf.load_encoder('learn_encoder')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit_one_cycle(5, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's unfreeze all it's layers and train it.\nclf.unfreeze()\nclf.fit_one_cycle(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"color:blue;text-align:center\">Testing and Classification</h2>\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.predict(\"The COVID is harming our lives and destroying job opportunities\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}