{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training\n- Fine tune a pretrained model from `HuggingFace` using `fastai` and `blurr`\n- Our 1st place solution used **ensembling of many models** by taking the **average of the prediction probabilities for each word** and **the entire dataset was used for training with no validation**\n\n## Steps:\n1. [Preprocessing](https://www.kaggle.com/nguyncaoduy/1-place-scl-ds-2021-voidandtwotsts-preprocess)\n2. [Training](https://www.kaggle.com/nguyncaoduy/1-place-scl-ds-2021-voidandtwotsts-train) - This Notebook\n3. [Ensembling](https://www.kaggle.com/nguyncaoduy/1-place-scl-ds-2021-voidandtwotsts-ensemble)","metadata":{}},{"cell_type":"code","source":"!pip install ohmeow-blurr==0.0.22 datasets==1.3.0 fsspec==0.8.5 -qq","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-21T15:41:51.314971Z","iopub.execute_input":"2021-08-21T15:41:51.315341Z","iopub.status.idle":"2021-08-21T15:41:57.177604Z","shell.execute_reply.started":"2021-08-21T15:41:51.315258Z","shell.execute_reply":"2021-08-21T15:41:57.176169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change this 2 lines to use to another pretrained model\npretrained_model_name = 'xlm-roberta-large'\nmodel_name = 'xlm-roberta-large'","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:41:57.181149Z","iopub.execute_input":"2021-08-21T15:41:57.181446Z","iopub.status.idle":"2021-08-21T15:41:57.187434Z","shell.execute_reply.started":"2021-08-21T15:41:57.181417Z","shell.execute_reply":"2021-08-21T15:41:57.186694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# turn off multithreading to avoid deadlock\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:41:57.19059Z","iopub.execute_input":"2021-08-21T15:41:57.190896Z","iopub.status.idle":"2021-08-21T15:41:57.196945Z","shell.execute_reply.started":"2021-08-21T15:41:57.190871Z","shell.execute_reply":"2021-08-21T15:41:57.196045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import *\nfrom fastai.text.all import *\n\nfrom blurr.data.all import *\nfrom blurr.modeling.all import *\n\nSEED = 42\nset_seed(SEED, True)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:41:57.199442Z","iopub.execute_input":"2021-08-21T15:41:57.199715Z","iopub.status.idle":"2021-08-21T15:42:01.523604Z","shell.execute_reply.started":"2021-08-21T15:41:57.199689Z","shell.execute_reply":"2021-08-21T15:42:01.522321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('../input/sclds2021preprocess/wordlist.json', 'r') as f:\n    wordlist = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:42:01.524927Z","iopub.execute_input":"2021-08-21T15:42:01.52526Z","iopub.status.idle":"2021-08-21T15:42:01.538065Z","shell.execute_reply.started":"2021-08-21T15:42:01.525231Z","shell.execute_reply":"2021-08-21T15:42:01.537432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ndf_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval}\n\ntrain_df = pd.read_csv('../input/sclds2021preprocess/train.csv', converters=df_converters)\nvalid_df = pd.read_csv('../input/sclds2021preprocess/valid.csv', converters=df_converters)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:42:01.542183Z","iopub.execute_input":"2021-08-21T15:42:01.542453Z","iopub.status.idle":"2021-08-21T15:42:21.414889Z","shell.execute_reply.started":"2021-08-21T15:42:01.542427Z","shell.execute_reply":"2021-08-21T15:42:21.414097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df), len(valid_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:42:21.418679Z","iopub.execute_input":"2021-08-21T15:42:21.41917Z","iopub.status.idle":"2021-08-21T15:42:21.427211Z","shell.execute_reply.started":"2021-08-21T15:42:21.419136Z","shell.execute_reply":"2021-08-21T15:42:21.426329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = sorted(list(set([lbls for sublist in train_df.labels.tolist() for lbls in sublist])))\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:42:21.429017Z","iopub.execute_input":"2021-08-21T15:42:21.429442Z","iopub.status.idle":"2021-08-21T15:42:21.438723Z","shell.execute_reply.started":"2021-08-21T15:42:21.429407Z","shell.execute_reply":"2021-08-21T15:42:21.437633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"task = HF_TASKS_AUTO.TokenClassification\nconfig = AutoConfig.from_pretrained(pretrained_model_name)\nconfig.num_labels = len(labels)\n\nhf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n                                                                               task=task, \n                                                                               config=config)\nhf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:42:21.440475Z","iopub.execute_input":"2021-08-21T15:42:21.440895Z","iopub.status.idle":"2021-08-21T15:43:38.067882Z","shell.execute_reply.started":"2021-08-21T15:42:21.440858Z","shell.execute_reply":"2021-08-21T15:43:38.067156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n                                                     is_split_into_words=True, \n                                                     tok_kwargs={'return_special_tokens_mask': True})\n\nblocks = (\n    HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), \n    HF_TokenCategoryBlock(vocab=labels)\n)\n\ndef get_y(inp): return [(label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels)]","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:38.069144Z","iopub.execute_input":"2021-08-21T15:43:38.069519Z","iopub.status.idle":"2021-08-21T15:43:38.078112Z","shell.execute_reply.started":"2021-08-21T15:43:38.069462Z","shell.execute_reply":"2021-08-21T15:43:38.076978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db = DataBlock(\n    blocks=blocks, \n    splitter=RandomSplitter(valid_pct=0.1, seed=SEED),\n    get_x=ColReader('tokens'),\n    get_y=get_y,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:38.07929Z","iopub.execute_input":"2021-08-21T15:43:38.079688Z","iopub.status.idle":"2021-08-21T15:43:38.089307Z","shell.execute_reply.started":"2021-08-21T15:43:38.079648Z","shell.execute_reply":"2021-08-21T15:43:38.088646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = db.dataloaders(train_df, bs=32)\ndls.show_batch(dataloaders=dls)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:38.091089Z","iopub.execute_input":"2021-08-21T15:43:38.091414Z","iopub.status.idle":"2021-08-21T15:43:44.635397Z","shell.execute_reply.started":"2021-08-21T15:43:38.091389Z","shell.execute_reply":"2021-08-21T15:43:44.634646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@delegates()\nclass TokenCrossEntropyLossFlat(BaseLoss):\n    \"Same as `CrossEntropyLossFlat`, but for mutiple tokens output\"\n    y_int = True\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n    def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n    def decodes(self, x):    return L([ i.argmax(dim=self.axis) for i in x ])\n    def activation(self, x): return L([ F.softmax(i, dim=self.axis) for i in x ])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:44.636644Z","iopub.execute_input":"2021-08-21T15:43:44.636972Z","iopub.status.idle":"2021-08-21T15:43:44.64429Z","shell.execute_reply.started":"2021-08-21T15:43:44.636943Z","shell.execute_reply":"2021-08-21T15:43:44.643452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = HF_BaseModelWrapper(hf_model)\nloss_func = TokenCrossEntropyLossFlat()\nopt_func = partial(Adam)\nlearn_cbs = [HF_BaseModelCallback]\nfit_cbs = [HF_TokenClassMetricsCallback()]\nsplitter = hf_splitter","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:44.645675Z","iopub.execute_input":"2021-08-21T15:43:44.646214Z","iopub.status.idle":"2021-08-21T15:43:45.249964Z","shell.execute_reply.started":"2021-08-21T15:43:44.646167Z","shell.execute_reply":"2021-08-21T15:43:45.249162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func, splitter=splitter, cbs=learn_cbs).to_fp16()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:45.251454Z","iopub.execute_input":"2021-08-21T15:43:45.251824Z","iopub.status.idle":"2021-08-21T15:43:45.257393Z","shell.execute_reply.started":"2021-08-21T15:43:45.251786Z","shell.execute_reply":"2021-08-21T15:43:45.256217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.unfreeze()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:45.258737Z","iopub.execute_input":"2021-08-21T15:43:45.259089Z","iopub.status.idle":"2021-08-21T15:43:45.311652Z","shell.execute_reply.started":"2021-08-21T15:43:45.259052Z","shell.execute_reply":"2021-08-21T15:43:45.311007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-4, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:43:45.312929Z","iopub.execute_input":"2021-08-21T15:43:45.313271Z","iopub.status.idle":"2021-08-21T15:44:33.430348Z","shell.execute_reply.started":"2021-08-21T15:43:45.313235Z","shell.execute_reply":"2021-08-21T15:44:33.429583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.recorder.plot_loss()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:33.433376Z","iopub.execute_input":"2021-08-21T15:44:33.433673Z","iopub.status.idle":"2021-08-21T15:44:33.625542Z","shell.execute_reply.started":"2021-08-21T15:44:33.433633Z","shell.execute_reply":"2021-08-21T15:44:33.624815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(learn.token_classification_report)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:33.626992Z","iopub.execute_input":"2021-08-21T15:44:33.6274Z","iopub.status.idle":"2021-08-21T15:44:33.634306Z","shell.execute_reply.started":"2021-08-21T15:44:33.627296Z","shell.execute_reply":"2021-08-21T15:44:33.633315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.export(f'{model_name}.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:33.635549Z","iopub.execute_input":"2021-08-21T15:44:33.63617Z","iopub.status.idle":"2021-08-21T15:44:43.819983Z","shell.execute_reply.started":"2021-08-21T15:44:33.636129Z","shell.execute_reply":"2021-08-21T15:44:43.818892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation\n- This is only relevant during model selection and testing\n- For the final training, full dataset is used so the accuracy below doesn't really reflect the power of the model.","metadata":{}},{"cell_type":"code","source":"@patch\ndef blurr_predict(self:Learner, items, rm_type_tfms=None):\n    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n    is_split_str = hf_before_batch_tfm.is_split_into_words and isinstance(items[0], str)\n    is_df = isinstance(items, pd.DataFrame)\n    if (not is_df and (is_split_str or not is_listy(items))): items = [items]\n    dl = self.dls.test_dl(items, rm_type_tfms=rm_type_tfms, num_workers=0)\n    with self.no_bar(): probs, _, decoded_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    trg_tfms = self.dls.tfms[self.dls.n_inp:]\n    outs = []\n    probs, decoded_preds = L(probs), L(decoded_preds)\n    for i in range(len(items)):\n        item_probs = [probs[i]]\n        item_dec_preds = [decoded_preds[i]]\n        item_dec_labels = tuplify([tfm.decode(item_dec_preds[tfm_idx]) for tfm_idx, tfm in enumerate(trg_tfms)])\n        outs.append((item_dec_labels, item_dec_preds, item_probs))\n    return outs","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:43.825246Z","iopub.execute_input":"2021-08-21T15:44:43.827648Z","iopub.status.idle":"2021-08-21T15:44:43.841482Z","shell.execute_reply.started":"2021-08-21T15:44:43.827599Z","shell.execute_reply":"2021-08-21T15:44:43.840329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import punctuation\n\ndef reconstruct(num, pred, raw_tokens, raw_address):\n    def complete_word(x):\n        y = x.strip().strip(punctuation)\n        if y != '' and y in wordlist:\n            x = x.replace(y, wordlist[y])\n        return x\n    \n    def normalize_bracket(x):\n        if '(' in x and ')' not in x:\n            x = x + ')'\n        elif ')' in x and '(' not in x:\n            x = '(' + x\n        return x\n    \n    ans = ['/'] * num\n    for idx in range(num):\n        res = pred[idx]\n        start_poi, end_poi = -1, -1\n        start_str, end_str = -1, -1\n        for i in range(len(res[0])):\n            if 'POI' in res[1][i]:\n                if start_poi == -1: start_poi = i\n                end_poi = i\n            if 'STR' in res[1][i]:\n                if start_str == -1: start_str = i\n                end_str = i\n        \n        if start_poi != -1:\n            txt1 = raw_address[idx]\n            for i in range(start_poi):\n                txt1 = txt1[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_poi, -1):\n                txt1 = txt1[:-len(raw_tokens[idx][i])].strip()\n            \n            txt1_check = ''.join(raw_tokens[idx][start_poi:end_poi + 1]).replace(' ', '')\n            assert txt1.replace(' ', '') == txt1_check\n            \n            last = len(txt1)\n            for i in range(end_poi, start_poi - 1, -1):\n                while last > 0 and txt1[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt1 = txt1[:last] + complete_word(raw_tokens[idx][i]) + txt1[last + len(raw_tokens[idx][i]):]\n        else:\n            txt1 = ''\n        \n        if start_str != -1:\n            txt2 = raw_address[idx]\n            for i in range(start_str):\n                txt2 = txt2[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_str, -1):\n                txt2 = txt2[:-len(raw_tokens[idx][i])].strip()\n            \n            txt2_check = ''.join(raw_tokens[idx][start_str:end_str + 1]).replace(' ', '')\n            assert txt2.replace(' ', '') == txt2_check\n            \n            last = len(txt2)\n            for i in range(end_str, start_str - 1, -1):\n                while last > 0 and txt2[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt2 = txt2[:last] + complete_word(raw_tokens[idx][i]) + txt2[last + len(raw_tokens[idx][i]):]\n        else:\n            txt2 = ''\n        \n        txt1 = txt1.strip(punctuation)\n        txt2 = txt2.strip(punctuation)\n        txt1 = normalize_bracket(txt1)\n        txt2 = normalize_bracket(txt2)\n        \n        ans[idx] = (txt1 + '/' + txt2)\n    \n    return ans","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:43.856857Z","iopub.execute_input":"2021-08-21T15:44:43.859414Z","iopub.status.idle":"2021-08-21T15:44:43.899797Z","shell.execute_reply.started":"2021-08-21T15:44:43.859369Z","shell.execute_reply":"2021-08-21T15:44:43.898658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_diff(df):\n    MAX_ROWS = 50\n    CNT = 0\n    for idx in range(len(df)):\n        if CNT == MAX_ROWS: break\n        row = df.iloc[idx]\n        if row['POI/street'] != row['pred']:\n            CNT += 1\n            print(idx, row['id'], row['POI/street'], 'vs', row['pred'])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:43.903072Z","iopub.execute_input":"2021-08-21T15:44:43.907973Z","iopub.status.idle":"2021-08-21T15:44:43.92028Z","shell.execute_reply.started":"2021-08-21T15:44:43.907929Z","shell.execute_reply":"2021-08-21T15:44:43.915699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_acc(df):\n    return df.loc[valid_df['pred'] == df['POI/street'], 'id'].count() / len(df)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:43.924582Z","iopub.execute_input":"2021-08-21T15:44:43.925486Z","iopub.status.idle":"2021-08-21T15:44:43.930801Z","shell.execute_reply.started":"2021-08-21T15:44:43.925431Z","shell.execute_reply":"2021-08-21T15:44:43.929708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_tokens = list(valid_df['tokens'])\nraw_address = list(valid_df['raw_address'])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:43.932745Z","iopub.execute_input":"2021-08-21T15:44:43.933484Z","iopub.status.idle":"2021-08-21T15:44:44.119788Z","shell.execute_reply.started":"2021-08-21T15:44:43.933442Z","shell.execute_reply":"2021-08-21T15:44:44.118756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_pred = learn.blurr_predict_tokens(raw_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:44.121528Z","iopub.execute_input":"2021-08-21T15:44:44.122071Z","iopub.status.idle":"2021-08-21T15:44:48.92276Z","shell.execute_reply.started":"2021-08-21T15:44:44.122033Z","shell.execute_reply":"2021-08-21T15:44:48.921955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = reconstruct(len(valid_df), raw_pred, raw_tokens, raw_address)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:48.924094Z","iopub.execute_input":"2021-08-21T15:44:48.924454Z","iopub.status.idle":"2021-08-21T15:44:48.96496Z","shell.execute_reply.started":"2021-08-21T15:44:48.924414Z","shell.execute_reply":"2021-08-21T15:44:48.964151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df['pred'] = pred\nvalid_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:48.9663Z","iopub.execute_input":"2021-08-21T15:44:48.966689Z","iopub.status.idle":"2021-08-21T15:44:52.529603Z","shell.execute_reply.started":"2021-08-21T15:44:48.966653Z","shell.execute_reply":"2021-08-21T15:44:52.528617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final evaluation with the same metric used for the competition\ncalc_acc(valid_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T15:44:52.531154Z","iopub.execute_input":"2021-08-21T15:44:52.536072Z","iopub.status.idle":"2021-08-21T15:44:52.654624Z","shell.execute_reply.started":"2021-08-21T15:44:52.536026Z","shell.execute_reply":"2021-08-21T15:44:52.653434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_diff(valid_df)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-21T15:44:52.658311Z","iopub.execute_input":"2021-08-21T15:44:52.660005Z","iopub.status.idle":"2021-08-21T15:44:52.739871Z","shell.execute_reply.started":"2021-08-21T15:44:52.659958Z","shell.execute_reply":"2021-08-21T15:44:52.739106Z"},"trusted":true},"execution_count":null,"outputs":[]}]}