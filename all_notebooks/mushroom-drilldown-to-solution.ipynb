{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python"}},"cells":[{"metadata":{"_cell_guid":"32c1ad20-1c15-4b9b-8c53-e6babd6b664b","collapsed":true,"_uuid":"135afc94798b1a15cdac7cbb071701d134fef72d"},"execution_count":null,"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# read data into dataset variable\nmushrooms=pd.read_csv(\"../input/mushrooms.csv\")\nmushrooms.head()","cell_type":"code"},{"metadata":{"_cell_guid":"fd1aad27-1733-4813-abd4-db86492abaf2","_uuid":"6539f316ff76c2a11825cdff67e61ca1887a4e85"},"source":"Labelencode > numerify\n---","cell_type":"markdown"},{"metadata":{"_cell_guid":"28aa3927-9d3d-46ae-986e-1500bdb39f18","collapsed":true,"_uuid":"e97cd9840035c4fde819806abf4bbb6c4fceaf7e"},"execution_count":null,"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\n\nfor c in mushrooms.columns:\n    mushrooms[c]=mushrooms[c].fillna(-1)\n    if mushrooms[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(mushrooms[c].values))\n        mushrooms[c] = lbl.transform(list(mushrooms[c].values))\n        \nprint(mushrooms.describe().T)\nmushrooms","cell_type":"code"},{"metadata":{"_cell_guid":"e2b63131-7a94-460e-a953-8761d41e0431","_uuid":"b33c0ecdce5230ace920e2dd0369c648b44029b9"},"source":"compare poison 1 edibable 0\n---","cell_type":"markdown"},{"metadata":{"_cell_guid":"8bea7239-d359-40d4-a7b0-a994f60bc275","collapsed":true,"_uuid":"a2ddbba1ff847fbfa53e70a01a02aa4c457e56dd"},"execution_count":null,"outputs":[],"source":"new_col= mushrooms.groupby('class').mean()\nprint(new_col.head().T)","cell_type":"code"},{"metadata":{"_cell_guid":"309efda1-6095-45f5-85f6-3923c4304864","collapsed":true,"_uuid":"62f40ffbddf63c7b55bb28d55c8ef9c0f5dc5691"},"execution_count":null,"outputs":[],"source":"def dddraw(X_reduced,name):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    # To getter a better understanding of interaction of the dimensions\n    # plot the first three PCA dimensions\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)\n    titel=\"First three directions of \"+name \n    ax.set_title(titel)\n    ax.set_xlabel(\"1st eigenvector\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd eigenvector\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd eigenvector\")\n    ax.w_zaxis.set_ticklabels([])\n\n    plt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"7ad54885-de72-4ef9-8759-9adf4609c29d","_uuid":"366796a95c1b8822292609abdfecab2e20344595"},"source":"Clustering\n---\nmakes groups visible\nbut still high forecast error ","cell_type":"markdown"},{"metadata":{"_cell_guid":"19c87f28-b1eb-4a53-a2d8-3baec7893859","collapsed":true,"_uuid":"61ab67c6112504b6b3db2e0a169b7e0c9806075c"},"execution_count":null,"outputs":[],"source":"from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans,Birch\nimport statsmodels.formula.api as sm\nfrom scipy import linalg\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nimport matplotlib.pyplot as plt\n\nn_col=20\nX = mushrooms.drop(['class'],axis=1) \n\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\nY=mushrooms['class']\nX=X.fillna(value=0)       # those ? converted to NAN are bothering me abit...        \nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\nnames = [\n         'PCA',\n         'FastICA',\n         'Gauss',\n         'KMeans',\n         #'SparsePCA',\n         #'SparseRP',\n         'Birch',\n         'NMF',    \n         'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n    FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=24),\n    #SparsePCA(n_components=n_col),\n    #SparseRandomProjection(n_components=n_col, dense_output=True),\n    Birch(branching_factor=10, n_clusters=12, threshold=0.5),\n    NMF(n_components=n_col),    \n    LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    #print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n\n    #print('Ypredict *log_sec',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y))","cell_type":"code"},{"metadata":{"_cell_guid":"17f7fbe2-cce4-4220-9be7-d0d9b7b295f5","_uuid":"cb7dac2c9b555419f99f3ff93508486f9b3171da"},"source":"Linear classifiers\n---\n\nThe classification is solved by Elastic Net, Decisiontree,KNN, randomforrest 100%\nand partially solved by SVC, kSVC 98%\n\n","cell_type":"markdown"},{"metadata":{"_cell_guid":"90c76f13-19c2-4e02-83f9-427a7c3b40d2","collapsed":true,"_uuid":"ffb828f4fd67a3a7824e5ec7ba444d2f083ea282"},"execution_count":null,"outputs":[],"source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n# import some data to play with\n       # those ? converted to NAN are bothering me abit...        \n\nfrom sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\nn_col=20\nX = mushrooms.drop(['class'],axis=1) \nY=mushrooms['class']\nX=X.fillna(value=0)\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\nnames = [\n         'ElasticNet',\n         'SVC',\n         'kSVC',\n         'KNN',\n         'DecisionTree',\n         'RandomForestClassifier',\n         'GridSearchCV',\n         'HuberRegressor',\n         'Ridge',\n         'Lasso',\n         'LassoCV',\n         'Lars',\n         'BayesianRidge',\n         'SGDClassifier',\n         'RidgeClassifier',\n         'LogisticRegression',\n         'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    ElasticNetCV(cv=10, random_state=0),\n    SVC(),\n    SVC(kernel = 'rbf', random_state = 0),\n    KNeighborsClassifier(n_neighbors = 1),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators = 200),\n    GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    Lasso(alpha=0.05),\n    LassoCV(),\n    Lars(n_nonzero_coefs=10),\n    BayesianRidge(),\n    SGDClassifier(),\n    RidgeClassifier(),\n    LogisticRegression(),\n    OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    print(confusion_matrix(Y, np.round(regr.predict(X) ) ) )\n    print('--'*40)\n\n    # Classification Report\n    print('Classification Report')\n    print(classification_report(Y,np.round( regr.predict(X) ) ))\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')","cell_type":"code"}],"nbformat_minor":1,"nbformat":4}