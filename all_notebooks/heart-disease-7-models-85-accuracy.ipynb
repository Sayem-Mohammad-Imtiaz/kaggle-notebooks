{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking if there are any NULL values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Heat Map Correlation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (10,10))\nsns.heatmap(dataset.corr(), annot= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Splitting dataset into Train and Test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Scaling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Selection of Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(['Logistic Regreesion', LogisticRegression(random_state=0)])\nmodels.append(['SVM', SVC(random_state=0)])\nmodels.append(['KNeighbors', KNeighborsClassifier()])\nmodels.append(['Naive Bayes', GaussianNB()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=0)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=0)])\nmodels.append(['XGBoost', XGBClassifier()])\n\nlst= []\n\nfor m in range(len(models)):\n  a= []\n  model = models[m][1]\n  model.fit(x_train, y_train)\n  y_pred = model.predict(x_test)\n  cm = confusion_matrix(y_test, y_pred)\n  accuracies = cross_val_score(estimator = model, X = x_train, y = y_train, cv = 10)\n  print(models[m][0])\n  print(cm)\n  print('Accuracy Score',accuracy_score(y_test, y_pred))\n  print('')\n  print(\"Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n  print('')\n  print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n  print('')\n  print('-----------------------------------')\n  print('')\n  a.append(models[m][0])\n  a.append((accuracy_score(y_test, y_pred))*100) \n  a.append(accuracies.mean()*100)\n  a.append(accuracies.std()*100)\n  lst.append(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Data Frame.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(lst, columns= ['Model', 'Accuracy', 'Mean Accuracy', 'Std. Deviation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by= ['Accuracy', 'Mean Accuracy'], inplace= True, ascending= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below shows the values of models in Descending Order.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying Grid Search on Top 3 above models for best parameters and model selection.**\n1. Random Forest\n2. SVM\n3. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nsvm = SVC()\nlr = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [(rf, [{'n_estimators': [50, 100, 200, 300, 500], 'criterion': ['gini', 'entropy'], 'random_state':[0]}]), \n        (svm, [{'C': [0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf'], 'random_state':[0]}]),\n        (lr, [{'C': [0.1, 0.5, 1.0], 'random_state':[0]}])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in data:\n  grid = GridSearchCV(estimator = i , param_grid = j , scoring = 'accuracy',cv = 10)\n  grid.fit(x_train,y_train)\n  best_accuracy = grid.best_score_\n  best_parameters = grid.best_params_\n  print('{} BestAccuracy : {:.2f}%'.format(i,best_accuracy*100))\n  print('BestParameters : ',best_parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Therefore, after applying GridSearch we can confirm that RandomForest is best suited model on the dataset and gives best accuracy of 84.75%.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}