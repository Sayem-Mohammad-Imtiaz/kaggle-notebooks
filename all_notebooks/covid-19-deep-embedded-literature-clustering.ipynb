{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19: Deep Embedded Literature Clustering"},{"metadata":{},"cell_type":"markdown","source":"\n\n### About\nThis is a forke of [this Kerne](http://www.kaggle.com/maksimeren/covid-19-literature-clustering). User maksimeren did an amazing job on this one.\nI only used the paper preprocessing of this kernen and I now try to built a DEC (Deep Embedded Cluster) to classify the Literature of the [COVID-19 Open Research Dataset Challenge (CORD-19) | Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n\n### Goal:\nTest whether this approach can achieve better paper clustering than K-Means\n\n**Approach**:\n<ol>\n    <li>Paper preprocessing\n    <li>Define Autoencoder model\n    <li>Define ClusteringLayer\n    <li>Define DEC and visualice cluster\n</ol>\n\n### Disclaimer\nI am trying out new concepts for myself here. These concepts are perhaps not the best methods to solve this classification task. I would be happy to get your feedback!\n\n### Dataset Description\n\n>*In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.*\n#### Cite: [COVID-19 Open Research Dataset Challenge (CORD-19) | Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) <br>\n**Paper preprocessing and interactive plots(cite):** [maksimeren/covid-19-literature-clustering](http://www.kaggle.com/maksimeren/covid-19-literature-clustering) <br>\n**Clustering section(cite):** The whole clustering process was modelled [on this article](https://towardsdatascience.com/deep-clustering-for-financial-market-segmentation-2a41573618cf?gi=da93cf19ea05) <br>"},{"metadata":{},"cell_type":"markdown","source":"# Load the Data\nLoad the data following the notebook by Ivan Ega Pratama, from Kaggle.\n#### Cite: [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool)"},{"metadata":{},"cell_type":"markdown","source":"### Loading Metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/CORD-19-research-challenge/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fetch All of JSON File Path"},{"metadata":{},"cell_type":"markdown","source":"Get path to all JSON files:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\nlen(all_json)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper Functions"},{"metadata":{},"cell_type":"markdown","source":" File Reader Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the Data into DataFrame"},{"metadata":{},"cell_type":"markdown","source":"Using the helper functions, let's read in the articles into a DataFrame that can be used easily:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding the Word Count Columns"},{"metadata":{},"cell_type":"markdown","source":"Adding word count columns for both abstract and body_text can be useful parameters later:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handle Possible Duplicates"},{"metadata":{},"cell_type":"markdown","source":"When we look at the unique values above, we can see that tehre are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:\n\n(Thank you Desmond Yeoh for recommending the below approach on Kaggle)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_covid['abstract'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['body_text'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we didn't have duplicates. Instead, it was articles without Abstracts."},{"metadata":{},"cell_type":"markdown","source":"## Take a Look at the Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"Now that we have our dataset loaded, we need to clean-up the text to improve any clustering or classification efforts. First, let's drop Null vales:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.dropna(inplace=True)\ndf_covid.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Limit number of articles to speed up computation:"},{"metadata":{},"cell_type":"markdown","source":"Now let's remove punctuation from each text:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert each text to lower case:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the text cleaned up, we can create our features vector which can be fed into a clustering or dimensionality reduction algorithm. For our first try, we will focus on the text on the body of the articles. Let's grab that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"token_columns = df_covid.drop([\"abstract_word_count\", \"body_word_count\",], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_columns.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text tokenize"},{"metadata":{},"cell_type":"markdown","source":"Now we tokenize and vectorize the \"body_text\" column with the Keras built in tokenizer. We tokenize the body text of all papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nmaxlen = 4096 #only use this number of most frequent words\ntraining_samples = 8000\nvalidation_samples = 4500\nmax_words = 100000\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df_covid[\"body_text\"]) # generates word index\nsequences = tokenizer.texts_to_sequences(df_covid[\"body_text\"]) # transforms strings in list of intergers\nword_index = tokenizer.word_index # calculated word index\nprint(f\"{len(word_index)} unique tokens found\")\n\ndata = pad_sequences(sequences, maxlen=maxlen) #transforms integer lists into 2D tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler() \ndata_1 = scaler.fit_transform(data) # the values of all features are rescaled into the range of [0, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\nimport keras.backend as K\nfrom keras.engine.topology import Layer, InputSpec\nfrom keras.layers import Dense, Input, Embedding\nfrom keras.models import Model\nfrom keras.optimizers import SGD\nfrom keras import callbacks\nfrom keras.initializers import VarianceScaling\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Autoencoder function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def autoencoder(dims, act='relu', init='glorot_uniform'):\n    \"\"\"\n    Fully connected symmetric auto-encoder model.\n  \n    dims: list of the sizes of layers of encoder like [500, 500, 2000, 10]. \n          dims[0] is input dim, dims[-1] is size of the latent hidden layer.\n\n    act: activation function\n    \n    return:\n        (autoencoder_model, encoder_model): Model of autoencoder and model of encoder\n    \"\"\"\n    n_stacks = len(dims) - 1\n    \n    input_data = Input(shape=(dims[0],), name='input')\n    x = input_data\n    \n    # internal layers of encoder\n    for i in range(n_stacks-1):\n        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n\n    # latent hidden layer\n    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n\n    x = encoded\n    # internal layers of decoder\n    for i in range(n_stacks-1, 0, -1):\n        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n\n    # decoder output\n    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n    \n    decoded = x\n    \n    autoencoder_model = Model(inputs=input_data, outputs=decoded, name='autoencoder')\n    encoder_model     = Model(inputs=input_data, outputs=encoded, name='encoder')\n    \n    return autoencoder_model, encoder_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 20 \nn_epochs   = 15\nbatch_size = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dims = [x.shape[-1], 500, 500, 2000, 10] \ninit = VarianceScaling(scale=1. / 3., mode='fan_in',\n                           distribution='uniform')\npretrain_optimizer = SGD(lr=1, momentum=0.9)\npretrain_epochs = n_epochs\nbatch_size = batch_size\nsave_dir = 'kaggle/working'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dims","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Clustering Layer and generate model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClusteringLayer(Layer):\n    '''\n    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n    '''\n\n    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super(ClusteringLayer, self).__init__(**kwargs)\n        self.n_clusters = n_clusters\n        self.alpha = alpha\n        self.initial_weights = weights\n        self.input_spec = InputSpec(ndim=2)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 2\n        input_dim = input_shape[1]\n        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n        self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n        \n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        ''' \n        student t-distribution, as used in t-SNE algorithm.\n        It measures the similarity between embedded point z_i and centroid µ_j.\n                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n                 (i.e., a soft assignment)\n       \n        inputs: the variable containing data, shape=(n_samples, n_features)\n        \n        Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n        '''\n        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n        q **= (self.alpha + 1.0) / 2.0\n        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n        \n        return q\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) == 2\n        return input_shape[0], self.n_clusters\n\n    def get_config(self):\n        config = {'n_clusters': self.n_clusters}\n        base_config = super(ClusteringLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder, encoder = autoencoder(dims, init=init)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)\nfrom IPython.display import Image\nImage(filename='autoencoder.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(encoder, to_file='encoder.png', show_shapes=True)\nfrom IPython.display import Image\nImage(filename='encoder.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\nautoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs)\n#autoencoder.save_weights(save_dir + '/ae_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\nmodel = Model(inputs=encoder.input, outputs=clustering_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True)\nfrom IPython.display import Image\nImage(filename='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(0.01, 0.9), loss='kld')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=n_clusters, n_init=20)\ny_pred = kmeans.fit_predict(encoder.predict(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_last = np.copy(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# computing an auxiliary target distribution\ndef target_distribution(q):\n    weight = q ** 2 / q.sum(0)\n    return (weight.T / weight.sum(1)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = 0\nindex = 0\nmaxiter = 1000 # 8000\nupdate_interval = 100 # 140\nindex_array = np.arange(x.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tol = 0.001 # tolerance threshold to stop training","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train DEC model iteratively"},{"metadata":{"trusted":true},"cell_type":"code","source":"for ite in range(int(maxiter)):\n    if ite % update_interval == 0:\n        q = model.predict(x, verbose=0)\n        p = target_distribution(q)  # update the auxiliary target distribution p\n\n    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n    loss = model.train_on_batch(x=x[idx], y=p[idx])\n    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n\n#model.save_weights(save_dir + '/DEC_model_final.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Eval.\nq = model.predict(x, verbose=0)\np = target_distribution(q)  # update the auxiliary target distribution p\n\n# evaluate the clustering performance\ny_pred = q.argmax(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Append Cluster Labels to our preprocessced data frame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all = df_covid.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all['cluster'] = y_pred\ndata_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimension reduction and visualisation of clustering labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.manifold import TSNE\n\nx_embedded = TSNE(n_components=2).fit_transform(x)\n\nx_embedded.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the Cluster as scatter plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(x_embedded[:,0], x_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles, Clustered(Autoencoder and custem Keras Layer), Tf-idf with Plain Text\")\n# plt.savefig(\"plots/t-sne_covid19_label_TFID.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox\n\noutput_notebook()\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= x_embedded[:,0], \n    y= x_embedded[:,1],\n    x_backup = x_embedded[:,0],\n    y_backup = x_embedded[:,1],\n    desc= y_labels, \n    titles= df_covid['title'],\n    authors = df_covid['authors'],\n    journal = df_covid['journal'],\n    abstract = df_covid['abstract_summary'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"t-SNE Covid-19 Articles, Clustered(Autoencoder and custem Keras Layer), Tf-idf with Plain Text\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# add callback to control \ncallback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var radio_value = cb_obj.active;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            labels = data['desc'];\n            \n            if (radio_value == '20') {\n                for (i = 0; i < x.length; i++) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                }\n            }\n            else {\n                for (i = 0; i < x.length; i++) {\n                    if(labels[i] == radio_value) {\n                        x[i] = x_backup[i];\n                        y[i] = y_backup[i];\n                    } else {\n                        x[i] = undefined;\n                        y[i] = undefined;\n                    }\n                }\n            }\n\n\n        source.change.emit();\n        \"\"\")\n\n# callback for searchbar\nkeyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var text_value = cb_obj.value;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            abstract = data['abstract'];\n            titles = data['titles'];\n            authors = data['authors'];\n            journal = data['journal'];\n\n            for (i = 0; i < x.length; i++) {\n                if(abstract[i].includes(text_value) || \n                   titles[i].includes(text_value) || \n                   authors[i].includes(text_value) || \n                   journal[i].includes(text_value)) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                } else {\n                    x[i] = undefined;\n                    y[i] = undefined;\n                }\n            }\n            \n\n\n        source.change.emit();\n        \"\"\")\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-4\", \"C-5\",\n                                  \"C-6\", \"C-7\", \"C-8\",\n                                  \"C-9\", \"C-10\", \"C-11\",\n                                  \"C-12\", \"C-13\", \"C-14\",\n                                  \"C-15\", \"C-16\", \"C-17\",\n                                  \"C-18\", \"C-19\", \"All\"], \n                          active=20, callback=callback)\n\n# search box\nkeyword = TextInput(title=\"Search:\", callback=keyword_callback)\n\n#header\nheader = Div(text=\"\"\"<h1>COVID-19 Literature Cluster</h1>\"\"\")\n\n# show\nshow(column(header, widgetbox(option, keyword),p))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}