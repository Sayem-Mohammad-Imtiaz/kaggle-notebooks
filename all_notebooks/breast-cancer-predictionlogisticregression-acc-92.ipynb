{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Content:-\n* Panda\n* Numpy\n* Matplotlib\n* Seaborn\n* Plotly\n* Recomender System\n* Conclusion\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Dataset Description \n* **id** - It is the id of the patients those are diagonising the cancer\n* **diagnosis**- When patients are Male or Female.\n* **radius_mean** - The size of the radius is mentioned, depending the sixe of the radius , it is fatal or not\n* **texture_mean** - The mean of the texture is mentioned below.\n* **perimeter_mean** - For uneven size of the tumore, we need the \n* **area_mean** -  The mean area of the tumor is mentioned\n* **smoothness_mean** - The area is smooth or not \n* **compactness_mean** - The area of the tumor is compacted \n* **concavity_mean** - The tumor is concave and whose mean is mentioned. \n* **concave points_mean** - The concave point are mentioned \n* **symmetry_mean** - The symmetric mean of the tumor is mentioned \n* **fractal_dimension_mean** - The fractal dimension is given \n* **radius_se** - The radius is mentioned of the dataset.\n* '**texture_se** -It is mentioned of the dataset\n* '**perimeter_se** -It is mentioned of the dataset\n* '**area_se** -It is mentioned of the dataset\n* '**smoothness_se**- It is mentioned of the dataset\n* '**compactness_se**- It is mentioned of the dataset\n* '**concavity_se**- It is mentioned of the dataset\n* '**concave points_se**- It is mentioned of the dataset\n* '**symmetry_se** -It is mentioned of the dataset\n* '**fractal_dimension_se** -It is mentioned of the dataset\n* '**radius_worst** -It is mentioned of the dataset\n* **texture_worst** -It is mentioned of the dataset\n* **perimeter_worst**-It is mentioned of the dataset\n* **area_worst** -It is mentioned of the dataset\n* **smoothness_worst** -It is mentioned of the dataset\n* **compactness_worst** -It is mentioned of the dataset\n* **concavity_worst**- It is mentioned of the dataset\n* **concave points_worst** -It is mentioned of the dataset\n* **symmetry_worst** -The symmentry worst is mentioned of the dataset\n* **fractal_dimension_worst**- The fractal dimension worst is mentioned of the dataset\n* **Unnamed: 32**- It is mentined of the dataset","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/cancer/breast.png\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Here the party begins , bring all the library","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/minions/minions.jpg\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nsns.set(style=\"whitegrid\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the dataset ","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape # dataset number of rows and number of columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns # list down the number of the columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(percentiles=[0.1,0.25,0.45,0.55,0.75,0.95])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(df['radius_mean'], df['perimeter_mean'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the percentage missing value from dataset","metadata":{}},{"cell_type":"code","source":"((df.isnull().sum()/len(df))*100).sort_values(ascending=True).plot(kind='barh',figsize=(10,10))\nplt.grid(b=True, which='both')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obs** - Only one column is missing ","metadata":{}},{"cell_type":"markdown","source":"# Count of cancer type\n* Number of Malign  and Bening is shown below","metadata":{}},{"cell_type":"code","source":"df['diagnosis'].value_counts().plot(kind='barh', figsize=(10,10))\nplt.grid(b=True, which='both')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (9, 9)\nlabels=['Benign','Malign']\nplt.pie(df['diagnosis'].value_counts(), explode=(0,0.1), labels=labels,autopct='%1.1f%%', shadow=True)\nplt.title('diagnosis ', fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = \"diagnosis\"\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0], marker=dict(colors=[\"#6ad49b\", \"#a678de\"]))\nlayout = go.Layout(title=\"\", height=600, legend=dict(x=0.1, y=1.1))\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To find the missing value in dataset","metadata":{}},{"cell_type":"code","source":"for i in df.columns:\n    null_rate=df[i].isnull().sum()/len(df)\n    if null_rate>0:\n        print(\"{}'s null rate {} %'\".format(i, round(null_rate,2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TSNE visualization ","metadata":{}},{"cell_type":"code","source":"X=df.drop(['diagnosis','Unnamed: 32'], axis=1)\ny=df['diagnosis']","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(10, 10))\n    for lab, col in zip(('M', 'B' ),\n                        ('blue', 'red')):\n        plt.scatter(Y_sklearn[y==lab, 0],\n                    Y_sklearn[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='upper center')\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Buble chart","metadata":{}},{"cell_type":"code","source":"fig=px.scatter_3d(x=df['radius_mean'], y=df['texture_mean'], z=df['area_mean'], size=df['texture_mean'],\n                 hover_data=[df['diagnosis']])\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=px.scatter(x=df['radius_mean'], y=df['perimeter_mean'], color=df['diagnosis'],size=df['fractal_dimension_worst'], hover_name=df['diagnosis'], \n                size_max=60)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obs**- these are the linear relationship ","metadata":{}},{"cell_type":"code","source":"fig=px.scatter(x=df['radius_mean'], y=df['perimeter_mean'], color=df['diagnosis'],size=df['fractal_dimension_worst'], hover_name=df['diagnosis'], \n                size_max=60)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=px.scatter(x=df['radius_mean'], y=df['smoothness_mean'], color=df['diagnosis'],size=df['fractal_dimension_worst'], hover_name=df['diagnosis'], \n             size_max=20)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obs** - These are the scatter diagram ","metadata":{}},{"cell_type":"code","source":"fig=px.scatter(x=df['radius_mean'], y=df['compactness_mean'], color=df['diagnosis'],size=df['fractal_dimension_worst'], hover_name=df['diagnosis'], \n             size_max=20)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=px.scatter(x=df['radius_mean'], y=df['concavity_mean'], color=df['diagnosis'],size=df['fractal_dimension_worst'], hover_name=df['diagnosis'], \n             size_max=20)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.kdeplot(df['radius_mean'], hue=df['diagnosis'],shade=True, legend=True)\nplt.show()\n\nsns.boxenplot(x=df['diagnosis'], y=df['radius_mean'], hue=df['diagnosis'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.subplot(2,1,1)\nsns.kdeplot(x=df['radius_mean'],hue=df['diagnosis'], shade=True, vertical=False, kernel=str, gridsize=100, legend=True,\n           shade_lowest=True,cbar=True)\nplt.title(\"Radius vs Diagnonis\")\n#plt.xlabel(\"Counts\")\nplt.show()\n\nplt.subplot(2,1,2)\nsns.boxenplot(y=df['radius_mean'],x=df['diagnosis'])\nplt.title(\"Boxenplot\")\n#plt.xlabel(\"Counts\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Dropping the less important feature ","metadata":{}},{"cell_type":"markdown","source":"# Visualization\nit is import to see that counts of different type of cancer \n","metadata":{}},{"cell_type":"markdown","source":">Pairplot helps to plot among the most useful feature","metadata":{}},{"cell_type":"code","source":"cols=['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\nplt.figure(figsize=(10,10))\nsns.pairplot(data=df[cols],hue='diagnosis', palette='RdBu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heatmap\nTo find the most correlated features","metadata":{}},{"cell_type":"code","source":"#generate the corellation matrix \ncorr=df.corr().round(2)\n#mask for the upper triangle\nmask=np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)]\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n#define custom colormap\ncmap=sns.diverging_palette(220,10, as_cmap=True)\n\n#draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Matrix","metadata":{}},{"cell_type":"code","source":"# Generate and visualize the correlation matrix\ncorr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dropping less important feature","metadata":{}},{"cell_type":"code","source":"# first, drop all \"worst\" columns\ncols = ['radius_worst', \n        'texture_worst', \n        'perimeter_worst', \n        'area_worst', \n        'smoothness_worst', \n        'compactness_worst', \n        'concavity_worst',\n        'concave points_worst', \n        'symmetry_worst', \n        'fractal_dimension_worst']\n\ndf=df.drop(cols, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then, drop all columns related to the \"perimeter\" and \"area\" attributes\ncols = ['perimeter_mean',\n        'perimeter_se', \n        'area_mean', \n        'area_se']\n\ndf=df.drop(cols, axis=1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lastly, drop all columns related to the \"concavity\" and \"concave points\" attributes\ncols = ['concavity_mean',\n        'concavity_se', \n        'concave points_mean', \n        'concave points_se']\ndf = df.drop(cols, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Matrix for the most important features","metadata":{}},{"cell_type":"code","source":"# Generate and visualize the correlation matrix\ncorr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the dataset ","metadata":{}},{"cell_type":"code","source":"diagnosis={'M':1, 'B':0}\ndf['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX=df.drop('Unnamed: 32', axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Model Training ","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=LogisticRegression()\nlr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=lr.predict(X_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred=lr.predict(x_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm=confusion_matrix(y_test, y_test_pred)\ntrueNegative=cfm[0][0]\nfalsePossitive=cfm[0][1]\nfalse_negative=cfm[1][0]\ntruePositive=cfm[1][1]\n\nprint(\"Confusion Matrix\", cfm)\nprint(\"true negative\", trueNegative)\nprint(\"False Positive\", falsePossitive)\nprint(\"false Negative\", false_negative)\nprint(\"True Positive\", truePositive)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"correct prediction\", \n      round((trueNegative+truePositive)/len(y_test_pred)*100, 1),'%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfm_df=pd.DataFrame(cfm, range(2), range(2))\nplt.figure(figsize=(10,10))\nsns.heatmap(cfm_df, cmap='Reds', annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(y_test, y_test_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred_prob=lr.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.roc_auc_score(y_test, y_test_pred_prob)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_skill=len(y==1)/len(y)\ny_test_prob=lr.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Logistic Regression\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **The Prediction**","metadata":{}},{"cell_type":"markdown","source":"n the previous section, we have successfully developed a logistic regression model. This model can take some unlabeled data and effectively assign each observation a probability ranging from 0 to 1. This is the key feature of a logistic regression model. However, for us to evaluate whether the predictions are accurate, the predictions must be encoded so that each instance can be compared directly with the labels in the test data. In other words, instead of numbers between 0 or 1, the predictions should show \"M\" or \"B\", denoting malignant and benign respectively. In our model, a probability of 1 corresponds to the \"Benign\" class, whereas a probability of 0 corresponds to the \"Malignant\" class. Therefore, we can apply a threshhold value of 0.5 to our predictions, assigning all values closer to 0 a label of \"M\" and assigniing all values closer to 1 a label of \"B\".\n\nIf this is confusiing, let's go through this step-by-step.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# predict the test data and show the first 5 predictions\npredict=lr.predict(x_test)\npredict[1:6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert the numericalinto nominal value and check the few result\n\nprediction_nominal=['M' if x<0.1 else 'B' for x in predict ]\nprediction_nominal[1:6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we will do that the probability closer to 0 will be labeled as 'M' and 'B' otherwise . And classify them as per stated here","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n>The accuracy labeled is 92.4% which is good .Though it is not that pretty good but still we can say tha prediction on this kind of dataset with the help of Logistic Regression is easy and effective . Hence it is a kind of achievment","metadata":{}}]}