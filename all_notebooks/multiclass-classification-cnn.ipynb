{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation steps\n\n# I. Import necessary modules\n\n* `import os`: to read files from disk\n* `import cv2`: OpenCV is an image and video processing library  \n* `import matplotlib.pyplot as plt` and `import seaborn as sn`: to show images and graphs\n* `from random import randint`: to show random images\n* `import numpy as np`: linear algebra; all images will represent numpy-arrays.\n* other modules and their components will be imported later.","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport cv2\nfrom random import randint\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I. Check data \n\n","metadata":{}},{"cell_type":"code","source":"CLASSES, gems = [], [] # names of classes, count of images for each class\n\nfor root, dirs, files in os.walk('../input/imagenette/imagenette'):\n    f = os.path.basename(root)    # get class name - \n    if len(files) > 0:\n        gems.append(len(files))\n        if f not in CLASSES:\n            CLASSES.append(f) # add folder name\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# II. Prepare training data\n\n## 1. Prepare parameters\n\n* resize processed images to `img_w, img_h` - this option will be used when cropped and as a parameter of neural network; \n* provide train directory path.","metadata":{}},{"cell_type":"code","source":"img_w, img_h = 220, 220    # width and height of image\ntrain_dir = '../input/imagenette/imagenette/train/'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Create function which reads images and class names\n","metadata":{}},{"cell_type":"code","source":"def read_imgs_lbls(_dir):\n    Images, Labels = [], []\n    for root, dirs, files in os.walk(_dir):\n        f = os.path.basename(root) \n        for file in files:\n            Labels.append(f)\n            try:\n                image = cv2.imread(root+'/'+file)              # read the image (OpenCV)\n                image = cv2.resize(image,(int(img_w*1), int(img_h*1)))       # resize the image (images are different sizes)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converts an image from BGR color space to RGB\n                Images.append(image)\n            except Exception as e:\n                print(e)\n    Images = np.array(Images)\n    return (Images, Labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Create function which converts string labels to numbers\n* Convert string labels to a list of numbers using list `CLASSES`. The index will represent label of class\n* when `Labels` list is ready - convert it to Numpy array.","metadata":{}},{"cell_type":"code","source":"def get_class_index(Labels):\n    for i, n in enumerate(Labels):\n        for j, k in enumerate(CLASSES):    # foreach CLASSES\n            if n == k:\n                Labels[i] = j\n    Labels = np.array(Labels)\n    return Labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Fill arrays of Images and corresponding Labels with data\n* Create two arrays `Train_Imgs, Train_Lbls` \n* Convert `Train_Lbls` with strings to list with corresponding numbers;\n* print the dimensions of both numpy arrays: `Train_Imgs` which stores pictures is 4-dimensional: **Number of images x Width of image x Height of image x Channel of image**.","metadata":{}},{"cell_type":"code","source":"Train_Imgs, Train_Lbls = read_imgs_lbls(train_dir)\nTrain_Lbls = get_class_index(Train_Lbls)\nprint('Shape of train images: {}'.format(Train_Imgs.shape))\nprint('Shape of train labels: {}'.format(Train_Lbls.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_Imgs = Train_Imgs[:1000]\nTrain_Lbls = Train_Lbls[:1000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Plot images and their labels for preview\n* Using `matplotlib` and `random` show 16 (4x4) random images from the set and their labels (as string and as int number).","metadata":{}},{"cell_type":"code","source":"dim = 4 #you can change it;  4x4 dimension flat plot\n\nf,ax = plt.subplots(dim,dim) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,dim):\n    for j in range(0,dim):\n        rnd_number = randint(0,len(Train_Imgs))\n        cl = Train_Lbls[rnd_number]\n        ax[i,j].imshow(Train_Imgs[rnd_number])\n        ax[i,j].set_title(CLASSES[cl]+': ' + str(cl))\n        ax[i,j].axis('off')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Crop edges of images using Canny algorithm\n> Canny is a popular edge detection algorithm, which detects the edges of objects present in an image.\n","metadata":{}},{"cell_type":"code","source":"def edge_and_cut(img):\n    try:\n        edges = cv2.Canny(img, img_w, img_h)            \n        \n        if(np.count_nonzero(edges)>edges.size/10000):           \n            pts = np.argwhere(edges>0)\n            y1,x1 = pts.min(axis=0)\n            y2,x2 = pts.max(axis=0)\n            \n            new_img = img[y1:y2, x1:x2]           # crop the region\n            new_img = cv2.resize(new_img,(img_w, img_h))  # Convert back\n        else:\n            new_img = cv2.resize(img,(img_w, img_h))\n    \n    except Exception as e:\n        print(e)\n        new_img = cv2.resize(img,(img_w, img_h))\n    \n    return new_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Show cropped images\n* Function `show_cropped` is kind-of duplicate `edge_and_cut()`: it shows same random examples of Canny algo work: **original image, Canny edges, image with bounding box, cropped image** for better understanding how Canny algo works.","metadata":{}},{"cell_type":"code","source":"def show_cropped(img):\n    emb_img = img.copy()\n    edges = cv2.Canny(img, img_w, img_h)\n    \n    if(np.count_nonzero(edges)>edges.size/10000):\n        pts = np.argwhere(edges>0)\n        y1,x1 = pts.min(axis=0)\n        y2,x2 = pts.max(axis=0)\n\n        new_img = img[y1:y2, x1:x2]  \n\n        edge_size = 1 #replace it with bigger size for larger images            \n\n        emb_img[y1-edge_size:y1+edge_size, x1:x2] = [255, 0, 0]\n        emb_img[y2-edge_size:y2+edge_size, x1:x2] = [255, 0, 0]\n        emb_img[y1:y2, x1-edge_size:x1+edge_size] = [255, 0, 0]\n        emb_img[y1:y2, x2-edge_size:x2+edge_size] = [255, 0, 0]\n\n        new_img = cv2.resize(new_img,(img_w, img_h))  # Convert to primary size  \n        \n    else:\n        new_img = cv2.resize(img,(img_w, img_h))\n    \n    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n    ax[0].imshow(img, cmap='gray')\n    ax[0].set_title('Original Image', fontsize=14)\n    ax[1].imshow(edges, cmap='gray')\n    ax[1].set_title('Canny Edges', fontsize=14)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('Bounding Box', fontsize=14)       \n    ax[3].imshow(new_img, cmap='gray')\n    ax[3].set_title('Cropped', fontsize=14)   ","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in range(0,3):\n    show_cropped(Train_Imgs[randint(0,len(Train_Imgs))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Replace train images with cropped images\n","metadata":{}},{"cell_type":"code","source":"def crop_images(Imgs):\n    CroppedImages = np.ndarray(shape=(len(Imgs), img_w, img_h, 3), dtype=np.int)\n\n    ind = 0\n    for im in Imgs: \n        x = edge_and_cut(im)\n        CroppedImages[ind] = x\n        ind += 1\n\n    return CroppedImages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_Imgs = crop_images(Train_Imgs)\n#print('Final shape of images in train set: {} '.format(Train_Imgs.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Split data into train and validation sets\n* use `sklearn` to split `Train_Imgs`, `Train_Lbls` into train (80%) and validation (20%) sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(Train_Imgs, Train_Lbls, shuffle = True, test_size = 0.2, random_state = 42)\nprint('Shape of X_train: {}, y_train: {} '.format(X_train.shape, y_train.shape))\nprint('Shape of X_val: {}, y_val: {} '.format(X_val.shape, y_val.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# III. Prepare for model creation\n## 1. Check devices\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevices = device_lib.list_local_devices()\nprint(devices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Import keras\nKeras is an open-source neural-network library written in Python which is capable of running on top of **TensorFlow**.\nFrom Keras needed:\n* `models` - type of models, import only `Sequential` \n* `layers` - layers corresponding to our model: as it a simple one take only `Conv2D`, `MaxPooling2D` and `AveragePooling2D`\n* `optimizers` - contains back propagation algorithms\n* `ImageDataGenerator` - for image augmenation (there are not so many samples of each class)","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import optimizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build a simple CNN  \n","metadata":{}},{"cell_type":"code","source":"filters = 32      # the dimensionality of the output space\nkernel_size = 3   # length of the 2D convolution window\nmax_pool = 2      # size of the max pooling windows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Parameters to fit the model\n","metadata":{}},{"cell_type":"code","source":"EPOCHS = 35                                  # while testing you can change it\nbatch_size = 32                              # number of training samples using in each mini batch during GD (gradient descent) \niter_per_epoch = len(X_train) // batch_size  # each sample will be passed [iter_per_epoch] times during training\nval_per_epoch = len(X_val) // batch_size     # each sample will be passed [val_per_epoch] times during validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# II. Provide a model\n\n## 1. Architect a model\n\n","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\n# first layer\nmodel.add(Conv2D(batch_size, (kernel_size, kernel_size), activation='relu', padding='same', input_shape=(img_w, img_h, 3))) # 32\nmodel.add(MaxPooling2D((max_pool, max_pool))) #reduce the spatial size of incoming features\n\n# second layer\nmodel.add(Conv2D(2*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 64\nmodel.add(MaxPooling2D((max_pool, max_pool))) \n\n# third layer\nmodel.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel.add(MaxPooling2D((max_pool, max_pool))) \n\n# fourth layer\nmodel.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel.add(AveragePooling2D(pool_size= (2, 2), strides= (2, 2))) \n\n# fifth layer\nmodel.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel.add(MaxPooling2D((max_pool, max_pool))) \n\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16*batch_size, activation='relu'))                                             # 512\nmodel.add(Dense(11, activation='softmax'))\nmodel.summary()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model summary shows that there are more than 2M parameters to train and the information about different layers.\n\n## 2. Compile a model\n* Compile the model using `adam` optimizer which is a generalization of stochastic gradient descent (SGD) algo. Provided loss function is `sparse_categorical_crossentropy` as we are doing multiclass classification.   ","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# III. Fit the train generator\n\n## 1. Image augmentation\n> **Image augmentation** is a creation of additional training data based on existing images, for example translation, rotation, flips and zoom.\n","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(              # this is the augmentation configuration used for training\n        rotation_range=25,\n        zoom_range=0.1,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.2,\n        horizontal_flip=True\n        )\n\nval_datagen = ImageDataGenerator()                # for val/testing only rescaling function ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* the original image + examples of work of `ImageDataGenerator`: ","metadata":{}},{"cell_type":"code","source":"n = randint(0,len(X_train))\nsamples = np.expand_dims(X_train[n], 0)\nit = train_datagen.flow(samples, batch_size=batch_size)\ncols = 7\n\nfig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15, 10))\nax[0].imshow(X_train[n], cmap='gray')\nax[0].set_title('Original', fontsize=10)\n\nfor i in range(1,cols):\n    batch = it.next()    # generate batch of images \n    image = batch[0].astype('uint32') # convert to unsigned int for viewing\n    ax[i].set_title('augmented {}'.format(i), fontsize=10)\n    ax[i].imshow(image, cmap='gray')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create two numpy array iterators `train_gen` and `val_gen` and fill them with additional images:","metadata":{}},{"cell_type":"code","source":"train_gen = train_datagen.flow(X_train, y_train, batch_size=batch_size)\nval_gen = val_datagen.flow(X_val, y_val, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Fit the model\n","metadata":{}},{"cell_type":"code","source":"m = model.fit_generator(\n       train_gen,\n       steps_per_epoch= iter_per_epoch,\n       epochs=EPOCHS, \n       validation_data = val_gen,\n       validation_steps = val_per_epoch,\n       verbose = 1 # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n## 3. Check the accuracy\n\n* plot the accuracy of model against size of epoch (train and val);\n* plot the loss of model against size of epoch (train and val).","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxs[0].plot(m.history['accuracy'])\naxs[0].plot(m.history['val_accuracy'])\naxs[0].set_title('Model accuracy')\naxs[0].legend(['Train', 'Val'], loc='upper left')\n\naxs[1].plot(m.history['loss'])\naxs[1].plot(m.history['val_loss'])\naxs[1].set_title('Model loss')\naxs[1].legend(['Train', 'Val'], loc='upper left')\n\nfor ax in axs.flat:\n    ax.set(xlabel='Epoch')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Score the model\n> Accuracy is a metric for evaluating classification models. `Accuracy  = Number of right predictions / Total number of predictions`.\n\n* Function `evaluate_generator` evaluates the model on a data generator: `score` is a list of scalars (loss and accuracy).","metadata":{}},{"cell_type":"code","source":"score = model.evaluate_generator(val_gen, steps= len(val_gen))\n\nfor idx, metric in enumerate(model.metrics_names):\n    print('{}:{}'.format(metric, score[idx]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Confusion matrix   \n> Confusion matrix can be pretty useful when evaluating multiclass classifications.   \n\n* `from sklearn.metrics import confusion_matrix`: Diagonal of matrix should be mostly filled with numbers.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pre_test=model.predict(X_val)\ny_pre_test=np.argmax(y_pre_test,axis=1)\ncm=confusion_matrix(y_val,y_pre_test)\n\nplt.figure(figsize = (15,15))\nsn.heatmap(cm, annot=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"x=(y_pre_test-y_val!=0).tolist()\nx=[i for i,l in enumerate(x) if l!=False]\n\nfig,ax=plt.subplots(1,5,sharey=False,figsize=(13,13))\nfig.tight_layout()\n\nfor i in range(5):\n    ax[i].imshow(X_val[x[i]][:,:,1])\n    ax[i].set_xlabel('{}, Pred: {}'.format(CLASSES[y_val[x[i]]],CLASSES[y_pre_test[x[i]]]))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Save the model\n* Save weights to reuse them instead of training again. Keras function `save` creates h5 file with weights. Use `new_model.load_weights('model_task2.h5')` to reuse it in other models.","metadata":{}},{"cell_type":"code","source":"model.save('model_task2.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IV. Evaluate on testing folder\n\n## 1. Get samples from test folder\n","metadata":{}},{"cell_type":"code","source":"test_dir = '../input/imagenette/imagenette/val/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Test_Imgs, Test_Lbls = read_imgs_lbls(test_dir)\nTest_Lbls = get_class_index(Test_Lbls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Crop test images","metadata":{}},{"cell_type":"code","source":"Test_Imgs = crop_images(Test_Imgs)\nprint('shape of images in test set: {} '.format(Test_Imgs.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Plot test images and model predictions\nPlot image from test folder with a label, class which model predicted, and actual class.","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(5,5) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,5,1):\n    for j in range(0,5,1):\n        rnd_number = randint(0,len(Test_Imgs))\n        pred_image = np.array([Test_Imgs[rnd_number]])\n        pred_class = model.predict_classes(pred_image)[0]\n        pred_prob = model.predict(pred_image).reshape(11)\n        act = CLASSES[Test_Lbls[rnd_number]]\n        ax[i,j].imshow(Test_Imgs[rnd_number])\n        ax[i,j].imshow(pred_image[0])\n        if(CLASSES[pred_class] != CLASSES[Test_Lbls[rnd_number]]):\n            t = '{} [{}]'.format(CLASSES[pred_class], CLASSES[Test_Lbls[rnd_number]])\n            ax[i,j].set_title(t, fontdict={'color': 'darkred'})\n        else:\n            t = '[OK] {}'.format(CLASSES[pred_class]) \n            ax[i,j].set_title(t)\n        ax[i,j].axis('off')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}