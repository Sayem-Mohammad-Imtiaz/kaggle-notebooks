{"cells":[{"metadata":{},"cell_type":"markdown","source":"HI Everyone,\n\nIn this tutorial, we are going to do a complete data science life cycle. We have a loan application dataset. It has many features which helps banking team to decide whether the user is eligible for loan or not.\nIt is the case of classification problem with 2 classes ( Approved or Not Appoved). \n\nWhat we will learn in here:\n1. Desctriptive Analysis\n2. Basic Statistics\n3. Missing Value Handling\n4. Encoding Categorical Features\n5. Visualization & Story Telling\n6. Train Test Split\n7. Random Forest Algorithm\n8. Model Tuning using GridSearch and k-fold\n9. Performance Metics.\n "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load & Explore Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/loan-predication/train_u6lujuX_CVtuZ9i (1).csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head() # top 5 reacords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() # Find total number of missing ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{},"cell_type":"markdown","source":"Before fixing all the missing values lets just explore our data\nSo data visualization is very effective method to get info from any kind of data.\nbut before doing anything we have to think one thing..... What are we going to find????\nOur findings will be:\n1. How many males or females apply for the loans?\n2. How many graduate or non-graduated male or female apply for the loan?\n3. Is self employed people are most likely to get loan?\n4. What kind of male or females apply for loan, married or unmarried?\n\nSo i think these 4 finding will be enough for you to explain a real case study."},{"metadata":{},"cell_type":"markdown","source":"## Q1. How many males or females apply for the loans?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to visualize our data we will use matplotlib\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To answer this data we will use value_counts to find uot gender counts.\ndf.Gender.value_counts() # So we can see mostly Males are applying for the loan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets plot it\ndf.Gender.value_counts().plot(kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can also plot the percentage\ndf.Gender.value_counts(normalize=True).plot(kind=\"bar\"); # so we can see almost 80 loan applicants are Males.\n# So here we find out our first answer.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Q1. How many males or females apply for the loans?\nAns. 81.3% Males, 18.6% Females"},{"metadata":{},"cell_type":"markdown","source":"## Q2. How many graduate or non-graduated male or female apply for the loan?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# So in this case we have to group our data according to gender and education\n df.groupby([\"Gender\", \"Education\"])[\"Loan_Status\"].count()\n    # So we can see there are 92 graduated females & 20 Non-Graduated females","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby([\"Gender\", \"Education\"])[\"Loan_ID\"].count().plot(kind=\"bar\");\n# So we can answer here our second question.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q3.Is self employed people are most likely to get loan?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check whether selfemployed people are more likely to get loan?\ndf.groupby([\"Self_Employed\", \"Loan_Status\"])[\"Loan_ID\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby([\"Self_Employed\", \"Loan_Status\"])[\"Loan_ID\"].count().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So answer 3rd is according to our data \n# So we can see most of the approvals are in account of the persons who is not self employed.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q4. What kind of male or females apply for loan, married or unmarried?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby([\"Gender\", \"Married\"])[\"Loan_ID\"].count().plot(kind=\"bar\");\n# So we can see mostly married males apply for loan.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So till now we answered our 4 questions. Thoough in real life you will find even more complicate questions. But still\n# its a good start.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# So finlly we reached to this step. It is very important and interesting step.\n# Lets check again how many null values we have\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_gen_dum = pd.get_dummies(df.Gender)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.LoanAmount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets replace gender with most frequent or mode value as it is a categorical feature\nmode_gen = df.Gender.value_counts().idxmax()# so we can see Male is the most common value so lets replace with it\ndf.Gender.fillna(mode_gen , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() # See now gender column doesnt have any mssing value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do same for the married column\ndf[ \"Married\"].fillna(df.Married.value_counts().idxmax(), inplace=True) # here all missing values are replace with \"YES\" as\n# It is the most frequent value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do it for dependent column.\ndf.Dependents.value_counts().idxmax() # We can see mostly people has no dependent. So we wil use that value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dependents.fillna(df.Dependents.value_counts().idxmax(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Self_Employed.value_counts().idxmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Self_Employed.fillna(df.Self_Employed.value_counts().idxmax(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now lets see for loan amount\ndf.LoanAmount.plot(\"hist\") # so we can see loan amount is float value. Let plot it first","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.LoanAmount.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So from our histograme we can see mostly loan amount lies between 100-150 and our mean value is 146. Which means \n# we can replace our missing values with mean in this case.\ndf.LoanAmount.fillna(df.LoanAmount.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loan Term column is the number of days for the loan. So here we can not use mean value as mean value probably gives us \n# decimal values and days are obvously not in decimals. So here also lets see what is the most common value.\n# We can see 85% cases term value is 360. So we will replace it with this only\ndf.Loan_Amount_Term.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Loan_Amount_Term.fillna(df.Loan_Amount_Term.value_counts().idxmax(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Credit_History.value_counts() # again we can see in case of credit history also we can only have two values\n# and mostl we have 1 value. So lets do i.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Credit_History.fillna(df.Credit_History.value_counts().idxmax(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() # so now finally we have replaced all the missing values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see there are few categrical featues ike Gender, Married, Education, Self_Employed, Property_Area, Load_Status\n# As machine learning algorithm only understands numerical values. So we have to change them into numeric values.\n# to do that we add dummy variables for each categorical value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before everythning lets delete Loan_ID column as we do not need it.\ndf.drop([\"Loan_ID\"],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do it\ndummy_gen = pd.get_dummies(df.Gender) # So we can see get_dummies() change our categorical values into dummy variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_gen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets add these values to our main df\ndf = pd.concat([dummy_gen, df],  axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()  # So we can see two new columns added to our dataset. So we dont need Gender column anymore.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also here there is one more problem. As we can see MAle & Female column are anti. Mean If Female =1 Then Male =0 vice versa.\nSo we have to do delete one otherwise we will face dummy variable trap. \nSo lets delete Female & Gender column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"Female\", \"Gender\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now notice if we convert married column & Self_Employed  Column to dummy variable, \n# We will get same column name so it will be a problem & confusing.\ndf.Married.replace(\"No\", \"Unmarried\", inplace=True)\ndf.Married.replace(\"Yes\", \"Married\", inplace=True)\n\ndf.Self_Employed.replace(\"No\", \"Job\", inplace=True)\ndf.Self_Employed.replace(\"Yes\", \"Business\", inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head() # so now our problem solved lets continuee..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do same for other columns. Dummy variable idea is like if u have n dummy variable take only n-1. In or case\n# 2 columns were there Male, Female. We took only one.\n# Lets apply for Married.\ndummy_mar = pd.get_dummies(df.Married)\ndf.drop([\"Married\"], axis=1, inplace=True) # delete the original married column\ndf = pd.concat([dummy_mar, df], axis=1)\ndf.drop([\"Unmarried\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Sel_Employed Column\ndummy_emp = pd.get_dummies(df.Self_Employed)\ndf = pd.concat([dummy_emp, df], axis=1)\ndf.drop([\"Business\", \"Self_Employed\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For `education Column\ndummy_edu = pd.get_dummies(df.Education)\ndf = pd.concat([dummy_edu, df], axis=1)\ndf.drop([\"Not Graduate\", \"Education\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For `Property_Area Column\ndummy_prop = pd.get_dummies(df.Property_Area)\ndf = pd.concat([dummy_prop, df], axis=1)\ndf.drop([\"Rural\", \"Property_Area\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one more problem i just found in dependent column\ndf.Dependents.value_counts() # you can see there is 3+ vlaue which is string. So lets change it with 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dependents.replace(\"3+\", 3, inplace=True) \ndf.Dependents =df.Dependents.astype(\"int\")  # change the column type to integer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dependents.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# well now we have replaced all the cateorical features to numerical values.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature & Target Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, 0:-1].values\ny = df.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we can see our y is still in categorical form. Let replace it also but with the help of labelencode class.\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y # So you can see now our y is also encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now scaling means every data is in same range. But if you see continoues variables in our dataset. ApplicantIncome has a huge\n# range if you compare to CoapplicantIncome & LoanAmount. So it is better to scale them to the same range. \n# We can do it wi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler_ApplicantIncome = StandardScaler()\nX = scaler_ApplicantIncome.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X # so we can see our data is now scaled.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train & Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 1 we will calculate vairability of all the columns so that we can identify the principal components.\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest - Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(CV_rfc.best_params_) # So these are our best parameters\nprint(CV_rfc.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=5, criterion='entropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = rfc1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cf = confusion_matrix(y_test, y_predict) # confusion Matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc1.score(X_test, y_test) # 81% accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks You..."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}