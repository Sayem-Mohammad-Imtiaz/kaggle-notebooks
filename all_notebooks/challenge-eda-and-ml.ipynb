{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Challenge START&GO Data Science 2020 \"Botando pra Quebrar\" \n\nHello! We are Andreis Purim and Eduarda Agostini, we're two brazilian students in currently doing a double-degree program at the Ã‰cole Centrale de Lille, in France. For our challenge in Data Science, we chose to make a mix of NLP algorithms in multiple intersting datasets.\n\nAll documentation in this notebook will be in english (though we will be presenting our project in French) because we believe it may be intersting to a wider audience. I hope you like it.\n\n# 1. IMDB\nOur first challenge will be to make a Machine Learning classifier for the 50k movie reviews IMDB Dataset. The dataset itself is very simple: the review and the sentiment (positive or negative).\n\n## 1.1. Visualizing our Data\nLet's start by plotting some graphs and of course, seeing how our data works."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nimport pandas\n%matplotlib inline\n\nReviews = pandas.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nprint(Reviews.shape)\nprint(Reviews.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's usa the spaCy library to take a look at how our phrases are constructed. spaCy is a beautifully constructed library for NLP that has a pretrained statistical models in various languages. We could use the starter models to make a transfer learning, but for now let's just use the complete model to see how it fares.\n\nBy loading the English core pretarined models, we can use it to deconstruct the phrase and see every part of it (with explanations!). Let's choose our second review, in this case, I don't want to print all words (because it'd be too huge), so I made a zip with range(20), in case you want to observe all words, just make a for in Chosen_Sentence\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\nSpacy = spacy.load('en_core_web_sm')\n\nChosen_Sentence = Spacy(Reviews['review'][1])\nfor i,word in zip(range(20),Chosen_Sentence):\n    print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note how our data still needs some cleaning. For example, there are HTML tags (like <br/>) which spaCy classifies as \"superfluous punctuation\". We'll clean the text before starting our models.\n\nAnother beautiful thing about spaCy is displaCy, displaCy is a visualizer which not only makes not only visualizing dependencies in NLP fun but also very helpful for us."},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy import displacy\n\ndisplacy.render(Chosen_Sentence, style='dep', jupyter=True, options={'distance': 50})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another powerful thing spaCy can do is to identify entities in the text, like organizations, people, nationalities, etc... you can use displaCy again to visualize the entities in the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"for entity in Chosen_Sentence.ents:\n    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))\n\ndisplacy.render(Chosen_Sentence, style='ent', jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so let's clear our dataset a little and see some graphics."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n# First, let's make a small function to clean our strings, because as we have seen before, there are tons of unwanted punctuations and other useless tags\ndef clear_sentence(sentence: str) -> str:\n    '''A function to clear texts using regex.'''\n    sentence = re.sub(r'\\W', ' ', str(sentence))\n    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence) \n    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n    sentence = re.sub(r'^b\\s+', '', sentence)\n    sentence = sentence.lower()\n    return sentence\n\n# Clears every sentence in review.\nReviews['review'] = [clear_sentence(sentence) for sentence in Reviews['review']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.2 Choosing algorithms for our IMDB\nOk, now that we know how our data looks like, let's choose some Machine Learning algorithms to work with our data. The first thing we need to do is to reduce our dataset (because some of these algorithms can take quite a while), so we'll be using only the first 5000 reviews.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nimport matplotlib.pyplot as matplotlib\nfrom matplotlib.lines import Line2D\nfrom xgboost import XGBClassifier\nimport seaborn\nimport time\n\n\n# Now, let's get a small sample of our reviews\nReviews_small = Reviews[0:5000]\nReviews_small\n\n# Makes two datasets, x and y, x will be the clear reviews and y will be the sentiment\nx_small = Reviews_small['review'].tolist()\ny_small = Reviews_small['sentiment'].tolist()\n\n# Split the dataset in a 80%/20% fashion\nX_train, X_test, y_train, y_test = train_test_split(x_small, y_small, test_size=0.2, random_state=0)\n\n\n# I'm making two dictionaries, one for models to transform our words in vectors and the other of models to work on these vectors\nVectorizer_Models = {\n    'Count': CountVectorizer(stop_words=\"english\"),\n    'Hash': HashingVectorizer(stop_words=\"english\"),\n    'Tfidf': TfidfVectorizer(stop_words=\"english\",ngram_range=(1, 2))\n}\n\nML_Models = {\n    'LinearSVC': LinearSVC(),\n    'SVC': SVC(),\n    'NuSVC': NuSVC(),\n    'DecisionTree': DecisionTreeClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=0),\n    'SGDC': SGDClassifier(),\n    'MultiNB': MultinomialNB(),    \n}\n\n# I'll make a new list for a future dataset to see the accuracy and time of each algorithm.\nplotting_data = []\n\nfor j,vector in enumerate(Vectorizer_Models):\n    \n    # Let's first vectorize, because the vectorized words will be used in common by all MLs. Also, starts counting the time to vectorize.\n    time_vector_start = time.time()\n    X_train_vectorized = Vectorizer_Models[vector].fit_transform(X_train) \n    X_test_vectorized= Vectorizer_Models[vector].transform(X_test)\n    time_vector_end = time.time()\n    \n    for i,ml in enumerate(ML_Models):\n        \n        # Small detail: Multinomial Naive-Baise does not work with negative numbers, so we can just use him with Count\n        if (ml == 'MultiNB' and vector != 'Count') == False:\n            # Ok, let's start the time and put our models to fit the data.\n            starting_time = time.time()\n            model = ML_Models[ml]\n            model.fit(X_train_vectorized, y_train)\n\n            # Predict the data and try to find the accuracy\n            y_predicted = model.predict(X_test_vectorized)\n            accuracy = accuracy_score(y_test, y_predicted)\n            ending_time = time.time()\n\n            # Now, get the times and append everything in our plotting data.\n            cut_time = round(time_vector_end - time_vector_start,2)\n            ml_time = round(ending_time - starting_time,2)\n            plotting_data.append([ml,vector,accuracy,ml_time,cut_time,cut_time+ml_time])\n\n\n# Makes a pandas dataset for our data (for better visualization)\nplot_times = pandas.DataFrame(plotting_data, columns=['ML','Vectorizer','Accuracy','ML_Time','Cut_time','Total_time'])\n\n# Now, let's make a Seaborn scatterplot\nseaborn.set(color_codes=True)\nmatplotlib.figure(figsize=(12, 8))\nmatplotlib.title(\"Best vectorization and Accuracy Algorithms\")\n\nax = seaborn.scatterplot(data=plot_times, x='Total_time', y='Accuracy', hue='ML', style='Vectorizer')\nmatplotlib.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nax.set(xlabel=\"Time (s)\", ylabel=\"Accuracy\")\nplot_times","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you have probably seen in the graph, the best algorithm is probably LinearSVC using Tfidf or Hash, with a small difference in accuracy and time, while things like Hash with XGB or RandomForest probably fared pretty bad in time.\n\nI won't explain in great detail why (if you google you'll probably find better answers) but this is because LinearSVC are Support Vector Machine, that is, machine learning algorithms made to use vectors as inputs, while RandomForest, while a very good algorithm, just can't handle vectors with hundreds of dimensions in a good time. In this case, you can see SGDClassifier rates a little higher than LinearSVC because SGD (Stochastic Gradient Descent) is a good approach of fitting linear classifiers in a manner similar to SVM.\n\nIn fact, our SGDClassifier is LinearSVM with some better training, as the docs in scikit state: \"Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. Often, an instance of SGDClassifier or SGDRegressor will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique.\"\n\nAnd for the vectorizes, you can see Count is way faster, Hash is a mix of fast and accurate, and Tfidf is accurate(r).\n\nBut, when we scale the data back to its 50.000 original size, you might notice LinearSVC might outscore SDGC."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [clear_sentence(sentence) for sentence in Reviews['review']]\ny = Reviews['sentiment'].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\nX_train_vectorized = Vectorizer_Models[vector].fit_transform(X_train) \nX_test_vectorized= Vectorizer_Models[vector].transform(X_test)\n\nChosen_Models = {\n    'LinearSVC': LinearSVC(),\n    'SGDC': SGDClassifier(),\n}\n\nfor ml in Chosen_Models:\n    starting_time = time.time()\n    model = Chosen_Models[ml]\n    model.fit(X_train_vectorized, y_train)\n    y_predicted = model.predict(X_test_vectorized)\n    accuracy = accuracy_score(y_test, y_predicted)\n    ending_time = time.time()\n    print(ml,'Accuracy:',\"{:.2f}\".format(accuracy*100),\"in\",\"{:.2f}s\".format(ending_time-starting_time))\n    print(confusion_matrix(y_test, y_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, let's stick to LinearSVC a little more and do one final thing: fine-tuning. Scikit comes with a nice tool called GridSearchCV that allows us to fine tune our model a little further.\n\nIdeas:\n```python\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n```\nand fine tune with\n```python\nfrom sklearn.model_selection import GridSearchCV\nGridSearchCV()\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fine tuning","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}