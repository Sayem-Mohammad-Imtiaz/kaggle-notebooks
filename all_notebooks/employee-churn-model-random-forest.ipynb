{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"#### Problem Statement\n\nUnderstanding why and when employees are most likely to leave can lead to actions to improve employee retention as well as possibly planning new hiring in advance. I will be usign a step-by-step systematic approach using a method that could be used for a variety of ML problems. This project would fall under what is commonly known as \"HR Anlytics\", \"People Analytics\".\n        \n#### Load Libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Basics\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, binarize, LabelEncoder\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n\n# Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Metrics \nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, auc, accuracy_score\n\n# Feature Selection\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Warnings\nimport warnings as ws\nws.filterwarnings('ignore')\n\npd.pandas.set_option('display.max_columns', None)\nsns.set_style(\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Dataset\ndata_hr = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata_hr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Description and Exploratory Visualisations\n\nIn this section, we will provide data visualizations that summarizes or extracts relevant characteristics of features in our dataset. Let's look at each column in detail, get a better understanding of the dataset, and group them together when appropriate."},{"metadata":{"trusted":true},"cell_type":"code","source":"print( 'DataSet Shape {}'.format(data_hr.shape))\n\ndata_hr.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary\ndef summary(data):\n    df = {\n     'Count' : data.shape[0],\n     'NA values' : data.isna().sum(),\n     '% NA' : round((data.isna().sum()/data.shape[0]) * 100, 2),\n     'Unique' : data.nunique(),\n     'Dtype' : data.dtypes\n    } \n    return(pd.DataFrame(df))\n\nprint('Shape is :', data_hr.shape)\nsummary(data_hr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here there is no missing value.  **EmployeeCount, Over18, StandardHours** has only one value and **EmployeeNumber** is nothing but ID. So that we can drop those columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data_hr.copy()\ndata.drop(['EmployeeCount', 'Over18', 'StandardHours','EmployeeNumber'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_var = [var for var in data if data[var].dtypes != 'O']\ncat_var = [var for var in data if data[var].dtypes == 'O']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 1. Numerical Variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"data[num_var].hist(bins = 25, figsize = (20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few observations can be made based on the information and histograms for numerical features:\n\n1. Many histograms are tail-heavy; indeed several distributions are right-skewed (e.g. MonthlyIncome DistanceFromHome, YearsAtCompany). Data transformation methods may be required to approach a normal distribution prior to fitting a model to the data.\n\n2. Age distribution is a slightly right-skewed normal distribution with the bulk of the staff between 25 and 45 years old.\n\n3. EmployeeCount and StandardHours are constant values for all employees. They're likely to be redundant features.\n\n4. Employee Number is likely to be a unique identifier for employees given the feature's quasi-uniform distribution.\n\nHere we can split continuous and discrete value for further analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_var = [var for var in num_var if len(data[var].unique()) > 10]\ndisc_var = [var for var in num_var if len(data[var].unique()) <= 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### a.continuous variable\n\nLet's create a kernel density estimation (KDE) plot colored by the value of the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (13,15))\ni = 0\nfor cont in cont_var:\n    \n    mu_yes = data[cont][data['Attrition'] == 'Yes'].mean()\n    mu_no = data[cont][data['Attrition'] == 'No'].mean()\n    \n    plt.subplot(5,3,i+1)\n    sns.kdeplot(data[cont][data['Attrition'] == 'Yes'], label = 'Yes (mean: {:.2f})'.format(mu_yes))\n    sns.kdeplot(data[cont][data['Attrition'] == 'No'], label = 'No (mean: {:.2f})'.format(mu_no))\n    plt.tight_layout()\n    plt.title('{} vs Attrition'.format(cont))\n    i+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### b. Discreate Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (13,15))\ni = 0\nfor disc in disc_var:\n    \n    j=0\n    col = ['Fields', '% of Leavers']\n    df_field = pd.DataFrame(columns = col)\n    \n    for field in list(data[disc].unique()):    \n        ratio = data[(data[disc] == field ) & (data['Attrition'] == 'Yes')].shape[0]/data[data[disc] == field].shape[0]\n        df_field.loc[j] = [field, ratio * 100]\n        j+=1\n    \n    plt.subplot(5,3,i+1)\n    sns.barplot(x = 'Fields', y = '% of Leavers', data = df_field)\n    plt.tight_layout()\n    plt.title('{} vs Attrition'.format(disc))\n    i+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Categorical Variables  "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (13,25))\ni = 0\nfor cat in cat_var[1:]:\n    \n    j=0\n    col = ['Fields', '% of Leavers']\n    df_field = pd.DataFrame(columns = col)\n    \n    for field in list(data[cat].unique()):    \n        ratio = data[(data[cat] == field ) & (data['Attrition'] == 'Yes')].shape[0]/data[data[cat] == field].shape[0]\n        df_field.loc[j] = [field, ratio * 100]\n        j+=1\n    \n    plt.subplot(5,3,i+1)\n    sns.barplot(x = 'Fields', y = '% of Leavers', data = df_field)\n    plt.tight_layout()\n    plt.xticks(rotation = 90)\n    plt.title('{} vs Attrition'.format(cat))\n    i+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Attrition Rate\nsns.countplot(x = 'Attrition', data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Correlation\nLet's take a look at some of most significant correlations. It is worth remembering that correlation coefficients only measure linear correlations."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Target'] = data['Attrition'].replace({'No':0,'Yes':1})\n\n# Find correlations with the target and sort\ncorr = data.corr()['Target'].sort_values()\n\nprint('-'*25)\nprint('Top 5 Positive Correlation')\nprint('-'*25)\nprint(corr.tail(5))\n\nprint('-'*25)\nprint('Top 5 Negative Correlation')\nprint('-'*25)\nprint(corr.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Map\ncorr = data.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n# Heatmap\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr, annot = True, fmt = '.2f', mask = mask, linewidths = 2, cmap=\"YlGnBu\", vmax = 0.5 )\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('Target',axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA Concluding Remarks\n\n\nLet's summarise the findings from this EDA:\n\n* The dataset does not feature any missing or erroneous data values, and all features are of the correct data type.\n* The strongest positive correlations with the target features are: Performance Rating, Monthly Rate, Num Companies Worked, Distance From Home.\n3. The strongest negative correlations with the target features are: Total Working Years, Job Level, Years In Current Role, and Monthly Income.\n4. The dataset is imbalanced with the majoriy of observations describing Currently Active Employees.\n5. Several features (ie columns) are redundant for our analysis, namely: EmployeeCount, EmployeeNumber, StandardHours, and Over18.\n6. Single employees show the largest proportion of leavers, compared to Married and Divorced counterparts.\n* Loyal employees with higher salaries and more responsbilities show lower proportion of leavers compared to their counterparts.\n9. People who live further away from their work show higher proportion of leavers compared to their counterparts.\n10. People who travel frequently show higher proportion of leavers compared to their counterparts.\n11. People who have to work overtime show higher proportion of leavers compared to their counterparts.\n12. Employee who work as Sales Representatives show a significant percentage of Leavers in the submitted dataset.\n13. Employees that have already worked at several companies previously (already \"bounced\" between workplaces) show higher proportion of leavers compared to their counterparts.\n\n\n### Pre-processing Pipeline\n\n#### Encoding\n\nMachine Learning algorithms can typically only have numerical values as their predictor variables. Hence Label Encoding becomes necessary as they encode categorical labels with numerical values. To avoid introducing feature importance for categorical features with large numbers of unique values, we will use both Lable Encoding and One-Hot Encoding as shown below.\n\n###### 1. Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_var = [var for var in cat_var if len(data[var].unique()) <=2]\n\nle = LabelEncoder()\nfor label in label_var:\n    data[label] = le.fit_transform(data[label])\n    \nprint('{} columns were Label Encoded'.format(label_var))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 2. OneHotEncoding "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data, drop_first = True)\nprint('Shape of the data is {}'.format(data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling Data (MinMaxScaler)\nscale = MinMaxScaler(feature_range = (0,5))\nHR_col = list(data.columns)\nHR_col.remove('Attrition')\nfor col in HR_col:\n    data[col] = data[col].astype(float)\n    data[[col]] = scale.fit_transform(data[[col]])\ndata['Attrition'] = pd.to_numeric(data['Attrition'], downcast='float')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spliting Dataset into Train and Test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('Attrition', axis = 1)\nY = data['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Test Split\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.25, random_state = 7)\n\nprint('X_Train Shape : {}'.format(x_train.shape))\nprint('X_Test Shape : {}'.format(x_test.shape))\nprint('Y_Train Shape : {}'.format(y_train.shape))\nprint('Y_Test Shape : {}'.format(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base Line Model\n\nLet's first use a range of baseline algorithms (using out-of-the-box hyper-parameters) before we move on to more sophisticated solutions. The algorithms considered in this section are: Logistic Regression, Random Forest, SVM, KNN, Decision Tree Classifier"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR',LogisticRegression(class_weight = 'balanced')))\nmodels.append(('RF',RandomForestClassifier(n_estimators=100, random_state=42, class_weight = 'balanced')))\nmodels.append(('SVM', SVC(gamma='auto', random_state=7)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DT', DecisionTreeClassifier(random_state=7)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's evaluate each model in turn and provide accuracy and standard deviation scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_score = []\nauc_score = []\nnames = []\n\ncol = ['Model', 'ROC AUC Mean','ROC AUC Std', 'ACC Mean','ACC Std']\nresult = pd.DataFrame(columns = col)\n\ni = 0\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits = 5, random_state = 42)\n    cv_acc_score = cross_val_score(model, x_train, y_train, cv = kfold, scoring = 'accuracy')\n    cv_auc_score = cross_val_score(model, x_train, y_train, cv = kfold, scoring = 'roc_auc')\n    \n    acc_score.append(cv_acc_score)\n    auc_score.append(cv_auc_score)\n    names.append(name)\n    \n    result.loc[i] = [name,cv_auc_score.mean(), cv_auc_score.std(), cv_auc_score.mean(), cv_auc_score.std()]\n    i+=1\n\nresult = result.sort_values('ROC AUC Mean', ascending = False)\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification Accuracy is the number of correct predictions made as a ratio of all predictions made.\nIt is the most common evaluation metric for classification problems. However, it is often misused as it is only really suitable when there are an equal number of observations in each class and all predictions and prediction errors are equally important. It is not the case in this project, so a different scoring metric may be more suitable."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\nplt.subplot(1,2,1)\nsns.boxplot(x = names, y = auc_score)\nplt.title('ROC AUC Score')\n\nplt.subplot(1,2,2)\nsns.boxplot(x = names, y = acc_score)\nplt.title('Accuracy Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on our ROC AUC comparison analysis, Logistic Regression and Random Forest show the highest mean AUC scores. We will shortlist these two algorithms for further analysis. See below for more details on these two algos."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalized Confusion Matrix\ndef get_norm_cnf_matrix(y_test, y_pred):\n\n    # Noramalized Confusion Matrix\n    y_test_0 = y_test.value_counts()[0]\n    y_test_1 = y_test.value_counts()[1]    \n    cnf_norm_matrix = np.array([[1.0 / y_test_0,1.0/y_test_0],[1.0/y_test_1,1.0/y_test_1]])\n    norm_cnf_matrix = np.around(confusion_matrix(y_test, y_pred) * cnf_norm_matrix,3)\n    \n    return(norm_cnf_matrix)\n\n# Confusion Matrix\ndef plt_cnf_matrix(y_test,y_pred):\n    \n    # Confusion Matrix`\n    cnf_matrix = confusion_matrix(y_test, y_pred)    \n    \n    # Normalized Confusion Matrix\n    norm_cnf_matrix = get_norm_cnf_matrix(y_test, y_pred)\n    \n    # Confusion Matrix plot\n    plt.figure(figsize = (15,3))\n    plt.subplots_adjust(hspace = 0.5, wspace = 0.5)\n    plt.subplot(1,2,1)\n    sns.heatmap(cnf_matrix, annot = True, fmt = 'g', cmap = plt.cm.Blues)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix')\n    \n    # Noramalized Confusion Matrix Plot\n    plt.subplot(1,2,2)\n    plt.subplots_adjust(hspace = 0.5, wspace = 0.5)\n    sns.heatmap(norm_cnf_matrix, annot = True, fmt = 'g', cmap = plt.cm.Blues)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')  \n    plt.title('Normalized Confusion Matrix')\n    plt.show()\n    \n    print('-'*25)\n    print('Classification Report')\n    print('-'*25)\n    print(classification_report(y_test, y_pred))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression\n\nLogistic regression is a statistical method for analysing a dataset in which there are one or more\nindependent variables that determine a binary outcome.\n\n\nIt predicts the probability of occurrence of an event by fitting data to a logit function.\n\n##### Fine Tuning\n\n**C** : Regularize Parameter nothing but Lambda. It regularize model to avoid overfitting. smaller values specify stronger\n    regularization.\n    \n    \n**penalty** : 'l1', 'l2', 'elasticnet' or 'none'"},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_parameter = {\n    'C' : np.arange(0.0001, 2, 0.01)\n}\n\nlm_model = LogisticRegression(solver='liblinear',class_weight = 'balanced')\nrandomized_model = RandomizedSearchCV(lm_model,hyper_parameter, n_jobs=-1, cv = 10, verbose = 1,  scoring='roc_auc')\nrandomized_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('='*25)\nprint('Best Estimertor')\nprint('='*25)\nprint(randomized_model.best_estimator_)\nprint('\\n')\n\nprint('='*25)\nprint('Best Parameter')\nprint('='*25)\nprint(randomized_model.best_params_)\nprint('\\n')\n\nprint('='*25)\nprint('Best Score')\nprint('='*25)\nprint(randomized_model.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_final_model = randomized_model.best_estimator_\ny_pred_lm = lm_final_model.predict(x_test)\nplt_cnf_matrix(y_test, y_pred_lm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = lm_final_model.predict_proba(x_test) # predict probabilities\nprobs = probs[:, 1] # we will only keep probabilities associated with the employee leaving\nlogit_roc_auc = roc_auc_score(y_test, probs) # calculate AUC score using test dataset\nprint('AUC score for Logistic Regression : %.3f' % logit_roc_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest \n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\n#####  parameters\n\n**n_estimators** : The number of trees in the forest.\n\n**criterion** : The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\n**max_depth** : The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n**min_samples_split**: The minimum number of samples required to split an internal node.\n\n**min_samples_leaf**: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n**max_features**: The number of features to consider when looking for the best split.\n\n**max_leaf_nodes**: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n\n**bootstrap**: Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nn_estimators = [50, 75, 100, 125, 150, 175]\nmax_depth = [5, 10, 15, 20, 25]\nmin_samples_split = [2,4,6,8,10]\nmin_samples_leaf = [1, 2, 3, 4]\nmax_features = ['auto', 'sqrt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_grid = {\n    'n_estimators' : n_estimators,\n    'max_depth' : max_depth,\n    'min_samples_split' : min_samples_split,\n    'min_samples_leaf' : min_samples_leaf\n}\n\nrm = RandomForestClassifier(class_weight = 'balanced',random_state = 7)\nrm_random_model = RandomizedSearchCV(rm, random_grid, n_jobs=-1, cv = 10, verbose = 1, random_state = 42, scoring = 'roc_auc', iid = True)\nrm_random_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('='*25)\nprint('Best Estimertor')\nprint('='*25)\nprint(rm_random_model.best_estimator_)\nprint('\\n')\n\nprint('='*25)\nprint('Best Parameter')\nprint('='*25)\nprint(rm_random_model.best_params_)\nprint('\\n')\n\nprint('='*25)\nprint('Best Score')\nprint('='*25)\nprint(rm_random_model.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm_final_model = rm_random_model.best_estimator_\ny_pred_rm = rm_final_model.predict(x_test)\nplt_cnf_matrix(y_test, y_pred_rm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm_probs = rm_final_model.predict_proba(x_test) # predict probabilities\nrm_probs = rm_probs[:, 1] # we will only keep probabilities associated with the employee leaving\nrm_roc_auc = roc_auc_score(y_test, rm_probs) # calculate AUC score using test dataset\nprint('AUC score for Logistic Regression : %.3f' % rm_roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rm_final_model.feature_importances_\nindices = np.argsort(importances)[::-1]\nnames = [x_train.columns[i] for i in indices]\n\nfea = pd.DataFrame({\n    'Names' : names,\n    'Score' : importances[indices]    \n})\n\nplt.figure(figsize = (10,5))\nsns.barplot(x = 'Names', y = 'Score', data = fea, color = 'darkBlue')\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":4}