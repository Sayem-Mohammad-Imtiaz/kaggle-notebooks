{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logistic Regression & Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Source : https://www.kaggle.com/azeembootwala/titanic?select=train_data.csv\n\nThe purpose of this project is to develop a logistic regression model for survival based on the train data. A confusion matrix is then used to provide the accuracy of the model.\n\n# Data Loading","metadata":{}},{"cell_type":"code","source":"# Import train data\ntrain_df = pd.read_csv('../input/titanic/train_data.csv')\n# View the first five rows of train_df\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import test data\ntest_df = pd.read_csv('../input/titanic/test_data.csv')\n# View the first five rows of test_df\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above shows the first 5 rows of the train data and test data. There's a total of 17 columns. The dependent variable is Survived and all the other columns are independent variables.\n\nThe first five rows of Age, Fare and Family_Size are in decimals and are smaller than 1 as they were normalized. In addition, the first two columns are irrelevant for the Logistic Regression below. These columns will be dropped. Let's take a deeper look at this below.\n\n# Data Cleaning\n\nBoth train data and test data simultaneously, but only the train data will be viewed after cleaning.","metadata":{}},{"cell_type":"code","source":"# Drop the first two columns which are not required for the analysis below\ntrain_df = train_df.drop(['Unnamed: 0', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Unnamed: 0', 'PassengerId'], axis=1)\n# View some details for each variable\ntrain_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the table above, there are 792 rows with data on each variable. Based on the description from Kaggle, all missing values have been filled with a median of the column values. All real valued data columns have been normalized. Thus, the columns for Age, Fare and Family_Size consist values between 0 and 1.\n\nLet's look at the correlation coefficient below to check if we need all the columns for the logistic regression model.","metadata":{}},{"cell_type":"code","source":"# Set the figure's size\nplt.figure(figsize=(20,15))\n# Plot heatmap\nsns.heatmap(train_df.corr(), annot = True, cmap = 'Blues_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the heatmap above, it seems like all variables are important and should be included in the logistic regression model. We should look at the assumptions of logistic regression before applying the model.\n\n# Assumptions of Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"The assumptions of Logistic Regression via the link (https://www.statisticssolutions.com/assumptions-of-logistic-regression/) is used to check if the data is a good fit.\n\n- The dependent variable - survival results are binary (0 = No, 1 = Yes). We will be using the binary logistic regression.\n\n\n- Logistic regression requires the observations to be independent of each other, which means that the observations should not be coming from a repeated measurements or matched data.\n\n   Based on the description on Kaggle, 4 columns (Title_1 to Title_4) have been added, re-engineered from the Name column to Title1 to Title4 signifying males & females depending on whether they were married or not .(Mr , Mrs ,Master,Miss). This indicates that the title columns are created based on the name, sex and age columns from the original dataset. Therefore, Title_1 to Title_4 are excluded as we will be keeping the Sex and Age columns.\n\n\n- Logistic Regression requires little or no multicollinearity among the independent variables.\n\n   From the heatmap above, fare is positively correlated with Pclass_1 (1st Class Ticket) and negatively correlated with Pclass_2 (2nd Class Ticket) and Pclass_3 (3rd Class Ticket). This means that as the fare price increase, it is highly possible that it is a 1st Class Ticket. Thus, Pclass_1 to Pclass_3 are excluded since we are keeping the fare column.\n\n\n- Logistic regression requires the independent variables are linearly related to the log odds. Our data do meet this requirement since the columns were normalized.\n\n\n- Logistic regression requires a large sample size. Since we will be using 7 independent variables, having 792 rows of data should be fine.","metadata":{}},{"cell_type":"code","source":"# Drop the title columns\ntrain_df = train_df.drop(['Title_1','Title_2','Title_3','Title_4' ], axis=1)\ntest_df = test_df.drop(['Title_1','Title_2','Title_3','Title_4' ], axis=1)\n# Drop the Pclass columns\ntrain_df = train_df.drop(['Pclass_1','Pclass_2','Pclass_3'], axis=1)\ntest_df = test_df.drop(['Pclass_1','Pclass_2','Pclass_3'], axis=1)\n# View the first five rows of df\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting the X and y variables\nSpilt the cleaned data into independent variable (X) and dependent variable (y).","metadata":{}},{"cell_type":"code","source":"# Independent Variables for train data\ntrain_x = train_df.iloc[:,1:]\n# View the first three rows of train_x\ntrain_x.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Independent Variables for test data\ntest_x = test_df.iloc[:,1:]\n# View the first three rows of test_x\ntest_x.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dependent Variable for train data\ntrain_y = train_df['Survived']\n# View the first three rows of train_y\ntrain_y.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dependent Variable for test data\ntest_y = test_df['Survived']\n# View the first three rows of test_y\ntest_y.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Fit the logistic regression model according to the given training data\nlr = linear_model.LogisticRegression(random_state=0).fit(train_x, train_y)\n# Check accuracy of model\nprint ('Train : ', lr.score(train_x,train_y))\nprint ('Test  : ', lr.score(test_x, test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above shows the ratio of the number of correct predictions to the number of observations in train data and test data. As the training set accuracy is higher than test set by 0.04%, there's a small indication of overfitting. From the test set, the model is predicting with 79% accuracy. This is good but not great.\n\nNote that I had checked the logistic regression score() for the data if we include all the Pclass and Title columns. As expected, the accuracy score is higher than 79%. However, as including these columns do not meet the assumptions of logistic regression, the accuracy of the model could not be trusted.","metadata":{}},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Predict for test_x\npred_y = lr.predict(test_x)\n# Confusion Matrix\nconfusion_matrix(test_y, pred_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the confusion matrix graph\nplot_confusion_matrix(lr, test_x, test_y, cmap = 'Blues_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 indicated the person did not survive whereas 1 indicates the person survived.\n\nThere are 24 data points in (1,1), it means that the outcome of the model correctly predits the person survived. There's 55 data points in (0,0), indicating that the model correctly predict the person did not survive. A total of 79 data points were correctly classified as survived or dead.\n\nFrom the above plot, Type 1 and type 2 errors has a total of 21 (9 + 12) data points which were incorrectly classified. These represents the predicted outcome differs from the actual outcome.\n\n# Classification Report","metadata":{}},{"cell_type":"code","source":"# View the classfication report\nprint(classification_report(test_y, pred_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The values in the report above is calculated via the values provided in the confusion matrix. The precision and recall are the probabilities calculated from the confusion matrix above.\n\nThe F-1 score measures the preciseness and robustness of the model.\n\nIn conclusion, there are no perfect models. The logistic model tells us the prediction is pretty good but may not be the best model for this dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}