{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spam vs Ham text classifition with Python\n\n## Rui La\n\n### Jan 2018","metadata":{}},{"cell_type":"markdown","source":"## Objective\nThis is a document classification project to classify spam vs ham. I will construct a spam filter to classify text message as ham or spam. ","metadata":{}},{"cell_type":"code","source":"# import some necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n%matplotlib inline \nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, Lasso, Ridge\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.kernel_ridge import KernelRidge\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, f1_score\nimport xgboost as xgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"# now let's import data in pandas dataframe\ndf = pd.read_csv('../input/spam.csv',usecols = [0,1],encoding='latin-1' )\ndf.rename(columns = {'v1':'Category','v2': 'Message'}, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('Category').describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nThe initial analysis tells me that there are 4825 ham text and 747 spam text. Most of the messages are unique. The highest repetition in ham text is \"Sorry, I'll call later\" I am wondering if they keep their words to call back or not. \n","metadata":{}},{"cell_type":"markdown","source":"## Exporation Data Analysis\n\nFirst, let's visualize the number of spam text vs. ham text","metadata":{}},{"cell_type":"code","source":"category_count = pd.DataFrame()\ncategory_count['count'] = df['Category'].value_counts()","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (12, 6))\nsns.barplot(x = category_count.index, y = category_count['count'], ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('category',fontsize = 15)\nax.tick_params(labelsize=15)","metadata":{"scrolled":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly, this is an **imbalanced dataset**. There are way more ham texts than spam texts here. The typical problem with an imbalanced dataset is that the simple metric like accuracy or precision may not reflect the real performance of predictive models. Since most text are \"ham\" in this project, even predicting all \"ham\" will give you **86%**  accuracy. I will go deep into the metric selection in the model construction part.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"spam_df = df[df['Category'] == 'spam'] #create sub-dataframe of spam text\nham_df = df[df['Category'] == 'ham'] #sub-dataframe of ham text","metadata":{"collapsed":true,"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ndef wordCount(text):\n    try:\n        text = text.lower()\n        regex = re.compile('['+re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') \n        txt = regex.sub(' ',text)  #remove punctuation\n        words = [w for w in txt.split(' ')\\\n                if not w in stop_words and len(w)>3] # remove stop words and words with length smaller than 3 letters\n        return len(words)\n    except:\n        return 0","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_df['len'] = spam_df['Message'].apply(lambda x: len([w for w in x.split(' ')]))\nham_df['len'] = ham_df['Message'].apply(lambda x: len([w for w in x.split(' ')]))\nspam_df['processed_len'] = spam_df['Message'].apply(lambda x: wordCount(x))\nham_df['processed_len'] = ham_df['Message'].apply(lambda x: wordCount(x))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Text length distribution analysis\n\nIt's interesting to analyze the length distribution of spam and ham texts. I expect the spam text will be longer than ham text since it wants to leave a deep impression on you. Let's check if it's true.","metadata":{}},{"cell_type":"code","source":"xmin = 0\nxmax = 50\nprint ('spam length info')\nprint (spam_df[['len', 'processed_len']].describe())\nprint ('ham length info')\nprint (ham_df[['len', 'processed_len']].describe())\nfig, ((ax,ax1),(ax2,ax3)) = plt.subplots (2,2,figsize = (12,9))\nspam_df['len'].plot.hist(bins = 20, ax = ax, edgecolor = 'white', color = 'orange')\nspam_df['processed_len'].plot.hist(bins = 20, ax = ax1, edgecolor = 'white', color = 'orange')\nham_df['len'].plot.hist(bins = 20, ax = ax2, edgecolor = 'white', color = 'blue')\nham_df['processed_len'].plot.hist(bins = 20, ax = ax3, edgecolor = 'white', color = 'blue')\nax.tick_params(labelsize = 15)\nax.set_xlabel('length of sentence', fontsize = 12)\nax.set_ylabel('spam_frequency', fontsize = 12)\nax.set_xlim([xmin,xmax])\nax1.tick_params(labelsize = 15)\nax1.set_xlabel('length of processed sentence', fontsize = 12)\nax1.set_ylabel('spam_frequency', fontsize = 12)\nax1.set_xlim([xmin,xmax])\nax2.tick_params(labelsize = 15)\nax2.set_xlabel('length of sentence', fontsize = 12)\nax2.set_ylabel('ham_frequency', fontsize = 12)\nax2.set_xlim([xmin,xmax])\nax3.tick_params(labelsize = 15)\nax3.set_xlabel('length of processed sentence', fontsize = 12)\nax3.set_ylabel('ham_frequency', fontsize = 12)\nax3.set_xlim([xmin,xmax])","metadata":{"scrolled":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These four plots are distributions of spam and ham text before and after precessing. The precessing including removing punctuations and stopwords in English. Also, I removed word with less than 3 letters.\n\nTo better compare length distributions, I limited the x scale in the range of (0, 50)\n\nAs you can see:\n\n    1. The spam text length distribution is more like normal distribution while the ham text length distribution is right skewed. \n    2. Also, the mean and median of spam text is longer than ham text. This is because usually the spam text include a lot of introduction sentence or tedious information. \n    3. After processing, both spam and ham text are shorter than original length. ","metadata":{}},{"cell_type":"code","source":"def tokenize(text):\n    exclude = set(string.punctuation)\n    regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') #remove punctuation\n    text = regex.sub(' ', text)\n    tokens = nltk.word_tokenize(text) # tokenize the text\n    tokens = list(filter(lambda x: x.lower() not in stop_words, tokens)) # remove stop words\n    tokens = [w.lower() for w in tokens if len(w) >=3] \n    tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n    return tokens","metadata":{"collapsed":true,"scrolled":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_df['tokens'] = spam_df['Message'].map(tokenize)\nham_df['tokens'] = ham_df['Message'].map(tokenize)","metadata":{"scrolled":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Most common words\n\nIn this part, I want to check what is the most 10 common words in spam and ham texts. ","metadata":{}},{"cell_type":"code","source":"spam_words = []\nfor token in spam_df['tokens']:\n    spam_words = spam_words + token #combine text in different columns in one list\nham_words = []\nfor token in ham_df['tokens']:\n    ham_words += token","metadata":{"collapsed":true,"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_count = Counter(spam_words).most_common(10)\nham_count = Counter(ham_words).most_common(10)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_count_df = pd.DataFrame(spam_count, columns = ['word', 'count'])\nham_count_df = pd.DataFrame(ham_count, columns = ['word', 'count'])","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_count\nfig, (ax,ax1) = plt.subplots(1,2,figsize = (18, 6))\nsns.barplot(x = spam_count_df['word'], y = spam_count_df['count'], ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('word',fontsize = 15)\nax.tick_params(labelsize=15)\nax.set_title('spam top 10 words', fontsize = 15)\nsns.barplot(x = ham_count_df['word'], y = ham_count_df['count'], ax = ax1)\nax1.set_ylabel('count', fontsize = 15)\nax1.set_xlabel('word',fontsize = 15)\nax1.tick_params(labelsize=15)\nax1.set_title('ham top 10 words', fontsize = 15)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Most common words in spam**\n\nIt's not surprise to see many words in this plot are related to communication (\"call\", \"txt\", \"reply\", \"mobile\", \"www\"). This is because most spam texts want the receivers to reach back to them. I also see \"claim\", \"prize\" and \"free\" in this list which maybe lead to fraud text\n\n**Most common words in ham**\n\nMany words in ham-common-words list are verbs such as \"get\", \"know\" and \"come\". This is reasonable in a conversation between two people who know each other. Also, I see \"day\" and \"time\" which indicates that the texts are used to arrange a meeting. ","metadata":{}},{"cell_type":"markdown","source":"## Word cloud","metadata":{}},{"cell_type":"code","source":"spam_words_str = ' '.join(spam_words)\nham_words_str = ' '.join(ham_words)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_word_cloud = WordCloud(width = 600, height = 400, background_color = 'white').generate(spam_words_str)\nham_word_cloud = WordCloud(width = 600, height = 400,background_color = 'white').generate(ham_words_str)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax, ax2) = plt.subplots(1,2, figsize = (18,8))\nax.imshow(spam_word_cloud)\nax.axis('off')\nax.set_title('spam word cloud', fontsize = 20)\nax2.imshow(ham_word_cloud)\nax2.axis('off')\nax2.set_title('ham word cloud', fontsize = 20)\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Word cloud is a straight forward way to demonstrate the most common words in spam and ham text. ","metadata":{}},{"cell_type":"markdown","source":"## Modelling\n\n**Convert tokens to a matrix of TF-IDF features**","metadata":{}},{"cell_type":"code","source":"df['tokens'] = df['Message'].map(tokenize)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_join(text):\n    return \" \".join(text)\ndf['text'] = df['tokens'].apply(text_join)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tv = TfidfVectorizer('english')\nfeatures = tv.fit_transform(df['text'])\ntarget = df.Category.map({'ham':0, 'spam':1})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**import libraries**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, Lasso, Ridge\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.kernel_ridge import KernelRidge\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, f1_score\nimport xgboost as xgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define a cross validation strategy**\n\nI use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, I add one line of code, in order to shuffle the dataset prior to cross-validation.\n\nFor the performance metric, as I mentioned in the beginning, this is an imbalanced dataset. Thus, instead of using accuracy, I used F1-score as metric. F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.\n\n","metadata":{}},{"cell_type":"code","source":"n_folds = 5\ndef f1_cv(model):\n    kf = KFold(n_folds, shuffle = True, random_state = 29).get_n_splits(features)\n    f1 = cross_val_score(model, features, target, scoring = 'f1', cv = kf )\n    return (f1)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Base model\n\nI use **SVC**, **Random Forest**, **Gradient Boosting Classification**, **LightGBM**, and **Multinomial Naive Bayes** as my training model","metadata":{}},{"cell_type":"code","source":"svc = SVC(kernel = 'sigmoid', gamma = 1.0)\nrfc = RandomForestClassifier(n_estimators = 31, random_state = 32)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GBoost = GradientBoostingClassifier( n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   random_state =5)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgb = lgb.LGBMClassifier(\n                              objective='binary',num_leaves=5,\n                              learning_rate=0.05, n_estimators=4420,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 1)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb = MultinomialNB(alpha = .2)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Base model scores\n\nLet's see how these base models perform on the data by evaluating the cross-validation f1 score","metadata":{}},{"cell_type":"code","source":"score = f1_cv(svc)\nprint ('\\nSVC score: {:4f}({:4f})\\n'.format(score.mean(), score.std()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = f1_cv(rfc)\nprint ('\\nRandomForest score: {:4f}({:4f})\\n'.format(score.mean(), score.std()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = f1_cv(mnb)\nprint ('\\nMultinomial NB score: {:4f}({:4f})\\n'.format(score.mean(), score.std()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that Gradient Boosting and Multinomial NB perform best with f1-score around 0.92. Now let's see how they perform in accuracy","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classifier(clf, X_train, y_train):    \n    clf.fit(X_train, y_train)\ndef predictor(clf, X_test):\n    return (clf.predict(X_test))","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = {'SVC':svc, 'RandomForest':rfc,  'MultinomialNB': mnb}\npreds = []\nfor key, value in clf.items():\n    #print(key)\n    classifier(value, X_train, y_train)\n    pred = predictor(value,X_test)\n    preds.append((key, [accuracy_score(y_test,pred)]))","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the models have accuracy higher then **97%**. The Multinomial is the best model based on accuracy score (**98%**) and f1 score (**0.921**). Since this is a very small dataset and easy classification problem, I think simple model is enough to filter the spam text. There is no need to use GradientBoost or neural network unless there is a huge dataset need to be classified. ","metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 2","name":"python2","language":"python"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}}}}