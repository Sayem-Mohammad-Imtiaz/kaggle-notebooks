{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nimport statsmodels.formula.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrices\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/HR_comma_sep.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c755334e3161d8578afc4af2160515b798884a4b"},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5373cd8666326a1c40fa2361ef7fa4bd56e15a77"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bce54fd5a3e0c657b81e806bb16265fa0a0e2841"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"978bf62d6805576c2e12280268a5ddd8fcf9cceb"},"cell_type":"code","source":"## Let's separate numerical and categorical vaiables into 2 dfs\n\ndef sep_data(df):\n    \n    numerics = ['int32','float32','int64','float64']\n    num_data = df.select_dtypes(include=numerics)\n    cat_data = df.select_dtypes(exclude=numerics)\n    \n    return num_data, cat_data\n\nnum_data,cat_data = sep_data(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a463cbd2e8a689acc6f7ec80b198d75d71e33f2"},"cell_type":"code","source":"## Let's create a summary of Numerical Variables\n\ndef print_summary(x):\n    \n    return pd.Series([x.count(),\n                        x.isnull().sum(),\n                        x.mean(),\n                        x.median(),\n                        x.std(),\n                        x.var(),\n                        x.min(),\n                        x.max(),\n                        x.dropna().quantile(0.25),\n                        x.dropna().quantile(0.75),\n                        x.dropna().quantile(0.90)    \n                        ],\n                       index = [\"Number of Observations\",\n                                \"Missing Values\",\n                               \"Mean\",\n                                \"Median\",\n                                \"Standard Deviation\",\n                                \"Variance\",\n                                \"Minimum Value\",\n                                \"Maximum Value\",\n                                \"25th Percentile\",\n                                \"75th Percentile\",\n                                \"90th Percentile\"])\n\nnumerical_summary = num_data.apply(func = print_summary)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0bc23e1684394529959b0f0f0952701a64bef26"},"cell_type":"code","source":"numerical_summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c958810d1ff316ef17c0e8fc8e1b543c5e258c2d"},"cell_type":"code","source":"## Separate X and Y variables\n\ny = data.loc[:,'left']\nX = pd.DataFrame(data.drop(columns='left'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27241a2d96a8076dec24ca0ecc50f7a5e65a39a3"},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34c2b02e2b8f4524af70af103270a109bf8e288c"},"cell_type":"markdown","source":"<font color = 'indigo'  size= \"12\"><b><body style=\"background-color:lightgrey;\">Below is what we should try :</b></body></font>\n1.  VIF - Which variables are highly correlated\n2. Check odds ratio - to see variance in data\n3. Run a Logistic Regression model to see most impactful variables (use OneHotEncoding for Categorical Variables)\n4. Run simple Decision Trees to see the explainability of data (EDA)\n5. Check prediction power of Decision Trees\n4. Run Random Forest with:\n    - Grid Search\n    - K Fold cross validations \n    - SMOTE \n    - Regularization\n    - Tree Pruning & Other hyperparameter tuning \n    - Confusion Matrics \n    - ROC \n    - Boosting, GBM and xGBoost "},{"metadata":{"trusted":true,"_uuid":"b9f396cde5e43a69e45f1df14e387d2d4f903b40"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = 16, 7.5\n\nsns.heatmap(data.corr(),cmap='tab10')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00e12a501408f87be5b5336d5b14ced46e2e3268"},"cell_type":"code","source":"## 1. Let's run VIF to check highly correlated and hence redundant variables.\n\nfeatures = num_data.drop(columns='left')\nfeature_list = \"+\".join(features.columns)\ny, X = dmatrices('left~'+feature_list,num_data,return_type='dataframe')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0688e976132282200f7270daef91c6883219b2b"},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['Features'] = X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10f524a70475ac2f8cb458b71376576e94145d31"},"cell_type":"code","source":"vif","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b8e63dc348932cc4aceabae2c630690738531f9"},"cell_type":"markdown","source":"### <font color = 'green'> The above table shows that there is no variable with a 'high\" Variance Inflation Factor ... So, this method suggests we should not drop any variable"},{"metadata":{"trusted":true,"_uuid":"175a29f11b020abdbe031e1969114b6d4a5c74fe","scrolled":false},"cell_type":"code","source":"## 2. Log odds plot - to see variance in data\n\nfor feature in num_data.columns.difference(['left']):\n    binned = pd.cut(num_data[feature],bins=10,labels=list(range(1,11)))\n    binned = binned.dropna()\n    ser = num_data.groupby(binned)['left'].sum() / (num_data.groupby(binned)['left'].count() - num_data.groupby(binned)['left'].sum()) \n    ser = np.log(ser)\n    fig,axes = plt.subplots(figsize=(16,8))\n    sns.barplot(x=ser.index,y=ser)\n    plt.ylabel('Log Odds Ratio')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b96994bdb5bf24813d937d274a0af6781cb8bf9"},"cell_type":"markdown","source":"### <font color = 'blue'> The above graphs will help us bin the categorical variables better, if need be. </font>"},{"metadata":{"_uuid":"c5864278266a8cce1b74a0dd95abe17e07c1edb5"},"cell_type":"markdown","source":"### <font color = 'orange'> The following block of code will be used to perform One Hot Encoding of categorical variables - It will also rename the One Hot Encoded Columns</font> "},{"metadata":{"trusted":true,"_uuid":"e72e819f0efa068ea8ee17ac3873df03c32f1ada","scrolled":false},"cell_type":"code","source":"## One hot encoding will be done on categorical variables - salary and department ... \n## We need to first run label coding before using OneHotEncoding\n\nohe_columns = data.select_dtypes(include='object').columns\n\nle = LabelEncoder()\ndata_le = data.copy()\n\nfor column in ohe_columns:\n    \n    data_le.loc[:,column] = le.fit_transform(data_le.loc[:,column])\n\n## One Hot Encoding method takes arrays as X, hence we need to convert features into arrays and remove headings.\nX = data_le.drop(columns='left').values   ## This approach will create rows of arrays which need to be passed to OneHotEncoder\ny = data_le.loc[:,'left']  ## This does not require array - hence we are just copying and not using .values\n\nohe = OneHotEncoder(categorical_features=[7,8]) ## This method takes index location of categorical variables in X array as input\nX = ohe.fit_transform(X).toarray()\n\n## Let's convert X into Data Frame \nX = pd.DataFrame(X)\n\n## Maintain columns that are unaffected by OneHotEncoding separately\ntotal_cols = data.columns\ncols_maintained = total_cols.drop(['Department','salary','left'])\n\n## Column names for OneHotEncoded Columns - One by one\n## 1. For Department\nfor ind in range(data[ohe_columns[0]].value_counts().count()):\n    \n    a = X[X[ind] == 1].index.values.astype(int) ## For any column, check where is \"1\" present as a value after OneHotEncoding\n    name_idx = a[0] ## Index of first occurance of \"1\"\n    name = data.loc[a[0],ohe_columns[0]] ## Value in \"Department\" column in data DataFrame\n    col_name = ohe_columns[0] + \"_\" + name ## Concatenate \"Department_\" + Value as the new column name\n    X.rename(columns={ind:col_name},inplace=True) ## Rename the column\n\n## 2. For Salary\nfor ind in range(data[ohe_columns[0]].value_counts().count(),(data[ohe_columns[0]].value_counts().count() + 3)):\n    \n    a = X[X[ind] == 1].index.values.astype(int) ## For any column, check where is \"1\" present as a value after OneHotEncoding\n    name_idx = a[0] ## Index of first occurance of \"1\"\n    name = data.loc[a[0],ohe_columns[1]] ## Value in \"Salary\" column in data DataFrame\n    col_name = ohe_columns[1] + \"_\" + name ## Concatenate \"Salary_\" + Value as the new column name\n    X.rename(columns={ind:col_name},inplace=True) ## Rename the column\n    \n## 3. For columns unchanged by OneHotEncoding\ncounter = 0\nfor ind in range((data[ohe_columns[0]].value_counts().count() + 3),(len(X.columns))):\n    \n    X.rename(columns={ind:cols_maintained[counter]},inplace=True)\n    counter = counter + 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"182da7f071a81681884ffc3948727ad4ceb8e319"},"cell_type":"code","source":"## Let's run Logistic Regression now ....\n## First, we need to split data into train and test \n## Scenario 1 --> where all dummy classes are present ....\n\ntrain_X, test_X, train_y, test_y = train_test_split(X,y,test_size = 0.3,random_state = 142)\n\nmodel = sm.Logit(train_y,train_X)\nresult = model.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9fd78f3652f1a3499f9ac6c1ebf5198884a7b46"},"cell_type":"code","source":"result.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1289871e909c1d7c990711a393d8ca8612e46905"},"cell_type":"code","source":"result.pvalues","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"210e594e1ed9b489c09a5ac8e17e2c50b7528bd3"},"cell_type":"code","source":"## Drop one of the dummy variables for each OneHotEncoded variable \n\ntrain_X_2 = train_X.drop(columns=['Department_IT','salary_high'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93064d7dd939101194d1da81b0551ad11bb7f59b"},"cell_type":"code","source":"## Scenario 2 --> Run Logistic on Xs with dropped data - avoiding dummy variable trap\n\nmodel_2 = sm.Logit(train_y,train_X_2)\nresult_2 = model_2.fit()\nresult_2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48bbe90467995bdc7c2c6ad7ef213b3b9680737e"},"cell_type":"code","source":"result_2.pvalues","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c512fc4b489f485862496e4f970fb7e7e029a175"},"cell_type":"markdown","source":"### <font color = 'maroon'> Here, we see that by dropping one variable for every dummy one hot encoded class, suddenly a lot of variables for \"Department\" and \"Salary\" become impatful. ... Hence, it is a best practice to keep (N-1) dummy variables for every variable with N unique values </font>"},{"metadata":{"trusted":true,"_uuid":"41bdc62457df95a6f4ed33e0626d45b3a95037c5"},"cell_type":"code","source":"test_X_2 = test_X.drop(columns=['Department_IT','salary_high'])\n\n## Create a data frame with 2 columns - one has the predicted probability and the other has the actual class \npredict_2 = pd.DataFrame(result_2.predict(test_X_2))\npredict_2.rename(columns={0:'pred_prob'},inplace=True)\npredict_test = pd.concat([predict_2,test_y],axis=1)\npredict_test.rename(columns={'left':'actual_class'},inplace=True)\nfpr_test,tpr_test,thr_test = metrics.roc_curve(test_y,predict_2)\nfpr_test = pd.DataFrame(fpr_test,columns=['fpr'])\ntpr_test = pd.DataFrame(tpr_test,columns=['tpr'])\nthr_test = pd.DataFrame(thr_test,columns=['threshold'])\nthr_df = pd.concat([fpr_test,tpr_test,thr_test],axis=1)\nroc_auc = metrics.auc(fpr_test,tpr_test)\n\n## Create a similar DataFrame for training data as well - This will be used to draw ROC\npredict_train = pd.DataFrame(result_2.predict(train_X_2))\npredict_train = pd.concat([predict_train,train_y],axis=1)\npredict_train.rename(columns={'left':'actual_class',0:'pred_prob'},inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efd10e8227c7468cdc0d9a4c7063df45768d9ae1"},"cell_type":"code","source":"## ROC for test data ....\n\n%matplotlib inline\n\nplt.plot(fpr_test,tpr_test,'b','AUC = %0.2f' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d3b9014940d4620d0d2919a0c7c310da9a25882"},"cell_type":"code","source":"## Threshold for test data \noptimal_tpr = thr_df.loc[(thr_df['fpr'] < 0.5),'tpr'].max()\noptimal_fpr = thr_df.loc[(thr_df['tpr'] == optimal_tpr),'fpr'].min()\noptimal_thr = thr_df.loc[(thr_df['tpr'] == optimal_tpr) & (thr_df['fpr'] == optimal_fpr),'threshold']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"388d01836a2119279aa8161ec04449082dbb7d4c"},"cell_type":"code","source":"roc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd0d6d916c6eec2014ebfa2f3cc2c185872a8ebe"},"cell_type":"code","source":"## Now the above calculations show that optimal threshold for classification is 0.1238 ... Let's create a confusion matrics for this\n\npredict_test['predicted_class'] = predict_test.apply(lambda x : 1 if x['pred_prob'] > 0.12389 else 0 ,axis=1 )\ncross_tab = pd.crosstab(predict_test['actual_class'],predict_test['predicted_class'])\nsns.heatmap(cross_tab, annot=True, cmap = 'Set3',fmt='.0f')\n\n## Let's calculate accuracy, precision and recall \ncross_tab.reset_index(inplace=True)\ncross_tab = pd.DataFrame(cross_tab)\ncross_tab.rename(columns={0:'pred_0',1:'pred_1'},inplace=True)\noverall_sum = cross_tab.sum().sum()\n\naccuracy = ((cross_tab['pred_0'][0] + cross_tab['pred_1'][1]) / overall_sum) * 100\nprint(\"The overall accuracy of the model on Test Data is \", np.round(accuracy,1), \"%\")\n\nrecall = (cross_tab['pred_1'][1] / (cross_tab['pred_1'][1] + cross_tab['pred_0'][1])) * 100 \nprint(\"The Recall for model on Test Data is \", np.round(recall,1), \"%\")\n\nprecision = (cross_tab['pred_1'][1] / (cross_tab['pred_1'][1] + cross_tab['pred_1'][0])) * 100 \nprint(\"The Precision for model on Test Data is \", np.round(precision,1), \"%\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f97049c81d3288667f058282a9bad9eae6e3d48"},"cell_type":"markdown","source":"### We will do the next part of the exercise in another notebook to avoid clutter (WIP) ..... Link below:\n[]https://www.kaggle.com/prafultickoo/hr-analytics-practice-2/edit)"},{"metadata":{"trusted":true,"_uuid":"b17701b6323d0a8c7388a230979628e45e45e35c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}