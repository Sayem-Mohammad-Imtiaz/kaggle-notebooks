{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n**Red Wine classification dataset is a publicly shared [UCI repo](https://archive.ics.uci.edu/ml/datasets/wine+quality). I have used the available version in Kaggle**\n\n**The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: [Web Link](http://www3.dsi.uminho.pt/pcortez/wine/) or the reference [Cortez et al., 2009](http://www3.dsi.uminho.pt/pcortez/Home.html). Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).**\n\n**These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.**\n\n**For simplicity I aim to approach this as classification problem**\n\n\n**I have made it beginner friendly for someone who is completely new to Machine Learning, I aim to make Machine Learning approach as easier as possible for you guys, so do read and upvote it so that it can reach maximum people**\n\n![red_wine](https://archive.ics.uci.edu/ml/assets/MLimages/Large186.jpg)\n\n# Contents\n\n**Input variables (based on physicochemical tests):**\n\n> fixed acidity\n\n> volatile acidity\n\n> citric acid\n\n> residual sugar\n\n> chlorides\n\n> free sulfur dioxide\n\n> total sulfur dioxide\n\n> density\n\n> pH\n\n> sulphates\n\n> alcohol\n\n\n**Output variable (based on sensory data):**\n\n\n> quality (score between 0 and 10)\n\n**HOPE YOU ENJOY MY NOTEBOOK!!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Loading Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pandas_profiling import ProfileReport\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nprint('The Dataset contains {} rows and {} columns '.format(df.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's explore the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pandas Profiling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ProfileReport(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'].value_counts().index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So the ratings are 3,4,5,6,7 and 8 making only 6 values in quality column**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nsns.heatmap(df.corr(), annot=True, cmap=plt.cm.plasma)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Values?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So our dataset is clean**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The dataset primarily contains values of float data types**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Histogram and Density Plots of Columns\n\n**Creating certain visualizations to make understanding of the columns easier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(bins=40, figsize=(10,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(kind='density', subplots=True, layout=(4,3), sharex=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What do we Understand?**\n\n**Data distribution for attribute “alcohol” is positively skewed, for attribute “density” data quite normally distributed. Take attention to the wine quality data distribution. It’s a bimodal distribution and there are more wines with average quality than wines with ‘good’ or ‘bad’ quality.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Citric Acid, Fixed Acidity and Density","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.groupby(by=\"fixed acidity\")[[\"fixed acidity\", \"density\", \"citric acid\"]].first().reset_index(drop=True)\n\n# Figure\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (16, 6))\n\na = sns.distplot(data[\"fixed acidity\"], ax=ax1, hist=False, kde_kws=dict(lw=6, ls=\"--\"))\nb = sns.distplot(data[\"density\"], ax=ax2, hist=False, kde_kws=dict(lw=6, ls=\"--\"))\nc = sns.distplot(data[\"citric acid\"], ax=ax3, hist=False, kde_kws=dict(lw=6, ls=\"--\"))\n\na.set_title(\"Fixed Acidity Distribution\", fontsize=16)\nb.set_title(\"Density Distribution\", fontsize=16)\nc.set_title(\"Citric Acid distribution\", fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scatterplot Analysis\n\n**Since we found out through correlation plots about certain columns having good correlation, let's make a scatter plot matrix that will tell us about the columns that had good correlations**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nsm = scatter_matrix(df, figsize=(16, 10), diagonal='kde')\n\n[s.xaxis.label.set_rotation(40) for s in sm.reshape(-1)]\n[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n\n#May need to offset label when rotating to prevent overlap of figure\n\n[s.get_yaxis().set_label_coords(-0.6,0.5) for s in sm.reshape(-1)]\n\n#Hide all ticks\n\n[s.set_xticks(()) for s in sm.reshape(-1)]\n[s.set_yticks(()) for s in sm.reshape(-1)]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**\n\n**Here we can observe positive linear correlation between the higly correlated columns, for instance `fixed acidity` and `density` columns had correlation value of 0.67 and the scatter plot shows the high correlation of it**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Human wine preferences scores varied from 3 to 8, so it’s straightforward to categorize answers into ‘bad’ or ‘good’ quality of wines. This allows us to practice with hyperparameter tuning on e.g. decision tree algorithms. Visualizing the graph of the number of values for each category, we could see that there are far many bad answers than good ones. Of course, machine learning algorithms operate digital values, so we assign for categorizes corresponding discrete values 0 or 1.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing wine as good and bad by giving the limit for the quality\n\nbins = (2, 6, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\n# Now lets assign a labels to our quality variable\n\nlabel_quality = LabelEncoder()\n\n# Bad becomes 0 and good becomes 1\ndf['quality'] = label_quality.fit_transform(df['quality'])\nprint(df['quality'].value_counts())\nsns.countplot(df['quality'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Development","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop(['quality'], axis=1)\ny = df['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)\n\n\ncols = ['fixed acidity',\n'volatile acidity',\n'citric acid',\n'residual sugar',\n'chlorides',\n'free sulfur dioxide',\n'total sulfur dioxide',\n'density',\n'pH',\n'sulphates',\n'alcohol'\n       ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Optimization\n\n**for DecisionTreeClassifier max_depth can be adjusted to increase accuracy, similarly n_estimators can be used for RandomForestClassifiers, However these are completely optional and you may chose to skip them**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(max_depth=200)\ndtc.fit(x_train, y_train)\npreds = dtc.predict(x_test)\nscore = dtc.score(x_test, y_test)\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's look for best depth values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Ks = 100\nmean_acc = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    dtc = DecisionTreeClassifier(max_depth = n).fit(x_train,y_train)\n    yhat=dtc.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( \"The best accuracy was with\", mean_acc.max(), \"with depth =\", mean_acc.argmax()+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classification Report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cf = metrics.classification_report(preds,y_test)\nprint(cf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(x_train, y_train)\npreds = rfc.predict(x_test)\nscore = rfc.score(x_test,y_test)\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ks = 100\nmean_acc = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    rfc = RandomForestClassifier(n_estimators = n).fit(x_train,y_train)\n    yhat=dtc.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( \"The best accuracy was with\", mean_acc.max(), \"with n_estimator =\", mean_acc.argmax()+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cf = metrics.classification_report(preds,y_test)\nprint(cf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC curve plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_plot = metrics.plot_roc_curve(rfc, x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_plot = metrics.plot_roc_curve(dtc, x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation Score Approach\n**Let's check if our metrics get improved using this**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_eval = cross_val_score(dtc, x_test, y_test, cv=10)\nprint('Cross Val Score accuracy is {:.2f}'.format(dtc_eval.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_eval = cross_val_score(rfc, x_test, y_test, cv=10)\nprint('Cross Val Score accuracy is {:.2f}'.format(rfc_eval.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GridSearchCV\n\n**For DecisionTree**\n\n**Instead of chosing all possible hyperparameters that can improve the scores, GridSearchCV does that for you selecting the best possible parameter to get best score**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_para = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\ndtc_cv = GridSearchCV(DecisionTreeClassifier(), tree_para, cv=10)\ndtc_cv.fit(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To check the best hyperparameter**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_new = DecisionTreeClassifier(criterion='entropy', max_depth = 8)\ndtc_new.fit(x_train,y_train)\nnew_score  = dtc_new.score(x_test, y_test)\nnew_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So this actually works!!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's do the same for RandomForest as well**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nrfc_cv = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\nrfc_cv.fit(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_new = RandomForestClassifier(criterion='gini', max_depth = 5, max_features='auto', n_estimators=500)\ndtc_new.fit(x_train,y_train)\nnew_score  = dtc_new.score(x_test, y_test)\nnew_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n**Using Cross_val_score and GridSearchCV can go a long way in making best scores possible for your developed model so feel free to use them as per your convinience**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}