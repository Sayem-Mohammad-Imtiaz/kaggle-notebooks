{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/banfakenews/Authentic-48K.csv', na_values=['#NAME?'])\ndf2 = pd.read_csv('../input/banfakenews/Fake-1K.csv', na_values=['#NAME?'])\ndf3 = pd.read_csv('../input/banfakenews/LabeledAuthentic-7K.csv', na_values=['#NAME?'])\ndf4 = pd.read_csv('../input/banfakenews/LabeledFake-1K.csv', na_values=['#NAME?'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df1, df2, df3, df4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('date', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How much data is missing\ndf.isnull().sum().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['relation','F-type'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['source'].replace('NaN','Not Found',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.fillna({'source':'Not Found'})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How much data is missing\ndf.isnull().sum().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical values or continuous\ndf.dtypes.sort_values().to_frame('feature_type').groupby(by = 'feature_type').size().to_frame('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['category'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.category\ny = df.source","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_dummies = pd.get_dummies(x, prefix='category', columns=['category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,category_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['source'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source_dummies = pd.get_dummies(y, prefix='source', columns=['source'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,source_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['category','source'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = df.headline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply a first round of text cleaning techniques\nimport re\nimport string\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nround1 = lambda x: clean_text_round1(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h.columns = ['headline']\nh = h.sort_index()\nh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h.drop_duplicates(inplace= True)\nh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the updated text\ndata_clean = pd.DataFrame(h.apply(round1))\ndata_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply a second round of cleaning\ndef clean_text_round2(text):\n    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n    text = re.sub('[‘’“”…]', '', text)\n    text = re.sub('\\n', '', text)\n    return text\n\nround2 = lambda x: clean_text_round2(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the updated text\ndata_clean = pd.DataFrame(data_clean.headline.apply(round2))\ndata_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to create a document-term matrix using CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer(tokenizer=lambda x: x.split())\ndata_cv = cv.fit_transform(data_clean.headline)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = data_clean.index\ndata_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = df.content","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c.columns = ['headline']\nc = c.sort_index()\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c.drop_duplicates(inplace= True)\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the updated text\ndata_clean_content = pd.DataFrame(c.apply(round1))\ndata_clean_content","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the updated text\ndata_clean_content = pd.DataFrame(data_clean_content.content.apply(round2))\ndata_clean_content","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to create a document-term matrix using CountVectorizer\n\ndata_cv_2 = cv.fit_transform(data_clean_content.content)\ndata_dtm_2 = pd.DataFrame(data_cv_2.toarray(), columns=cv.get_feature_names())\ndata_dtm_2.index = data_clean.index\ndata_dtm_2","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}