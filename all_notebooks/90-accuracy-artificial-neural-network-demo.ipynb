{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Artificial Neural Network for the IBM attrition data.\n\nHere I implemented a quick Keras model to work on this dataset. The idea was to see how high of a accuracy I could get.\n\nDeep Learning works better with bigger datasets, this one being really tiny. Yet I could get some good results very quickly.\n\nWhen starting this exercise I was not aware it was not a real dataset from real employees... ","metadata":{"_uuid":"eb0cd07123eb7b592f18ca298ef05f536795ae85","_cell_guid":"61705030-0705-b12f-427c-b21fe4d68ae7"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import numpy as np \nimport pandas as pd \n\nfrom subprocess import check_output\ndataset = pd.read_csv('../input/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n#dataset = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')","metadata":{"collapsed":true,"_uuid":"1e0d9e387282e1064deb6a0938758126b97c9fee","_cell_guid":"4f37f4d4-3950-796d-100b-31c86ce02644"}},{"cell_type":"markdown","source":"## Cleaning the data\n\nRemoving the following columns:\n\n* Standard hours: As it was always 80 for all employees this columns was useless.\n\n* Over18 : Yes for all\n\n* Employee number: This column was not usefull for what we are looking for and could have confused the ANN.\n\nThe Attrition as been moved to the first column to make the slicing easier later on.","metadata":{"_uuid":"a932d04d4794b54bd59150b99e9da9615280fdfe","_cell_guid":"8752addf-0ab4-f12a-23a6-fa08252545dc"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#removing standard hours and employeeID + attrition as first row\ndataset = dataset[['Attrition',\n                   'Age',\n                   'BusinessTravel',\n                   'DailyRate',\n                   'Department',\n                   'DistanceFromHome',\n                   'Education',\n                   'EducationField',\n                   'EmployeeCount',\n                   'EnvironmentSatisfaction',\n                   'Gender',\n                   'HourlyRate',\n                   'JobInvolvement',\n                   'JobLevel',\n                   'JobRole',\n                   'JobSatisfaction',\n                   'MaritalStatus',\n                   'MonthlyIncome',\n                   'MonthlyRate',\n                   'NumCompaniesWorked',\n                   'OverTime',\n                   'PercentSalaryHike',\n                   'PerformanceRating',\n                   'RelationshipSatisfaction',\n                   'StockOptionLevel',\n                   'TotalWorkingYears',\n                   'TrainingTimesLastYear',\n                   'WorkLifeBalance',\n                   'YearsAtCompany',\n                   'YearsInCurrentRole',\n                   'YearsSinceLastPromotion',\n                   'YearsWithCurrManager']]","metadata":{"collapsed":true,"_uuid":"54a6136c5f8d38129c92aff9670c8c95591ea36e","_cell_guid":"437caf12-0060-8852-a6fd-00231087d531"}},{"cell_type":"markdown","source":"## Adding some more derived information\n\nPlaying with the data we can add some more information to help the network.\n\nWe don't need to be expert in the field in Deep Learning, but providing some ratios and extra information can help the network to converge faster.","metadata":{"_uuid":"741b420061e7b64e6bb502771e6ef583bfe71c1a","_cell_guid":"0dca7615-ca45-3f6d-d182-a9f800280713"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"dataset['JobInvolment_On_Salary']= dataset['JobInvolvement'] / dataset['MonthlyIncome'] * 1000\ndataset['MarriedAndBad_Worklife_Balance'] = np.where(dataset['MaritalStatus']=='Married', \n                                               dataset['WorkLifeBalance']-2,\n                                               dataset['WorkLifeBalance']+1)\ndataset['DistanceFromHome_rootedTo_JobSatisfaction'] = dataset['DistanceFromHome']**(1/dataset['JobSatisfaction'])\ndataset['TotalJobSatisfaction'] = dataset['EnvironmentSatisfaction'] + dataset['JobSatisfaction'] + dataset['RelationshipSatisfaction'] \ndataset['OldLowEmployeeTendToStay'] = dataset['YearsAtCompany'] / dataset['JobLevel']\ndataset['Mothers'] = np.where((dataset['Gender']=='Female') & (dataset['Age']>=36), 1,0)\ndataset['Rate'] = dataset['DailyRate'] * 20 + dataset['HourlyRate'] * 8 * 20 + dataset['MonthlyRate']\ndataset['RateExtended'] = dataset['Rate'] * (8 - dataset['JobSatisfaction'] - dataset['EnvironmentSatisfaction'])","metadata":{"collapsed":true,"_uuid":"a8c18a55b8815b073206ac2a5ec5442b4bd8691f","_cell_guid":"1992aaa7-b4e3-8395-9eb4-845c1a0b796a"}},{"cell_type":"markdown","source":"## Separating the data from the labels","metadata":{"_uuid":"018df0941f971095212b4a1ef7f29ed2ee195845","_cell_guid":"74de3db1-2188-faf8-77cc-398d2cdeb4e1"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"X = dataset.iloc[:, 1:].values\ny = dataset.iloc[:, 0].values","metadata":{"collapsed":true,"_uuid":"c1aa9b07c61ce654bd12ef51dc07da4c10479ab8","_cell_guid":"c5e1f637-43bb-94b5-1917-0a716c9adbde"}},{"cell_type":"markdown","source":"## Encoding the data and the labels\n\nNeural network only understand numbers. We need to transform the columns with strings into numbers.\n\nWhat we will do is creating categories. \n\nex: Gender: Male as 0 and Female as 1","metadata":{"_uuid":"2f3f6367e65c4ec228b8bc35aad419ebd3ae144b","_cell_guid":"6d7106d7-6369-f017-b62d-461db00f4c0b"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_3 = LabelEncoder()\nX[:, 3] = labelencoder_X_3.fit_transform(X[:, 3])\nlabelencoder_X_6= LabelEncoder()\nX[:, 6] = labelencoder_X_6.fit_transform(X[:, 6])\nlabelencoder_X_9= LabelEncoder()\nX[:, 9] = labelencoder_X_9.fit_transform(X[:, 9])\nlabelencoder_X_13= LabelEncoder()\nX[:, 13] = labelencoder_X_13.fit_transform(X[:, 13])\nlabelencoder_X_15= LabelEncoder()\nX[:, 15] = labelencoder_X_15.fit_transform(X[:, 15])\nlabelencoder_X_19= LabelEncoder()\nX[:, 19] = labelencoder_X_19.fit_transform(X[:, 19])\nX = X.astype(float)\nlabelencoder_y= LabelEncoder()\ny = labelencoder_y.fit_transform(y)","metadata":{"collapsed":true,"_uuid":"b52824fc6b574901d890a60727aae74e37e3bfa4","_cell_guid":"84c438a4-ea6d-0b14-96e0-89d4192db36e"}},{"cell_type":"markdown","source":"## Dummy variable and dummy trap\n\nHere we will create dummy variable for all the categorical data we just encode. (Only if there is more than 2 categories).\n\nWe are doing this because if we leave Single as 0, Married as 1 and Divorced as 2, the network would understand that divorced > married, which doesn't make any sense.\n\nYet we do not want to fall in the dummy variable trap and we will be removing the first column of each of those dummy variables.\n\nWhy?\n\nBecause if we have 1 0 0 for a Single Person now. We could guess it is single even if we had removed the first column: 0 0 (not divorced, not married --> single). This way we can remove some duplicated features on each OneHotEncoding","metadata":{"_uuid":"ac8afa8bad372251e090eac751432dc2441ce1d5","_cell_guid":"5c6294a6-a8fe-16fc-6c44-1f9d56a0c269"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#no dummy trap\nonehotencoder1 = OneHotEncoder(categorical_features = [1])\nX = onehotencoder1.fit_transform(X).toarray()\nX = X[:,1:]\nonehotencoder3 = OneHotEncoder(categorical_features = [4])\nX = onehotencoder3.fit_transform(X).toarray()\nX = X[:,1:]\nonehotencoder6 = OneHotEncoder(categorical_features = [8])\nX = onehotencoder6.fit_transform(X).toarray()\nX = X[:,1:]\nonehotencoder13 = OneHotEncoder(categorical_features = [19])\nX = onehotencoder13.fit_transform(X).toarray()\nX = X[:,1:]\nonehotencoder15 = OneHotEncoder(categorical_features = [28])\nX = onehotencoder15.fit_transform(X).toarray()\nX = X[:,1:]","metadata":{"collapsed":true,"_uuid":"c165477604a9eaea513e799312cda2915237b27c","_cell_guid":"aa0acc68-1f63-3619-7e50-fb43f2c27963"}},{"cell_type":"markdown","source":"## Splitting the dataset into Training and Testing set","metadata":{"_uuid":"33d82399dec4e3a7e90ef35c6e920d3519d8773b","_cell_guid":"85bed163-5ac9-c359-1b5e-a3afa5690e91"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","metadata":{"collapsed":true,"_uuid":"e131643316830ba852198ee44743488bde838458","_cell_guid":"038a0cff-0dc6-f067-c185-dd21e0643f2a"}},{"cell_type":"markdown","source":"## Scaling the features\n\nWe want to have all the features on a similar scale.\n\nThis helps in the computational aspect, but also helps the networks not having some features that look much more important than others by their size.","metadata":{"_uuid":"6351d07df16d1309ea961c6d6e3e754e6d062e6d","_cell_guid":"58f707ea-6cec-a638-5ead-cb6305674ace"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"collapsed":true,"_uuid":"e71633e70a960942a6bc119aa33d3bc5655af1cf","_cell_guid":"125fb11f-4248-7c46-f027-f1e07cf81385"}},{"cell_type":"markdown","source":"## Hyperparameters\n\nTo avoid overfitting such a tiny dataset we will use dropout (randomly putting \"off\" 10% of the neurons to help them become more independent)\n\nAll those parameters are the \"winners\" of a GridSearch I performed locally.","metadata":{"_uuid":"760178fb5f9019bb1295dd3b4577519cbbafb1f5","_cell_guid":"dad7c67a-527e-3964-e170-d5aace2da597"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"dropout = 0.1\nepochs = 100\nbatch_size = 30\noptimizer = 'adam'\nk = 20","metadata":{"collapsed":true,"_uuid":"913a731659c9a328ef2fcb06269dddcb4e464d4f","_cell_guid":"3f23b6e6-cb75-49e5-5ad1-ecd4378d3d43"}},{"cell_type":"markdown","source":"## Training the Neural Network, using a K Fold Cross Validation\n\nFor initializing the weight we will use a truncated normal distribution.\n\nAs we want a probability for output we will use a sigmoid activation function on the output layer.\n\nBecause we are working on a categorization problem with only 2 category we will calculate our loss with binary cross entropy.\n\nWe will use a 10 Fold Cross validation here: the validation data will be pick randomly K times and we will train the network 10 times on each of those training-validation set.","metadata":{"_uuid":"a793fad91548467c7fe27a0faad277357f84b49a","_cell_guid":"c0a0916c-fc79-cc03-a50a-dfd8eeb652ac"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\ndef build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(16, kernel_initializer=\"truncated_normal\", activation = 'relu', input_shape = (X.shape[1],)))\n    classifier.add(Dropout(dropout))\n    classifier.add(Dense(1, kernel_initializer=\"truncated_normal\", activation = 'sigmoid', )) #outputlayer\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = [\"accuracy\"])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, batch_size = batch_size, epochs = epochs, verbose=0)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 30)\nmax = accuracies.max()","metadata":{"collapsed":true,"_uuid":"5a7f452fc92f86227d409610fd4b445fcc4658f0","_cell_guid":"af2f0c5f-7edf-e634-85ee-7cbc6ab99316"}},{"cell_type":"markdown","source":"## Display of the epochs\n\nHere I hide the display of the previous cell because it was too many lines.\n\nIn case you want to run it you just have to remove \"verbose=0\" or put it to 1","metadata":{"_uuid":"a916b84e9d018f240f7b0b6b862542cab862f797","_cell_guid":"1a8277d9-37fe-3831-4da6-be52121bf014"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"print(\"Best accuracy: \",max)","metadata":{"collapsed":true,"_uuid":"89c470cd99b9385296b24123dfe39a138ca8a6c6","_cell_guid":"cbc7d634-f917-2fcc-cf5b-5a36bfac91a0"}},{"cell_type":"markdown","source":"## Done. \n\nFeel free to ask me anything about what was done here.\n\nBest,\n\nCharles","metadata":{"_uuid":"9d8336df2b40d963400df3d6bbafb4ebad4d4297","_cell_guid":"0c426852-c39a-5758-f215-5ace8b38d216"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"","metadata":{"collapsed":true,"_uuid":"29ffbcf393d895bdd5fe7c3e415936b44323fe0d","_cell_guid":"3cecc731-9868-3bc4-b327-c101180b66bc"}}],"metadata":{"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","version":"3.6.0","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python"},"_change_revision":0,"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"_is_fork":false}}