{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Our main goal is to create a machine learning model capable of detecting the difference between a rock or a mine based on the response of the 60 separate sonar frequencies.**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:00.048146Z","iopub.execute_input":"2021-08-08T13:45:00.048605Z","iopub.status.idle":"2021-08-08T13:45:01.027034Z","shell.execute_reply.started":"2021-08-08T13:45:00.048509Z","shell.execute_reply":"2021-08-08T13:45:01.025964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/connectionist-bench-sonar-mines-vs-rocks/sonar.all-data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:01.028528Z","iopub.execute_input":"2021-08-08T13:45:01.028823Z","iopub.status.idle":"2021-08-08T13:45:01.056315Z","shell.execute_reply.started":"2021-08-08T13:45:01.028795Z","shell.execute_reply":"2021-08-08T13:45:01.055467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:01.058517Z","iopub.execute_input":"2021-08-08T13:45:01.058954Z","iopub.status.idle":"2021-08-08T13:45:01.092537Z","shell.execute_reply.started":"2021-08-08T13:45:01.058908Z","shell.execute_reply":"2021-08-08T13:45:01.091405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:01.094091Z","iopub.execute_input":"2021-08-08T13:45:01.094375Z","iopub.status.idle":"2021-08-08T13:45:01.139586Z","shell.execute_reply.started":"2021-08-08T13:45:01.094346Z","shell.execute_reply":"2021-08-08T13:45:01.138886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have 60 frequencies as our features and our label is either R or M wich means we detect Rock or Mine.","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df, x='Label')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:01.1408Z","iopub.execute_input":"2021-08-08T13:45:01.141306Z","iopub.status.idle":"2021-08-08T13:45:01.293049Z","shell.execute_reply.started":"2021-08-08T13:45:01.14126Z","shell.execute_reply":"2021-08-08T13:45:01.292339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,10), dpi=100)\n\nsns.heatmap(df.corr(),cmap=\"BuPu\")","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:01.29414Z","iopub.execute_input":"2021-08-08T13:45:01.294588Z","iopub.status.idle":"2021-08-08T13:45:02.314058Z","shell.execute_reply.started":"2021-08-08T13:45:01.294557Z","shell.execute_reply":"2021-08-08T13:45:02.312933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That purplish color alongside the diognal of our heat map shows that frequencies that are close to each other are somehow more corrolated.","metadata":{}},{"cell_type":"markdown","source":"Now I'm looking for frequencies that are more corrolated with out label so first thing to do is to change my label to 0,1 and I am going to do this using map() in python.","metadata":{}},{"cell_type":"code","source":"df['NumbLabel'] = df['Label'].map({'R':0, 'M':1})","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.315473Z","iopub.execute_input":"2021-08-08T13:45:02.315803Z","iopub.status.idle":"2021-08-08T13:45:02.322738Z","shell.execute_reply.started":"2021-08-08T13:45:02.315771Z","shell.execute_reply":"2021-08-08T13:45:02.32151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see the corrolations in numbers:","metadata":{}},{"cell_type":"code","source":"df.corr()['NumbLabel'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.325381Z","iopub.execute_input":"2021-08-08T13:45:02.32584Z","iopub.status.idle":"2021-08-08T13:45:02.345325Z","shell.execute_reply.started":"2021-08-08T13:45:02.325702Z","shell.execute_reply":"2021-08-08T13:45:02.344177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train|Test Split","metadata":{}},{"cell_type":"markdown","source":"Since Sklearn has no problem with categorical labels, we are going to use out Label feature in its own way although we already convert it to 0 and 1 in NumbLabel.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(['Label', 'NumbLabel'], axis = 1)\ny = df['Label']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=101)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.346968Z","iopub.execute_input":"2021-08-08T13:45:02.347246Z","iopub.status.idle":"2021-08-08T13:45:02.533863Z","shell.execute_reply.started":"2021-08-08T13:45:02.347219Z","shell.execute_reply":"2021-08-08T13:45:02.532611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we're going to use cross validation, these X_test and y_test are more kinda a hold out. Therefor I fill out the test_size with 10 percent.","metadata":{}},{"cell_type":"markdown","source":"In KNN modeling, feature scaling is neccary because we are caculating distance and all our feature should have the same unit in order to help us comparing them better.\nThus, I am going to make a pipeline incluing scaling and out knn model.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nscaler = StandardScaler()\nknn = KNeighborsClassifier()\n\noperations = [('scaler', scaler),('knn', knn)]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.535073Z","iopub.execute_input":"2021-08-08T13:45:02.535393Z","iopub.status.idle":"2021-08-08T13:45:02.676872Z","shell.execute_reply.started":"2021-08-08T13:45:02.535362Z","shell.execute_reply":"2021-08-08T13:45:02.675858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\npipe = Pipeline(operations)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.678064Z","iopub.execute_input":"2021-08-08T13:45:02.678369Z","iopub.status.idle":"2021-08-08T13:45:02.684428Z","shell.execute_reply.started":"2021-08-08T13:45:02.67834Z","shell.execute_reply":"2021-08-08T13:45:02.683545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next step we are going to make a grid seearch and testing out diffrect values for k to find out wich k is best for out model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nk_values = list(range(1,30))\nparam_grid = {'knn__n_neighbors': k_values}","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.68575Z","iopub.execute_input":"2021-08-08T13:45:02.686302Z","iopub.status.idle":"2021-08-08T13:45:02.693962Z","shell.execute_reply.started":"2021-08-08T13:45:02.686244Z","shell.execute_reply":"2021-08-08T13:45:02.693046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_classifier = GridSearchCV(pipe, param_grid, cv = 5, scoring='accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.695194Z","iopub.execute_input":"2021-08-08T13:45:02.695539Z","iopub.status.idle":"2021-08-08T13:45:02.706098Z","shell.execute_reply.started":"2021-08-08T13:45:02.695502Z","shell.execute_reply":"2021-08-08T13:45:02.705238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_classifier.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:02.707613Z","iopub.execute_input":"2021-08-08T13:45:02.707963Z","iopub.status.idle":"2021-08-08T13:45:04.662356Z","shell.execute_reply.started":"2021-08-08T13:45:02.707933Z","shell.execute_reply":"2021-08-08T13:45:04.661299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The grid-search has been performed now let's see what's our best parameters:","metadata":{}},{"cell_type":"code","source":"cv_classifier.best_estimator_.get_params()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:04.663583Z","iopub.execute_input":"2021-08-08T13:45:04.663864Z","iopub.status.idle":"2021-08-08T13:45:04.671586Z","shell.execute_reply.started":"2021-08-08T13:45:04.663836Z","shell.execute_reply":"2021-08-08T13:45:04.67054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(cv_classifier.cv_results_)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:04.672889Z","iopub.execute_input":"2021-08-08T13:45:04.673251Z","iopub.status.idle":"2021-08-08T13:45:04.72417Z","shell.execute_reply.started":"2021-08-08T13:45:04.673154Z","shell.execute_reply":"2021-08-08T13:45:04.723173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the results for diffrent n_neighbors in above data frame","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(9,6), dpi=100)\nplt.plot(pd.DataFrame(cv_classifier.cv_results_)['mean_test_score'])","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:04.725529Z","iopub.execute_input":"2021-08-08T13:45:04.725808Z","iopub.status.idle":"2021-08-08T13:45:04.916099Z","shell.execute_reply.started":"2021-08-08T13:45:04.72578Z","shell.execute_reply":"2021-08-08T13:45:04.91502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This prove why we should use only 1 neighbor because as we can see as our k value increase our accuracy decrease and k=1 has the best accuracy","metadata":{}},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"y_pred = cv_classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:04.917412Z","iopub.execute_input":"2021-08-08T13:45:04.917792Z","iopub.status.idle":"2021-08-08T13:45:04.926582Z","shell.execute_reply.started":"2021-08-08T13:45:04.917755Z","shell.execute_reply":"2021-08-08T13:45:04.925636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nconfusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:04.927999Z","iopub.execute_input":"2021-08-08T13:45:04.928296Z","iopub.status.idle":"2021-08-08T13:45:04.940809Z","shell.execute_reply.started":"2021-08-08T13:45:04.928251Z","shell.execute_reply":"2021-08-08T13:45:04.939765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-08T13:45:04.942117Z","iopub.execute_input":"2021-08-08T13:45:04.942559Z","iopub.status.idle":"2021-08-08T13:45:04.958756Z","shell.execute_reply.started":"2021-08-08T13:45:04.942518Z","shell.execute_reply":"2021-08-08T13:45:04.957596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}