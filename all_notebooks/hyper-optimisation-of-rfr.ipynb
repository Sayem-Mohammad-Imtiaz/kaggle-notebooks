{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA of US cars for auction"},{"metadata":{},"cell_type":"markdown","source":"1. Brief inspection of data\n2. Data visualisations and exploratory analysis\n3. Categoric transformations\n4. Model evaluations\n5. Hyper optimisation of Random Forest\n6. Feature evaluation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sqlite3 as sq3\nimport matplotlib\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"us_car_file_path = pd.read_csv('../input/usa-cers-dataset/USA_cars_datasets.csv')\nus = us_car_file_path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SQL Connection**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"conn = sq3.connect('USA_cars_datasets.db')\nus.to_sql('cars', conn, if_exists='replace', index=False)\ne = pd.read_sql_query","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set(font_scale=2, style='white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Brief inspection of data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print (us.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print (us.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print (us.head())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print (us.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data visualisation and exploratory analysis"},{"metadata":{},"cell_type":"markdown","source":"**Origin of cars by state**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%HTML\n<div class='tableauPlaceholder' id='viz1588413042357' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ca&#47;Car_state&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Car_state&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ca&#47;Car_state&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1588413042357');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model_quantity = e('''\n                    select count(brand) as total, brand\n                    from cars\n                    group by brand\n                    order by total desc\n                    ''', conn)\n\nchart = sns.catplot(x='brand', y='total', data=model_quantity, kind='bar', height=10, aspect=3, palette='plasma')\nchart.set_xticklabels(rotation=90)\nchart.set_axis_labels('Vehicle Brand','Number of Vehicles')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"price_brand = e('''\n                select avg(price) as average, brand\n                from cars\n                group by brand\n                order by average desc\n                \n                ''', conn)\n\nplt.figure(figsize=(26,12))\nchart = sns.barplot(x='brand',y='average', data=price_brand)\nplt.xticks(rotation=90)\nplt.xlabel('Vehicle Brand')\nplt.ylabel('Average Price [$]')\nplt.title('Average Price of Vehicle By Brand')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cars_comp = e('''\n                select price as price_$, brand, mileage as mileage_km, year\n                from cars\n                where brand in('ford', 'dodge', 'nissan', 'chevrolet')\n                \n                ''', conn)\n\n\n\nchart = sns.pairplot(cars_comp, hue='brand', palette='husl', height=9, aspect=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the figures above we have some vehicles set at zero for price."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"zero_price = us.loc[us['price'] == 0]\nprint ('Number of cars set at zero for price =', len(zero_price))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"car_median = (us['price'].median())\nus['price'] = us['price'].replace(0, car_median)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill the cells with the mean value from the price column"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"zero_price = us.loc[us['price'] == 0]\nprint ('Number of cars set at zero for price =',len(zero_price))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Investigate the price variation between the classifications 'clean vehicle' and 'salvage insurance'."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"price_status = e('''\n                    select title_status, price, brand\n                    from cars\n                    where brand in ('ford', 'dodge', 'nissan', 'chevrolet')\n                    \n                    ''', conn)\n\nchart = sns.catplot(x='brand', y='price', hue='title_status', data=price_status, kind='bar', height=11, aspect=2, palette='BuGn_r', saturation=1)\nchart.set_axis_labels('Vehicle Brand', 'Price [$]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Categoric Transformations\n\n**Categorical conversions and bin distributions**"},{"metadata":{},"cell_type":"markdown","source":"Earlier we observed the different datatypes in the dataset, now we make some more subtle observations. \nFirst, the year the car was made is better off as a categorical variable rather than numeric, if we carry on with the model as is when we evaluate feaures at the end of the model we would see that the year the car was made gets an insignificant weighting, as a categorical variable it's importance is scaled better. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"us['year'] = [str(i) for i in us['year']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, the mileage can be distributed to unequal sized bins. The difference in mileage on cars with lower mileages scales much greater than large differences of mileage for cars with a higher amount of mileage recorded."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"bin_labels = ['1','2','3','4','5','6','7','8','9','10','11','12']\ncut_bins = [0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000,250000,1000000]\nus['mileage_class'] = pd.cut(us['mileage'], bins=cut_bins, labels=bin_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature selection**"},{"metadata":{},"cell_type":"markdown","source":"The vin and the lot number can be dropped from the dataframe as they will cause of overfitting of the model, naturally a reg and a lot number are not contributing factors when determining the price of a car, also, we renamed the mileage according to a class number so we can drop the mileage column. As we have only 8 cars from Canada we will drop country from the dataframe as well."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"numeric_features = us.select_dtypes(include=['int64','float64'])\nnumeric_features = numeric_features.drop(['Unnamed: 0', 'lot','price','mileage'], axis=1)\nnumeric_features = list(numeric_features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"categoric_features = us.select_dtypes(include=['object','category'])\ncategoric_features = categoric_features.drop(['vin','country'], axis=1)\ncategoric_features = list(categoric_features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_features =  categoric_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X = us.drop(['price'],axis=1)[all_features]\ny = us['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=1)\n\n\ncat_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n    (\"onehot\", OneHotEncoder(handle_unknown='ignore'))])\n\nprocess_features = ColumnTransformer(transformers=[('cat', cat_transformer, categoric_features)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Model Evaluations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"models = []\nmodels.append(('SVM', svm.SVR()))\nmodels.append(('DTR', tree.DecisionTreeRegressor()))\nmodels.append(('RFR', RandomForestRegressor()))\n\nresults = []\nnames = []\nresults_mean = []\nfor name, model in models:\n    pl = Pipeline([('process', process_features), ('models', model)])\n    kf = KFold(n_splits=10)\n    cv_results = cross_val_score(pl, X_train, y_train, cv=kf, scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    results_mean.append(cv_results.mean())\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model_comp = {'Model':names,'Results':results_mean}\nmodel_comp = pd.DataFrame(data=model_comp)\nplt.figure(figsize=(14,7))\nchart = sns.barplot(x='Model',y='Results',data=model_comp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Hyper optimisation of Random Forest"},{"metadata":{},"cell_type":"markdown","source":"We will use RandomizedSearchCV to evaluate the best parameters for the RFR"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n_estimators = [10,20,50,75,100,200]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 150, num = 10)]\nmax_depth.append(None)\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [1, 2, 3, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, instantiate the RSCV by incorporating it into our pipeline"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rf_random = RandomizedSearchCV(estimator=RandomForestRegressor(),param_distributions = random_grid, n_iter = 100, \n                               cv = 3, verbose=1, random_state=42)\n\npl_rand = Pipeline([('process', process_features), ('models', rf_random)])\npl_rand.fit(X_train, y_train)\n\nprint (pl_rand['models'].best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will compare the base model from earlier to the tuned model using the parameters printed above to see if the tuned parameters make a significant difference from the base model."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rf_random_tuned = RandomForestRegressor(n_estimators=200, min_samples_split=2,min_samples_leaf=1,\n                                        max_features='sqrt',max_depth=87,bootstrap=False)\n\nresults_tuned = []\n\npl_rf = Pipeline([('process', process_features), ('model', rf_random_tuned)])\nkf = KFold(n_splits=10)\ncv_results = cross_val_score(pl_rf, X_train, y_train, cv=kf, scoring='r2')\nresults_tuned.append(cv_results.mean())\nprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"comp = results_mean + results_tuned\nnames_ = ['SVM','DTR','RFR','RFR_Tuned']\n\n\nresult_params = {'Model':names_,'Score':comp}\nresult_params = pd.DataFrame(data=result_params)\n\nplt.figure(figsize=(14,7))\nchart = sns.barplot(x='Model',y='Score',data=result_params)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the tuned parameters with RSCV we have managed a ~3% increase in R2 score, which isn't neglible."},{"metadata":{},"cell_type":"markdown","source":"### 6. Feature evaluation"},{"metadata":{},"cell_type":"markdown","source":"1. Get encoded column names from columntransformer\n2. Store names in a list and zip together with results from RandomForestRegressor feature_importances_\n3. Evaluate the top results i.e. features with the most weighting"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pl_rf.fit(X_train, y_train)\nonehot_cols = list(pl_rf.named_steps['process'].\n                      named_transformers_['cat'].\n                      named_steps['onehot'].\n                      get_feature_names(input_features=categoric_features))\n\n\nfeature_list = onehot_cols\n\n\nimp = pl_rf['model'].feature_importances_\n\n\nfeature_imp = zip(feature_list,imp)\n\ntop_features = []\n\nfor i in feature_imp:\n    top_features.append(i)\n    \ntop_features = pd.DataFrame(data=top_features,columns=['Feature','Weight'])\ntop_features = top_features.sort_values(by='Weight',ascending=False).head(n=30)\n\nplt.figure(figsize=(26,12))\nchart = sns.barplot(x='Feature',y='Weight', data=top_features, saturation=1, palette='plasma')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can make some estimations about the importance of some features from the figure above.\n1. Model, mileage and condition carry the most weighting in general\n2. The status of a vehicle i.e salvage or clean carry the same weighting, which we would expect, so it's reassuring to see\n3. While some states appear in the top results of the feature evaluation, this could detract from the generalistion of a model if it were included in further optimising the model\n "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}