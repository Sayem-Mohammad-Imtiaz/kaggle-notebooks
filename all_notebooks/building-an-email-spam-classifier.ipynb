{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-06T12:59:29.349934Z","iopub.execute_input":"2021-08-06T12:59:29.350297Z","iopub.status.idle":"2021-08-06T12:59:29.371045Z","shell.execute_reply.started":"2021-08-06T12:59:29.350224Z","shell.execute_reply":"2021-08-06T12:59:29.370425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook is serving two purpose:\n\n1. As the Intro notebook for my first dataset on kaggle. üéâ\n2. My solution to the classification exercise i have been following up on the book <b color=\"#900C3F\">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow Concepts</b>. The exercise is on building spam classifer using the [Spam Assassin Public Dataset](https://homl.info/spamassassin). I already made a comprehensive dataset from the source thou.\n\n**I hope you find this notebook inciteful üòä**\n\n#### Question\n\nBuild a spam classifier (a more challenging exercise):\n\n-  Download examples of spam and ham from Apache SpamAssassin‚Äôs public datasets. [link](https://homl.info/spamassassin)\n-  Unzip the datasets and familiarize yourself with the data format.\n-  Split the datasets into a training set and a test set.\n-  Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word. For example, if all emails only ever contain four words, ‚ÄúHello,‚Äù ‚Äúhow,‚Äù ‚Äúare,‚Äù ‚Äúyou,‚Äù then the email ‚ÄúHello you Hello Hello you‚Äù would be converted into a vector [1, 0, 0, 1] (meaning [‚ÄúHello‚Äù is present, ‚Äúhow‚Äù is absent, ‚Äúare‚Äù is absent, ‚Äúyou‚Äù is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word. \n\nYou may want to add hyperparameters to your preparation pipeline to control whether or not to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with ‚ÄúURL,‚Äù replace all numbers with ‚ÄúNUMBER,‚Äù or even perform stemming (i.e., trim off word endings; there are Python libraries available to do this).\n\n**Download examples of spam and ham from Apache SpamAssassin‚Äôs public datasets.‚úîÔ∏è**\n\n**Unzip the datasets and familiarize yourself with the data format.‚úîÔ∏è**","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(\"../input/spam-assassin-email-classification-dataset/spam_assassin.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:05:27.642627Z","iopub.execute_input":"2021-08-06T13:05:27.642937Z","iopub.status.idle":"2021-08-06T13:05:28.230636Z","shell.execute_reply.started":"2021-08-06T13:05:27.642909Z","shell.execute_reply":"2021-08-06T13:05:28.22976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:06:02.42857Z","iopub.execute_input":"2021-08-06T13:06:02.428877Z","iopub.status.idle":"2021-08-06T13:06:02.455183Z","shell.execute_reply.started":"2021-08-06T13:06:02.428851Z","shell.execute_reply":"2021-08-06T13:06:02.454349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data, target = dataset.text, dataset.target","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:06:13.738215Z","iopub.execute_input":"2021-08-06T13:06:13.738543Z","iopub.status.idle":"2021-08-06T13:06:13.745782Z","shell.execute_reply.started":"2021-08-06T13:06:13.738511Z","shell.execute_reply":"2021-08-06T13:06:13.74456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:06:25.016319Z","iopub.execute_input":"2021-08-06T13:06:25.0166Z","iopub.status.idle":"2021-08-06T13:06:26.059592Z","shell.execute_reply.started":"2021-08-06T13:06:25.016573Z","shell.execute_reply":"2021-08-06T13:06:26.058677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n\nfor train_index, test_index in sss.split(data, target):\n    train_X, test_X = data.loc[train_index], data.loc[test_index]\n    train_y, test_y = target.loc[train_index], target.loc[test_index]","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:06:39.543061Z","iopub.execute_input":"2021-08-06T13:06:39.543489Z","iopub.status.idle":"2021-08-06T13:06:39.555903Z","shell.execute_reply.started":"2021-08-06T13:06:39.543462Z","shell.execute_reply":"2021-08-06T13:06:39.555269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split the datasets into a training set and a test set.‚úîÔ∏è**\n\nI already went through the process of extracting irrelevant and meaningless words from the spam dataset and serializing them with pickle, thus upon load we can join them with the `nltk.corpus.stopwords.words('english')` to create a much better set of stopwords for the `TfidfVectorizer`.","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:07:30.064856Z","iopub.execute_input":"2021-08-06T13:07:30.0653Z","iopub.status.idle":"2021-08-06T13:07:30.851022Z","shell.execute_reply.started":"2021-08-06T13:07:30.065273Z","shell.execute_reply":"2021-08-06T13:07:30.850481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_assassin_stopwords = pickle.load(open('../input/spamassassin-stopwords/spamassassin_stopwords.p', 'rb'))\nenglish_stopwords = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:10:27.749723Z","iopub.execute_input":"2021-08-06T13:10:27.750111Z","iopub.status.idle":"2021-08-06T13:10:27.776064Z","shell.execute_reply.started":"2021-08-06T13:10:27.750059Z","shell.execute_reply":"2021-08-06T13:10:27.77546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = spam_assassin_stopwords + english_stopwords","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:10:38.859042Z","iopub.execute_input":"2021-08-06T13:10:38.8595Z","iopub.status.idle":"2021-08-06T13:10:38.862851Z","shell.execute_reply.started":"2021-08-06T13:10:38.859475Z","shell.execute_reply":"2021-08-06T13:10:38.86223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(stop_words=stop_words, token_pattern=r'(?u)\\b([a-zA-Z]{4,12})\\b')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:10:50.675338Z","iopub.execute_input":"2021-08-06T13:10:50.675612Z","iopub.status.idle":"2021-08-06T13:10:50.679716Z","shell.execute_reply.started":"2021-08-06T13:10:50.675589Z","shell.execute_reply":"2021-08-06T13:10:50.678866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf.fit(train_X)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:11:05.63357Z","iopub.execute_input":"2021-08-06T13:11:05.633966Z","iopub.status.idle":"2021-08-06T13:11:06.988889Z","shell.execute_reply.started":"2021-08-06T13:11:05.633941Z","shell.execute_reply":"2021-08-06T13:11:06.988394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Write a data preparation pipeline to convert each email into a feature vector.‚úîÔ∏è**\n\nThe question actually required that I make use of the `CountVectorizer`, but I opt-for the `TfidfVectorizer` because it takes term frequency into account.\n\n## Choosing a model\n\nNow that we've accomplished that! Let's proceed to testing this models in various classifiers. I will be working just two classifiers, the decision tree and the random forest classifier.","metadata":{}},{"cell_type":"code","source":"train_X = tfidf.transform(train_X)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:11:48.587078Z","iopub.execute_input":"2021-08-06T13:11:48.587546Z","iopub.status.idle":"2021-08-06T13:11:49.859082Z","shell.execute_reply.started":"2021-08-06T13:11:48.587518Z","shell.execute_reply":"2021-08-06T13:11:49.858242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:12:01.667617Z","iopub.execute_input":"2021-08-06T13:12:01.667933Z","iopub.status.idle":"2021-08-06T13:12:01.800271Z","shell.execute_reply.started":"2021-08-06T13:12:01.6679Z","shell.execute_reply":"2021-08-06T13:12:01.798838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_clf = DecisionTreeClassifier(random_state=0)\ncross_val_score(dt_clf, train_X, train_y, cv=5, n_jobs=3)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:12:31.267465Z","iopub.execute_input":"2021-08-06T13:12:31.267785Z","iopub.status.idle":"2021-08-06T13:12:35.049686Z","shell.execute_reply.started":"2021-08-06T13:12:31.267753Z","shell.execute_reply":"2021-08-06T13:12:35.048371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_clf = RandomForestClassifier(random_state=0)\ncross_val_score(rf_clf, train_X, train_y, cv=5, n_jobs=3)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:12:58.475753Z","iopub.execute_input":"2021-08-06T13:12:58.476066Z","iopub.status.idle":"2021-08-06T13:13:03.635131Z","shell.execute_reply.started":"2021-08-06T13:12:58.476039Z","shell.execute_reply":"2021-08-06T13:13:03.634616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the random forest model performs best we'll be making use of it. Let's now move on to fine tune the model.\n\n## Fine-tuning the model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:13:29.475324Z","iopub.execute_input":"2021-08-06T13:13:29.475757Z","iopub.status.idle":"2021-08-06T13:13:29.478912Z","shell.execute_reply.started":"2021-08-06T13:13:29.47573Z","shell.execute_reply":"2021-08-06T13:13:29.478435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space = {\n    'n_estimators': range(100, 351, 50),\n    'bootstrap': [True, False]\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:13:42.37586Z","iopub.execute_input":"2021-08-06T13:13:42.376283Z","iopub.status.idle":"2021-08-06T13:13:42.380547Z","shell.execute_reply.started":"2021-08-06T13:13:42.376258Z","shell.execute_reply":"2021-08-06T13:13:42.37886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search = GridSearchCV(rf_clf, space, cv=3, n_jobs=-1, scoring='accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:13:58.683491Z","iopub.execute_input":"2021-08-06T13:13:58.683849Z","iopub.status.idle":"2021-08-06T13:13:58.68855Z","shell.execute_reply.started":"2021-08-06T13:13:58.683824Z","shell.execute_reply":"2021-08-06T13:13:58.687696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:14:15.706353Z","iopub.execute_input":"2021-08-06T13:14:15.70678Z","iopub.status.idle":"2021-08-06T13:15:26.340355Z","shell.execute_reply.started":"2021-08-06T13:14:15.706742Z","shell.execute_reply":"2021-08-06T13:15:26.339062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:15:26.341662Z","iopub.execute_input":"2021-08-06T13:15:26.341879Z","iopub.status.idle":"2021-08-06T13:15:26.348364Z","shell.execute_reply.started":"2021-08-06T13:15:26.341853Z","shell.execute_reply":"2021-08-06T13:15:26.347473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = grid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:15:26.349939Z","iopub.execute_input":"2021-08-06T13:15:26.350326Z","iopub.status.idle":"2021-08-06T13:15:26.364952Z","shell.execute_reply.started":"2021-08-06T13:15:26.350254Z","shell.execute_reply":"2021-08-06T13:15:26.364062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:15:26.366121Z","iopub.execute_input":"2021-08-06T13:15:26.36646Z","iopub.status.idle":"2021-08-06T13:15:34.655524Z","shell.execute_reply.started":"2021-08-06T13:15:26.366433Z","shell.execute_reply":"2021-08-06T13:15:34.653584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X = tfidf.transform(test_X)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:15:34.657831Z","iopub.execute_input":"2021-08-06T13:15:34.65828Z","iopub.status.idle":"2021-08-06T13:15:35.270958Z","shell.execute_reply.started":"2021-08-06T13:15:34.658249Z","shell.execute_reply":"2021-08-06T13:15:35.269882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_X)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:15:42.657971Z","iopub.execute_input":"2021-08-06T13:15:42.658496Z","iopub.status.idle":"2021-08-06T13:15:42.902193Z","shell.execute_reply.started":"2021-08-06T13:15:42.658461Z","shell.execute_reply":"2021-08-06T13:15:42.901211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:16:04.579373Z","iopub.execute_input":"2021-08-06T13:16:04.579697Z","iopub.status.idle":"2021-08-06T13:16:04.584902Z","shell.execute_reply.started":"2021-08-06T13:16:04.579672Z","shell.execute_reply":"2021-08-06T13:16:04.583187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(predictions, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:16:16.179268Z","iopub.execute_input":"2021-08-06T13:16:16.179691Z","iopub.status.idle":"2021-08-06T13:16:16.188018Z","shell.execute_reply.started":"2021-08-06T13:16:16.179665Z","shell.execute_reply":"2021-08-06T13:16:16.185948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üéâ... **99% Accuracy on the test set!!!**\n\nWell that concludes this exercise for me! I can't wait to see what the pro's can actually make out of this üòä.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}