{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking and updating Null Values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can convert Age to Int\ndf.age=df.age.astype(int)\nprint(df.dtypes)\nprint('shape:',df.shape)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.platelets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can see that the death rate is high for people over 80\nplt.scatter(df.age,df.DEATH_EVENT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Death Rate for less than 50 y/o:',int(df[df['age']<=50].DEATH_EVENT.sum()*100/df[df['age']<=50].shape[0]),'%')\nprint('Death Rate for more than 80 y/o:',int(df[df['age']>=80].DEATH_EVENT.sum()*100/df[df['age']>=80].shape[0]),'%')\nprint('Death Rate for more than 50 and less than 80 y/o:',int(df[(df['age']>50)&(df['age']<80)].DEATH_EVENT.sum()*100/df[(df['age']>50)&(df['age']<80)].shape[0]),'%')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# We can see that there is a significant difference between the death rates for people below and above the 80 y/o mark\n#so we split them into two categories Old and Not Old\nbins = [0,80,100]\ngroup_names = ['MiddleAged', 'Old']\ndf['Age_bin']=pd.cut(df['age'],bins,labels=group_names)\ndf.drop('age',axis=1,inplace=True)\nage_variables = pd.get_dummies(df['Age_bin'])\nage_variables.drop('Old',axis=1,inplace=True)\ndf=pd.concat([df,age_variables],axis=1)\ndf.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Age_bin',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model training\nfrom sklearn.model_selection import train_test_split\nX=df.drop('DEATH_EVENT',axis=1)\ny=df['DEATH_EVENT']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.28,random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initially we do not perform any hyperparameter tuning, we use the default parameters itself\nfrom sklearn.ensemble import RandomForestClassifier\nrf_random=RandomForestClassifier()\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(rf_random.feature_importances_)\nfeat_importances=pd.Series(rf_random.feature_importances_,index=X.columns)\nfeat_importances.nlargest(11).plot(kind='barh')\nplt.show()\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nprint('Chi Square:')\nprint(chi2(X,y))\nprint('F:')\nprint(f_classif(X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can see that Diabetes, Sex, Smoking have almost no correlation with Death, so we drop them.\nX_train.drop(['diabetes','sex','smoking'],axis=1,inplace=True)\nX_test.drop(['diabetes','sex','smoking'],axis=1,inplace=True)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random=RandomForestClassifier()\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Classifier with GridSearch CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"o=[]\nfor i in range (1,100):\n    o.append(i)\nfrom sklearn.model_selection import GridSearchCV\nparameters=[{'n_estimators':o,'criterion':['gini','entropy']}]\ngrid_search=GridSearchCV(estimator=rf_random,param_grid=parameters,scoring='accuracy',cv=10)\ngrid_search=grid_search.fit(X_train,y_train)\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random=RandomForestClassifier(n_estimators=49,criterion='gini')\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weight=({0:1,1:1.8})\nrf_random=RandomForestClassifier(n_estimators=59,criterion='entropy',class_weight=class_weight)\nrf_random.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, rf_random.predict(X_test)))\nprint(classification_report(y_test,rf_random.predict(X_test)))\nprint(confusion_matrix(y_test,rf_random.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogmodel=LogisticRegression()\nlogmodel.fit(X_train,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test)))\nprint(classification_report(y_test,logmodel.predict(X_test)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we try feature scaling for improving the model performance\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train_scaled=sc.fit_transform(X_train)\nX_test_scaled=sc.transform(X_test)\nX_train_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel=LogisticRegression()\nlogmodel.fit(X_train_scaled,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test_scaled)))\nprint(classification_report(y_test,logmodel.predict(X_test_scaled)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the performance doesn't improve, we now try the MinMax scaler\nfrom sklearn import preprocessing\nminmaxscaler=preprocessing.MinMaxScaler(feature_range=(0,1))\nX_train_scaled=minmaxscaler.fit_transform(X_train)\nX_test_scaled=minmaxscaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel=LogisticRegression()\nlogmodel.fit(X_train_scaled,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test_scaled)))\nprint(classification_report(y_test,logmodel.predict(X_test_scaled)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#by adding class weight to balance the dataset\nclass_weight=({0:1,1:1.8})\nlogmodel=LogisticRegression(class_weight=class_weight)\nlogmodel.fit(X_train_scaled,y_train)\nprint('Accuracy score:',accuracy_score(y_test, logmodel.predict(X_test_scaled)))\nprint(classification_report(y_test,logmodel.predict(X_test_scaled)))\nprint(confusion_matrix(y_test,logmodel.predict(X_test_scaled)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Algorithm with GridSearchCv and Class weights is the best model","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}