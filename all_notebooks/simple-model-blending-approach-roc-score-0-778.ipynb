{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Solution code for Analytics Vidhya Janata Hack 2020. This is a very basic approach using simple model blending technique. **\n\n**But this can act as a pipeline for any machine learning competition for beginners. Upvote it if you like it.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing lib\n\nimport numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading data\ntrain= pd.read_csv('/kaggle/input/av-janata-hack-payment-default-prediction/train_20D8GL3.csv')\ntest= pd.read_csv('/kaggle/input/av-janata-hack-payment-default-prediction/test_O6kKpvt.csv')\nsample= pd.read_csv('/kaggle/input/av-janata-hack-payment-default-prediction/sample_submission_gm6gE0l.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data cleaning steps**\n1. Correcting: check for outliers or ambigous data and fix\n2. Completing: deal with missing values\n3. Creating: mainly feature engineering\n4. Converting: Encoding for categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#step1: correcting\n#Check data description, these have kinda non-existent labels,so needed fix\n#marriage col- should have 1,2,3 but it has an unlabelled 0 as well. replace 0 with 3(others category).\n#education col- similary education has unlabelled 6 and 0.\n#PAY_0 TO PAY_6 has ambiguity in form of -1 and -2 values.\n\nall_data = [train, test]     #to perform ops on train+test both\nfor df in all_data:\n    df['MARRIAGE'].replace({0 : 3},inplace = True)\n    df[\"EDUCATION\"].replace({6 : 5, 0 : 5}, inplace = True)\n    df[\"PAY_0\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_2\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_3\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_4\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_5\"].replace({-1 : 0, -2 : 0}, inplace = True)\n    df[\"PAY_6\"].replace({-1 : 0, -2 : 0}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#step2: completing- dealing with null values\nprint(train.isnull().sum())\nprint('-----------------')\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No Null values in the dataset- step 2 done."},{"metadata":{"trusted":true},"cell_type":"code","source":"#step3: creating (feature engg)\n#created Age bins out of Age but it didn't turned out to be an important feature, will update soon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#step4: converting (encoding) + feature scaling here if using models other than tree based.\ncat_cols = [\"SEX\",\"MARRIAGE\",\"EDUCATION\"]\ntrain = pd.get_dummies(train, columns = cat_cols, prefix=['SEX','MARRIAGE','EDUCATION'])\ntest = pd.get_dummies(test, columns = cat_cols, prefix=['SEX','MARRIAGE','EDUCATION'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### All 4 steps of data cleaning process completed. \n### To do: Experiment with different encoding methods like target encoding and do feature engineering in later versions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#final check on the data\nprint(train.shape)\nprint(test.shape)\nprint('--------------------')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove ID and target field from features list\nfeatures= [\n 'LIMIT_BAL',\n 'AGE',\n 'PAY_0',\n 'PAY_2',\n 'PAY_3',\n 'PAY_4',\n 'PAY_5',\n 'PAY_6',\n 'BILL_AMT1',\n 'BILL_AMT2',\n 'BILL_AMT3',\n 'BILL_AMT4',\n 'BILL_AMT5',\n 'BILL_AMT6',\n 'PAY_AMT1',\n 'PAY_AMT2',\n 'PAY_AMT3',\n 'PAY_AMT4',\n 'PAY_AMT5',\n 'PAY_AMT6',\n 'SEX_1',\n 'SEX_2',\n 'MARRIAGE_1',\n 'MARRIAGE_2',\n 'MARRIAGE_3',\n 'EDUCATION_1',\n 'EDUCATION_2',\n 'EDUCATION_3',\n 'EDUCATION_4',\n 'EDUCATION_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting: for local validation, later will train model on all of train set without split\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y= train_test_split(train[features], train.default_payment_next_month,test_size= 0.2, random_state=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_x.shape)\nprint(train_y.shape)\nprint('---------------')\nprint(test_x.shape)\nprint(test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modelling starts here\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import SGDClassifier\n\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import confusion_matrix, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    #ensemble\n    AdaBoostClassifier(),\n    ExtraTreesClassifier(),\n    GradientBoostingClassifier(),\n    RandomForestClassifier(),\n    \n    #linear models\n    LogisticRegression(),\n          \n    XGBClassifier(),\n    LGBMClassifier(),\n    CatBoostClassifier()\n         ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models = pd.DataFrame(columns=['Model_name','ROC'])\n\ni=0\nfor model in models:\n    model.fit(train_x,train_y)\n    pred_y = model.predict(test_x)\n    proba = model.predict_proba(test_x)[:,1]\n    roc_score = roc_auc_score(test_y, proba)\n    name = str(model)\n    print(name[0:name.find(\"(\")])\n    df_models.loc[i,'Model_name']= name[0:name.find(\"(\")]\n \n    df_models.loc[i,'ROC']= roc_score\n    print(confusion_matrix(test_y,pred_y))\n    print(\"------------------------------------------------------------\")\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_models.sort_values('ROC', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We have our top 3 models as Gradient boosting classifier, lgbm and catboost.\n#### TO DO: Hypertuning for all these models, pass categorical fields to catboost. Also, feature scaling could help logistic regression perform much better."},{"metadata":{"trusted":true},"cell_type":"code","source":"#for submission: \n# Model blend from all three models and train on all of training dataset this time.\n\nmodel= GradientBoostingClassifier()\nmodel.fit(train[features],train.default_payment_next_month)\npp1= model.predict_proba(test[features])\n\nmodel2= LGBMClassifier()\nmodel2.fit(train[features],train.default_payment_next_month)\npp2= model2.predict_proba(test[features])\n\nmodel3= CatBoostClassifier()\nmodel3.fit(train[features],train.default_payment_next_month)\npp3= model3.predict_proba(test[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using very simple average blend\npp_blend= (pp1 +pp2+pp3)/3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp_blend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission file\nsub = pd.DataFrame({'ID':test['ID'],'default_payment_next_month':pp_blend[:,1]})\nsub.to_csv('blend cat+gradient+lgbm.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thank you. Here are few things which could help score better.\n1. Use better validation strategy (stratified k fold)\n2. Use model stacking technique\n3. Use gridsearchcv or randomsearch for hypertuning of parameters\n4. Better feature engineering\n5. Scaling (Minmax scalar) could help linear models like logistic regression to perfom better"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}