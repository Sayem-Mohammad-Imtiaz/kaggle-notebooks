{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture \n\nfrom sklearn.decomposition import PCA\n\nfrom scipy.spatial.distance import cdist\nfrom matplotlib.patches import Ellipse\n\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading and preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Read","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_index('CUST_ID', inplace=True)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nans","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_sample = df.isnull().sum().sort_values(ascending=False)\nnan_sample = nan_sample[nan_sample > 0]\nnan_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in nan_sample.index:\n    df.loc[df[i].isnull(), i] = df[i].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Duplicated","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers in data (log scale)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(14, 10))\ndf.boxplot()\nplt.yscale('log')\nplt.xticks(rotation=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Determine the optimal number of components for a given variance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"options = [MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler]\n\nvar = 0.95\npca = PCA(n_components=var, svd_solver='full')\n\nfor name_scaler in options:\n    scaler = name_scaler() \n    scaled = scaler.fit_transform(df.values)\n    \n    pca_values = pca.fit_transform(scaled)\n    print(f\"{name_scaler.__name__}\\nOptimal number: {pca.n_components_}\\nVariance: {np.sum(pca.explained_variance_ratio_)}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm, MinMaxScaler","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"PCA $\\leftarrow$ 2 components (for vizualize)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sse = lambda values: dict((k, KMeans(n_clusters=k, max_iter=10000).fit(values).inertia_) for k in range(1, 20))\n\nfig = plt.figure(figsize=(16, len(options) * 4))\n\nfor i, name_scaler in enumerate(options):\n    scaler = name_scaler() \n    scaled = scaler.fit_transform(df.values)\n    d = sse(scaled)\n    \n    fig.add_subplot(len(options) // 2 + 1, 2, i + 1)\n    \n    plt.bar(x=d.keys(), height=d.values(), width=1, edgecolor='k', facecolor='orange', label=f\"{name_scaler.__name__}\")\n    plt.plot(list(d.keys()), list(d.values()), 'ro-')\n    plt.xlabel(\"Number of cluster\")\n#     plt.ylabel(\"SSE\")\n    plt.legend()\n\nfig.suptitle(\"Sum squared error\", fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KMeans","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Vizualize KMeans, number of clusters $\\overline{2, 10}$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph_clusters(method_name, array2d, name_scaler, nmin=2, nmax=9, style=plt.cm.plasma):\n    \n    count_axis_x = 3\n    count_axis_y = (nmax - nmin + 1) // 3 + 1\n    \n    f = plt.figure(figsize=(count_axis_x  * 6, count_axis_y * 5))\n\n    for i in range(nmin, nmax + 1):\n        model = method_name(n_clusters=i).fit(array2d)\n        f.add_subplot(count_axis_y, count_axis_x, i - 1)\n        plt.scatter(array2d[:, 0], array2d[:, 1], s=10, cmap=style, c=model.labels_, label=\"number of\\nclusters = \" + str(i))\n        plt.legend()\n    \n    plt.suptitle(name_scaler)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\nfor name_scaler in options:\n    graph_clusters(KMeans, pca.fit_transform(name_scaler().fit_transform(df.values)), name_scaler.__name__, 2, 10, plt.cm.viridis)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vizualize, numbers of clusters = 4 with MaxAbsScaler (min Error)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_values = pca.fit_transform(MaxAbsScaler().fit_transform(df.values))\nkmeans = KMeans(n_clusters=4, max_iter=1000).fit(pca_values)\n\nplt.subplots(figsize=(10, 8))\nsns.scatterplot(x=\"Pca1\", y=\"Pca2\", hue=\"Cluster\", \n                     data=pd.DataFrame({'Pca1': pca_values[:, 0],\n                                        'Pca2': pca_values[:, 1],\n                                        'Cluster': kmeans.labels_}), palette=plt.cm.tab20, s=100, \n                     alpha=1, edgecolor='k', linewidth=1.2)\n\ncenters = kmeans.cluster_centers_\n\nr = [cdist(pca_values[kmeans.labels_ == i], [center]).max() for i, center in enumerate(kmeans.cluster_centers_)]\n\nplt.scatter(centers[:, 0], centers[:, 1], facecolor='green', marker='H', s=120, edgecolor='k')\nfor c, rad in zip(centers, r):\n    plt.gcf().gca().add_artist(plt.Circle(c, rad, facecolor='darkgreen', lw=3, alpha=0.25, zorder=10))\n\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add cluster number and grouping by features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df, pd.DataFrame(kmeans.labels_, columns=['Cluster'], index=df.index)], axis=1)\ndata = data[['Cluster'] + [col for col in data.columns if col != 'Cluster']]\n\nfor c in data.columns[1:]:\n    grid = sns.FacetGrid(data, col='Cluster', height=3, aspect=1.3)\n    grid.map(plt.hist, c, bins=20, edgecolor='k')\n    grid.set_xticklabels(rotation=40)\n    \npd.DataFrame(data['Cluster'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unbalanced features $\\uparrow$. Sadly","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## AgglomerativeClustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Vizualize AgglomerativeClustering, number of clusters $\\overline{2, 10}$ ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for name_scaler in options:\n    graph_clusters(AgglomerativeClustering, pca.fit_transform(name_scaler().fit_transform(df.values)), name_scaler.__name__, 2, 10, plt.cm.viridis)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vizualize, numbers of clusters = 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ag = AgglomerativeClustering(n_clusters=4, \n                             affinity='euclidean', \n                             linkage='ward').fit(pca_values)\n\nplt.subplots(figsize=(10, 8))\nsns.scatterplot(x=\"Pca1\", y=\"Pca2\", hue=\"Cluster\", \n                     data=pd.DataFrame({'Pca1': pca_values[:, 0],\n                                        'Pca2': pca_values[:, 1],\n                                        'Cluster': ag.labels_}), palette=plt.cm.tab20, s=100, \n                     alpha=1, edgecolor='k', linewidth=1.2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add cluster number and grouping by features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df, pd.DataFrame(ag.labels_, columns=['Cluster_ag'], index=df.index)], axis=1)\ndata = data[['Cluster_ag'] + [col for col in data.columns if col != 'Cluster_ag']]\n\nfor c in data.columns[1:]:\n    grid = sns.FacetGrid(data, col='Cluster_ag', height=3, aspect=1.3)\n    grid.map(plt.hist, c, bins=20, edgecolor='k')\n    grid.set_xticklabels(rotation=40)\n\npd.DataFrame(data['Cluster_ag'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result ~ was repeated","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## GaussianMixture","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Vizualize GaussianMixture, number of clusters $\\overline{2, 10}$ ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for name_scaler in options:\n    f = plt.figure(figsize=(18, 15))\n    \n    for i in range(2, 11):\n        model = GaussianMixture(n_components=i).fit(pca.fit_transform(name_scaler().fit_transform(df.values)))\n        f.add_subplot(3, 3, i - 1)\n        plt.scatter(pca_values[:, 0], pca_values[:, 1], s=10, cmap=plt.cm.magma_r, c=model.predict(pca_values), \n                    label=\"number of\\nclusters = \" + str(i))\n        plt.legend()\n        plt.suptitle(name_scaler.__name__)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Draw ellipse from: https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_ellipse(position, covariance, ax=None, **kwargs):\n    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n    ax = ax or plt.gca()\n    \n    # Convert covariance to principal axes\n    if covariance.shape == (2, 2):\n        U, s, Vt = np.linalg.svd(covariance)\n        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n        width, height = 2 * np.sqrt(s)\n    else:\n        angle = 0\n        width, height = 2 * np.sqrt(covariance)\n    \n    # Draw the Ellipse\n    for nsig in range(1, 4):\n        v = np.random.randint(255, size=3)\n        rgb = plt.cm.viridis.colors\n        \n        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n                             angle, facecolor=rgb[v[nsig - 1]], edgecolor='k', **kwargs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gmm = GaussianMixture(n_components=4, init_params='kmeans', covariance_type='full').fit(pca_values)\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12, 18))\n\nsns.scatterplot(x=\"Pca1\", y=\"Pca2\", hue=\"Cluster\", \n                     data=pd.DataFrame({'Pca1': pca_values[:, 0],\n                                        'Pca2': pca_values[:, 1],\n                                        'Cluster': gmm.predict(pca_values)}), palette=plt.cm.Spectral, s=100, \n                     alpha=1, edgecolor='k', linewidth=1.2, ax=ax1)\n\n\nax2.set_yticks(ax1.get_yticks())\nax2.set_ylabel(ax1.get_ylabel())\n\nfor pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n    draw_ellipse(pos, covar, alpha=0.2, ax=ax2)\n\n# ax1.axis('equal')\n# ax2.axis('equal')\nf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add cluster number and grouping by features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df, pd.DataFrame(ag.labels_, columns=['Cluster_gmm'], index=df.index)], axis=1)\ndata = data[['Cluster_gmm'] + [col for col in data.columns if col != 'Cluster_gmm']]\n\nfor c in data.columns[1:]:\n    grid = sns.FacetGrid(data, col='Cluster_gmm', height=3, aspect=1.3)\n    grid.map(plt.hist, c, bins=20, edgecolor='k')\n    grid.set_xticklabels(rotation=40)\n\npd.DataFrame(data['Cluster_gmm'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a result, we can say that KMeans and GMM give the best results. Despite the outliers, RobustScaler is not the best option. Try replacing the PCA.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}