{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:31.303079Z","iopub.execute_input":"2021-06-28T17:56:31.303543Z","iopub.status.idle":"2021-06-28T17:56:31.311303Z","shell.execute_reply.started":"2021-06-28T17:56:31.303505Z","shell.execute_reply":"2021-06-28T17:56:31.309752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Wrangling","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/housesalesprediction/kc_house_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:31.314751Z","iopub.execute_input":"2021-06-28T17:56:31.316734Z","iopub.status.idle":"2021-06-28T17:56:31.427062Z","shell.execute_reply.started":"2021-06-28T17:56:31.316672Z","shell.execute_reply":"2021-06-28T17:56:31.42577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:31.428878Z","iopub.execute_input":"2021-06-28T17:56:31.429216Z","iopub.status.idle":"2021-06-28T17:56:31.531773Z","shell.execute_reply.started":"2021-06-28T17:56:31.429185Z","shell.execute_reply":"2021-06-28T17:56:31.530468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, Null Values need to be checked as it is \nvery important to remove Null values for \nMultiple Regression. However, we did not find \nany Null values in this dataset. Then 2 columns: \n‘id’ and ‘date’ are removed from DataFrame as \nthese contain useless information.","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:31.534179Z","iopub.execute_input":"2021-06-28T17:56:31.534554Z","iopub.status.idle":"2021-06-28T17:56:31.548096Z","shell.execute_reply.started":"2021-06-28T17:56:31.534519Z","shell.execute_reply":"2021-06-28T17:56:31.546727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:31.550374Z","iopub.execute_input":"2021-06-28T17:56:31.55105Z","iopub.status.idle":"2021-06-28T17:56:31.583079Z","shell.execute_reply.started":"2021-06-28T17:56:31.550974Z","shell.execute_reply":"2021-06-28T17:56:31.580589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df.drop(['id','date'], axis=1)\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:31.585131Z","iopub.execute_input":"2021-06-28T17:56:31.585642Z","iopub.status.idle":"2021-06-28T17:56:31.610681Z","shell.execute_reply.started":"2021-06-28T17:56:31.585603Z","shell.execute_reply":"2021-06-28T17:56:31.609123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation:\nWe can demonstrate that all variables \nare in good correlation with ‘price’. Only \n‘zipcode’ has a negative correlation of -0.05 but \nare very near to 0 with the target variable. \n‘sqft_living’, ‘grades’ and ‘bathrooms’ are \nhaving a positive strong correlation with the \ntarget variable ‘price’.\n","metadata":{}},{"cell_type":"code","source":"corr = df1.corr()\nplt.figure(figsize=(25,15))\nsns.heatmap(corr, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:31.61233Z","iopub.execute_input":"2021-06-28T17:56:31.612788Z","iopub.status.idle":"2021-06-28T17:56:33.573776Z","shell.execute_reply.started":"2021-06-28T17:56:31.612744Z","shell.execute_reply":"2021-06-28T17:56:33.572705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting data into train and test\nWe used train_test_split from sklearn library to \nsplit our data into 75% and 25% for train and \ntest sets respectively. We created x_train, \nx_test, y_train and y_test. The Random state for \ntrain and test is 3.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:33.575247Z","iopub.execute_input":"2021-06-28T17:56:33.57559Z","iopub.status.idle":"2021-06-28T17:56:33.82194Z","shell.execute_reply.started":"2021-06-28T17:56:33.575559Z","shell.execute_reply":"2021-06-28T17:56:33.820863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df1.drop(['price'], axis=1)\ny = df1['price']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=3)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:33.823096Z","iopub.execute_input":"2021-06-28T17:56:33.823362Z","iopub.status.idle":"2021-06-28T17:56:33.845038Z","shell.execute_reply.started":"2021-06-28T17:56:33.823336Z","shell.execute_reply":"2021-06-28T17:56:33.843671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization:","metadata":{}},{"cell_type":"markdown","source":"**1st Plot:** Shows the bedrooms count, and it can be observed that most of the properties are having 3 bedrooms and 4 bedrooms.","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(7, 5))\nsns.countplot(df1[\"bedrooms\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:33.846602Z","iopub.execute_input":"2021-06-28T17:56:33.84703Z","iopub.status.idle":"2021-06-28T17:56:34.052159Z","shell.execute_reply.started":"2021-06-28T17:56:33.846987Z","shell.execute_reply":"2021-06-28T17:56:34.051036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2nd Plot:**  Shows the bathroom count, \nand it can be observed that most of the houses \nare having 2.5, 1, and 1.75 bathrooms.","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(15, 5))\nsns.countplot(df1[\"bathrooms\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:34.055254Z","iopub.execute_input":"2021-06-28T17:56:34.055605Z","iopub.status.idle":"2021-06-28T17:56:34.479004Z","shell.execute_reply.started":"2021-06-28T17:56:34.055575Z","shell.execute_reply":"2021-06-28T17:56:34.478049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3rd Plot:**  Shows property with waterfront and we can \nobserve that the maximum of the houses is not \nhaving a waterfront and only a few have a \nwaterfront feature. ","metadata":{}},{"cell_type":"code","source":"sns.countplot(df1[\"waterfront\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:34.481043Z","iopub.execute_input":"2021-06-28T17:56:34.481357Z","iopub.status.idle":"2021-06-28T17:56:34.590203Z","shell.execute_reply.started":"2021-06-28T17:56:34.481329Z","shell.execute_reply":"2021-06-28T17:56:34.58939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4th Plot:**  Shows how many \nfloors maximum properties have, and we can \nobserve that most of the properties are having 1 \nand 2 floors.\n","metadata":{}},{"cell_type":"code","source":"sns.countplot(df1[\"floors\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:34.59109Z","iopub.execute_input":"2021-06-28T17:56:34.591332Z","iopub.status.idle":"2021-06-28T17:56:34.728753Z","shell.execute_reply.started":"2021-06-28T17:56:34.591307Z","shell.execute_reply":"2021-06-28T17:56:34.72766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Machine Learning models:**\n\n4 Machine Learning models are used:","metadata":{}},{"cell_type":"markdown","source":"## 1. Multiple Linear Regression:","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:34.729998Z","iopub.execute_input":"2021-06-28T17:56:34.730267Z","iopub.status.idle":"2021-06-28T17:56:34.836395Z","shell.execute_reply.started":"2021-06-28T17:56:34.730241Z","shell.execute_reply":"2021-06-28T17:56:34.835496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(x_train,y_train)            # Fitting model with x_train and y_train\nlm_pred = lm.predict(x_test)       # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, lm_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, lm_pred))\nprint(\"Accuracy :\",lm.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:34.837857Z","iopub.execute_input":"2021-06-28T17:56:34.838234Z","iopub.status.idle":"2021-06-28T17:56:34.897711Z","shell.execute_reply.started":"2021-06-28T17:56:34.838193Z","shell.execute_reply":"2021-06-28T17:56:34.896676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = {'True Labels': y_test, 'Predicted Labels': lm_pred}\ndf_lm = pd.DataFrame(data = labels)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_lm)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:34.89936Z","iopub.execute_input":"2021-06-28T17:56:34.89978Z","iopub.status.idle":"2021-06-28T17:56:35.633111Z","shell.execute_reply.started":"2021-06-28T17:56:34.89974Z","shell.execute_reply":"2021-06-28T17:56:35.632052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have used first Multiple Linear Regression for \nthis dataset. This model provided an average \nresult. Below are the results:\n* RMSE: 444.30\n* R2 Score: 0.71\n* Accuracy: 70.78 % \n\nShows the lmplot for this multiple \nlinear regression model and it plots a straight \nline, but this is not much close to 45 degrees.\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Decision Tree:","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:35.634269Z","iopub.execute_input":"2021-06-28T17:56:35.634569Z","iopub.status.idle":"2021-06-28T17:56:35.743959Z","shell.execute_reply.started":"2021-06-28T17:56:35.634541Z","shell.execute_reply":"2021-06-28T17:56:35.7428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unpruned Tree","metadata":{}},{"cell_type":"code","source":"dtree_up = DecisionTreeRegressor()\ndtree_up.fit(x_train, y_train)               # Fitting model with x_train and y_train\ndtree_pred_up = dtree_up.predict(x_test)     # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, dtree_pred_up, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, dtree_pred_up))\nprint(\"Accuracy :\",dtree_up.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:35.745276Z","iopub.execute_input":"2021-06-28T17:56:35.745616Z","iopub.status.idle":"2021-06-28T17:56:35.995448Z","shell.execute_reply.started":"2021-06-28T17:56:35.745581Z","shell.execute_reply":"2021-06-28T17:56:35.994483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HyperParameter Tuned Decision Tree Regressor:","metadata":{}},{"cell_type":"code","source":"d = np.arange(1, 21, 1)\n\ndtree = DecisionTreeRegressor(random_state=5)\nhyperParam = [{'max_depth':d}]\n\ngsv = GridSearchCV(dtree,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train, y_train)                          # Fitting model with xtrain_scaler and y_train\ndtree_pred_mms = best_model.best_estimator_.predict(x_test)     # Predicting the results\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\n\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, dtree_pred_mms, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, dtree_pred_mms))\nprint(\"Accuracy :\",best_model.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:35.996564Z","iopub.execute_input":"2021-06-28T17:56:35.996841Z","iopub.status.idle":"2021-06-28T17:56:46.980481Z","shell.execute_reply.started":"2021-06-28T17:56:35.996816Z","shell.execute_reply":"2021-06-28T17:56:46.979394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = {'True Labels': y_test, 'Predicted Labels': dtree_pred_mms}\ndf_lm = pd.DataFrame(data = labels)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_lm)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:46.982067Z","iopub.execute_input":"2021-06-28T17:56:46.982474Z","iopub.status.idle":"2021-06-28T17:56:47.641981Z","shell.execute_reply.started":"2021-06-28T17:56:46.982416Z","shell.execute_reply":"2021-06-28T17:56:47.640975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we used Decision Tree for our model. For \nthis, we used 2 variants of model unpruned \nsimple decision tree model and tuned regressor \nwith multiple max_depth. Results are:\n1. Decision Tree (Unpruned):\n* RMSE: 422.72\n* R2 Score: 0.76\n* Accuracy: 76.05 % \n2. Decision Tree (Pruned): which was pruned using max_depth for 1 to 20 range. This model is hyperparameter tuned using sklearn’s GridSearchCV.\n* Max_depth: 11\n* RMSE: 406.80\n* R2 Score: 0.79\n* Accuracy: 79.46 %\n\nShows the lmplot which is a straight \nline and closer to 45 degrees. This plot turns out \nto be much better than the Multiple Linear \nRegression model.\n","metadata":{}},{"cell_type":"markdown","source":"## 3. Random Forest: ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:47.643104Z","iopub.execute_input":"2021-06-28T17:56:47.643362Z","iopub.status.idle":"2021-06-28T17:56:47.672824Z","shell.execute_reply.started":"2021-06-28T17:56:47.643338Z","shell.execute_reply":"2021-06-28T17:56:47.671867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple Random Forest","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor()\nrf.fit(x_train, y_train)             # Fitting model with x_train and y_train\nrf_pred = rf.predict(x_test)         # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, rf_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, rf_pred))\nprint(\"Accuracy :\",rf.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:56:47.674035Z","iopub.execute_input":"2021-06-28T17:56:47.674346Z","iopub.status.idle":"2021-06-28T17:57:02.765301Z","shell.execute_reply.started":"2021-06-28T17:56:47.674308Z","shell.execute_reply":"2021-06-28T17:57:02.764257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HyperParameter Tuned Random Forest Regressor:","metadata":{}},{"cell_type":"code","source":"nEstimator = [140,160,180,200,220]\ndepth = [10,15,20,25,30]\n\nRF = RandomForestRegressor()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\n\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\ngsv.fit(x_train, y_train)\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nscores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\nmaxDepth=gsv.best_params_['max_depth']\nnEstimators=gsv.best_params_['n_estimators']\n\nmodel = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\nmodel.fit(x_train, y_train)        # Fitting model with x_train and y_train\n\n# Predicting the results:\nrf_pred_tune = model.predict(x_test)\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, rf_pred_tune, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, rf_pred_tune))\nprint(\"Accuracy :\",model.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T17:57:02.766595Z","iopub.execute_input":"2021-06-28T17:57:02.766877Z","iopub.status.idle":"2021-06-28T18:11:51.988764Z","shell.execute_reply.started":"2021-06-28T17:57:02.766849Z","shell.execute_reply":"2021-06-28T18:11:51.987639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = {'True Labels': y_test, 'Predicted Labels': rf_pred_tune}\ndf_lm = pd.DataFrame(data = labels)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_lm)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:11:51.990267Z","iopub.execute_input":"2021-06-28T18:11:51.990587Z","iopub.status.idle":"2021-06-28T18:11:52.6646Z","shell.execute_reply.started":"2021-06-28T18:11:51.990557Z","shell.execute_reply":"2021-06-28T18:11:52.663693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have used Random Forest for this dataset. \nWe have used 2 variants of Random Forest; 1st \nis normal Random Forest and 2nd is \nHyperparameter tuned, Random Forest. We are \nusing GridSearchCV from sklearn. For the 2nd \nmodel, we have used parameters like \n‘n_estimators’ and ‘max_depth’. We will \niterate through all parameters and find the best \none. Results are:\n1. Random Forest (Simple):\n* RMSE: 351.26\n* R2 Score: 0.89\n* Accuracy: 88.58 % \n2. Random Forest (Tuned): n_estimators = [140,160,180,200,220] and max_depth = [10,15,20,25,30]\n* Best n_estimators: 180\n* Best max_depth: 30\n* RMSE: 351.30\n* R2 Score: 0.89\n* Accuracy: 88.58 %\n\nShows the lmplot and it can be \nobserved that this time we got a straight line \nwhich is close to 45 degrees. Random Forest \nwith tuned parameters looks very efficient for \nthis dataset.","metadata":{}},{"cell_type":"markdown","source":"## 4. StatsModel OLS:","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:11:52.665859Z","iopub.execute_input":"2021-06-28T18:11:52.666158Z","iopub.status.idle":"2021-06-28T18:11:53.435712Z","shell.execute_reply.started":"2021-06-28T18:11:52.666129Z","shell.execute_reply":"2021-06-28T18:11:53.434844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = sm.add_constant(x)\n# Results will contain output of Ordinary Least Squares(OLS). Fit will apply a technique to obtain the fit of the model.\nresults = sm.OLS(y,x1).fit() \nresults.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:11:53.436745Z","iopub.execute_input":"2021-06-28T18:11:53.437158Z","iopub.status.idle":"2021-06-28T18:11:53.538816Z","shell.execute_reply.started":"2021-06-28T18:11:53.43713Z","shell.execute_reply":"2021-06-28T18:11:53.537562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('R2: ', results.rsquared)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:11:53.543878Z","iopub.execute_input":"2021-06-28T18:11:53.544336Z","iopub.status.idle":"2021-06-28T18:11:53.556914Z","shell.execute_reply.started":"2021-06-28T18:11:53.544292Z","shell.execute_reply":"2021-06-28T18:11:53.555414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing floors from the Independent Variables because P > 0.05\nx2 = x.drop(['floors'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:11:53.560693Z","iopub.execute_input":"2021-06-28T18:11:53.561413Z","iopub.status.idle":"2021-06-28T18:11:53.568887Z","shell.execute_reply.started":"2021-06-28T18:11:53.561368Z","shell.execute_reply":"2021-06-28T18:11:53.567822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x3 = sm.add_constant(x2)\n# Results will contain output of Ordinary Least Squares(OLS). Fit will apply a technique to obtain the fit of the model.\nresults1 = sm.OLS(y,x2).fit() \nresults1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:11:53.570468Z","iopub.execute_input":"2021-06-28T18:11:53.57112Z","iopub.status.idle":"2021-06-28T18:11:53.676345Z","shell.execute_reply.started":"2021-06-28T18:11:53.571066Z","shell.execute_reply":"2021-06-28T18:11:53.675274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('R2: ', results1.rsquared)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T18:11:53.677918Z","iopub.execute_input":"2021-06-28T18:11:53.678576Z","iopub.status.idle":"2021-06-28T18:11:53.684779Z","shell.execute_reply.started":"2021-06-28T18:11:53.678533Z","shell.execute_reply":"2021-06-28T18:11:53.683784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"StatsModel is the last model we are using to get \nthe best ‘price’ prediction. First, we are using a \nbasic model and from (Fig. 23) we can observe \nthe P values of all Independent Variables. It is \nobserved that only the floor is having P > 0.05, \ni.e, 0.063. So, for the next model we will \nremove the ‘floor’ variable and run this model \nagain to get very good results.\n1. StatsModel OLS:\n* Accuracy = 70 %\n2. StatsModel OLS after removing ‘floors’ \n(P>0.05):\n* Accuracy = 90.50 %\n\nClearly shows that after removing the \n‘floor’ variable we are getting 90.50 % \naccuracy which is the highest among all other \nmodels. Also, the F-Statistics value is very \nsmall and close to 0.","metadata":{}},{"cell_type":"markdown","source":"# **Conclusion:**\nThis dataset is House Sales in King \nCounty, USA, where we predicted ‘price’. This \ndataset had few variables which were removed \nduring data cleaning and the correlation of all \nvariables were good with target variables. We \nhave used 4 machine learning models for this \ndataset, Multiple Linear Regression produced \nan average result and accuracy of 70.78 %, \nhowever, hyperparameter tuned Decision Tree \nalso provided accuracy of around 79.46 %. \nRandom Forest worked well and for both \nsimple and hyperparameter tuned Random \nForest Model, accuracy came out to be 88.58 %. \nHowever, after using StatsModel OLS, we \nfound that the ‘floors’ variable has P values > \n0.05, so we removed that variable and received \na very good model with 90.50 % accuracy. \nStatsModel after removing the ‘floors’ variable \nturns out to be the best model for our dataset.","metadata":{}}]}