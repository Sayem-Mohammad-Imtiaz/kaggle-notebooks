{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_data = pd.read_csv('../input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv', delimiter=',', skipinitialspace=True)\nimdb_data = pd.read_csv('../input/imdbmovies/imdb.csv', error_bad_lines=False, warn_bad_lines=False, skipinitialspace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take only the interesting columns\n#imdb_data = imdb_data[['title', 'imdbRating', 'nrOfWins', 'nrOfNominations', 'nrOfPhotos', 'nrOfNewsArticles', 'nrOfUserReviews']]\nimdb_data = imdb_data.drop(['fn', 'tid', 'wordsInTitle', 'url', 'year'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_data.loc[:,'imdb_title'] = wiki_data['Title'].str.strip() + \" (\" + wiki_data['Release Year'].astype('str') + \")\"\nall_data = wiki_data.merge(imdb_data, left_on=['imdb_title'], right_on=['title']).drop(['title', 'imdb_title'], axis=1)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['Release Year'].hist()\nplt.title(\"Distribution of Release Year\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['imdbRating'].hist()\nplt.title(\"Distribution of imdbRating\")\nplt.xlabel(\"Rating (0-10)\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['ratingCount'].hist()\nplt.title(\"Distribution of ratingCount\")\nplt.xlabel(\"ratingCount\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_ratings_quarter = all_data.ratingCount.quantile(0.25)\nall_data = all_data.loc[(all_data['Release Year'] > 2000) & (all_data['ratingCount'] > most_ratings_quarter)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with NaN values (we have plenty of data anyways)\nall_data = all_data.dropna(axis=0)\n# 0 = \"haven't seen\", 1 = \"liked\", -1 = \"not liked\", NaN=\"not answered\"\nall_data.loc[:, \"Class\"] = np.NaN\nall_data = all_data.drop_duplicates('Title')\nall_data = all_data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scrape Wikipedia image URLs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport requests\ndef get_img_url(wiki_url):\n    page = str(requests.get(wiki_url).text)\n    start = page.find(\"//upload\")\n    \n    end_html = page[start:].find(\"/>\") - 1\n    end_png = page[start:].find(\".png\") + 4\n    end_jpg = page[start:].find(\".jpg\") + 4\n    end =  min([n for n in [end_html, end_png, end_jpg]  if n > 0])\n    \n    if start > 0 and end > 0 and len(page[start:start+end]) > 5:\n        return page[start:start+end]\n    return None\n\nimg_urls = []\nfor index, movie in all_data.iterrows():\n    img_urls.append(get_img_url(movie['Wiki Page']))\n    print(\"index:\", index, \"Done with\", 100 * round(index / all_data.shape[0], 3), \"%\")\n    time.sleep(2)\n    clear_output()\n\nall_data.loc[:, \"Wiki_img\"] = np.array(img_urls)\nall_data.to_csv('all_data_no_class.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load exsisting data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data.to_csv('all_data_no_class.csv')\nall_data = pd.read_csv('../input/moviedataset/all_data_no_class.csv', index_col=0)\nall_data = all_data.drop_duplicates('Title')\nall_data = all_data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Collection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import clear_output\nfrom IPython.display import IFrame, HTML\nimport random\n## Data collection\n\nuser_input = None\nwhile user_input != \"e\":\n    try:\n        # If enough data: use cosine similarity to show similar movies that you've liked. However, 1/10 probability go with random anyways.\n        if all_data.loc[all_data['Class'] == 1].shape[0] > 5 and random.random() > 0.1:\n            movie_title = get_similar_to_seen_movies(all_data, 1)[0]\n            random_movie = all_data.loc[all_data['Title'] == movie_title]\n        else:\n            random_movie = all_data.loc[all_data['Class'].isna()].sample()\n            movie_title = random_movie.Title.values[0]\n    except Exception as e:\n        print(e)\n        print(\"All movies sampled, maybe?\")\n        break\n    \n    #\n    wiki_url = random_movie['Wiki_img']\n    if wiki_url.isna().values[0]:\n        print(\"No image available\")\n    else:\n        display(IFrame(src=wiki_url.values[0],  width=900, height=400))\n\n    print(\"Instructions: y='yes', n='no', enter='havent seen', e=exit\")\n    print(\"Did you like\", movie_title, \"(\" + str(random_movie['Release Year'].values[0]) + \")?\")\n    user_input = str(input())\n    while user_input != \"y\" and user_input != \"n\" and user_input != \"e\" and user_input != \"\":\n        print(\"Invalid input! Try again.\")\n        user_input = str(input())\n    \n    if user_input == \"y\":\n        all_data.loc[all_data['Title'] == movie_title, 'Class'] = 1\n    elif user_input == \"n\":\n        all_data.loc[all_data['Title'] == movie_title, 'Class'] = -1\n    elif user_input == \"\":\n        all_data.loc[all_data['Title'] == movie_title, 'Class'] = 0\n    clear_output()\nall_data.to_csv('all_data_with_class.csv')\nprint(\"Exited. Now\", all_data['Class'].notna().sum(),\"/\", all_data.shape[0], \"movies are labeled.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NLP-classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import KFold\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\n\nml_data = all_data.loc[(all_data['Class'].notna()) & all_data['Class'] != 0]\nX = ml_data.Plot.values\ny = ml_data.Class.values\n\ntext_clf = Pipeline([\n     ('vect', CountVectorizer(tokenizer=nltk.word_tokenize, max_features=3000)),\n     ('tfidf', TfidfTransformer()),\n     ('sampler', RandomUnderSampler()),\n     ('clf', MultinomialNB()),\n])\n#LinearSVC()\nprint(\"K-Fold testing:\")\naccuracies = []\nfor train_index, test_index in KFold(n_splits=5, shuffle=True).split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    text_clf.fit(X_train, y_train)\n    accuracies.append(text_clf.score(X_test, y_test))\n    print(f'Model test accuracy: {accuracies[-1]:.2f}')\n    #print(confusion_matrix(y_test, text_clf.predict(X_test)))\n\nprint(\"Mean accuracy:\", round(sum(accuracies)/len(accuracies), 2))\n\nprint(\"\\nNormal testing:\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, shuffle=True)\nprint(\"Train data size:\", X_train.shape[0])\nprint(\"Test data size:\", X_test.shape[0])\ntext_clf.fit(X_train, y_train)\nprint('Model test accuracy:', text_clf.score(X_test, y_test))\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, text_clf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make recommendation of unseen movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_plot_recommendations(data, clf_pipeline):\n    ml_data = data.loc[(data['Class'].notna()) & data['Class'] != 0]\n    X_train = ml_data.Plot.values\n    y_train = ml_data.Class.values\n    clf_pipeline.fit(X_train, y_train)\n\n    X_test = data.loc[(data['Class'].notna()) & data['Class'] == 0].Plot.values\n\n    probs = clf_pipeline.predict_proba(X_test)\n    y_pred = probs.argmax(axis=1)\n    y_probs = probs.max(axis=1)\n    \n    top5_idxs = (-y_probs).argsort()[:5]\n    bot5_idxs = (y_probs).argsort()[:5]\n    top5_recommend = data.loc[(data['Class'].notna()) & data['Class'] == 0].iloc[top5_idxs]\n    bot5_recommend = data.loc[(data['Class'].notna()) & data['Class'] == 0].iloc[bot5_idxs]\n    return top5_recommend, bot5_recommend\n\ndef show_recommendations(top5_recommend, bot5_recommend):\n    html_code = \"<h3>Recommended:</h3><table><tr>\"\n    for idx, movie in top5_recommend.iterrows():\n        html_code += '<td><img title=' + str(movie.Title) + ' style=\"max-width:150px;height:auto;\" src=' + str(movie.Wiki_img) + '></td>'\n    html_code += \"</tr></table>\"\n    display(HTML(html_code))\n    \n    html_code = \"<h3>Disadvised:</h3><table><tr>\"\n    for idx, movie in bot5_recommend.iterrows():\n        if str(movie.Wiki_img) == \"nan\":\n            html_code += '<td><p><b>' + str(movie.Title) + '</b><br>Not image available</p></td>'\n        else:\n            html_code += '<td><img title=' + str(movie.Title) + ' style=\"max-width:150px;height:auto;\" src=' + str(movie.Wiki_img) + '></td>'\n    html_code += \"</tr></table>\"\n    display(HTML(html_code))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top5, bot5 = get_plot_recommendations(all_data, text_clf)\nshow_recommendations(top5, bot5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ordinary Machine Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nml_data = all_data.loc[(all_data['Class'].notna()) & all_data['Class'] != 0].drop(['Title', 'type', 'Origin/Ethnicity', 'Director', 'Cast', 'Genre', 'Wiki Page', 'Plot',\n                                                                                  'Wiki_img'], axis=1)\nX = ml_data.drop('Class', axis=1).values\ny = ml_data.Class.values\n\nmodel = Pipeline([\n     ('scaler', StandardScaler()),\n     ('sampler', RandomUnderSampler()),\n     ('clf', RandomForestClassifier(max_depth=10, random_state=0)),\n])\n#RandomForestClassifier(max_depth=10, random_state=0)\n#LinearSVC()\nprint(\"K-Fold testing:\")\naccuracies = []\nfor train_index, test_index in KFold(n_splits=5, shuffle=True).split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model.fit(X_train, y_train)\n    accuracies.append(model.score(X_test, y_test))\n    print(f'Model test accuracy: {accuracies[-1]:.2f}')\n    #print(confusion_matrix(y_test, model.predict(X_test)))\n\nprint(\"Mean accuracy:\", round(sum(accuracies)/len(accuracies), 2))\n\nprint(\"\\nNormal testing:\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, shuffle=True)\nprint(\"Train data size:\", X_train.shape[0])\nprint(\"Test data size:\", X_test.shape[0])\nmodel.fit(X_train, y_train)\nprint('Model test accuracy:', model.score(X_test, y_test))\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ml_recommendations(data, clf_pipeline):\n    ml_data = data.loc[(data['Class'].notna()) & data['Class'] != 0].drop(['Title', 'type', 'Origin/Ethnicity', 'Director', 'Cast', 'Genre', 'Wiki Page', 'Plot',\n                                                                                  'Wiki_img'], axis=1)\n    X_train = ml_data.drop('Class', axis=1).values\n    y_train = ml_data.Class.values\n    clf_pipeline.fit(X_train, y_train)\n\n    X_test = data.loc[(data['Class'].notna()) & data['Class'] == 0].drop(['Title', 'type', 'Origin/Ethnicity', 'Director', 'Cast', 'Genre', 'Wiki Page', 'Plot',\n                                                                                   'Wiki_img', 'Class'], axis=1).values\n\n    probs = clf_pipeline.predict_proba(X_test)\n    y_pred = probs.argmax(axis=1)\n    y_probs = probs.max(axis=1)\n    \n    top5_idxs = (-y_probs).argsort()[:5]\n    bot5_idxs = (y_probs).argsort()[:5]\n    top5_recommend = data.loc[(data['Class'].notna()) & data['Class'] == 0].iloc[top5_idxs]\n    bot5_recommend = data.loc[(data['Class'].notna()) & data['Class'] == 0].iloc[bot5_idxs]\n    return top5_recommend, bot5_recommend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top5, bot5 = get_ml_recommendations(all_data, model)\nshow_recommendations(top5, bot5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cosine similarity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n# similar to not shown/asked movies\ndef get_similar_to_seen_movies(data, n=5):\n    X_all = data.loc[(data['Class'].notna())]\n    X_train = data.loc[(data['Class'].notna()) & data['Class'] != 0].Plot.values\n    X_test = data.loc[data['Class'].isna()].Plot.values\n    movie_titles = data.loc[data['Class'].isna()].Title.values\n    \n    tf_idf = TfidfVectorizer().fit(X_all)\n\n\n    cosine_similarities = linear_kernel(tf_idf.transform(X_train), tf_idf.transform(X_test)).max(axis=0)\n    idxs = cosine_similarities.argsort()[-n:]\n    return movie_titles[idxs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_to_seen_movies(all_data, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nliked_data = all_data.loc[all_data['Class'] == 1] # movies that you liked\ndisliked_data = all_data.loc[all_data['Class'] == 0]\nX_train, X_test1 = train_test_split(liked_data.Plot.values, test_size=0.2)\nX_test2 = disliked_data.Plot.values\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nembedder = TfidfVectorizer().fit(X_train)\nX_train = embedder.transform(X_train)\n\nX_test1 = embedder.transform(X_test1)\nX_test2 = embedder.transform(X_test2)\n\n\nliked_cosine_similarities = linear_kernel(X_train, X_test1)\ndisliked_cosine_similarities = linear_kernel(X_train, X_test2)\nprint(liked_cosine_similarities)\nprint(liked_cosine_similarities.mean())\n\nprint(disliked_cosine_similarities)\nprint(disliked_cosine_similarities.mean())\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}