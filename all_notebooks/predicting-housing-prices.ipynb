{"cells":[{"metadata":{},"cell_type":"raw","source":"Predicting the prices of commodities is one of the practical applications of machine learning APIs. Consider the housing market. Neither party in the purchase/sale of a home wants to be fleeced, so getting an accurate ballpark idea of a home's worth is important. The California Housing data in this notebook is a good place to see the practicality of machine learning methods for this purpose since it contains block by block median home prices for homes in California.\n\nCrucially, the data is a great opportunity to show how to clean and prepare data. As the saying goes: garbage in, garbage out. We'll do this by checking for skewness, missing data, and correcting for context in the data.\n\nFirst, we will import the packages we'll need for the analysis:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Basics for loading and handling the data \nimport numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))\n\n# To plot figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\n# For modelling the data\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To load the data, let's take a look inside the 'housing' folder:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/housing\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll be using 'housing.csv', so let's set that as our data path and take a look at the a brief description of the data set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"../input/housing/housing.csv\"\nhousing = pd.read_csv(data_path)\nhousing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the information displayed above, we can see that there are 20640 rows and 10 columns of data for each block of homes: longitude, latitude, median age, total rooms, total bedrooms, population, households, median income, median house value, and ocean proximity. \n\nWhat we're aiming to do is explain the median house value through the explanatory variables in the other columns. \n\nFor our first look at the data, it'd be smart to plot some histograms and see how the values range across the column categories:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot histograms of the data\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Right off the bat, we can see two issues which would probably hamper our results: the range of values recorded for the median age of the houses and for the median house value was capped at 50 years and $500,000 respectively. These caps in the data would distort the predictive accuracy of our model by leading it to believe that homes cannot be older than 50 years and that the maximum value for a home cannot be greater than half a million dollars.\n\nThe best way to deal with this type of issue is to eliminate these value counts from the data. Our model will be more accurate without them since there are surely homes older than 50 years and homes worth more than half a million dollars; these data points are simply erroneous and reflect arbitrary limits imposed on the data collection process.\n\nLet's take a look at how many of these counts we'll be eliminating:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(housing[housing['median_house_value']>450000]['median_house_value'].value_counts().head())\nprint(\"\")\nprint(\"\")\n\nprint(housing[housing['housing_median_age']>45]['housing_median_age'].value_counts().head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the basis of median housing value, we'll remove 965 entries.\n\nOn the basis of housing median age, we'll remove 1273 entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removes the skewed data\nhousing=housing.loc[housing['median_house_value']<500001.0,:]\nhousing=housing.loc[housing['housing_median_age']<52,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we check for missing data. Missing data can occur sometimes when an aspect of the observation is unavailable. Maybe there are missing records that make it impossible to determine the age of homes in a neighborhood or maybe we are unable to enter every home in the state to count the number of bedrooms. These are reasonable limitations.\n\nTo see if we have missing data for any variables, let's run a command to identify "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see what's missing \nsample_incomplete_rows = housing[housing.isnull().any(axis=1)]\nmissing=sample_incomplete_rows.head()\nprint(missing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output above, we can see that we do have missing observations. Notably we are missing some entries for total bedrooms. The solution to this dilemma is - rather than throwing out all observation rows with missing data - to simply fill in missing variables with the dataset median for that variable if it is numerical (e.g., total bedrooms) or most frequent if the variable is categorical (e.g., ocean proximity). \n\nWe will take care of this in the pipeline assemble to treat the data, but first let's create context variables for total rooms, total bedrooms, and population.\n\nWhat we are interested in (in explaining value) is total rooms per household, total bedrooms per total rooms, and population per household.\n\nThe reasoning is that, intuitively, these modifications which place the variables within context are better at explaining the value of a home.\n\nThe code for creating these columns in the data is very basic:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to create and run a pipeline for the final treatment of the data. The process will proceed as follows:\n\n1) Median House value will be separated from the explanatory variables.\n\n2) Numerical data will have the imputer fill in median values for missing data.\n\n3) Categorical data will have the imputer fill in most frequent values for missing data.\n\n4) OneHotEncoder will then transform the categorical data into binary values so that the data can be run through a model.\n\n5) The explanatory variables will be placed back together and ready for a regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pipeline to prepare the data\n# num data will have imputer fill in median values for missing data\n# cat data will have imputer fill in most frequesnt values in case we're missing ocean proximity entries\n    #then OneHotEncoder will provide binary values for the cat data\n\n\n#Separating predictors and labels\nhousing_labels = housing[\"median_house_value\"].copy()  \nhousing_x = housing.drop(\"median_house_value\", axis=1)\n\n\n\n\nhousing_cat= housing_x[[\"ocean_proximity\"]]\nhousing_num = housing_x.select_dtypes(include=[np.number])\n\n\n# Create a list of all numeric variable we need\nnum_attribs = list(housing_num)\n# List of categorical variables\ncat_attribs = [\"ocean_proximity\"]\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        #('std_scaler', StandardScaler()),     # We don't need to standardize this data\n    ])\n\ncat_pipeline = Pipeline([\n        ('imputermode', SimpleImputer(strategy=\"most_frequent\")),\n        ('lab_encoder',  OneHotEncoder()),\n    ])\n\n\n\n# Full pipeline\n# Runs a pipeline for categorical and numerical features\nfull_pipeline = ColumnTransformer([\n        (\"num_pipeline\", num_pipeline, num_attribs),\n        (\"cat_pipeline\", cat_pipeline, cat_attribs),\n    ])\n\n\nhousing_prepared = full_pipeline.fit_transform(housing_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data has been prepared and we can now run a regression using RandomForestRegressor from Scikit-learn. This is a great entry-level regression model for the purpose of introducing someone to this type of analysis. We'll also stick to some standard parameters (n_estimators=100, cv=3). "},{"metadata":{"trusted":true},"cell_type":"code","source":"# This box is for the regression\n\n\n# Now we can test a RandomForestRegressor model\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = forest_reg.predict(housing_prepared)\n\n\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=3)\nforest_rmse_scores = np.sqrt(-forest_scores)\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There we have it. Our model can predict the median price of homes on a block with a mean squared error of $65,000. This isn't bad when you consider that an error of 255 results in a squared error of 65,025. It's actually very impressive. We could possibly achieve better and more trustworthy results using a combination of TensorFlow and train/test splitting of the dataset, but this regression as it stands is a great introduction to data preparation and machine learning using Scikit-learn. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}