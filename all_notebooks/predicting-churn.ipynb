{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\n## 1.1 Premise\n\nA manager at a bank is concerned that more and more customers are leaving the bank's credit card services. \n\nThe bank would really appreciate it if someone could help it predict who is going to churn, so that it can proactively approach such customers to offer better services, and turn them back.\n\n## 1.2 Plan\n\n- Perform **exploratory data analysis** to learn the *properties/characteristics* of the features present.\n- Fit several **classification models** to predict whether a customer will churn or not.\n- Apply *hyper-parameter optimization* techniques.\n- **Evaluate performance** of fitted models.","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Modelling\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Load the data\ndata = pd.read_csv(\n    '/kaggle/input/credit-card-customers/BankChurners.csv',\n    index_col='CLIENTNUM',\n    na_values=['Unknown']  # interpret \"Unknown\" as missing \n)\ndata.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Summary\n\nThe dataset consists of 10,127 rows and 20 columns.","metadata":{}},{"cell_type":"code","source":"# Drop the last 2 columns as advised in the dataset's description\n# See https://www.kaggle.com/sakshigoyal7/credit-card-customers\ndata = data.iloc[:, :-2]\ndata.info()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Missing Values\n\n`Education_Level`, `Marital_Status` and `Income_Category` have missing values. ","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nmissing = data.isna().sum()\npd.DataFrame({\n    'No. of missing values': missing,\n    '% missing': missing.apply(lambda x: f'{x/len(data):.2%}')\n}).style.background_gradient()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Strategy for Handling Missing Values\n\nA common and straight-forward way of dealing with missing values is to *drop affected rows or columns*. The advantage of this is that you'll be left with genuine, unaltered data. The disadvantage is that you lose some data; which is especially undesirable if the dataset is small, or large proportions of its values are missing.\n\nAnother common tactic is *imputation*, which involves determining values to fill in the blanks. The advantage here is that no data is thrown out. But then, depending on the method used, the imputed values might be misleading.\n\nRemoving rows with missing values would in this case leave only 7,081 rows for modelling. That is rather small, so we'll use imputation to get as much of the data as possible. This will be implemented as a component in the model fitting pipeline.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Numeric Columns\n\nThere are 14 numeric columns.","metadata":{}},{"cell_type":"code","source":"numeric_cols = data.select_dtypes(include='number')\nnumeric_cols.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.1 Summary Statistics","metadata":{}},{"cell_type":"code","source":"# Get summary statistics for numeric columns\nnumeric_cols.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.2 Histograms","metadata":{}},{"cell_type":"code","source":"# Plot histograms of numeric columns\nhistograms = numeric_cols.hist(figsize=(12, 12))","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">`Credit_Limit`, `Avg_Open_To_Buy`, `Total_Amt_Chng_Q4_Q1`, `Total_Trans_Amt`, `Total_Ct_Chng_Q4_Q1` and `Avg_Utilization_Ratio` are skewed to the right (positively skewed).\n\n>`Total_Revolving_Bal` has a curious peak close to the origin, which is investigated below:","metadata":{}},{"cell_type":"code","source":"total_rev_bal = data['Total_Revolving_Bal'].value_counts()\nprint(f'A very large number of customers ({total_rev_bal[0]:,}) have 0 Total_Revolving_Bal.')\nprint(total_rev_bal.nlargest(5))  # top 5 frequencies\nprint('\\nFrequencies of the first 5 values confirm that the peak is specifically at 0:')\nprint(total_rev_bal.sort_index().head())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.3 Box Plots","metadata":{}},{"cell_type":"code","source":"# Plot boxplots\nfig, axes = plt.subplots(nrows=math.ceil(numeric_cols.shape[1] / 3),\n                         ncols=3, figsize=(12, 18))\n\nfig.tight_layout(h_pad=3)  # Add padding to sub-plots\n\nfor col, ax in zip(numeric_cols.columns, axes.flatten()):\n    numeric_cols[col].plot.box(ax=ax,)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> `Credit_Limit`, `Avg_Open_To_Buy`, `Total_Amt_Chng_Q4_Q1`, `Total_Trans_Amt` and `Total_Ct_Chng_Q4_Q1` have a very large number of outliers.","metadata":{}},{"cell_type":"markdown","source":"### 2.3.4 Normal Probability Plots","metadata":{}},{"cell_type":"code","source":"# Plot probability plots (qq-plots)\nfig, axes = plt.subplots(nrows=math.ceil(numeric_cols.shape[1] / 3),\n                         ncols=3, figsize=(12, 18))\n\nfig.tight_layout(h_pad=5)  # Add padding to sub-plots\n\nfor col, ax in zip(numeric_cols.columns, axes.flatten()):\n    stats.probplot(numeric_cols[col], dist='norm', plot=ax)\n    ax.set_title(col)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `Customer_Age`, `Dependent_count`, `Months_on_book` and `Total_Trans_Ct` are somewhat normally distributed, which is good.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Categorical columns\n\nThere are 6 categorical columns.","metadata":{}},{"cell_type":"code","source":"categorical_cols = data.select_dtypes(include='O')\ncategorical_cols.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.1 Summary Statistics","metadata":{}},{"cell_type":"code","source":"# Get summary statistics for categorical columns\ncategorical_cols.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.2 Count Plots","metadata":{}},{"cell_type":"code","source":"# Plot countplots of categorical columns\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 15), )\n\nfor col, ax in zip(categorical_cols.drop('Attrition_Flag', 1), axes.flatten()):\n    sns.countplot(x=col, ax=ax, hue='Attrition_Flag', data=categorical_cols)\n    ax.tick_params(axis='x', rotation=45)\n    ax.set_title(col, size=16)\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Modelling & Prediction\n\nLet's now attempt to fit several classification models to predict whether a customer will leave.\n\nThe target variable - `Attrition_Flag` - is heavily imbalanced, with one class having significantly higher occurences than the rest.","metadata":{}},{"cell_type":"code","source":"attrition_bar_plot = data['Attrition_Flag'].value_counts().plot.bar()\n_ = attrition_bar_plot.set_title('Bar Plot of Attrition_Flag', size=14,\n                                 fontweight='bold')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> There exist strategies for handling such situations, some of which can be implemented using the [imbalanced-learn][1] package.\n\n> In this case, we'll use [Randomized over-sampling][2].\n\n[1]: https://imbalanced-learn.org/stable/user_guide.html\n[2]: https://imbalanced-learn.org/stable/over_sampling.html#naive-random-over-sampling","metadata":{}},{"cell_type":"code","source":"# Select the features and target\nX = data.drop('Attrition_Flag', axis=1)\ny = data['Attrition_Flag']\n\nnumeric_cols = X.select_dtypes(include='number').columns\ncategorical_cols = X.select_dtypes(include='O').columns\n\n# Prepare a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n# Apply random oversampling to counteract imbalance\nrandom_oversampler = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = random_oversampler.fit_resample(X_train, y_train)\n\n# Preprocessing pipeline for numeric cols\nnumeric_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),  # fill blanks with column's mean\n    ('scaler', StandardScaler())  # normalize values\n])\n\n# Preprocessing pipeline for categorical cols\ncategorical_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # fill blanks with column's mode\n    ('encoder', OneHotEncoder())  # one-hot-encode values\n])\n\n# Combined preprocessing pipeline\ncombined_features = ColumnTransformer([\n    ('numeric', numeric_pipe, numeric_cols),\n    ('categorical', categorical_pipe, categorical_cols)\n])\n\n\ndef fit_and_evaluate(classifier, params=None):\n    \"\"\"Fit a classification model and print metrics.\n    \n    Parameters\n    ----------\n    classifier: An instance of a classification predictor that follows the sklearn API.\n        The model to fit.\n    params: dict\n        A dictionary of hyper-parameter values to pass to RandomizedSearchCV \n        for model tuning.\n    \n    Returns\n    -------\n    The model with hyper-parameters yielding the highest cross-validated score.\n    \"\"\"\n    pipe = Pipeline([\n        ('features', combined_features),\n        ('classifier', classifier)\n    ])\n    \n    params = {} if not params else params  # set empty dict as default\n    \n    model = RandomizedSearchCV(estimator=pipe, param_distributions=params,\n                               scoring='roc_auc', n_jobs=2, cv=4, random_state=0)\n    model.fit(X_resampled, y_resampled)\n    \n    print(f'Cross Validation AUC: {model.best_score_:%}')\n    print(f'Test AUC: {model.score(X_test, y_test):%}')\n    print('\\nClassification Report:\\n' + '-'*58)\n    print(classification_report(model.predict(X_test), y_test))\n    \n    return model.best_estimator_","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"clf = RandomForestClassifier(random_state=7)\nparams = {'classifier__max_depth': range(3, 8),\n          'classifier__class_weight': [\"balanced\", \"balanced_subsample\"]}\n\nrf_model = fit_and_evaluate(clf, params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"clf = GradientBoostingClassifier(random_state=0)\nparams = {'classifier__max_depth': range(3, 8),\n          'classifier__n_estimators': range(100, 500, 100)}\n\ngb_model = fit_and_evaluate(clf, params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"clf = SVC(class_weight='balanced', random_state=0)\nparams = {'classifier__C': np.logspace(1, 4, 10)}\n\nsvc_model = fit_and_evaluate(clf, params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Logistic Regression","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression(random_state=2, class_weight='balanced')\nparams = {'classifier__C': np.logspace(1, 4, 10)}\n\nlr_model = fit_and_evaluate(clf, params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Conclusion\n\nAmong the models tested above, the `GradientBoostingClassifier` seems the most promising.\n\nLet's visualise sample predictions to check if the predictions from the various models are consistent:","metadata":{}},{"cell_type":"code","source":"sample = X.sample(25, random_state=5)\n\nresults = pd.DataFrame({\n    'Random Forest Classifier': rf_model.predict(sample),\n    'Gradient Boosting Classifier': gb_model.predict(sample),\n    'Support Vector Classifier': svc_model.predict(sample),\n    'Logistic Regression': lr_model.predict(sample)\n})\n\n\ndef color_code(cell):\n    \"\"\"Set a DataFrame cell's background color according to its value.\"\"\"\n    if cell == 'Existing Customer':\n        color = 'aqua'\n    else:\n        color = 'orangered'\n    return f'background-color: {color}'\n\nresults.style.applymap(color_code)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}