{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Activation,Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM\nfrom keras.layers import Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/bitcoin-tweets-14m/cleanprep.csv',nrows=20000)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns=['date','name','text','sentiment','polarity']\ntrain=train.drop(['date','name','polarity'],axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets save stopwords in a variable\nstop = list(stopwords.words(\"english\"))\nprint(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save list of punctuation/special characters in a variable\npunctuation = list(string.punctuation)\nprint(punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an object to convert the words to its lemma form\nlemma = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets make a combine list of stopwords and punctuations\nsw_pun = stop + punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to preprocess the messages\ndef preprocess(tweet):\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet) # removing urls \n    tweet = re.sub('[^\\w]',' ',tweet) # remove embedded special characters in words (for example #earthquake)         \n    #tweet = re.sub('[\\d]','',tweet) # this will remove numeric characters\n    tweet = tweet.lower()\n    words = tweet.split()  \n    sentence = \"\"\n    for word in words:     \n        if word not in (sw_pun):  # removing stopwords & punctuations                \n            word = lemma.lemmatize(word,pos = 'v')  # converting to lemma    \n            if len(word) > 3: # we will consider words with length  greater than 3 only\n                sentence = sentence + word + ' '             \n    return(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply preprocessing functions on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : preprocess(s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying the function on the train and the test datasets\ntrain['text'] = train['text'].apply(lambda s : remove_emoji(s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vocabulary creation\nLets create our own vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to create vocab\nfrom collections import Counter\ndef create_vocab(df):\n    vocab = Counter()\n    for i in range(df.shape[0]):\n        vocab.update(df.text[i].split())\n    return(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate training and testing datasets\nmaster=train.reset_index(drop=True)\n\n# call vocabulary creation function on master dataset\nvocab = create_vocab(master)\n\n# lets check the no. of words in the vocabulary\nlen(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the most common 50 words in the vocabulary\nvocab.most_common(50)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets consider only those words which have appeared more than once in the corpus\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the final vocab by considering words with more than one occurence\nfinal_vocab = []\nmin_occur = 2\nfor k,v in vocab.items():\n    if v >= min_occur:\n        final_vocab.append(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the no. of the words in the final vocabulary\nvocab_size = len(final_vocab)\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets apply this vocab on our train and test datasets, we will keep only those words in training and testing datasets which appear in the vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to filter the dataset, keep only words which are present in the vocab\ndef filter(tweet):\n    sentence = \"\"\n    for word in tweet.split():  \n        if word in final_vocab:\n            sentence = sentence + word + ' '\n    return(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply filter function on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : filter(s))\ntrain.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n# fit a tokenizer\ndef create_tokenizer(lines):\n    # num_words = vocab_size will create a tokenizer,configured to only take into account the vocab_size(6025)\n    tokenizer = Tokenizer(num_words=vocab_size)\n    # Build th word index, Turns strings into lists of integer indices\n    tokenizer.fit_on_texts(lines) \n    return tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and apply tokenizer on the training dataset\ntokenizer = create_tokenizer(train.text)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous allons maintenant appliquer la fonction text_to_matrix () pour convertir du texte en vecteurs.\n\nLa fonction text_to_matrix () sur le Tokenizer peut être utilisée pour créer un vecteur par document fourni par entrée. La longueur des vecteurs est la taille totale du vocabulaire, qui est de 6025 ici (nous avons passé 6025 en tant que num_words dans tokenizer)\n\n\n* ‘binary‘: Si chaque mot est présent ou non dans le document. C'est la valeur par défaut.\n* ‘count‘: Le nombre de chaque mot dans le document.\n* ‘tfidf‘: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word \n* ‘freq‘: La fréquence de chaque mot dans le document."},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting texts into vectors\ntrain_text = tokenizer.texts_to_matrix(train.text, mode = 'freq')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building & Evaluation"},{"metadata":{},"cell_type":"markdown","source":"## Neural Network\n\nNous allons créer un réseau de neurones artificiels, les sentiments sont évalués par la fonction f1, qui ne sont pas affichés par défaut après chaque époque, donc créons une fonction pour obtenir le score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(train_text, train.sentiment, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model(n_words):\n    # define network\n    model = Sequential()\n    model.add(Dense(1024, input_shape=(n_words,), activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(512,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n    \n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=10,),\n                  ModelCheckpoint(filepath='./NN.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nn_words = X_train.shape[1]\nmodel = define_model(n_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train,y_train,epochs=20,\n                    verbose=2,\n                    callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\n\n# load the model from disk\nloaded_model_NN = keras.models.load_model('./NN.h5',custom_objects=dependencies)\n\n# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_NN.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions on the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n\ntest=pd.read_csv('../input/tweets/live_tweet.csv')\n\ntest_id = test.tweet\ntest.drop([\"id\",\"date\",\"name\"],1,inplace = True)\n\n#apply tokenizer on the test dataset\ntest_set = tokenizer.texts_to_matrix(test.text, mode = 'freq')\n\n# make predictions on the test dataset\ny_test_pred = loaded_model_NN.predict_classes(test_set)\n\n\nresult = pd.DataFrame()\nresult.to_csv('prediction_NN.csv',index=False)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model utilisant les Word Embeddings\n\nUn autre moyen populaire et puissant d'associer un vecteur à un mot est l'utilisation de vecteurs de mots denses, également appelés `word embeddings`. \n\nLa couche Embedding est un dictionnaire qui mappe des indices entiers (qui représentent des mots spécifiques) à des vecteurs denses. Il prend des entiers en entrée, il recherche ces entiers dans un dictionnaire interne et il renvoie les vecteurs associés. Il s’agit en fait d’une recherche dans le dictionnaire.\n\nAlors que les vecteurs obtenus par encodage one-hot sont binaires, clairsemés (principalement constitués de zéros) et de très haute dimension (même dimensionnalité que le nombre de mots dans le vocabulaire), les embeddings de mots sont des vecteurs à virgule flottante de faible dimension (c'est-à-dire , vecteurs denses, par opposition aux vecteurs clairsemés); \n\nContrairement aux vecteurs de mots obtenus via un encodage one-hot, les embeddings de mots sont appris à partir de données. Il est courant de voir des word embeddings de dimensions 256 , 512 ou 1 024 lorsqu'il s'agit de vocabulaires très volumineux.\n\nD'autre part, one-hot encoding conduisent généralement à des vecteurs de 20 000 dimensions ou plus (capturant un vocabulaire de 6 025 tokens). Ainsi, les word embeddings regroupent plus d'informations dans beaucoup moins de dimensions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding\n# La couche Embedding prend au moins deux arguments: le nombre de jetons tokens et la dimension des embeddings (here, 64).\nembedding_layer = Embedding(vocab_size, 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Nombre de mots à considérer comme caractéristiques\nmax_features = vocab_size\n\n# Coupe le texte après ce nombre de mots (parmi les max_features les mots les plus courants)\nmaxlen = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# créer et appliquer un tokenizer sur l'ensemble de données d'entraînement\ntokenizer = create_tokenizer(train.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import preprocessing\n# conversion de texte en séquences\nsequences = tokenizer.texts_to_sequences(train.text)\nfor i in [20,300,43]:\n    print('La phrase % a été transcrite en '%tokenizer.sequences_to_texts([sequences[i]]),sequences[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen), padding shorter sequences with 0s\ntrain_text = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(train_text, train.sentiment, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network with Embedding Layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n# Spécifie la longueur d'entrée maximale du Embedding layer afin que vous puissiez ultérieurement aplatir les entrées embedded. \n\n# Après le calque Embedding, les activations ont la forme (samples, maxlen, 8)\nmodel.add(Embedding(vocab_size, 8, input_length=maxlen))\n\n# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)\nmodel.add(Flatten())\n\n# Dense layer for classification\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./embd.h5',monitor='val_loss',\n                                  save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model\nhistory = model.fit(np.asarray(X_train).astype(np.float32),y_train,\n                    epochs=10,batch_size=32,\n                    callbacks=callbacks_list,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the model from disk\nloaded_model_embd = keras.models.load_model('./embd.h5',custom_objects=dependencies)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"nous ne gardons que les 20 premiers mots de chaque tweets. Mais notez que le simple aplatissement des séquences incorporées et la formation d'une seule couche dense sur le dessus conduit à un modèle qui traite chaque mot de la séquence d'entrée séparément, sans prendre en compte les relations entre les mots et la structure des phrases (par exemple, ce modèle traiterait probablement les deux \" ce film est une bombe »et« ce film est la bombe »comme étant des critiques négatives). \n\nIl est préférable d’ajouter des couche recurrent layers ou 1D convolutional layers au-dessus des embedded sequences pour apprendre les fonctionnalités qui prennent en compte chaque séquence dans son ensemble."},{"metadata":{},"cell_type":"markdown","source":"## SIMPLE RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding, SimpleRNN\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./SRNN.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    epochs=20,batch_size=32,\n                    callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# load the model from disk\nloaded_model_SRNN = keras.models.load_model('./SRNN.h5',custom_objects=dependencies)\ny_pred = loaded_model_SRNN.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stack multiple SimpleRNN layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words=20000\n\nfrom keras.layers import Embedding, SimpleRNN\nmodel = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./STRNN.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    epochs=20,batch_size=32,\n                    callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./LSTM.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bi-Direction LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./BILSTM.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural Network with Embedding layer seems to the best model for this classification task."},{"metadata":{},"cell_type":"markdown","source":"## GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./GRU.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacked GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./SGRU.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacked GRU with Dropouts"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='./DSGRU.h5',monitor='val_loss',save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}