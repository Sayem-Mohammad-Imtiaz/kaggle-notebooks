{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n!apt-get update\n! apt install -y openjdk-11-jre-headless","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nsc= SparkContext(master= 'local', appName= 'Fake and real news')\nss= SparkSession(sc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.types import StringType, StructField, StructType\ndef read_data(path):\n  schema= StructType(\n      [StructField('title',StringType(),True),\n      StructField('text',StringType(),True),\n      StructField('subject',StringType(),True),\n      StructField('date',StringType(),True)])\n  pd_df= pd.read_csv(path)\n  sp_df= ss.createDataFrame(pd_df, schema= schema)\n  return sp_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_true= '/kaggle/input/fake-and-real-news-dataset/True.csv'\npath_fake= '/kaggle/input/fake-and-real-news-dataset/Fake.csv'\ntrue_df= read_data(path_true)\nfake_df= read_data(path_fake)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_df.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import lit, rand\ndata= true_df.withColumn('fake', lit(0)).union(fake_df.withColumn('fake', lit(1))).orderBy(rand())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupBy('fake').count().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the values of the subject column\ndata.select('subject').distinct().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import SQLTransformer, RegexTokenizer, StopWordsRemover, CountVectorizer, Imputer, IDF\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nStopWordsRemover.loadDefaultStopWords('english')\n\n# 0. Extract tokens from title\ntitle_tokenizer= RegexTokenizer(inputCol= 'title', outputCol= 'title_words',\n                                pattern= '\\\\W', toLowercase= True)\n# 1. Remove stop words from title\ntitle_sw_remover= StopWordsRemover(inputCol= 'title_words', outputCol= 'title_sw_removed')\n# 2. Compute Term frequency from title\ntitle_count_vectorizer= CountVectorizer(inputCol= 'title_sw_removed', outputCol= 'tf_title')\n# 3. Compute Term frequency-inverse document frequency from title\ntitle_tfidf= IDF(inputCol= 'tf_title', outputCol= 'tf_idf_title')\n# 4. Extract tokens from text\ntext_tokenizer= RegexTokenizer(inputCol= 'text', outputCol= 'text_words',\n                                pattern= '\\\\W', toLowercase= True)\n# 5. Remove stop words from text\ntext_sw_remover= StopWordsRemover(inputCol= 'text_words', outputCol= 'text_sw_removed')\n# 6. Compute Term frequency from text\ntext_count_vectorizer= CountVectorizer(inputCol= 'text_sw_removed', outputCol= 'tf_text')\n# 7. Compute Term frequency-inverse document frequency text\ntext_tfidf= IDF(inputCol= 'tf_text', outputCol= 'tf_idf_text')\n# 8. StringIndexer subject\nsubject_str_indexer= StringIndexer(inputCol= 'subject', outputCol= 'subject_idx')\n# 9. VectorAssembler\nvec_assembler= VectorAssembler(inputCols=['tf_idf_title', 'tf_idf_text', 'subject_idx'], outputCol= 'features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\nrf= RandomForestClassifier(featuresCol= 'features', labelCol= 'fake', predictionCol= 'fake_predict', maxDepth= 7, numTrees= 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nrf_pipe= Pipeline(stages=[title_tokenizer, # 0\n                title_sw_remover, # 1\n                title_count_vectorizer, # 2\n                title_tfidf, # 3\n                text_tokenizer, # 4\n                text_sw_remover, # 5\n                text_count_vectorizer, # 6\n                text_tfidf, # 7\n                subject_str_indexer, # 8\n                vec_assembler, # 9\n                rf]) # 10 model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test= data.randomSplit([0.8, 0.2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model= rf_pipe.fit(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.evaluation import  MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n\naccuracy= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'accuracy')\nf1= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'f1')\nareaUnderROC= BinaryClassificationEvaluator(labelCol= 'fake', metricName= 'areaUnderROC')\n\ndef classification_evaluator(data_result):\n    data_result.crosstab(col1= 'fake_predict', col2= 'fake').show()\n    print('accuracy:' ,accuracy.evaluate(data_result))\n    print('f1:' ,f1.evaluate(data_result))\n    print('areaUnderROC:' ,areaUnderROC.evaluate(data_result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on training data set\nrf_train_result= rf_model.transform(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_evaluator(rf_train_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on test data set\nrf_test_result= rf_model.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_evaluator(rf_test_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrModel = rf_model.stages[10]\ntrainingSummary = lrModel.summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainingSummary.roc.show(5)\nprint(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fMeasure = trainingSummary.fMeasureByThreshold\nmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(5,5))\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(trainingSummary.roc.select('FPR').collect(),\n         trainingSummary.roc.select('TPR').collect())\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn \nfrom pyspark.ml.classification import RandomForestClassifier\n\n\npredictions_train = rf_model.transform(train)\n\ny_true = predictions_train.select(['fake']).collect()\ny_pred = predictions_train.select(['fake_predict']).collect()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_true, y_pred, output_dict=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nax = plt.axes()\nsns.heatmap(pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)).iloc[:-1, :].T,ax = ax, annot=True)\nax.set_title('On Training Set')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn \nfrom pyspark.ml.classification import RandomForestClassifier\n\n\npredictions_test = rf_model.transform(test)\n\ny_true = predictions_test.select(['fake']).collect()\ny_pred = predictions_test.select(['fake_predict']).collect()\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_true, y_pred, output_dict=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nax = plt.axes()\nsns.heatmap(pd.DataFrame(classification_report(y_true, y_pred, output_dict=True)).iloc[:-1, :].T,ax = ax, annot=True)\nax.set_title('On Testing Set')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_temp = rf_test_result.select(\"fake\").groupBy(\"fake\")\\\n                        .count().sort('count', ascending=False).toPandas()\nclass_temp = class_temp[\"fake\"].values.tolist()\nclass_names = map(str, class_temp)\n# # # print(class_name)\nclass_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_true = rf_test_result.select(\"fake\")\ny_true = y_true.toPandas()\n\ny_pred = rf_test_result.select(\"fake_predict\")\ny_pred = y_pred.toPandas()\n\ncnf_matrix = confusion_matrix(y_true, y_pred)\ncnf_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Real', 'Fake'],\n                      title='Confusion matrix, without normalization')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc.stop()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}