{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/imdb-5000-movie-dataset/movie_metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identifying null values "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Majority of the data contains null values which needs to be removed  or imputed"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### IMDB link is a index variable which is not required for the analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop IMDB link\ndf.drop('movie_imdb_link',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove the null values row vise and check for the loss of data "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(axis=0,subset=['director_name', 'num_critic_for_reviews','duration','actor_3_facebook_likes','actor_2_name','actor_1_facebook_likes','actor_1_name','actor_3_name','facenumber_in_poster','num_user_for_reviews','language','country','actor_2_facebook_likes','plot_keywords'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Investigating genre and director name"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['genres'].value_counts().unique().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### It is clear that genre contains 1850 unique categories and thus needs to further investigated"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['genres'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### It is clear that some genres are attached together for movies. This is mainly because movies wants to cater a diverse set of individuals thus opts to include many genres for the movie rather than going for a single theme."},{"metadata":{},"cell_type":"markdown","source":"##### We need to split the genres first and then apply one hot encoding to get a sparse matrix for each genre "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['genres'] = df['genres'].str.split('|') #split the genres\ngenres = df['genres'].str.join('|').str.get_dummies()# getting dummies\ndf = pd.concat([df, genres], axis=1) # connecting back the genres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['genres'],axis=1,inplace=True) #dropping genres variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['director_name'].value_counts().unique().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Since there are a lot of unique values we need to drop this variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['director_name'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['content_rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['content_rating'].isna().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['content_rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since majority is R in content rating we impute this\ndf['content_rating'].fillna('R', inplace = True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.get_dummies(data = df, columns = ['content_rating'] , prefix = ['content_rating'] , drop_first = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(df.drop(['gross'],axis=1),df['gross'],random_state=1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Budget variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.distplot(X_train['budget'])\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It apears that the budget variable contains a lot of peaks and contains a high standard deviation thus we need to check for a summary of the distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['budget'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best way to impute the missing values in this scenerio is by the use of the median"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statistics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statistics.median(X_train['budget'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#impute the median to the budet as it is skewed\nX_train['budget'].fillna(85000000.0, inplace = True) \nX_test['budget'].fillna(85000000.0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aspect ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(X_train['aspect_ratio'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aspect ratio appears to be bimodal therefore lets impute the median for this as well as a skewness is illustarated on top of the two modes in the distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"statistics.median(X_train['aspect_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#impute the median since the data is skewed\nX_train['aspect_ratio'].fillna(1.85, inplace = True) \nX_test['aspect_ratio'].fillna(1.85, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gross (Target variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gross illustrates a skewed distribution thus the median is needed to be imputed"},{"metadata":{"trusted":true},"cell_type":"code","source":"statistics.median(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since the gross income is skewed will impute the median for NA values\ny_train.fillna(72962455.0, inplace = True) \ny_test.fillna(72962455.0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Color "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(X_train['color'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since majority of the color variable is Color will impute that element\nX_train['color'].fillna('Color', inplace = True) \nX_test['color'].fillna('Color', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears no more NA values is available"},{"metadata":{},"cell_type":"markdown","source":"## Getting dummies for the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train= pd.get_dummies(data = X_train, columns = ['color'] , prefix = ['color'] , drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test= pd.get_dummies(data = X_test, columns = ['color'] , prefix = ['color'] , drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#movie title is a categorical variable with a lot of categories\nX_train.drop(['movie_title'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop(['movie_title'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Movie title was dropped as it only acts as an index variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot keywords has alot of unique keywords therefore it was dropped\nX_train.drop(['plot_keywords'],axis=1,inplace=True)\nX_test.drop(['plot_keywords'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(X_train['country'])\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most movies appears to be made in the USA followed by the UK. Other countries seems to provide less contribution to the dataset. Therefore in turn we can state that there is a bias towards US based data.\nLets recode the country variable such that it only represents USA,UK and other."},{"metadata":{"trusted":true},"cell_type":"code","source":"# country is recoded to three categories as USA, UK and other\nvalue_count=X_train[\"country\"].value_counts()\nvals = value_count[:2].index\nprint (vals)\n\nX_train['country'] = X_train.country.where(X_train.country.isin(vals), 'other')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_count=X_test[\"country\"].value_counts()\nvals = value_count[:2].index\nprint (vals)\n\nX_test['country'] = X_test.country.where(X_test.country.isin(vals), 'other')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting dummies for country\nX_train= pd.get_dummies(data = X_train, columns = ['country'] , prefix = ['country'] , drop_first = True)\nX_test= pd.get_dummies(data = X_test, columns = ['country'] , prefix = ['country'] , drop_first = True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.countplot(X_train['language'])\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot it is clear that most mmovies are infact English. Hindi movies which is part of the Bollywood industry being the largest industry at presents lacks substantial representation.\nTherefore we can recode the country variable as English and other."},{"metadata":{"trusted":true},"cell_type":"code","source":"#The language is divided to English and other\ncount_lanuage=X_train[\"language\"].value_counts()\ncount_lanuage1=X_test[\"language\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vals1 = count_lanuage[:1].index\n\nvals2 = count_lanuage1[:1].index\n\nX_train['language'] = X_train.language.where(X_train.language.isin(vals1), 'other')\nX_test['language'] = X_test.language.where(X_test.language.isin(vals2), 'other')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['language'].value_counts(),X_train['language'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting dummies for language\nX_train = pd.get_dummies(data = X_train, columns = ['language'] , prefix = ['language'] , drop_first = True)\nX_test = pd.get_dummies(data = X_test, columns = ['language'] , prefix = ['language'] , drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping actor names as they have too much of unique values "},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping categorical variables actor 1 name, actor 2 name, actor 3 name\nX_train.drop(['actor_1_name','actor_2_name','actor_3_name'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop(['actor_1_name','actor_2_name','actor_3_name'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape,X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape,y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Advance analysis "},{"metadata":{},"cell_type":"markdown","source":"### Importing standard scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stdscaler=StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std=stdscaler.fit(X_train)\nX_std=stdscaler.transform(X_train)\nX_std_test=stdscaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the random forest model to our train data\nrtree=RandomForestRegressor(random_state=0).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the R squared\nrtree.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy needs to be improved"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking if the data has overfitted\nrtree.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### It appears that the data has overfitted therefore we need to tune our parameters"},{"metadata":{},"cell_type":"markdown","source":"### Lets fit the standardized data as well first"},{"metadata":{"trusted":true},"cell_type":"code","source":"rtree1=RandomForestRegressor(random_state=0).fit(X_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rtree1.score(X_std_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The scaling has not helped as random forest does not require us to do feature scaling"},{"metadata":{},"cell_type":"markdown","source":"### Lets identify the most importamt features first"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm_importance = permutation_importance(rtree, X_test, y_test)\n\nsorted_idx = perm_importance.importances_mean.argsort()[::-1]\nplt.figure(figsize=(5,10))\nsns.barplot(y=X_test.columns[sorted_idx], x=perm_importance.importances_mean[sorted_idx])\nplt.xlabel(\"Permutation Importance\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### It is clear thatnumber of voted users are the most important feature\n"},{"metadata":{},"cell_type":"markdown","source":"### Lets do a random grid search to find the best parameters for the set of data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# Creating the base model to tune the parameters\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 99 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 33, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#identifying the best parameters\nrf_random.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the data\nrtree3=RandomForestRegressor(max_features='auto',random_state=0,min_samples_split=2,n_estimators= 2000,min_samples_leaf= 1,max_depth= 20, bootstrap= True).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rtree3.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient boost regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad=GradientBoostingRegressor(random_state=0,learning_rate=0.15).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad.score(X_test,y_test),grad.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### This appears to have a higher accuracy and has not over fitted the data\n"},{"metadata":{},"cell_type":"markdown","source":"##### But before we tune lets check the accuracy we obtain by the use of xtreme gradient boost as well "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xboost=XGBRegressor().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xboost.score(X_train,y_train),xboost.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### grid search has to be applied for gradient boost as it has better prediction than extreme gradient boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in gradient boost\nn_estimators = [100,250,500,750,1000,1250,1500,1750]\n# Number of features to consider at every split\nmax_features = ['sqrt','auto','log2']\n#learning rate\nlearning_rate=[0.15,0.1,0.05,0.01,0.005,0.001]\n#maxdepth\nmax_depths = [2,3,4,5,6,7]\n#min sample splits\nmin_samples_splits = [2,4,6,8,10,20,40,60,100]\n#minleaf\nmin_samples_leafs = [1,3,5,7,9]\n#subsamples\nsubsample=[0.7,0.75,0.8,0.85,0.9,0.95,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depths,\n               'min_samples_split': min_samples_splits,\n               'min_samples_leaf': min_samples_leafs,\n               'learning_rate':learning_rate,\n              'subsample':subsample}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n#Creating the base model to tune\ngdr = GradientBoostingRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\ngdr_random = RandomizedSearchCV(estimator = gdr, param_distributions = random_grid, n_iter = 33, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model to execute the search\n\ngdr_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gdr_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad1=GradientBoostingRegressor(learning_rate=0.01, max_depth=7, max_features='sqrt',\n                          min_samples_leaf=5, min_samples_split=20,\n                          n_estimators=1250, subsample=0.7,random_state=0).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad1.score(X_test,y_test),grad1.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting regressor "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"er = VotingRegressor([('rf', rtree3),('gb',grad1)]).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"er.score(X_test,y_test),er.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}