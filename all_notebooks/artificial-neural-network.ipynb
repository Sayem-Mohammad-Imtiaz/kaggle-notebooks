{"cells":[{"metadata":{"_uuid":"25b0f2c6e1fac5495243acf4772b167b34f01797"},"cell_type":"markdown","source":"<h1><strong>Introduction</strong></h1>\nThis is an implementation of Arificial Neural Network(ANN). For data preprocessing, I have used sklearn and for data generation, numpy. Inspired by <strong>Keras</strong> I've written the class to resemble the brilliant framework as closely as possible."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler\nprint(os.listdir(\"../input\"))\nnp.random.seed(0)\n# Any results you write to the current directory are saved as output.","execution_count":79,"outputs":[{"output_type":"stream","text":"['iris-dataset', 'overfit', 'ring-data']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"types = ['f8', 'f8']\ntrain_set = np.genfromtxt(\"../input/overfit/data_overfit.csv\", dtype=types, delimiter=',',names=True)\n#enc=LabelEncoder()\n#sca=MinMaxScaler()\n#y=enc.fit_transform(train_set[:,2])\n#train_set[:,2]=y\n#trainY=np.eye(2)[train_set[:,2].astype(int)]\ndata=[]\nfor tup in train_set:\n    data.append(list(tup))\ndata=np.array(data)\nnp.random.shuffle(data)\n'''sca.fit(trainX)\ntrainX=sca.transform(trainX)'''","execution_count":80,"outputs":[{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"'sca.fit(trainX)\\ntrainX=sca.transform(trainX)'"},"metadata":{}}]},{"metadata":{"_uuid":"5d8c8ce8088844c19ad6cfba03a55d6e2e29c7f9"},"cell_type":"markdown","source":"<h1><strong>Making the splits</strong></h1>\nSplitting the dataset into train,test and validation sets"},{"metadata":{"trusted":true,"_uuid":"81defd442cc8cb13e5806a7f8b764a04c1e58acf"},"cell_type":"code","source":"trainX,trainY=data[:4,0].reshape(-1,1),data[:4,1].reshape(-1,1)\ntestX,testY=data[4:,0].reshape(-1,1),data[4:,1].reshape(-1,1)","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trainX.shape,trainY.shape)\n#print(valX.shape,valY.shape)\nprint(testX.shape,testY.shape)","execution_count":84,"outputs":[{"output_type":"stream","text":"(4, 1) (4, 1)\n(135, 1) (135, 1)\n","name":"stdout"}]},{"metadata":{"_uuid":"56024e0c9cdb5db8fba53da311c680e27f11fa7e","_kg_hide-input":false},"cell_type":"markdown","source":"<h1><strong>Lets Plot</strong><h1>"},{"metadata":{"trusted":true,"_uuid":"57a61e8bf42f3b010959830718d2071d51e92b21"},"cell_type":"code","source":"plt.scatter(data[:,0], data[:,1], marker='.')","execution_count":85,"outputs":[{"output_type":"execute_result","execution_count":85,"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7fc436b9deb8>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"75909b6d08197e44cbd3d31c83c173fa6d15b500"},"cell_type":"code","source":"#utility sigmoid activation function\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))","execution_count":86,"outputs":[]},{"metadata":{"_uuid":"1bb261e9e9df81863ecdff4a6c942e713ba5d3cc"},"cell_type":"markdown","source":"<h1><strong>The actual Model class for ANN </strong></h1>"},{"metadata":{"trusted":true,"_uuid":"2e184f8e60c2af0634bb440e839a6e0e7297e25b"},"cell_type":"code","source":"#the actual class that gives the model\nclass Model:\n    def __init__(self):\n        self.layer_count=0\n        self.weights={}\n        self.biases={}\n        self.l_dims=[]\n        self.grad_W={}\n        self.grad_b={}\n        self.layer_ops={} #stores the output of each layer.\n        self.summary={} #stores the name and shape of each layer\n        self.history={\"train_loss\":[],\"val_loss\":[],\"train_acc\":[],\"val_acc\":[]}\n        self.best=None\n    \n    #returns the layer parameters\n    def Dense(self,nodes=1,input_shape=None):\n        self.layer_count+=1\n        if input_shape:\n            self.l_dims.append(input_shape)\n        self.l_dims.append(nodes)\n        return np.random.random_sample((nodes,self.l_dims[self.layer_count-1])),np.random.random_sample((nodes,1))\n        \n    #adds the weights passed as layers\n    def add(self,weights):\n        self.weights[f\"w_{self.layer_count}\"]=weights[0]\n        self.biases[f\"b_{self.layer_count}\"]=weights[1]\n        self.summary[f\"Dense_{self.layer_count}\"]=weights[0].shape\n        \n    def predict(self,x):\n        self.layer_ops[\"v_0\"]=x\n        for i in range(self.layer_count-1):\n            x=self.layer_ops[f\"v_{i}\"]\n            x=np.dot(x,self.weights[f'w_{i+1}'].T)\n            x=np.add(x,self.biases[f\"b_{i+1}\"].T)\n            x=sigmoid(x)\n            self.layer_ops[f'v_{i+1}']=x #saving the layer output\n        x=np.dot(x,self.weights[f'w_{self.layer_count}'].T)\n        self.layer_ops[f'v_{self.layer_count}']=x\n        return x\n    \n    def fit(self,X,Y,validation_data=None,epochs=100000,batch_size=1,lr=0.005):\n        batches=X.shape[0]//batch_size\n        best_acc=0\n        if validation_data:\n            valX,valY=validation_data\n        for e in range(epochs):\n            loss=0\n            for i in range(batches):\n                x=X[i*batch_size:i+1*batch_size]\n                y=Y[i*batch_size:i+1*batch_size]\n                v=self.predict(x)\n                grad_base=((v-y)).T #the centerpiece of the gardient for every layer\n                for j in reversed(range(1,self.layer_count+1)):\n                    self.grad_W[f'{\"w_\"}{j}']=grad_base\n                    #calculation of gradient for each layer\n                    for k in reversed(range(j,self.layer_count)):\n                        self.grad_W[f'{\"w_\"}{j}']=np.dot(self.weights[f'{\"w_\"}{k+1}'].T,self.grad_W[f'{\"w_\"}{j}'])\n                        self.grad_W[f'{\"w_\"}{j}']=self.grad_W[f'{\"w_\"}{j}']*(self.layer_ops[f'{\"v_\"}{k}']*\n                                                                             (1-self.layer_ops[f'{\"v_\"}{k}'])).T\n                    self.grad_b[f\"b_{j}\"]=np.sum(self.grad_W[f\"w_{j}\"],axis=1).reshape(self.biases[f\"b_{j}\"].shape) #bias gradient\n                    self.grad_W[f'{\"w_\"}{j}']=np.dot(self.grad_W[f'{\"w_\"}{j}'],self.layer_ops[f'{\"v_\"}{j-1}']) #weight gradient\n                    #bias and weights adjustment\n                    self.weights[f'{\"w_\"}{j}']-=lr*self.grad_W[f'{\"w_\"}{j}']\n                    self.biases[f'b_{j}']-=lr*self.grad_b[f\"b_{j}\"]\n                    #self.weights[f'{\"w_\"}{j}']/=trainX.shape[0]\n                    #self.biases[f'{\"b_\"}{j}']/=trainX.shape[0]\n                loss+=np.sum((v-y)*(v-y))/(2*batch_size) #batch loss aggregation\n            val_loss=0\n            train_acc=accuracy_score(np.argmax(trainY,axis=1),np.argmax(self.predict(trainX),axis=1))\n            #saving performance on train set\n            self.history[\"train_acc\"].append(train_acc)\n            self.history[\"train_loss\"].append(loss)\n            label=self.predict(trainX)\n            #validation\n            if validation_data:\n                val_v=self.predict(valX)\n                #print(val_v.shape,valY.shape)\n                val_loss+=np.sum((val_v-valY)*(val_v-valY))/(2*valX.shape[0])\n                val_acc=accuracy_score(np.argmax(valY,axis=1),np.argmax(self.predict(valX),axis=1))\n                self.history[\"val_acc\"].append(val_acc)\n                self.history[\"val_loss\"].append(val_loss)\n                if best_acc<val_acc:\n                    self.best=Model()\n                    for key,value in self.weights.items():\n                        self.best.weights[key]=np.copy(value)\n                    for key,value in self.biases.items():\n                        self.best.biases[key]=np.copy(value)\n                    best_acc=val_acc\n                print(f\"epoch {e}  train loss: {loss}  validation loss:{val_loss}\")\n            else:\n                print(f\"epoch {e}  train loss: {loss} \")","execution_count":87,"outputs":[]},{"metadata":{"_uuid":"153548f15a6bc3d12c17d657dd88de1d0a499a10"},"cell_type":"markdown","source":"<h1><strong> Creation and fitting of the model </strong></h1>"},{"metadata":{},"cell_type":"markdown","source":"Works fine for one layer. Learns but underfits."},{"metadata":{"trusted":true,"_uuid":"ae20771d7a92dfbbc58f7cd9fa1d4cd1185c3b3b","scrolled":true},"cell_type":"code","source":"model=Model()\nmodel.add(model.Dense(nodes=6,input_shape=trainX.shape[1]))\nmodel.add(model.Dense(nodes=7))\nmodel.add(model.Dense(nodes=7))\nmodel.add(model.Dense(trainY.shape[1]))\nmodel.fit(trainX[:4],trainY[:4],validation_data=[testX,testY],batch_size=1,epochs=200000,lr=0.01)","execution_count":88,"outputs":[{"output_type":"stream","text":"epoch 0  train loss: 17.857424149732168  validation loss:3.768432304926361\nepoch 1  train loss: 10.396898758839006  validation loss:2.338766903350849\nepoch 2  train loss: 6.096850644489643  validation loss:1.4916433740255313\nepoch 3  train loss: 3.615614785902496  validation loss:0.9855024631773089\nepoch 4  train loss: 2.1824733669374754  validation loss:0.6800932558436291\nepoch 5  train loss: 1.3538868842446043  validation loss:0.4936440872052367\nepoch 6  train loss: 0.8742981176742642  validation loss:0.3782616284420123\nepoch 7  train loss: 0.5963365981018042  validation loss:0.3057457735220489\nepoch 8  train loss: 0.4349619777189337  validation loss:0.259385351228697\nepoch 9  train loss: 0.3410708742843407  validation loss:0.22919983665206392\nepoch 10  train loss: 0.2862913713201549  validation loss:0.20917174084911586\nepoch 11  train loss: 0.2542172431299725  validation loss:0.19563158196149613\nepoch 12  train loss: 0.23535218298932242  validation loss:0.18631170995859286\nepoch 13  train loss: 0.22419262547599006  validation loss:0.17978926704397277\nepoch 14  train loss: 0.2175438431110493  validation loss:0.17515624140891237\nepoch 15  train loss: 0.21354748153853909  validation loss:0.17182255455952383\nepoch 16  train loss: 0.21111963593670727  validation loss:0.16939745397845707\nepoch 17  train loss: 0.20962591884273163  validation loss:0.167617273861038\nepoch 18  train loss: 0.208693394238882  validation loss:0.16630085734997208\nepoch 19  train loss: 0.2081015970804086  validation loss:0.16532163182950585\nepoch 20  train loss: 0.20771928273978738  validation loss:0.16458982074604137\nepoch 21  train loss: 0.20746764305621923  validation loss:0.16404090601560786\nepoch 22  train loss: 0.207298858265698  validation loss:0.16362800275236133\nepoch 23  train loss: 0.20718354885302376  validation loss:0.16331672391441956\nepoch 24  train loss: 0.20710340263475482  validation loss:0.16308165838630467\nepoch 25  train loss: 0.2070468189706244  validation loss:0.1629039143461918\nepoch 26  train loss: 0.20700631779495335  validation loss:0.16276937937201483\nepoch 27  train loss: 0.2069769853451492  validation loss:0.1626674716490928\nepoch 28  train loss: 0.2069555321596769  validation loss:0.1625902334465412\nepoch 29  train loss: 0.20693971513143442  validation loss:0.16253166682262615\nepoch 30  train loss: 0.20692797784563238  validation loss:0.16248724307596893\nepoch 31  train loss: 0.20691922313721114  validation loss:0.16245353825424316\nepoch 32  train loss: 0.20691266671343603  validation loss:0.16242796099590465\nepoch 33  train loss: 0.20690774118458843  validation loss:0.16240854853075928\nepoch 34  train loss: 0.20690403194379034  validation loss:0.1623938133098774\nepoch 35  train loss: 0.20690123352499354  validation loss:0.16238262743084073\nepoch 36  train loss: 0.20689911937393268  validation loss:0.16237413538789894\nepoch 37  train loss: 0.20689752057194577  validation loss:0.16236768811431693\nepoch 38  train loss: 0.20689631064763167  validation loss:0.16236279306812132\nepoch 39  train loss: 0.20689539460160966  validation loss:0.16235907642835862\nepoch 40  train loss: 0.20689470089426162  validation loss:0.1623562544458757\nepoch 41  train loss: 0.20689417554719816  validation loss:0.16235411172156328\nepoch 42  train loss: 0.2068937777712265  validation loss:0.16235248473108474\nepoch 43  train loss: 0.2068934767081757  validation loss:0.16235124932549433\nepoch 44  train loss: 0.20689324899240516  validation loss:0.16235031124629945\nepoch 45  train loss: 0.20689307691967554  validation loss:0.16234959892684703\nepoch 46  train loss: 0.20689294706853203  validation loss:0.16234905802826632\nepoch 47  train loss: 0.20689284926029183  validation loss:0.16234864729163692\nepoch 48  train loss: 0.20689277577325807  validation loss:0.16234833538910237\nepoch 49  train loss: 0.2068927207483142  validation loss:0.16234809853322169\nepoch 50  train loss: 0.2068926797388777  validation loss:0.1623479186619062\nepoch 51  train loss: 0.20689264936992205  validation loss:0.16234778206031947\nepoch 52  train loss: 0.20689262707949335  validation loss:0.16234767831452002\nepoch 53  train loss: 0.20689261092268793  validation loss:0.16234759951697497\nepoch 54  train loss: 0.20689259942295002  validation loss:0.16234753966330764\nepoch 55  train loss: 0.20689259145924097  validation loss:0.16234749419424474\nepoch 56  train loss: 0.20689258618041176  validation loss:0.1623474596478112\nepoch 57  train loss: 0.20689258294021193  validation loss:0.16234743339523675\nepoch 58  train loss: 0.2068925812479525  validation loss:0.16234741344042408\nepoch 59  train loss: 0.20689258073105704  validation loss:0.16234739826768113\nepoch 60  train loss: 0.2068925811066269  validation loss:0.16234738672610025\nepoch 61  train loss: 0.2068925821598541  validation loss:0.16234737794176474\nepoch 62  train loss: 0.20689258372763225  validation loss:0.16234737125108487\nepoch 63  train loss: 0.20689258568611155  validation loss:0.16234736615017778\nepoch 64  train loss: 0.20689258794125073  validation loss:0.16234736225643098\nepoch 65  train loss: 0.20689259042164163  validation loss:0.16234735927931584\nepoch 66  train loss: 0.20689259307306265  validation loss:0.16234735699822567\nepoch 67  train loss: 0.20689259585434172  validation loss:0.1623473552456473\nepoch 68  train loss: 0.20689259873421598  validation loss:0.16234735389438346\nepoch 69  train loss: 0.20689260168894677  validation loss:0.16234735284784937\nepoch 70  train loss: 0.20689260470050858  validation loss:0.1623473520327056\nepoch 71  train loss: 0.206892607755214  validation loss:0.16234735139326334\nepoch 72  train loss: 0.20689261084267005  validation loss:0.1623473508872366\nepoch 73  train loss: 0.20689261395498496  validation loss:0.16234735048251633\nepoch 74  train loss: 0.20689261708616613  validation loss:0.16234735015472118\nepoch 75  train loss: 0.2068926202316635  validation loss:0.16234734988533786\nepoch 76  train loss: 0.20689262338802183  validation loss:0.16234734966030853\nepoch 77  train loss: 0.20689262655261773  validation loss:0.16234734946895885\nepoch 78  train loss: 0.20689262972345868  validation loss:0.16234734930318329\nepoch 79  train loss: 0.20689263289903237  validation loss:0.16234734915682722\nepoch 80  train loss: 0.20689263607818997  validation loss:0.16234734902521733\nepoch 81  train loss: 0.20689263926005935  validation loss:0.1623473489048049\nepoch 82  train loss: 0.20689264244397826  validation loss:0.1623473487928953\nepoch 83  train loss: 0.20689264562944365  validation loss:0.16234734868744255\nepoch 84  train loss: 0.2068926488160738  validation loss:0.1623473485868929\nepoch 85  train loss: 0.20689265200357876  validation loss:0.16234734849006666\nepoch 86  train loss: 0.20689265519173844  validation loss:0.16234734839606804\nepoch 87  train loss: 0.20689265838038542  validation loss:0.1623473483042169\nepoch 88  train loss: 0.2068926615693929  validation loss:0.16234734821399663\nepoch 89  train loss: 0.20689266475866455  validation loss:0.1623473481250151\nepoch 90  train loss: 0.20689266794812716  validation loss:0.1623473480369745\nepoch 91  train loss: 0.206892671137725  validation loss:0.16234734794964867\nepoch 92  train loss: 0.20689267432741598  validation loss:0.16234734786286606\nepoch 93  train loss: 0.20689267751716803  validation loss:0.16234734777649606\nepoch 94  train loss: 0.20689268070695693  validation loss:0.16234734769043982\nepoch 95  train loss: 0.20689268389676413  validation loss:0.16234734760462205\nepoch 96  train loss: 0.20689268708657535  validation loss:0.1623473475189858\nepoch 97  train loss: 0.20689269027638038  validation loss:0.16234734743348764\nepoch 98  train loss: 0.2068926934661708  validation loss:0.16234734734809472\nepoch 99  train loss: 0.20689269665594076  validation loss:0.162347347262782\nepoch 100  train loss: 0.20689269984568517  validation loss:0.16234734717753047\nepoch 101  train loss: 0.2068927030354009  validation loss:0.1623473470923258\nepoch 102  train loss: 0.2068927062250851  validation loss:0.16234734700715703\nepoch 103  train loss: 0.20689270941473564  validation loss:0.16234734692201575\nepoch 104  train loss: 0.20689271260435113  validation loss:0.1623473468368958\nepoch 105  train loss: 0.20689271579393034  validation loss:0.16234734675179233\nepoch 106  train loss: 0.2068927189834724  validation loss:0.16234734666670173\nepoch 107  train loss: 0.2068927221729765  validation loss:0.16234734658162117\nepoch 108  train loss: 0.20689272536244224  validation loss:0.16234734649654856\nepoch 109  train loss: 0.20689272855186924  validation loss:0.16234734641148235\nepoch 110  train loss: 0.20689273174125714  validation loss:0.1623473463264214\nepoch 111  train loss: 0.2068927349306057  validation loss:0.16234734624136465\nepoch 112  train loss: 0.206892738119915  validation loss:0.16234734615631144\nepoch 113  train loss: 0.2068927413091844  validation loss:0.16234734607126125\nepoch 114  train loss: 0.20689274449841433  validation loss:0.16234734598621375\nepoch 115  train loss: 0.20689274768760454  validation loss:0.1623473459011685\nepoch 116  train loss: 0.20689275087675468  validation loss:0.16234734581612545\nepoch 117  train loss: 0.20689275406586516  validation loss:0.16234734573108428\nepoch 118  train loss: 0.20689275725493575  validation loss:0.16234734564604483\nepoch 119  train loss: 0.20689276044396632  validation loss:0.16234734556100713\nepoch 120  train loss: 0.20689276363295708  validation loss:0.16234734547597102\nepoch 121  train loss: 0.2068927668219077  validation loss:0.1623473453909364\nepoch 122  train loss: 0.20689277001081854  validation loss:0.16234734530590336\nepoch 123  train loss: 0.20689277319968924  validation loss:0.16234734522087177\nepoch 124  train loss: 0.20689277638851994  validation loss:0.16234734513584156\nepoch 125  train loss: 0.20689277957731084  validation loss:0.16234734505081286\nepoch 126  train loss: 0.20689278276606154  validation loss:0.1623473449657856\nepoch 127  train loss: 0.20689278595477262  validation loss:0.1623473448807596\nepoch 128  train loss: 0.20689278914344342  validation loss:0.1623473447957351\nepoch 129  train loss: 0.20689279233207425  validation loss:0.16234734471071194\nepoch 130  train loss: 0.20689279552066514  validation loss:0.16234734462569012\nepoch 131  train loss: 0.20689279870921615  validation loss:0.16234734454066974\nepoch 132  train loss: 0.20689280189772705  validation loss:0.16234734445565074\nepoch 133  train loss: 0.2068928050861979  validation loss:0.16234734437063306\nepoch 134  train loss: 0.2068928082746289  validation loss:0.16234734428561678\nepoch 135  train loss: 0.20689281146301983  validation loss:0.16234734420060185\nepoch 136  train loss: 0.2068928146513708  validation loss:0.1623473441155883\nepoch 137  train loss: 0.20689281783968186  validation loss:0.16234734403057607\nepoch 138  train loss: 0.20689282102795284  validation loss:0.16234734394556521\nepoch 139  train loss: 0.2068928242161837  validation loss:0.16234734386055577\nepoch 140  train loss: 0.20689282740437484  validation loss:0.16234734377554771\nepoch 141  train loss: 0.20689283059252594  validation loss:0.16234734369054096\nepoch 142  train loss: 0.206892833780637  validation loss:0.16234734360553557\nepoch 143  train loss: 0.20689283696870805  validation loss:0.16234734352053157\nepoch 144  train loss: 0.20689284015673914  validation loss:0.1623473434355289\nepoch 145  train loss: 0.20689284334473024  validation loss:0.16234734335052764\nepoch 146  train loss: 0.20689284653268142  validation loss:0.16234734326552766\nepoch 147  train loss: 0.2068928497205926  validation loss:0.16234734318052907\nepoch 148  train loss: 0.20689285290846385  validation loss:0.1623473430955319\nepoch 149  train loss: 0.20689285609629499  validation loss:0.16234734301053602\nepoch 150  train loss: 0.20689285928408632  validation loss:0.16234734292554157\nepoch 151  train loss: 0.2068928624718375  validation loss:0.16234734284054847\nepoch 152  train loss: 0.20689286565954873  validation loss:0.16234734275555668\nepoch 153  train loss: 0.20689286884722013  validation loss:0.1623473426705663\nepoch 154  train loss: 0.2068928720348514  validation loss:0.16234734258557731\nepoch 155  train loss: 0.20689287522244287  validation loss:0.16234734250058963\nepoch 156  train loss: 0.20689287840999418  validation loss:0.1623473424156033\nepoch 157  train loss: 0.2068928815975056  validation loss:0.16234734233061837\nepoch 158  train loss: 0.20689288478497703  validation loss:0.1623473422456347\nepoch 159  train loss: 0.2068928879724086  validation loss:0.16234734216065252\nepoch 160  train loss: 0.2068928911598001  validation loss:0.16234734207567164\nepoch 161  train loss: 0.2068928943471516  validation loss:0.16234734199069215\nepoch 162  train loss: 0.2068928975344631  validation loss:0.16234734190571404\n","name":"stdout"},{"output_type":"stream","text":"epoch 163  train loss: 0.20689290072173472  validation loss:0.16234734182073726\nepoch 164  train loss: 0.20689290390896645  validation loss:0.1623473417357618\nepoch 165  train loss: 0.206892907096158  validation loss:0.16234734165078776\nepoch 166  train loss: 0.20689291028330975  validation loss:0.16234734156581507\nepoch 167  train loss: 0.20689291347042155  validation loss:0.16234734148084376\nepoch 168  train loss: 0.20689291665749338  validation loss:0.16234734139587378\nepoch 169  train loss: 0.2068929198445251  validation loss:0.1623473413109052\nepoch 170  train loss: 0.206892923031517  validation loss:0.1623473412259379\nepoch 171  train loss: 0.20689292621846875  validation loss:0.1623473411409721\nepoch 172  train loss: 0.2068929294053808  validation loss:0.16234734105600757\nepoch 173  train loss: 0.20689293259225283  validation loss:0.16234734097104436\nepoch 174  train loss: 0.20689293577908477  validation loss:0.1623473408860826\nepoch 175  train loss: 0.2068929389658768  validation loss:0.16234734080112215\nepoch 176  train loss: 0.20689294215262888  validation loss:0.16234734071616305\nepoch 177  train loss: 0.20689294533934108  validation loss:0.16234734063120537\nepoch 178  train loss: 0.2068929485260132  validation loss:0.16234734054624902\nepoch 179  train loss: 0.2068929517126454  validation loss:0.162347340461294\nepoch 180  train loss: 0.2068929548992377  validation loss:0.1623473403763404\nepoch 181  train loss: 0.20689295808579009  validation loss:0.1623473402913881\nepoch 182  train loss: 0.20689296127230236  validation loss:0.16234734020643718\nepoch 183  train loss: 0.20689296445877467  validation loss:0.16234734012148763\nepoch 184  train loss: 0.2068929676452072  validation loss:0.16234734003653947\nepoch 185  train loss: 0.20689297083159974  validation loss:0.16234733995159267\nepoch 186  train loss: 0.20689297401795226  validation loss:0.16234733986664718\nepoch 187  train loss: 0.20689297720426486  validation loss:0.16234733978170313\nepoch 188  train loss: 0.2068929803905375  validation loss:0.16234733969676035\nepoch 189  train loss: 0.2068929835767703  validation loss:0.16234733961181896\nepoch 190  train loss: 0.20689298676296305  validation loss:0.16234733952687896\nepoch 191  train loss: 0.20689298994911592  validation loss:0.1623473394419403\nepoch 192  train loss: 0.20689299313522885  validation loss:0.16234733935700296\nepoch 193  train loss: 0.20689299632130165  validation loss:0.16234733927206707\nepoch 194  train loss: 0.20689299950733459  validation loss:0.1623473391871325\nepoch 195  train loss: 0.20689300269332758  validation loss:0.16234733910219934\nepoch 196  train loss: 0.20689300587928075  validation loss:0.1623473390172675\nepoch 197  train loss: 0.20689300906519392  validation loss:0.16234733893233688\nepoch 198  train loss: 0.20689301225106713  validation loss:0.16234733884740782\nepoch 199  train loss: 0.20689301543690036  validation loss:0.16234733876248003\nepoch 200  train loss: 0.2068930186226938  validation loss:0.16234733867755358\nepoch 201  train loss: 0.20689302180844701  validation loss:0.16234733859262857\nepoch 202  train loss: 0.2068930249941606  validation loss:0.16234733850770486\nepoch 203  train loss: 0.20689302817983407  validation loss:0.16234733842278254\nepoch 204  train loss: 0.20689303136546752  validation loss:0.16234733833786158\nepoch 205  train loss: 0.2068930345510612  validation loss:0.16234733825294198\nepoch 206  train loss: 0.20689303773661488  validation loss:0.16234733816802377\nepoch 207  train loss: 0.20689304092212862  validation loss:0.16234733808310686\nepoch 208  train loss: 0.20689304410760245  validation loss:0.16234733799819137\nepoch 209  train loss: 0.20689304729303643  validation loss:0.16234733791327718\nepoch 210  train loss: 0.20689305047843032  validation loss:0.16234733782836439\nepoch 211  train loss: 0.20689305366378438  validation loss:0.16234733774345295\nepoch 212  train loss: 0.20689305684909842  validation loss:0.16234733765854287\nepoch 213  train loss: 0.2068930600343726  validation loss:0.16234733757363412\nepoch 214  train loss: 0.20689306321960677  validation loss:0.1623473374887268\nepoch 215  train loss: 0.20689306640480112  validation loss:0.1623473374038208\nepoch 216  train loss: 0.20689306958995546  validation loss:0.1623473373189162\nepoch 217  train loss: 0.20689307277506996  validation loss:0.1623473372340129\nepoch 218  train loss: 0.20689307596014458  validation loss:0.16234733714911107\nepoch 219  train loss: 0.206893079145179  validation loss:0.16234733706421045\nepoch 220  train loss: 0.20689308233017373  validation loss:0.16234733697931128\nepoch 221  train loss: 0.20689308551512853  validation loss:0.1623473368944134\nepoch 222  train loss: 0.20689308870004322  validation loss:0.16234733680951693\nepoch 223  train loss: 0.2068930918849181  validation loss:0.16234733672462182\nepoch 224  train loss: 0.20689309506975323  validation loss:0.16234733663972808\nepoch 225  train loss: 0.20689309825454827  validation loss:0.1623473365548357\nepoch 226  train loss: 0.20689310143930337  validation loss:0.16234733646994468\nepoch 227  train loss: 0.20689310462401847  validation loss:0.16234733638505502\nepoch 228  train loss: 0.2068931078086939  validation loss:0.1623473363001667\nepoch 229  train loss: 0.2068931109933291  validation loss:0.1623473362152798\nepoch 230  train loss: 0.20689311417792458  validation loss:0.16234733613039415\nepoch 231  train loss: 0.20689311736248017  validation loss:0.16234733604550994\nepoch 232  train loss: 0.20689312054699574  validation loss:0.16234733596062706\nepoch 233  train loss: 0.2068931237314715  validation loss:0.1623473358757456\nepoch 234  train loss: 0.20689312691590722  validation loss:0.16234733579086547\nepoch 235  train loss: 0.20689313010030302  validation loss:0.16234733570598664\nepoch 236  train loss: 0.20689313328465908  validation loss:0.16234733562110923\nepoch 237  train loss: 0.206893136468975  validation loss:0.16234733553623318\nepoch 238  train loss: 0.20689313965325115  validation loss:0.1623473354513585\nepoch 239  train loss: 0.20689314283748747  validation loss:0.1623473353664851\nepoch 240  train loss: 0.20689314602168385  validation loss:0.1623473352816131\nepoch 241  train loss: 0.2068931492058402  validation loss:0.16234733519674246\nepoch 242  train loss: 0.2068931523899567  validation loss:0.16234733511187321\nepoch 243  train loss: 0.20689315557403318  validation loss:0.16234733502700532\nepoch 244  train loss: 0.2068931587580698  validation loss:0.1623473349421388\nepoch 245  train loss: 0.20689316194206653  validation loss:0.16234733485727362\nepoch 246  train loss: 0.20689316512602346  validation loss:0.16234733477240976\nepoch 247  train loss: 0.20689316830994042  validation loss:0.16234733468754733\nepoch 248  train loss: 0.20689317149381733  validation loss:0.16234733460268622\nepoch 249  train loss: 0.20689317467765453  validation loss:0.16234733451782646\nepoch 250  train loss: 0.2068931778614518  validation loss:0.16234733443296814\nepoch 251  train loss: 0.2068931810452091  validation loss:0.16234733434811105\nepoch 252  train loss: 0.2068931842289264  validation loss:0.1623473342632554\nepoch 253  train loss: 0.20689318741260398  validation loss:0.16234733417840108\nepoch 254  train loss: 0.20689319059624156  validation loss:0.16234733409354815\nepoch 255  train loss: 0.2068931937798393  validation loss:0.16234733400869655\nepoch 256  train loss: 0.20689319696339714  validation loss:0.16234733392384637\nepoch 257  train loss: 0.20689320014691498  validation loss:0.16234733383899752\nepoch 258  train loss: 0.20689320333039304  validation loss:0.16234733375414998\nepoch 259  train loss: 0.2068932065138312  validation loss:0.16234733366930382\nepoch 260  train loss: 0.20689320969722938  validation loss:0.162347333584459\nepoch 261  train loss: 0.20689321288058773  validation loss:0.1623473334996156\nepoch 262  train loss: 0.20689321606390618  validation loss:0.16234733341477353\nepoch 263  train loss: 0.20689321924718468  validation loss:0.1623473333299328\nepoch 264  train loss: 0.2068932224304232  validation loss:0.1623473332450935\nepoch 265  train loss: 0.20689322561362203  validation loss:0.1623473331602555\nepoch 266  train loss: 0.206893228796781  validation loss:0.1623473330754189\nepoch 267  train loss: 0.2068932319798999  validation loss:0.1623473329905836\nepoch 268  train loss: 0.20689323516297903  validation loss:0.1623473329057497\nepoch 269  train loss: 0.20689323834601814  validation loss:0.16234733282091718\nepoch 270  train loss: 0.20689324152901756  validation loss:0.162347332736086\nepoch 271  train loss: 0.2068932447119769  validation loss:0.16234733265125617\nepoch 272  train loss: 0.2068932478948963  validation loss:0.1623473325664277\nepoch 273  train loss: 0.20689325107777598  validation loss:0.16234733248160055\nepoch 274  train loss: 0.20689325426061567  validation loss:0.16234733239677482\nepoch 275  train loss: 0.2068932574434156  validation loss:0.16234733231195037\nepoch 276  train loss: 0.20689326062617558  validation loss:0.1623473322271274\nepoch 277  train loss: 0.20689326380889564  validation loss:0.16234733214230573\nepoch 278  train loss: 0.20689326699157576  validation loss:0.1623473320574854\nepoch 279  train loss: 0.2068932701742162  validation loss:0.16234733197266646\nepoch 280  train loss: 0.20689327335681665  validation loss:0.16234733188784886\nepoch 281  train loss: 0.2068932765393771  validation loss:0.16234733180303262\nepoch 282  train loss: 0.20689327972189775  validation loss:0.16234733171821775\nepoch 283  train loss: 0.20689328290437856  validation loss:0.16234733163340428\nepoch 284  train loss: 0.2068932860868195  validation loss:0.1623473315485921\nepoch 285  train loss: 0.2068932892692206  validation loss:0.1623473314637813\nepoch 286  train loss: 0.20689329245158167  validation loss:0.16234733137897187\nepoch 287  train loss: 0.2068932956339029  validation loss:0.1623473312941638\nepoch 288  train loss: 0.20689329881618437  validation loss:0.16234733120935704\nepoch 289  train loss: 0.20689330199842593  validation loss:0.16234733112455169\nepoch 290  train loss: 0.20689330518062757  validation loss:0.16234733103974766\nepoch 291  train loss: 0.2068933083627893  validation loss:0.16234733095494502\nepoch 292  train loss: 0.20689331154491128  validation loss:0.16234733087014377\nepoch 293  train loss: 0.2068933147269933  validation loss:0.16234733078534383\nepoch 294  train loss: 0.2068933179090353  validation loss:0.1623473307005453\nepoch 295  train loss: 0.2068933210910377  validation loss:0.16234733061574805\nepoch 296  train loss: 0.2068933242730001  validation loss:0.16234733053095216\nepoch 297  train loss: 0.20689332745492256  validation loss:0.1623473304461577\nepoch 298  train loss: 0.20689333063680534  validation loss:0.16234733036136456\nepoch 299  train loss: 0.20689333381864813  validation loss:0.1623473302765728\nepoch 300  train loss: 0.20689333700045104  validation loss:0.16234733019178238\nepoch 301  train loss: 0.2068933401822141  validation loss:0.16234733010699332\nepoch 302  train loss: 0.2068933433639374  validation loss:0.1623473300222056\nepoch 303  train loss: 0.20689334654562075  validation loss:0.16234732993741932\nepoch 304  train loss: 0.2068933497272642  validation loss:0.1623473298526344\nepoch 305  train loss: 0.20689335290886776  validation loss:0.16234732976785074\nepoch 306  train loss: 0.20689335609043147  validation loss:0.16234732968306848\nepoch 307  train loss: 0.20689335927195537  validation loss:0.16234732959828754\nepoch 308  train loss: 0.20689336245343945  validation loss:0.16234732951350803\nepoch 309  train loss: 0.20689336563488347  validation loss:0.16234732942872984\nepoch 310  train loss: 0.2068933688162879  validation loss:0.16234732934395305\nepoch 311  train loss: 0.20689337199765231  validation loss:0.16234732925917758\nepoch 312  train loss: 0.20689337517897693  validation loss:0.16234732917440345\nepoch 313  train loss: 0.20689337836026153  validation loss:0.16234732908963076\nepoch 314  train loss: 0.20689338154150647  validation loss:0.16234732900485932\nepoch 315  train loss: 0.20689338472271132  validation loss:0.16234732892008932\nepoch 316  train loss: 0.20689338790387665  validation loss:0.1623473288353206\nepoch 317  train loss: 0.20689339108500196  validation loss:0.16234732875055333\nepoch 318  train loss: 0.20689339426608738  validation loss:0.16234732866578738\nepoch 319  train loss: 0.20689339744713303  validation loss:0.16234732858102274\n","name":"stdout"},{"output_type":"stream","text":"epoch 320  train loss: 0.20689340062813863  validation loss:0.1623473284962595\nepoch 321  train loss: 0.20689340380910456  validation loss:0.1623473284114976\nepoch 322  train loss: 0.20689340699003056  validation loss:0.16234732832673712\nepoch 323  train loss: 0.20689341017091673  validation loss:0.16234732824197795\nepoch 324  train loss: 0.20689341335176312  validation loss:0.16234732815722017\nepoch 325  train loss: 0.2068934165325696  validation loss:0.16234732807246371\nepoch 326  train loss: 0.2068934197133361  validation loss:0.1623473279877086\nepoch 327  train loss: 0.20689342289406293  validation loss:0.1623473279029549\nepoch 328  train loss: 0.2068934260747499  validation loss:0.16234732781820252\nepoch 329  train loss: 0.20689342925539694  validation loss:0.16234732773345148\nepoch 330  train loss: 0.20689343243600414  validation loss:0.16234732764870186\nepoch 331  train loss: 0.20689343561657159  validation loss:0.16234732756395356\nepoch 332  train loss: 0.20689343879709904  validation loss:0.16234732747920663\nepoch 333  train loss: 0.20689344197758683  validation loss:0.1623473273944611\nepoch 334  train loss: 0.20689344515803473  validation loss:0.16234732730971682\nepoch 335  train loss: 0.20689344833844275  validation loss:0.16234732722497397\nepoch 336  train loss: 0.2068934515188109  validation loss:0.16234732714023248\nepoch 337  train loss: 0.20689345469913933  validation loss:0.16234732705549226\nepoch 338  train loss: 0.2068934578794277  validation loss:0.16234732697075352\nepoch 339  train loss: 0.20689346105967638  validation loss:0.1623473268860161\nepoch 340  train loss: 0.20689346423988517  validation loss:0.16234732680128\nepoch 341  train loss: 0.20689346742005413  validation loss:0.16234732671654525\nepoch 342  train loss: 0.20689347060018334  validation loss:0.16234732663181195\nepoch 343  train loss: 0.20689347378027262  validation loss:0.1623473265470799\nepoch 344  train loss: 0.2068934769603221  validation loss:0.16234732646234926\nepoch 345  train loss: 0.20689348014033165  validation loss:0.16234732637762003\nepoch 346  train loss: 0.2068934833203015  validation loss:0.16234732629289209\nepoch 347  train loss: 0.2068934865002314  validation loss:0.1623473262081655\nepoch 348  train loss: 0.20689348968012158  validation loss:0.16234732612344033\nepoch 349  train loss: 0.20689349285997183  validation loss:0.16234732603871646\nepoch 350  train loss: 0.20689349603978233  validation loss:0.16234732595399398\nepoch 351  train loss: 0.2068934992195529  validation loss:0.16234732586927283\nepoch 352  train loss: 0.20689350239928378  validation loss:0.16234732578455308\nepoch 353  train loss: 0.2068935055789747  validation loss:0.16234732569983462\nepoch 354  train loss: 0.20689350875862586  validation loss:0.16234732561511755\nepoch 355  train loss: 0.2068935119382372  validation loss:0.1623473255304019\nepoch 356  train loss: 0.20689351511780857  validation loss:0.1623473254456875\nepoch 357  train loss: 0.2068935182973403  validation loss:0.1623473253609745\nepoch 358  train loss: 0.20689352147683204  validation loss:0.1623473252762629\nepoch 359  train loss: 0.20689352465628413  validation loss:0.16234732519155257\nepoch 360  train loss: 0.20689352783569623  validation loss:0.16234732510684372\nepoch 361  train loss: 0.2068935310150687  validation loss:0.16234732502213617\nepoch 362  train loss: 0.20689353419440124  validation loss:0.16234732493742995\nepoch 363  train loss: 0.20689353737369393  validation loss:0.16234732485272516\nepoch 364  train loss: 0.20689354055294681  validation loss:0.16234732476802163\nepoch 365  train loss: 0.2068935437321599  validation loss:0.1623473246833195\nepoch 366  train loss: 0.20689354691133316  validation loss:0.1623473245986187\nepoch 367  train loss: 0.20689355009046656  validation loss:0.16234732451391934\nepoch 368  train loss: 0.2068935532695602  validation loss:0.16234732442922126\nepoch 369  train loss: 0.2068935564486141  validation loss:0.16234732434452454\nepoch 370  train loss: 0.20689355962762795  validation loss:0.16234732425982923\nepoch 371  train loss: 0.20689356280660204  validation loss:0.16234732417513528\nepoch 372  train loss: 0.2068935659855365  validation loss:0.16234732409044264\nepoch 373  train loss: 0.20689356916443105  validation loss:0.16234732400575144\nepoch 374  train loss: 0.20689357234328576  validation loss:0.16234732392106146\nepoch 375  train loss: 0.2068935755221006  validation loss:0.16234732383637293\nepoch 376  train loss: 0.20689357870087574  validation loss:0.16234732375168573\nepoch 377  train loss: 0.20689358187961107  validation loss:0.1623473236669999\nepoch 378  train loss: 0.20689358505830646  validation loss:0.16234732358231543\nepoch 379  train loss: 0.20689358823696213  validation loss:0.1623473234976323\nepoch 380  train loss: 0.20689359141557812  validation loss:0.16234732341295055\nepoch 381  train loss: 0.20689359459415407  validation loss:0.16234732332827015\nepoch 382  train loss: 0.20689359777269026  validation loss:0.16234732324359108\nepoch 383  train loss: 0.20689360095118667  validation loss:0.1623473231589134\nepoch 384  train loss: 0.20689360412964328  validation loss:0.16234732307423705\nepoch 385  train loss: 0.20689360730806  validation loss:0.16234732298956212\nepoch 386  train loss: 0.206893610486437  validation loss:0.16234732290488849\nepoch 387  train loss: 0.20689361366477424  validation loss:0.16234732282021624\nepoch 388  train loss: 0.20689361684307167  validation loss:0.16234732273554528\nepoch 389  train loss: 0.20689362002132913  validation loss:0.16234732265087576\nepoch 390  train loss: 0.20689362319954707  validation loss:0.1623473225662076\nepoch 391  train loss: 0.20689362637772502  validation loss:0.16234732248154074\nepoch 392  train loss: 0.20689362955586313  validation loss:0.16234732239687524\nepoch 393  train loss: 0.20689363273396147  validation loss:0.1623473223122111\nepoch 394  train loss: 0.20689363591202017  validation loss:0.16234732222754839\nepoch 395  train loss: 0.20689363909003888  validation loss:0.16234732214288697\nepoch 396  train loss: 0.20689364226801793  validation loss:0.16234732205822688\nepoch 397  train loss: 0.20689364544595712  validation loss:0.16234732197356816\nepoch 398  train loss: 0.20689364862385645  validation loss:0.16234732188891088\nepoch 399  train loss: 0.20689365180171598  validation loss:0.16234732180425487\nepoch 400  train loss: 0.20689365497953582  validation loss:0.16234732171960026\nepoch 401  train loss: 0.20689365815731586  validation loss:0.162347321634947\nepoch 402  train loss: 0.206893661335056  validation loss:0.1623473215502951\nepoch 403  train loss: 0.2068936645127564  validation loss:0.16234732146564454\nepoch 404  train loss: 0.20689366769041706  validation loss:0.1623473213809953\nepoch 405  train loss: 0.20689367086803792  validation loss:0.1623473212963475\nepoch 406  train loss: 0.20689367404561904  validation loss:0.162347321211701\nepoch 407  train loss: 0.2068936772231601  validation loss:0.1623473211270559\nepoch 408  train loss: 0.2068936804006617  validation loss:0.16234732104241215\nepoch 409  train loss: 0.20689368357812332  validation loss:0.16234732095776974\nepoch 410  train loss: 0.20689368675554512  validation loss:0.16234732087312867\nepoch 411  train loss: 0.20689368993292728  validation loss:0.162347320788489\nepoch 412  train loss: 0.20689369311026953  validation loss:0.16234732070385063\nepoch 413  train loss: 0.20689369628757215  validation loss:0.16234732061921361\nepoch 414  train loss: 0.20689369946483477  validation loss:0.16234732053457804\nepoch 415  train loss: 0.2068937026420577  validation loss:0.16234732044994377\nepoch 416  train loss: 0.20689370581924094  validation loss:0.16234732036531085\nepoch 417  train loss: 0.20689370899638446  validation loss:0.16234732028067927\nepoch 418  train loss: 0.20689371217348787  validation loss:0.1623473201960491\nepoch 419  train loss: 0.20689371535055168  validation loss:0.16234732011142022\nepoch 420  train loss: 0.2068937185275758  validation loss:0.1623473200267928\nepoch 421  train loss: 0.20689372170456002  validation loss:0.1623473199421666\nepoch 422  train loss: 0.20689372488150437  validation loss:0.16234731985754183\nepoch 423  train loss: 0.2068937280584092  validation loss:0.16234731977291844\nepoch 424  train loss: 0.20689373123527416  validation loss:0.16234731968829635\nepoch 425  train loss: 0.20689373441209932  validation loss:0.16234731960367565\nepoch 426  train loss: 0.20689373758888469  validation loss:0.16234731951905626\nepoch 427  train loss: 0.2068937407656302  validation loss:0.1623473194344383\nepoch 428  train loss: 0.20689374394233606  validation loss:0.1623473193498217\nepoch 429  train loss: 0.20689374711900205  validation loss:0.1623473192652064\nepoch 430  train loss: 0.20689375029562826  validation loss:0.16234731918059245\nepoch 431  train loss: 0.20689375347221478  validation loss:0.1623473190959799\nepoch 432  train loss: 0.2068937566487615  validation loss:0.1623473190113687\nepoch 433  train loss: 0.20689375982526842  validation loss:0.1623473189267588\nepoch 434  train loss: 0.2068937630017357  validation loss:0.1623473188421503\nepoch 435  train loss: 0.20689376617816302  validation loss:0.16234731875754316\nepoch 436  train loss: 0.20689376935455067  validation loss:0.1623473186729374\nepoch 437  train loss: 0.20689377253089852  validation loss:0.16234731858833298\nepoch 438  train loss: 0.20689377570720663  validation loss:0.1623473185037299\nepoch 439  train loss: 0.20689377888347485  validation loss:0.16234731841912817\nepoch 440  train loss: 0.20689378205970357  validation loss:0.16234731833452778\nepoch 441  train loss: 0.20689378523589227  validation loss:0.16234731824992882\nepoch 442  train loss: 0.20689378841204129  validation loss:0.16234731816533116\nepoch 443  train loss: 0.20689379158815063  validation loss:0.16234731808073488\nepoch 444  train loss: 0.20689379476422004  validation loss:0.1623473179961399\nepoch 445  train loss: 0.20689379794024987  validation loss:0.16234731791154633\nepoch 446  train loss: 0.20689380111623984  validation loss:0.16234731782695405\nepoch 447  train loss: 0.20689380429218998  validation loss:0.16234731774236327\nepoch 448  train loss: 0.20689380746810043  validation loss:0.16234731765777372\nepoch 449  train loss: 0.20689381064397122  validation loss:0.16234731757318552\nepoch 450  train loss: 0.2068938138198021  validation loss:0.1623473174885987\nepoch 451  train loss: 0.20689381699559323  validation loss:0.16234731740401329\nepoch 452  train loss: 0.2068938201713447  validation loss:0.16234731731942917\nepoch 453  train loss: 0.2068938233470563  validation loss:0.16234731723484647\nepoch 454  train loss: 0.20689382652272828  validation loss:0.16234731715026504\nepoch 455  train loss: 0.20689382969836034  validation loss:0.162347317065685\nepoch 456  train loss: 0.20689383287395277  validation loss:0.16234731698110635\nepoch 457  train loss: 0.20689383604950548  validation loss:0.16234731689652904\nepoch 458  train loss: 0.20689383922501825  validation loss:0.16234731681195308\nepoch 459  train loss: 0.20689384240049144  validation loss:0.1623473167273785\nepoch 460  train loss: 0.20689384557592477  validation loss:0.16234731664280524\nepoch 461  train loss: 0.20689384875131842  validation loss:0.16234731655823337\nepoch 462  train loss: 0.20689385192667234  validation loss:0.16234731647366282\nepoch 463  train loss: 0.20689385510198638  validation loss:0.16234731638909364\nepoch 464  train loss: 0.20689385827726076  validation loss:0.16234731630452578\nepoch 465  train loss: 0.20689386145249544  validation loss:0.16234731621995935\nepoch 466  train loss: 0.2068938646276902  validation loss:0.16234731613539424\nepoch 467  train loss: 0.2068938678028455  validation loss:0.16234731605083044\nepoch 468  train loss: 0.20689387097796083  validation loss:0.16234731596626806\nepoch 469  train loss: 0.20689387415303645  validation loss:0.16234731588170703\nepoch 470  train loss: 0.20689387732807232  validation loss:0.16234731579714737\nepoch 471  train loss: 0.20689388050306842  validation loss:0.162347315712589\nepoch 472  train loss: 0.20689388367802491  validation loss:0.16234731562803204\nepoch 473  train loss: 0.2068938868529416  validation loss:0.1623473155434764\nepoch 474  train loss: 0.2068938900278185  validation loss:0.16234731545892214\nepoch 475  train loss: 0.2068938932026557  validation loss:0.16234731537436922\nepoch 476  train loss: 0.2068938963774531  validation loss:0.16234731528981763\nepoch 477  train loss: 0.20689389955221088  validation loss:0.16234731520526743\nepoch 478  train loss: 0.20689390272692876  validation loss:0.16234731512071857\nepoch 479  train loss: 0.20689390590160694  validation loss:0.16234731503617106\nepoch 480  train loss: 0.2068939090762455  validation loss:0.16234731495162494\nepoch 481  train loss: 0.20689391225084425  validation loss:0.16234731486708018\nepoch 482  train loss: 0.20689391542540325  validation loss:0.16234731478253675\nepoch 483  train loss: 0.20689391859992246  validation loss:0.16234731469799465\nepoch 484  train loss: 0.20689392177440202  validation loss:0.16234731461345397\nepoch 485  train loss: 0.2068939249488419  validation loss:0.16234731452891463\n","name":"stdout"},{"output_type":"stream","text":"epoch 486  train loss: 0.206893928123242  validation loss:0.16234731444437658\nepoch 487  train loss: 0.2068939312976022  validation loss:0.16234731435983996\nepoch 488  train loss: 0.20689393447192295  validation loss:0.16234731427530466\nepoch 489  train loss: 0.20689393764620373  validation loss:0.1623473141907707\nepoch 490  train loss: 0.20689394082044488  validation loss:0.16234731410623812\nepoch 491  train loss: 0.20689394399464633  validation loss:0.16234731402170688\nepoch 492  train loss: 0.20689394716880802  validation loss:0.16234731393717702\nepoch 493  train loss: 0.2068939503429299  validation loss:0.1623473138526485\nepoch 494  train loss: 0.20689395351701215  validation loss:0.1623473137681213\nepoch 495  train loss: 0.20689395669105456  validation loss:0.16234731368359553\nepoch 496  train loss: 0.20689395986505743  validation loss:0.16234731359907106\nepoch 497  train loss: 0.20689396303902058  validation loss:0.16234731351454795\nepoch 498  train loss: 0.20689396621294381  validation loss:0.1623473134300262\nepoch 499  train loss: 0.20689396938682744  validation loss:0.1623473133455058\nepoch 500  train loss: 0.20689397256067132  validation loss:0.16234731326098675\nepoch 501  train loss: 0.20689397573447538  validation loss:0.16234731317646908\nepoch 502  train loss: 0.20689397890823985  validation loss:0.16234731309195274\nepoch 503  train loss: 0.2068939820819646  validation loss:0.1623473130074378\nepoch 504  train loss: 0.20689398525564956  validation loss:0.16234731292292412\nepoch 505  train loss: 0.20689398842929485  validation loss:0.1623473128384119\nepoch 506  train loss: 0.20689399160290037  validation loss:0.16234731275390094\nepoch 507  train loss: 0.2068939947764662  validation loss:0.1623473126693914\nepoch 508  train loss: 0.2068939979499923  validation loss:0.1623473125848832\nepoch 509  train loss: 0.2068940011234787  validation loss:0.16234731250037635\nepoch 510  train loss: 0.2068940042969254  validation loss:0.16234731241587086\nepoch 511  train loss: 0.20689400747033232  validation loss:0.16234731233136668\nepoch 512  train loss: 0.20689401064369956  validation loss:0.16234731224686394\nepoch 513  train loss: 0.20689401381702702  validation loss:0.16234731216236248\nepoch 514  train loss: 0.20689401699031498  validation loss:0.16234731207786243\nepoch 515  train loss: 0.20689402016356306  validation loss:0.16234731199336372\nepoch 516  train loss: 0.20689402333677148  validation loss:0.16234731190886642\nepoch 517  train loss: 0.20689402650994018  validation loss:0.1623473118243704\nepoch 518  train loss: 0.20689402968306916  validation loss:0.1623473117398757\nepoch 519  train loss: 0.20689403285615843  validation loss:0.1623473116553824\nepoch 520  train loss: 0.2068940360292079  validation loss:0.1623473115708905\nepoch 521  train loss: 0.20689403920221794  validation loss:0.16234731148639991\nepoch 522  train loss: 0.206894042375188  validation loss:0.16234731140191067\nepoch 523  train loss: 0.20689404554811838  validation loss:0.16234731131742275\nepoch 524  train loss: 0.20689404872100908  validation loss:0.16234731123293622\nepoch 525  train loss: 0.2068940518938602  validation loss:0.16234731114845105\nepoch 526  train loss: 0.20689405506667152  validation loss:0.1623473110639672\nepoch 527  train loss: 0.20689405823944312  validation loss:0.16234731097948474\nepoch 528  train loss: 0.20689406141217503  validation loss:0.16234731089500368\nepoch 529  train loss: 0.20689406458486714  validation loss:0.1623473108105239\nepoch 530  train loss: 0.2068940677575197  validation loss:0.1623473107260455\nepoch 531  train loss: 0.20689407093013246  validation loss:0.1623473106415685\nepoch 532  train loss: 0.20689407410270572  validation loss:0.16234731055709276\nepoch 533  train loss: 0.206894077275239  validation loss:0.16234731047261844\nepoch 534  train loss: 0.20689408044773278  validation loss:0.16234731038814543\nepoch 535  train loss: 0.2068940836201867  validation loss:0.1623473103036738\nepoch 536  train loss: 0.20689408679260107  validation loss:0.1623473102192035\nepoch 537  train loss: 0.20689408996497563  validation loss:0.1623473101347346\nepoch 538  train loss: 0.2068940931373106  validation loss:0.162347310050267\nepoch 539  train loss: 0.20689409630960587  validation loss:0.1623473099658008\nepoch 540  train loss: 0.20689409948186127  validation loss:0.16234730988133592\nepoch 541  train loss: 0.2068941026540772  validation loss:0.16234730979687245\nepoch 542  train loss: 0.20689410582625334  validation loss:0.1623473097124103\nepoch 543  train loss: 0.2068941089983898  validation loss:0.1623473096279495\nepoch 544  train loss: 0.20689411217048653  validation loss:0.16234730954349\nepoch 545  train loss: 0.20689411534254354  validation loss:0.16234730945903195\nepoch 546  train loss: 0.20689411851456102  validation loss:0.16234730937457517\nepoch 547  train loss: 0.20689412168653865  validation loss:0.16234730929011976\nepoch 548  train loss: 0.20689412485847664  validation loss:0.16234730920566579\nepoch 549  train loss: 0.206894128030375  validation loss:0.1623473091212131\nepoch 550  train loss: 0.20689413120223363  validation loss:0.16234730903676178\nepoch 551  train loss: 0.20689413437405252  validation loss:0.16234730895231178\nepoch 552  train loss: 0.2068941375458317  validation loss:0.1623473088678632\nepoch 553  train loss: 0.2068941407175714  validation loss:0.16234730878341588\nepoch 554  train loss: 0.20689414388927124  validation loss:0.16234730869897002\nepoch 555  train loss: 0.2068941470609314  validation loss:0.16234730861452543\nepoch 556  train loss: 0.20689415023255195  validation loss:0.16234730853008225\nepoch 557  train loss: 0.20689415340413272  validation loss:0.16234730844564044\nepoch 558  train loss: 0.2068941565756739  validation loss:0.16234730836119995\nepoch 559  train loss: 0.20689415974717537  validation loss:0.16234730827676078\nepoch 560  train loss: 0.20689416291863721  validation loss:0.162347308192323\nepoch 561  train loss: 0.2068941660900594  validation loss:0.16234730810788653\nepoch 562  train loss: 0.2068941692614417  validation loss:0.16234730802345146\nepoch 563  train loss: 0.2068941724327844  validation loss:0.16234730793901778\nepoch 564  train loss: 0.2068941756040874  validation loss:0.1623473078545854\nepoch 565  train loss: 0.2068941787753509  validation loss:0.1623473077701544\nepoch 566  train loss: 0.20689418194657463  validation loss:0.16234730768572472\nepoch 567  train loss: 0.20689418511775876  validation loss:0.1623473076012964\nepoch 568  train loss: 0.20689418828890305  validation loss:0.16234730751686943\nepoch 569  train loss: 0.2068941914600078  validation loss:0.16234730743244383\nepoch 570  train loss: 0.2068941946310729  validation loss:0.16234730734801958\nepoch 571  train loss: 0.20689419780209828  validation loss:0.1623473072635967\nepoch 572  train loss: 0.20689420097308386  validation loss:0.16234730717917512\nepoch 573  train loss: 0.2068942041440298  validation loss:0.16234730709475495\nepoch 574  train loss: 0.20689420731493618  validation loss:0.1623473070103361\nepoch 575  train loss: 0.20689421048580287  validation loss:0.16234730692591864\nepoch 576  train loss: 0.20689421365662988  validation loss:0.1623473068415025\nepoch 577  train loss: 0.20689421682741718  validation loss:0.16234730675708772\nepoch 578  train loss: 0.20689421999816485  validation loss:0.1623473066726743\nepoch 579  train loss: 0.20689422316887296  validation loss:0.1623473065882622\nepoch 580  train loss: 0.2068942263395412  validation loss:0.1623473065038515\nepoch 581  train loss: 0.20689422951016986  validation loss:0.16234730641944214\nepoch 582  train loss: 0.20689423268075882  validation loss:0.16234730633503408\nepoch 583  train loss: 0.20689423585130812  validation loss:0.16234730625062743\nepoch 584  train loss: 0.20689423902181792  validation loss:0.16234730616622212\nepoch 585  train loss: 0.2068942421922879  validation loss:0.16234730608181816\nepoch 586  train loss: 0.20689424536271828  validation loss:0.16234730599741562\nepoch 587  train loss: 0.2068942485331089  validation loss:0.16234730591301436\nepoch 588  train loss: 0.20689425170346  validation loss:0.16234730582861442\nepoch 589  train loss: 0.20689425487377136  validation loss:0.1623473057442159\nepoch 590  train loss: 0.20689425804404307  validation loss:0.16234730565981867\nepoch 591  train loss: 0.20689426121427507  validation loss:0.16234730557542293\nepoch 592  train loss: 0.20689426438446748  validation loss:0.1623473054910284\nepoch 593  train loss: 0.2068942675546202  validation loss:0.16234730540663528\nepoch 594  train loss: 0.20689427072473332  validation loss:0.1623473053222435\nepoch 595  train loss: 0.2068942738948068  validation loss:0.1623473052378531\nepoch 596  train loss: 0.2068942770648406  validation loss:0.16234730515346402\nepoch 597  train loss: 0.2068942802348346  validation loss:0.16234730506907633\nepoch 598  train loss: 0.2068942834047892  validation loss:0.1623473049846899\nepoch 599  train loss: 0.206894286574704  validation loss:0.1623473049003049\nepoch 600  train loss: 0.20689428974457916  validation loss:0.16234730481592125\nepoch 601  train loss: 0.20689429291441463  validation loss:0.16234730473153897\nepoch 602  train loss: 0.20689429608421067  validation loss:0.16234730464715796\nepoch 603  train loss: 0.2068942992539667  validation loss:0.16234730456277843\nepoch 604  train loss: 0.20689430242368329  validation loss:0.16234730447840018\nepoch 605  train loss: 0.20689430559336028  validation loss:0.16234730439402323\nepoch 606  train loss: 0.20689430876299741  validation loss:0.16234730430964764\nepoch 607  train loss: 0.20689431193259517  validation loss:0.1623473042252735\nepoch 608  train loss: 0.20689431510215306  validation loss:0.16234730414090065\nepoch 609  train loss: 0.20689431827167154  validation loss:0.1623473040565291\nepoch 610  train loss: 0.2068943214411501  validation loss:0.162347303972159\nepoch 611  train loss: 0.2068943246105892  validation loss:0.16234730388779026\nepoch 612  train loss: 0.20689432777998867  validation loss:0.16234730380342277\nepoch 613  train loss: 0.20689433094934837  validation loss:0.16234730371905673\nepoch 614  train loss: 0.20689433411866856  validation loss:0.162347303634692\nepoch 615  train loss: 0.20689433728794893  validation loss:0.16234730355032867\nepoch 616  train loss: 0.20689434045718988  validation loss:0.16234730346596668\nepoch 617  train loss: 0.20689434362639103  validation loss:0.162347303381606\nepoch 618  train loss: 0.20689434679555258  validation loss:0.16234730329724664\nepoch 619  train loss: 0.20689434996467448  validation loss:0.16234730321288868\nepoch 620  train loss: 0.2068943531337568  validation loss:0.16234730312853204\nepoch 621  train loss: 0.20689435630279937  validation loss:0.1623473030441768\nepoch 622  train loss: 0.2068943594718025  validation loss:0.16234730295982291\nepoch 623  train loss: 0.20689436264076583  validation loss:0.1623473028754704\nepoch 624  train loss: 0.20689436580968953  validation loss:0.16234730279111917\nepoch 625  train loss: 0.2068943689785736  validation loss:0.16234730270676934\nepoch 626  train loss: 0.20689437214741824  validation loss:0.1623473026224209\nepoch 627  train loss: 0.20689437531622315  validation loss:0.16234730253807372\nepoch 628  train loss: 0.20689437848498832  validation loss:0.16234730245372792\nepoch 629  train loss: 0.20689438165371388  validation loss:0.16234730236938347\nepoch 630  train loss: 0.2068943848223998  validation loss:0.16234730228504038\nepoch 631  train loss: 0.20689438799104615  validation loss:0.16234730220069862\nepoch 632  train loss: 0.20689439115965286  validation loss:0.16234730211635823\nepoch 633  train loss: 0.20689439432822  validation loss:0.16234730203201922\nepoch 634  train loss: 0.20689439749674737  validation loss:0.16234730194768154\nepoch 635  train loss: 0.20689440066523523  validation loss:0.1623473018633452\nepoch 636  train loss: 0.20689440383368352  validation loss:0.1623473017790102\nepoch 637  train loss: 0.20689440700209197  validation loss:0.1623473016946766\nepoch 638  train loss: 0.20689441017046095  validation loss:0.1623473016103443\nepoch 639  train loss: 0.20689441333879025  validation loss:0.1623473015260134\nepoch 640  train loss: 0.20689441650707993  validation loss:0.16234730144168383\nepoch 641  train loss: 0.20689441967533  validation loss:0.16234730135735562\nepoch 642  train loss: 0.20689442284354045  validation loss:0.16234730127302874\nepoch 643  train loss: 0.20689442601171135  validation loss:0.16234730118870322\nepoch 644  train loss: 0.2068944291798425  validation loss:0.16234730110437903\nepoch 645  train loss: 0.20689443234793403  validation loss:0.16234730102005626\nepoch 646  train loss: 0.20689443551598602  validation loss:0.16234730093573482\nepoch 647  train loss: 0.2068944386839985  validation loss:0.16234730085141466\nepoch 648  train loss: 0.2068944418519712  validation loss:0.16234730076709594\nepoch 649  train loss: 0.20689444501990434  validation loss:0.16234730068277856\n","name":"stdout"},{"output_type":"stream","text":"epoch 650  train loss: 0.20689444818779779  validation loss:0.1623473005984625\nepoch 651  train loss: 0.20689445135565168  validation loss:0.16234730051414778\nepoch 652  train loss: 0.20689445452346608  validation loss:0.16234730042983445\nepoch 653  train loss: 0.20689445769124068  validation loss:0.16234730034552244\nepoch 654  train loss: 0.20689446085897575  validation loss:0.16234730026121183\nepoch 655  train loss: 0.20689446402667122  validation loss:0.16234730017690255\nepoch 656  train loss: 0.20689446719432714  validation loss:0.1623473000925946\nepoch 657  train loss: 0.20689447036194322  validation loss:0.162347300008288\nepoch 658  train loss: 0.20689447352951987  validation loss:0.1623472999239828\nepoch 659  train loss: 0.20689447669705688  validation loss:0.1623472998396789\nepoch 660  train loss: 0.20689447986455428  validation loss:0.1623472997553764\nepoch 661  train loss: 0.20689448303201213  validation loss:0.16234729967107522\nepoch 662  train loss: 0.2068944861994302  validation loss:0.1623472995867754\nepoch 663  train loss: 0.20689448936680882  validation loss:0.16234729950247692\nepoch 664  train loss: 0.20689449253414782  validation loss:0.16234729941817977\nepoch 665  train loss: 0.20689449570144713  validation loss:0.162347299333884\nepoch 666  train loss: 0.20689449886870692  validation loss:0.16234729924958954\nepoch 667  train loss: 0.20689450203592707  validation loss:0.1623472991652965\nepoch 668  train loss: 0.20689450520310765  validation loss:0.16234729908100476\nepoch 669  train loss: 0.20689450837024864  validation loss:0.16234729899671443\nepoch 670  train loss: 0.20689451153734986  validation loss:0.16234729891242541\nepoch 671  train loss: 0.20689451470441175  validation loss:0.1623472988281377\nepoch 672  train loss: 0.20689451787143395  validation loss:0.1623472987438514\nepoch 673  train loss: 0.2068945210384164  validation loss:0.1623472986595664\nepoch 674  train loss: 0.20689452420535948  validation loss:0.1623472985752828\nepoch 675  train loss: 0.20689452737226277  validation loss:0.16234729849100055\nepoch 676  train loss: 0.20689453053912651  validation loss:0.1623472984067196\nepoch 677  train loss: 0.20689453370595062  validation loss:0.16234729832244008\nepoch 678  train loss: 0.2068945368727353  validation loss:0.16234729823816188\nepoch 679  train loss: 0.20689454003948005  validation loss:0.16234729815388507\nepoch 680  train loss: 0.2068945432061855  validation loss:0.16234729806960954\nepoch 681  train loss: 0.20689454637285146  validation loss:0.1623472979853354\nepoch 682  train loss: 0.2068945495394775  validation loss:0.16234729790106253\nepoch 683  train loss: 0.20689455270606416  validation loss:0.1623472978167911\nepoch 684  train loss: 0.2068945558726111  validation loss:0.16234729773252102\nepoch 685  train loss: 0.2068945590391186  validation loss:0.1623472976482523\nepoch 686  train loss: 0.20689456220558644  validation loss:0.16234729756398486\nepoch 687  train loss: 0.2068945653720148  validation loss:0.1623472974797188\nepoch 688  train loss: 0.20689456853840324  validation loss:0.1623472973954541\nepoch 689  train loss: 0.20689457170475248  validation loss:0.16234729731119074\nepoch 690  train loss: 0.2068945748710619  validation loss:0.16234729722692876\nepoch 691  train loss: 0.20689457803733172  validation loss:0.16234729714266813\nepoch 692  train loss: 0.20689458120356222  validation loss:0.16234729705840883\nepoch 693  train loss: 0.2068945843697529  validation loss:0.16234729697415087\nepoch 694  train loss: 0.20689458753590403  validation loss:0.16234729688989427\nepoch 695  train loss: 0.2068945907020155  validation loss:0.16234729680563903\nepoch 696  train loss: 0.20689459386808748  validation loss:0.16234729672138512\nepoch 697  train loss: 0.20689459703411991  validation loss:0.1623472966371326\nepoch 698  train loss: 0.2068946002001126  validation loss:0.16234729655288144\nepoch 699  train loss: 0.2068946033660659  validation loss:0.16234729646863158\nepoch 700  train loss: 0.2068946065319796  validation loss:0.16234729638438308\nepoch 701  train loss: 0.2068946096978537  validation loss:0.16234729630013597\nepoch 702  train loss: 0.20689461286368824  validation loss:0.16234729621589014\nepoch 703  train loss: 0.20689461602948314  validation loss:0.1623472961316457\nepoch 704  train loss: 0.20689461919523847  validation loss:0.1623472960474026\nepoch 705  train loss: 0.2068946223609542  validation loss:0.16234729596316086\nepoch 706  train loss: 0.2068946255266304  validation loss:0.1623472958789205\nepoch 707  train loss: 0.20689462869226702  validation loss:0.16234729579468143\nepoch 708  train loss: 0.206894631857864  validation loss:0.16234729571044382\nepoch 709  train loss: 0.20689463502342148  validation loss:0.16234729562620745\nepoch 710  train loss: 0.20689463818893947  validation loss:0.1623472955419725\nepoch 711  train loss: 0.20689464135441787  validation loss:0.16234729545773885\nepoch 712  train loss: 0.20689464451985645  validation loss:0.16234729537350656\nepoch 713  train loss: 0.2068946476852557  validation loss:0.16234729528927566\nepoch 714  train loss: 0.20689465085061542  validation loss:0.1623472952050461\nepoch 715  train loss: 0.2068946540159354  validation loss:0.16234729512081783\nepoch 716  train loss: 0.2068946571812158  validation loss:0.16234729503659093\nepoch 717  train loss: 0.2068946603464567  validation loss:0.1623472949523654\nepoch 718  train loss: 0.20689466351165808  validation loss:0.16234729486814126\nepoch 719  train loss: 0.20689466667681977  validation loss:0.1623472947839184\nepoch 720  train loss: 0.20689466984194207  validation loss:0.16234729469969697\nepoch 721  train loss: 0.2068946730070248  validation loss:0.16234729461547684\nepoch 722  train loss: 0.20689467617206772  validation loss:0.16234729453125804\nepoch 723  train loss: 0.20689467933707137  validation loss:0.16234729444704057\nepoch 724  train loss: 0.20689468250203538  validation loss:0.1623472943628245\nepoch 725  train loss: 0.2068946856669598  validation loss:0.16234729427860972\nepoch 726  train loss: 0.20689468883184453  validation loss:0.16234729419439636\nepoch 727  train loss: 0.2068946919966899  validation loss:0.16234729411018436\nepoch 728  train loss: 0.20689469516149547  validation loss:0.16234729402597367\nepoch 729  train loss: 0.20689469832626167  validation loss:0.16234729394176436\nepoch 730  train loss: 0.20689470149098832  validation loss:0.16234729385755636\nepoch 731  train loss: 0.20689470465567528  validation loss:0.16234729377334972\nepoch 732  train loss: 0.2068947078203227  validation loss:0.16234729368914447\nepoch 733  train loss: 0.20689471098493079  validation loss:0.16234729360494052\nepoch 734  train loss: 0.20689471414949906  validation loss:0.16234729352073798\nepoch 735  train loss: 0.20689471731402798  validation loss:0.16234729343653675\nepoch 736  train loss: 0.20689472047851723  validation loss:0.16234729335233689\nepoch 737  train loss: 0.206894723642967  validation loss:0.16234729326813832\nepoch 738  train loss: 0.20689472680737714  validation loss:0.16234729318394117\nepoch 739  train loss: 0.20689472997174777  validation loss:0.16234729309974533\nepoch 740  train loss: 0.20689473313607878  validation loss:0.16234729301555084\nepoch 741  train loss: 0.2068947363003703  validation loss:0.16234729293135772\nepoch 742  train loss: 0.20689473946462222  validation loss:0.16234729284716592\nepoch 743  train loss: 0.20689474262883462  validation loss:0.16234729276297552\nepoch 744  train loss: 0.2068947457930075  validation loss:0.16234729267878645\nepoch 745  train loss: 0.2068947489571408  validation loss:0.16234729259459868\nepoch 746  train loss: 0.20689475212123468  validation loss:0.1623472925104123\nepoch 747  train loss: 0.20689475528528878  validation loss:0.16234729242622722\nepoch 748  train loss: 0.20689475844930355  validation loss:0.16234729234204356\nepoch 749  train loss: 0.20689476161327863  validation loss:0.16234729225786124\nepoch 750  train loss: 0.20689476477721425  validation loss:0.16234729217368027\nepoch 751  train loss: 0.2068947679411102  validation loss:0.16234729208950066\nepoch 752  train loss: 0.2068947711049668  validation loss:0.16234729200532239\nepoch 753  train loss: 0.20689477426878372  validation loss:0.16234729192114547\nepoch 754  train loss: 0.20689477743256113  validation loss:0.1623472918369699\nepoch 755  train loss: 0.20689478059629907  validation loss:0.1623472917527956\nepoch 756  train loss: 0.20689478375999737  validation loss:0.16234729166862272\nepoch 757  train loss: 0.20689478692365626  validation loss:0.16234729158445121\nepoch 758  train loss: 0.20689479008727552  validation loss:0.16234729150028102\nepoch 759  train loss: 0.20689479325085508  validation loss:0.1623472914161122\nepoch 760  train loss: 0.20689479641439545  validation loss:0.16234729133194473\nepoch 761  train loss: 0.20689479957789605  validation loss:0.16234729124777855\nepoch 762  train loss: 0.20689480274135716  validation loss:0.16234729116361377\nepoch 763  train loss: 0.20689480590477877  validation loss:0.1623472910794503\nepoch 764  train loss: 0.20689480906816082  validation loss:0.16234729099528827\nepoch 765  train loss: 0.20689481223150324  validation loss:0.16234729091112754\nepoch 766  train loss: 0.2068948153948063  validation loss:0.1623472908269681\nepoch 767  train loss: 0.20689481855806985  validation loss:0.16234729074281007\nepoch 768  train loss: 0.20689482172129364  validation loss:0.16234729065865344\nepoch 769  train loss: 0.20689482488447808  validation loss:0.16234729057449807\nepoch 770  train loss: 0.2068948280476229  validation loss:0.16234729049034405\nepoch 771  train loss: 0.20689483121072821  validation loss:0.16234729040619142\nepoch 772  train loss: 0.2068948343737941  validation loss:0.16234729032204018\nepoch 773  train loss: 0.20689483753682034  validation loss:0.1623472902378902\nepoch 774  train loss: 0.20689484069980718  validation loss:0.1623472901537416\nepoch 775  train loss: 0.20689484386275445  validation loss:0.16234729006959436\nepoch 776  train loss: 0.20689484702566216  validation loss:0.1623472899854485\nepoch 777  train loss: 0.2068948501885302  validation loss:0.1623472899013039\nepoch 778  train loss: 0.20689485335135888  validation loss:0.1623472898171607\nepoch 779  train loss: 0.20689485651414807  validation loss:0.16234728973301885\nepoch 780  train loss: 0.20689485967689766  validation loss:0.16234728964887837\nepoch 781  train loss: 0.20689486283960773  validation loss:0.16234728956473915\nepoch 782  train loss: 0.20689486600227833  validation loss:0.16234728948060134\nepoch 783  train loss: 0.20689486916490926  validation loss:0.16234728939646492\nepoch 784  train loss: 0.20689487232750092  validation loss:0.16234728931232986\nepoch 785  train loss: 0.20689487549005292  validation loss:0.16234728922819608\nepoch 786  train loss: 0.2068948786525654  validation loss:0.16234728914406368\nepoch 787  train loss: 0.2068948818150384  validation loss:0.1623472890599326\nepoch 788  train loss: 0.20689488497747183  validation loss:0.16234728897580292\nepoch 789  train loss: 0.20689488813986578  validation loss:0.16234728889167455\nepoch 790  train loss: 0.2068948913022202  validation loss:0.16234728880754754\nepoch 791  train loss: 0.20689489446453513  validation loss:0.16234728872342188\nepoch 792  train loss: 0.20689489762681046  validation loss:0.16234728863929757\nepoch 793  train loss: 0.2068949007890464  validation loss:0.16234728855517458\nepoch 794  train loss: 0.20689490395124283  validation loss:0.16234728847105298\nepoch 795  train loss: 0.20689490711339972  validation loss:0.16234728838693274\nepoch 796  train loss: 0.20689491027551699  validation loss:0.16234728830281378\nepoch 797  train loss: 0.20689491343759495  validation loss:0.1623472882186962\nepoch 798  train loss: 0.20689491659963316  validation loss:0.16234728813458002\nepoch 799  train loss: 0.206894919761632  validation loss:0.1623472880504651\nepoch 800  train loss: 0.20689492292359124  validation loss:0.16234728796635162\nepoch 801  train loss: 0.2068949260855111  validation loss:0.16234728788223945\nepoch 802  train loss: 0.20689492924739145  validation loss:0.1623472877981286\nepoch 803  train loss: 0.20689493240923226  validation loss:0.16234728771401913\nepoch 804  train loss: 0.2068949355710335  validation loss:0.16234728762991102\nepoch 805  train loss: 0.20689493873279538  validation loss:0.16234728754580424\nepoch 806  train loss: 0.20689494189451754  validation loss:0.16234728746169882\nepoch 807  train loss: 0.20689494505620043  validation loss:0.16234728737759474\nepoch 808  train loss: 0.20689494821784357  validation loss:0.162347287293492\nepoch 809  train loss: 0.2068949513794475  validation loss:0.1623472872093906\nepoch 810  train loss: 0.2068949545410118  validation loss:0.1623472871252906\nepoch 811  train loss: 0.20689495770253652  validation loss:0.16234728704119183\nepoch 812  train loss: 0.2068949608640218  validation loss:0.1623472869570945\n","name":"stdout"},{"output_type":"stream","text":"epoch 813  train loss: 0.2068949640254677  validation loss:0.16234728687299851\nepoch 814  train loss: 0.20689496718687395  validation loss:0.16234728678890384\nepoch 815  train loss: 0.2068949703482406  validation loss:0.1623472867048106\nepoch 816  train loss: 0.20689497350956787  validation loss:0.16234728662071865\nepoch 817  train loss: 0.20689497667085574  validation loss:0.16234728653662803\nepoch 818  train loss: 0.20689497983210417  validation loss:0.16234728645253876\nepoch 819  train loss: 0.2068949829933128  validation loss:0.16234728636845083\nepoch 820  train loss: 0.20689498615448218  validation loss:0.1623472862843643\nepoch 821  train loss: 0.206894989315612  validation loss:0.1623472862002791\nepoch 822  train loss: 0.2068949924767024  validation loss:0.16234728611619523\nepoch 823  train loss: 0.2068949956377531  validation loss:0.16234728603211268\nepoch 824  train loss: 0.20689499879876455  validation loss:0.16234728594803152\nepoch 825  train loss: 0.20689500195973637  validation loss:0.16234728586395172\nepoch 826  train loss: 0.20689500512066875  validation loss:0.16234728577987326\nepoch 827  train loss: 0.20689500828156165  validation loss:0.16234728569579615\nepoch 828  train loss: 0.206895011442415  validation loss:0.16234728561172038\nepoch 829  train loss: 0.20689501460322895  validation loss:0.16234728552764596\nepoch 830  train loss: 0.2068950177640033  validation loss:0.16234728544357288\nepoch 831  train loss: 0.20689502092473827  validation loss:0.1623472853595011\nepoch 832  train loss: 0.2068950240854337  validation loss:0.16234728527543074\nepoch 833  train loss: 0.2068950272460896  validation loss:0.16234728519136174\nepoch 834  train loss: 0.20689503040670615  validation loss:0.16234728510729404\nepoch 835  train loss: 0.20689503356728317  validation loss:0.1623472850232277\nepoch 836  train loss: 0.2068950367278207  validation loss:0.1623472849391627\nepoch 837  train loss: 0.2068950398883187  validation loss:0.16234728485509908\nepoch 838  train loss: 0.20689504304877723  validation loss:0.1623472847710368\nepoch 839  train loss: 0.2068950462091963  validation loss:0.1623472846869759\nepoch 840  train loss: 0.2068950493695759  validation loss:0.16234728460291628\nepoch 841  train loss: 0.20689505252991605  validation loss:0.16234728451885802\nepoch 842  train loss: 0.20689505569021668  validation loss:0.16234728443480115\nepoch 843  train loss: 0.20689505885047768  validation loss:0.16234728435074558\nepoch 844  train loss: 0.20689506201069943  validation loss:0.16234728426669137\nepoch 845  train loss: 0.20689506517088166  validation loss:0.16234728418263847\nepoch 846  train loss: 0.20689506833102436  validation loss:0.16234728409858695\nepoch 847  train loss: 0.20689507149112768  validation loss:0.1623472840145368\nepoch 848  train loss: 0.20689507465119142  validation loss:0.162347283930488\nepoch 849  train loss: 0.2068950778112158  validation loss:0.16234728384644054\nepoch 850  train loss: 0.20689508097120052  validation loss:0.1623472837623944\nepoch 851  train loss: 0.2068950841311459  validation loss:0.16234728367834966\nepoch 852  train loss: 0.20689508729105188  validation loss:0.16234728359430628\nepoch 853  train loss: 0.20689509045091833  validation loss:0.1623472835102642\nepoch 854  train loss: 0.2068950936107453  validation loss:0.1623472834262234\nepoch 855  train loss: 0.20689509677053275  validation loss:0.16234728334218398\nepoch 856  train loss: 0.2068950999302807  validation loss:0.162347283258146\nepoch 857  train loss: 0.20689510308998926  validation loss:0.16234728317410926\nepoch 858  train loss: 0.20689510624965834  validation loss:0.16234728309007393\nepoch 859  train loss: 0.20689510940928807  validation loss:0.16234728300603996\nepoch 860  train loss: 0.2068951125688781  validation loss:0.16234728292200729\nepoch 861  train loss: 0.20689511572842892  validation loss:0.16234728283797603\nepoch 862  train loss: 0.20689511888794007  validation loss:0.16234728275394605\nepoch 863  train loss: 0.20689512204741184  validation loss:0.16234728266991746\nepoch 864  train loss: 0.20689512520684414  validation loss:0.1623472825858902\nepoch 865  train loss: 0.20689512836623708  validation loss:0.16234728250186425\nepoch 866  train loss: 0.20689513152559036  validation loss:0.1623472824178397\nepoch 867  train loss: 0.2068951346849043  validation loss:0.16234728233381648\nepoch 868  train loss: 0.2068951378441788  validation loss:0.16234728224979464\nepoch 869  train loss: 0.2068951410034138  validation loss:0.16234728216577413\nepoch 870  train loss: 0.20689514416260932  validation loss:0.16234728208175492\nepoch 871  train loss: 0.20689514732176548  validation loss:0.16234728199773713\nepoch 872  train loss: 0.206895150480882  validation loss:0.1623472819137206\nepoch 873  train loss: 0.20689515363995922  validation loss:0.16234728182970545\nepoch 874  train loss: 0.20689515679899695  validation loss:0.1623472817456917\nepoch 875  train loss: 0.20689515995799518  validation loss:0.1623472816616793\nepoch 876  train loss: 0.20689516311695416  validation loss:0.16234728157766817\nepoch 877  train loss: 0.20689516627587345  validation loss:0.16234728149365843\nepoch 878  train loss: 0.2068951694347533  validation loss:0.16234728140965007\nepoch 879  train loss: 0.20689517259359377  validation loss:0.162347281325643\nepoch 880  train loss: 0.2068951757523948  validation loss:0.1623472812416373\nepoch 881  train loss: 0.20689517891115639  validation loss:0.16234728115763294\nepoch 882  train loss: 0.20689518206987847  validation loss:0.16234728107363\nepoch 883  train loss: 0.20689518522856112  validation loss:0.1623472809896283\nepoch 884  train loss: 0.20689518838720433  validation loss:0.16234728090562803\nepoch 885  train loss: 0.20689519154580813  validation loss:0.16234728082162903\nepoch 886  train loss: 0.20689519470437243  validation loss:0.1623472807376314\nepoch 887  train loss: 0.2068951978628973  validation loss:0.1623472806536351\nepoch 888  train loss: 0.20689520102138265  validation loss:0.16234728056964018\nepoch 889  train loss: 0.20689520417982876  validation loss:0.16234728048564662\nepoch 890  train loss: 0.2068952073382353  validation loss:0.16234728040165441\nepoch 891  train loss: 0.20689521049660234  validation loss:0.16234728031766352\nepoch 892  train loss: 0.20689521365492997  validation loss:0.162347280233674\nepoch 893  train loss: 0.20689521681321826  validation loss:0.16234728014968577\nepoch 894  train loss: 0.20689521997146706  validation loss:0.16234728006569893\nepoch 895  train loss: 0.2068952231296763  validation loss:0.16234727998171342\nepoch 896  train loss: 0.20689522628784635  validation loss:0.16234727989772924\nepoch 897  train loss: 0.20689522944597666  validation loss:0.16234727981374647\nepoch 898  train loss: 0.20689523260406778  validation loss:0.16234727972976504\nepoch 899  train loss: 0.2068952357621194  validation loss:0.1623472796457849\nepoch 900  train loss: 0.20689523892013148  validation loss:0.1623472795618061\nepoch 901  train loss: 0.20689524207810417  validation loss:0.1623472794778287\nepoch 902  train loss: 0.2068952452360374  validation loss:0.16234727939385266\nepoch 903  train loss: 0.20689524839393117  validation loss:0.1623472793098779\nepoch 904  train loss: 0.20689525155178584  validation loss:0.16234727922590456\nepoch 905  train loss: 0.20689525470960068  validation loss:0.16234727914193248\nepoch 906  train loss: 0.20689525786737617  validation loss:0.16234727905796179\nepoch 907  train loss: 0.2068952610251123  validation loss:0.16234727897399248\nepoch 908  train loss: 0.20689526418280896  validation loss:0.16234727889002448\nepoch 909  train loss: 0.20689526734046637  validation loss:0.16234727880605782\nepoch 910  train loss: 0.20689527049808412  validation loss:0.16234727872209254\nepoch 911  train loss: 0.2068952736556626  validation loss:0.16234727863812856\nepoch 912  train loss: 0.20689527681320138  validation loss:0.16234727855416592\nepoch 913  train loss: 0.20689527997070103  validation loss:0.16234727847020466\nepoch 914  train loss: 0.20689528312816108  validation loss:0.16234727838624477\nepoch 915  train loss: 0.20689528628558188  validation loss:0.16234727830228618\nepoch 916  train loss: 0.20689528944296295  validation loss:0.16234727821832895\nepoch 917  train loss: 0.20689529260030487  validation loss:0.1623472781343731\nepoch 918  train loss: 0.20689529575760723  validation loss:0.16234727805041851\nepoch 919  train loss: 0.20689529891487024  validation loss:0.16234727796646534\nepoch 920  train loss: 0.2068953020720938  validation loss:0.16234727788251352\nepoch 921  train loss: 0.2068953052292781  validation loss:0.16234727779856303\nepoch 922  train loss: 0.20689530838642275  validation loss:0.1623472777146139\nepoch 923  train loss: 0.20689531154352814  validation loss:0.16234727763066606\nepoch 924  train loss: 0.20689531470059394  validation loss:0.16234727754671965\nepoch 925  train loss: 0.20689531785762047  validation loss:0.16234727746277455\nepoch 926  train loss: 0.20689532101460742  validation loss:0.16234727737883076\nepoch 927  train loss: 0.20689532417155515  validation loss:0.16234727729488832\nepoch 928  train loss: 0.2068953273284633  validation loss:0.16234727721094724\nepoch 929  train loss: 0.2068953304853321  validation loss:0.16234727712700753\nepoch 930  train loss: 0.20689533364216156  validation loss:0.16234727704306912\nepoch 931  train loss: 0.20689533679895145  validation loss:0.1623472769591321\nepoch 932  train loss: 0.20689533995570208  validation loss:0.1623472768751964\nepoch 933  train loss: 0.2068953431124132  validation loss:0.16234727679126204\nepoch 934  train loss: 0.20689534626908496  validation loss:0.16234727670732907\nepoch 935  train loss: 0.20689534942571733  validation loss:0.1623472766233974\nepoch 936  train loss: 0.20689535258231018  validation loss:0.1623472765394671\nepoch 937  train loss: 0.20689535573886375  validation loss:0.16234727645553818\nepoch 938  train loss: 0.20689535889537786  validation loss:0.16234727637161048\nepoch 939  train loss: 0.20689536205185252  validation loss:0.1623472762876842\nepoch 940  train loss: 0.20689536520828777  validation loss:0.1623472762037593\nepoch 941  train loss: 0.20689536836468367  validation loss:0.16234727611983577\nepoch 942  train loss: 0.20689537152104018  validation loss:0.1623472760359135\nepoch 943  train loss: 0.20689537467735716  validation loss:0.16234727595199264\nepoch 944  train loss: 0.20689537783363482  validation loss:0.16234727586807313\nepoch 945  train loss: 0.206895380989873  validation loss:0.16234727578415492\nepoch 946  train loss: 0.2068953841460718  validation loss:0.16234727570023802\nepoch 947  train loss: 0.20689538730223134  validation loss:0.1623472756163225\nepoch 948  train loss: 0.2068953904583514  validation loss:0.1623472755324084\nepoch 949  train loss: 0.20689539361443215  validation loss:0.16234727544849561\nepoch 950  train loss: 0.20689539677047325  validation loss:0.16234727536458418\nepoch 951  train loss: 0.20689539992647518  validation loss:0.16234727528067402\nepoch 952  train loss: 0.20689540308243756  validation loss:0.1623472751967652\nepoch 953  train loss: 0.20689540623836064  validation loss:0.16234727511285776\nepoch 954  train loss: 0.2068954093942444  validation loss:0.1623472750289517\nepoch 955  train loss: 0.2068954125500886  validation loss:0.16234727494504694\nepoch 956  train loss: 0.2068954157058934  validation loss:0.1623472748611436\nepoch 957  train loss: 0.20689541886165877  validation loss:0.16234727477724153\nepoch 958  train loss: 0.206895422017385  validation loss:0.16234727469334082\nepoch 959  train loss: 0.20689542517307158  validation loss:0.16234727460944146\nepoch 960  train loss: 0.20689542832871877  validation loss:0.16234727452554346\nepoch 961  train loss: 0.20689543148432676  validation loss:0.1623472744416468\nepoch 962  train loss: 0.2068954346398953  validation loss:0.16234727435775145\nepoch 963  train loss: 0.2068954377954243  validation loss:0.16234727427385745\nepoch 964  train loss: 0.20689544095091417  validation loss:0.16234727418996486\nepoch 965  train loss: 0.20689544410636435  validation loss:0.16234727410607355\nepoch 966  train loss: 0.20689544726177528  validation loss:0.16234727402218363\nepoch 967  train loss: 0.20689545041714685  validation loss:0.162347273938295\nepoch 968  train loss: 0.20689545357247904  validation loss:0.1623472738544078\nepoch 969  train loss: 0.2068954567277718  validation loss:0.16234727377052185\nepoch 970  train loss: 0.20689545988302516  validation loss:0.16234727368663732\nepoch 971  train loss: 0.20689546303823916  validation loss:0.1623472736027541\nepoch 972  train loss: 0.20689546619341365  validation loss:0.16234727351887224\nepoch 973  train loss: 0.2068954693485491  validation loss:0.1623472734349917\nepoch 974  train loss: 0.2068954725036448  validation loss:0.16234727335111254\nepoch 975  train loss: 0.20689547565870128  validation loss:0.16234727326723467\nepoch 976  train loss: 0.20689547881371834  validation loss:0.16234727318335818\nepoch 977  train loss: 0.20689548196869606  validation loss:0.16234727309948305\nepoch 978  train loss: 0.20689548512363437  validation loss:0.16234727301560925\nepoch 979  train loss: 0.20689548827853335  validation loss:0.16234727293173679\nepoch 980  train loss: 0.2068954914333929  validation loss:0.16234727284786568\nepoch 981  train loss: 0.206895494588213  validation loss:0.1623472727639959\n","name":"stdout"},{"output_type":"stream","text":"epoch 982  train loss: 0.20689549774299384  validation loss:0.1623472726801275\nepoch 983  train loss: 0.20689550089773534  validation loss:0.16234727259626042\nepoch 984  train loss: 0.2068955040524373  validation loss:0.16234727251239467\nepoch 985  train loss: 0.2068955072071  validation loss:0.16234727242853025\nepoch 986  train loss: 0.2068955103617232  validation loss:0.16234727234466723\nepoch 987  train loss: 0.2068955135163072  validation loss:0.1623472722608055\nepoch 988  train loss: 0.20689551667085176  validation loss:0.1623472721769452\nepoch 989  train loss: 0.2068955198253569  validation loss:0.1623472720930862\nepoch 990  train loss: 0.20689552297982278  validation loss:0.1623472720092285\nepoch 991  train loss: 0.20689552613424914  validation loss:0.1623472719253722\nepoch 992  train loss: 0.20689552928863628  validation loss:0.16234727184151723\nepoch 993  train loss: 0.20689553244298384  validation loss:0.16234727175766359\nepoch 994  train loss: 0.2068955355972922  validation loss:0.1623472716738113\nepoch 995  train loss: 0.20689553875156125  validation loss:0.16234727158996037\nepoch 996  train loss: 0.20689554190579082  validation loss:0.16234727150611078\nepoch 997  train loss: 0.2068955450599811  validation loss:0.16234727142226255\nepoch 998  train loss: 0.2068955482141318  validation loss:0.16234727133841564\nepoch 999  train loss: 0.20689555136824334  validation loss:0.16234727125457007\nepoch 1000  train loss: 0.2068955545223155  validation loss:0.16234727117072587\nepoch 1001  train loss: 0.20689555767634832  validation loss:0.16234727108688293\nepoch 1002  train loss: 0.20689556083034177  validation loss:0.1623472710030414\nepoch 1003  train loss: 0.2068955639842958  validation loss:0.16234727091920118\nepoch 1004  train loss: 0.20689556713821022  validation loss:0.1623472708353624\nepoch 1005  train loss: 0.2068955702920856  validation loss:0.16234727075152489\nepoch 1006  train loss: 0.20689557344592155  validation loss:0.16234727066768875\nepoch 1007  train loss: 0.20689557659971833  validation loss:0.1623472705838539\nepoch 1008  train loss: 0.2068955797534754  validation loss:0.16234727050002049\nepoch 1009  train loss: 0.2068955829071932  validation loss:0.16234727041618835\nepoch 1010  train loss: 0.20689558606087183  validation loss:0.1623472703323576\nepoch 1011  train loss: 0.206895589214511  validation loss:0.16234727024852816\nepoch 1012  train loss: 0.20689559236811075  validation loss:0.16234727016470005\nepoch 1013  train loss: 0.20689559552167125  validation loss:0.16234727008087332\nepoch 1014  train loss: 0.2068955986751923  validation loss:0.1623472699970479\nepoch 1015  train loss: 0.206895601828674  validation loss:0.16234726991322387\nepoch 1016  train loss: 0.2068956049821164  validation loss:0.16234726982940115\nepoch 1017  train loss: 0.2068956081355195  validation loss:0.16234726974557978\nepoch 1018  train loss: 0.20689561128888315  validation loss:0.16234726966175977\nepoch 1019  train loss: 0.2068956144422074  validation loss:0.1623472695779411\nepoch 1020  train loss: 0.20689561759549238  validation loss:0.16234726949412379\nepoch 1021  train loss: 0.2068956207487381  validation loss:0.16234726941030778\nepoch 1022  train loss: 0.20689562390194424  validation loss:0.16234726932649315\nepoch 1023  train loss: 0.20689562705511133  validation loss:0.16234726924267986\nepoch 1024  train loss: 0.20689563020823898  validation loss:0.1623472691588679\nepoch 1025  train loss: 0.20689563336132707  validation loss:0.16234726907505723\nepoch 1026  train loss: 0.20689563651437598  validation loss:0.16234726899124796\nepoch 1027  train loss: 0.20689563966738553  validation loss:0.16234726890744008\nepoch 1028  train loss: 0.20689564282035575  validation loss:0.1623472688236335\nepoch 1029  train loss: 0.20689564597328658  validation loss:0.16234726873982827\nepoch 1030  train loss: 0.20689564912617806  validation loss:0.16234726865602436\nepoch 1031  train loss: 0.2068956522790302  validation loss:0.1623472685722218\nepoch 1032  train loss: 0.20689565543184313  validation loss:0.16234726848842057\nepoch 1033  train loss: 0.2068956585846165  validation loss:0.1623472684046207\nepoch 1034  train loss: 0.20689566173735072  validation loss:0.1623472683208222\nepoch 1035  train loss: 0.20689566489004543  validation loss:0.162347268237025\nepoch 1036  train loss: 0.20689566804270096  validation loss:0.16234726815322914\nepoch 1037  train loss: 0.20689567119531704  validation loss:0.1623472680694347\nepoch 1038  train loss: 0.20689567434789377  validation loss:0.16234726798564153\nepoch 1039  train loss: 0.20689567750043125  validation loss:0.16234726790184972\nepoch 1040  train loss: 0.20689568065292938  validation loss:0.16234726781805928\nepoch 1041  train loss: 0.20689568380538817  validation loss:0.16234726773427022\nepoch 1042  train loss: 0.20689568695780763  validation loss:0.1623472676504824\nepoch 1043  train loss: 0.2068956901101877  validation loss:0.162347267566696\nepoch 1044  train loss: 0.20689569326252846  validation loss:0.16234726748291092\nepoch 1045  train loss: 0.2068956964148299  validation loss:0.16234726739912717\nepoch 1046  train loss: 0.20689569956709203  validation loss:0.1623472673153448\nepoch 1047  train loss: 0.20689570271931476  validation loss:0.16234726723156379\nepoch 1048  train loss: 0.2068957058714983  validation loss:0.16234726714778408\nepoch 1049  train loss: 0.20689570902364235  validation loss:0.16234726706400565\nepoch 1050  train loss: 0.20689571217574715  validation loss:0.16234726698022864\nepoch 1051  train loss: 0.20689571532781262  validation loss:0.16234726689645296\nepoch 1052  train loss: 0.2068957184798388  validation loss:0.1623472668126786\nepoch 1053  train loss: 0.20689572163182554  validation loss:0.16234726672890562\nepoch 1054  train loss: 0.206895724783773  validation loss:0.16234726664513394\nepoch 1055  train loss: 0.20689572793568112  validation loss:0.16234726656136367\nepoch 1056  train loss: 0.20689573108754997  validation loss:0.1623472664775947\nepoch 1057  train loss: 0.20689573423937946  validation loss:0.16234726639382713\nepoch 1058  train loss: 0.2068957373911697  validation loss:0.16234726631006083\nepoch 1059  train loss: 0.20689574054292056  validation loss:0.1623472662262959\nepoch 1060  train loss: 0.20689574369463212  validation loss:0.1623472661425323\nepoch 1061  train loss: 0.20689574684630424  validation loss:0.1623472660587701\nepoch 1062  train loss: 0.2068957499979372  validation loss:0.16234726597500918\nepoch 1063  train loss: 0.20689575314953085  validation loss:0.16234726589124962\nepoch 1064  train loss: 0.2068957563010851  validation loss:0.1623472658074914\nepoch 1065  train loss: 0.20689575945259997  validation loss:0.16234726572373448\nepoch 1066  train loss: 0.2068957626040755  validation loss:0.16234726563997898\nepoch 1067  train loss: 0.20689576575551188  validation loss:0.16234726555622478\nepoch 1068  train loss: 0.20689576890690883  validation loss:0.1623472654724719\nepoch 1069  train loss: 0.20689577205826656  validation loss:0.16234726538872044\nepoch 1070  train loss: 0.20689577520958485  validation loss:0.1623472653049703\nepoch 1071  train loss: 0.20689577836086398  validation loss:0.16234726522122148\nepoch 1072  train loss: 0.20689578151210353  validation loss:0.16234726513747402\nepoch 1073  train loss: 0.20689578466330416  validation loss:0.16234726505372787\nepoch 1074  train loss: 0.20689578781446516  validation loss:0.16234726496998308\nepoch 1075  train loss: 0.20689579096558697  validation loss:0.16234726488623968\nepoch 1076  train loss: 0.20689579411666936  validation loss:0.16234726480249756\nepoch 1077  train loss: 0.20689579726771254  validation loss:0.1623472647187568\nepoch 1078  train loss: 0.20689580041871644  validation loss:0.16234726463501734\nepoch 1079  train loss: 0.206895803569681  validation loss:0.16234726455127926\nepoch 1080  train loss: 0.20689580672060623  validation loss:0.16234726446754255\nepoch 1081  train loss: 0.20689580987149214  validation loss:0.16234726438380714\nepoch 1082  train loss: 0.20689581302233878  validation loss:0.1623472643000731\nepoch 1083  train loss: 0.2068958161731462  validation loss:0.16234726421634038\nepoch 1084  train loss: 0.2068958193239141  validation loss:0.16234726413260903\nepoch 1085  train loss: 0.20689582247464283  validation loss:0.16234726404887903\nepoch 1086  train loss: 0.20689582562533232  validation loss:0.16234726396515042\nepoch 1087  train loss: 0.20689582877598228  validation loss:0.16234726388142307\nepoch 1088  train loss: 0.20689583192659322  validation loss:0.16234726379769704\nepoch 1089  train loss: 0.2068958350771646  validation loss:0.1623472637139724\nepoch 1090  train loss: 0.2068958382276969  validation loss:0.1623472636302491\nepoch 1091  train loss: 0.20689584137818967  validation loss:0.16234726354652715\nepoch 1092  train loss: 0.20689584452864335  validation loss:0.16234726346280656\nepoch 1093  train loss: 0.20689584767905775  validation loss:0.1623472633790873\nepoch 1094  train loss: 0.2068958508294326  validation loss:0.16234726329536936\nepoch 1095  train loss: 0.20689585397976837  validation loss:0.1623472632116528\nepoch 1096  train loss: 0.20689585713006475  validation loss:0.1623472631279375\nepoch 1097  train loss: 0.20689586028032192  validation loss:0.16234726304422362\nepoch 1098  train loss: 0.2068958634305397  validation loss:0.16234726296051105\nepoch 1099  train loss: 0.20689586658071832  validation loss:0.1623472628767998\nepoch 1100  train loss: 0.20689586973085752  validation loss:0.16234726279308997\nepoch 1101  train loss: 0.2068958728809574  validation loss:0.1623472627093814\nepoch 1102  train loss: 0.20689587603101806  validation loss:0.16234726262567423\nepoch 1103  train loss: 0.20689587918103935  validation loss:0.16234726254196843\nepoch 1104  train loss: 0.20689588233102157  validation loss:0.16234726245826386\nepoch 1105  train loss: 0.2068958854809643  validation loss:0.1623472623745607\nepoch 1106  train loss: 0.20689588863086766  validation loss:0.1623472622908589\nepoch 1107  train loss: 0.20689589178073198  validation loss:0.16234726220715837\nepoch 1108  train loss: 0.2068958949305568  validation loss:0.16234726212345926\nepoch 1109  train loss: 0.20689589808034242  validation loss:0.16234726203976146\nepoch 1110  train loss: 0.20689590123008877  validation loss:0.162347261956065\nepoch 1111  train loss: 0.2068959043797958  validation loss:0.1623472618723699\nepoch 1112  train loss: 0.2068959075294636  validation loss:0.1623472617886761\nepoch 1113  train loss: 0.20689591067909205  validation loss:0.1623472617049837\nepoch 1114  train loss: 0.2068959138286813  validation loss:0.16234726162129257\nepoch 1115  train loss: 0.20689591697823115  validation loss:0.16234726153760284\nepoch 1116  train loss: 0.20689592012774183  validation loss:0.16234726145391445\nepoch 1117  train loss: 0.20689592327721315  validation loss:0.1623472613702274\nepoch 1118  train loss: 0.20689592642664534  validation loss:0.16234726128654167\nepoch 1119  train loss: 0.20689592957603808  validation loss:0.16234726120285728\nepoch 1120  train loss: 0.20689593272539142  validation loss:0.16234726111917425\nepoch 1121  train loss: 0.20689593587470573  validation loss:0.16234726103549257\nepoch 1122  train loss: 0.20689593902398073  validation loss:0.1623472609518122\nepoch 1123  train loss: 0.20689594217321639  validation loss:0.16234726086813323\nepoch 1124  train loss: 0.20689594532241276  validation loss:0.1623472607844556\n","name":"stdout"},{"output_type":"stream","text":"epoch 1125  train loss: 0.20689594847156995  validation loss:0.1623472607007792\nepoch 1126  train loss: 0.20689595162068775  validation loss:0.16234726061710422\nepoch 1127  train loss: 0.20689595476976638  validation loss:0.1623472605334306\nepoch 1128  train loss: 0.20689595791880566  validation loss:0.16234726044975828\nepoch 1129  train loss: 0.20689596106780572  validation loss:0.1623472603660873\nepoch 1130  train loss: 0.20689596421676643  validation loss:0.16234726028241772\nepoch 1131  train loss: 0.20689596736568788  validation loss:0.16234726019874945\nepoch 1132  train loss: 0.2068959705145701  validation loss:0.1623472601150825\nepoch 1133  train loss: 0.206895973663413  validation loss:0.16234726003141695\nepoch 1134  train loss: 0.2068959768122167  validation loss:0.16234725994775265\nepoch 1135  train loss: 0.2068959799609811  validation loss:0.16234725986408977\nepoch 1136  train loss: 0.20689598310970628  validation loss:0.1623472597804282\nepoch 1137  train loss: 0.20689598625839217  validation loss:0.162347259696768\nepoch 1138  train loss: 0.20689598940703868  validation loss:0.1623472596131091\nepoch 1139  train loss: 0.20689599255564609  validation loss:0.16234725952945156\nepoch 1140  train loss: 0.20689599570421405  validation loss:0.16234725944579537\nepoch 1141  train loss: 0.2068959988527429  validation loss:0.16234725936214053\nepoch 1142  train loss: 0.2068960020012325  validation loss:0.16234725927848695\nepoch 1143  train loss: 0.20689600514968268  validation loss:0.1623472591948348\nepoch 1144  train loss: 0.20689600829809368  validation loss:0.16234725911118397\nepoch 1145  train loss: 0.20689601144646558  validation loss:0.1623472590275344\nepoch 1146  train loss: 0.20689601459479795  validation loss:0.16234725894388627\nepoch 1147  train loss: 0.20689601774309124  validation loss:0.1623472588602395\nepoch 1148  train loss: 0.2068960208913453  validation loss:0.16234725877659403\nepoch 1149  train loss: 0.2068960240395598  validation loss:0.16234725869294991\nepoch 1150  train loss: 0.2068960271877354  validation loss:0.16234725860930713\nepoch 1151  train loss: 0.2068960303358715  validation loss:0.16234725852566573\nepoch 1152  train loss: 0.20689603348396854  validation loss:0.1623472584420256\nepoch 1153  train loss: 0.20689603663202616  validation loss:0.16234725835838684\nepoch 1154  train loss: 0.20689603978004473  validation loss:0.16234725827474944\nepoch 1155  train loss: 0.20689604292802388  validation loss:0.16234725819111334\nepoch 1156  train loss: 0.20689604607596376  validation loss:0.16234725810747858\nepoch 1157  train loss: 0.2068960492238644  validation loss:0.16234725802384523\nepoch 1158  train loss: 0.20689605237172587  validation loss:0.16234725794021315\nepoch 1159  train loss: 0.20689605551954804  validation loss:0.16234725785658244\nepoch 1160  train loss: 0.20689605866733088  validation loss:0.16234725777295306\nepoch 1161  train loss: 0.20689606181507464  validation loss:0.16234725768932504\nepoch 1162  train loss: 0.20689606496277896  validation loss:0.16234725760569838\nepoch 1163  train loss: 0.20689606811044417  validation loss:0.162347257522073\nepoch 1164  train loss: 0.20689607125807014  validation loss:0.16234725743844897\nepoch 1165  train loss: 0.20689607440565672  validation loss:0.1623472573548263\nepoch 1166  train loss: 0.20689607755320416  validation loss:0.16234725727120497\nepoch 1167  train loss: 0.20689608070071236  validation loss:0.16234725718758497\nepoch 1168  train loss: 0.20689608384818128  validation loss:0.16234725710396636\nepoch 1169  train loss: 0.20689608699561096  validation loss:0.16234725702034905\nepoch 1170  train loss: 0.20689609014300148  validation loss:0.1623472569367331\nepoch 1171  train loss: 0.20689609329035266  validation loss:0.16234725685311846\nepoch 1172  train loss: 0.20689609643766466  validation loss:0.16234725676950523\nepoch 1173  train loss: 0.2068960995849374  validation loss:0.16234725668589325\nepoch 1174  train loss: 0.2068961027321708  validation loss:0.1623472566022827\nepoch 1175  train loss: 0.20689610587936502  validation loss:0.1623472565186734\nepoch 1176  train loss: 0.20689610902651995  validation loss:0.16234725643506548\nepoch 1177  train loss: 0.20689611217363574  validation loss:0.16234725635145889\nepoch 1178  train loss: 0.20689611532071225  validation loss:0.16234725626785365\nepoch 1179  train loss: 0.20689611846774958  validation loss:0.16234725618424978\nepoch 1180  train loss: 0.20689612161474763  validation loss:0.1623472561006472\nepoch 1181  train loss: 0.2068961247617064  validation loss:0.162347256017046\nepoch 1182  train loss: 0.20689612790862588  validation loss:0.16234725593344612\nepoch 1183  train loss: 0.20689613105550628  validation loss:0.16234725584984763\nepoch 1184  train loss: 0.20689613420234748  validation loss:0.16234725576625045\nepoch 1185  train loss: 0.20689613734914936  validation loss:0.16234725568265454\nepoch 1186  train loss: 0.20689614049591198  validation loss:0.16234725559906005\nepoch 1187  train loss: 0.20689614364263534  validation loss:0.1623472555154669\nepoch 1188  train loss: 0.20689614678931953  validation loss:0.162347255431875\nepoch 1189  train loss: 0.2068961499359645  validation loss:0.16234725534828456\nepoch 1190  train loss: 0.20689615308257017  validation loss:0.16234725526469543\nepoch 1191  train loss: 0.20689615622913668  validation loss:0.1623472551811076\nepoch 1192  train loss: 0.2068961593756639  validation loss:0.16234725509752113\nepoch 1193  train loss: 0.20689616252215196  validation loss:0.162347255013936\nepoch 1194  train loss: 0.20689616566860075  validation loss:0.16234725493035218\nepoch 1195  train loss: 0.20689616881501036  validation loss:0.1623472548467697\nepoch 1196  train loss: 0.20689617196138063  validation loss:0.16234725476318862\nepoch 1197  train loss: 0.20689617510771183  validation loss:0.1623472546796089\nepoch 1198  train loss: 0.2068961782540037  validation loss:0.16234725459603044\nepoch 1199  train loss: 0.2068961814002563  validation loss:0.16234725451245335\nepoch 1200  train loss: 0.20689618454646977  validation loss:0.16234725442887765\nepoch 1201  train loss: 0.20689618769264412  validation loss:0.16234725434530323\nepoch 1202  train loss: 0.20689619083877908  validation loss:0.1623472542617302\nepoch 1203  train loss: 0.2068961939848749  validation loss:0.16234725417815843\nepoch 1204  train loss: 0.20689619713093146  validation loss:0.16234725409458806\nepoch 1205  train loss: 0.20689620027694877  validation loss:0.16234725401101902\nepoch 1206  train loss: 0.20689620342292697  validation loss:0.1623472539274513\nepoch 1207  train loss: 0.20689620656886593  validation loss:0.16234725384388496\nepoch 1208  train loss: 0.20689620971476558  validation loss:0.16234725376031994\nepoch 1209  train loss: 0.20689621286062604  validation loss:0.16234725367675626\nepoch 1210  train loss: 0.20689621600644734  validation loss:0.16234725359319388\nepoch 1211  train loss: 0.20689621915222942  validation loss:0.16234725350963292\nepoch 1212  train loss: 0.2068962222979723  validation loss:0.16234725342607323\nepoch 1213  train loss: 0.20689622544367586  validation loss:0.16234725334251493\nepoch 1214  train loss: 0.20689622858934034  validation loss:0.16234725325895796\nepoch 1215  train loss: 0.20689623173496546  validation loss:0.16234725317540236\nepoch 1216  train loss: 0.20689623488055145  validation loss:0.16234725309184805\nepoch 1217  train loss: 0.20689623802609824  validation loss:0.162347253008295\nepoch 1218  train loss: 0.20689624117160596  validation loss:0.16234725292474336\nepoch 1219  train loss: 0.20689624431707426  validation loss:0.16234725284119308\nepoch 1220  train loss: 0.20689624746250346  validation loss:0.16234725275764417\nepoch 1221  train loss: 0.20689625060789332  validation loss:0.16234725267409655\nepoch 1222  train loss: 0.20689625375324402  validation loss:0.16234725259055033\nepoch 1223  train loss: 0.20689625689855548  validation loss:0.16234725250700538\nepoch 1224  train loss: 0.20689626004382794  validation loss:0.1623472524234618\nepoch 1225  train loss: 0.20689626318906112  validation loss:0.16234725233991956\nepoch 1226  train loss: 0.20689626633425498  validation loss:0.16234725225637867\nepoch 1227  train loss: 0.20689626947940962  validation loss:0.16234725217283913\nepoch 1228  train loss: 0.20689627262452528  validation loss:0.16234725208930087\nepoch 1229  train loss: 0.20689627576960146  validation loss:0.16234725200576403\nepoch 1230  train loss: 0.20689627891463866  validation loss:0.16234725192222843\nepoch 1231  train loss: 0.20689628205963656  validation loss:0.16234725183869428\nepoch 1232  train loss: 0.20689628520459521  validation loss:0.16234725175516138\nepoch 1233  train loss: 0.2068962883495147  validation loss:0.16234725167162986\nepoch 1234  train loss: 0.20689629149439512  validation loss:0.16234725158809965\nepoch 1235  train loss: 0.20689629463923612  validation loss:0.1623472515045708\nepoch 1236  train loss: 0.20689629778403812  validation loss:0.16234725142104334\nepoch 1237  train loss: 0.20689630092880093  validation loss:0.16234725133751712\nepoch 1238  train loss: 0.20689630407352436  validation loss:0.1623472512539923\nepoch 1239  train loss: 0.20689630721820867  validation loss:0.1623472511704688\nepoch 1240  train loss: 0.2068963103628539  validation loss:0.16234725108694664\nepoch 1241  train loss: 0.2068963135074597  validation loss:0.16234725100342584\nepoch 1242  train loss: 0.20689631665202649  validation loss:0.16234725091990637\nepoch 1243  train loss: 0.206896319796554  validation loss:0.16234725083638823\nepoch 1244  train loss: 0.2068963229410425  validation loss:0.1623472507528714\nepoch 1245  train loss: 0.20689632608549166  validation loss:0.16234725066935596\nepoch 1246  train loss: 0.20689632922990167  validation loss:0.16234725058584187\nepoch 1247  train loss: 0.20689633237427243  validation loss:0.16234725050232904\nepoch 1248  train loss: 0.206896335518604  validation loss:0.16234725041881767\nepoch 1249  train loss: 0.20689633866289647  validation loss:0.16234725033530756\nepoch 1250  train loss: 0.20689634180714966  validation loss:0.16234725025179877\nepoch 1251  train loss: 0.2068963449513637  validation loss:0.16234725016829138\nepoch 1252  train loss: 0.2068963480955386  validation loss:0.16234725008478526\nepoch 1253  train loss: 0.20689635123967434  validation loss:0.16234725000128059\nepoch 1254  train loss: 0.20689635438377074  validation loss:0.1623472499177771\nepoch 1255  train loss: 0.20689635752782815  validation loss:0.16234724983427506\nepoch 1256  train loss: 0.20689636067184616  validation loss:0.1623472497507743\nepoch 1257  train loss: 0.20689636381582518  validation loss:0.16234724966727496\nepoch 1258  train loss: 0.20689636695976496  validation loss:0.16234724958377691\nepoch 1259  train loss: 0.20689637010366552  validation loss:0.1623472495002802\nepoch 1260  train loss: 0.2068963732475269  validation loss:0.16234724941678488\nepoch 1261  train loss: 0.2068963763913491  validation loss:0.16234724933329084\nepoch 1262  train loss: 0.2068963795351322  validation loss:0.16234724924979818\nepoch 1263  train loss: 0.20689638267887608  validation loss:0.1623472491663068\nepoch 1264  train loss: 0.20689638582258074  validation loss:0.16234724908281678\nepoch 1265  train loss: 0.2068963889662462  validation loss:0.16234724899932815\nepoch 1266  train loss: 0.2068963921098726  validation loss:0.1623472489158408\nepoch 1267  train loss: 0.2068963952534597  validation loss:0.1623472488323548\nepoch 1268  train loss: 0.20689639839700774  validation loss:0.16234724874887016\nepoch 1269  train loss: 0.20689640154051658  validation loss:0.1623472486653868\nepoch 1270  train loss: 0.20689640468398618  validation loss:0.16234724858190483\nepoch 1271  train loss: 0.20689640782741672  validation loss:0.16234724849842416\nepoch 1272  train loss: 0.2068964109708079  validation loss:0.16234724841494488\nepoch 1273  train loss: 0.20689641411416007  validation loss:0.16234724833146696\nepoch 1274  train loss: 0.20689641725747304  validation loss:0.16234724824799032\nepoch 1275  train loss: 0.2068964204007468  validation loss:0.162347248164515\nepoch 1276  train loss: 0.2068964235439815  validation loss:0.1623472480810411\nepoch 1277  train loss: 0.20689642668717698  validation loss:0.16234724799756847\nepoch 1278  train loss: 0.20689642983033318  validation loss:0.1623472479140972\nepoch 1279  train loss: 0.20689643297345026  validation loss:0.16234724783062732\nepoch 1280  train loss: 0.20689643611652822  validation loss:0.1623472477471587\nepoch 1281  train loss: 0.206896439259567  validation loss:0.16234724766369144\nepoch 1282  train loss: 0.20689644240256663  validation loss:0.1623472475802255\nepoch 1283  train loss: 0.2068964455455271  validation loss:0.1623472474967609\nepoch 1284  train loss: 0.2068964486884484  validation loss:0.1623472474132977\nepoch 1285  train loss: 0.20689645183133062  validation loss:0.1623472473298358\nepoch 1286  train loss: 0.20689645497417353  validation loss:0.16234724724637528\nepoch 1287  train loss: 0.20689645811697735  validation loss:0.16234724716291604\nepoch 1288  train loss: 0.20689646125974198  validation loss:0.16234724707945816\n","name":"stdout"},{"output_type":"stream","text":"epoch 1289  train loss: 0.20689646440246737  validation loss:0.16234724699600162\nepoch 1290  train loss: 0.20689646754515387  validation loss:0.16234724691254646\nepoch 1291  train loss: 0.20689647068780107  validation loss:0.16234724682909257\nepoch 1292  train loss: 0.2068964738304091  validation loss:0.16234724674564002\nepoch 1293  train loss: 0.20689647697297786  validation loss:0.1623472466621888\nepoch 1294  train loss: 0.20689648011550757  validation loss:0.16234724657873897\nepoch 1295  train loss: 0.20689648325799817  validation loss:0.16234724649529045\nepoch 1296  train loss: 0.20689648640044953  validation loss:0.16234724641184328\nepoch 1297  train loss: 0.20689648954286183  validation loss:0.16234724632839745\nepoch 1298  train loss: 0.20689649268523494  validation loss:0.16234724624495298\nepoch 1299  train loss: 0.20689649582756886  validation loss:0.16234724616150978\nepoch 1300  train loss: 0.2068964989698636  validation loss:0.16234724607806802\nepoch 1301  train loss: 0.20689650211211927  validation loss:0.16234724599462752\nepoch 1302  train loss: 0.2068965052543357  validation loss:0.16234724591118838\nepoch 1303  train loss: 0.2068965083965131  validation loss:0.16234724582775054\nepoch 1304  train loss: 0.20689651153865118  validation loss:0.16234724574431408\nepoch 1305  train loss: 0.20689651468075038  validation loss:0.16234724566087896\nepoch 1306  train loss: 0.20689651782281024  validation loss:0.16234724557744515\nepoch 1307  train loss: 0.20689652096483083  validation loss:0.1623472454940127\nepoch 1308  train loss: 0.20689652410681247  validation loss:0.1623472454105816\nepoch 1309  train loss: 0.20689652724875493  validation loss:0.1623472453271518\nepoch 1310  train loss: 0.20689653039065822  validation loss:0.16234724524372338\nepoch 1311  train loss: 0.20689653353252244  validation loss:0.16234724516029625\nepoch 1312  train loss: 0.20689653667434746  validation loss:0.1623472450768705\nepoch 1313  train loss: 0.2068965398161333  validation loss:0.16234724499344605\nepoch 1314  train loss: 0.2068965429578801  validation loss:0.16234724491002298\nepoch 1315  train loss: 0.20689654609958763  validation loss:0.1623472448266012\nepoch 1316  train loss: 0.2068965492412561  validation loss:0.1623472447431808\nepoch 1317  train loss: 0.20689655238288535  validation loss:0.16234724465976177\nepoch 1318  train loss: 0.20689655552447558  validation loss:0.162347244576344\nepoch 1319  train loss: 0.20689655866602663  validation loss:0.16234724449292756\nepoch 1320  train loss: 0.2068965618075385  validation loss:0.1623472444095125\nepoch 1321  train loss: 0.2068965649490113  validation loss:0.1623472443260988\nepoch 1322  train loss: 0.20689656809044485  validation loss:0.1623472442426864\nepoch 1323  train loss: 0.2068965712318394  validation loss:0.16234724415927537\nepoch 1324  train loss: 0.2068965743731948  validation loss:0.1623472440758657\nepoch 1325  train loss: 0.20689657751451102  validation loss:0.16234724399245729\nepoch 1326  train loss: 0.2068965806557881  validation loss:0.16234724390905025\nepoch 1327  train loss: 0.20689658379702594  validation loss:0.16234724382564458\nepoch 1328  train loss: 0.206896586938225  validation loss:0.1623472437422402\nepoch 1329  train loss: 0.20689659007938468  validation loss:0.1623472436588372\nepoch 1330  train loss: 0.2068965932205052  validation loss:0.16234724357543556\nepoch 1331  train loss: 0.20689659636158655  validation loss:0.16234724349203514\nepoch 1332  train loss: 0.20689659950262887  validation loss:0.16234724340863615\nepoch 1333  train loss: 0.2068966026436323  validation loss:0.16234724332523842\nepoch 1334  train loss: 0.20689660578459604  validation loss:0.1623472432418421\nepoch 1335  train loss: 0.2068966089255211  validation loss:0.1623472431584471\nepoch 1336  train loss: 0.20689661206640686  validation loss:0.16234724307505347\nepoch 1337  train loss: 0.2068966152072534  validation loss:0.16234724299166117\nepoch 1338  train loss: 0.20689661834806117  validation loss:0.16234724290827016\nepoch 1339  train loss: 0.20689662148882956  validation loss:0.16234724282488053\nepoch 1340  train loss: 0.20689662462955893  validation loss:0.16234724274149223\nepoch 1341  train loss: 0.20689662777024903  validation loss:0.16234724265810518\nepoch 1342  train loss: 0.20689663091090013  validation loss:0.16234724257471958\nepoch 1343  train loss: 0.20689663405151196  validation loss:0.16234724249133528\nepoch 1344  train loss: 0.20689663719208481  validation loss:0.1623472424079523\nepoch 1345  train loss: 0.20689664033261854  validation loss:0.16234724232457068\nepoch 1346  train loss: 0.20689664347311304  validation loss:0.1623472422411904\nepoch 1347  train loss: 0.20689664661356857  validation loss:0.16234724215781152\nepoch 1348  train loss: 0.2068966497539849  validation loss:0.16234724207443385\nepoch 1349  train loss: 0.2068966528943622  validation loss:0.16234724199105757\nepoch 1350  train loss: 0.2068966560347002  validation loss:0.16234724190768265\nepoch 1351  train loss: 0.20689665917499916  validation loss:0.162347241824309\nepoch 1352  train loss: 0.206896662315259  validation loss:0.1623472417409368\nepoch 1353  train loss: 0.20689666545547994  validation loss:0.16234724165756587\nepoch 1354  train loss: 0.20689666859566155  validation loss:0.16234724157419625\nepoch 1355  train loss: 0.20689667173580412  validation loss:0.16234724149082802\nepoch 1356  train loss: 0.20689667487590746  validation loss:0.16234724140746112\nepoch 1357  train loss: 0.20689667801597175  validation loss:0.16234724132409548\nepoch 1358  train loss: 0.20689668115599696  validation loss:0.16234724124073127\nepoch 1359  train loss: 0.20689668429598307  validation loss:0.16234724115736834\nepoch 1360  train loss: 0.20689668743593007  validation loss:0.1623472410740068\nepoch 1361  train loss: 0.20689669057583793  validation loss:0.16234724099064657\nepoch 1362  train loss: 0.2068966937157068  validation loss:0.16234724090728764\nepoch 1363  train loss: 0.20689669685553636  validation loss:0.1623472408239301\nepoch 1364  train loss: 0.20689669999532703  validation loss:0.16234724074057388\nepoch 1365  train loss: 0.2068967031350783  validation loss:0.162347240657219\nepoch 1366  train loss: 0.2068967062747908  validation loss:0.1623472405738654\nepoch 1367  train loss: 0.20689670941446392  validation loss:0.16234724049051322\nepoch 1368  train loss: 0.20689671255409806  validation loss:0.16234724040716236\nepoch 1369  train loss: 0.206896715693693  validation loss:0.16234724032381284\nepoch 1370  train loss: 0.20689671883324898  validation loss:0.1623472402404646\nepoch 1371  train loss: 0.20689672197276593  validation loss:0.1623472401571177\nepoch 1372  train loss: 0.2068967251122436  validation loss:0.16234724007377224\nepoch 1373  train loss: 0.20689672825168223  validation loss:0.16234723999042802\nepoch 1374  train loss: 0.20689673139108178  validation loss:0.1623472399070852\nepoch 1375  train loss: 0.20689673453044213  validation loss:0.16234723982374363\nepoch 1376  train loss: 0.20689673766976346  validation loss:0.1623472397404035\nepoch 1377  train loss: 0.2068967408090458  validation loss:0.16234723965706463\nepoch 1378  train loss: 0.20689674394828894  validation loss:0.16234723957372718\nepoch 1379  train loss: 0.20689674708749292  validation loss:0.16234723949039098\nepoch 1380  train loss: 0.206896750226658  validation loss:0.16234723940705617\nepoch 1381  train loss: 0.20689675336578375  validation loss:0.16234723932372266\nepoch 1382  train loss: 0.2068967565048706  validation loss:0.1623472392403905\nepoch 1383  train loss: 0.20689675964391824  validation loss:0.16234723915705965\nepoch 1384  train loss: 0.20689676278292676  validation loss:0.1623472390737302\nepoch 1385  train loss: 0.2068967659218963  validation loss:0.16234723899040202\nepoch 1386  train loss: 0.20689676906082666  validation loss:0.1623472389070752\nepoch 1387  train loss: 0.20689677219971797  validation loss:0.16234723882374974\nepoch 1388  train loss: 0.20689677533857026  validation loss:0.16234723874042561\nepoch 1389  train loss: 0.20689677847738333  validation loss:0.1623472386571028\nepoch 1390  train loss: 0.20689678161615738  validation loss:0.16234723857378128\nepoch 1391  train loss: 0.20689678475489232  validation loss:0.16234723849046115\nepoch 1392  train loss: 0.20689678789358817  validation loss:0.16234723840714232\nepoch 1393  train loss: 0.20689679103224493  validation loss:0.1623472383238249\nepoch 1394  train loss: 0.20689679417086262  validation loss:0.16234723824050873\nepoch 1395  train loss: 0.20689679730944133  validation loss:0.16234723815719393\nepoch 1396  train loss: 0.20689680044798076  validation loss:0.16234723807388057\nepoch 1397  train loss: 0.20689680358648124  validation loss:0.16234723799056844\nepoch 1398  train loss: 0.20689680672494273  validation loss:0.16234723790725764\nepoch 1399  train loss: 0.20689680986336492  validation loss:0.16234723782394822\nepoch 1400  train loss: 0.20689681300174811  validation loss:0.1623472377406401\nepoch 1401  train loss: 0.20689681614009217  validation loss:0.1623472376573333\nepoch 1402  train loss: 0.20689681927839723  validation loss:0.16234723757402786\nepoch 1403  train loss: 0.2068968224166633  validation loss:0.1623472374907238\nepoch 1404  train loss: 0.20689682555489014  validation loss:0.16234723740742107\nepoch 1405  train loss: 0.20689682869307782  validation loss:0.16234723732411963\nepoch 1406  train loss: 0.20689683183122667  validation loss:0.16234723724081954\nepoch 1407  train loss: 0.20689683496933622  validation loss:0.16234723715752072\nepoch 1408  train loss: 0.2068968381074069  validation loss:0.16234723707422333\nepoch 1409  train loss: 0.20689684124543833  validation loss:0.16234723699092724\nepoch 1410  train loss: 0.20689684438343076  validation loss:0.1623472369076325\nepoch 1411  train loss: 0.20689684752138413  validation loss:0.1623472368243391\nepoch 1412  train loss: 0.20689685065929841  validation loss:0.16234723674104704\nepoch 1413  train loss: 0.2068968537971736  validation loss:0.16234723665775627\nepoch 1414  train loss: 0.20689685693500987  validation loss:0.16234723657446684\nepoch 1415  train loss: 0.2068968600728068  validation loss:0.1623472364911788\nepoch 1416  train loss: 0.2068968632105649  validation loss:0.16234723640789206\nepoch 1417  train loss: 0.20689686634828375  validation loss:0.16234723632460668\nepoch 1418  train loss: 0.20689686948596367  validation loss:0.16234723624132258\nepoch 1419  train loss: 0.2068968726236045  validation loss:0.16234723615803986\nepoch 1420  train loss: 0.20689687576120616  validation loss:0.1623472360747585\nepoch 1421  train loss: 0.20689687889876884  validation loss:0.16234723599147846\nepoch 1422  train loss: 0.20689688203629245  validation loss:0.16234723590819972\nepoch 1423  train loss: 0.20689688517377705  validation loss:0.16234723582492233\nepoch 1424  train loss: 0.2068968883112224  validation loss:0.16234723574164628\nepoch 1425  train loss: 0.20689689144862897  validation loss:0.16234723565837153\nepoch 1426  train loss: 0.20689689458599633  validation loss:0.1623472355750982\nepoch 1427  train loss: 0.2068968977233245  validation loss:0.16234723549182617\nepoch 1428  train loss: 0.2068969008606138  validation loss:0.16234723540855542\nepoch 1429  train loss: 0.20689690399786387  validation loss:0.16234723532528608\nepoch 1430  train loss: 0.20689690713507503  validation loss:0.16234723524201805\nepoch 1431  train loss: 0.2068969102722471  validation loss:0.16234723515875135\nepoch 1432  train loss: 0.20689691340938005  validation loss:0.16234723507548598\nepoch 1433  train loss: 0.206896916546474  validation loss:0.16234723499222195\nepoch 1434  train loss: 0.2068969196835288  validation loss:0.16234723490895928\nepoch 1435  train loss: 0.20689692282054473  validation loss:0.1623472348256979\nepoch 1436  train loss: 0.2068969259575214  validation loss:0.16234723474243784\nepoch 1437  train loss: 0.20689692909445914  validation loss:0.1623472346591792\nepoch 1438  train loss: 0.20689693223135772  validation loss:0.16234723457592184\nepoch 1439  train loss: 0.2068969353682174  validation loss:0.16234723449266575\nepoch 1440  train loss: 0.20689693850503793  validation loss:0.16234723440941115\nepoch 1441  train loss: 0.20689694164181954  validation loss:0.16234723432615775\nepoch 1442  train loss: 0.2068969447785618  validation loss:0.16234723424290579\nepoch 1443  train loss: 0.20689694791526528  validation loss:0.1623472341596551\nepoch 1444  train loss: 0.20689695105192954  validation loss:0.16234723407640572\nepoch 1445  train loss: 0.20689695418855486  validation loss:0.16234723399315776\nepoch 1446  train loss: 0.20689695732514116  validation loss:0.16234723390991107\nepoch 1447  train loss: 0.20689696046168823  validation loss:0.16234723382666574\nepoch 1448  train loss: 0.2068969635981965  validation loss:0.16234723374342172\n","name":"stdout"},{"output_type":"stream","text":"epoch 1449  train loss: 0.20689696673466568  validation loss:0.16234723366017906\nepoch 1450  train loss: 0.20689696987109565  validation loss:0.16234723357693776\nepoch 1451  train loss: 0.20689697300748672  validation loss:0.16234723349369776\nepoch 1452  train loss: 0.20689697614383873  validation loss:0.16234723341045912\nepoch 1453  train loss: 0.2068969792801516  validation loss:0.16234723332722178\nepoch 1454  train loss: 0.20689698241642548  validation loss:0.16234723324398576\nepoch 1455  train loss: 0.20689698555266026  validation loss:0.1623472331607511\nepoch 1456  train loss: 0.20689698868885614  validation loss:0.1623472330775178\nepoch 1457  train loss: 0.2068969918250129  validation loss:0.16234723299428583\nepoch 1458  train loss: 0.20689699496113056  validation loss:0.1623472329110552\nepoch 1459  train loss: 0.20689699809720927  validation loss:0.1623472328278259\nepoch 1460  train loss: 0.20689700123324897  validation loss:0.1623472327445979\nepoch 1461  train loss: 0.20689700436924952  validation loss:0.16234723266137124\nepoch 1462  train loss: 0.206897007505211  validation loss:0.16234723257814593\nepoch 1463  train loss: 0.2068970106411335  validation loss:0.16234723249492197\nepoch 1464  train loss: 0.206897013777017  validation loss:0.16234723241169938\nepoch 1465  train loss: 0.20689701691286158  validation loss:0.162347232328478\nepoch 1466  train loss: 0.20689702004866697  validation loss:0.16234723224525804\nepoch 1467  train loss: 0.20689702318443345  validation loss:0.1623472321620394\nepoch 1468  train loss: 0.20689702632016072  validation loss:0.1623472320788221\nepoch 1469  train loss: 0.2068970294558491  validation loss:0.1623472319956061\nepoch 1470  train loss: 0.2068970325914984  validation loss:0.1623472319123915\nepoch 1471  train loss: 0.2068970357271086  validation loss:0.1623472318291782\nepoch 1472  train loss: 0.20689703886267988  validation loss:0.16234723174596624\nepoch 1473  train loss: 0.20689704199821213  validation loss:0.16234723166275558\nepoch 1474  train loss: 0.20689704513370522  validation loss:0.1623472315795463\nepoch 1475  train loss: 0.20689704826915917  validation loss:0.16234723149633837\nepoch 1476  train loss: 0.20689705140457454  validation loss:0.1623472314131317\nepoch 1477  train loss: 0.20689705453995064  validation loss:0.16234723132992646\nepoch 1478  train loss: 0.20689705767528768  validation loss:0.16234723124672248\nepoch 1479  train loss: 0.2068970608105858  validation loss:0.16234723116351982\nepoch 1480  train loss: 0.20689706394584484  validation loss:0.16234723108031854\nepoch 1481  train loss: 0.2068970670810648  validation loss:0.16234723099711856\nepoch 1482  train loss: 0.2068970702162457  validation loss:0.16234723091391995\nepoch 1483  train loss: 0.20689707335138774  validation loss:0.1623472308307227\nepoch 1484  train loss: 0.20689707648649067  validation loss:0.16234723074752674\nepoch 1485  train loss: 0.2068970796215545  validation loss:0.16234723066433213\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-88-35d0cf55c2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-87-d88033317ba8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, validation_data, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0;31m#self.weights[f'{\"w_\"}{j}']/=trainX.shape[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;31m#self.biases[f'{\"b_\"}{j}']/=trainX.shape[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#batch loss aggregation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"_uuid":"6884896dfe9b02e3d0b7cc564d15df6285cf9c0e"},"cell_type":"markdown","source":"<h1><strong>Visualization of the Model History</strong></h1>"},{"metadata":{"trusted":true,"_uuid":"dd37a66c392d1e3d2c89fa51b63786e08e9d4815"},"cell_type":"code","source":"# Plot training & validation loss values/\nplt.plot(model.history['train_loss'])\nplt.plot(model.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"748f0c8c6d2d7f8fe2f03af66a3d38a3a43a8904"},"cell_type":"markdown","source":"<h1><strong>Testing it out</strong></h1>"},{"metadata":{"_uuid":"cd4688369cd287fa2d78e4374362fa243eaeb6c9"},"cell_type":"markdown","source":"The accuracy does'nt seem to be great. So let's see whether it did better than at least a linear model."},{"metadata":{"trusted":true},"cell_type":"code","source":"yout=model.predict(data[:,0].reshape((-1,1)))\nplt.scatter(data[:,0],yout,marker='.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(data[:,0], data[:,1], marker='.',)\nplt.scatter(data[:,0],yout,marker='.',color=[\"orange\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}