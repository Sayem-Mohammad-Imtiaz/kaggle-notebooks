{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":" # Multiclass Classification - Ensemble Method\n \n ## Introduction\n \nThese data are the results of a chemical analysis of **wines grown** in the same region in Italy but derived from three different cultivars. \nThe analysis determined the quantities of 13 constituents found in each of the three types of wines.\n \n ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Import dependent libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic libraries\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# SKlearn related libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, zero_one_loss, hamming_loss\n\n# Boosting technique algorithm\nimport xgboost as xgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Load the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset path\nDATA_PATH = \"/kaggle/input/wineuci/Wine.csv\"\n\ncolumns = ['class','alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n    'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n    'proanthocyanins', 'color_intensity', 'hue',\n    'od280/od315_of_diluted_wines', 'proline']\n\nwine_data = pd.read_csv(DATA_PATH, names=columns, header=0)\n\nwine_data.info()\nprint(\"==\"*40)\nwine_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the label into 0, 1, 2\ndef trans_class(class_label):\n    return int(class_label) - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data['class'] = wine_data['class'].apply(trans_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(wine_data['class'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Is there any null values : {wine_data.isnull().sum().any()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='whitegrid', palette='muted')\n\n# Pairplot to see the attribute comparison\nfig, ax = plt.subplots(1,2,figsize=(12,6))\n\nsns.distplot(wine_data['alcohol'], kde=True, hist=True, ax=ax[0])\n\nsns.distplot(wine_data['malic_acid'], kde=True, hist=True, ax=ax[1])\n\nplt.show()\n\ng = sns.jointplot(x=wine_data['alcohol'], y=wine_data['malic_acid'], color='r')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data['class'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Prepare dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making X and Y data from the dataset\nX = wine_data.loc[:, wine_data.columns != 'class'].values\ny = wine_data['class'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train shape : {X.shape}, Label : { y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(f\"Train shape : {X_train.shape}, Label : { y_train.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DMatrix\n\nXGB algorithm expects the data in 'DMatrix' form. We will use the data to construct Dmatrix object.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dm_train = xgb.DMatrix(data=X_train, label=y_train)\ndm_test = xgb.DMatrix(data=X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Key parameters\nparams = {\n    'max_depth': 6,\n    'min_child_weight':1,\n    'objective': 'multi:softmax',\n    'subsample':1,\n    'colsample_bytree':1,\n    'num_class': 3,\n    'n_gpus': 0\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf = xgb.train(params, dm_train) # Train the model with dataset\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\npredictions = xgb_clf.predict(dm_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Classification Report \\n {}\".format(classification_report(y_test, predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Misclassification Rate\n\nIn multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.\n\nIf normalize is True, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Misclassification rate {}\".format(zero_one_loss(y_test, predictions, normalize=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hamming Loss\n\nIn multiclass classification, the Hamming loss corresponds to the Hamming distance between y_true and y_pred which is equivalent to the subset zero_one_loss function, when normalize parameter is set to True.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Hamming rate {:.2f}\".format(hamming_loss(y_test, predictions)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}