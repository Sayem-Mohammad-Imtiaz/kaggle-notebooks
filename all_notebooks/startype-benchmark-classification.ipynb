{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport scipy\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/star-type-classification/Stars.csv')\ndata.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_features = ['Temperature', 'L', 'R', 'A_M']\ndiscrete_features = ['Color', 'Spectral_Class']\nstar_type = 'Type'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Color = [c.replace('-',' ').lower() for c in data.Color]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_dict = dict()\nfor sc in data.Color.unique():\n    if sc not in color_dict:\n        color_dict[sc] = len(color_dict)\ncolor_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Let's have a look to the continuos features","metadata":{}},{"cell_type":"code","source":"g = sns.PairGrid(data[continuous_features])\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot, legend=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"better to take the logarithm for Temperature, L and R","metadata":{}},{"cell_type":"code","source":"data['Temperature'] = np.log10(data['Temperature'])\ndata['L'] = np.log10(data['L'])\ndata['R'] = np.log10(data['R'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.PairGrid(data[continuous_features])\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot, legend=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## .. and the discrete features","metadata":{}},{"cell_type":"code","source":"ax=sns.displot(data[discrete_features[0]])\nax.set_xticklabels(rotation=90)\nax.set(title=discrete_features[0])\nax=sns.displot(data[discrete_features[1]])\nax.set(title=discrete_features[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The colors haven't been considered in this analysis (maybe as an update) ","metadata":{}},{"cell_type":"code","source":"color_dict = {'red': 0,\n 'blue white': 1,\n 'white': 2,\n 'yellowish white': 3,\n 'pale yellow orange': 4,\n 'blue': 5,\n 'whitish': 6,\n 'yellow white': 7,\n 'orange': 8,\n 'white yellow': 9,\n 'yellowish': 10,\n 'orange red': 11}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spectral_class_dict = dict()\nfor sc in data[discrete_features[1]].unique():\n    if sc not in spectral_class_dict:\n        spectral_class_dict[sc] = len(spectral_class_dict)\nspectral_class_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Star Types","metadata":{}},{"cell_type":"code","source":"star_type_dict = {\n    0: 'Red Dwarf',\n    1: 'Brown Dwarf',\n    2: 'White Dwarf',\n    3: 'Main Sequence',\n    4: 'Super Giants',\n    5: 'Hyper Giants'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=sns.displot(data[star_type].map(star_type_dict))\nax.set_xticklabels(rotation=90)\nax.set(title='Star Types')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_of_randomization = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"spectral_array = np.array([])\nfor t in data[discrete_features[1]].map(spectral_class_dict):\n    s = np.zeros(len(spectral_class_dict))\n    s[t] = 1.0\n    spectral_array = np.append( spectral_array, s) \nspectral_array = spectral_array.reshape(-1,len(spectral_class_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_array = np.hstack((data[continuous_features].to_numpy(),spectral_array)) #no colors for now","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = continuous_features+list(spectral_class_dict.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_array = np.array([])\nfor t in data[star_type]:\n    s = np.zeros(len(star_type_dict))\n    s[t] = 1.0\n    output_array = np.append( output_array, s) \noutput_array = output_array.reshape(-1,len(star_type_dict))\noutput_array.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_type_1d = data[star_type]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Neural network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FUNCTION TO PLOT THE TRAINING\ndef plot_training(fit, evaluation):\n    best_epoch = fit.epoch[fit.history['val_loss'].index(min(fit.history['val_loss']))]\n    fig, ax = plt.subplots(2,1,figsize=(3,5))\n    \n    ax[0].plot(fit.epoch,fit.history['val_loss'],'.-',color='red', label='validation')\n    ax[0].plot(fit.epoch,fit.history['loss'],'.-',color='orange', label='train')\n    ax[0].set(ylabel='Loss',ylim=[0,1])\n    ax[0].axvspan(best_epoch-0.5,best_epoch+0.5, alpha=0.5, color='red')\n    #ax[0].autoscale(False)\n    ax[0].scatter(best_epoch, evaluation[0],s=2, zorder=1,color='green')\n    ax[0].legend()\n    \n    ax[1].plot(fit.epoch,fit.history['val_accuracy'],'.-',color='red', label='validation')\n    ax[1].plot(fit.epoch,fit.history['accuracy'],'.-',color='orange', label='train')\n    ax[1].set(ylabel='Accuracy',ylim=[0,1])\n    ax[1].axvspan(best_epoch-0.5,best_epoch+0.5, alpha=0.5, color='red')\n    #ax[1].autoscale(False)\n    ax[1].scatter(best_epoch, evaluation[1],s=2, zorder=1,color='green')\n    ax[1].legend()\n    plt.show()\n    print(\"[Best epoch]:\", best_epoch)\n    print(\"[Loss]:\", min(fit.history['val_loss']), \" test:\", evaluation[0])\n    print(\"[Accuracy]:\", max(fit.history['val_accuracy']), \" test:\", evaluation[1])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Z-scoring the continuous features\nnorm_continuous_array = scipy.stats.zscore(data[continuous_features].to_numpy())\n\ninput_array = np.hstack((norm_continuous_array,spectral_array))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 1\nDATASET_SIZE = input_array.shape[0]\nbase_depth = 96\ndropout_prob = 0.4\nactivation_func = tf.nn.leaky_relu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.7 * DATASET_SIZE)//BATCH_SIZE\nval_size = int(0.15 * DATASET_SIZE)//BATCH_SIZE\ntest_size = int(0.15 * DATASET_SIZE)//BATCH_SIZE\n\nprint(\"\\n[Train size]:\",train_size,\"\\n[Valid size]:\", val_size,\"\\n[Test size]:\", test_size )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NN_accuracy = []\nfor i in range(n_of_randomization):\n\n    dataset = tf.data.Dataset.from_tensor_slices( (input_array,output_array) ).shuffle(1000).batch(BATCH_SIZE)\n    train_data = dataset.take(train_size)\n    test_data = dataset.skip(train_size)\n    valid_data = test_data.skip(test_size)\n    test_data = test_data.take(test_size)\n    \n    StarType_Classifier = tf.keras.Sequential([\n        tf.keras.Input(shape=(input_array.shape[1],)),\n        tf.keras.layers.Dense(base_depth,activation=activation_func),\n        tf.keras.layers.Dropout(dropout_prob),\n        tf.keras.layers.Dense(base_depth,activation=activation_func),\n        tf.keras.layers.Dropout(dropout_prob),\n        tf.keras.layers.Dense(base_depth,activation=activation_func),\n        tf.keras.layers.Dropout(dropout_prob),\n        tf.keras.layers.Dense(len(star_type_dict),activation=tf.nn.softmax)\n    ], name=\"star_type_classifier\")\n\n    StarType_Classifier.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])\n    \n    fit = StarType_Classifier.fit(train_data, epochs=400, validation_data=valid_data,\n                        batch_size=BATCH_SIZE, verbose=False,\n                        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.000001),\n                                   tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0, patience=100, verbose=1, mode='auto', restore_best_weights=True)])\n\n    evaluation = StarType_Classifier.evaluate(test_data)\n    #plot_training(fit, evaluation)\n    NN_accuracy.append( evaluation[1] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(NN_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Random forest","metadata":{}},{"cell_type":"code","source":"feature_importance_df = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.inspection import permutation_importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RF_accuracy = list()\nfor i in range(n_of_randomization):\n    X_train, X_test, y_train, y_test = train_test_split(input_array, output_type_1d, test_size=0.2, random_state=i)\n    random_forest_clf = RandomForestClassifier(max_depth=4, random_state=i)\n    random_forest_clf.fit(X_train,y_train)\n    cross_val_score(random_forest_clf, X_train, y_train, cv=5)\n    RF_accuracy.append( random_forest_clf.score(X_test, y_test) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(RF_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = permutation_importance(random_forest_clf, input_array, output_type_1d, n_repeats=n_of_randomization, random_state=0)\n\nimportances = result.importances_mean\nstd = result.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(input_array.shape[1]):\n    print(\"%d. %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(input_array.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(input_array.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlim([-1, input_array.shape[1]])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df['Random forest']  = pd.Series( importances, index=feature_names )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Decision tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DT_accuracy = list()\nfor i in range(n_of_randomization):\n    X_train, X_test, y_train, y_test = train_test_split(input_array, output_type_1d, test_size=0.2, random_state=i)\n    decision_tree_clf = DecisionTreeClassifier(random_state=i)\n    decision_tree_clf.fit(X_train,y_train)\n    cross_val_score(decision_tree_clf, X_train, y_train, cv=5)\n    DT_accuracy.append( decision_tree_clf.score(X_test, y_test) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(DT_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = permutation_importance(decision_tree_clf, input_array, output_type_1d, n_repeats=n_of_randomization, random_state=0)\n\nimportances = result.importances_mean\nstd = result.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(input_array.shape[1]):\n        print(\"%d. %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(input_array.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(input_array.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlim([-1, input_array.shape[1]])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df['Decision tree'] = pd.Series( importances, index=feature_names )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Gradient boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GB_accuracy = list()\nfor i in range(n_of_randomization):\n    X_train, X_test, y_train, y_test = train_test_split(input_array, output_type_1d, test_size=0.2, random_state=i)\n    gradient_boosting_clf = GradientBoostingClassifier(max_depth=4, random_state=i)\n    gradient_boosting_clf.fit(X_train,y_train)\n    cross_val_score(gradient_boosting_clf, X_train, y_train, cv=5)\n    GB_accuracy.append( gradient_boosting_clf.score(X_test, y_test) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(GB_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = permutation_importance(gradient_boosting_clf, input_array, output_type_1d, n_repeats=n_of_randomization, random_state=0)\n\nimportances = result.importances_mean\nstd = result.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(input_array.shape[1]):\n        print(\"%d. %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(input_array.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(input_array.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlim([-1, input_array.shape[1]])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df['Gradient boosting'] = pd.Series( importances, index=feature_names )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Support Vector Machine","metadata":{}},{"cell_type":"code","source":"from sklearn import svm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_accuracy = list()\nfor i in range(n_of_randomization):\n    X_train, X_test, y_train, y_test = train_test_split(input_array, output_type_1d, test_size=0.2, random_state=i)\n    svm_clf = svm.SVC(gamma='auto')\n    svm_clf.fit(X_train,y_train)\n    cross_val_score(svm_clf, X_train, y_train, cv=5)\n    SVM_accuracy.append( svm_clf.score(X_test, y_test) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(SVM_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = permutation_importance(svm_clf, input_array, output_type_1d, n_repeats=n_of_randomization, random_state=0)\n\nimportances = result.importances_mean\nstd = result.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(input_array.shape[1]):\n        print(\"%d. %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(input_array.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(input_array.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlim([-1, input_array.shape[1]])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df['Support vector machine'] = pd.Series( importances, index=feature_names )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. AdaBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AB_accuracy = list()\nfor i in range(n_of_randomization):\n    X_train, X_test, y_train, y_test = train_test_split(input_array, output_type_1d, test_size=0.2, random_state=i)\n    adaboost_clf = AdaBoostClassifier(n_estimators=100)\n    adaboost_clf.fit(X_train,y_train)\n    cross_val_score(adaboost_clf, X_train, y_train, cv=5)\n    AB_accuracy.append( adaboost_clf.score(X_test, y_test) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(AB_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = permutation_importance(adaboost_clf, input_array, output_type_1d, n_repeats=n_of_randomization, random_state=0)\n\nimportances = result.importances_mean\nstd = result.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(input_array.shape[1]):\n        print(\"%d. %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(input_array.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(input_array.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlim([-1, input_array.shape[1]])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df['AdaBoost'] = pd.Series( importances, index=feature_names )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perfomance Summary and Best Features","metadata":{}},{"cell_type":"code","source":"accuracy_data = pd.DataFrame.from_dict({\n    'AdaBoost': AB_accuracy,\n    'Neural network': NN_accuracy,\n    'Support vector machine': SVM_accuracy,\n    'Gradient boosting': GB_accuracy,\n    'Random forest': RF_accuracy,\n    'Decision tree': DT_accuracy\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.boxplot(x='Model',y='value',data=accuracy_data.melt(var_name='Model'),showfliers=False,ax=ax)\nsns.stripplot(x='Model',y='value',data=accuracy_data.melt(var_name='Model'),ax=ax,color='black')\nax.set(xlabel='Model',ylabel='Accuracy',title='Perfomance')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average accuracy greater than 95% for the majority of the model.\nTop performing model is based on Decision Tree.","metadata":{}},{"cell_type":"code","source":"sorted_models = ['AdaBoost', 'Support vector machine','Gradient boosting','Random forest','Decision tree']\nsorted_models.reverse()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(feature_importance_df[sorted_models], annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best features for predictions are R, A_M and Temperature. The analysis didn't considered the Colors.","metadata":{}}]}