{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# imports \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import confusion_matrix\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, classification_report\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import RandomizedSearchCV\nimport seaborn as sns\nimport pprint\nimport lightgbm as lgb\n#from feature_selector import FeatureSelector\npp = pprint.PrettyPrinter(indent=4)\n%matplotlib inline\npd.set_option('display.max_rows', 500)\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.under_sampling import RandomUnderSampler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading the data\n\n#friday = pd.read_csv('/kaggle/input/Friday-WorkingHours-Afternoon-DDos.csv',low_memory = False)\nwednesday = pd.read_csv('/kaggle/input/Wednesday-workingHours.csv',low_memory = False)\n#friday = friday.rename(str.lstrip, axis='columns')\nwednesday = wednesday.rename(str.lstrip, axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop rows with NA (only done because NA is not defining element in the feature set)\ndf = wednesday\n#df = df.sample(n = 200000)\nprint(df['Label'].unique())\nprint(df.shape)\ndf = df.dropna()\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining important features in the set| not included in feature selection.\nimp = ['Destination Port', 'Flow Duration', 'Total Fwd Packets','Total Backward Packets']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for performing feature selection\nX = df.loc[:,df.columns != \"Label\"]\n#removing first four features considered very important\nX = X.drop(imp,axis = 1)\ny = df.loc[:,df.columns == \"Label\"]\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#class distribution <- classifying feature\ndef ClassDistribution(y):\n    ax = sns.countplot(y['Label'],label=\"Count\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n    print(y.groupby('Label').size())\n\nClassDistribution(y)\n#B, M = y.value_counts()\n#print('Number of Benign: ',B)\n#print('Number of Malignant : ',M)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove columns with more than 75% missing values\ndef RemoveMissing(train):\n    train_missing = (train.isnull().sum() / len(train)).sort_values(ascending = False)\n    #train_missing.head()\n    train_missing = train_missing.index[train_missing > 0.75]\n    all_missing = list(set(train_missing))\n    train.drop(all_missing,axis = 1)\n    print('There are %d columns with more than 75%% missing values' % len(all_missing))\n    return train\n\nX = RemoveMissing(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove columns with only 1 unique value\ndef remove_single_unique_values(dataframe):\n    cols_to_drop = dataframe.nunique()\n    cols_to_drop = cols_to_drop.loc[cols_to_drop.values==1].index\n    print('There are %d columns with only 1 unique value' % len(cols_to_drop))\n    dataframe = dataframe.drop(cols_to_drop,axis=1)\n    return dataframe\n\nX = remove_single_unique_values(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#heatmap\nf,ax = plt.subplots(figsize=(18, 18))\ncorr = X.corr()\nsns.heatmap(corr, annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = np.full((corr.shape[0],), True, dtype=bool)\nto_drop = []\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.7:\n            if columns[j]:\n                columns[j] = False\n                to_drop.append((corr.columns)[j])\nprint(to_drop)\nX = X.drop(to_drop,axis = 1)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add back the important features removed earlier\nfor i in imp:\n    X[i] = df[i]\n\nprint(X.shape)\nprint(X.columns)\n#encoding and scaling stuff.\ncategorical = []\nfor x in X.columns:\n    if X[x].dtype == 'object':\n        categorical.append(x)\n#encoding and scaling\n\nencoder = LabelEncoder()\nfor a in categorical:\n    X[a] = encoder.fit_transform(X[a])\n\n# feature scaling\nscaler = RobustScaler()\nX = scaler.fit_transform(X)\ny = encoder.fit_transform(y.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=123, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DOES NOT ENHANCE PERFORMACE DO NOT RECOMMEND.\n#added in version 14 imputation and undersampling\n#for benchmarking against unaltered data distribution run V13\n\"\"\"\n\n#Impute missing values\nprint(x_train.shape)\nimp = SimpleImputer()\nimp.fit(x_train)\nx_train = imp.transform(x_train)\nx_test = imp.transform(x_test)\nprint(x_train.shape)\n\n# Implement RandomUnderSampler\nrandom_undersampler = RandomUnderSampler()\nx_train, y_train = random_undersampler.fit_sample(x_train, y_train)\nprint(x_train.shape)\n# Shuffle the data\nperms = np.random.permutation(x_train.shape[0])\nx_train = x_train[perms]\ny_train = y_train[perms]\nprint(x_train.shape)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#added in version 14 imputation and undersampling\n#for benchmarking against unaltered data distribution run V13\n# SMOTE + Random Undersampling\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\nover = SMOTE()\nunder = RandomUnderSampler()\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\nx_train, y_train = pipeline.fit_resample(x_train, y_train)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(n_estimators = 10, random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial',n_jobs = 3)\n\nrf_clf = RandomForestClassifier(n_estimators=10, random_state = 43)\n\n#does not support multilabel\n#pca_clf = PCA(n_components='mle')\n\nann_clf = MLPClassifier(hidden_layer_sizes=(13,7),activation = 'logistic')\n\nlda_clf = LinearDiscriminantAnalysis(solver='svd')\n\n#knn_clf = KNeighborsClassifier(n_neighbors=3)\n\nnb_clf = GaussianNB()\n\ndtree_clf = DecisionTreeClassifier()\n\n\nmodels = {\"logreg\": logreg_clf,\n            \"rf\": rf_clf,\n            \"Neural-Network\": ann_clf,\n            \"LDA\" : lda_clf,\n#            \"KNN\" : knn_clf,\n            \"Naive-Bayes\" : nb_clf,\n            \"Decision-Tree\" : dtree_clf}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nScores = []\ndef Utility():\n    for x in models:\n        print(x)\n        score = (cross_val_score(models[x], x_train, y_train, cv=5))\n        Scores.append(score)\n        print(score)\n        \n\nUtility()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in Scores:\n    print(np.mean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#For two classes\n#prints classificationreport , ROC, PR, and CM\ndef plot_confusion_matrix(y_true, y_pred,normalize=True,title=None,cmap=plt.cm.Blues):\n    cm = confusion_matrix(y_true, y_pred)\n    print(cm)\n    classes = np.unique(y_test)\n    np.append(classes,np.unique(y_pred))\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    labels = classes\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=labels, yticklabels=labels,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    return ax\n\n#fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n#y_pred = []\n\nfor name,model in models.items() :\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    #y_prob = model.predict_proba(x_test)[:, 1:]\n    print(name)\n    print(classification_report(y_test, y_pred))\n    plot_confusion_matrix(y_test,y_pred, title = name)\n    \n    #precision, recall,_ = precision_recall_curve(y_test, y_prob)\n    #model_auc_score = roc_auc_score(y_test, y_prob)\n    #fpr, tpr, _ = roc_curve(y_test, y_prob)\n    #axes[0].plot(fpr, tpr, label= f\"{name}, auc = {model_auc_score:.3f}\")\n    #axes[1].plot(recall, precision, label= f\"{name}\")\n\"\"\"\naxes[0].legend(loc=\"lower right\")\naxes[0].set_xlabel(\"FPR\")\naxes[0].set_ylabel(\"TPR\")\naxes[0].set_title(\"ROC curve\")\naxes[1].legend()\naxes[1].set_xlabel(\"recall\")\naxes[1].set_ylabel(\"precision\")\naxes[1].set_title(\"PR curve\")\nplt.tight_layout()\nplt.show()\n\"\"\"\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy.interpolate import interp1d\n#Multiclass plots of PR and ROC\ndef Multiclassplots(classifier,Name):\n    print(classifier)\n    #print(Name)\n    #print(len(set(y.Label)))\n    \n    n_classes = len(set(y))\n\n    Y = label_binarize(y, classes=[*range(n_classes)])\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42,stratify = Y)\n\n    fig,ax = plt.subplots(1,2, figsize = (14,6))\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(X_train,y_train)\n    y_score = clf.predict_proba(X_test)\n    precision = dict()\n    recall = dict()\n    for i in range(n_classes):\n        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_score[:,i])\n        ax[0].plot(recall[i],precision[i], lw=2, label='class {}'.format(i))\n\n    fpr = dict()\n    tpr = dict()\n\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i],\n                                      y_score[:,i])\n        auc_score = roc_auc_score(y_test[:,i], y_score[:,i])\n        ax[1].plot(fpr[i], tpr[i], lw=2, label='class {}, auc = {stacked_auc_score:.3f}'.format(i))\n\n    ax[0].legend(loc=\"lower right\")\n    ax[0].set_xlabel(\"FPR\")\n    ax[0].set_ylabel(\"TPR\")\n    ax[0].set_title(\"ROC curve \"+ Name)\n    ax[1].legend()\n    ax[1].set_xlabel(\"recall\")\n    ax[1].set_ylabel(\"precision\")\n    ax[1].set_title(\"PR curve \" + Name)\n    plt.tight_layout()\n    plt.show()\n\n\n#for x in models:\n#    Multiclassplots(models[x],x)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define meta-learner\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import FunctionTransformer\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nlogreg_clf = LogisticRegression(penalty=\"l2\", C=100, fit_intercept=True)\n# Fitting voting clf --> average ensemble\nvoting_clf = VotingClassifier([(\"mlp\", ann_clf),\n                               (\"LDA\", lda_clf),\n                               (\"decision_tree\", dtree_clf)],\n                              voting=\"soft\",\n                              flatten_transform=True)\nxgb_clf = xgb.XGBClassifier(objective=\"multi:softmax\",\n                            learning_rate=0.1,\n                            n_estimators=100,\n                            max_depth=10,\n                            random_state=123)\nbbg_clf = BalancedBaggingClassifier(base_estimator= ann_clf,random_state=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf.fit(x_train,y_train)\ny_score = xgb_clf.predict_proba(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bbg_clf.fit(x_train,y_train)\ny_score = bbg_clf.predict_proba(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_clf.fit(x_train, y_train)\nmlp_model, lda_model, dtree_model = voting_clf.estimators_\nmodels = {\"mlp\": mlp_model,\n          \"lda\": lda_model,\n          \"dtree\": dtree_model,\n          \"avg_ensemble\": voting_clf}\n\n# Build first stack of base learners\nfirst_stack = make_pipeline(voting_clf,\n                            FunctionTransformer(lambda X: X[:, 1::2]))\n\n# Use CV to generate meta-features\nmeta_features = cross_val_predict(first_stack, x_train, y_train, cv=3, method=\"transform\")\nprint('check1')\n# Refit the first stack on the full training set\nfirst_stack.fit(x_train, y_train)\nprint('first stack fitted')\n# Fit the meta learner\nsecond_stack = logreg_clf.fit(meta_features, y_train)\nthird_stack = xgb_clf.fit(meta_features,y_train)\nfourth_stack = bbg_clf.fit(meta_features,y_train)\nprint('second third and fourth')\nmodels[\"stacked\"] = logreg_clf\nmodels[\"boosting\"] = third_stack\nmodels[\"bagging\"] = fourth_stack","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in models:\n    Multiclassplots(models[x],x)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}