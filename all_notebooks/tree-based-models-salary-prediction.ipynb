{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Welcome to my second notebook!\nI will try to use the NBA 2K20 dataset to predict player's salary. Please give me your feedback, i will be glad to read it! If you like the kernel, please vote up."},{"metadata":{},"cell_type":"markdown","source":"## Content\n\n1. [Introduction](#Introduction)\n2. [Reading and cleaning data](#Reading+and+cleaning+data)\n3. [Tree-Based Models](#Tree-Based+Models)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction\n\n#### Context\nEach entry of the dataset represents a NBA player. The idea is to predict the salary of the players based on some caractheristcs, which I will describe below.\n\n#### Columns definitions\n\n* **full_name** (string) - Player's full name\n* **rating** (numeric) - Rating in NBA 2K20\n* **jersey** (string) - Jersey's number\n* **team** (string) - Team\n* **position** (string) - Position in the team\n* **b_day** (string) - Birth date\n* **height** (string) - Height\n* **weight** (string) - Weight\n* **salary** (string) - Salary\n* **country** (string) - Country of birth\n* **draft_year** (string) - Year in which the player was drafted\n* **draft_round** (string) - Round in which the player was drafted\n* **draft_peak** (string) - Overall draft pick in which the player was selected\n* **college** (string) - College attended"},{"metadata":{},"cell_type":"markdown","source":"## 2. Reading and cleaning data\n\nAs it is not the focus of this notebook, I will not give many details, but fell free to ask questions in the comments =)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"/kaggle/input/nba2k20-player-dataset/nba2k20-full.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing unnecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"jersey\", \"full_name\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Position column cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_position(row):\n\n    if row.position == \"G-F\":\n        row.position = \"F-G\"\n    elif row.position == \"C-F\":\n        row.position = \"F-C\"\n    \n    return row\n\ndf = df.apply(clean_position, axis = \"columns\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get age of players"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nfrom dateutil.relativedelta import relativedelta\n\ndef get_year(row, col):\n    \n    row[col] = -(row[col].years)\n\n    return row\n\n\ndf[\"age\"] = pd.to_datetime(df[\"b_day\"])\nnow = dt.datetime.now()\ndf.age = df.age.apply(relativedelta, args = (now, ) )\n\ndf = df.apply(get_year, args = (\"age\",), axis = \"columns\")\ndf.drop(\"b_day\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get weight in kilos and height in meters"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"height\"] = np.array([float(x.strip()[-4:]) for x in df.height])\ndf[\"weight\"] = np.array([float(x.split(\"/\")[-1].split()[0]) for x in df.weight])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a column *conference*, which sets the player's team conference"},{"metadata":{"trusted":true},"cell_type":"code","source":"west = [\"Denver Nuggets\", \"Minnesota Timberwolves\", \"Oklahoma City Thunder\", \"Portland Trail Blazers\", \n           \"Utah Jazz\", \"Dallas Mavericks\", \"Houston Rockets\", \"Memphis Grizzlies\", \"New Orleans Pelicans\", \n           \"San Antonio Spurs\", \"Golden State Warriors\", \"Los Angeles Clippers\", \"Los Angeles Lakers\", \"Phoenix Suns\",\n           \"Sacramento Kings\"]\neast = [\"Boston Celtics\", \"Brooklyn Nets\", \"New York Knicks\", \"Philadelphia 76ers\", \"Toronto Raptors\",\n            \"Atlanta Hawks\", \"Charlotte Hornets\", \"Miami Heat\", \"Orlando Magic\", \"Washington Wizards\", \"Chicago Bulls\",\n           \"Cleveland Cavaliers\", \"Detroit Pistons\", \"Indiana Pacers\", \"Milwaukee Bucks\"]\n\ndef find_conference(row):\n    \n    if row.team in west:\n        row[\"conference\"] = \"West\"\n    elif row.team in east:\n        #print(\"{} é East\".format(row.team))\n        row[\"conference\"] = \"East\"\n    else:\n        #print(\"{} é Nan\".format(row.team))\n        row[\"conference\"] = \"No team\"\n    \n    return row\n\ndf = df.apply(find_conference, axis = \"columns\")\ndf.drop(\"team\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a column *region*, which sets the player's region of birth"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_country(row):\n    \n    if row.country == \"USA\":\n        row[\"region\"] = \"USA\"\n    else:\n        row[\"region\"] = \"Not USA\"\n    \n    return row\n\ndf = df.apply(find_country, axis = \"columns\")\ndf.drop(\"country\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the *$* in the salary column and transforming it to float."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.salary = [float(x[1:]) for x in df.salary]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find the number of seasons in the NBA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"seasons\"] = pd.to_datetime(df[\"draft_year\"], format = \"%Y\")\nnow = dt.datetime.now()\n\ndf.seasons = df.seasons.apply(relativedelta, args = (now, ) )\ndf = df.apply(get_year, args = (\"seasons\",), axis = \"columns\")\ndf.drop(\"draft_year\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correcting the name of the column *draft_peak* and changing *Undrafted* values in the draft columns to the number 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns = {\"draft_peak\":\"draft_pick\"}, inplace = True)\n\ndf[\"draft_pick\"] = pd.to_numeric(df[\"draft_pick\"].map(lambda x: 0 if x == \"Undrafted\" else x))\ndf[\"draft_round\"] = pd.to_numeric(df[\"draft_round\"].map(lambda x: 0 if x == \"Undrafted\" else x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split colleges that have 5 or more players in the NBA and colleges that have less than 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = df.college.value_counts()\nbig_schools = aux.index[0:16]\nbig_schools = big_schools.values\n\nsmall_schools = aux.index[16:]\nsmall_schools = small_schools.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def size_school(row):\n\n    if row.college in big_schools:\n        row[\"school_size\"] = \"Big\"\n    elif row.college in small_schools:\n        row[\"school_size\"] = \"Small\"\n    else:\n        row[\"school_size\"] = \"Didn't go to college\"\n        \n\n    return row\n\ndf = df.apply(size_school, axis = \"columns\")\ndf.drop(\"college\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting categorical variables into numeric through label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nlbl = preprocessing.LabelEncoder()\nfor col in df.columns.values:\n    if df.loc[:,col].dtype == \"object\":\n        df.loc[:,col] = lbl.fit_transform(df.loc[:,col].astype(str))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Tree-Based Models\n\nI will just use tree-based models in this data, because it is what I am studying at the moment."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.columns.values\ny_columns = [\"salary\"]\nx_columns = [x for x in columns if x != \"salary\"]\n\nX = df[x_columns]\ny = df[y_columns]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, I will use a Regression Tree to look at the variance problem of this model based on its *max_depth* hyperparameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = np.arange(1, 22, 2)\ntrain_error = []\ntest_error = []\n\nfor i in max_depth:\n    model = DecisionTreeRegressor(criterion = \"mse\", splitter = \"best\", max_depth = i)\n    model.fit(X_train, y_train)\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    train_error.append(mean_squared_error(y_train, y_pred_train))\n    test_error.append(mean_squared_error(y_test, y_pred_test))\n    \n\nplt.figure(figsize=(6,4))\nplt.plot(max_depth, train_error, '-bo', color = \"red\", label = \"Train Error\")\nplt.plot(max_depth, test_error, '-bo', color= \"blue\", label = \"Test Error\")\nplt.xlabel('max_depth', fontsize = 15)\nplt.ylabel('MSE', fontsize = 15)\nplt.xticks(max_depth)\nplt.legend()\nplt.show(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the Regression Tree has a lot of bias when *max_depth* is low (around 1 to 3), and as I increase the size of the tree, it starts overfitting (high variance).\nOne technique that can help reducing this variance problem is bagging (aka. Bootstrap Aggregating), which is used in the Random Forest."},{"metadata":{},"cell_type":"markdown","source":"Another technique used in the Random Forest algorithm to help reduce the variance is to choose $k$ features out of $d$ features to split a node, where $k <= d$ (usually $k = \\sqrt d$)."},{"metadata":{},"cell_type":"markdown","source":"I will fix a high number of estimators (trees) and vary the max depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = np.arange(1, 22, 2)\ntrain_error = []\ntest_error = []\n\nfor i in max_depth:\n    model = RandomForestRegressor(n_estimators = 5000, criterion = \"mse\", max_depth = i,\n                                 max_features = \"sqrt\")\n    model.fit(X_train, y_train.to_numpy().ravel())\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    train_error.append(mean_squared_error(y_train, y_pred_train))\n    test_error.append(mean_squared_error(y_test, y_pred_test))\n    \n\nplt.figure(figsize=(6,4))\nplt.plot(max_depth, train_error, '-bo', color = \"red\",label = \"Train Error\")\nplt.plot(max_depth, test_error, '-bo', color= \"blue\", label = \"Test Error\")\nplt.xlabel('max_depth', fontsize = 15)\nplt.ylabel('MSE', fontsize = 15)\nplt.xticks(max_depth)\nplt.legend()\nplt.show(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see that the test error does not increase as the *max_depth* parameter increases, it shows that using bagging helped control the model variance.\n\nNotice that when I use Bagging, I could use the *Out-Of-Bag Error* to test the model (because it is expected that around 60% of the original dataset will be on the new dataset made using bagging) instead of separating the data into training and testing previously, but to make it simpler I chose to not use the *ooberror*."},{"metadata":{},"cell_type":"markdown","source":"Another technique that can be used is called *boosting*. It uses some weak learners to create a strong learner, therefore it reduces the model bias. In this case, I will use it based on low *max_depth* trees.\n\nI will use one of the oldest boosting algorithms, which is the Gradient Boosting, to model the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = np.arange(1, 6, 1)\ntrain_error = []\ntest_error = []\n\nfor i in max_depth:\n    model = GradientBoostingRegressor(loss = \"ls\", learning_rate = 0.01, n_estimators = 5000,\n                                     criterion = \"mse\")\n    model.fit(X_train, y_train.to_numpy().ravel())\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    train_error.append(mean_squared_error(y_train, y_pred_train))\n    test_error.append(mean_squared_error(y_test, y_pred_test))\n    \n\nplt.figure(figsize=(6,4))\nplt.plot(max_depth, train_error, '-bo', color = \"red\",label = \"Train Error\")\nplt.plot(max_depth, test_error, '-bo', color= \"blue\", label = \"Test Error\")\nplt.xlabel('max_depth', fontsize = 15)\nplt.ylabel('MSE', fontsize = 15)\nplt.xticks(max_depth)\nplt.legend()\nplt.show(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing our test and train error to the Regression Tree and Random Forest results, Gradient Boosting had a much better result with low *max_depth* values."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}