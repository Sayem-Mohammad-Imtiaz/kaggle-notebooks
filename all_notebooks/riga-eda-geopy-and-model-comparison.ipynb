{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Yo!\nSo I've done some simple work with this dataset and acheved average MAE for sale prices around 12000 and for rent prices around 65.  \n  \nHere I want to show you what you can find with EDA, which variables might be worth to engineer and how GepPy library can help you with finding coordinates and addresses in order to obtain more information and improve your model. "},{"metadata":{},"cell_type":"markdown","source":"### Some preparation\nImporting libraries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, ElasticNet,  HuberRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import KFold\n\n\nimport re\n\nfrom geopandas.tools import geocode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nseed = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Introducting functions for EDA:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing(df):\n    df_missing = pd.DataFrame(df.isna().sum().sort_values(ascending = False), columns = ['missing_count'])\n    df_missing['missing_share'] = df_missing.missing_count / len(df)\n    return df_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_chart(df, x, title = None, hue = None):\n    plt.figure(figsize = (10, 6))\n    plt.title(title, fontsize=14)\n    ax = sns.countplot(x = x, hue = hue, data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def factor_chart(df, x, y, hue = None):\n    ax = sns.factorplot(x = x, y = y, data = df, hue = hue, kind = 'box', size=6, aspect = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatter(df, x, y, hue = None):\n    plt.figure(figsize = (20, 10))\n    ax = sns.scatterplot(x = x, y = y, data = df, hue = hue)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A bit of styling"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And there we go!"},{"metadata":{},"cell_type":"markdown","source":"### Raw data exploration "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/riga-real-estate-dataset/riga_re.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.price.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"We see that we have **10%** of dataset with missing price, that is our target variable.  \n  \nMoreover, if we take a closer look at the observations with missing price, we will se that they miss most information about other variables as well and contribute to the number of missings in other columns.  \n  \nAs we do not have a test dataset, they have no use so I **remove them**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = df[~df.price.isna()].reset_index(drop = True).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing(df_all) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Wow**, this action has actually helped us to get rid of most other missing variables as well!  \n  \nNow, none of the variables have share of missing higher than 5% so all of them are worth to explore."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Unexpectedly, rooms feature is categorical, we will take a closer look at it later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Variance of some numeric variables is very high.  \n  \nFloor and max floor feature values are reasonable, but area, price, lat and lon features have some wierd values and definite mistakes so cleaning will be necessary."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of observations:', len(df_all), '\\n')\nprint('Unique values:')\nprint(df_all.nunique().sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"We can see that actually not all street addresses are unique, so there are some groups of apartments that are in the same building, so it can help us while imputing floors and areas."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### Data cleaning and missing imputation"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"I want to use street addresses to impute missing lat, lon and addresses.  \n  \nSo let's fix missing streets first with their coordinates."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.street.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Unfortunately we see, that lat and lon values for these 5 observation are just incorrect, because if you put them on the map, you'll find yourself at Italian mountains (I wish it could happen for real).  \n  \nTherefore, finding addresses for them is impossible. Considering the fact that these observations have a lot of other missing features as well, we'd better drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = df_all.drop(df_all[df_all.street.isna()].index).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing(df_all) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Twofer**! Now, thanks to this drop, we should care about imputation for much less features."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now I want to extract actual street names from 'street' feature in order to use it as a categorical variable and to impute missing districts.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for removing digits from a string\n\ndef no_digits(text):\n    return ''.join([i for i in text if not i.isdigit()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['street_name_0'] = df_all['street'].apply(lambda x: no_digits(re.sub('\\W+',' ', str(x))).strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set(df_all.street_name_0.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"If you uncomment and launch the line above, you'll see that some useless liters have left, so we will fix em as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['st_n'] = None\nfor i in range(len(df_all)):\n    if ((df_all.loc[i, 'street_name_0'][:3] != 'St ') & (df_all.loc[i, 'street_name_0'][:2] != 'J ') & \n        (df_all.loc[i, 'street_name_0'][:2] != 'M ')):\n        df_all.loc[i, 'st_n'] = df_all.loc[i, 'street_name_0'].split(' ')[0]\n    elif (df_all.loc[i, 'street_name_0'][:3] != 'St '):\n         df_all.loc[i, 'st_n'] = df_all.loc[i, 'street_name_0'].split(' ')[0] + ' ' + df_all.loc[i, 'street_name_0'].split(' ')[1]\n    else:\n        df_all.loc[i, 'st_n'] = 'St ' + df_all.loc[i, 'street_name_0'].split(' ')[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set(df_all.st_n.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now it's better!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.drop(['street_name_0'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now let's impute **districts**."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.district.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.st_n == 'Ogļu'].groupby('district').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"See that Ogļu street belongs to Ķīpsala district strictly, so we impute it first."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[1107, 'district'] = 'Ogļu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.st_n == 'Pupuku iela'].groupby('district').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Oh, but looks like Pupuku iela is unique, so we use google maps then and find that this street belongs to Bišumuiža district."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[3172, 'district'] = 'Bišumuiža'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now let's impute lat and lan.  \n  \nFor this purpose I will use **GeoPy** library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent=\"specify_your_app_name_here\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lat(add):\n    try:\n        return geolocator.geocode(add).latitude\n    except:\n        return None\n\ndef lon(add):\n    try:\n        return geolocator.geocode(add).longitude\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"But as we've already noticed, some lat and lan values are **wrong**, so we have to make them empty and impute them by using their street addresses and GeoPy"},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all, x = 'lon', y = 'lat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[(df_all.lat>56.88)&(df_all.lat<57.1)&(df_all.lon>20)], x = 'lon', y = 'lat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now it's better"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[~((df_all.lat>56.88)&(df_all.lat<57.1)&(df_all.lon>20)), ['lat', 'lon']] = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"And now we come to the most painful part of this project for me.  \n \nThe problem with finding coordinates with geopy and all other libraries is that the address should be specifically correct in order to return desired coordinates. \n\nFor example, if you request 'Jūrmalas g. 15' coordinates, you wont get any result, because geopy cannot understand 'g.', so we have to replace all the instances of 'Jūrmalas g.' on 'Jūrmalas gatve'.  \n \nOr if you request 'Skolas 38' it will send you to a Norway hinterland.  \n\nTo fix all of this has brought me a lot of pain, but I did it so below is what I could do to achieve the result.  \n  \nAlso important to note, that geopy api sometimes throws you **connection errors**.  \nI made my coordinate finding functions robust to it, but it still requires to execute the line with the function **several times**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['district'] = df_all[\"district\"].replace('Krasta r-ns', 'Krasta masīvs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Street_New'] = df_all['street']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' g.', ' gatve'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-2', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k 1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-3', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-k-3', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-4', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k. 1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('k5', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('krastm.', 'krastmala'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' pr.', ' prospekts'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('Pulkv.', 'Pulkveža'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('bulv.', 'bulvāris'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('šķ. l.', 'šķērslīnija'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('šķ l.', 'šķērslīnija'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' l. ', ' līnija '))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' d. ', ' dambis '))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('J. Daliņa', 'Jāņa Daliņa'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('J. Vācieša', 'Jukuma Vācieša'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' g. ', ' gatve '))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' lauk.', ' laukums'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('k1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('k2', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-13d', '-13'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-36d', '-36'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-45d', '-45'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-94b', '-94'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' 19/1', ' 19'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('M. Balasta', 'Mazais Balasta'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('M. Kuldīgas', 'Mazā Kuldīgas'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('M. Nometņu', 'Mazā Nometņu'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('Asteres', 'Aisteres'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' 17 a', ' 17'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' š. ', ' šoseja '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Street_Full'] = df_all.apply(lambda x: str(x['Street_New']).split(' ')[0] + ' iela ' + str(x['Street_New']).split(' ')[1] +\n                                    ', ' + str(x['district']) + ', ' + 'Rīga' if \n                                    len(x['Street_New'].split(' ')) == 2 else str(x['Street_New']) + ', ' + \n                                    'Rīga', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ATTENTION!**   \n  \nThe following code is using GeoPy coordinates fuctions, but they **do not work well** if you execute them on kaggle environment!  \n \nTherefore, if you want to perform the same coordinate imputation on your data, please, copy the following commented code lines and execute them on your environment (Jupiter, for ex.)\n \nHere instead I will use the preloaded dataset that is the result of the commented code lines below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This lines request full address that is stored in Street_Full feature. \n# Has to be launched 3-4 times, until the number of missing values stops decreasing (reaching 24 for both lat and lon specifically in this case)\n\n#df_all['lat'] = df_all.apply(lambda x: lat(str(x['Street_Full'])) if np.isnan(x['lat']) == True else x['lat'], axis=1)\n#df_all['lon'] = df_all.apply(lambda x: lon(str(x['Street_Full'])) if np.isnan(x['lon']) == True else x['lon'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# However, some full addresses do not work with district name, so for the left missings we use only street name and 'Riga'\n# Also 2-3 times to execute (until 1 missing left for both lat and lon). \n\n#df_all['lat'] = df_all.apply(lambda x: lat(str(x['Street_Full'].split(',')[0]) + str(x['Street_Full'].split(',')[-1])) if np.isnan(x['lat']) == True else x['lat'], axis=1)\n#df_all['lon'] = df_all.apply(lambda x: lon(str(x['Street_Full'].split(',')[0]) + str(x['Street_Full'].split(',')[-1])) if np.isnan(x['lon']) == True else x['lon'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remaining missing did not work with full address, but only street name was enough here.\n# 1 execution is enough here\n\n#df_all['lat'] = df_all.apply(lambda x: lat(str(x['Street_Full'].split(',')[0])) if np.isnan(x['lat']) == True else x['lat'], axis=1)\n#df_all['lon'] = df_all.apply(lambda x: lon(str(x['Street_Full'].split(',')[0])) if np.isnan(x['lon']) == True else x['lon'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a preloaded dataset that I use for the following analysis here.  \n  \nHowever, **DO NOT FORGET** to remove 2 following lines if you want to use geopy on your env."},{"metadata":{"trusted":true},"cell_type":"code","source":"riga_fixed_coordinates = pd.read_csv('../input/riga-fixed-coordinates/riga_fixed_coordinates.csv')\nmissing(riga_fixed_coordinates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = riga_fixed_coordinates.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[~(df_all.lat>56.88)&(df_all.lat<57.1)&(df_all.lon>20)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Phew, that was tiresome!  \n  \nBut as we see, we successfully imputed all the coordinates and none of them are out of Riga boundaries now.  \n\nLet's check it on 'map' again."},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all, x = 'lon', y = 'lat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Splendid!**  "},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now it's time to imput remaining missings.  \n \nGo with **area**."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.area.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Let's look if there is a flat with the same addresses.  \n  \nI assume that flats in the same buildings will have more or less the same Area/Rooms, so we can use this as a proxy."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.street == 'Slokas 130']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Therefore\ndf_all.loc[3981, 'area'] = 80.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now go with imputing rooms and use the same approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.rooms.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.street == 'Dārzaugļu 1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Double area, but I doubt to put 8 rooms here.  \n  \nLet's look at rooms number distribution by rooms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.groupby(['rooms']).area.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now we can see why rooms feature is categorical.  \n  \nFor some reason there is a **'Citi'** value in rooms that means 'other'.  \n \nLet's try to figure out what it can give us."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.rooms == 'Citi']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"As we see, there are only 12 observations with this value of floor and they vary a lot, but most do have comparatively large area that makes them outliers.  \n  \nI would assume that these observations do have some unique certain specifics that can impact their price, but they are very few, so I just decided to drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = df_all.drop(df_all[df_all.rooms == 'Citi'].index, axis = 0).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the missing again as index was reseted\ndf_all[df_all.rooms.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"So from what's left, we can impute '6' for our missing room. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[1610, 'rooms'] = '6'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"And now introduce numeric feature of rooms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['rooms_num']= df_all['rooms'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Imputing total floors now.  \n  \nAnd again, same addresses should help."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.total_floors.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all.street == 'Zentenes 18']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Value for imputation is clearly visible."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[1902, 'total_floors'] = 9.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"And now we're done with imputation!  \n  \nNo missings left!"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing(df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nax = sns.distplot(df_all.price, bins = 20) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Distribution range of price is wild. Why is it so?  \n  \nLet's try to find some clues in our features."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Op_type**"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'op_type')\nfactor_chart(df_all, x = 'op_type', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"And here we found it!  \n  \nObviously, montly payment price for rent is much smaller than sale price of a whole apartment so these are clearly 2 different targets.  \n  \nTherefore, as we have quite enough observations for both major categories, it worths to separate the dataset on 2 parts for the further analysis and modelling later on.  \n  \nBut before what about other op_type values?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[~df_all.op_type.isin(['For rent', 'For sale'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"We can directly notice by the price value, which of the observations with other op_type actually represent either for rent or for sale types.  \n  \nSo we fix them and leave only 2 major op_types in order to split the dataset later."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[~df_all.op_type.isin(['For rent', 'For sale']) & (df_all.price < 1000), 'op_type'] = 'For rent'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[~df_all.op_type.isin(['For rent', 'For sale']) & (df_all.price > 1000), 'op_type'] = 'For sale'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'op_type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Area**  \n  \n  \nExpectedly, one of the most important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[df_all.op_type == 'For sale'], x = 'area', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'area', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"No surprise, strong positive correlation.  \n  \nHowever, we can notice, that dots are highly dispersed and there is definitely heteroskedasticity problem here.  \n \nOne of the ways to handle it is to use log1p of the target variable instead of the target itself. This makes a model robust to outliers and usually obtains better accuracy.\n \nNevertheless, in my case it did not help as much as outliers removal, so here I will use only it."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Condition**"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'condition')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'condition', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'condition', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Apartments with all amenities do have higher price than with partial or without.  \n  \nAt the same time, the number of observations without amenities is insignificantly slow, so I just introduce a dummy for All amenities to represent this condition difference in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['All_Amen'] = 0\ndf_all.loc[df_all.condition == 'All amenities', 'All_Amen'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Rooms**"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'rooms') \nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'rooms', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'rooms', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Alright, no surprise that higher numbers of rooms correlated with higher prices.  \n  \nAlthough, it's surprising, that 6 rooms flats for rent are cheaper than 5 rooms - other features impact might be involved."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Floor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'floor')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'floor', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'floor', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Looks like floor is correlated with price and will be included in the model.  \n \nSome observations seem to be outlying, but it can probably be some other features impact, so we will have to track it when removing outliers."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Total floors**"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'total_floors')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'total_floors', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'total_floors', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Same conclusion here as for floors. Seems important, should be included, might have outliers."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**House seria**"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'house_seria')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'house_seria', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'house_seria', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Some house serias do have varying price levels, so I will dummy them and track their impact in models"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Same conclusion goes for **house type**"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'house_type')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'house_type', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'house_type', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Districts and streets might have their own specific levels of prices.   \n \nSome are prestigious and some are poor, and it can impact the price strongly, while area or distance from center will not track it.  \n  \nTherefore, this categorical information might be very important, so I will dummy them as well. "},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_chart(df_all, x = 'district')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'district', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'district', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For sale'], x = 'lon', y = 'lat', hue = 'district')\nscatter(df_all[df_all.op_type == 'For rent'], x = 'lon', y = 'lat', hue = 'district')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**lat and lon**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[df_all.op_type == 'For sale'], x = 'lat', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'lat', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[df_all.op_type == 'For sale'], x = 'lon', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'lon', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Notice, how price is the higher, the closer a dot is to the center.  \n  \nTherefore, knowing the coordinates of Riga center, we can calculate each observation's distance from center and use it as a feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"Riga_Center_Lat = 56.949600\nRiga_Center_Lon = 24.105200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopy.distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def center_dist(lat_i, lon_i):\n    return geopy.distance.vincenty((Riga_Center_Lat, Riga_Center_Lon), (lat_i, lon_i)).km","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['center_dist'] = df_all.apply(lambda x: center_dist(x['lat'], x['lon']), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[df_all.op_type == 'For sale'], x = 'center_dist', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'center_dist', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Expectedly, correlation of center_dist with price is negative and visibly significant, so it will be included in the models"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now, let's introduce some other features, that can improve our predictions additionally"},{"metadata":{},"cell_type":"markdown","source":"**Area_Room_Ratio** will reflect how big the rooms are in the apartment. The assumption here is that people might prefer  larger rooms"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Area_Room_Ratio'] = df_all.area / df_all.rooms_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[df_all.op_type == 'For sale'], x = 'Area_Room_Ratio', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'Area_Room_Ratio', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Seems like my assumption was sort of correct and this feature will be worth to use. Though, some outliers and strong heteroskedasticity remain a problem"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Floor_Ratio**  \nThe second assumption is that people prefer comparatively higher flats in a house, so their prices are higher for all buildings"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Floor_Ratio'] = df_all.floor / df_all.total_floors ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[df_all.op_type == 'For sale'], x = 'Floor_Ratio', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'Floor_Ratio', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"And here we have an unexpected data mistake!  \n  \nObviously, Floor_Ratio cannot be higher than 1, but we have such values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all['Floor_Ratio'] > 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"It seems like floor and total_floors values are switched for these observations.  \n  \nLet's switch 'em back then."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[(df_all['Floor_Ratio'] > 1), 'floor'] = df_all['floor'] / df_all['Floor_Ratio']\ndf_all.loc[(df_all['Floor_Ratio'] > 1), 'total_floors'] = df_all['Floor_Ratio'] * df_all['total_floors']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[(df_all['Floor_Ratio'] > 1), 'Floor_Ratio'] = df_all['floor'] / df_all['total_floors']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[df_all['Floor_Ratio'] > 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now we're good!"},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_all[df_all.op_type == 'For sale'], x = 'Floor_Ratio', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'Floor_Ratio', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Some positive correlation is visible, so it worths to try this feature in the models"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"OK, we're almost done with cleaning! What's left is only..."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### Dropping outliers"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"First, it is time to separate our dataset on 2 independent sets and work with them separately now"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sale = df_all[df_all.op_type == 'For sale'].reset_index(drop = True).copy()\ndf_rent = df_all[df_all.op_type == 'For rent'].reset_index(drop = True).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now we go for outliers detection  \n  \nI've made it basing on observing scatterplots and removing dots that highly deviate.  \n \nI decided to not to use quantile outlier detection here because since the values are highly disperced and our datasets are not really big, a big share of information would've been lost and models would've not been describing our data really.  \n \nBut if this dataset will be expanded later on, I would definitely use boxcox transformation to remove outliers and to build more robust model "},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_sale, x = 'area', y = 'price', hue = None)\nscatter(df_rent, x = 'area', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_rent[df_rent.price < 300], x = 'area', y = 'price', hue = 'center_dist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Notice, that in rent dataset there is a relatively big chunk of dots below price = 100, that have their own mood on price.\n  \nHowever, playing with 'hue' parameter for the chart I could not find the explaination for this group's deviant behavior. \n \nTherefore, since available features do not explain this, I specify these dots as outliers so they do not harm our model.  \n \nIf you find the explaination for this, please, let me know."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"After some boring iterations of charts I've ended up with these datasets.  \n \nUnusually expensive or cheap flats' prices definitely have some other explaination of their deviant prices (such as interior for ex.) so they were removed together with other highly deviated dots"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sale_clean = df_sale[(df_sale.price < 300000) & (df_sale.area <160)  \n                  & (~((df_sale.price < 50000) &(df_sale.area > 80))) \n                 & (~((df_sale.price < 100000)&(df_sale.area > 130)))\n                  & (df_sale.Area_Room_Ratio<80)\n                 ].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rent_clean = df_rent[(df_rent.price < 1390) & (df_rent.area <125) & (df_rent.price > 60) \n                  & (~((df_rent.price < 110) &(df_rent.area > 40))) \n                 & (~((df_rent.price < 400)&(df_rent.area > 100)))\n                  & (~((df_rent.price > 1000)&(df_rent.area < 70)))\n                  &(df_rent.Area_Room_Ratio < 65)\n                 ].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now charts look more neat and outliers impact is higly reduced."},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter(df_sale_clean, x = 'area', y = 'price', hue = None)\nscatter(df_rent_clean, x = 'area', y = 'price', hue = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Drop useless columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sale_clean.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sale_clean = df_sale_clean.drop(['op_type', 'street', 'rooms', 'condition', 'Street_New', 'Street_Full'], axis = 1)\ndf_rent_clean = df_rent_clean.drop(['op_type', 'street', 'rooms', 'condition', 'Street_New', 'Street_Full'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Splitting on test and train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_splits(df):\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(['price'], axis = 1), \n                                                          df['price'], train_size=0.8, test_size=0.2, \n                                                          random_state = seed)\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Get dummies for categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_sale_clean = pd.get_dummies(df_sale_clean, drop_first = True)\nOH_rent_clean = pd.get_dummies(df_rent_clean, drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test = get_splits(OH_sale_clean)\nOH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test = get_splits(OH_rent_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Drop columns, that are relevant only for train sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop_sale = OH_sale_train.columns[(OH_sale_train == 0).all()]\nOH_sale_train = OH_sale_train.drop(cols_to_drop_sale, axis = 1)\nOH_sale_test = OH_sale_test.drop(cols_to_drop_sale, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop_rent = OH_rent_train.columns[(OH_rent_train == 0).all()]\nOH_rent_train = OH_rent_train.drop(cols_to_drop_rent, axis = 1)\nOH_rent_test = OH_rent_test.drop(cols_to_drop_rent, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"And finally..."},{"metadata":{},"cell_type":"markdown","source":"### Modelling"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"First, let's choose the most appropriate models for tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [RandomForestRegressor(random_state = seed), \n          Ridge(random_state = seed), \n          RidgeCV(), \n          Lasso(random_state = seed), \n          LassoCV(random_state = seed), \n          ElasticNet(random_state = seed),\n          HuberRegressor(), \n          KernelRidge(), \n          GradientBoostingRegressor(random_state = seed), \n          ExtraTreesRegressor(random_state = seed), \n          XGBRegressor(random_state = seed)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_names = [str(i).split('(')[0] for i in models]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def models_summary(train, test, y_train, y_test):\n    models_MAE = []\n    models_RMSE = []\n    models_RMSLE = []\n    for model in models:\n        model.fit(train, y_train)\n        preds = model.predict(test)\n        models_MAE.append(mean_absolute_error(y_test, preds))\n        models_RMSE.append(np.sqrt(mean_squared_error(y_test, preds)))\n        models_RMSLE.append(np.sqrt(mean_squared_log_error(y_test, abs(preds))))\n    return pd.DataFrame(list(zip(models_names, models_MAE, models_RMSE, models_RMSLE)),\n              columns=['models','MAE', 'RMSE', 'RMSLE']).sort_values(by = 'MAE').set_index('models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"For sale:"},{"metadata":{"trusted":true},"cell_type":"code","source":"models_sale_res = models_summary(OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test)\nmodels_sale_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"For rent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"models_rent_res = models_summary(OH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test)\nmodels_rent_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Seems like **ExtraTrees** has the lowest MAE, RMSE and RMSLE for both sets and it is the best fit for our disperced data.  \n \nOf course, this results do not take tuning into account and famous XGB or GBS potentially can outbeat ExtraTrees.  \n \nHowever, I've tuned them by myself and ExtraTrees still was the best for my datasets.  \n \nTherefore, I will show results only for this model, but if you manage to overcome my scores with XGB - please, let me know!"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### Tuning ExtraTrees\nFor this purpose I will use combined dataset and use GridSearch on 5 kfolds. This will allow to getan average error score for any split wilth the same train/set proportion as we set before (80/20).  \n  \nBelow I already selected my combination of parameters that I consider the best by efficiency and time ratio,but youcan play with your parameters here. \n \nNote, that you can achieve higher scores by setting higher n_estimators, but you'd have to wait longer."},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ET_model = ExtraTreesRegressor(random_state = seed,\n                                n_estimators=400, \n                                min_samples_split=2,\n                                min_samples_leaf=1, \n                                max_features=200,\n                              )\n\nparams_grid = {#'n_estimators': range(50,50,201),\n               #'max_features': range(50,401,50),\n               #'min_samples_split': range(2,5),\n               #'min_samples_leaf': range(1,4)\n              }  \n\nET_grid = GridSearchCV(estimator = ET_model, param_grid = params_grid, n_jobs = -1,\n                               cv = kf, scoring = 'neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ET_grid_sale = ET_grid.fit(OH_sale_clean.drop(['price'], axis = 1), OH_sale_clean.price)\nprint(ET_grid_sale.best_params_)\nprint(ET_grid_sale.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ET_grid_rent = ET_grid.fit(OH_rent_clean.drop(['price'], axis = 1), OH_rent_clean.price)\nprint(ET_grid_rent.best_params_)\nprint(ET_grid_rent.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"So for the sale dataset we can expect **MAE** around **12200** and for rent **MAE** around **66.32** in average case scenario.  \n \nBy increasing n_estimatiors, MAE score can be reduced by ~150 and ~4 for sale and rent datasets respectively"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Let's calculate our results for our selected train and test sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ET_sale = ExtraTreesRegressor(random_state = seed,\n                                n_estimators=400, \n                                min_samples_split=2,\n                                min_samples_leaf=1, \n                                max_features=200,\n                              )\nET_rent = ExtraTreesRegressor(random_state = seed,\n                                n_estimators=400, \n                                min_samples_split=2,\n                                min_samples_leaf=1, \n                                max_features=200,\n                              )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(ET_sale, OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(ET_rent, OH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Here we ahcieve **MAE 11473** for sale houses and **64.62** for rent and we can see that tuning has helped to slightly improve our score compare the baseline model, where we had MAE 11501 for sale and 65.25 for rent.\n \nI think it's not so bad, but can't yet say so (waiting for your kernel here!)"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now I'd like to take a closer look on performance of our models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def results(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    res_tab = pd.DataFrame({'y_test': y_test, 'preds': preds, 'error': (preds - y_test),\n             'error_share': abs(y_test - preds)/y_test}).sort_values(by = 'error_share', ascending = False)\n    return res_tab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sale_results = results(ET_sale, OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test)\nrent_results = results(ET_rent, OH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sale_results[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rent_results[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"To make everything visible, let's plot error and error_share from lowest to highest "},{"metadata":{"trusted":true},"cell_type":"code","source":"def error_lines(df, y):\n    plt.figure(figsize = (10, 6))\n    ax = sns.lineplot(x = range(len(df)), y = y, data = df.sort_values(by = [y], ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"For sale:"},{"metadata":{"trusted":true},"cell_type":"code","source":"error_lines(sale_results, 'error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_lines(sale_results, 'error_share')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Right and left sides of the error graphs are symmetric and the graph is centered around 0.  \n \nThis allows us to say that our model is not over or underestimating price and most errors explained by outliers.  \n \nOther graph shows, that we mistake less by 10% for the half of the set and less than 25% for nearly 80% of the set.  \n  \nSo yeah, not so bad! \n  \nI guess..."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"For rent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"error_lines(rent_results, 'error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_lines(rent_results, 'error_share')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Here we see that model for rent type apartments used to underestimate y value, so more careful work with low-price outliers can help to reduce error rate greatly.  \n \nStill, model performs quite good, having less than 20% error share for 80% of the observations. "},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### CONCLUSION "},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Ok, so far I've managed to build a model that is seemingly not bad.  \n \nTo get better scores I've tried to tune all of the mentioned models, used log1p in order to handle heteroscedasticity problem that our values clearly have, tried different combination of mentioned features etc.  \n \nHere I present you the best of mine, but I believe you can do it better!  \n \nTo give you some ideas:  \n \n1) More outliers can be removed with box-cox tool so the model will be more robust to them (but maybe will lose explanatory power)  \n \n2) Try other 'fashionable' models like LightGBM and CatBoost. Due to some problems with my python packages I could not install them properly, so maybe they will outperform ExtraTrees here  \n \n3) My model clearly has too much features, so I suspect it is Multidimensionally cursed. \nDefinitely, most of it comes from huge number of streets dummies. Not using them would drop my model performance, but maybe there is a way to cleverly choose or group some of them.  \n\n4) To tackle heteroskedasticity you can apply log1p to the features like area, Area/Room ratio, Floor/Max floor ration or to the target variable itself. This can mitigate the negative effect of outliers, which are definitely worsening my score a lot."},{"metadata":{"trusted":false},"cell_type":"markdown","source":"That's all from me, guys!  \n \nLet me know if you found any better solutions or my mistakes and if you have any advice for my model, stats and code overall (I know it's not the state of the art at all, so your criticism is highly welcome) \n \nPeace!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}