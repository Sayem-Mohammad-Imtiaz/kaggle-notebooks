{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Telecom Churn Analysis\n\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\nIn this analysis, we will analyse the customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn."},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Reading and Understanding the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 300)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.feature_selection as skfs\nimport sklearn.linear_model as sklm\nimport sklearn.decomposition as skd\nimport sklearn.ensemble as ske\nimport sklearn.tree as skt\nimport sklearn.feature_selection as skfs\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import svm\nimport xgboost as xgb\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing/df.shape[0]*100\n    return missing, missing_perc\n\ndef plotCorrelation(cols, df, figsize=(20,10)):\n    plt.figure(figsize=figsize)\n    sns.heatmap(df[cols].corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, center = 0)\n    plt.show()\n    \ndef plotAUC(y_true, y_pred_proba):\n    fpr, tpr, threshold = skm.roc_curve(y_true, y_pred_proba[:,1])\n    roc_auc = skm.auc(fpr, tpr)\n    plt.figure(figsize=(6,6))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\ndef display_scores(y_true, y_pred, y_pred_proba, plot=False):\n    cfm = skm.confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cfm.ravel()\n    print(f\"Accuracy Score: {round(skm.accuracy_score(y_true, y_pred)*100,4)}%\")\n    print(f\"Sensitivity/Recall/TPR: {round(tp/(tp+fn)*100,4)}%\")\n    print(f\"FPR: {round(fp/(tn+fp)*100,4)}%\")\n    print(f\"Specificity: {round(tn/(tn+fp)*100,4)}%\")\n    print(f\"Precision: {round(tp/(tp+fp)*100,4)}%\")\n    print(f\"F1 Score: {round(skm.f1_score(y_true, y_pred)*100,4)}%\")\n    print(f\"Prediction AUC Score: {round(skm.roc_auc_score(y_true, y_pred)*100,4)}%\")\n    print(f\"Mean Square Error: {round(skm.mean_squared_error(y_true, y_pred),10)}\")\n    if plot:\n        plotAUC(y_true, y_pred_proba)\n\n# function for prediction metrics\ndef displayPredictionMetrics(model, X_train, y_train, showROC=False):\n    y_pred_proba = model.predict_proba(X_train)\n    y_pred = model.predict(X_train)\n    display_scores(y_train, y_pred, y_pred_proba, showROC)\n    \ndef plot_avgMonthlyCalls(data,calltype,colList):\n    # create a color palette\n    palette = plt.get_cmap('Set1')\n    \n    fig, ax = plt.subplots(figsize=(8,4))\n    ax.plot(data[colList].mean())\n    ax.set_xticklabels(['Jun','Jul','Aug'])\n\n    # Add titles\n    plt.title(\"Avg. \"+calltype+\" MOU  V/S Month\", loc='left', fontsize=12, fontweight=0, color='orange')\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Avg. \"+calltype+\" MOU\")\n    plt.show()\n    \ndef plot_byChurnMou(data,colList, calltype):\n    fig, ax = plt.subplots(figsize=(7,4))\n    df=data.groupby(['churn'])[colList].mean().T\n    plt.plot(df)\n    ax.set_xticklabels(['Jun','Jul','Aug','Sep'])\n    ## Add legend\n    plt.legend(['Non-Churn', 'Churn'])\n    # Add titles\n    plt.title(\"Avg. \"+calltype+\" MOU  V/S Month\", loc='left', fontsize=12, fontweight=0, color='orange')\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Avg. \"+calltype+\" MOU\")\n    \ndef plot_byChurn(data, col):\n    # per month churn vs Non-Churn\n    fig, ax = plt.subplots(figsize=(7,4))\n    colList = list(data.filter(regex=(col)).columns)\n    colList = colList[:3]\n    plt.plot(data.groupby('churn')[colList].mean().T)\n    ax.set_xticklabels(['Jun','Jul','Aug','Sep'])\n    ## Add legend\n    plt.legend(['Non-Churn', 'Churn'])\n    # Add titles\n    plt.title( str(col) +\" V/S Month\", loc='left', fontsize=12, fontweight=0, color='orange')\n    plt.xlabel(\"Month\")\n    plt.ylabel(col)\n    plt.show()\n    # Numeric stats for per month churn vs Non-Churn\n    return data.groupby('churn')[colList].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file = \"/kaggle/input/telecom-churn-dataset/telecom_churn_data.csv\"\ntelecom = pd.read_csv(data_file)\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# check dataset shape\ndatasetShape(telecom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for duplicates\nif(len(telecom) == len(telecom.mobile_number.unique())):\n    print(\"No duplicates found!!\")\nelse:\n    print(\"Duplicates occuring\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the mobile number\ntelecom.drop('mobile_number', axis=1, inplace=True)\n\n# drop the duplicate rows\ntelecom.drop_duplicates(inplace=True)\ndatasetShape(telecom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all columns having no values\ntelecom.dropna(axis=1, how=\"all\", inplace=True)\ntelecom.dropna(axis=0, how=\"all\", inplace=True)\ndatasetShape(telecom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"Verifying percentage of NaN values in remaining columns\")\nprint(telecom.isnull().mean().round(4) * 100)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# remove columns having null values more than 50%\ntelecom.dropna(thresh=telecom.shape[0]*0.5,how='all',axis=1, inplace=True)\ndatasetShape(telecom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop Irrelevant Features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# divide features\nnumerical_features, categorical_features = divideFeatures(telecom)\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"categorical_features.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All object type features are only dates. We will drop all date columns that are object types.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresToDrop = categorical_features.columns.to_list()\ntelecom.drop(featuresToDrop, axis=1, inplace=True)\ndatasetShape(telecom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Value Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot missing values\n\nmissing, missing_perc = calc_missing(telecom)\nmissing.plot(kind='bar',figsize=(30,12))\nplt.title('Missing Values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# missing values with percentage\npd.concat([missing, missing_perc], axis=1, keys=['Total','Percent']).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom[missing.index].head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"telecom[missing.index].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As all the columns are numerical, have many outliers and missing values are less than 8% in these columns, we will impute all the missing value columns with median values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToImputeWithMedian = missing.index.to_list()\nfor col in colsToImputeWithMedian:\n    telecom.loc[telecom[col].isna(), col] = telecom[col].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing, missing_perc = calc_missing(telecom)\nprint(\"Any Missing Values?\",missing.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All Missing values are treated."},{"metadata":{},"cell_type":"markdown","source":"### Drop single valued features"},{"metadata":{"trusted":true},"cell_type":"code","source":"singleValuedFeatures = []\nfor x in telecom.columns.to_list():\n    if len(telecom[x].unique()) == 1:\n        singleValuedFeatures.append(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom.drop(singleValuedFeatures, axis=1, inplace=True)\ndatasetShape(telecom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Derive New Features"},{"metadata":{},"cell_type":"markdown","source":"**New column for average amount of total recharge amount for months 6th & 7th**"},{"metadata":{"trusted":true},"cell_type":"code","source":"telecom['avg_amt_m6m7'] = (telecom['total_rech_amt_6']+telecom['total_rech_amt_7'])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding churn customers with total data and calls usage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# mapping churn as 1 and not churn as 0\ntelecom['churn'] = ((telecom['total_ic_mou_9']==0) & (telecom['total_og_mou_9']==0) & (telecom['vol_2g_mb_9']==0) & (telecom['vol_3g_mb_9']==0)).map({True:1,False:0})\ntelecom['churn'].value_counts()/telecom.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter High Value Customers"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# filtering high value customers with value > 70% quantile in avg_amt_6_7 column\ntelecom = telecom[telecom[\"avg_amt_m6m7\"] >= telecom['avg_amt_m6m7'].quantile(0.7)]\ntelecom[\"avg_amt_m6m7\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop columns for churn phase i.e. 9"},{"metadata":{"trusted":true},"cell_type":"code","source":"remainingCols = [x for x in telecom.columns.to_list() if '_9' not in x]\nremainingCols.remove('sep_vbc_3g')\ntelecom = telecom[remainingCols]\ndatasetShape(telecom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Data Visualization - EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features, categorical_features = divideFeatures(telecom)\ntelecom.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for amount of defaulters in the data using countplot\nplt.figure(figsize=(16,3))\nsns.countplot(y=\"churn\", data=telecom)\nplt.show()\ntelecom[\"churn\"].value_counts()/telecom.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot aon with histplot\naon_bins = [0, 365, 730, 1095, 1460, 1825, 2190, 2555, 2920, 3285, 3650, 5015]\nbucket_l = [\"Year \"+str(x) for x in range(1,12)]\naon_bins_data = pd.cut(telecom.aon, aon_bins, labels=bucket_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aon_bins_data.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see most of the customers belong to last 4 years.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Avg. total monthly incoming MOU vs AON\nic_col = telecom.filter(regex ='total_ic_mou').columns\nplot_avgMonthlyCalls(telecom, calltype='incoming', colList=ic_col)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plotting Avg. total monthly outgoing MOU vs AON\nog_col = telecom.filter(regex ='total_og_mou').columns\nplot_avgMonthlyCalls(telecom, calltype='outgoing', colList=og_col)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,100))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(34, 4, i+1)\n    sns.boxplot(y=numerical_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outliers in many features. These outliers will be treated in Data Preparation step."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# distplots for categorical data\n\ncat_features = ['monthly_2g_6','monthly_2g_7','monthly_2g_8','monthly_3g_6','monthly_3g_7','monthly_3g_8', 'churn']\nfig = plt.figure(figsize=(16,10))\nfor i in range(len(cat_features)):\n    fig.add_subplot(3, 3, i+1)\n    telecom[cat_features].iloc[:,i].hist()\n    plt.xlabel(telecom.columns[i])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some patterns can be seen in above plotted categorical data. It will help in identifying useful features."},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# graph for incoming and outgoing in month 6,7,8 by churn\nic_col = ['total_ic_mou_6','total_ic_mou_7', 'total_ic_mou_8']\nog_col = ['total_og_mou_6','total_og_mou_7', 'total_og_mou_8']\nplot_byChurnMou(telecom, ic_col, 'Incoming')\nplot_byChurnMou(telecom, og_col, 'Outgoing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# graph for total recharge amount in month 6,7,8 by churn\nplot_byChurn(telecom,'total_rech_amt')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# graph for arpu in month 6,7,8 by churn\nplot_byChurn(telecom,'arpu')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"colsToPlot = ['arpu_6','arpu_7','arpu_8','onnet_mou_6','onnet_mou_7','onnet_mou_8','offnet_mou_6','offnet_mou_7','offnet_mou_8','loc_og_mou_6','loc_og_mou_7','loc_og_mou_8','total_og_mou_6','total_og_mou_7','total_og_mou_8','total_ic_mou_6','total_ic_mou_7','total_ic_mou_8','avg_amt_m6m7','aon']\n# plot scatter plots for all major numerical features\n\nsns.pairplot(telecom[colsToPlot], size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some correlated features present in dataset. We will use these features in model building."},{"metadata":{},"cell_type":"markdown","source":"### MultiVariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation within outgoing minutes of usage for month 6\nog_mou_6 = telecom.columns[telecom.columns.str.contains('.*_og_.*mou_6',regex=True)]\nplotCorrelation(og_mou_6, telecom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation within outgoing minutes of usage for month 7\nog_mou_7 = telecom.columns[telecom.columns.str.contains('.*_og_.*mou_7',regex=True)]\nplotCorrelation(og_mou_7, telecom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation within incoming minutes of usage for month 6\nic_mou_6 = telecom.columns[telecom.columns.str.contains('.*_ic_.*mou_6',regex=True)]\nplotCorrelation(ic_mou_6, telecom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation within incoming minutes of usage for month 7\nic_mou_7 = telecom.columns[telecom.columns.str.contains('.*_ic_.*mou_7',regex=True)]\nplotCorrelation(ic_mou_7, telecom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation will be used for feature selection."},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"### Outlier Treatment\n\nTreating with the SalePrice target feature and other numerical features, which are skewed. We will take log of the feature values using np.log1p()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\ntelecom.describe(percentiles=[.25,.5,.75,.90,.95,.99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All the features are having outliers in the data. We will not remove any outlier data but we will treat the skewed data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample skewed feature\nplt.figure(figsize=(16,4))\nsns.distplot(telecom.loc_og_mou_7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract all skewed features\ntemp_numerical_features, temp_categorical_features = divideFeatures(telecom)\n# remove categorical features stored as int\ntemp_numerical_features.drop(cat_features, axis=1, inplace=True)\ntemp_numerical_features.drop(['arpu_6','arpu_7','arpu_8'], axis=1, inplace=True)\nskewed_features = temp_numerical_features.apply(lambda x: x.skew()).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform skewed features\nfor feat in skewed_features.index:\n    # features which are more than 50% skewed are transformed\n    if skewed_features.loc[feat] > 0.5:\n        telecom[feat] = np.log1p(telecom[feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample treated feature\nplt.figure(figsize=(16,4))\nsns.distplot(telecom.loc_og_mou_7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# outlier treatment for categorical features\ncat_features.remove('churn')\ndef getCategoricalSkewed(categories, threshold):\n    tempSkewedFeatures = []\n    for feat in categories:\n        for featValuePerc in list(telecom[feat].value_counts()/telecom.shape[0]):\n            if featValuePerc > threshold:\n                tempSkewedFeatures.append(feat)\n    return list(set(tempSkewedFeatures))\n\n# display all categorical skewed features which have value_counts > 90%\ncategoricalSkewed = getCategoricalSkewed(cat_features, .90)\nif len(categoricalSkewed) > 0:\n    for feat in categoricalSkewed:\n        print(\"Ratio of non-churn vs churn:\")\n        print(telecom[feat].value_counts()/len(telecom)*100)\nelse:\n    print(\"No Categorical Skewed variables\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Derive New Features\n\nWe will make new features by taking average for every feature for month 6 and 7."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"newFeaturesAdded = []\nnewFeatures = telecom.filter(regex='_6|_7').columns.str[:-2]\nfor idx, col in enumerate(newFeatures.unique()):\n    newF = \"avg_\"+col+\"_avg67\"\n    newFeaturesAdded.append(newF)\n    telecom[newF] = (telecom[col+\"_6\"]  + telecom[col+\"_7\"])/ 2\ndatasetShape(telecom)\ntelecom[newFeaturesAdded].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split Train-Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffle samples\ndf_shuffle = telecom.sample(frac=1, random_state=seed).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_y = df_shuffle.pop('churn')\ndf_X = df_shuffle.copy()\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.75, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]/len(df_shuffle)*100)}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = skp.StandardScaler()\nnumerical_features, categorical_features = divideFeatures(X_train)\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train[numerical_features.columns] = scaler.fit_transform(X_train[numerical_features.columns])\n\n# scale test data with transform()\nX_test[numerical_features.columns] = scaler.transform(X_test[numerical_features.columns])\n\n# view sample data\nX_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model A - High Performance Model Using PCA"},{"metadata":{},"cell_type":"markdown","source":"### Find Principal Components "},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = skd.PCA(random_state=seed)\npca.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Components from the PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the explained variance ratio for each component"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a scree plot for the explained variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"var_cumu = np.cumsum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[12,8])\nplt.vlines(x=65, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.95, xmax=150, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have got 65 components to describe 95% of variance in the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_final = skd.IncrementalPCA(n_components=65)\ndf_train_pca = pca_final.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the heatmap of the corr matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = np.corrcoef(df_train_pca.transpose())\nplt.figure(figsize=[40,20])\nsns.heatmap(corrmat, annot=True, center = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have removed multicollinearity from our dataset, and now our models will be much more stable**\n\nApplying the transformation on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_pca = pca_final.transform(X_test)\ndatasetShape(df_test_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Data Modelling\n\n**Using class_weight='balanced' in Learning Algorithms to handle class imbalance problem.**\n\n**Using scoring='recall' for every GridSearch to tune the hyperparameters in order to improve the sensitivity.**"},{"metadata":{},"cell_type":"markdown","source":"### Model 1 - Random Forest Classifier Model"},{"metadata":{},"cell_type":"markdown","source":"**Initial Model Building**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nrfc = ske.RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=seed)\nrfc.fit(df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find train prediction metrics\ndisplayPredictionMetrics(rfc, df_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find test prediction metrics\ndisplayPredictionMetrics(rfc, df_test_pca, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HyperParameter Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tune_rfc_hyperparameter(model, parameters,x_train,y_train,n_folds = 5):\n    \n    m = skms.GridSearchCV(model, parameters, cv=n_folds, n_jobs=-1, scoring=\"recall\", return_train_score=True, verbose=3)\n    m.fit(x_train, y_train)\n    scores = m.cv_results_\n\n    # find the value of the hyperparameter\n    for key in parameters.keys():\n        hyperparameter = key\n        break\n\n    print('We can get the sensitivity of',m.best_score_,'using',m.best_params_)\n    \n    # plotting accuracies for parameters\n    plt.figure(figsize=(16,5))\n    plt.plot(scores[\"param_\"+hyperparameter], scores[\"mean_train_score\"], label=\"training accuracy\")\n    plt.plot(scores[\"param_\"+hyperparameter], scores[\"mean_test_score\"], label=\"test accuracy\")\n    plt.xlabel(hyperparameter)\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_hyper_init = ske.RandomForestClassifier(class_weight='balanced', random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning max_depth\nparameters = {'max_depth': range(2, 30, 5)}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning n_estimators\nparameters = {'n_estimators': [100, 200, 300]}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning max_features\nparameters = {'max_features': [40, 50, 60]}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning min_samples_leaf\nparameters = {'min_samples_leaf': range(10, 80, 15)}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning min_samples_split\nparameters = {'min_samples_split': range(20, 110, 20)}\ntune_rfc_hyperparameter(rf_hyper_init, parameters,df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning all final hyperparameters for rfc\n\nparameters = {\n    'max_depth': [2, 7, 12],\n    'n_estimators': [100],\n    'max_features': [55],\n    'min_samples_leaf': [50,70],\n    'min_samples_split': [100]}\n\nrfc_hyper = ske.RandomForestClassifier(class_weight='balanced')\n\n# cross validation\nmodel_cv_rfc_hyper = skms.GridSearchCV(estimator = rfc_hyper, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)            \nmodel_cv_rfc_hyper.fit(df_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display all final tuned hyper parameters for rfc\nprint('We can get the sensitivity of',model_cv_rfc_hyper.best_score_,'using',model_cv_rfc_hyper.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`We will evaluate this classifier in Model Evaluation step.`"},{"metadata":{},"cell_type":"markdown","source":"### Model 2 - Support Vector Machine Model"},{"metadata":{},"cell_type":"markdown","source":"**Initial Model Building**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_svc = y_train.map({1:1, 0:-1})\ny_test_svc = y_test.map({1:1, 0:-1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nsvc = svm.SVC(class_weight='balanced', probability=True, kernel='rbf')\nsvc.fit(df_train_pca,y_train_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find train prediction metrics\ndisplayPredictionMetrics(svc, df_train_pca, y_train_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find test prediction metrics\ndisplayPredictionMetrics(svc, df_test_pca, y_test_svc, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HyperParameter Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_svc_hyperparameters(scores, param):\n    gamma = scores[scores['param_gamma']==param]\n    plt.plot(gamma[\"param_C\"], gamma[\"mean_test_score\"])\n    plt.plot(gamma[\"param_C\"], gamma[\"mean_train_score\"])\n    plt.xlabel('C')\n    plt.ylabel('Accuracy')\n    plt.title(\"Gamma=\"+str(param))\n    plt.ylim([0, 1])\n    plt.legend(['test_score', 'training_score'])\n    plt.xscale('log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning all final hyperparameters for svc\ngamma = [1e-1,1e-2, 1e-3, 1e-4]\nC = [1, 10, 100, 1000]\nparameters = {'gamma': gamma, 'C': C}\nsvc_hyper = svm.SVC(class_weight='balanced', probability=True, kernel='rbf')\n\n# cross validation\nmodel_cv_svc_hyper = skms.GridSearchCV(estimator = svc_hyper, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)            \nmodel_cv_svc_hyper.fit(df_train_pca, y_train_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display cv scores and plot gamma vs C\nsvc_scores = pd.DataFrame(model_cv_svc_hyper.cv_results_)\nsvc_scores['param_C'] = svc_scores['param_C']\n\nplt.figure(figsize=(16,5))\n\nfor x, g in enumerate(gamma):\n    plt.subplot(1, 4, x+1)\n    plot_svc_hyperparameters(svc_scores, g)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# display all final tuned hyper parameters for svc\nprint('We can get the sensitivity of',model_cv_svc_hyper.best_score_,'using',model_cv_svc_hyper.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`We will evaluate this classifier in Model Evaluation step.`"},{"metadata":{},"cell_type":"markdown","source":"### Model 3 - Gradient Boosting Model"},{"metadata":{},"cell_type":"markdown","source":"**Initial Model Building**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nxgbc = xgb.XGBClassifier(scale_pos_weight=(y_train.value_counts()[0]/y_train.value_counts()[1]), n_jobs=-1)\nxgbc.fit(df_train_pca,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find train prediction metrics\ndisplayPredictionMetrics(xgbc, df_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find test prediction metrics\ndisplayPredictionMetrics(xgbc, df_test_pca, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HyperParameter Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_xgb_hyperparameters(scores, param):\n    lr = scores[scores['param_subsample']==param]\n    plt.plot(lr[\"param_learning_rate\"], lr[\"mean_test_score\"])\n    plt.plot(lr[\"param_learning_rate\"], lr[\"mean_train_score\"])\n    plt.xlabel('learning_rate')\n    plt.ylabel('Accuracy')\n    plt.title(\"Subsample=\"+str(param))\n    plt.ylim([0.4, 1])\n    plt.legend(['test_score', 'training_score'])\n    plt.xscale('log')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# tuning all final hyperparameters for xgb\nlearning_rate = [0.1,0.2,0.3]\nsubsample = [0.3,0.4,0.5]\nparameters = {'learning_rate': learning_rate, 'subsample': subsample}\nxgb_hyper = xgb.XGBClassifier(scale_pos_weight=(y_train.value_counts()[0]/y_train.value_counts()[1]))\n\n# cross validation\nmodel_cv_xgb_hyper = skms.GridSearchCV(estimator = xgb_hyper, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)\nmodel_cv_xgb_hyper.fit(df_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display cv scores and plot gamma vs C\nxgb_scores = pd.DataFrame(model_cv_xgb_hyper.cv_results_)\nxgb_scores['param_learning_rate'] = xgb_scores['param_learning_rate']\n\nplt.figure(figsize=(16,5))\n\nfor x, s in enumerate(subsample):\n    plt.subplot(1, 3, x+1)\n    plot_xgb_hyperparameters(xgb_scores, s)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display all final tuned hyper parameters for xgb\nprint('We can get the sensitivity of',model_cv_xgb_hyper.best_score_,'using',model_cv_xgb_hyper.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`We will evaluate this classifier in Model Evaluation step.`"},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Model Evaluation\n\n### Model 1 - Random Forest Classifier Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluating RFC Model with best parameters on Train Dataset\",model_cv_rfc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_rfc_hyper, df_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluating RFC Model with best parameters on Test Dataset\",model_cv_rfc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_rfc_hyper, df_test_pca, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2 - Support Vector Machine Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluating SVC Model with best parameters on Train Dataset\",model_cv_svc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_svc_hyper, df_train_pca, y_train_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluating SVC Model with best parameters on Test Dataset\",model_cv_svc_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_svc_hyper, df_test_pca, y_test_svc, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3 - XGBoost Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluating XGB Model with best parameters on Train Dataset\",model_cv_xgb_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_xgb_hyper, df_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluating XGB Model with best parameters on Test Dataset\",model_cv_xgb_hyper.best_params_,\"\\n\")\ndisplayPredictionMetrics(model_cv_xgb_hyper, df_test_pca, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## High Performance Model Observations"},{"metadata":{},"cell_type":"markdown","source":"`The sensitivity score of RandomForestClassifier model for Train is 74.8341% and Test is 77.918%`\n\n`The sensitivity score of SupportVectorClassifier model for Train is 84.9923% and Test is 84.2271%`\n\n`The sensitivity score of XGBClassifier model for Train is 96.0184% and Test is 71.1356%`\n\n| Classifier              | Train Sensitivity | Test Sensitivity |\n|-------------------------|-------------------|------------------|\n| RandomForestClassifier  | 74.8341%          | 77.918%          |\n| SupportVectorClassifier | 84.9923%          | **84.2271%**     |\n| XGBClassifier           | 96.0184%          | 71.1356%         |\n\n**The SupportVectorClassifier model is stable and performing good with around 84.2271% of test accuracy. It can be used in production environment.**"},{"metadata":{},"cell_type":"markdown","source":"## Model B - Feature Importance Model Without PCA"},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Data Modelling"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Model with all Features"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# find stats for model using all features\nlr = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1)\nlr.fit(X_train,y_train)\ndisplayPredictionMetrics(lr, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RFE Feature Selection"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lr_rfe = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1, max_iter=500, verbose=1)\nrfe = skfs.RFE(lr_rfe, 30)\nrfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfeCols = X_train.columns[rfe.support_]\nX_train_rfe = X_train[rfeCols]\nX_test_rfe = X_test[rfeCols]\nprint(\"Selected features by RFE are\",list(rfeCols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check metrics by building model with RFE selected features\nlr = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1)\nlr.fit(X_train_rfe,y_train)\ndisplayPredictionMetrics(lr, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Manual Feature Selection\n\nNow we will proceed with **manual feature** selection by building the model using **statsmodel** library with **p-value & VIF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_glm = sm.GLM(y_train, X_train_rfe, family = sm.families.Binomial())\nlr_glm_model = lr_glm.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to drop the mentioned feature variable, build the model and print the summary and VIF\ndef displaySummary(dropFeature=None):\n    # update variable from global scope\n    global X_train_rfe, y_train\n    \n    # dropping variable with very high VIF\n    if dropFeature is not None:\n        X_train_rfe.drop(dropFeature, axis=1, inplace=True)\n        X_test_rfe.drop(dropFeature, axis=1, inplace=True)\n        print(f\"Removed feature: {dropFeature} \\n\")\n        \n    # print model metrics for current available features\n    lr = sklm.LogisticRegression(random_state=seed, class_weight='balanced', n_jobs=-1)\n    lr.fit(X_train_rfe, y_train)\n    displayPredictionMetrics(lr, X_train_rfe, y_train)\n\n    # Running the linear model\n    lr_glm = sm.GLM(y_train, X_train_rfe, family = sm.families.Binomial())\n    lr_glm_model = lr_glm.fit()\n    \n    # check the summary of our linear model\n    print(lr_glm_model.summary())\n    \n    # Calculate the VIFs for the new model after removing constant\n    if 'const' in X_train_rfe.columns:\n        X_train_rfe = X_train_rfe.drop(['const'], axis=1)\n    vif = pd.DataFrame()\n    X = X_train_rfe\n    vif['Features'] = X.columns\n    vif['VIF'] = [round(variance_inflation_factor(X.values, i), 2) for i in range(X.shape[1])]\n    vif = vif.sort_values(by = \"VIF\", ascending = False).set_index('Features')\n    print(vif)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary('arpu_7')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary('monthly_3g_8')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary('avg_vol_3g_mb_avg67')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary('arpu_8')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary('loc_ic_mou_8')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary('loc_og_mou_8')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"displaySummary('avg_loc_ic_t2m_mou_avg67')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('total_rech_amt_8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('std_og_t2m_mou_6')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('onnet_mou_7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('total_og_mou_8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('onnet_mou_8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('roam_og_mou_7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('avg_max_rech_amt_avg67')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('loc_og_mou_7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displaySummary('vol_3g_mb_8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***All the features' p-values and VIF are optimal now. <br>These set of features are fine to proceed with the Model Selection.***\n\n### Correlation Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot correlation among final selected featuers\nplt.figure(figsize = (20, 10))\nsns.heatmap(X_train_rfe.corr(), annot = True, linewidths=.2, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nrfc_nopca = ske.RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=seed)\nrfc_nopca.fit(X_train_rfe,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find train prediction metrics\ndisplayPredictionMetrics(rfc_nopca, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find test prediction metrics\ndisplayPredictionMetrics(rfc_nopca, X_test_rfe, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HyperParameter Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_hyper_init_nopca = ske.RandomForestClassifier(class_weight='balanced', random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning max_depth\nparameters = {'max_depth': range(2, 30, 5)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning n_estimators\nparameters = {'n_estimators': range(100, 800, 200)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning max_features\nparameters = {'max_features': [8, 10, 12, 14]}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning min_samples_leaf\nparameters = {'min_samples_leaf': range(1, 50, 10)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning min_samples_split\nparameters = {'min_samples_split': range(10, 100, 10)}\ntune_rfc_hyperparameter(rf_hyper_init_nopca, parameters, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning all final hyperparameters for rfc\n\nparameters = {\n    'max_depth': [7],\n    'n_estimators': [100],\n    'max_features': [12],\n    'min_samples_leaf': [41],\n    'min_samples_split': [70,90]}\n\nrfc_hyper_nopca = ske.RandomForestClassifier(class_weight='balanced', random_state=seed)\n\n# cross validation\nmodel_cv_rfc_hyper_nopca = skms.GridSearchCV(estimator = rfc_hyper_nopca, n_jobs=-1, param_grid = parameters, \n                             scoring= 'recall', cv = 3, return_train_score=True, verbose = 3)            \nmodel_cv_rfc_hyper_nopca.fit(X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display all final tuned hyper parameters for rfc\nprint('We can get the sensitivity of',model_cv_rfc_hyper_nopca.best_score_,'using',model_cv_rfc_hyper_nopca.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_interpretable = ske.RandomForestClassifier(class_weight='balanced', random_state=seed, max_depth=7, max_features=12, \n                          min_samples_leaf=41, min_samples_split=90, n_estimators=100)\nrfc_interpretable.fit(X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluating RFC Model with best parameters on Train Dataset\",model_cv_rfc_hyper_nopca.best_params_,\"\\n\")\ndisplayPredictionMetrics(rfc_interpretable, X_train_rfe, y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"Evaluating RFC Model with best parameters on Test Dataset\",model_cv_rfc_hyper_nopca.best_params_,\"\\n\")\ndisplayPredictionMetrics(rfc_interpretable, X_test_rfe, y_test, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find Important Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rfc_interpretable model.feature_importances_\nfeature_importances = pd.DataFrame(rfc_interpretable.feature_importances_,\n                                   index = X_train_rfe.columns, columns=['importance']).sort_values('importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"Top 10 Model parameters (excluding constant) are:\")\nfeature_importances[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Interpretable Model Recommendations"},{"metadata":{},"cell_type":"markdown","source":"**A less number of high value customers have churned as around 8-10%.**\n\n**Outgoing Calls on romaing for 8th month is strong indicators of churn behaviour. So it should be focused to identify if there is any issue with calling on roaming.**\n\n**Total Incoming Calls for 8th month is most strong indicator of churn behaviour.**\n\n**Customers that are joined in last 4 years are more likely to churn and should be focussed for retaining by providing special schemes.**\n\n**Last day recharge feature is also important for predicting churn behaviour.**\n\n**Using RFC with selected set of features, we can have 79.9% of sensitivity in predicting churn behaviour.**\n\n***The top 10 important features are:*** <br>\n<blockquote><b>total_ic_mou_8, roam_og_mou_8, last_day_rch_amt_8, loc_og_t2m_mou_8, loc_ic_t2m_mou_8, vol_2g_mb_8, aon, max_rech_amt_8, std_og_mou_8, avg_amt_m6m7</b></blockquote>"},{"metadata":{},"cell_type":"markdown","source":"# The END"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}