{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Start by some imports and reading the data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-14T11:06:47.112335Z","iopub.execute_input":"2021-07-14T11:06:47.112849Z","iopub.status.idle":"2021-07-14T11:06:47.274685Z","shell.execute_reply.started":"2021-07-14T11:06:47.112818Z","shell.execute_reply":"2021-07-14T11:06:47.273417Z"}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nRootDir = \"/kaggle/input/sentimental-analysis-for-tweets\"\n\n\nfilename = RootDir + \"/sentiment_tweets3.csv\"\ndf = pd.read_csv(filename)\nprint (df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:09:19.080297Z","iopub.execute_input":"2021-07-14T11:09:19.080663Z","iopub.status.idle":"2021-07-14T11:09:19.112388Z","shell.execute_reply.started":"2021-07-14T11:09:19.080634Z","shell.execute_reply":"2021-07-14T11:09:19.111403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 10,314 tweets in our dataset. Each one has a label: 0=not depressed, 1=depressed. Let's get the tweets (input) and labels (output), and print a sample of each ttype of tweet:","metadata":{}},{"cell_type":"code","source":"tweets = df.values[:,1]\nlabels = df.values[:,2].astype(float)\nprint (tweets[40], labels[40])\nprint (tweets[8002], labels[8002])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:12:26.198736Z","iopub.execute_input":"2021-07-14T11:12:26.199274Z","iopub.status.idle":"2021-07-14T11:12:26.2097Z","shell.execute_reply.started":"2021-07-14T11:12:26.199242Z","shell.execute_reply":"2021-07-14T11:12:26.208465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we load BERT:","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nbert_model = SentenceTransformer('distilbert-base-nli-mean-tokens')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:14:22.92902Z","iopub.execute_input":"2021-07-14T11:14:22.929426Z","iopub.status.idle":"2021-07-14T11:20:21.847959Z","shell.execute_reply.started":"2021-07-14T11:14:22.929392Z","shell.execute_reply":"2021-07-14T11:20:21.846198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now run the BERT model on all tweets to get their encoding","metadata":{}},{"cell_type":"code","source":"embeddings = bert_model.encode(tweets, show_progress_bar=True)\nprint (embeddings.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The embeddings will be our features to train a classifier, but first we need tp split the data into training and test sets:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(embeddings, labels, \n                                          test_size=0.2, random_state=42)\nprint (\"Training set shapes:\", X_train.shape, y_train.shape)\nprint (\"Test set shapes:\", X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:22:16.647089Z","iopub.execute_input":"2021-07-14T11:22:16.647473Z","iopub.status.idle":"2021-07-14T11:22:16.672957Z","shell.execute_reply.started":"2021-07-14T11:22:16.647442Z","shell.execute_reply":"2021-07-14T11:22:16.672065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 768 features in the embedding vector for every tweet. Now build a simple classification model to work on them","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Sequential, layers\n\nclassifier = Sequential()\nclassifier.add (layers.Dense(256, activation='relu', input_shape=(768,)))\nclassifier.add (layers.Dense(1, activation='sigmoid'))\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  \n    \nhist = classifier.fit (X_train, y_train, epochs=10, batch_size=16, \n                      validation_data=(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:23:38.216979Z","iopub.execute_input":"2021-07-14T11:23:38.21743Z","iopub.status.idle":"2021-07-14T11:23:48.436255Z","shell.execute_reply.started":"2021-07-14T11:23:38.217402Z","shell.execute_reply":"2021-07-14T11:23:48.435646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the loss and accuracy:","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot\n\npyplot.figure(figsize=(15,5))\npyplot.subplot(1, 2, 1)\npyplot.plot(hist.history['loss'], 'r', label='Training loss')\npyplot.plot(hist.history['val_loss'], 'g', label='Validation loss')\npyplot.legend()\npyplot.subplot(1, 2, 2)\npyplot.plot(hist.history['accuracy'], 'r', label='Training accuracy')\npyplot.plot(hist.history['val_accuracy'], 'g', label='Validation accuracy')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T11:29:28.75334Z","iopub.execute_input":"2021-07-14T11:29:28.753728Z","iopub.status.idle":"2021-07-14T11:29:29.038485Z","shell.execute_reply.started":"2021-07-14T11:29:28.753699Z","shell.execute_reply":"2021-07-14T11:29:29.037475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems we reach a very good prediction accuracy (>98%) immediately, with almost no improvement by additional epochs","metadata":{}}]}