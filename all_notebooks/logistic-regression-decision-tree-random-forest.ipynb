{"cells":[{"metadata":{},"cell_type":"markdown","source":"****Input all the neccessay packages****"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\n\nsns.set()\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data = pd.read_csv('../input/churn-modelling/Churn_Modelling.csv',\n                         index_col='RowNumber')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We used the describe() function to get the statistics associated with each column. This will be helpful for the purpose of scaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.CreditScore.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will start with preprocessing each features.\n1)Cerdit Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.CreditScore.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets remove the Customer ID and Surname, as they won't be of any help for a good analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.drop(labels=['CustomerId','Surname'],\n                axis=1,\n                inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.Geography.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data.Gender.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we require to do one hot encoding of the above two selected labels. We can see that there are 3 classess in Geography and Two labels in Gender. Lets convert the string type to int."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data_cleaned = pd.get_dummies(churn_data, \n                                    prefix=['Geo','Gen'], \n                                    prefix_sep='_',\n                                    dummy_na=False, \n                                    columns=['Geography','Gender'],\n                                    sparse=False,\n                                    drop_first=False,\n                                    dtype=int) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data_cleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have cleaned the data, and converted all the categorical data to numeric, we are ready for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_data_cleaned.hist(bins=10,\n                        figsize=(20,20),\n                        xrot=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=churn_data_cleaned.columns\nprint(labels)\nscaler=preprocessing.StandardScaler()\nscaled_churn_data_cleaned=scaler.fit_transform(churn_data_cleaned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_churn_data_cleaned=pd.DataFrame(scaled_churn_data_cleaned)\nscaled_churn_data_cleaned.columns=labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_churn_data_cleaned.hist(bins=10,\n                               figsize=(20,20),\n                               xrot=30)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,1,figsize=(20,20))\nfor i in scaled_churn_data_cleaned.columns:\n    sns.kdeplot(scaled_churn_data_cleaned[i],\n                 label=[i],\n                 bw=1.5,\n                 ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see now that all the data have been scaled down in the limit of [-4,4]."},{"metadata":{},"cell_type":"markdown","source":"Now, that we have done the necessary scaling, we can safely move forward and try to identify the corelation among all the set of features."},{"metadata":{},"cell_type":"markdown","source":"First we will try to calculate the correlation matrix with Exited(Output feature) column in consideration, and lets see how the dependency matrix looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=scaled_churn_data_cleaned.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,1,figsize=(20,10))\nsns.heatmap(corr,\n            annot=True,\n            cmap='RdYlGn',\n            ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets try to visualize the correlation data in box plt using the seaborn package."},{"metadata":{"trusted":true},"cell_type":"code","source":"nr=7\nnc=2\nfig,ax=plt.subplots(nrows=nr,ncols=nc,figsize=(20,20))\ni=0\nfor j in range(nr):\n    for k in range(nc):\n        axes=ax[j,k]\n        \n        sns.boxplot(x=scaled_churn_data_cleaned['Exited'],\n                    y=scaled_churn_data_cleaned.iloc[:,i],\n                    ax=axes)\n        i+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready for building the elements for modelling the data set. \nBefore proceeding lets remove the output feature, Exited from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_churn_data_cleaned=scaled_churn_data_cleaned.drop('Exited',\n                                                         axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_churn_data_cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can apply PCA to the given input dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nn_comp = 2\npca=PCA(n_components=n_comp)\nprincipal_components=pca.fit_transform(scaled_churn_data_cleaned)\nlen(principal_components)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pc_df=pd.DataFrame(principal_components,\n                  columns=['principal_components_%s'%(i+1) for i in range(n_comp)],\n                  index=range(1,len(principal_components)+1))\nprint(pc_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_components=pc_df\noutput_components=churn_data.Exited\nprint(input_components.shape,output_components.shape)\nfinal_df=pd.concat([input_components,output_components],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing in 2D using the Principal components 1 & principal components 2 as y and x axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,1,figsize=(20,20))\nax.set_xlabel('principal_components_1',fontsize=20)\nax.set_ylabel('principal_components_2',fontsize=20)\nax.set_title('Customers Exited on PC1 & PC2',fontsize=20)\n\nTargets=[0,1]\ncolors=['r','k']\n\nfor target,color in zip(Targets,colors):\n    index_no_target=final_df['Exited']==target\n    ax.scatter(final_df.loc[index_no_target,'principal_components_1'],\n               final_df.loc[index_no_target,'principal_components_2'],\n              c=color)\n    ax.legend(Targets)\n    ax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observed that only 15.44% and 14.3% variances is attributed to eachc of the first two principal components.\nSince we have 13 dimension in feature space, we will try to retain most of the variance using 10 principal components."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp=10\npca_10=PCA(n_components=n_comp)\npca10_comp=pca_10.fit_transform(scaled_churn_data_cleaned)\ndf_PCA_10=pd.DataFrame(pca10_comp,\n                       columns=['Principal_component_%s'%(i+1) for i in range(n_comp)],\n                      index=range(1,len(pca10_comp)+1))\nprint(df_PCA_10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(pca_10.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Train split of the datdset\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(df_PCA_10,\n                                               output_components,\n                                               test_size=0.4,\n                                               random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that 95.8% of the variance is retained in these 10 Principal Components. "},{"metadata":{},"cell_type":"markdown","source":"Now we can proceed for creating the model development using the ten principal components created.\nFirst we will start with Logistic Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,roc_curve,auc,log_loss\n\nmodel=LogisticRegression()\nmodel.fit(x_train,y_train)\ny_pred=model.predict(x_test)\ny_pred_proba=model.predict_proba(x_test)[:, 1]\n[fpr,tpr,thr]=roc_curve(y_test,y_pred_proba)\n\nprint('Train/Test split results:')\nprint(model.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(model.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\nprint(model.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\nprint(model.__class__.__name__+\" score is  %.2f\" % model.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score is pretty well, lets now try with some other algorithm and see if we can get any better results."},{"metadata":{},"cell_type":"markdown","source":"Now lets start with Decision tree model, and see the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nmodel=DecisionTreeClassifier(random_state=0)\nmodel.fit(x_train,y_train)\ny_pred=model.predict(x_test)\n\nscore=model.score(x_test,y_test)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(model,x_train,y_train,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel=RandomForestClassifier(n_estimators=100,\n                            bootstrap=True,\n                            max_features='sqrt')\nmodel.fit(x_train,y_train)\ny_pred=model.predict(x_train)\nprint(model.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see a significant improvement in the model accuracy on going from Decision Tree to Random Forest classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Raw Cell Format","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":4}