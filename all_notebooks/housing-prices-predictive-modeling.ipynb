{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking Out the Data Statistically"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = pd.read_csv('/kaggle/input/usa-housing/USA_Housing.csv')\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape/Size of the data\n\nhousing.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- lets see if there is any null/missing values in the datasets or not. It's important to remove or\n- replace all the missing values before moving further."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.isnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Checking different column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there is no missing or null value in the dataset so far, hence we can proceed further hassle free.\nTill now, we had a detailed look at the given data and fortunately we don't have any missing values. So, the data cleaning is not required for this data."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\nLet's create some simple plots to check out the data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(housing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see here in the last line of graph that all the features seems to be in a linear relationship with price except Avg. Area Number of Bedroom.\nWe can also see this by plotting a separate graph"},{"metadata":{},"cell_type":"markdown","source":"Now, let's understand the correlation between variable by plotting correlation plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the above correlaton using a correlation matrix in heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(housing.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that price is more correlated to Avg. Income Area, House Age and Area Population than Number of Bedrooms and Rooms. Lets see these metrics in tabular format."},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing/checking the linear relationship between Price and Avg. Area Income by plotting their scatter Plot as below:\nplt.scatter(housing.Price, housing[['Avg. Area Income']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we gotta check the distribution of our target variable that is Price as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(housing['Price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the price plot seems like a bell shaped curve and all the price is normally distributed."},{"metadata":{},"cell_type":"markdown","source":"# Training a Linear Regression Model\nLet's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. We will toss out the Address column because it only has text info that the linear regression model can't use."},{"metadata":{},"cell_type":"markdown","source":"# X and y arrays"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = housing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n               'Avg. Area Number of Bedrooms', 'Area Population']]\ny = housing['Price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split\nNow let's split the data into a training set and a testing set. We will train out model on the training set and then use the test set to evaluate the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred_score = cross_val_score(model, X, y, cv=10)\n    return pred_score.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data For Linear Regression\nLinear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model.\n\nAs such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression.\n\nTry different preparations of your data using these heuristics and see what works best for your problem.\n\n-Linear Assumption. Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n\n-Remove Noise. Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n\n-Remove Collinearity. Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n\n-Gaussian Distributions. Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n\n-Rescale Inputs: Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization."},{"metadata":{},"cell_type":"markdown","source":"# 1. Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\nLet's evaluate the model by checking out it's coefficients and how we can interpret them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the intercept\nprint(lin_reg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpreting the coefficients:\n\n* Holding all other features fixed, a 1 unit increase in Avg. Area Income is associated with an increase of $21.52.\n\n* Holding all other features fixed, a 1 unit increase in Avg. Area House Age is associated with an increase of $164883.28.\n\n* Holding all other features fixed, a 1 unit increase in Avg. Area Number of Rooms is associated with an increase of $122368.67.\n\n* Holding all other features fixed, a 1 unit increase in Avg. Area Number of Bedrooms is associated with an increase of $2233.80.\n\n* Holding all other features fixed, a 1 unit increase in Area Population is associated with an increase of $15.15."},{"metadata":{},"cell_type":"markdown","source":"# Predictions from our Model\nSo Let's go ahead and do some predictions off our test set and see how well it do!"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = lin_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we Plot the 2 entities : Observed V/S Predicted Values using scatter plot and see how close 2 are."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Residual Histogram\nResiduals = Observed value - Predicted Value which is (y_test - pred) in our case here.\nSo we see the distribution of the residuals using distribution plot down below."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot((y_test - pred), bins=50);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regression Evaluation Metrics\nHere are three common evaluation metrics for regression problems:\n\nMean Absolute Error (MAE) is the mean of the absolute value of the errors:\n\n$$\n1/n∑i=1/n|y{i}−y^{i}|\n$$ \n\nMean Squared Error (MSE) is the mean of the squared errors:\n\n$$\n1/n∑i=1/n(yi−y^i)^2\n$$ \n\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n\n$$\n1/n∑i=√1/n(yi−y^i)^2\n$$ \n\nComparing these metrics:\n\nMAE is the easiest to understand, because it's the average error.\nMSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\nRMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\nAll of these are loss functions, because we want to minimize them."},{"metadata":{"trusted":true},"cell_type":"code","source":"print_evaluate(y_test, lin_reg.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 2. Robust Regression"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.\n\nOne instance in which robust estimation should be considered is when there is a strong suspicion of heteroscedasticity.\n\nA common situation in which robust estimation is used occurs when the data contain outliers. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is inefficient and can be biased. Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. (In many situations, including some areas of geostatistics and medical statistics, it is precisely the outliers that are of interest.)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Random Sample Consensus - RANSAC"},{"metadata":{},"cell_type":"markdown","source":"Random Sample Consensus - RANSAC\nRandom sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\n\nA basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RANSACRegressor\n\nmodel = RANSACRegressor()\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\nprint_evaluate(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, pred) , cross_val(RANSACRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Ridge Regression"},{"metadata":{},"cell_type":"markdown","source":"Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\n\n$ minw∣∣∣∣Xw−y∣∣∣∣22+α∣∣∣∣w∣∣∣∣22 $\n \nα>=0  is a complexity parameter that controls the amount of shrinkage: the larger the value of  α , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n\nRidge regression is an L2 penalized model. Add the squared sum of the weights to the least-squares cost function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel = Ridge()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint_evaluate(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, pred) , cross_val(Ridge())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. LASSO Regression"},{"metadata":{},"cell_type":"markdown","source":"A linear model that estimates sparse coefficients.\n\nMathematically, it consists of a linear model trained with  ℓ1  prior as regularizer. The objective function to minimize is:\n\n$ minw12nsamples∣∣∣∣Xw−y∣∣∣∣22+α∣∣∣∣w∣∣∣∣1 $\n \nThe lasso estimate thus solves the minimization of the least-squares penalty with  α∣∣∣∣w∣∣∣∣1  added, where  α  is a constant and  ∣∣∣∣w∣∣∣∣1  is the  ℓ1−norm  of the parameter vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nmodel = Lasso()\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)\nprint_evaluate(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Elastic Net"},{"metadata":{},"cell_type":"markdown","source":"A linear regression model trained with L1 and L2 prior as regularizer.\n\nThis combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n\nElastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n\nA practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge’s stability under rotation.\n\nThe objective function to minimize is in this case\n\n$ minw12nsamples∣∣∣∣Xw−y∣∣∣∣22+αρ∣∣∣∣w∣∣∣∣1+α(1−ρ)2∣∣∣∣w∣∣∣∣22 $"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet()\nmodel.fit(X_train,y_train)\nmodel.predict(X_test)\n\nprint_evaluate(y_test,pred)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, pred) , cross_val(ElasticNet())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Polynomial Regression"},{"metadata":{},"cell_type":"markdown","source":"One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n\nFor example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n\n$ y^(w,x)=w0+w1x1+w2x2 $\n \nIf we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\n\n$ y^(w,x)=w0+w1x1+w2x2+w3x1x2+w4x21+w5x22 $\n \nThe (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable\n\n$ z=[x1,x2,x1x2,x21,x22] $\n \nWith this re-labeling of the data, our problem can be written\n\n$ y^(w,x)=w0+w1z1+w2z2+w3z3+w4z4+w5z5 $\n \nWe see that the resulting polynomial regression is in the same class of linear models we’d considered above (i.e. the model is linear in w) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.4, random_state=101)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)\npred = lin_reg.predict(X_test)\n\nprint_evaluate(y_test, pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\n\n# Scoring the Random Forest Regression\n\nprint_evaluate(y_test,pred)\n\nrandom_r2_score = r2_score(model.predict(X_test), y_test)\nrandom_r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, pred),cross_val(RandomForestRegressor()) ]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\nmodel = tree.DecisionTreeRegressor()\n\nmodel.fit(X_train,y_train)\n\npred = model.predict(X_test)\n\nprint_evaluate(y_test,pred)\n\n# checking R2 score\n\ntree_r2_score = r2_score(model.predict(X_test),y_test)\ntree_r2_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df_2 = pd.DataFrame(data=[[\"Decision Tree Regression\", *evaluate(y_test, pred), cross_val(tree.DecisionTreeRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"In this notebook you discovered the linear regression algorithm for machine learning.\n\nYou covered a lot of ground including:\n\nThe common linear regression models (Ridge, Lasso, ElasticNet, ...).\nThe representation used by the model.\nLearning algorithms used to estimate the coefficients in the model.\nRules of thumb to consider when preparing data for use with linear regression.\nHow to evaluate a linear regression model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}