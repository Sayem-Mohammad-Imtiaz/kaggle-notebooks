{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plan\n\nI. Cleaning data. \n\n0. Rename columns and delete \"?\" and replace \"spaces\" to \"_\". Do it with Python for.\n1. Find how much values each og five ranges contain. Ranges are 1st range: from min to min + 0.2*(max - min), 2nd range: from min + 0.2*(max - min) to min + 0.4*(max - min), 3rd range: from min + 0.4*(max - min) to min + 0.6*(max - min), 4th range: from min + 0.6*(max - min) to min + 0.8*(max - min), 5th range: from min + 0.8*(max - min) to max. \n2. Drop all columns which values group in one of range. For example, each column contains 6819 values. In column 'Operating_Profit_Rate' almost values (6817) group in 5th range as for \"Bankrapcy == 0\" as for \"Bankrapcy == 1\" Such columns contains an empty (usefulness) information for us. Therefore I drop them. \n3. Build sns.pairplot to visualize intermediate result.\n\n\nII. Learn simple ML-model with GridSearchCV, RandomizedSearchCV, RandomForestClassifier. Build a Decision Tree.\n\nCalculate Precision Score, Recall Score and F1 Score. Plot Confision Matrix. \n\nPlot a pie chart of features importances. \n\n\n# Conclusion. \n\n\nThe most important feature is \"Working Capital to Total Assets\". It have got maximun weight about 26.7% to determine bankruptcy of company. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/company-bankruptcy-prediction/data.csv\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.columns[1]].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterating the columns \ni = 0\nfor i in range(len(df.columns)): \n    print(df.columns[i]) \n    #print(\"Max - Min:\", df[df.columns[i]].max() - df[df.columns[i]].min())\n    #print(\"(Max - Min) / Mean:\", (df[df.columns[i]].max() - df[df.columns[i]].min() / df[df.columns[i]].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rename columns. Eliminate spaces. "},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor i in range(len(df.columns)): \n    old_name = df.columns[i]\n    new_name = df.columns[i].strip().replace(\" \", \"_\")\n\n    df.rename(columns={old_name: new_name}, inplace = True)\n\n    print(df.columns[i], \":\", \"Mean:\", df[df.columns[i]].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={\"Bankrupt?\": \"Bankrupt\"}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bankrupt_false = df.loc[df.Bankrupt == 0]\ndf_bankrupt_true = df.loc[df.Bankrupt == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=df, x=\"ROA(C)_before_interest_and_depreciation_before_interest\", \n                y=\"Cash_Flow_Per_Share\", hue=\"Bankrupt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.histplot(data=df_bankrupt_false, x=\"ROA(C)_before_interest_and_depreciation_before_interest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build Seaborn ScatterPlot on data. We will take pair of columns. \nx = \"ROA(C)_before_interest_and_depreciation_before_interest\"\ny - each another column\n\nBuild ScatterPlot with code\n\nsns.scatterplot(data=df, \n                x=\"ROA(C)_before_interest_and_depreciation_before_interest\", \n                y=\"Tax_rate_(A)\", hue=\"Bankrupt\")\n\nget image\n\n![https://ibb.co/cykbQMR](https://ibb.co/cykbQMR)\n\nthen\n\nbuild ScatterPlot with code\n\nsns.scatterplot(data=df, \n                x=\"ROA(C)_before_interest_and_depreciation_before_interest\", \n                y=\"Operating_Profit_Rate\", hue=\"Bankrupt\")\n                \n![https://ibb.co/Tg42HhT](https://ibb.co/Tg42HhT)\n\nAt last image we can see practically straight line at the top. I don't think that such line will help predict bancrupcy. \n\nIf you can not see images you can run code to get them below. \n\nI checked all pair of columns and dpor all columns which won't help us. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=df, \n                x=\"ROA(C)_before_interest_and_depreciation_before_interest\", \n                y=\"Operating_Profit_Rate\", hue=\"Bankrupt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=df, \n                x=\"ROA(C)_before_interest_and_depreciation_before_interest\", \n                y=\"Tax_rate_(A)\", hue=\"Bankrupt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most valueable pairs with \"ROA(C)_before_interest_and_depreciation_before_interest\" are\n* Operating_Expense_Rate\n* Research_and_development_expense_rate\n* Interest-bearing_debt_interest_rate\n* Tax_rate_(A)\n\nI make 2 new DataFrames: most_df and wo_week_df.\n\n*most_df* contains the most valueable columns.\n\n*wo_week_df* does not contain the weekest columns such as \n*     Pre-tax_net_Interest_Rate\n*     Operating_Profit_Rate\n*     After-tax_net_Interest_Rate\n*     Non-industry_income_and_expenditure/revenue\n*     Continuous_interest_rate_(after_tax)\n*     Revenue_Per_Share_(Yuan_Â¥)\n*     Realized_Sales_Gross_Profit_Growth_Rate\n    \nAnd then I will work with these two DF. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=df, \n                x=\"ROA(C)_before_interest_and_depreciation_before_interest\", \n                y=\"Quick_Assets/Current_Liability\", hue=\"Bankrupt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=df, \n                x=\"ROA(C)_before_interest_and_depreciation_before_interest\", \n                y=\"Cash/Current_Liability\", hue=\"Bankrupt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each column contents values from min_value to max_value, for example, from 0 to 1, as for column \"ROA(C)_before_interest_and_depreciation_before_interest\"\n# Distribute values for 5 groups:\n# 1st group: all values from 0 to 0.2, 5th group: all values from 0.8 to 1\n# It is not a percentiles. It's important. \n\n# Find border values for each group for data in column \"ROA(C)_before_interest_and_depreciation_before_interest\"\n\nmin_value = df['ROA(C)_before_interest_and_depreciation_before_interest'].min()\nmax_value = df['ROA(C)_before_interest_and_depreciation_before_interest'].max()\n\nmm_range = max_value - min_value\nmm_range\n\nq00 = min_value\nq20 = min_value + mm_range * 0.2\nq40 = min_value + mm_range * 0.4\nq60 = min_value + mm_range * 0.6\nq80 = min_value + mm_range * 0.8\nq100 = max_value\n\nprint(q00, q20, q40, q60, q80, q100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print count of values in each group\n\nprint(df['ROA(C)_before_interest_and_depreciation_before_interest'].count())\nprint()\nprint(\"0-.2\", \".2-.4\", \".4-.6\", \".6-.8\", \" .8-1\")\nprint(df.loc[(df['ROA(C)_before_interest_and_depreciation_before_interest'] >= q00) & (df['ROA(C)_before_interest_and_depreciation_before_interest'] <= q20)]['ROA(C)_before_interest_and_depreciation_before_interest'].count(), \n      df.loc[(df['ROA(C)_before_interest_and_depreciation_before_interest'] >= q20) & (df['ROA(C)_before_interest_and_depreciation_before_interest'] <= q40)]['ROA(C)_before_interest_and_depreciation_before_interest'].count(),\n      df.loc[(df['ROA(C)_before_interest_and_depreciation_before_interest'] >= q40) & (df['ROA(C)_before_interest_and_depreciation_before_interest'] <= q60)]['ROA(C)_before_interest_and_depreciation_before_interest'].count(),\n      df.loc[(df['ROA(C)_before_interest_and_depreciation_before_interest'] >= q60) & (df['ROA(C)_before_interest_and_depreciation_before_interest'] <= q80)]['ROA(C)_before_interest_and_depreciation_before_interest'].count(),\n      df.loc[(df['ROA(C)_before_interest_and_depreciation_before_interest'] >= q80) & (df['ROA(C)_before_interest_and_depreciation_before_interest'] <= q100)]['ROA(C)_before_interest_and_depreciation_before_interest'].count(),\n     sep=\"  \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find border values for each group for data in column \"Operating_Expense_Rate\"\n\nmin_value = df['Operating_Expense_Rate'].min()\nmax_value = df['Operating_Expense_Rate'].max()\n\nmm_range = max_value - min_value\nmm_range\n\nq00 = min_value\nq20 = min_value + mm_range * 0.2\nq40 = min_value + mm_range * 0.4\nq60 = min_value + mm_range * 0.6\nq80 = min_value + mm_range * 0.8\nq100 = max_value\n\nprint(q00, q20, q40, q60, q80, q100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print count of values in each group\n\nprint(df['Operating_Expense_Rate'].count())\nprint()\nprint(\"0-.2\", \".2-.4\", \".4-.6\", \".6-.8\", \" .8-1\")\nprint(df.loc[(df['Operating_Expense_Rate'] >= q00) & (df['Operating_Expense_Rate'] <= q20)]['Operating_Expense_Rate'].count(), \n      df.loc[(df['Operating_Expense_Rate'] >= q20) & (df['Operating_Expense_Rate'] <= q40)]['Operating_Expense_Rate'].count(),\n      df.loc[(df['Operating_Expense_Rate'] >= q40) & (df['Operating_Expense_Rate'] <= q60)]['Operating_Expense_Rate'].count(),\n      df.loc[(df['Operating_Expense_Rate'] >= q60) & (df['Operating_Expense_Rate'] <= q80)]['Operating_Expense_Rate'].count(),\n      df.loc[(df['Operating_Expense_Rate'] >= q80) & (df['Operating_Expense_Rate'] <= q100)]['Operating_Expense_Rate'].count(),\n     sep=\"  \")\n\n# See output\n# df['Operating_Expense_Rate'].count() = 6819\n# Count of values in range from 0.0 to 0.2 = 4818 or 4818 / 6819 = 0.7, or 70%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_name = 'Current_Assets/Total_Assets'\n\n# Find border values for each group for data in column \"Operating_Expense_Rate\"\n\nmin_value = df[column_name].min()\nmax_value = df[column_name].max()\n\nmm_range = max_value - min_value\nmm_range\n\nq00 = min_value\nq20 = min_value + mm_range * 0.2\nq40 = min_value + mm_range * 0.4\nq60 = min_value + mm_range * 0.6\nq80 = min_value + mm_range * 0.8\nq100 = max_value\n\nprint(q00, q20, q40, q60, q80, q100)\n\n# Print count of values in each group\n\nprint(df[column_name].count())\nprint()\nprint(\"0-.2\", \".2-.4\", \".4-.6\", \".6-.8\", \" .8-1\")\nprint(df.loc[(df[column_name] >= q00) & (df[column_name] <= q20)][column_name].count(), \n      df.loc[(df[column_name] >= q20) & (df[column_name] <= q40)][column_name].count(),\n      df.loc[(df[column_name] >= q40) & (df[column_name] <= q60)][column_name].count(),\n      df.loc[(df[column_name] >= q60) & (df[column_name] <= q80)][column_name].count(),\n      df.loc[(df[column_name] >= q80) & (df[column_name] <= q100)][column_name].count(),\n     sep=\"  \")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find border values for each group for data in column \"Pre-tax_net_Interest_Rate\"\n\nmin_value = df['Pre-tax_net_Interest_Rate'].min()\nmax_value = df['Pre-tax_net_Interest_Rate'].max()\n\nmm_range = max_value - min_value\nmm_range\n\nq00 = min_value\nq20 = min_value + mm_range * 0.2\nq40 = min_value + mm_range * 0.4\nq60 = min_value + mm_range * 0.6\nq80 = min_value + mm_range * 0.8\nq100 = max_value\n\nprint(q00, q20, q40, q60, q80, q100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print count of values in each group\n\nprint(df['Pre-tax_net_Interest_Rate'].count())\n\nprint()\n\nprint(\"0-.2\", \".2-.4\", \".4-.6\", \".6-.8\", \"  .8-1\")\n\nprint(df.loc[(df['Pre-tax_net_Interest_Rate'] >= q00) & (df['Pre-tax_net_Interest_Rate'] <= q20)]['Pre-tax_net_Interest_Rate'].count(), \n      df.loc[(df['Pre-tax_net_Interest_Rate'] >= q20) & (df['Pre-tax_net_Interest_Rate'] <= q40)]['Pre-tax_net_Interest_Rate'].count(),\n      df.loc[(df['Pre-tax_net_Interest_Rate'] >= q40) & (df['Pre-tax_net_Interest_Rate'] <= q60)]['Pre-tax_net_Interest_Rate'].count(),\n      df.loc[(df['Pre-tax_net_Interest_Rate'] >= q60) & (df['Pre-tax_net_Interest_Rate'] <= q80)]['Pre-tax_net_Interest_Rate'].count(),\n      df.loc[(df['Pre-tax_net_Interest_Rate'] >= q80) & (df['Pre-tax_net_Interest_Rate'] <= q100)]['Pre-tax_net_Interest_Rate'].count(),\n     sep=\"     \")\n\n# See output\n# df['Pre-tax_net_Interest_Rate'].count() = 6819\n# Count of values in range from 0.6 to 0.8 = 6796 or 6796 / 6819 = 0.9966270714180965, more than 99%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check our method for one more column \"Operating_Profit_Rate\"\n# Find border values for each group for data in column \"Operating_Profit_Rate\"\n\nmin_value = df['Operating_Profit_Rate'].min()\nmax_value = df['Operating_Profit_Rate'].max()\n\nmm_range = max_value - min_value\nmm_range\n\nq00 = min_value\nq20 = min_value + mm_range * 0.2\nq40 = min_value + mm_range * 0.4\nq60 = min_value + mm_range * 0.6\nq80 = min_value + mm_range * 0.8\nq100 = max_value\n\nprint(q00, q20, q40, q60, q80, q100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print count of values in each group\n\nprint(df['Operating_Profit_Rate'].count())\n\nprint()\n\nprint(\"0-.2\", \".2-.4\", \".4-.6\", \".6-.8\", \"  .8-1\")\n\nprint(df.loc[(df['Operating_Profit_Rate'] >= q00) & (df['Operating_Profit_Rate'] <= q20)]['Operating_Profit_Rate'].count(), \n      df.loc[(df['Operating_Profit_Rate'] >= q20) & (df['Operating_Profit_Rate'] <= q40)]['Operating_Profit_Rate'].count(),\n      df.loc[(df['Operating_Profit_Rate'] >= q40) & (df['Operating_Profit_Rate'] <= q60)]['Operating_Profit_Rate'].count(),\n      df.loc[(df['Operating_Profit_Rate'] >= q60) & (df['Operating_Profit_Rate'] <= q80)]['Operating_Profit_Rate'].count(),\n      df.loc[(df['Operating_Profit_Rate'] >= q80) & (df['Operating_Profit_Rate'] <= q100)]['Operating_Profit_Rate'].count(),\n     sep=\"     \")\n\n# See output\n# df['Operating_Profit_Rate'].count() = 6819\n# Count of values in range from 0.8 to 1.0 = 6817 or 6817 / 6819 = 0.9997067018624431, more than 99%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I guess if count of values in one of range {from 0.0 to 0.2, from 0.2 to 0.4, from 0.4 to 0.6, from 0.6 to 0.8, from 0.8 to 1.0} are more than 95% of total count of values in column than this column we can drop. This column will give us never usefull data. \n\n\nWhen data concentrate around one value it means that all of companies have a similaraty in factors. Therefore, we cann't predict bankrupcy using them. \n\n\nCreate new DataFrame *df_drop* and drop all columns which have got concenration in values. \n\nFor example, we have a column which contains values and count values:\n\nValue              Count            % of total count (6819) of values in column\n\n0.0 to 0.2          446                        6.54\n\n0.2 to 0.4         1748                       25.63\n\n0.4 to 0.6         2113                       30.98\n\n0.6 to 0.8         1627                       23.86\n\n0.8 to 1.0          885                       12.98\n\n\nWe don't drop such columns like column in an example. \n\nWe drop columns which on of range has % of total count '>= 75%' (or '>= 0.75'). \n\n\nYou can see code below. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_drop = df\ndf_drop.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"flag = False\n\ni = 0\n\ntotal_columns = len(df.columns)\nprint(total_columns)\nprint()\n\ncount_total = 6819\n# count_total = df_drop[df_drop.columns[i]].count()\n\nfor i in range(1, total_columns):\n    for j in range(0, 10, 2):\n        min_value = df[df.columns[i]].min() # We need to work with source DataFrame because \n        max_value = df[df.columns[i]].max() # if we work with df_drop each step which delete columns shift their numbers\n        \n        print(\"i:\", i, \"min:\", min_value, \"max:\", max_value)\n        \n        mm_range = max_value - min_value\n        \n        qlow = min_value + mm_range * (j / 10)\n        qhigh = min_value + mm_range * (j / 10 + 0.2)\n        \n        print(\"j:\", j, \"qlow:\", qlow, \"qhigh:\", qhigh)\n        \n                \n        if ((df.loc[(df[df.columns[i]] >= qlow) & (df[df.columns[i]] <= qhigh)][df.columns[i]].count()) / count_total) > 0.6:\n            flag = True\n            column_name = df.columns[i]\n            df_drop = df_drop.drop(columns=[column_name]) #We get name of columns from source DataFrame 'df'. And drop columns by this name in 'df_drop'\n            break\n    print(flag)\n    flag = False\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_drop.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.pairplot(df_drop, hue=\"Bankrupt\")\n\n# To plot needs time in Kaggle. Be patience. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Random Forest or something close to them \nfrom sklearn import tree\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\nfrom IPython.display import Image\n#import pydotplus\n\nfrom IPython.display import HTML\nstyle = \"<style>svg{width:70% !important;height:70% !important;}</style>\"\nHTML(style)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# X and y from df_drop\n\nUse DataFrame df_drop.\n\nIt splits to X (source data) and y (result Series). "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_drop.drop(['Bankrupt'], axis = 1)\ny = df_drop['Bankrupt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset Split.\n# 1st variant. Split with \"train_test_split\"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Learn RandomForestClassifier using GridSearchCV\nclf_rf = RandomForestClassifier(criterion='entropy', random_state=0)\nparametrs = {'n_estimators': range(10, 30, 10),\n             'max_depth': range(1, 12, 2),\n             'min_samples_leaf': range(1, 7),\n             'min_samples_split': range(2, 11, 2)}\ngrid_search_cv_clf = GridSearchCV(clf_rf, parametrs, cv=3, n_jobs=-1)\ngrid_search_cv_clf.fit(X_train, y_train)\ngrid_search_cv_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"{'max_depth': 9,\n 'min_samples_leaf': 2,\n 'min_samples_split': 8,\n 'n_estimators': 10}"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf = grid_search_cv_clf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances_df = pd.DataFrame({'features':list(X_train.columns), \n                                       'feature_importances':  best_clf.feature_importances_})\\\n                        .sort_values(by='feature_importances')\n\nfeature_importances_df.plot.pie(\n                        explode=[0.1]*len(X_train.columns),\n                        labels = feature_importances_df.features,\n                        y = 'feature_importances',\n                        autopct='%1.1f%%',\n                        shadow=True,\n                        legend=False,\n                        figsize=(10, 10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# features                          feature_importances\n\nCurrent_Assets/Total_Assets\t                0.098624\n\nTotal_Asset_Growth_Rate\t                    0.111920\n\nCFO_to_Assets\t                            0.116886\n\nInventory_and_accounts_receivable/Net_value\t0.118806\n\nWorking_Capital_to_Total_Assets\t            0.267565\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = grid_search_cv_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Decission Tree\n# Use parameters found with RandomForest adn GridSearchCV\n# {'max_depth': 9, 'min_samples_leaf': 2, 'min_samples_split': 8\n\n\nclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=9, min_samples_split=8, min_samples_leaf=2)\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=list(X), class_names=['Non-Bankrupt', 'Bankrupt'], filled=True))\n#graph_png = pydotplus.graph_from_dot_data(graph)\n#Image(graph.create_png())\ndisplay(SVG(graph.pipe(format='svg')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(y_test, y_pred, average=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_score(y_test, y_pred, average='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_test, y_pred, average=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_test, y_pred, average='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, y_pred, average=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a new more compact Decission Tree with \"min_samples_leaf = 4\" and max_depth=5\n\n\nclf_compact = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=4, min_samples_leaf=2)\nclf_compact.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph = Source(tree.export_graphviz(clf_compact, out_file=None, feature_names=list(X), class_names=['Non-Bankrupt', 'Bankrupt'], filled=True))\n#graph_png = pydotplus.graph_from_dot_data(graph)\n#Image(graph.create_png())\ndisplay(SVG(graph.pipe(format='svg')))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}