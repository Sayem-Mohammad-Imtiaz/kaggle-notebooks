{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"!unzip /content/bbc-fulltext.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nimport re\nf=[]\nfor (dirpath, dirnames, filenames) in os.walk('/content/bbc'):\n    for filename in filenames:\n        f.append(dirpath+'/'+filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=[]\nn_id=0\nfor filepath in f:\n    try:\n      with open(filepath,'r', encoding='utf-8') as infile:\n          text=''.join(infile.readlines())\n          d.append({\n              'id':n_id,\n              'filepath':filepath,\n              'text':text,\n              'category':filepath.split('/')[3]\n          })\n          n_id+=1        \n    except Exception:\n        pass\ndf=pd.DataFrame(d)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalizeText(text):\n    output=text.lower()\n    output=re.sub('[\\s]+',' ', output)\n    output=re.sub('[^a-z0-9\\ ]','', output)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\ndef stopFilterWords(text):\n    words=word_tokenize(text)\n    stop_words=set(stopwords.words('english'))\n    new_text=\"\"\n    for w in words:\n        if w not in stop_words:\n            new_text=new_text+\" \"+w\n    return new_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text=df.text.apply(normalizeText)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text=df.text.apply(stopFilterWords)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import stem\nfrom nltk.tokenize import word_tokenize\ndef lemmat(text):\n    lem=stem.WordNetLemmatizer()\n    words=word_tokenize(text)\n    new_text=\"\"\n    for word in words:\n        w=lem.lemmatize(word)\n        new_text=new_text+\" \"+w\n    return new_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text=df.text.apply(lemmat)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate TF-IDF\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport numpy as np\ntext = df['text']\nvect=CountVectorizer(ngram_range=(3,3))\nvector_values=vect.fit_transform(text)\ndata=vect.get_feature_names()\nprint(np.shape(data))\nvect1=TfidfVectorizer(ngram_range=(2,2), use_idf=True)\nvector_values1=vect1.fit_transform(text)\ndata=vect1.get_feature_names()\nX=vector_values1.toarray()\nprint(np.shape(vector_values1))\nprint(data[0])\nprint(X[0])\nsums=X.sum(axis=0)\nprint(sums[0])\nval=[]\nfor col, term in enumerate(data): \n    try:\n        val.append((term,sums[col]))\n    except Exception as e:\n        pass\ndf1=pd.DataFrame(val, columns=['term','ranking'])\ndf1=df1.sort_values('ranking',ascending=False)\nprint(df1.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cosine Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\ndistance = 1-cosine_similarity(vector_values1)\nprint(distance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Commented out IPython magic to ensure Python compatibility.\n#Elbow method\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\n# %matplotlib inline\ndistort=[]\n#vector_values1=vector_values1.reshape(vector_values1.shape[1],2)\nr=range(2,10)\nfor i in r:\n  Kcluster=KMeans(n_clusters=i).fit(vector_values1)\n  predicts=Kcluster.fit_predict(vector_values1)\n  sills=silhouette_score(vector_values1,predicts,metric='euclidean')\n  #print(sills)\n  distort.append(sills)\nplt.plot(r, distort, 'bx-')\nplt.xlabel('No of Clusters')\nplt.ylabel('Distortions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_values1.ndim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_values1[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n# Create a PCA instance: pca\npca = TruncatedSVD(n_components=2)\nprincipalComponents = pca.fit_transform(vector_values1)\n# Plot the explained variances\nfeatures = range(2)\nplt.bar(features, pca.explained_variance_ratio_, color='black')\nplt.xlabel('PCA features')\nplt.ylabel('variance %')\nplt.xticks(features)\n# Save components to a DataFrame\nPCA_components = pd.DataFrame(principalComponents)\nPCA_components.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = TruncatedSVD(n_components=5).fit(vector_values1)\ndata2D = pca.transform(vector_values1)\nprint(np.shape(data2D))\nprint(np.shape(vector_values1))\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 12), dpi=80, facecolor='w', edgecolor='k')\nplt.scatter(data2D[:,0], data2D[:,1], c = 'r')\nplt.scatter(data2D[:,1], data2D[:,2], c = 'b')\nplt.scatter(data2D[:,2], data2D[:,3], c = 'g')\nplt.scatter(data2D[:,3], data2D[:,4], c = 'c')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(PCA_components[0], PCA_components[1], alpha=0.3, color='black')\nplt.scatter(PCA_components[1], PCA_components[2], alpha=0.3, color='blue')\nplt.scatter(PCA_components[2], PCA_components[3], alpha=0.3, color='green')\nplt.scatter(PCA_components[3], PCA_components[4], alpha=0.3, color='red')\nplt.scatter(PCA_components[4], PCA_components[5], alpha=0.3, color='magenta')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nlen1=int(0.8*df.shape[0])\ndf1=df.iloc[:len1,:]\nkmeans = KMeans(n_clusters=5).fit(vector_values1[:len1])\ncenters2D = pca.transform(kmeans.cluster_centers_)\n#len=range(0,df.shape[0])\nlen=range(0,len1)\ncentres=kmeans.labels_.tolist()\nfor i in len:\n  df1['Centre']=centres\n  df1['Vector_Value']=np.array(data2D[:len1,:]).tolist()\nplt.scatter(centers2D[:,0], centers2D[:,1], c='k',\n            marker='x', s=200, linewidths=10)\nplt.show()  \n# pca = TruncatedSVD(n_components=5).fit(vector_values1)\n# data2D = pca.transform(vector_values1)\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 12), dpi=80, facecolor='w', edgecolor='k')\nplt.scatter(data2D[:,0], data2D[:,1], c = 'r')\nplt.scatter(data2D[:,1], data2D[:,2], c = 'b')\nplt.scatter(data2D[:,2], data2D[:,3], c = 'g')\nplt.scatter(data2D[:,3], data2D[:,4], c = 'c')\n# plt.hold(True)\nplt.scatter(centers2D[:,0], centers2D[:,1], c='k',\n            marker='x', s=200, linewidths=10)\nplt.show()   \ndf1.head()\ndf1['Centre'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.externals import joblib\njoblib.dump(kmeans, 'model_clust.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df.iloc[len1:,:]\nvect2=vector_values1[len1:]\nprint(df2.shape)\ndf2.sample(100)\n#saved_model=joblib.load('model_clust.pkl')\n#saved_model.predict(vect2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_model=joblib.load('model_clust.pkl')\nsaved_model.predict(vect2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}