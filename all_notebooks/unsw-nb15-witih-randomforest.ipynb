{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport warnings; warnings.simplefilter('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, RobustScaler, Normalizer\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\nimport time\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if '.csv' in filename:\n            print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv')\ntest = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv')\nif train.shape[0]<100000:\n    print(\"Fixing train test\")\n    train, test = test, train\n\ndrop_columns = ['attack_cat', 'id']\nfor df in [train, test]:\n    for col in drop_columns:\n        if col in df.columns:\n            print('Dropping '+col)\n            df.drop([col], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Util methods"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def detection_rate(y_true, y_pred):\n    CM = metrics.confusion_matrix(y_true, y_pred)\n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    return TP/(TP+FN)\n\ndef false_positive_rate(y_true, y_pred):\n    CM = metrics.confusion_matrix(y_true, y_pred)\n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    return FP/(FP+TN)\n\ndef false_alarm_rate(y_true, y_pred):\n    CM = metrics.confusion_matrix(y_true, y_pred)\n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    return (FP+FN)/(TP+TN+FP+FN)\n\ndef get_xy(df):\n    return pd.get_dummies(df.drop(['label'], axis=1)), df['label']\n\ndef get_cat_columns(train):\n    categorical = []\n    for col in train.columns:\n        if train[col].dtype == 'object':\n            categorical.append(col)\n    return categorical\n\ndef label_encode(train, test):\n    for col in get_cat_columns(train):\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))\n    return train, test\n\ndef process_feature(df):\n    df.loc[~df['state'].isin(['FIN', 'INT', 'CON', 'REQ', 'RST']), 'state'] = 'others'\n    df.loc[~df['service'].isin(['-', 'dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3']), 'service'] = 'others'\n    df.loc[df['proto'].isin(['igmp', 'icmp', 'rtp']), 'proto'] = 'igmp_icmp_rtp'\n    df.loc[~df['proto'].isin(['tcp', 'udp', 'arp', 'ospf', 'igmp_icmp_rtp']), 'proto'] = 'others'\n    return df\n\ndef get_train_test(train, test, feature_engineer=True, label_encoding=False, scaler=StandardScaler()):\n    x_train, y_train = train.drop(['label'], axis=1), train['label']\n    x_test, y_test = test.drop(['label'], axis=1), test['label']\n    \n    if feature_engineer:\n        x_train, x_test = process_feature(x_train), process_feature(x_test)\n    \n    categorical_columns = get_cat_columns(x_train)\n    non_categorical_columns = [x for x in x_train.columns if x not in categorical_columns]\n    if scaler is not None:\n        x_train[non_categorical_columns] = scaler.fit_transform(x_train[non_categorical_columns])\n        x_test[non_categorical_columns] = scaler.transform(x_test[non_categorical_columns])\n\n    if label_encoding:\n        x_train, x_test = label_encode(x_train, x_test)\n        features = x_train.columns\n    else:\n        x_train = pd.get_dummies(x_train)\n        x_test = pd.get_dummies(x_test)\n        print(\"Column mismatch {0}, {1}\".format(set(x_train.columns)- set(x_test.columns),  set(x_test.columns)- set(x_train.columns)))\n        features = list(set(x_train.columns) & set(x_test.columns))\n    print(f\"Number of features {len(features)}\")\n    x_train = x_train[features]\n    x_test = x_test[features]\n\n    return x_train, y_train, x_test, y_test\n\ndef results(y_test, y_pred):\n    acc = metrics.accuracy_score(y_test, y_pred)\n    pre = metrics.precision_score(y_test, y_pred)\n    rec = metrics.recall_score(y_test, y_pred)\n    f1 = metrics.f1_score(y_test, y_pred)\n    print(f\"Acc {acc}, Precision {pre}, Recall {rec}, F1-score {f1}\")\n    \n    CM = metrics.confusion_matrix(y_test, y_pred)\n    TN = CM[0][0]\n    FN = CM[1][0]\n    TP = CM[1][1]\n    FP = CM[0][1]\n    \n    # detection rate or true positive rate\n    DR = TP*100/(TP+FN)\n    # false positive rate\n    FPR = FP*100/(FP+TN)\n    # false alarm rate \n    FAR = (FP+FN)*100/(TP+TN+FP+FN)\n    \n    print(\"DR {0}, FPR {1}, FAR {2}\".format(DR, FPR, FAR))\n    print(metrics.classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def cross_validation(params, X, Y):\n    y_probs = []\n    y_vals = []\n\n    # for tr_idx, val_idx in tqdm(kf.split(X, Y), total=folds):\n    for tr_idx, val_idx in kf.split(X, Y):\n        x_train, y_train = X.iloc[tr_idx], Y[tr_idx]\n        x_val, y_val = X.iloc[val_idx], Y[val_idx]\n        clf = RandomForestClassifier(**params)\n        clf.fit(x_train, y_train)\n        y_prob = clf.predict_proba(x_val)[:, 1]\n        \n        y_probs.append(y_prob)\n        y_vals.append(y_val)\n        \n    acc, pre, rec, f1, far, fpr, dr, auc = 0, 0, 0, 0, 0, 0, 0, 0\n    folds = len(y_probs)\n    for i in range(folds):\n        y_prob, y_val = y_probs[i], y_vals[i]\n        y_pred = np.where(y_prob>=0.5, 1, 0)\n\n        acc += metrics.accuracy_score(y_val, y_pred)/folds\n        f1 += metrics.f1_score(y_val, y_pred)/folds\n        pre += metrics.precision_score(y_val, y_pred) /folds\n        rec += metrics.recall_score(y_val, y_pred) /folds\n        dr += detection_rate(y_val, y_pred) /folds\n        fpr += false_positive_rate(y_val, y_pred) /folds\n        far += false_alarm_rate(y_val, y_pred)/folds\n        auc += metrics.roc_auc_score(y_val, y_prob) /folds \n    \n    print(f\"Acc {acc}, Precision {pre}, Recall {rec}, F1-score {f1} \\nFAR {far}, FPR {fpr}, DR {dr} , AUC {auc}\")\n    \ndef test_run(params, X, Y):\n    clf = RandomForestClassifier(**params)\n    clf.fit(X, Y)\n    y_pred = clf.predict(x_test)\n    results(y_test, y_pred)\n    \n    y_prob = clf.predict_proba(x_test)[:, 1]\n    print(\"Auc {0}\".format(metrics.roc_auc_score(y_test, y_prob)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = 10\nseed = 1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\nparams = {\n    'n_estimators': 100,\n    'random_state':1,\n    'class_weight': {0:2, 1:1}\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Why overfitting will be bad\nEvaluating the model on the same data, that was used for training gives wrong info about the actual performance of the model. Here we can see the model achieve nearly 100% accuracy of train data. However, when ten-fold cross validation is used on train data it reduces to 96%. And on test data the model's performance falls drastically to 87.16%. This indicates the model needs to be generalized and effective measures need to be taken to reduce overfit."},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y, x_test, y_test = get_train_test(train, test, feature_engineer=False, label_encoding=True, scaler=None)\n\nclf = RandomForestClassifier()\nclf.fit(X,Y)\ny_pred = clf.predict(X)\nresults(Y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validation(params, X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_run(params, X, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combinations of Feature Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Features with low importance\ndrop_columns = ['response_body_len', 'is_sm_ips_ports', 'ct_flw_http_mthd', 'trans_depth', 'dwin', 'ct_ftp_cmd', 'is_ftp_login']\nfor df in [train, test]:\n    df.drop(drop_columns, axis=1, inplace=True)\nX, Y, x_test, y_test = get_train_test(train, test, feature_engineer=True, label_encoding=False, scaler=RobustScaler())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"| Preprocess | Train Acc | Test Acc|\n|:-----:|:------:|:------:|\n| LabelEncoded | 95.92 | 87.40 |\n| OneHotEncoded | 95.65 | 87.87 |\n| OneHotEncoded, FeatureEngineer| 95.66 | 88.03 |\n| OneHotEncoded, FeatureEngineer, MinMaxScaler| 95.80 | 87.49 |\n| OneHotEncoded, FeatureEngineer, RobustScaler| 95.67 | 87.92 |\n| OneHotEncoded, FeatureEngineer, StandardScaler| 95.67 | 87.89 |\n| OneHotEncoded, FeatureEngineer, FeatureSelection, StandardScaler| 95.72 | 87.81 |\n| OneHotEncoded, FeatureEngineer, FeatureSelection, RobustScaler| 95.70 | 88.03 |\n| OneHotEncoded, FeatureEngineer, FeatureSelection, MinMaxScaler| 95.77 | 87.73 |"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'random_state':1,\n    'class_weight': {0:2, 1:1}\n}\nstart_time = time.clock()\ncross_validation(params, X, Y)\nprint(\"Time spent in 10-fold cross validation of train data \", time.clock()-start_time)\n\nstart_time = time.clock()\ntest_run(params, X, Y)\nprint(\"Time spent in test run \", time.clock()-start_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hypertuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y, x_test, y_test = get_train_test(train, test, feature_engineer=True, label_encoding=False, scaler=RobustScaler())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<table>\n    <tr>\n    <th>PreProcessing</th> <th style=\"text-align:center\">Parameters</th><th>Train Acc</th><th>Test Acc</th>\n    </tr>\n    <tr>\n      <td rowspan=\"8\">OneHot encoding, StandardScaler</td><td>n_estimators 10, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.10</td><td>91.05</td>\n    </tr>\n    <tr>\n        <td>n_estimators 10, max_depth 10, max_features 20, class_weight {0:2, 1: 1}</td> <td>94.16</td> <td>91.61</td>\n    </tr>\n    <tr>\n        <td>n_estimators 10, max_depth 20, max_features 20, class_weight {0:2, 1: 1}</td> <td>95.64</td> <td>89.16</td>\n    </tr>\n    <tr>\n        <td>n_estimators 30, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td> <td>94.19</td> <td>91.66</td>\n    </tr>\n     <tr>\n        <td>n_estimators 10</td> <td>95.66</td> <td>87.90</td>\n    </tr>\n    <tr>\n        <td>n_estimators 10, max_depth 20</td> <td>95.73</td> <td>86.91</td>\n    </tr>\n    <tr>\n        <td>n_estimators 10, max_features 20</td> <td>95.76</td> <td>87.57</td>\n    </tr>\n    <tr>\n        <td>n_estimators 50</td> <td>96.06</td> <td>87.31</td>\n    </tr>\n     <tr>\n      <td rowspan=\"2\">OneHot encoding, MinMaxScaler</td><td>n_estimators 10, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td><td>93.98</td><td>91.49</td>\n    </tr>\n    <tr>\n      <td>n_estimators 10, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.20</td><td>91.32</td>\n    </tr>\n    <tr>\n      <td rowspan=\"3\">OneHot encoding, RobustScaler</td><td>n_estimators 10, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td><td>94.08</td><td>91.90</td>\n    </tr>\n    <tr>\n      <td>n_estimators 50, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td><td>94.21</td><td>91.68</td>\n    </tr>\n    <tr>\n      <td>n_estimators 20, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.21</td><td>91.36</td>\n    </tr>\n    <tr>\n      <td rowspan=\"1\">OneHot encoding</td><td>n_estimators 10, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.20</td><td>91.02</td>\n    </tr>\n</table>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for n_estimators in [20, 50]:\n    for max_features in [10, 30]:\n        print(\"n_estimators {0} max_features {1}\".format(n_estimators, max_features))\n        params = {\n           'n_estimators': n_estimators,\n            'random_state':1,\n            'max_depth':10,\n            'max_features': max_features,\n            'class_weight': {0:2, 1:1}\n        }\n        cross_validation(params, X, Y)\n        test_run(params, X, Y)\n        print()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}