{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CORD-19: Most Collaborative & Influential Authors \n\n## Motivations\n\nThis notebook seeks to create a methodology to determine and quantify those individuals adding the most to the literature collected in this dataset. The goal would be to help identify the researchers leading this field and providing the greatest impact to other's work. This can be utilized in other models and potential weightings to highlight work that may deserve additional focus over the general corpus.\n\n## Methodology\n\nThere are various mean to quantify such an individual as well as varying amounts of data to consider. I have split the measures into XXX different metrics each relying on a foundational “Scoring Mechanism.” May in the future also split based on all time vs those wirting since the start of the pandemic if this work proves useful to others.\n\n## Suggestions and Other Resources\n\nBuilding “impact frameworks” like this have always been a deep interest of mine so please comment with recommendations and I would love to augment this work with other input.\n \n## Data Sources\n\nBeyond the provided data, I additionally took advantage of the CORD19 Metadata Enrichment Dataset. This can be loaded into any notebook using the *+ Add Data* button at the top-right of the notebook and using the url linked below.\n\n### [CORD-19 Metadata Enrichment Dataset](https://www.kaggle.com/dannellyz/cord19-metadata-enrichment)"},{"metadata":{},"cell_type":"markdown","source":"# Starting with only the papers in the CORD-19 Dataset\nWhile this is the most limted dataset it does hold most true to the effort of the research effort to evaluate the given data. This would in turn give the most influential authors ranked only by those contibuting most to this set of literature.\n\n## Scoring Mechaism: CORD-19 Impact\nCurrently this is is made up of the simple equation of two points for every paper and one point for every references to their work in another paper in the dataset. This equations says that authoring a paper on the subject is twice as significant as having your work referenced in someone elses paper. I am happy to take thoughts on reworking this model. I specifically did not go tieh H-Factor due to its controversial nature. Also happy to discuss this decision as well.\n\n$$\\sum_{\\text{All Papers}} 2*_{(\\text{Authored Paper in CORD-19)}} + 1*_{(\\text{Author's Paper Referenced in other CORD-19 Paper)}} \\rightarrow \\text{Normalized} $$\n\n### Most Influential Authors\n\n#### All Time\n1. Yuen, Kwok-Yung\n2. Perlman, Stanley\t\n3. Baric, Ralph S.\t\n4. Drosten, Christian\t\n5. Enjuanes, Luis\t"},{"metadata":{},"cell_type":"markdown","source":"## Count cited works referenced in the CORD dataset"},{"metadata":{},"cell_type":"markdown","source":"Load in the dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#Set max columns expansion\npd.set_option('display.max_columns', None)\n\n#Ensure that the Enrichment Dataset is loaded\nenrich_file_path = \"/kaggle/input/cord19-metadata-enrichment/\"\n\n#Load CORD Metadata\ncord_file_path = \"/kaggle/input/CORD-19-research-challenge/\"\nmetadata = pd.read_csv(cord_file_path + \"metadata.csv\", index_col=\"cord_uid\")\n\n#Set the publish time to Datetime\nmetadata[\"publish_time\"] = pd.to_datetime(metadata[\"publish_time\"])\nmetadata.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get all of the paper paths into a list."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\npaper_folders = ['noncomm_use_subset/','comm_use_subset/','custom_license/','biorxiv_medrxiv/',]\nall_papers = []\nfor folder in paper_folders:\n    #Get all papers in a given folder (2x for traversal)\n    current_dir = cord_file_path + 2*folder\n    papers = os.listdir(current_dir)\n    #Add back in the path for full reference\n    paper_dirs = [current_dir + paper for paper in papers]\n    #Append to all papers list\n    all_papers.extend(paper_dirs)\nprint(\"There are a total of {} papers.\".format(len(all_papers)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count the papers in the various bibliographies. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport json\nimport itertools\nimport multiprocessing\nfrom tqdm.notebook import tqdm\n\n#Get a list of all unique titles found in CORD-19\ncord19_titles = set(metadata.title)\n\n#Start by reading the json and use generator\ndef get_series(paper):\n    one_json = json.load(open(paper))[\"bib_entries\"]\n    title_series = pd.DataFrame.from_dict(one_json, orient=\"index\").title\n    #Drop those not found in CORD-19\n    return list(title_series)\n\ndef get_title_counts():\n    #Place all papers into a dataframe\n    #Use multiprocess to speed up\n    #Send to lower for compare\n    p=multiprocessing.Pool(4)\n    all_bibs = pd.Series(itertools.chain(*p.map(get_series, all_papers))).str.lower()\n    \n    #Count the numer of times each title exists\n    title_counts = all_bibs.value_counts()\n    return title_counts\n\ntitle_counts = get_title_counts()\ntitle_counts.sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge into the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make Title Counts into Dataframe\ntitles_df = pd.DataFrame(title_counts).reset_index()\ntitles_df.columns = [\"title\", \"title_counts\"]\n#Merge with Metadata\n#set title to lower in order to merge\nmetadata[\"title\"] = metadata[\"title\"].str.lower()\nscore_df = metadata.merge(titles_df, on=\"title\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expand the CORD-19 Dataset to have a row for each author"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the authors column by the ; seperator\nscore_df[\"authors\"] = score_df.authors.str.split(\";\")\n\n#Explode the df on the aurhors column\n#This makes a duplicate row for each of the items in the authors list\nscore_df = score_df.explode(\"authors\")\nscore_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get CORD-19 Impact Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill title_coutns NaNs with 0\nscore_df[\"title_counts\"] = score_df[\"title_counts\"].fillna(0)\n\n#Drop columns not used in calc\nscore_df = score_df[[\"authors\", \"title_counts\"]]\n\n#Group by authors and get a count and sum of the \nscore_df = score_df.groupby(\"authors\").agg([\"count\", \"sum\"])\n\n#Normalize the columns since they are such unqiue types\nfrom sklearn import preprocessing\nx = score_df.values\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nscore_df_norm = pd.DataFrame(x_scaled, index=score_df.index, columns=[\"works_authored\", \"works_cited\"])\n\n#Calc Impact\nscore_df_norm[\"CORD19_impact\"] = 2*score_df_norm[\"works_authored\"] + score_df_norm[\"works_cited\"]\n\n#Merge in raw for comparison\ncord_impact_final = pd.merge(score_df,score_df_norm, left_index=True, right_index=True)\ncord_impact_final.sort_values(\"CORD19_impact\", ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Paper Significance: Adding Journal Impact and Social Media References\nThis extends the original dataset with features derived from both the [Microsoft Academic Knowledge API](https://www.kaggle.com/dannellyz/cord19-metadata-enrich-microsoft-academic-api) as well as the [Altmetric API](https://www.kaggle.com/dannellyz/cord19-metadata-enrich-altmetric-api). Those datasets are integrated into the [enrichment dataset](https://www.kaggle.com/dannellyz/cord19-metadata-enrichment). \n\n## Scoring Mechaism: Paper Significance\nThis equation takes the root of both the scoring systems as they are exponentially distributed. This is account for the viral nature of the Altmetric Score and the insular nature of the Journal Rankings.\n\n$$\\sum_{\\text{All Papers}} \\sqrt{_\\text{Scimago Journal & Country Rank}} + \\sqrt{_\\text{Altmetric Score}} \\rightarrow \\text{Normalized}$$\n\n### Most influential Authors\n\n#### All Time\n1. Baric, Ralph S.\t\n2. Yuen, Kwok-Yung\t\n3. Drosten, Christian\t\n4. Perlman, Stanley\t\n5. Daszak, Peter"},{"metadata":{},"cell_type":"markdown","source":"### Add in Journal Rankings to Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"journal_rankings = pd.read_csv(enrich_file_path + \"scimago_journal_rankings.csv\", encoding=\"utf-8\")\njournal_rankings[\"SJR\"] = journal_rankings[\"SJR\"].str.replace(\",\",\"\").astype(\"float\")\njournal_rankings.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Send both journal columns to lower in case of odd caps\nmetadata[\"journal\"] = metadata[\"journal\"].str.lower()\njournal_rankings[\"Title\"] = journal_rankings[\"Title\"].str.lower()\n\n#Add journal raknings to the journals\n#Read in journal names to fix from \n#https://www.kaggle.com/dannellyz/cord19-meta-enrich-replacing-fixing-journal-names\n\njrnl_names = pd.read_csv(enrich_file_path + \"journal_abrv_replace.csv\", names=[\"metadata_name\", \"sjr_name\"])\njrnl_dict = dict(zip(jrnl_names.metadata_name, jrnl_names.sjr_name))\nmetadata[\"journal\"] = metadata[\"journal\"].replace(jrnl_dict)\npaper_significance = metadata.merge(journal_rankings[[\"Title\",\"SJR\"]], left_on=\"journal\", right_on=\"Title\", how=\"left\")\npaper_significance.drop([\"Title\"], axis=1, inplace=True)\npaper_significance.notnull().groupby([\"journal\", \"SJR\"]).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add in Altmetric Score to dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load Altmetric Data\n#Due to licensing this data can not be shared, but a notebook to get an API key and the data can be found here\n#https://www.kaggle.com/dannellyz/cord19-metadata-enrich-altmetric-api\nprivate_file_path = \"/kaggle/input/altmetric-private/\"\naltmetric_metadata = pd.read_csv(private_file_path + \"altmetric_metadata.csv\", usecols=[\"doi\", \"score\"])\naltmetric_metadata.sort_values(by=\"score\", ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add to Dataframe\npaper_significance = paper_significance.merge(altmetric_metadata, left_on=\"doi\", right_on=\"doi\", how=\"left\")\npaper_significance.drop([\"doi\"], axis=1, inplace=True)\npaper_significance.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explode like done in the previous example"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the authors column by the ; seperator\npaper_significance[\"authors\"] = paper_significance.authors.str.split(\";\")\n\n#Explode the df on the aurhors column\n#This makes a duplicate row for each of the items in the authors list\npaper_significance_all = paper_significance.explode(\"authors\")\npaper_significance_all.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get Paper Significance Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n#Fill SJR and score NaNs with 0\npaper_significance_all[\"SJR\"] = paper_significance_all[\"SJR\"].fillna(0)\npaper_significance_all[\"score\"] = paper_significance_all[\"score\"].fillna(0)\n\n#Drop columns not used in calc\npaper_sig = paper_significance_all[[\"authors\", \"SJR\", \"score\"]]\n\n#Group by authors and get a count and sum of the \npaper_sig_groups = paper_sig.groupby(\"authors\").sum()\n#paper_sig_groups.columns = paper_sig_groups.columns.droplevel()\npaper_sig_groups[\"sjr_root\"] = np.sqrt(paper_sig_groups[\"SJR\"])\npaper_sig_groups[\"score_root\"] = np.sqrt(paper_sig_groups[\"score\"])\n\n#Normalize the columns since they are such unqiue types\nfrom sklearn import preprocessing\n#Save df for merging later\nx = paper_sig_groups.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\npaper_sig_norm = pd.DataFrame(x_scaled, index=paper_sig_groups.index, columns=[\"SJR\", \"score\",\"sjr_root\",\"score_root\"])\n\npaper_sig_norm[\"Paper_Significance\"] = paper_sig_norm[\"sjr_root\"] + paper_sig_norm[\"score_root\"] \n#Merge in raw for comparison\npaper_sig_final = pd.merge(paper_sig_groups[[\"SJR\", \"score\"]],paper_sig_norm, left_index=True, right_index=True)\npaper_sig_final.sort_values(\"Paper_Significance\", ascending=False).head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}