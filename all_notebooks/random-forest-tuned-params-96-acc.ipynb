{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \"Horizontal_Distance_To_Fire_Points\"]\nnum_vars_y = num_vars[:]\nnum_vars_y.append(\"Cover_Type\")\n\ndef split_df(df):\n    X = df.loc[:, df.columns != 'Cover_Type']\n    Y = df.loc[:, df.columns == 'Cover_Type']\n    del df\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                                        test_size = 0.30, \n                                                        random_state = 101)\n    return X_train ,X_test, Y_train, Y_test\n\ndef sc_split_df(df):\n    X = df.loc[:, df.columns != 'Cover_Type']\n    Y = df.loc[:, df.columns == 'Cover_Type']\n    del df\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                                        test_size = 0.30, \n                                                        random_state = 101)\n    del X\n    del Y\n\n    X_train, X_test = X_train.copy(), X_test.copy()\n\n    sc = StandardScaler()\n    X_train_sc = sc.fit_transform(X_train[num_vars])\n    X_test_sc = sc.transform(X_test[num_vars])\n\n    del X_train\n    del X_test\n    \n    return X_train_sc ,X_test_sc, Y_train, Y_test\n\n\ndef forest_test(X_train, X_test, Y_train, Y_test, hyperparams = {\n                                                                'bootstrap': False,\n                                                                'max_depth': 37,\n                                                                'max_features': \"auto\",\n                                                                'min_samples_leaf': 1,\n                                                                'min_samples_split': 3,\n                                                                'n_estimators': 555\n                                                            }):\n    hyperparams['n_jobs'] = -2\n        \n    start = time.process_time()\n    \n    trainedforest = RandomForestClassifier()\n    trainedforest = trainedforest.set_params(**hyperparams)\n    trainedforest.fit(X_train,Y_train.values.ravel())\n    \n    print(time.process_time() - start)\n    \n    predictionforest = trainedforest.predict(X_test)\n    print(confusion_matrix(Y_test,predictionforest))\n    print(classification_report(Y_test,predictionforest))\n    print(\"\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom sklearn import tree\nimport plotly.express as px\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/forest-cover-type-dataset/covtype.csv\", sep = \",\", header = 0)\nprint (df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.imshow(\n    df[num_vars_y].corr()\n)\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_matrix(\n    df,\n    dimensions=num_vars,\n    color=\"Cover_Type\"\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sc ,X_test_sc, Y_train, Y_test = sc_split_df(df[num_vars_y])\nX_train_sc ,X_test_sc, Y_train, Y_test = pd.DataFrame(X_train_sc) ,pd.DataFrame(X_test_sc), pd.DataFrame(Y_train), pd.DataFrame(Y_test)\n\nforest_test(X_train_sc ,X_test_sc, Y_train, Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}