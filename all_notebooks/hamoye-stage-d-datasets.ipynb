{"cells":[{"metadata":{},"cell_type":"markdown","source":"**PLANET DATASET: UNDERSTANDING THE AMAZON FROM SPACE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imporing Required Libraries\n\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for Data Visualisation\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom matplotlib.image import imread\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for Loading and Preparing dataset\n\nfrom os import listdir\nfrom numpy import zeros\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom pandas import read_csv\nfrom keras import backend\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nimport os\nimport time\nfrom sklearn.metrics import fbeta_score\nimport tensorflow as tf\nfrom keras.optimizers import SGD\nimport sys\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Conv2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for loading models\n\nfrom tensorflow.keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries for train-test-split\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading the train and test datasets\n\ntrain_datasets = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\ntest_image_datasets = '../input/planets-dataset/planet/planet/test-jpg/'\ntrain_image_datasets = '../input/planets-dataset/planet/planet/train-jpg/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the train_datasets\ntrain_datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the first 9 train images from the Planet Dataset\n\nplt.figure(figsize=( 20 , 20 ))\n\nfolder = 'planet/train-jpg/'\n\nfor i in range(9):\n    pyplot.subplot(330 + 1 + i)\n    \n    # Define File\n    file = train_image_datasets + 'train_' + str(i) + '.jpg'\n    \n    # Load image pixel\n    image = imread(file)\n    \n    # Plot pixel data\n    pyplot.imshow(image)\n    \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the individual tags from the train_datasets by spltting with empty strings\n\ndatasets_list = []\nfor tag_values in train_datasets.tags.values:\n    datasets = tag_values.split(' ')\n    for dataset in datasets:\n        if dataset not in datasets_list:\n            datasets_list.append(dataset)\n            \n# Check dataset list\n\nlen(datasets_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Order datasets in strings \n\ndatasets_list.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating tag_datasets and inverse_datasets_map\n\ntag_mappings = {datasets_list[i]: i for i in range(len(datasets_list))}\n\ninverse_datasets_map = {i:datasets_list[i] for i in range(len(datasets))}\n\n# Create a mapping of files to tags\n\ndef map_files(mapping_csv):\n    mapping = dict()\n    for i in range(len(mapping_csv)):\n        name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n        mapping[name] = tags.split(' ')\n    return mapping\n\nfile_mapping = map_files(train_datasets)\n\n# Creating a one hot encoding\n\ndef encode_one_hot(tags, mapping):\n    \n    #creating empty vector\n    encoding = np.zeros(len(mapping), dtype = 'uint8')\n    \n    #mark 1 for each tag in the vector\n    for tag in tags:\n        encoding[mapping[tag]] = 1\n    return encoding ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing the image dataset\n\n# Load the images\ndef load_dataset(path, file_mapping, tag_mappings):\n    photos = list()\n    targets = list()\n    \n# For files in the directory\n    for file in os.listdir(train_image_datasets):\n        \n        # Load images\n        photo = load_img(path + file, target_size=(64,64))\n        \n        # Converting to numpy array\n        photo = img_to_array(photo, dtype='uint8')\n        \n        # Tags\n        tags = file_mapping[file[:-4]]\n        \n        # One hot encode tags\n        target = encode_one_hot(tags, tag_mappings)\n        \n        #stores\n        photos.append(photo)\n        targets.append(target)\n        \n    X = np.asarray(photos,dtype='uint8')\n    Y = np.asarray(targets,dtype='uint8')\n    return X,Y\n\nX, Y = load_dataset(train_image_datasets, file_mapping, tag_mappings)\nprint(X.shape, Y.shape)\n\n# Compressing X and Y arrays into one single file\nnp.savez_compressed('planet_data.npz', X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\n\ndata = np.load('./planet_data.npz')\nX,Y = data['arr_0'], data['arr_1']\nprint('load_dataset: ', X.shape, Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train and val sets\n\nX_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.3, random_state = 42)\nprint('shapes: ', X_train.shape, X_val.shape, Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the Fscore metric\n# Create a funcion to compute evaluation metrics\n\ndef fbeta(Y_true, Y_pred, beta=2):\n    \n    #clip predictors\n    Y_pred = backend.clip(Y_pred, 0, 1)\n    \n    #Calculating elements\n    tp = backend.sum(backend.round(backend.clip(y_true * Y_pred, 0, 1)), axis=1)\n    fp = backend.sum(backend.round(backend.clip(y_true - Y_pred, 0, 1)), axis=1)\n    fn = backend.sum(backend.round(backend.clip(y_true - Y_pred, 0, 1)), axis=1)\n    \n    #Calculating precision\n    p = tp / (tp + fp + backend.epsilon())\n    \n    #Calculate recall\n    r = tp / (tp + fn + backend.epsilon())\n    \n    #Calculate fbeta, averaged across each class\n    bb = beta ** 2\n    fbeta_score = backend.mean(1 + bb) * (p * r) / (bb * p * r + backend.epsilon())\n    \n    return fbeta_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">*Create base model_PLANET*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialise the model_planet\nmodel_planet = tf.keras.models.Sequential()\n\n# add conv and pooling layers\nmodel_planet.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=[64, 64, 3]))\nmodel_planet.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel_planet.add(tf.keras.layers.MaxPool2D(2,2))\n\n# add second conv and pooling layers\nmodel_planet.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel_planet.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel_planet.add(tf.keras.layers.MaxPool2D(2,2))\n\n# add third conv and pooling layers\nmodel_planet.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel_planet.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel_planet.add(tf.keras.layers.MaxPool2D(2,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add flattening and output layers\nmodel_planet.add(tf.keras.layers.Flatten())\nmodel_planet.add(tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel_planet.add(tf.keras.layers.Dense(17, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compilled the created model_planet\nopt = SGD(lr=0.01, momentum=0.9)\nmodel_planet.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n\ntrainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot learning curves\ndef summerize_diagostics(history):\n    \n    #plot loss\n    plt.subplot(211)\n    plt.title('Cross Entropy Loss')\n    plt.plot(history.history['loss'], color='blue', label='train')\n    plt.plot(history.history['val_loss'], color='orange', label='test')\n    \n    #plot accuracy\n    plt.subplot(212)\n    plt.title('Fbeta')\n    plt.plot(history.history['fbeta'], color='blue', label='train')\n    plt.plot(history.history['val_fbeta'], color='orange', label='test')\n    \n    #save plot to file\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_plot.png')\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create data generators\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    model = Sequential()\n    model.add(Conv2D(filters=128, kernel_size=3, input_shape=(128, 128, 3)))\n    model.add(Flatten())\n    model.add(Dense(17, activation='sigmoid'))\n    opt = Adam(lr=1e-2)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[multi_label_acc, fbeta])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datasets['image_name'] = train_datasets['image_name'].apply(lambda x: x[:-4])\ntrain_datasets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datasets.to_csv('submission.csv', index=False, header=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}