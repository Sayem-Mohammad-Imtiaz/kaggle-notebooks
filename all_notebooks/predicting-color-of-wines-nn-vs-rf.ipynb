{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objectives"},{"metadata":{},"cell_type":"markdown","source":"* Predict the colors of wines (Red or White)\n* Create a Neural Network with Keras\n* Create a Random Forest\n* Compare both models\n* Obtain Feature importace"},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np                # linear algebra\nimport pandas as pd               # data frames\nimport seaborn as sns             # visualizations\nimport matplotlib.pyplot as plt   # visualizations\n%matplotlib inline\nimport scipy.stats                # statistics\nfrom sklearn import preprocessing\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Reading the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data and store them in two objects\n\nred = pd.read_csv('../input/winequality-red.csv')\nwhite = pd.read_csv('../input/winequality-white.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have two separate datasets **Red Wines** and **White Wines** and these are their dimensions:"},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"# Explore dimension of the datasets\n\nprint(\"There are {} red wines with {} attributes in red dataset. \\n\".format(red.shape[0],red.shape[1]))\nprint(\"There are {} red wines with {} attributes in white dataset. \\n\".format(white.shape[0],white.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let´s append both datasets (**Red** and **White**), after that a new variable was created called **\"Red\"** that will let us know if the wine is **red(1)** or **white(0)**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Let´s create a new variable that let us know if the wine is red(1) or white(0)\nred['red']=1\nwhite['red']=0\n\n# Union of both datasets\nwines = pd.concat([red,white])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wines.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wines.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Cleaning the Data"},{"metadata":{},"cell_type":"markdown","source":"There are **1177 duplicated** records:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Let's see if we have duplicated records\n\ntwice = wines[wines.duplicated()]\ntwice.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"twice.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They are distributed like this:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot(x=\"red\", data=twice, palette=\"husl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.DataFrame(twice['red'].value_counts(dropna=False)).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"pd.DataFrame(wines['red'].value_counts(dropna=False)).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove Duplicates"},{"metadata":{},"cell_type":"markdown","source":"Let's get rid off the duplicated ones, now we have these dimensions:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wine = wines.drop_duplicates(keep='first')\nwine.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balance of the Target Variable"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.countplot(x=\"red\", data=wine, palette=\"RdPu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Explore the Data"},{"metadata":{},"cell_type":"markdown","source":"These are the main statistics of the variables:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wine.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look of their dispersion."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(data=wine.drop(columns=['red']), orient='horizontal', palette='RdPu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the boxplot above that their range of values varies from one variable to another, so we will need to scale their values to enhance the maipulation of this data."},{"metadata":{},"cell_type":"markdown","source":"# Scale the data"},{"metadata":{},"cell_type":"markdown","source":"The method to scale the data will be **Standardization**. Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. **Gaussian with 0 mean and unit variance**)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Scaling the continuos variables\nwine_scale = wine.copy()\nscaler = preprocessing.StandardScaler()\ncolumns = wine.columns[0:12]\nwine_scale[columns] = scaler.fit_transform(wine_scale[columns])\nwine_scale.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(data=wine_scale.drop(columns=['red']), orient='horizontal', palette='RdPu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation between features"},{"metadata":{},"cell_type":"markdown","source":"### Scatter Plot"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"g = sns.PairGrid(wine_scale.iloc[:,1:13], hue=\"red\", palette=\"RdPu\")\ng.map(plt.scatter);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr=wine_scale.iloc[:,1:13].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, annot=True, cmap='RdPu', center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The variables \"free sulfur dioxide\" and\t\"total sulfur dioxide\" have a strong correlation between them.\n\n* We can see that the features more related with the wine's color are **volatile acidity**, **total sulfur dioxide** and **chlorides**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.jointplot(x=\"total sulfur dioxide\", y=\"free sulfur dioxide\", data=wine, color='c')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The relationship between \"**free sulfur dioxide**\" and \"**total sulfur dioxide**\"shows heteroscedasticity and the one that is more related with our objective variable is \"**total sulfur dioxide**\""},{"metadata":{},"cell_type":"markdown","source":"I'm going to leave both variables anyway hoping that the Neural Network and the Random Forest could be able to manage the previous findings."},{"metadata":{},"cell_type":"markdown","source":"# Training and Test Datasets"},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into **80%** for **training** and **20%** for **test**. Here is the representation of the target variable in the **Original Dataset**, **Training Dataset** and **Test**:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sample = np.random.choice(wine_scale.index, size=int(len(wine_scale)*0.8), replace=False)\ntrain_data, test_data = wine_scale.iloc[sample], wine_scale.drop(sample)\n\nprint(\"Number of training samples is\", len(train_data))\nprint(\"Number of testing samples is\", len(test_data))\nprint(train_data[:5])\nprint(test_data[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\nsns.countplot(x=\"red\", data=wine_scale, palette=\"RdPu\", ax=axes[0])\nsns.countplot(x=\"red\", data=train_data, palette=\"RdPu\", ax=axes[1])\nsns.countplot(x=\"red\", data=test_data, palette=\"RdPu\", ax=axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that training dataset is the more balanced of the three which is good and the test dataset is the more unbalanced one, so this will be challenging for our models."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = train_data.drop('red', axis=1)\ntargets = train_data['red']\nfeatures_test = test_data.drop('red', axis=1)\ntargets_test = test_data['red']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Networks With Keras"},{"metadata":{},"cell_type":"markdown","source":"Our workflow will be as follow: first we will present our neural network with the training data, **features** and **targets**. The network will then learn to associate **features** and **targets**. Finally, we will ask the network to produce predictions for **features_test**, and we will verify if these predictions match the labels from **targets_test**.\n\nLet's build our network:"},{"metadata":{},"cell_type":"markdown","source":"Here our network consists of a sequence (**Sequential**) of **two Dense layers**, which are densely-connected or fully-connected neural layers. The second (and last) layer is a 2-way \"sigmoid\" layer, which means it will return an array of 2 probability scores (summing to 1). Each score will be the probability that the current wine belongs to Red Wines of our two wine classes (Red and White)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\n\n# Building the model\nNnetwork = models.Sequential()\nNnetwork.add(layers.Dense(40, activation='sigmoid', input_shape=(12,)))\nNnetwork.add(layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compilation"},{"metadata":{},"cell_type":"markdown","source":"To make our network ready for training, we need to pick three more things, as part of the \"compilation\" step:\n\n*     A **loss function**: this is how the network will be able to measure how good a job it's doing on its training data, and thus how it will be able to steer itself in the right direction.\n\n*     An **optimizer**: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n                        sgd = stochastic gradient descent\n\n*     Some **metrics** to monitor during training and testing. Here we will only care about accuracy (the fraction of the wines that were correctly classified)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the model\nNnetwork.compile(loss = 'binary_crossentropy',\n                 optimizer='sgd',\n                 metrics=['accuracy'])\nNnetwork.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now ready to train our network, which in Keras is done via a call to the fit method of the network: we \"fit\" the model to its training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model\nNnetwork.fit(features, targets, epochs=10, batch_size=100, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's see the performance in Train and Test to evaluate the possibility of Overfitting"},{"metadata":{},"cell_type":"markdown","source":"This is the accuracy of our Neural Network in test data:"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"test_loss, test_acc = Nnetwork.evaluate(features_test, targets_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('test_acc:', test_acc, '\\ntest_loss:', test_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the accuracy of our Neural Network in Train data:"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_loss, train_acc = Nnetwork.evaluate(features, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('train_acc:', train_acc, '\\ntrain_loss:', train_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We had better accuracy in our test set than in our training set which is very very good!"},{"metadata":{},"cell_type":"markdown","source":"Our **test set accuracy** turns out to be quite a bit higher than the **training set accuracy**. This gap between training accuracy and test accuracy is good and it seems that we are **avoiding \"overfitting\"** which is the fact that machine learning models tend to perform worse on new data than on their training data."},{"metadata":{},"cell_type":"markdown","source":"*Here we won't explore the Hyperparameter Tuning and we are going to keep this result*"},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some relevant hyper-parameters for the random forest:\n\n* **max_depth**: max_depth represents the depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information about the data.\n\n* **n_estimators**: the number of decision trees used.\n\n* **max_features**: The number of features (predictor variables) that the model will randomly consider when looking for the best split"},{"metadata":{"trusted":true},"cell_type":"code","source":"Rforest = RandomForestClassifier(max_depth=4, n_estimators=10, max_features=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rforest.fit(features, targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the accuracy of our Random Forest in test data:"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Accuracy\nscore = Rforest.score(features_test, targets_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the accuracy of our Random Forest in training data:"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Accuracy\nscore_train = Rforest.score(features, targets)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(score_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, our **test set accuracy** turns out to be a bit higher than the **training set accuracy**. This gap between training accuracy and test accuracy is good and it seems that we are **avoiding \"overfitting\"**."},{"metadata":{},"cell_type":"markdown","source":"## More Performance Metrics from Random Forest"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"### Predictions\ny_pred_rf = Rforest.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"### Probabilities\ny_prob_rf = Rforest.predict_proba(features_test)\ny_prob_rf = y_prob_rf.T[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn import metrics\n# measure confusion matrix\ncm_rf = metrics.confusion_matrix(targets_test, y_pred_rf, labels=[0, 1])\ncm_rf = cm_rf.astype('float')\ncm_rf_norm = cm_rf / cm_rf.sum(axis=1)[:, np.newaxis]\nprint(\"True Positive (rate): \", cm_rf[1,1], \"({0:0.4f})\".format(cm_rf_norm[1,1]))\nprint(\"True Negative (rate): \", cm_rf[0,0], \"({0:0.4f})\".format(cm_rf_norm[0,0]))\nprint(\"False Positive (rate):\", cm_rf[1,0], \"({0:0.4f})\".format(cm_rf_norm[1,0]))\nprint(\"False Negative (rate):\", cm_rf[0,1], \"({0:0.4f})\".format(cm_rf_norm[0,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"np.shape(y_prob_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(targets_test, y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# measure Area Under Curve (AUC)\nauc_rf = metrics.roc_auc_score(targets_test, y_pred_rf)\nprint()\nprint(\"AUC:\", auc_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# ------------------------------------------------------------------------------\n# Plot: Receiver-Operator Curve (ROC)\n# ------------------------------------------------------------------------------\n\nfig, axis1 = plt.subplots(figsize=(8,8))\nplt.plot(fpr, tpr, 'r-', label='ROC')\nplt.plot([0,1], [0,1], 'k--', label=\"1-1\")\nplt.title(\"Receiver Operator Characteristic (ROC)\")\nplt.xlabel(\"False positive (1 - Specificity)\")\nplt.ylabel(\"True positive (selectivity)\")\nplt.legend(loc='lower right')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metrics of Random Forest are almost perfect!"},{"metadata":{},"cell_type":"markdown","source":"# Accuracy of Neural Networks vs Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Accuracy of the Neural Network:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Accuracy of Random Forest\ntest_loss, test_acc_nn = Nnetwork.evaluate(features_test, targets_test)\nprint('Accuracy of NN in test data:', test_acc, '\\ntest_loss:', test_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy of the Random Forest:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Accuracy of Random Forest\nscore = Rforest.score(features_test, targets_test)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both Models have presented a superior performance but we can see that the **Accuracy of the Random Forest is almost 3% bigger** than the Accuracy of the Neural Network."},{"metadata":{},"cell_type":"markdown","source":"# * And the winner is... RANDOM FOREST!!!*"},{"metadata":{},"cell_type":"markdown","source":"There is an interesting thing that Random Forest let us do and is to know which features are more important to predict the target variable. Let's take a look at this:"},{"metadata":{},"cell_type":"markdown","source":"# Feature importance"},{"metadata":{},"cell_type":"markdown","source":"We can rank the features according to how much each feature was used to split the dataset while training. This is a measure of their importance, i.e, of how much each feature contributes to successfully isolate pure partitions."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"importances = Rforest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in Rforest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in zip(features.columns, Rforest.feature_importances_):\n    print(f)\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(features.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(features.shape[1]), indices)\nplt.xlim([-1, features.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.\n\nAs expected from the correlation matrix, the plot suggests that the 3 most informative features are:\n\n* **chlorides**\n* **total sulfur dioxide**\n* **volatile acidity**\n\nThose three features explains an important part of the wine's colors."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}