{"cells":[{"metadata":{"_uuid":"5315020687f05a2ae8985011b0c2680c6df543c4"},"cell_type":"markdown","source":"# Text Analysis\n\n#### Dataset : Women's E-Commerce Clothing Reviews\n\n### 1. Pre-Processing of Data\n   * Remove all extra charecters such as punctuations, non charecters, etc\n   * Tokenisation\n   * Lametisation of data. (preffered over stemming as stemming can corrupt data in some cases)"},{"metadata":{"_uuid":"1fe9088b8a1c224e1ef75b714d79dca05a8fc6ed","scrolled":false,"trusted":true},"cell_type":"code","source":"# Importing Required Variables\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport sys,math, copy, time\nimport re\nimport csv\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom scipy.spatial.distance import cosine\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n# Reading the Data\nclothing_review = pd.read_csv(\"../input/Womens Clothing E-Commerce Reviews.csv\")\nclothing_review = clothing_review.dropna(subset=['Review Text'])\nclothing_review = clothing_review[clothing_review['Clothing ID'] == 862]\n\n#Getting Keywords\nkeyWords = [ \"dress\",\"pretty\"]\n\n# Clearing the data from extra characters\ndata = []\nfor i in clothing_review[\"Review Text\"]:\n    j = i.lower()\n    j = re.sub(r'[^A-Za-z ]', '', j)\n    data.append(j)\n    \n# Tokenising the data\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i in range(len(data)) :\n    data[i] = tokenizer.tokenize(data[i])\n\n# Getting the list of stop words\nstopWords = list(stopwords.words('english'))\nstopWords = [re.sub(r'[^A-Za-z ]', '', j) for j in stopWords]\n\n# Lemmatizing and removing stop words\nwordnet_lemmatizer = WordNetLemmatizer()\ndataFiltered = []\nfor each_review in data :\n    temp = []\n    for word in each_review : \n        if not word in stopWords :\n            temp.append(wordnet_lemmatizer.lemmatize(word))\n    dataFiltered.append(temp)\n\n\ndataFiltered.append(keyWords)\n\n# Creating the word list\nwordList = np.array(dataFiltered)\nwordList = np.hstack(wordList)\nwordList = list(set(wordList))\nwordList.sort()\nnumber_of_reviews = len(dataFiltered)\nwordListIndex = { wordList[i]: i for i in range(len(wordList))}\nnDocsPerWord = {i : 0 for i in wordList}\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa7cb6e708ddfa27db441388b1f783aec7c5f676"},"cell_type":"markdown","source":"### 2. Creation of TF matrix"},{"metadata":{"_uuid":"1bfebfa1b6d0387de932fed3f140d2edd7045376","trusted":true},"cell_type":"code","source":"tf = np.zeros(shape=(number_of_reviews,len(wordList)))\nte = np.zeros(shape=(number_of_reviews,len(wordList)))\n\nfor i in range(len(dataFiltered)):\n    this_doc_accounted = []\n    for j in dataFiltered[i] :\n        tf[i][wordListIndex[j]] += 1\n        te[i][wordListIndex[j]] = 1\n        if not j in this_doc_accounted :\n            this_doc_accounted.append(j)\n            nDocsPerWord[j] += 1\n            \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"071dd4a8ebe4e5d9ddf68caf6a5015326060d27a"},"cell_type":"markdown","source":"### 3. Creation of TF-IDF matrix from calculated TF matrix"},{"metadata":{"_uuid":"fbd834530563141569f688311237713f57813697","trusted":true},"cell_type":"code","source":"tfIdf = copy.deepcopy(tf)\n\nfor i in range(number_of_reviews) :\n    for k in dataFiltered[i]:\n        j = wordListIndex[k]\n        if tfIdf[i][j] != 0 :\n            tfIdf[i][j] = tfIdf[i][j]*math.log(number_of_reviews/nDocsPerWord[wordList[j]])\n\nprint(tfIdf.shape)\n\nk = 20\nsum1 = te.sum(axis=0)\nprint(sum1.shape)\nto_del = []\nfor i in range(len(sum1)) :\n    if sum1[i] < k :\n        to_del.append(i)\nte = np.delete(te, to_del, axis = 1)\nprint(te.shape)\n\n\nsum1 = tf.sum(axis=0)\nprint(sum1.shape)\nto_del = []\nfor i in range(len(sum1)) :\n    if sum1[i] < k :\n        to_del.append(i)\ntf = np.delete(tf, to_del, axis = 1)\nprint(tf.shape)\n\nsum1 = tfIdf.sum(axis=0)\nprint(sum1.shape)\nto_del = []\nfor i in range(len(sum1)) :\n    if sum1[i] < k :\n        to_del.append(i)\ntfIdf = np.delete(tfIdf, to_del, axis = 1)\nprint(tfIdf.shape)\n\n\nwith open(\"te.dat\",'w') as writefile :\n    for i in te :\n        for j in i :\n#             print(j,end=\"\\t\")\n            writefile.write(str(j) + \"\\t\")\n        writefile.write(\"\\n\")\n#         print()\nwith open(\"tf.dat\",'w') as writefile :\n    for i in tf :\n        for j in i :\n#             print(j,end=\"\\t\")\n            writefile.write(str(j) + \"\\t\")\n        writefile.write(\"\\n\")\n#         print()\nwith open(\"tfIdf.dat\",'w') as writefile :\n    for i in tfIdf :\n        for j in i :\n#             print(j,end=\"\\t\")\n            writefile.write(str(j) + \"\\t\")\n        writefile.write(\"\\n\")\n#         print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b7be63dc09547b112ef53de4d88fdfbe2d3900f"},"cell_type":"markdown","source":"### 4. Information Retrieval\n\n* ####  From TF-IDF matrix"},{"metadata":{"_uuid":"fc647c9dc4b506f3cfc1ea6be2a29f57e9a6454e","scrolled":true,"trusted":true},"cell_type":"markdown","source":"#Information retrieval\n\n# query = np.zeros(len(wordList))\n# for i in keyWords :\n#     query[wordListIndex[i]] = math.log(number_of_reviews/ 1 if nDocsPerWord[i]==0 else nDocsPerWord[i])\n\n\ntfidf_start = time.time()\nquery = tfIdf[-1]\nquery_result = [ cosine(i,query) for i in tfIdf]\ntfidf_end = time.time()\nquery_result.pop()\n\n# print(query_result)\n\nmax_iter = 5\nfor i in range(max_iter) :\n    min_value = np.partition(query_result, 4)[i]\n    min_val_index = query_result.index(min_value)\n    print(i,\") Cosine Value :\",min_value,\"\\n\", clothing_review[\"Review Text\"].iloc[ min_val_index],\"\\n\")"},{"metadata":{"_uuid":"0ca4004dab7fa825e99a573cd5dde9aaaaa1906d"},"cell_type":"markdown","source":"* #### From LSA using TF matrix"},{"metadata":{"_uuid":"25d1abbd62d3ca7f8d11d98a666a2440013d1f16","trusted":true},"cell_type":"markdown","source":"tf_matrix = tf # D x V matrix \nA = tf_matrix.T \n\nU, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\n\nK =  len(keyWords) # number of components\n\nA_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # D x V matrix \n\ndocs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n\n# print (A_reduced)\n# print (docs_rep)\n# print (terms_rep)\n\nkey_word_indices = [wordList.index(key_word) for key_word in keyWords] # vocabulary indices \n\nkey_words_rep = terms_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n\n\nsvd_start = time.time()\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nsvd_end = time.time()\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\n\nmax_iter = 5\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print(rank + 1, \") Cosine value : \", float(query_doc_cos_dist[sort_index]) ,\"\\n\", clothing_review[\"Review Text\"].iloc[sort_index],\"\\n\")\n    max_iter -= 1\n    if max_iter == 0 :\n        break"},{"metadata":{"_uuid":"f857b8d2d8bf39f5366fc0e5babb6766983182f9"},"cell_type":"markdown","source":"### 5. Plotting"},{"metadata":{"_uuid":"9a4df8df15b88fcccf6c86f7e160897a77396b3e","scrolled":false,"trusted":true},"cell_type":"markdown","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=query_doc_cos_dist) # all documents \nplt.scatter(query_rep[0], query_rep[1], marker='+', c='red') # the query \nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")"},{"metadata":{"_uuid":"8de6ebf15312b082e266d62139c678b9f58d45c0"},"cell_type":"markdown","source":"### 6. Analysis of Both Methods\n\n#### Results \n    For Input : Dress Pretty\n\n* TF-IDF :\n\n 1. I have two of these tops. the maroon and tan and white stripe. the material is great and the neckline lays great. it isn't too thick so i can wear it on cooler florida days. you can dress it up under a jacket or dress down with some jeans. \n 1. I love this top! the unique neck detail is awesome! i dress it up; i dress it casual. works for every adventure. more!!! \n 1. I got the red- it is a great color. this is one of those thin soft t shirts. it drapes really nicely and dresses up or dresses down. the only downside for me is that it takes a little extra effort in the washing department, but really not much- just have to line dry. \n 1. I bought this shirt in the neutral and white and love it. so many people compliment it. i usually pair it with white pants and cute wedges to dress it up (obviously with a statement necklace too). but the greatest thing about this shirt is the fact that you can also dress it down. the material is fabulous but i have not washed it yet so i am not sure if it is going to shrink (which i hope it doesn't because it is one of my favorite pieces).  unlike the other reviewer i did not think that the \n 1. I bought three of these tees. i kept the turquoise one only though. the color was pretty - a little drab but not too bad.  i love the style., the fabric is so soft and the one i wore was super comfy. the sleeves are cute without being too dramatic (like the ruffle sleeve tee - the sleeves were kind of ridiculous on me) now - the cons - the colors are pretty drab. i normally look better in more vibrant colors. the \"red\" i bought made me look like a corpse. it's not pretty at all. can't re \n \n\n* SVD using TF matrix\n \n 1. Very comfortable shirt, light weight top with lovely floral colors. great for spring/summer with white jeans. love everything one september makes! \n 1. Great, comfortable tank. cool added detail in the straps. runs big so size down. i wear both a medium and large at retailer and got this in a small. \n 1. I usually wear small, ordered xs and it's still way too large. quality not that great, just meh, returning it. \n 1. I bought this shirt with the intentions of using it as a nursing top. to that effect, i'm happy with the purchase, and will get a lot of use out of it. i bought two on blush and one blue at the same time. however, i was surprised that the shirt was so open and loose. i thought it was just a deep v-neck, but the design actually really opens up once the hook and eye is undone at the top. you are then very exposed, there is nothing but an opened front shirt. this is convenient for a nursing mother, \n 1. Ordered online, just received this vest, so disappointed! love the garment but it is way too small. i wear a medium in most clothing and size 6 in a dress, but the medium in this vest isn't just snug it is definitely too small, even worn alone, i can't button it. i am now ordering the large and keeping my fingers crossed as i want to wear a tee under it. hoping i won't need to return both the m and the l, thinking of ordering an xl.\n\n#### Time\n\n   From the following code we can see that the SVD takes less time than TFIDF"},{"metadata":{"_uuid":"531eccb83f8b0969c52d0c5688fd058500b39b17","scrolled":true,"trusted":true},"cell_type":"markdown","source":"print(\"TFIDF : \",tfidf_end - tfidf_start)\nprint(\"SVD with TF : \",svd_end - svd_start)"},{"metadata":{"_uuid":"51135d89ddb3c5b93ded7243039d4d6e7684afc4"},"cell_type":"markdown","source":"#### Space\n\n   LSA takes less space than TFIDF. As in TFIDF we have to store an 'n x m' matrix ( n = number of documents, m = number of words). Whereas in LSA we just have to store the 'n x k' matrix, ( k = the reduced dimentionality ), and the Eigen matrix '' ."},{"metadata":{"_uuid":"386a40d910d9c32e7c682098ba72a30295113c1d"},"cell_type":"markdown","source":"#### Comparing the output behaviour\n\nThe Following code compares the cosine output from both the methods. We can see that LSA curve has more gradual change i.e. it is able to identify similarities that the TF-IDF was not able to do[](http://)"},{"metadata":{"_uuid":"62c202281cb74a52f392bae0d83b5dddc041c450","trusted":true},"cell_type":"markdown","source":"query_result.sort()\nplt.plot(range(len(query_result)),query_result, \"-o\",label=\"TFIDF\")\nquery_doc_cos_dist.sort()\nplt.plot(range(len(query_doc_cos_dist)),query_doc_cos_dist, \"-o\", label=\"SVD with TF\")\nplt.legend(loc='best')\nplt.ylabel(\"Cosine output\")\nplt.xlabel(\"Rank\")\nplt.show()\nprint()"},{"metadata":{"_uuid":"7f05d5169756b6c031460bb4b6bc48602716310c"},"cell_type":"markdown","source":"* Curvature \n * Observation : We can see that LSA curve has more gradual change as compared to TFIDF which gets saturated after a point.\n * Inference : TFIDF looses ability to identify similarities after some extent (Theoretically, TFIDF can only tell the similarities in those documents only where the words appear.)\n* Position of each curve\n * Observation : The SVD curve lies more flat towards the origin i.e. it has more values closer to 0.\n * Inference : The SVD algorithm identifies each document to be more similar to the input query as compared to TFIDF. \n \n\n### Conclusion\n\n \nIn terms of computational resources and time, SVD is better than TFIDF.\nIn terms of output given each of the above method is good in its own way.\n \n TF-IDF method only gives those documents where the word is appearing. This is good for people who are searching for keywords and want them to be present in the document regardless of what the document means.\n \n Whereas in SVD-TF, apart from those documents where both the word comes, those documents are also given which bears the same meaning as the keyword. SVD grasps the meaning of the document. This is good for people who are searching for texts which bear same meaning as the keyword instead of presence of just the keywords."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}