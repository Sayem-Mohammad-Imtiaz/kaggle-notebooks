{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Instalation and Imports","metadata":{"id":"w7Ue3ZBWvaNb"}},{"cell_type":"markdown","source":"This notebook is an exercise and exploration about one of my most belove interest: the astronomy. And I'm posting my notebook here, and also on github, to share my passion and results with more people that also love science and Data Science.\n\nPlease, if you found any errors (code, grammar, astronomical or mathematical erros) here, report me and I'll try to improve this notebook the best that i can (i'm not a professional, i just love astronomy and data science). Thank you, and have a nice jouney through the cosmos","metadata":{}},{"cell_type":"code","source":"!pip install kneed","metadata":{"id":"QL0xJY6JJsEt","outputId":"ecc7cdcf-204e-4506-ab2d-b5babb1aaf60","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom kneed import KneeLocator\n\nimport scipy\nimport scipy.cluster.hierarchy as shc\nfrom scipy.stats import shapiro, boxcox, t, mannwhitneyu\nfrom statistics import stdev\nfrom math import sqrt\nfrom statsmodels.graphics.gofplots import qqplot\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\n\nsns.set()","metadata":{"id":"a6LWY0XdNovl","outputId":"0825aa2a-d583-436f-de53-287f7b0d55bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#The dataset","metadata":{"id":"Cadss7UdvhEd"}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo = pd.read_csv('../input/combo17-galaxy-dataset/COMBO17.csv')","metadata":{"id":"Ll0TiUTDOyFo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo.head()","metadata":{"id":"8bzBMYtlPEL0","outputId":"bbe78b70-7146-4836-ef18-4fef04f9027c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Changing feature names to better understanding","metadata":{"id":"ZzGIjORIvmVd"}},{"cell_type":"code","source":"combo = combo.rename(columns={ 'UjMAG': 'UVmag', 'BjMAG': 'Bluemag', 'VjMAG': 'Opticmag',\n        'W420FE': 'lenwave1', 'W462FE': 'lenwave2', 'W485FD': 'lenwave3',\n       'W518FE': 'lenwave4', 'W571FS': 'lenwave5', 'W604FE': 'lenwave6',\n       'W646FD': 'lenwave7', 'W696FE': 'lenwave8','W753FE': 'lenwave9',\n        'W815FS': 'lenwave10',  'W856FD': 'lenwave11', 'W914FD': 'lenwave12',\n        'W914FE': 'lenwave13', 'UFS\t': 'UVfilter', 'BFS': 'BLUEfilter', 'VFD': 'OPTICfilter', 'RFS': 'REDfilter', 'IFD': 'InfraREdfilter'})","metadata":{"id":"zHxophroPI0H","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo.head(2)","metadata":{"id":"Ob4CPkiJPkFm","outputId":"247a26cc-f1bf-4c06-a772-44e845e60bf5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Exploratory Data Analysis ","metadata":{"id":"A7WJNAnrQP41"}},{"cell_type":"code","source":"combo.shape","metadata":{"id":"AF9yHsQdQiv2","outputId":"c4b26b70-cad9-4728-b38f-6d08f38cb3dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in combo.isnull().sum() / combo.shape[0]:\n  print(c)","metadata":{"id":"Rhw9NwUoPw61","outputId":"edca1242-e232-4443-98d8-cc1951ab11b5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo = combo.dropna()\ncombo.shape","metadata":{"id":"s0ACNjNnRYPz","outputId":"02ed02bd-d2a1-448f-a0c5-3a6d45da2219","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo.info()","metadata":{"id":"niLzY78yRvc3","outputId":"476a9144-b7c6-443e-cad2-125ee24811b8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping not very usefull features to us","metadata":{"id":"17npGXUav0Pa"}},{"cell_type":"code","source":"error_features = []\nfor feature in combo.columns:\n  if feature[0] == 'e':\n    error_features.append(feature)","metadata":{"id":"bthu4OYKV5Dj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo = combo.drop(error_features, axis=1)","metadata":{"id":"1BgyjJ5UV5A7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo.columns","metadata":{"id":"dPkpi2tVV5G9","outputId":"df62f791-bdac-414b-c4a0-e8b795da994e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Describe and Relaation inside data  ","metadata":{"id":"E68auapYv_DF"}},{"cell_type":"code","source":"combo.describe()","metadata":{"id":"aSPEsmJ0RsTr","outputId":"695c6615-42d8-4674-886f-496b8d52f5aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,9))\nsns.heatmap(combo.corr())","metadata":{"id":"1O6VzCtzRdSH","outputId":"8c7f4d77-bb3d-434f-cd01-d63e1ecc6a2f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding the three main variables","metadata":{"id":"rt6x8wUDwz2t"}},{"cell_type":"markdown","source":"Working with three main features\n\n- Rmag (Red Magnetude - Bright)\n\n- ApDRmag (Size)\n\n- Mcz (redshift - Distance)","metadata":{"id":"hfXXpO_8Vstx"}},{"cell_type":"markdown","source":"This features can e analysed as main properties of the galaxies, so let's see how they are related to eac other, and try to find some interesting pattern.","metadata":{"id":"m9mRPwXycb6w"}},{"cell_type":"code","source":"#defining a function of find the Person Correlation and the R square of each combination of features\ndef find_corr_galaxy(x=0, y=0):\n  corr_matrix = np.corrcoef(x, y)\n  pearson_corr_galaxy = corr_matrix[0,1]\n  r2_galaxy = pearson_corr_galaxy ** 2\n  return pearson_corr_galaxy, r2_galaxy","metadata":{"id":"XXUeuzs6FR5G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as we can see, there is a good correlation between this two features, and also a strong R2\n#that means 47% of the variance in one features can be explnaed by the variance  in the other one\npearson_corr, r2 = find_corr_galaxy(x=combo.Rmag, y=combo.ApDRmag)\nprint(f'Pearson Correlation Bright vs Size: {pearson_corr:.2f}')\nprint(f'R2: {r2:.2f}')","metadata":{"id":"3epbraq4G4x-","outputId":"29550d69-d9ca-4506-9af1-07f4dab0abf2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ploting this correlation helps us to a better understanding of the situation \nplt.figure(figsize=(15,10))\nsns.scatterplot(x='Rmag', y='ApDRmag', data=combo)","metadata":{"id":"THzmTWysS2QP","outputId":"b33da77a-954c-419f-907e-57294343f84e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here its clearly that the correlation is not that strong\n#only 4% of the variance in the size can be explaned by the distance of the galaxy from us\npearson_corr, r2 = find_corr_galaxy(x=combo.Mcz, y=combo.ApDRmag)\nprint(f'Pearson Correlation Distance vs Size: {pearson_corr:.2f}')\nprint(f'R2: {r2:.2f}')","metadata":{"id":"wzL6OZRAuoEf","outputId":"2ad98b2e-f815-41eb-d4f3-9182dae5ae01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ploting this correlation its clear that the correlation is weak\n#but still it looks like a tendency\nplt.figure(figsize=(15,10))\nsns.scatterplot(x=combo.Mcz,y=combo.ApDRmag)","metadata":{"id":"yZ09-GGEDUJE","outputId":"b6cf940e-26c4-4375-b3ae-6f3964a02f98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#and we have a reasonable between color and bright \npearson_corr, r2 = find_corr_galaxy(x=combo.Mcz, y=combo.Rmag)\nprint(f'Pearson Correlation Distance vs Bright {pearson_corr:.2f}')\nprint(f'R2: {r2:.2f}')","metadata":{"id":"WeS6Wl_JvUhh","outputId":"dba419f0-beb3-4074-fbbe-8cf933e22fc7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.scatterplot(x=combo.Mcz,y=combo.Rmag)","metadata":{"id":"fwP04TbuEJ75","outputId":"717c1978-7289-457a-8c0b-d5e87d74cb10","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Relationship Bright x lenght wave\n\n- A interesting point to be known is that more bright the galaxy is more intense will be the emission of eletromagnetic waves, and we can take a look if this happen hear ploting the Magnitude of the galaxy but the intensity of fotons that are recived here for each length wave.","metadata":{"id":"50WXmWR4zwRF"}},{"cell_type":"code","source":"#deffining a function to plot all the length waves by magnitude\ndef plot_lenwave(column_name): \n  plt.figure(figsize=(15,10))\n\n  sns.lineplot(x = combo[column_name], y = combo['REDfilter'], palette = 'red', label = 'RED-FILTER' )\n  sns.lineplot(x = combo[column_name], y = combo['BLUEfilter'], palette = 'blue', label = 'BLUE-FILTER')\n  sns.lineplot(x = combo[column_name], y = combo['OPTICfilter'], palette = 'green', label = 'GREEN-FILTER')\n  sns.lineplot(x = combo[column_name], y = combo['UFS'], palette = 'Purple', label = 'ULTRAVIOLET-FILTER')\n  sns.lineplot(x= combo[column_name], y = combo['InfraREdfilter'], palette = 'yellow', label = 'INFRARED-FILTER')\n  plt.ylabel('Wave length Intensity')\n  plt.legend()\nplot_lenwave(column_name = 'Rmag')","metadata":{"id":"Ns4fkr9hE5dO","outputId":"f132fdc3-0fc0-4008-f4a4-818cda032315","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot above confirms our thought, the less the magnitude, the more intense is the emition of fotons (waves) ","metadata":{"id":"vrzQYc2rg43T"}},{"cell_type":"markdown","source":"Here there is a pick in the of emission between the galaxies with -2 and 0. Comparing with the previous plot we can infer that galaxies between this range are the galaxies with low magnitude, and by consequence, more blue","metadata":{"id":"0h77csRSjvn8"}},{"cell_type":"code","source":"plot_lenwave(column_name='ApDRmag')","metadata":{"id":"mrD-qhJA0Nth","outputId":"04009e99-7908-4aef-c84d-1d604c41de1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now its clear that the galaxies near to us are the ones with most intense emission.  ","metadata":{"id":"lWNHfQm0mK6a"}},{"cell_type":"code","source":"plot_lenwave(column_name='Mcz')","metadata":{"id":"_CQ5PooY3WR2","outputId":"fc6b9e73-b710-44d0-9790-b02271b945c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###What do we get from these plots?\n\n- So, based in this plots we can already now that we have a clear tendency to have more bright and median size galaxies near from us. And as long you keep going more and more away from our galaxy you will find less bright galaxies and also bigger.","metadata":{"id":"c21n5MBdmXtx"}},{"cell_type":"markdown","source":"###Creating a new feature:\n\n- when you are studing astronomy its preatty comon that you have to understand what is a color index. A color index is the measurement of how much ligth he galaxy provides in an especifc range in the eletromagnetic spectrum. That's why to know the color index of any object we have to subtract the magnitude of an object in an especifc magnitude in the eletromagnetic spectrum from the magnitude in another point of the eletroc=magnetic spectrum.\n\n- The most common color index that we have is the BV-index, but the UB-index its also used a lot.","metadata":{"id":"-O2lJJ3p5DfK"}},{"cell_type":"code","source":"#creating columns for color-index\ncombo['BV_color_index'] = combo.Bluemag - combo.Opticmag\ncombo['UB_color_index'] = combo.UVmag - combo.Bluemag","metadata":{"id":"EvsmacJz5CG8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo.head(1)","metadata":{"id":"Zv8usCT_4MZz","outputId":"93b1f0e2-9240-4b26-88e0-09192a78fa66","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this point we have reached one of the most amazing parts of the data visualization with astronomy. Ploting the Distance of the galaxies from us, and the UB-color_index, the espectrum between the Ultra violet and the Optic Blue we can find a really interesting patter. Each point is a galaxy, relaed to the distance and the UB-color_index. And as long you go by the x-axis (the distance) you can find clear vertical bands of points. Those band are cluster os galaxies! Together in the same point of space they share the same distance, because due to the gravitacional force they are grouped together and start to form groups and clsuter, just like our Local Group and the Virgo Supercluster. And we can clearly see the results of those incredible gravitation force IN FRONT OF OUOR EYES. If thats not beautiful i don't know what else is.","metadata":{"id":"YkCCplXpo287"}},{"cell_type":"code","source":"#to be able to find this plot we need to cut some outliers that can make some noise nad create trouble\n#thats why we use .loc \nplt.figure(figsize=(15,10))\nsns.scatterplot(x=combo.Mcz.loc[(combo.UB_color_index>-1.5)&(combo.UB_color_index<1)], y=combo.UB_color_index, size=0.3)","metadata":{"id":"V9F4waGj5CKF","outputId":"d11df0f1-2f12-4615-9fb7-137064175b10","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we can find the same result here with another color index.\nplt.figure(figsize=(15,10))\nsns.scatterplot(x=combo.Mcz.loc[(combo.BV_color_index>-1.5)&(combo.BV_color_index<1)], y=combo.BV_color_index, size=0.3)","metadata":{"id":"QcrUvAf8dTE6","outputId":"45907c51-5376-4460-884e-b0b7fffd7b93","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Cluster Analysis\n\nKnow, let's ay we want to group all our galaxies into groups, analyse if there is a pattern going on here. But we want to do that with some relevant features for the galaxies, thing taht metter the most. For a galaxy clusterization i choosed three main features: \n\n- The DISTANCE of the galaxy from us (MCZ feature correspond to it, its the RedShift of the galaxy. The greater the RedShift the Greater the distance)\n\n- The MAGNITUDE IN RED of the galaxy (Rmag feature. The greater the magnitude more red a galaxy is, and by consequence less bright it is. Low magnitude usually means blue and brighter colors, and bigmagnitudes means red and less bright galaxies)\n\n- The SIZE of the galaxy (ApDRmag is due to this. This is relative to the total and apperent magnitude of the galaxy)","metadata":{"id":"qBOYfqC9kJPN"}},{"cell_type":"code","source":"#separeting this features \ncombo_cluster = combo.iloc[:,[1,2,4]]","metadata":{"id":"XOGLBLfXkIPp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Dealing with outliers:\n\nHere we have some specificities of our dataset. Galaxies have a huge distributions of colors, shapes, and distances. To be able to do our cluster and also have a representative of the variance in the characteristics of those galaxies we will cut only the EXTREMY outliers, and the ones who will make some trouble to us in the future. (I test tons of possibilities of cuts for those outliers, the best trade of that i founded are seted bellow.)\n\nDisclaimer: usually cut outliers helps a lot distance based algorithms, just like K-means, for example, but that's not an absolute true. usually you have to try a lot of possibilities and find the best fit to your data specifities.","metadata":{"id":"GVzbLrwEoeLc"}},{"cell_type":"code","source":"#boxplot to visualize the Magnitude of our galaxies\n#they are great to look for outliers\nsns.boxenplot(combo_cluster.Rmag)","metadata":{"id":"lWfFd7hgjU0R","outputId":"d0ec847f-8809-4c5e-976e-dd96d6bc38d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#histogramm of the same data above\n#it allows us for look to outliers and have some notion of the sahpe of our data\n#a right-skewed shape in this case\ncombo_cluster.Rmag.hist()","metadata":{"id":"tlMDP5j-jtaW","outputId":"48e60270-3cbd-4c72-d628-e1a47576adf9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's cut the some bad outliers here\ncombo_cluster = combo_cluster.loc[combo_cluster['Rmag'] >18]","metadata":{"id":"tSAltsjikSvV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#boxplot for Maganitude after the cut \n#it seems better, and didin't affected to much the galaxies characteristics\nsns.boxenplot(combo_cluster.Rmag)","metadata":{"id":"K0mQ8UoEknM9","outputId":"f8858bf7-bcb2-4b0e-c81f-60661997b84e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#boxplot for galaxy Size\n#here we have some really bad outliers, we need to be a little more agressive with them this time\nsns.boxplot(combo_cluster.ApDRmag)","metadata":{"id":"nhbLb2XojlD_","outputId":"881d5ef1-92b2-46cf-d369-949eee36bd5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#histogram from the same data abpve\ncombo_cluster.ApDRmag.hist()","metadata":{"id":"HjB2mZKrkHCH","outputId":"1a753546-89dc-40f2-b710-d0214d354807","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cuting the outliers, but still alows to small and giant galaxies appers in the dataset\ncombo_cluster = combo_cluster.loc[combo_cluster['ApDRmag'] >-2]","metadata":{"id":"SgwLPZz_zbKv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#boxplot of the size after the cut\nsns.boxplot(combo_cluster.ApDRmag)","metadata":{"id":"rtFHZvkVzbOL","outputId":"8f52e9b6-a221-473f-c2e6-c20672117aa5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the distnce of the galaxies from us do not have some baroutliers\nsns.boxplot(combo_cluster.Mcz)","metadata":{"id":"UxSgV_njzbSk","outputId":"33db91b3-af3a-4a66-f618-3840bb2af091","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#histogram representing a bimodal data (if you want to see this better, increase the number of the bins) \ncombo_cluster.Mcz.hist()","metadata":{"id":"YYA5UHh_j9zo","outputId":"f25fc163-8623-4c66-f706-d7d78e53e0d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#after some search i founded a good point to discart the mot distant galaxies.\ncombo_cluster = combo_cluster.loc[combo_cluster['Mcz']<1.2]","metadata":{"id":"NiRFgchF731f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_cluster['Mcz'].hist()","metadata":{"id":"dWxJk_yD8AjQ","outputId":"5f778dcd-e096-4348-ab11-24146e61e4d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#taking a look in outr data right now\ncombo_cluster.head()","metadata":{"id":"9hz0DC8RpCer","outputId":"bc2333e5-a424-4a1f-db55-db012e8e7a74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#there is no missing that here, but jjust for safety:\ncombo_cluster = combo_cluster.dropna()","metadata":{"id":"Ikg1a9BwKWvk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Algorithms\n\nHere we going to create three different versions of our dataset. One will be the pure dataset, and the other twos will pass for some normalization. Usually the normalization the algorithms based in distances between the poits because it set the same \"weigth\" to the features. It do not allow tat features with huge scales (maginitude -Rmag- for example, who goes from 18 to 26) affect the model much more than features with tiny scales (just like Mcz, each goes from 0 to 1.2)","metadata":{"id":"83EPyLTLs1Mg"}},{"cell_type":"markdown","source":"### Scaling the Dataset","metadata":{"id":"IIUDvhDSx3zI"}},{"cell_type":"code","source":"scaler = StandardScaler()\nmin = MinMaxScaler()\ncombo_cluster_scaler = scaler.fit_transform(combo_cluster)\ncombo_cluster_min = min.fit_transform(combo_cluster)\ncombo_cluster_scaler","metadata":{"id":"AHAHxTBDGgqZ","outputId":"c2551d5a-bc00-4ae6-c446-1147a44da977","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to start our clusterization. We'll test three main algorithms: K-MEANS, AGGLOMERTAIVE CLUSTERING and DBSCAN.","metadata":{"id":"xYxi7PcxttIq"}},{"cell_type":"markdown","source":"###AGGLOMERATIVE CLUSTERING: \n\nA clustering based in the idea of starts a cluster with just one point, and from this point, start the aggregation of this points, creating a dendogram based in the distance from each point. The idea is creat small cluster that starts to agregate and create big clusters.\n\n- The distance between the points can be calculated based in the minimun distance possible etween the clusters, the maxium, the average and so on. here we are going to try soome of them with he euclidean distance.\n\n- Our metric for cluster anlysis will be the Silhouette score. The silhouette score measures the average distance between the intra-cluster points and the inter-clusters. Small intra-cluster distance are btter, the points inside the cluster have good cohesion, and big inter-cluster distance are very good, because it means that the clusters are very far fom each other and have a good delimitation. In the Silhouette score greater is better.","metadata":{"id":"k59GICfYuBr6"}},{"cell_type":"code","source":"def agglomerative_cluster(dataset):\n  silhouette_coefficient = []   #list to silhouette score\n  for linkage in ['ward', 'complete', 'average', 'single']:   # the distance measuremtn will be based in\n    for number in range(2,11):\n      agglomerative = AgglomerativeClustering(n_clusters= number, affinity='euclidean', linkage = linkage) #agglomerative model\n      agglomerative.fit(dataset)    #fitting the model\n      score = silhouette_score(dataset, agglomerative.labels_)\n      silhouette_coefficient.append(score)\n\n  #as the list is not separed by the linkage type, we can slice the list into peaces that we know \n  #that represent each type of linkage. take the linkage that generate the best silhouette score to analyse it.    \n  print(f'ward: {max(silhouette_coefficient[:9])}, {len(silhouette_coefficient[:9])}')\n  print(f'maximun: {max(silhouette_coefficient[9:18])}, {len(silhouette_coefficient[9:18])}')\n  print(f'average: {max(silhouette_coefficient[18:27])}, {len(silhouette_coefficient[18:27])}')\n  print(f'minimun: {max(silhouette_coefficient[27:36])}, {len(silhouette_coefficient[27:36])}')\n\nagglomerative_cluster(dataset=combo_cluster)","metadata":{"id":"t4GFFSC0uMWG","outputId":"4b2f749c-4e57-4cfe-bf51-07be01ff32cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agglomerative_cluster(dataset=combo_cluster_min)","metadata":{"id":"ye97Yrnf4Ph3","outputId":"aee7d64f-a2da-4ec0-a385-ba73e4f61a36","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agglomerative_cluster(dataset=combo_cluster_scaler)","metadata":{"id":"Rw60CVrO4PR0","outputId":"54345755-8e41-4f29-f121-fc4e6c7d8848","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Agglomertaive cluster sets a random point to starts the cluster, That results in different cluster each times you run the algorithm. So the silhouette score will not be the same when yu run it. But here our best score is around 0.61 (best value possible is 1, the worst is 0)","metadata":{"id":"reJHVa7NxR-0"}},{"cell_type":"code","source":"silhouette_coefficient = []\nfor linkage in ['ward', 'complete', 'average', 'single']:\n  for number in range(2,11):\n    agglomerative = AgglomerativeClustering(n_clusters= number, affinity='euclidean', linkage = linkage)\n    agglomerative.fit(combo_cluster_scaler)\n    score = silhouette_score(combo_cluster_scaler, agglomerative.labels_)\n    silhouette_coefficient.append(score)\n      \nprint(f'ward: {max(silhouette_coefficient[:9])}, length: {len(silhouette_coefficient[:9])}')\nprint(f'maximun: {max(silhouette_coefficient[9:18])}, length: {len(silhouette_coefficient[9:18])}')\nprint(f'average: {max(silhouette_coefficient[18:27])}, length: {len(silhouette_coefficient[18:27])}')\nprint(f'minimun: {max(silhouette_coefficient[27:36])}, length: {len(silhouette_coefficient[27:36])}')","metadata":{"id":"_lO3IwEn-18b","outputId":"9c23736c-603c-403b-f539-4c5984e59136","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's plot the silhouette coefficient by number of clusters for the best linkage\nplt.style.use('fivethirtyeight')\nplt.plot(range(2,11), silhouette_coefficient[18:27])\nplt.xticks(range(2,11))\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Coefficient')\nplt.show()","metadata":{"id":"8OqWO4iyDKZ6","outputId":"8ef7a99d-f029-44c0-f462-d2616bc6d5a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see one of the prolems that we must deal in this case. Our best agglomerative clustering is good beacuse in creates a cluster who involves almost all the points in the dataset, being just a few points out of that cluster. This means that our cluster its not relevant for our analysis. You can see it graphically in the dendogram bellow. Just one color means there is just one cluster.","metadata":{"id":"FNrw5gtZ4gm2"}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.title('Galaxy Dendograms')\ndend = shc.dendrogram(shc.linkage(combo_cluster_scaler, method='average'))","metadata":{"id":"0oy3I6mvgjCU","outputId":"adbd2d0e-85af-4456-9096-a2506aaf6875","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###K-MEANS:\n\nIn the K-mEANS algorithm you MUST chose the number of clusters that you want to set. This number will create the same nuber of centroids. This centroids will be used as the center of the cluster. In each iteration of the algorithm the centroid will migrate to the best possible place, where the average of the distance of this centroid to the nerest points are the smallest as possible. \n\n- Thats why we need to set a Number of iterations, in this case: 500\n- need to set a number of the cluster (centroids) to be created\n- the init parameter can be used as 'random', so the centroids will be seted randomly is the n-dimensional space (here: 3D space). But when you set the k-means++ parametrs the centroids will be seted with some distance between those centroids, helping the algorithm to converge. ","metadata":{"id":"0cGCIKSb5jny"}},{"cell_type":"code","source":"# as before, try different combinations of cluster hyperparameters, and saving the sum of square error \ndef sse_kmeans_cluster(dataset):\n  sse = []\n  for k in range(1,11):\n    kmeans = KMeans(n_clusters = k, init='k-means++', n_init=50, max_iter=500, random_state=42 )\n    kmeans.fit(dataset)\n    sse.append(kmeans.inertia_)\n  print(sse)\n  return sse\nsse = sse_kmeans_cluster(dataset=combo_cluster)","metadata":{"id":"s_dPrOFwJe24","outputId":"0bfbd867-d137-45cc-851d-b0c34bb056af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here we plot the sum of square error that each combination of clusters\n#as we can see, around the two and 3 cluster we have a good trade of between low error and a goos number of clusters\nplt.style.use('fivethirtyeight')\nplt.plot(range(1,11), sse)\nplt.xticks(range(1,11))\nplt.xlabel('Number of Clusters')\nplt.ylabel('Error')\nplt.show()","metadata":{"id":"UDHPDXI6MkXd","outputId":"24b76d93-f577-47ae-94d2-145954ecf4da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are going to use a k new method: the elbow method. Using the derivative of the function, and where it have the greatest value will be you elbow, be best trade of for you.","metadata":{"id":"Wck-OGRf94t5"}},{"cell_type":"code","source":"knee = KneeLocator(\n    range(1,11), sse, curve = 'convex', direction = 'decreasing'\n)\nprint(f'The flexure points: {knee.elbow}')","metadata":{"id":"0TcYQcejasKh","outputId":"af92541d-efb3-4782-d0cd-b5d33d1e2ca7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now using the silhouette coefficient\n#first with the combo_cluster dataset\ndef silhouette_kmeans_cluster(dataset):\n  silhouette_coefficient = []\n  for k in range(2,11):\n    kmeans = KMeans(n_clusters = k, init='k-means++', n_init=30, max_iter=500, random_state=42 )\n    kmeans.fit(dataset)\n    score = silhouette_score(dataset, kmeans.labels_)\n    silhouette_coefficient.append(score)\n  return silhouette_coefficient\nsilhouette_kmeans_cluster(dataset=combo_cluster)","metadata":{"id":"K9B_OkFDasOB","outputId":"8ad07551-eae3-4f5e-f029-f160a7a2c8f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now with combo_cluster_min\nsilhouette_kmeans_cluster(dataset=combo_cluster_min)","metadata":{"id":"f9y1To15jKUB","outputId":"aa612711-ab65-45be-8586-c707177f823a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ploting with combo_cluster_min\n#and here we have the ebst k-means possible\nplt.style.use('fivethirtyeight')\nplt.plot(range(2,11), silhouette_coefficient[:9])\nplt.xticks(range(2,11))\nplt.xlabel('Number of clusters')\nplt.ylabel('coeficiente de silhouette')\nplt.show()","metadata":{"id":"Ejc6zXf_asSD","outputId":"a7211c75-4e1f-4091-bda1-0ddd09c8b7f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with combo_cluster_scaler\nsilhouette_kmeans_cluster(dataset=combo_cluster_scaler)","metadata":{"id":"C6HhdQQfjKYx","outputId":"65201427-1f02-4d9c-e537-0b19f4d9422b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###DBSCAN:\n\nThe Density Based Spacial Clustering for Application with Noise) we have a different approach, Here we need three types of points:\n\n- Core points: this points will be used as \"centers\" for N other points (choosen by you) inside a specific radius, where all the other points around will be used as border points\n\n- Border points: this points is every single point wich do not have N points around them, like the core points\n\n- Outlier is a point wich are not reachable for core points , and not a border point\n\nThis means, it uses distance, but also deal with non spherical clusters, and with random shapes. And maybe it will be usefull for our problem.","metadata":{"id":"jYE0eXHe__hq"}},{"cell_type":"code","source":"def dbscan_cluster(dataset):\n  silhouette_coefficient=[]\n  #setting radius for core points\n  for radius in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n    #setting number of points inside the radius\n    for n_samples in [2,3,4,5,6,7,8,9,10]:\n      dbscan = DBSCAN(eps=radius,min_samples = n_samples)\n      dbscan.fit(dataset)\n      score = silhouette_score(dataset, dbscan.labels_)\n      silhouette_coefficient.append(score)\n      \n  print(f'radius-0.1: {max(silhouette_coefficient[:9])}, {len(silhouette_coefficient[:9])}')\n  print(f'radius-0.2: {max(silhouette_coefficient[9:18])}, {len(silhouette_coefficient[9:18])}')\n  print(f'radius-0.3: {max(silhouette_coefficient[18:27])}, {len(silhouette_coefficient[18:27])}')\n  print(f'radius-0.4: {max(silhouette_coefficient[27:36])}, {len(silhouette_coefficient[27:36])}')\n  print(f'radius-0.5: {max(silhouette_coefficient[36:45])}, {len(silhouette_coefficient[36:45])}')\n  print(f'radius-0.6: {max(silhouette_coefficient[45:54])}, {len(silhouette_coefficient[45:54])}')\n  print(f'radius-0.7: {max(silhouette_coefficient[54:63])}, {len(silhouette_coefficient[54:63])}')\n  print(f'radius-0.8: {max(silhouette_coefficient[63:72])}, {len(silhouette_coefficient[63:72])}')\n  print(f'radius-0.9: {max(silhouette_coefficient[72:81])}, {len(silhouette_coefficient[72:81])}')\n","metadata":{"id":"Rjc26sspxSLM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using combo_cluster\ndbscan_cluster(dataset=combo_cluster)","metadata":{"id":"sTIRCm-CtUDF","outputId":"2ebbf836-5433-488c-92ab-160498374ad6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the combo_cluster_min creates an unique cluster, thats not usefull for us\n#dbscan_cluster(dataset=combo_cluster_min)","metadata":{"id":"yRzgDEfoMbfd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using the combo_cluster_scaler\ndbscan_cluster(dataset=combo_cluster_scaler)","metadata":{"id":"JOXQPK3MMb1n","outputId":"93f4c541-f977-478c-bc54-abff578023a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using the best dbscan \ndbscan_silhouette_coefficient=[]\nfor n_samples in [2,3,4,5,6,7,8,9,10]:\n  dbscan = DBSCAN(eps=0.7,min_samples = n_samples)\n  dbscan.fit(combo_cluster)\n  score = silhouette_score(combo_cluster, dbscan.labels_)\n  dbscan_silhouette_coefficient.append(score)","metadata":{"id":"Q9ikCGVXrc7Y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using radius of 0.7 and searching for number of samples inside this radus, in this case: 7 \nplt.style.use('fivethirtyeight')\nplt.plot(range(2,11), dbscan_silhouette_coefficient)\nplt.xticks(range(2,11))\nplt.xlabel('number of Samples')\nplt.ylabel('Silhouette Coefficient')\nplt.show()","metadata":{"id":"JbF1Z7KeqGFY","outputId":"c3a8541b-456a-4b1d-a488-dbbcaa37f344","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Best algorithms and hyperparameters\n\nTo a better visualization of our tools i will display our bests options for each algorithm, and then we set their classification for each point in the combo-cluster dataset","metadata":{"id":"OVqomYYgFWzU"}},{"cell_type":"code","source":"#train agglomerative cluster with the best hyperparameters\nagglomerative_cluster = AgglomerativeClustering(n_clusters = 2, linkage='average', affinity='euclidean')\nagglomerative_cluster.fit_predict(combo_cluster_scaler)\n\n#append the labels to every galaxy \ncombo_cluster['hierarchical_label'] = agglomerative_cluster.labels_","metadata":{"id":"v64Cqhn4sDJb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train kmeans cluster with the best hyperparameters\nkmeans_model = KMeans(n_clusters = 2, init='k-means++', n_init=50, max_iter=500, random_state=42 )\nfinal_kmeans = kmeans_model.fit(combo_cluster_min)\nkmeans_label = final_kmeans.labels_\n\n#append the labels to every galaxy\ncombo_cluster['kmeans_label'] = kmeans_label","metadata":{"id":"CQf2UltxzOva","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train DBSCAN cluster with the best hyperparameters\ndbscan_model = DBSCAN(eps=0.7, min_samples=7)\nbest_dbscan = dbscan_model.fit(combo_cluster)\ndbscan_labels = best_dbscan.labels_\n\n#append label\ncombo_cluster['dbscan_label'] = dbscan_labels ","metadata":{"id":"RszC0cxbrGx5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking a look in the dataset \ncombo_cluster.head(3)","metadata":{"id":"mkwtPn1BskJD","outputId":"dfa0b71e-b0ef-4457-e808-6085ce29221f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding the Results","metadata":{"id":"8aUGLppTyx8C"}},{"cell_type":"code","source":"# here we can see our greatest problem with the agglomertaive cluster\n# almost every point were classified as the same cluster, that's not usefull \n# REMEMBER: silhouette_coefficient = 0.57\ncombo_cluster['hierarchical_label'].value_counts()","metadata":{"id":"mnqwax81xa31","outputId":"9fc6cc0f-12be-4c68-bcf8-8b72c0bd864b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we hhave a better distribution, with two clusters created\n#REMEMBER: silhouette_coefficient = 0.49\ncombo_cluster['kmeans_label'].value_counts()","metadata":{"id":"0lZgaVw7zB_N","outputId":"08652c02-492d-47fc-d8d2-207107be8765","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here we have 3 clsuter 9notice: the third cluster (-1) was created based in the extreme values of the label 0 from kmeans \n# REMEMBER: silhouette_coefficient = 0.66 (our best value) \ncombo_cluster['dbscan_label'].value_counts()","metadata":{"id":"r7t8HGkM01Xz","outputId":"2ec2fa67-bd92-4cf0-d0a8-6f1c7184b267","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3D Plots","metadata":{"id":"FdREIAXgy5UN"}},{"cell_type":"markdown","source":"Now we can see in a 3D plot how the clusters were asign to the galaxies\n\n- Firts when can see that in the AGGLOMERATIVE cluster all the points have the same color (red), that means they are part of the same cluster, there is no difference between them, and we can see  that the points are not equal, or part of the sam estructure. So that won't be usefull for us. \n\n- That was problably caused by our outliers here, but i still prefer keep these points to be plausible with he reality.","metadata":{"id":"rtuXA_EiQV7s"}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ncores = {0:'red', 1:'blue'}\n\nfig = plt.figure(figsize=(21, 15))\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(combo_cluster.Rmag, combo_cluster.ApDRmag, combo_cluster.Mcz,\n           c= combo_cluster.hierarchical_label.apply(lambda x: cores[x]))\nax.set_title('Hierarchical 3D DISTRIBUTION')\nax.set_xlabel('MAGNITUDE(Rmag)')\nax.set_ylabel('TAMANHO (ApDRmag)')\nax.set_zlabel('DISTANCIA (Mcz)')","metadata":{"id":"1XlNKQ_5asbG","outputId":"1edc6a3e-140c-47ed-9029-da8d0dafe1d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the K-MEANS to display our points we can see a better clusterization, where we have two groups of galaxies, a little bit overlaped, but far better than the agglomertaive algorithm. That overlaping usually is due to the similarity between some galaxies, that probably happen because some galaxies are in transition from one group to another, or just anomalous galaxies. ","metadata":{"id":"_RwUiCYHRhHw"}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ncores = {1:'red', 0:'blue'}\n\nfig = plt.figure(figsize=(21, 15))\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(combo_cluster.Rmag, combo_cluster.ApDRmag, combo_cluster.Mcz,\n           c= combo_cluster.kmeans_label.apply(lambda x: cores[x]))\nax.set_title('KMeans 3D DISTRIBUTION')\nax.set_xlabel('MAGNITUDE(Rmag)')\nax.set_ylabel('TAMANHO (ApDRmag)')\nax.set_zlabel('DISTANCIA (Mcz)')","metadata":{"id":"C57kQeFL1e0M","outputId":"1bd8b859-d4c0-495c-8a47-54fce4a6d55a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we have our DBSCAN classification, and as we can see there are three cluster now, two very similar to the K-means cluster, and one more with some extremy vaues related with the maginitude and the size of the galaxy. One consequence of the good clusterization is that here we have less overlaping than before, that could the know by the better silhouette coeficcient.","metadata":{"id":"crk1xTLeR9RJ"}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ncores = {0:'red', 1:'blue', -1:'green', 2:'yellow'}\n\nfig = plt.figure(figsize=(21, 15))\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(combo_cluster.Rmag, combo_cluster.ApDRmag, combo_cluster.Mcz,\n           c= combo_cluster.dbscan_label.apply(lambda x: cores[x]))\nax.set_title('DBSCAN 3D DISTRIBUTION')\nax.set_xlabel('MAGNITUDE(Rmag)')\nax.set_ylabel('TAMANHO (ApDRmag)')\nax.set_zlabel('DISTANCIA (Mcz)')","metadata":{"id":"DiGvHPM61fCz","outputId":"672457d1-f64c-4abe-cf7d-a669b1323c1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a look by other positions. Lokking by the side of the plot we can see a non spherical cluster, and thats why the DBSCAN got better results than the other algorithms. The blue uster seems to be nearest to the Earth, and in general have lower magnitudes.","metadata":{"id":"g1DOLFtWWV9W"}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ncores = {0:'red', 1:'blue', -1:'green'}\n\nfig = plt.figure(figsize=(21, 15))\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(combo_cluster.Rmag, combo_cluster.ApDRmag, combo_cluster.Mcz,\n           c= combo_cluster.dbscan_label.apply(lambda x: cores[x]))\nax.set_title('DBSCAN 3D DISTRIBUTION')\nax.set_xlabel('MAGNITUDE(Rmag)')\nax.set_ylabel('TAMANHO (ApDRmag)')\nax.set_zlabel('DISTANCIA (Mcz)')\nax.view_init(0,90)","metadata":{"id":"Xak4LX8806cj","outputId":"9f15520f-f909-4fc2-e90e-f0c002c94f49","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A view from above shows us the relationship between size and magnitude. The more the size ncreases more red is the galaxy. Thats not a rule, and there is some overlaping there, but still possible to understand that. (Remeber, the greater the magnitude, more red and less brigth the galaxy is (and usually bigger).","metadata":{"id":"AlyQWoqtW4R3"}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ncores = {0:'red', 1:'blue', -1:'green'}\n\nfig = plt.figure(figsize=(21, 15))\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(combo_cluster.Rmag, combo_cluster.ApDRmag, combo_cluster.Mcz,\n           c= combo_cluster.dbscan_label.apply(lambda x: cores[x]))\nax.set_title('DBSCAN 3D DISTRIBUTION')\nax.set_xlabel('MAGNITUDE(Rmag)')\nax.set_ylabel('TAMANHO (ApDRmag)')\nax.set_zlabel('DISTANCIA (Mcz)')\nax.view_init(90,270)","metadata":{"id":"S4yonL4Nz0d7","outputId":"0f43c929-cded-4787-b9c6-1cec535fffda","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## the means of the Cluster\n\nNow we can group by the dataset by the labels that we created with the clustering algorithms, finding the mean of each feature and knowing the average value of each feature that we want.","metadata":{"id":"_ydBnac-X42F"}},{"cell_type":"code","source":"combo_cluster.groupby('kmeans_label').mean()","metadata":{"id":"xOnGHF0Yasgt","outputId":"be8720c3-a964-4511-afde-4c071e900b58","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo_cluster.groupby('dbscan_label').mean()","metadata":{"id":"P24cQ5nQasdh","outputId":"84aee525-3a5b-4eed-8c3c-8b7f51d72f80","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Statistical Analysis","metadata":{"id":"VVN8hWFajZp5"}},{"cell_type":"markdown","source":"As we can se, the DBSCAN algorithm did a better job than the other ones. In fact it did a silhouette score of 0.64. Not a bad score. This algorithm returned three cluster. Where greater Rmag means more red galaxies,  more positive ApDRmag means bigger galaxies and higher Mcz is related to far galaxies, and here is their analysis:\n  \n\n- Label 0: Here we have a clear Red Giant galaxies. Those kind os struture usualy are bigger tahn the other, because they were formed by the merge of other galaxies, like two blue galaxies or small red galaxies.\n\n- Label 1: A variety of blues galaxies. There are two types of galaxies, barred spiral galaxies and non-barred spiral galaxies. We can't tell with those data what kind each galaxy is, but with The Magnitude in Red (Rmag) we can clearly say that are Blue median size galaxies in avarege.\n\n- Label -1:There just a few samples of that type here, so its hard to be sure os what we're seeing. but it seems to be a very intense and dwarf blue galaxies. With a very low Rmag and small size comparing with the two other samples.","metadata":{"id":"e5zHXS90nmOr"}},{"cell_type":"markdown","source":"##EVALUATING SIZE","metadata":{"id":"ke68WSZZqgSQ"}},{"cell_type":"markdown","source":"###Hypothesis Test with Two sample T-test\n\nWith those datas into our hands we can start to try to figure some queestions that could come to your mind. A good thought is: That difference in size between these two samples are true ? Can we say these two populations are really different by size? \n\nTo to that we can just to a simple Two Samples T-test. In that case we can treat this the groups of galaxies (Label 0 and label 1) as two samples of all the galaxies that we could have. Label 0 will be called as the Red population and Label 1 will be called as Blue population. So, with these the samples in our hands we are capable to look for the difference in the mean size of these two groups of galaxies. To do that we're going to create a Hypotesis Test, wich is a test to validate if two groups are similar enough or not. So we create a Null Hypothesis, wich is the idea of the mean size of these two samples are  similar enough to considere then as part of the same population galaxies size. An we create an Alternative Hypothesis, that tells us teh mean sie os those groups are so different that we could teel the are part of two distinquisy populations. O let's put our hand on the hypothesis test:\n\n- Hypothesis Tes\n  - H0: mean size Red sample = mean size Blue sample\n  - H1: mean size Red sample != mean size Blue sample\n\n- Alpha level:\n  -  0.05: this number set an acceptable level of certainty that we need to have to retain or reject the null hypothesis. This means we need to have less than 5% of probability to take a t-test result by chance and not due to a true characteristics of the mean\n\n","metadata":{"id":"XedDdRdWCtSZ"}},{"cell_type":"markdown","source":"###EVALUATING THE NORMALITY OF OUR DATA: ( we have three aprouches\n\n  - Histogram: the easier and less acurated way to analyse a distribution. Its a good start but ever the point of a last decision.\n\n  - QQplot: acording to some discutions s the most acurated way to vizualize the normalite of the data, especialy for tons of data, just like our case. here we plot all the points of our distributions against the expected quartils of a normal distribution. If your point follow the normal line of those quartil you can considere your data as a normal or like-normal curve.\n\n  - Normality test (Shapiro Wilk test): a good statistical test, where H0 is the hypothesis of normality of your data and H1 is the non-normality of the data. If p > 0.05 you have a normal curve, else, your curve is not normal. Acording to my researchs this test is affected for just a few differences in your data from the normal curve when your dataset is hugh. So we'll be not using those test as a last word for normality. ","metadata":{"id":"OD84p41JPwO6"}},{"cell_type":"code","source":"#taking a look in all ApDRmag points\n#it looks like normal, but with ligth right-skewed data\ncombo_cluster['ApDRmag'].hist(bins=20)","metadata":{"id":"aKKHUWW6QXFs","outputId":"420c3a11-f2b1-43eb-b3d0-924bbdaf7c4e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qqplot(combo_cluster['ApDRmag'], line='s')\nplt.show()","metadata":{"id":"X7h-0RMWuQZS","outputId":"b6b14d2c-dac3-4782-a510-69f64b40ca92","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as we can se, despite of the normal like curve, \n#the shapiro wilk test tells us our data is really far from the normality [p(0.05)>>>>>0.2085440954504141e-21]\nstats, p = shapiro(combo_cluster['ApDRmag'])\nprint(stats, p)","metadata":{"id":"REpxmMqtUfl1","outputId":"e193d828-b698-4c34-9e6b-7fb30da8b2cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we'll cut some outliers, mainly the smallest values, that is due to Label -1 (dwarf galaxies), so we can cut them\nsize = combo_cluster['ApDRmag'].loc[(combo_cluster['ApDRmag'] >-1.08)&(combo_cluster['ApDRmag']<1.3)] \nsize.hist(bins = 30)\n","metadata":{"id":"nuxA6s-TVxZs","outputId":"298d21f9-7a40-4ab7-8c33-3d3b3f068a4d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we can clearly see that the data is much more normal-like data\nqqplot(size, line='s')\nplt.show()","metadata":{"id":"wXpjHa0iOI-u","outputId":"ebf30a1b-8017-41ee-a7ba-61598dfc8b0d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stats, p = shapiro(size)\nprint(f'{stats}, {p:.10f}')","metadata":{"id":"UrE8oulaVxdS","outputId":"c36030a0-f2d1-4ed1-d712-55e1e7b39525","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can considure our data as a normal like data. So lets start to separete our data and see how their distribution looks like.\n\nFirts of all we need to separate our data in two groups. One group will cointain the size of Label 0 group, wich we're going to call \"red_sample\", and the other will contain the size from Label 1 group, the \"blue_sample. ","metadata":{"id":"_4eUZMWkV6l3"}},{"cell_type":"code","source":"red_sample = combo_cluster['ApDRmag'].loc[(combo_cluster['ApDRmag'] >-1.08)&(combo_cluster['ApDRmag']<1.3)&(combo_cluster['dbscan_label']==0)]\nblue_sample = combo_cluster['ApDRmag'].loc[(combo_cluster['ApDRmag'] >-1.08)&(combo_cluster['ApDRmag']<1.3)&(combo_cluster['dbscan_label']==1)]","metadata":{"id":"vCSntbT_VxvS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RED SAMPLE\n\nDoes the red sample follow the normality?","metadata":{"id":"71LPBDCOXfed"}},{"cell_type":"code","source":"red_sample.hist(bins=20)","metadata":{"id":"sGL65mAQn-E0","outputId":"3dc40d62-c947-47f6-e24e-d1ea8111da07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qqplot(red_sample, line='s')\nplt.show()","metadata":{"id":"Ct9hl0gVVxzt","outputId":"fcfc5d38-8e2b-4ad3-cba5-da1c2ccae7b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BLUE SAMPLE\n\nDoes blue_sample follow the normality?","metadata":{"id":"U5_ed4sgXhyx"}},{"cell_type":"code","source":"blue_sample.hist(bins=20)","metadata":{"id":"PmjxqGuapwoq","outputId":"67a92b1d-0fd4-4ee4-d0ee-f4c348d83284","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qqplot(blue_sample, line='s')\nplt.show()","metadata":{"id":"XCr-7uFynzlI","outputId":"d1d0bc55-1824-4478-d16e-f89e4b17a33d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###The T-test\n\nNow we are able to start our TWO SAMPLE T-TEST for INDEPENDENT SAMPLES\n\nTo be able to do this test you need to know how far the the means are from each other in standard deviation units. To find that you can use the formula:\n  - t-statistc = difference of the sample means /standard error of the difference between the means\n  - and the standard error of the differences between the means is:\n    - The square of standard error from the first sample / number of samples inside of sample 1 +   square of standard error from the second sample / number of samples inside of sample 2","metadata":{"id":"KGzDR7bmXlcQ"}},{"cell_type":"markdown","source":"Here we are going to create a step-by-step two sample t-test for independent samples. That's good if you want to understand better how do we get the final result. After that we can compare it with a scipy function that returns a similar function","metadata":{"id":"nh6MJsMXXxNH"}},{"cell_type":"code","source":"#let's deffine a function to find the t-statistic\ndef t_statistic(): \n  #set an alpha value \n  alpha = 0.05\n  \n  #first lets find the red sample mean and the blue sample mean, then find their differencw\n  bsm = blue_sample.mean() #blue sample mean\n  rsm = red_sample.mean() #red sample mean\n  dsm = rsm - bsm #difference of sample means \n\n  #now we need to find the square root of the differences between the menas\n  #find the standard deviation of the samples. I'll be using Benso's correction fot samples\n  std_rs = stdev( red_sample) #for population use pstdev()\n  std_bs = stdev(blue_sample)\n\n  #find number of samples in each group\n  n_rs = len(red_sample)\n  n_bs = len(blue_sample)\n\n  #calculate standard error of the difference between the means\n  dbm = sqrt(((std_rs**2)/n_rs) + ((std_bs**2)/n_bs))\n\n  #t-statistic\n  t = dsm / dbm \n\n  # find the degrees of freedom\n  df = n_rs+n_bs - 2\n\n  #find the critical value using the  percent point function (ppf)\n  crit_val =  scipy.stats.t.ppf(1.0 -alpha, df)\n\n  #find p-value with cumulative distribution function (cdf)\n  # *2 because we dont care if the the blue mean is bigger or smaller than the red mean (two-tailed t-test)\n  p = (1 - scipy.stats.t.cdf(abs(t), df)) * 2 \n\n\n\n  return t, df, crit_val, p\nt_stats, df, cv, p = t_statistic() \nprint(f'T-statistc: {t_stats}')\nprint(f'Degrees of freedom: {df}')\nprint(f'T-critical value: {cv}')\nprint(f'P-value: {p}')","metadata":{"id":"YsUHUt4EqPuS","outputId":"6c6a0353-a1c8-4021-8163-30a2b18418dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we will evaluate our hypothesis:\n\n- With an alpha level of 0.05, and a P-value so low that our function can't reach the true value we can take the following thought:\n  - P < alpha: WE REJECT THE NULL HYPOTHESIS. That Means the probability of those sample, taken by chance, have a incredible small probability of be part of the same population. So we accept the NULL-HYPOTHESIS, where there are actually a true difference between the sample means of the galaxies size. \n\n  - With this we can say: We have two different populations of galaxies divided by their sudes and related with their color. Blue galaxxies size are different from Red galaxies size. If you want to know each is bigger, you can do a one tailed t-test.","metadata":{"id":"G05YEEpDsEy0"}},{"cell_type":"code","source":"#comparing with t-test independent of Scipy\n#and it seans that the t-statistic is pretty similar to our function\nfrom scipy.stats import ttest_ind\nstat, p = ttest_ind(red_sample, blue_sample)\nprint(f'T-statistic: {stat}')\nprint(f'P-value: {p}')","metadata":{"id":"AeyDjg9KqPyH","outputId":"34006fd0-d536-4a70-e747-f65323257dd4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Evaluating R square: \n\nThe R square coefficient tells us how much of the variation of dependent variable can be explaned by the groups that we choosed. You can think it as: how much of he size of a galaxy can be explaned by the group of it belongs ? If a galaxy is the biggest galaxy you ever seem, and it is in the Label 0 group, how much of it's size can be explaned by this galaxy being in the label 0?","metadata":{"id":"c6gE6GhLzqbJ"}},{"cell_type":"code","source":"r2 = (pow(t_stats,2))/((pow(t_stats,2))+df)\nprint(r2)","metadata":{"id":"0Q0zGTS6zteA","outputId":"d462db43-dc9f-4a8c-ead8-bdfa00489e5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we got a R2 value of 0.10. That means, if we take a galaxy and look to the it's size, we could say only 10% of that could be explaned by the galaxy be part of Label 0 or Label 1. The other 90% is due to other unkow ariables, maybe like distance from Earth, the age of the galaxy, the format of the galaxy, and many other factors.","metadata":{"id":"QNFtOVdHY-h6"}},{"cell_type":"markdown","source":"###Confidence Interval for 95%\n\nThe confidence interval teels us we 95% of our data will be in the distribution. That's great if we want to know if we have a goos estimation of some parametter. Let's see that\n\n- IC 95% = (Mean difference +/- t_critic * Standard Error)","metadata":{"id":"oiJbHV4I78GR"}},{"cell_type":"code","source":"def ic_95():\n  bsm = blue_sample.mean() #blue sample mean\n  rsm = red_sample.mean() #red sample mean\n  dsm = rsm - bsm #difference of sample means \n\n  #now we need to find the square root of the differences between the menas\n  #find the standard deviation of the samples. I'll be using Benso's correction fot samples\n  std_rs = stdev( red_sample) #for population use pstdev()\n  std_bs = stdev(blue_sample)\n\n  #find number of samples in each group\n  n_rs = len(red_sample)\n  n_bs = len(blue_sample)\n\n  #calculate standard error of the difference between the means\n  dbm = sqrt(((std_rs**2)/n_rs) + ((std_bs**2)/n_bs))\n\n  under_bounder = (dsm - cv*dbm )\n  upper_bounder = (dsm + cv*dbm )\n\n  return under_bounder, upper_bounder","metadata":{"id":"gaCa1icB9JD-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"under, upper = ic_95()\n\nprint(f'IC95%: ({under}, {upper})')","metadata":{"id":"-cZu6biw-1ay","outputId":"98833905-f9c9-4a92-c337-fe5d2cc7de83","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This output means that 95% of the difference between the means are around 0.22 and 0.26. Knowing that we can say our data are far from 0, that is the point where there is no difference between the sizes of the galaxy. So we are pretty sure that this two groups have a real difference between their sizes.","metadata":{"id":"NaVZJpluZ-E-"}},{"cell_type":"markdown","source":"## EVALUATING DISTANCE\n\nRedShift (Mcz- distance)\n\nKnow we want to understand how the distance of the galaxies are distributed. There is more galaxies close or far from us ? These galaxies are grouped in clusters or they have an homogeneous distribution ? Let's taht a look on them:","metadata":{"id":"gAl8dbsTpp7h"}},{"cell_type":"markdown","source":"###Evaluating the normality of the data","metadata":{"id":"3pQKx0fC1yeR"}},{"cell_type":"code","source":"#it seems we have a bimodal distribution. There is a small pick of galaxies relatively close to us, and another cluster far from us\ncombo_cluster['Mcz'].hist(bins=30)","metadata":{"id":"LSktjkl-njjT","outputId":"d54951a9-7dc6-46b2-893e-9988a92881bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the shape os the data can be confirmed using this qqplot\nqqplot(combo_cluster['Mcz'], line='s')\nplt.show()","metadata":{"id":"xTt62pF9njnJ","outputId":"8d0dabaa-05c2-4e8d-9dce-5caf9d2a6c5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take the galaxy's distance acording to the group we classified in our BbSCAN cluster.","metadata":{"id":"MNz9cZxydPGE"}},{"cell_type":"code","source":"near_sample = combo_cluster['Mcz'].loc[combo_cluster['dbscan_label']==1]\nfar_sample = combo_cluster['Mcz'].loc[combo_cluster['dbscan_label']==0]","metadata":{"id":"qSalkuwUnjqG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distribution of galaxies in nearest group acording to DBSCAN\nnear_sample.hist(bins=30)","metadata":{"id":"KTpd8KEXnjt-","outputId":"0b1030d6-3547-4093-836b-f7e77cf89787","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#definitely this is not a normal distribution\nqqplot(near_sample, line='s')\nplt.show()","metadata":{"id":"t9b0SCZfnjxb","outputId":"1188c596-9f31-47d3-d14f-04054e05f823","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the far galaxies do not follow the normal distribution two\nfar_sample.hist(bins=30)","metadata":{"id":"fWEbb_1cnj1Y","outputId":"9f3358d4-1348-4c75-e1e1-5cbc752e81da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qqplot(far_sample, line='s')\nplt.show()","metadata":{"id":"HP7UdsVqnj4B","outputId":"bcc82872-f1ac-4d52-e46d-a7cf823774db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Non-parametric Data\n\nBut how we can compare and how if we can truly considere this galaxies part of different groups or just an misleading of our eyes?\n\nWhen we do not have a parametric data (normal distributed data) we can use non-parametric tests. There is a long debate about the statistical power of those tests, and when to apply them. Here er have two samples that do not follow the normal distribution, so we cannot compare them using standard deviation and parametric tools. But, we can compare each data is bigger than other (we cannot tell how much each point is bigger or smaler than the other, thi means we lose the magnitude of the difference between them. However, we can certainly tekll each of then is bigger tahn the other). In our casa, where we have a good number of observations (a lot of galaxies), and we want to compare two independent samples, we are able to use the Mann-Whitney non-parametric test.\n\nmann-Withney tells us how much the two groups are separed for each other using the position of each data in a Rank of all the galaxies in our dataset, and not the absolute distance of each galaxy, just like the example before with the t-test. Here we have the steps to find the U-statistics (Mann_Whtney score).","metadata":{"id":"Kbg5Q8sOeFSQ"}},{"cell_type":"markdown","source":"### HYPOTHESIS TEST AND MANN-WHITNEY TEST:\n  \n  - H0: no difference between the medias of the distance\n\n  - H1: there is a difference between the medians of the distances \n\nSTEP 1: PLACE SAMPLE IN TWO COLUMNS (already done, near_sample, far_sample)\n\nSTEP 2: STACK THE THE SAMPLE IN ONE COLUMN RANKED BY THE VALUE (the smaller gets one and the bgger gets (n)\n\nSTEP 3: FOR TIED RANKS, SUM THE **POSITION** OF ALL THE THIS TIED DATAS AND GET THEIR AVERAGE.\n\nSTEP 4: ASIGN EACH POSITION TO EACH NUMBER THAT YOU RANKED. NOW, INSIDE OF EACH GROUP YOU HAVE THE ABSOLUTE VALUE OF THE GALAXY DISTANCE AND IT'S POSITION N THE RANK.\n\nSTEP 5: SUM THE POSITIONS IN THE RANK INSIDE EACH GROUP. IN THE AND YOU WILL HAVE THE SUM OF RANKS FOR NEAR_SAMPLE AND THE SUM OF THE RANKS FOR FAR_SAMPLE\n\nSTEP 6: CALCULATE THE U-STATS FOR EACH GROUP USING THE FORMULA:\n    \n  - U-stats = SUM of the ranks - n(n+1)/2\n    \n  - Where n is the number of observations inside the sample\n\nSTEP 7: CHOOSE THE SMALLEST VALUE AND COMPARE IT TO THE U-CRITICAL FOR YOUR ALPHA LEVEL (0.05 IN THIS CASE). COMPARE the P-VALUE, FOUNDED WITH THE U-CRITICAL WITH YOUR ALPHA LEVEL (0.05)\n\nSTEP 8: GET YOUR RESULT","metadata":{"id":"0wIugW5XgFap"}},{"cell_type":"markdown","source":"\n### THE MANN-WHITNEY TEST","metadata":{"id":"oJaar-bAqgB8"}},{"cell_type":"code","source":"stats, p = mannwhitneyu(near_sample, far_sample)","metadata":{"id":"Sv4AptP8p7Jp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(stats)\nprint(f'U-stats: {p}')","metadata":{"id":"R_oS52cXvHtO","outputId":"bcaede0e-a5f3-4ca2-cc32-21534fa5f878","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With so small P-value we can reject the null-ypothesis, where the assumption is that the distribution of the both groups are identical and there is a 50% probability that an observation from a value ramdonly selected from one population exceeds an observation ramdonly selected from the other population.\n\nThis means we are almost sure that this two samples come from a different populatoins of distance from our galaxy.","metadata":{"id":"MWrMDms64daA"}},{"cell_type":"markdown","source":"###BONUS: OUR GALAXY\n\nAs you may know, we live in the MILKWAY galaxy. our galaxy is a beautiful blue and barred spiral galaxy. But what if our galaxy could be part of the blue galaxies of teses dataset? Where the MILKWAY would far in the distribution ? Are we more or less brighter than the other galaxies in thee group?\n\nTo be able to do that we can just supose the normality of our data, as we did before (using the histogram and the qqplot) and than find the Z-score, wich is how far a value (in this case the MilkWy's Magnitude) is from the mean of the distribution in terms of standard deviation. \n\n- The Rmag of the Milkway its something around 21, so let's use this value.","metadata":{"id":"h9rj1Tok5r_a"}},{"cell_type":"code","source":"#separating our data (The magntude of the galaxies but only the ones from the Label 1 group)\nblue = combo_cluster['Rmag'].loc[combo_cluster['dbscan_label']==1]","metadata":{"id":"BPNbhwwP2l9J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#just to know:\nblue.mean()","metadata":{"id":"ehk5Jo0gWCER","outputId":"a4f7dfb6-6ba8-436a-f09a-3d843a86cb38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Evaluating the normality of the data","metadata":{"id":"2etKhHOW2aU1"}},{"cell_type":"code","source":"#this kind of seem a normal distributions, with soome distortions and a little bit right skewed.\nblue.hist(bins=20)","metadata":{"id":"WEt5tDWL5wWk","outputId":"b1f4069d-2ebe-47e9-85fb-3ab1b3dd4bd3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the qqplot confirms what we thought before with the histogram\nqqplot(blue, line='s')\nplt.show()","metadata":{"id":"jG0kjKHZ6D0X","outputId":"f3eba104-04f9-4c23-b9e8-58ab652e3835","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###The Z-Score","metadata":{"id":"9aX9A0k12eyD"}},{"cell_type":"code","source":"#Getting Z-score variables\nx = 21 #milkway Rmag\nmu = blue.mean() #mean of the distribution\nstd = stdev(blue) #standard deviation of the sample","metadata":{"id":"qKDYKvvq4ASx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding the Z-score\nz_stats = (x - mu) / std #z-score formula\nprint(f'Z-Score: {z_stats}')","metadata":{"id":"OOcRn0DAOyDB","outputId":"571df91d-1547-412b-a81b-a1f7745635dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have that our galaxy is 1.36 standard deviations units away from the mean, to the left because of the  negative value. But i really want to know how much our galaxy is brighter than the other. And how we have a normal dat we also can have a Probability Density Function, and knowing the Z-score the can just calculate the integral of this value and find the probability of that the value of our galaxy or low by chance inthis sample. Thats also means that the point where our galaxy falls in the distribution tells us how much brighter is the milkway relative to other galaxies in this sample. ","metadata":{"id":"AmsQYMN3teOD"}},{"cell_type":"code","source":"#p-value for left tail \np_value = scipy.stats.norm.sf(abs(z_stats))\nprint(f'P-value: {p_value}')","metadata":{"id":"Wb6XGM-5O8VK","outputId":"347b904d-2346-4b02-f7dc-bce1efe7afc0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we now know that our galaxy is in the 8.5% brightest gaaxie in this sample. We are preaaty bright and blue, congrats Milkway!","metadata":{"id":"nolqEjQ4ulwW"}},{"cell_type":"markdown","source":"\n#THE END\n\nThank you for visit this notebook. Here we travel across the galaxies using some mathematical and statistical tools, and also a litte bit of machine learning. I hope you have liked it! thank you a lot!!\n","metadata":{"id":"9W23bzwjvTZn"}}]}