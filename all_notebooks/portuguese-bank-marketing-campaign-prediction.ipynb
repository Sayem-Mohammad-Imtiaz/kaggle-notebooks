{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Problem Statement\n\n### Business Use Case\n\nThere has been a revenue decline for a Portuguese bank and they would like to know what actions to take. After investigation, they found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have higher chance to subscribe for a term deposit and focus marketing efforts on such clients.\n\n### Data Science Problem Statement\n\nPredict if the client will subscribe to a term deposit based on the analysis of the marketing campaigns the bank performed.\n\n### Evaluation Metric\nWe will be using AUC - Probability to discriminate between subscriber and non-subscriber. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns',None)\nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Loading Data Modelling Libraries\n\nWe will use the popular scikit-learn library to develop our machine learning algorithms. In sklearn, algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the matplotlib and seaborn library. Below are common classes to load."},{"metadata":{},"cell_type":"markdown","source":"### Loading Data Modelling Libraries\n\nWe will use the popular scikit-learn library to develop our machine learning algorithms. In sklearn, algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the matplotlib and seaborn library. Below are common classes to load."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier ,RandomForestClassifier ,GradientBoostingClassifier\nfrom xgboost import XGBClassifier \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import roc_auc_score ,mean_squared_error,accuracy_score,classification_report,roc_curve,confusion_matrix\nfrom sklearn.feature_selection import RFE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Load and Prepare dataset\n\n- In this task, we'll load the dataframe in pandas, drop the unnecessary columns and display the top five rows of the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# accessing to the folder where the file is stored\npath = '../input/bank-term-deposit-dataset/new_train2.csv'\n\n# Load the dataframe\ndataframe = pd.read_csv(path)\n\nprint('Shape of the data is: ',dataframe.shape)\n\ndataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding the dataset\n\n**Data Set Information**\n\nThe data is related to direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be subscribed ('yes') or not ('no') subscribed.\n\nThere are two datasets:\n`train.csv` with all examples (32950) and 14 inputs including the target feature, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n\n`test.csv` which is the test data that consists  of 8238 observations and 13 features without the target feature\n\nGoal:- The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y)."},{"metadata":{},"cell_type":"markdown","source":"### Check Numeric and Categorical Features\n\nIf you are familiar with machine learning, you will know that a dataset consists of numerical and categorical columns.\n\nLooking at the dataset, we think we can identify the categorical and continuous columns in it. Right? But it might also be possible that the numerical values are represented as strings in some feature. Or the categorical values in some features might be represented as some other datatypes instead of strings. Hence it's good to check for the datatypes of all the features.\n"},{"metadata":{},"cell_type":"markdown","source":"### Numeric Data \nNumerical data is a type of data that is expressed in terms of numbers rather than natural language descriptions. Ex: Person's height, weight, IQ etc. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# IDENTIFYING NUMERICAL FEATURES\n\nnumeric_data = dataframe.select_dtypes(include=np.number) # select_dtypes selects data with numeric features\nnumeric_col = numeric_data.columns      # we will store the numeric features in a variable\nprint(\"====\"*20)\nprint(\"Numeric Features:\")\nprint(numeric_data.head())\nprint(\"====\"*20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Data\nCategorical data is a type of data that can be stored into groups or categories with the aid of names or labels. \n\nFor example, gender is a categorical data because it can be categorized into male and female according to some unique qualities possessed by each gender. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# IDENTIFYING CATEGORICAL FEATURES\ncategorical_data = dataframe.select_dtypes(exclude=np.number) # we will exclude data with numeric features\ncategorical_col = categorical_data.columns      # we will store the categorical features in a variable\n\nprint(\"====\"*20)\nprint(\"Categorical Features:\")\nprint(categorical_data.head())\nprint(\"====\"*20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHECK THE DATATYPES OF ALL COLUMNS:\n    \nprint(dataframe.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHECK THE NON-NULL COUNT OF ALL COLUMNS:\nprint(dataframe.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for Class Imbalance\n\nClass imbalance occurs when the observations belonging to one class in the target are significantly higher than the other class or classes. A class distribution of **80:20 or greater** is typically considered as an imbalance for a binary classification. \n\nSince most machine learning algorithms assume that data is equally distributed, applying them on imbalanced data often results in bias towards majority classes and poor classification of minority classes. Hence we need to identify & deal with class imbalance. \n\nThe code below that takes the target variable and outputs the distribution of classes in the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are finding the percentage of each class in the feature 'y'\nclass_values = (dataframe['y'].value_counts()/dataframe['y'].value_counts().sum())*100\nprint(class_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations : \n- The class distribution in the target is ~89:11. This is a clear indication of imbalance.\n- By now you should be well familiar with the methods on how to deal with the imbalance in data."},{"metadata":{},"cell_type":"markdown","source":"###  Univariate analysis of Categorical columns\n\nUnivariate analysis means analysis of a single variable. Itâ€™s mainly describes the characteristics of the variable.\n\nIf the variable is categorical we can use either a bar chart or a pie chart to find the distribution of the classes in the variable.\n\n- The code plots the frequency of all the values in the categorical variables. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting the categorical columns\ncategorical_col = dataframe.select_dtypes(include=['object']).columns\nplt.style.use('ggplot')\n# Plotting a bar chart for each of the cateorical variable\nfor column in categorical_col:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    dataframe[column].value_counts().plot(kind='bar')\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations :\n\nFrom the above visuals, we can make the following observations: \n- The top three professions that our customers belong to are - administration, blue-collar jobs and technicians.\n- A huge number of the customers are married.\n- Majority of the customers do not have a credit in default\n- Many of our past customers have applied for a housing loan but very few have applied for personal loans.\n- Cell-phones seem to be the most favoured method of reaching out to customers.\n- Many customers have been contacted in the month of **May**.\n- The plot for the target variable shows heavy imbalance in the target variable. \n- The missing values in some columns have been represented as `unknown`. `unknown` represents missing data. In the next task, we will treat these values.  "},{"metadata":{},"cell_type":"markdown","source":"### Imputing `unknown` values of categorical columns \n\nIn the previous task we have seen some categorical variables have a value called `unknown`. `unknown` values are a kind of missing data.\nDepending on the use case, we can decide how to deal with these values. One method is to directly impute them with the mode value of respective columns.\n\n- The code below imputes the value `unknown` in the categorical columns with the mode value of that column. You can modify this function to replace any unwanted value(for e.g `NaN` value) in a column with a value of your choice."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe has no missing values, but still we will apply and see hoe the imputation works\n# Impute mising values of categorical data with mode\nfor column in categorical_col:\n    mode = dataframe[column].mode()[0]\n    dataframe[column] = dataframe[column].replace('unknown',mode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate analysis of Continuous columns\nJust like for categorical columns, by performing a univariate analysis on the continuous columns, we can get a sense of the distrbution of values in every column and of the outliers in the data. Histograms are great for plotting the distribution of the data and boxplots are the best choice for visualizing outliers. \n\n- The code below plots a histogram of all the continuous features and other that plots a boxplot of the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in numeric_col:\n    plt.figure(figsize=(20,5))\n    plt.subplot(121)\n    sns.distplot(dataframe[column])\n    plt.title(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis - Categorical Columns\n\nBivariate analysis involves checking the relationship between two variables simultaneously. In the code below, we plot every categorical feature against the target by plotting a barchart. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in categorical_col:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    sns.countplot(x=dataframe[column],hue=dataframe['y'],data=dataframe)\n    plt.title(column)    \n    plt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- The common traits seen for customers who have subscribed for the term deposit are :\n    - Customers having administrative jobs form the majority amongst those who have subscirbed to the term deposit with technicians being the second majority.\n    - They are married \n    - They hold a university degree\n    - They do not hold a credit in default\n    - Housing loan doesn't seem a priority to check for since an equal number of customers who have and have not subscribed to it seem to have subscribed to the term deposit.\n    - Cell-phones should be the preferred mode of contact for contacting customers."},{"metadata":{},"cell_type":"markdown","source":"### Function to Label Encode Categorical variables\n\nBefore applying our machine learning algorithm, we need to recollect that any algorithm can only read numerical values. It is therefore essential to encode categorical features into numerical values. Encoding of categorical variables can be performed in two ways:\n- Label Encoding\n- One-Hot Encoding.\n\nFor the given dataset, we are going to label encode the categorical columns. \n\n- In the code below we will perform label encoding on all the categorical features and also the target (since it is categorical) in the  dataset. You can modify the below function in order to perform One-Hot Encoding as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing lable encoder\nle = LabelEncoder()\n\n# Initializing Label Encoder\nle = LabelEncoder()\n\n# Iterating through each of the categorical columns and label encoding them\nfor feature in categorical_col:\n    try:\n        dataframe[feature] = le.fit_transform(dataframe[feature])\n    except:\n        print('Error encoding '+feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.to_csv('preprocessed_data.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this is cell only when you are running this file on to your systems.\n# intsall pandas profililing\n\nfrom pandas_profiling import ProfileReport\nprof = ProfileReport(dataframe)\nprof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accessing to the folder where the file is stored\npath = '../input/bank-term-deposit-dataset/preprocessed_data.csv'\n\n# Load the dataframe\ndataframe = pd.read_csv(path)\n\nprint('Shape of the data is: ',dataframe.shape)\n\ndataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying vanilla models on the data\n\nSince we have performed preprocessing on our data and also done with the EDA part, it is now time to apply vanilla machine learning models on the data and check their performance."},{"metadata":{},"cell_type":"markdown","source":"### Fit vanilla classification models\n\nSince we have label encoded our categorical variables, our data is now ready for applying machine learning algorithms. \n\nThere are many Classification algorithms are present in machine learning, which are used for different classification applications. Some of the main classification algorithms are as follows-\n- Logistic Regression\n- DecisionTree Classifier\n- RandomForest Classfier\n\nThe code we have written below internally splits the data into training data and validation data. It then fits the classification model on the train data and then makes a prediction on the validation data and outputs the scores for this prediction."},{"metadata":{},"cell_type":"markdown","source":"#### PREPARING THE TRAIN AND TEST DATA\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictors\nX = dataframe.iloc[:,:-1]\n\n# Target\ny = dataframe.iloc[:,-1]\n\n# Dividing the data into train and test subsets\nx_train,x_val,y_train,y_val = train_test_split(X,y,test_size=0.3,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### FITTING THE MODEL AND PREDICTING THE VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# run Logistic Regression model\nmodel = LogisticRegression()\n# fitting the model\nmodel.fit(x_train, y_train)\n# predicting the values\ny_scores = model.predict(x_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GETTING THE METRICS TO CHECK OUR MODEL PERFORMANCE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the auc roc curve\nauc = roc_auc_score(y_val, y_scores)\n#print('Classification Report:')\n#print(classification_report(y_val,y_scores))\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_scores)\nprint('ROC_AUC_SCORE is',roc_auc_score(y_val, y_scores))\n    \n#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n    \nplt.plot(false_positive_rate, true_positive_rate)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The above two steps are combined and run in a single cell for all the remaining models respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run Decision Tree Classifier\nmodel = DecisionTreeClassifier()\n\nmodel.fit(x_train, y_train)\ny_scores = model.predict(x_val)\nauc = roc_auc_score(y_val, y_scores)\n#print('Classification Report:')\n#print(classification_report(y_val,y_scores))\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_scores)\nprint('ROC_AUC_SCORE is',roc_auc_score(y_val, y_scores))\n    \n#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n    \nplt.plot(false_positive_rate, true_positive_rate)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run Random Forrest Classifier\nmodel = RandomForestClassifier()\n\nmodel.fit(x_train, y_train)\ny_scores = model.predict(x_val)\nauc = roc_auc_score(y_val, y_scores)\n#print('Classification Report:')\n#print(classification_report(y_val,y_scores))\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_scores)\nprint('ROC_AUC_SCORE is',roc_auc_score(y_val, y_scores))\n    \n#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n    \nplt.plot(false_positive_rate, true_positive_rate)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data\nx_train,x_val,y_train,y_val = train_test_split(X,y, test_size=0.3, random_state=42, stratify=y)\n# selecting the classifier\nrfc = RandomForestClassifier()\n# selecting the parameter\nparam_grid = { \n'max_features': ['auto', 'sqrt', 'log2'],\n'max_depth' : [4,5,6,7,8],\n'criterion' :['gini', 'entropy']\n             }\n# using grid search with respective parameters\ngrid_search_model = GridSearchCV(rfc, param_grid=param_grid)\n# fitting the model\ngrid_search_model.fit(x_train, y_train)\n# printing the best parameters\nprint('Best Parameters are:',grid_search_model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying the best parameters obtained using Grid Search on Random Forest model\n\nIn the task below, we fit a random forest model using the best parameters obtained using Grid Search. Since the target is imbalanced, we apply Synthetic Minority Oversampling (SMOTE) for undersampling and oversampling the majority and minority classes in the target respectively. \n\n__Kindly note that SMOTE should always be applied only on the training data and not on the validation and test data.__\n\nYou can try experimenting with and without SMOTE and check for the difference in recall. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score,roc_curve,classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.over_sampling import SMOTE\nfrom yellowbrick.classifier import roc_auc\n\n\n# A function to use smote\ndef grid_search_random_forest_best(dataframe,target):\n    \n    # splitting the data\n    x_train,x_val,y_train,y_val = train_test_split(dataframe,target, test_size=0.3, random_state=42)\n    \n    # Applying Smote on train data for dealing with class imbalance\n    smote = SMOTE()\n    \n    X_sm, y_sm =  smote.fit_sample(x_train, y_train)\n    # Intializing the Random Forrest Classifier\n    rfc = RandomForestClassifier(max_features='log2', max_depth=8, criterion='gini',random_state=42)\n    # Fit the model on data\n    rfc.fit(X_sm, y_sm)\n    # Get the predictions on the validation data\n    y_pred = rfc.predict(x_val)\n    # Evaluation of result with the auc_roc graph\n    visualizer = roc_auc(rfc,X_sm,y_sm,x_val,y_val)\n\n\ngrid_search_random_forest_best(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction on the test data\n\nIn the below task, we have performed a prediction on the test data. We have used Random Forrest for this prediction. \n\nWe have to perform the same preprocessing operations on the test data that we have performed on the train data. For demonstration purposes, we have preprocessed the test data and this preprocessed data is present in the csv file `new_test.csv`\n\nWe then make a prediction on the preprocessed test data using the random forrest model with the best parameter values we've got. And as the final step, we will read the `submission.csv`and concatenate this prediction with the `Id` column which is the unique client id and then convert this into a csv file which becomes the `final_submission.csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessed Test File\ntest = pd.read_csv('../input/bank-term-deposit-dataset/new_test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize Smote\nsmote = SMOTE()\n\n# Applying SMOTE\nX_sm, y_sm =  smote.fit_sample(x_train, y_train)\n\n# Initialize our Random forrest model with the best parameter values derived\nrfc = RandomForestClassifier(max_features='log2', max_depth=8, criterion='gini',random_state=42)\n\n# Fitting the model\nrfc.fit(X_sm,y_sm)\n\n# Predict on the preprocessed test file\ny_pred = rfc.predict(test)\n\n# storing the predictions\nprediction = pd.DataFrame(y_pred,columns=['y'])\n\n# reading the submission file with client ids\nsubmission = pd.read_csv('../input/bank-term-deposit-dataset/submission.csv')\n\n# Concatenate predictions and create our final submission file\nfinal_submission = pd.concat([submission['Id'],prediction['y']],1)\n\n# Results\nfinal_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}