{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# necessary imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set()\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv') # reading the data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*It seems that there are no missing values in our data. Great, let's see the distribution of data:*","metadata":{}},{"cell_type":"code","source":"# let's see how data is distributed for every column.\n\nplt.figure(figsize = (20, 25))\nplotnumber = 1\n\nfor column in data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is some skewness in data.","metadata":{}},{"cell_type":"markdown","source":"Also, we can see that Glucose, Insulin, Skin Thickness, BMI and Blood Pressure which have value as 0. That's not possible. We can either remove such data or simply replace it with their respective mean values.","metadata":{}},{"cell_type":"code","source":"# replacing zero values with the mean of the columnn\n\ndata['BMI'] = data['BMI'].replace(0, data['BMI'].mean())\ndata['BloodPressure'] = data['BloodPressure'].replace(0, data['BloodPressure'].mean())\ndata['Glucose'] = data['Glucose'].replace(0, data['Glucose'].mean())\ndata['Insulin'] = data['Insulin'].replace(0, data['Insulin'].mean())\ndata['SkinThickness'] = data['SkinThickness'].replace(0, data['SkinThickness'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# again checking the data distribution\n\nplt.figure(figsize = (20, 25))\nplotnumber = 1\n\nfor column in data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have dealt with the 0 values and data looks better.But, there still are outliers present in some columns. Let's deal with them.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = data, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outlier = data['Pregnancies'].quantile(0.98)\n# removing the top 2% data from the pregnancies column\ndata = data[data['Pregnancies']<outlier]\n\noutlier = data['BMI'].quantile(0.99)\n# removing the top 1% data from BMI column\ndata = data[data['BMI']<outlier]\n\noutlier = data['SkinThickness'].quantile(0.99)\n# removing the top 1% data from SkinThickness column\ndata = data[data['SkinThickness']<outlier]\n\noutlier = data['Insulin'].quantile(0.95)\n# removing the top 5% data from Insulin column\ndata = data[data['Insulin']<outlier]\n\noutlier = data['DiabetesPedigreeFunction'].quantile(0.99)\n# removing the top 1% data from DiabetesPedigreeFunction column\ndata = data[data['DiabetesPedigreeFunction']<outlier]\n\noutlier = data['Age'].quantile(0.99)\n# removing the top 1% data from Age column\ndata = data[data['Age']<outlier]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# again checking the data distribution\n\nplt.figure(figsize = (20, 25))\nplotnumber = 1\n\nfor column in data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (16, 8))\n\ncorr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\nsns.heatmap(corr, mask = mask, annot = True, fmt = '.2g', linewidths = 1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.drop(columns = ['Outcome'])\ny = data['Outcome']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the data into testing and training data.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaling the data \n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting data to model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nlr = LogisticRegression()\n\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nlr_train_acc = accuracy_score(y_train, lr.predict(X_train))\nlr_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Logistic Regression Model is {lr_train_acc}\")\nprint(f\"Test Accuracy of Logistic Regression Model is {lr_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix \n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K Neighbors Classifier (KNN)","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nknn_train_acc = accuracy_score(y_train, knn.predict(X_train))\nknn_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of KNN Model is {knn_train_acc}\")\nprint(f\"Test Accuracy of KNN Model is {knn_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix \n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\n\ny_pred = svc.predict(X_test)\n\nsvc_train_acc = accuracy_score(y_train, svc.predict(X_train))\nsvc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of SVC Model is {svc_train_acc}\")\nprint(f\"Test Accuracy of SVC Model is {svc_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\ny_pred = dtc.predict(X_test)\n\ndtc_train_acc = accuracy_score(y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decision Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decision Tree Model is {dtc_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'min_samples_split' : range(2, 10, 1),\n    'min_samples_leaf' : range(2, 10, 1)\n}\n\ngrid_search = GridSearchCV(dtc, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtc = grid_search.best_estimator_\n\ny_pred = dtc.predict(X_test)\n\ndtc_train_acc = accuracy_score(y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decesion Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decesion Tree Model is {dtc_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrand_clf = RandomForestClassifier(criterion = 'gini', max_depth = 3, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 4, n_estimators = 180)\nrand_clf.fit(X_train, y_train)\n\ny_pred = rand_clf.predict(X_test)\n\nrand_clf_train_acc = accuracy_score(y_train, rand_clf.predict(X_train))\nrand_clf_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Random Forest Model is {rand_clf_train_acc}\")\nprint(f\"Test Accuracy of Random Forest Model is {rand_clf_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Boosting","metadata":{}},{"cell_type":"markdown","source":"### Ada Boost Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nparameters = {\n    'n_estimators' : [50, 70, 90, 120, 180, 200],\n    'learning_rate' : [0.001, 0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, parameters, n_jobs = -1, cv = 5, verbose = 1)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada = AdaBoostClassifier(base_estimator = dtc, algorithm = 'SAMME', learning_rate = 0.001, n_estimators = 120)\nada.fit(X_train, y_train)\n\nada_train_acc = accuracy_score(y_train, ada.predict(X_train))\nada_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Ada Boost Model is {ada_train_acc}\")\nprint(f\"Test Accuracy of Ada Boost Model is {ada_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\n\nparameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.1, 1, 10],\n    'n_estimators': [100, 150, 180, 200]\n}\n\ngrid_search = GridSearchCV(gb, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb = GradientBoostingClassifier(learning_rate = 0.1, loss = 'deviance', n_estimators = 150)\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\n\ngb_train_acc = accuracy_score(y_train, gb.predict(X_train))\ngb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Gradient Boosting Classifier Model is {gb_train_acc}\")\nprint(f\"Test Accuracy of Gradient Boosting Classifier Model is {gb_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Boosting (SGB)","metadata":{}},{"cell_type":"code","source":"sgbc = GradientBoostingClassifier(learning_rate = 0.1, subsample = 0.9, max_features = 0.75, loss = 'deviance',\n                                  n_estimators = 100)\n\nsgbc.fit(X_train, y_train)\n\ny_pred = sgbc.predict(X_test)\n\nsgbc_train_acc = accuracy_score(y_train, sgbc.predict(X_train))\nsgbc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of SGB Model is {sgbc_train_acc}\")\nprint(f\"Test Accuracy of SGB Model is {sgbc_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cat Boost Classifier","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(iterations = 30, learning_rate = 0.1)\ncat.fit(X_train, y_train)\n\ny_pred = cat.predict(X_test)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncat_train_acc = accuracy_score(y_train, cat.predict(X_train))\ncat_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Cat Boost Classifier Model is {cat_train_acc}\")\nprint(f\"Test Accuracy of Cat Boost Classifier Model is {cat_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extreme Gradient Boosting (XGBoost) ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gblinear', learning_rate = 1, max_depth = 3, n_estimators = 10)\nxgb.fit(X_train, y_train)\n\ny_pred = xgb.predict(X_test)\n\nxgb_train_acc = accuracy_score(y_train, xgb.predict(X_train))\nxgb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of XGB Model is {xgb_train_acc}\")\nprint(f\"Test Accuracy of XGB Model is {xgb_test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking","metadata":{}},{"cell_type":"code","source":"# let's divide our dataset into training set and holdout set by 50% \n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val_train, test, val_test = train_test_split(X, y, test_size = 0.5, random_state = 355)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's split the training set again into training and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(train, test, test_size = 0.2, random_state = 355)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using Logistic Regression and SVM algorithm as base models.\n# Let's fit both of the models first on the X_train and y_train data.\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm = SVC()\nsvm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get the predictions of all the base models on the validation set val_train.","metadata":{}},{"cell_type":"code","source":"predict_val1 = lr.predict(val_train)\npredict_val2 = svm.predict(val_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's stack the prediction values for validation set together as 'predict_val'","metadata":{}},{"cell_type":"code","source":"predict_val = np.column_stack((predict_val1, predict_val2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get the prediction of all the base models on test set X_set.","metadata":{}},{"cell_type":"code","source":"predict_test1 = lr.predict(X_test)\npredict_test2 = svm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's stack the prediction values for validation set together as 'predict_set'","metadata":{}},{"cell_type":"code","source":"predict_test = np.column_stack((predict_test1, predict_test2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use the Stacked data 'predict_val' and val_test as the input feature for meta_model i.e. randomforest classifier.","metadata":{}},{"cell_type":"code","source":"rand_clf = RandomForestClassifier()\nrand_clf.fit(predict_val, val_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the accuracy of our meta_model using predict_test and y_test.","metadata":{}},{"cell_type":"code","source":"stacking_acc = accuracy_score(y_test, rand_clf.predict(predict_test))\nprint(stacking_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\nconfusion_matrix(y_test, rand_clf.predict(predict_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nprint(classification_report(y_test, rand_clf.predict(predict_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy score increases a lot after use of stacking.","metadata":{}},{"cell_type":"code","source":"models = ['Logistic Regression', 'KNN', 'SVC', 'Decision Tree', 'Random Forest','Ada Boost', 'Gradient Boosting', 'SGB', 'XgBoost', 'Stacking', 'Cat Boost']\nscores = [lr_test_acc, knn_test_acc, svc_test_acc, dtc_test_acc, rand_clf_test_acc, ada_test_acc, gb_test_acc, sgbc_test_acc, xgb_test_acc, stacking_acc, cat_test_acc]\n\nmodels = pd.DataFrame({'Model' : models, 'Score' : scores})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (18, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see \"Cat Boost\" and \"Stacking\" is giving best results.","metadata":{}},{"cell_type":"markdown","source":"### If you like this kernel, please do a upvote.","metadata":{}}]}