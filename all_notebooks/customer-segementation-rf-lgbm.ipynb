{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport missingno as mno\nimport matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy\nimport itertools\nfrom scipy.stats import spearmanr\nfrom collections import defaultdict\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets get the data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/customer-segmentation/Train.csv')\ntest=pd.read_csv('../input/customer-segmentation/Test.csv')\n\n#Joining both the testa and train data frames together \ndf=pd.concat([train,test],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at the data type of the features!\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize the missing values in the dataframe! \nmno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looks like we have missing values in both categorical and numerical features! \n\n#### Categorical features: \n1. 'Gender',\n2. 'Ever_Married',\n3. 'Graduated',\n4. 'Profession',\n5. 'Spending_Score',\n6. 'Var_1'\n\n#### Numerical features:\n1. 'Work_Experience'\n2. 'Family_Size'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### First let's fix the missing values in the categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtainign the categroical columns alone\ncatcols = []\nfor i in df.columns:\n  if df[i].dtype == \"object\":\n      catcols.append(i)\ncatcols     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catcols[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing the missing values in the categorical variables as \"not_available\"\ndf[catcols[:-1]] = df[catcols[:-1]].fillna(\"not_available\")\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since most of the classification algorithms accepts only numerical features, we make sure that the categorical features are converted into numerical features!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_map = {'Female': 1, 'Male': 0}\nmarriage_map = {'not_available': 99, 'No': 0, 'Yes': 1}\ngraduate_map = {'not_available': 99, 'No': 0, 'Yes': 1}\nprofession_map = {'Artist': 0,'Doctor': 1,'Engineer': 2,'Entertainment': 3,'Executive': 4,'Healthcare': 5,\n                   'Homemaker': 6,'Lawyer': 7,'Marketing': 8,'not_available': 99}\nspending_map = {'Average': 1, 'High': 2, 'Low': 0}\nvar_map = {'Cat_1': 1,'Cat_2': 2,'Cat_3': 3,'Cat_4': 4,'Cat_5': 5, 'Cat_6': 6, 'Cat_7': 7,'not_available': 99}\ntarget_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### using label encoder to convert the categorical features into numerical features!!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf[\"Prof+Grad\"] = df[\"Profession\"]+\"_\"+df[\"Graduated\"].astype(str)\ndf[\"Prof+Grad\"] = le.fit_transform(df['Prof+Grad'].astype(str))\n\ndf[\"Gender\"] =  df[\"Gender\"].map(gender_map)\ndf[\"Ever_Married\"] =  df[\"Ever_Married\"].map(marriage_map)\ndf[\"Graduated\"] =  df[\"Graduated\"].map(graduate_map)\ndf[\"Profession\"] =  df[\"Profession\"].map(profession_map)\ndf[\"Spending_Score\"] =  df[\"Spending_Score\"].map(spending_map)\ndf[\"Var_1\"] =  df[\"Var_1\"].map(var_map)\ndf[\"Segmentation\"] =  df[\"Segmentation\"].map(target_map)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets move with replacing the numeric variables \ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As u can see the only two numeric features which possess missing values is work experience and family size! so lets fix it !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Before fixing the missign values in the numerical features always understand the distribution of that paticular variable!\n### For instance the Work_experience variable has a log-normal distribution! In such cases imputing the missing values with mean value wont be suitable! When the distribution is skewed, always use median values to replace the missing values!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Work_Experience'].hist()\nprint(\"Median value of Family size feature is:\",train['Work_Experience'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Family_Size'].hist()\nprint(\"Median value of Family size feature is:\",train['Family_Size'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Work_Experience']=df['Work_Experience'].fillna(train['Work_Experience'].median())\ndf['Family_Size']=df['Family_Size'].fillna(train['Family_Size'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now since the age variable has a wide range of values it's better to bin the age values into different buckets and turn them into categorical variables!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"age_bins=[0,20,40,60,80,100]\nage_labels=[ \"<=20\",\"21-40\",\"41-60\", \"61-80\",\">80\"]\n\ndf['Age']=pd.cut(df['Age'], bins=age_bins,labels=age_labels)\ndf['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age']=le.fit_transform(df['Age'])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looks like we have converted all the categorical features to numerical features and finally took care of the missing values as well!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Generating aggregated features with columns Age and profession","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.groupby(['Age']).agg({'Spending_Score':['count','mean','sum'],\n                                   'Work_Experience':['count','sum','min','max','mean'],\n                                   'Profession':['min','max'],\n                                       'Family_Size':['sum','min','max'],\n                                       'Age':['count'],\n                                    'Var_1':['count','max','min']})\ntemp.columns = ['_'.join(x) for x in temp.columns]\ndf = pd.merge(df,temp,on=['Age'],how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.groupby(['Profession']).agg({\n                                       'Age':['count','sum','min','max']})\ntemp.columns = ['_Prof_'.join(x) for x in temp.columns]\ndf = pd.merge(df,temp,on=['Profession'],how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# No lets see if there is any multicollinearity present \n\n# Handling Multicollinearity using dendrogram\n* One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.\n* We manually pick a threshold by visual inspection of the dendrogram to group our features into clusters and choose a feature from each cluster to keep. We only select those features from our dataset.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.set_index('ID')\ndf_corr=df[df.columns[~df.columns.isin(['Segmentation','train_or_test'])]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(60, 30))\ncorr = spearmanr(df_corr).correlation\ncorr_linkage = hierarchy.ward(np.nan_to_num(corr))\ndendro = hierarchy.dendrogram(corr_linkage, labels=df_corr.columns, ax=ax1,\n                              leaf_rotation=90)\ndendro_idx = np.arange(0, len(dendro['ivl']))\n\nax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\nax2.set_xticks(dendro_idx)\nax2.set_yticks(dendro_idx)\nax2.set_xticklabels(dendro['ivl'], rotation='vertical')\nax2.set_yticklabels(dendro['ivl'])\nfig.tight_layout()\nplt.show()\nfig.savefig('test2png.png', dpi=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting the best set of features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numvar=[]\nfor i in np.arange(0.0, 2.1, 0.2):\n    cluster_ids = hierarchy.fcluster(corr_linkage, i, criterion='distance')\n    cluster_id_to_feature_ids = defaultdict(list)\n    for idx, cluster_id in enumerate(cluster_ids):\n        cluster_id_to_feature_ids[cluster_id].append(idx)\n    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n    numvar.append([i,len(selected_features)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting features based on chosen dendrogram y-axis value\ncluster_ids = hierarchy.fcluster(corr_linkage, 1.4, criterion='distance')\ncluster_id_to_feature_ids = defaultdict(list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\nselected_features = list(np.array(selected_features)+1)\nselected_features.append(-1)\nselected_features.insert(0,0)\ndf_corr = df_corr.iloc[:,selected_features[0:-2]]\n#df_corr.to_csv('modelreadydf.csv',index=0)\n#df_corr = pd.read_csv('modelreadydf.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featured_df=df_corr\nfeatured_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = featured_df.iloc[0:len(train),:]\nX_test = featured_df.iloc[len(train):,:]\ny_train=train['Segmentation']\ny_test=test['Segmentation']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Construction of model\n\n1. RF-baseline\n2. RF-gridsearchcv\n3. LGBM-gridsearchcv","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Random forest baseline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\npred_X=classifier.predict(X_train)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, pred_X)))\n\npred_rf_baseline = classifier.predict(X_test)\nprint('Testing accuracy is {}'.format(accuracy_score(y_test, pred_rf_baseline)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest using gridsearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nn_estimators = [100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] \nforest= RandomForestClassifier()\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best depth:\",bestF.best_estimator_.get_params()['max_depth'])\nprint(\"Best n_estimators:\",bestF.best_estimator_.get_params()['n_estimators'])\nprint(\"Best min_samples_split:\",bestF.best_estimator_.get_params()['min_samples_split'])\nprint(\"Best min_samples_leaf:\",bestF.best_estimator_.get_params()['min_samples_leaf'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf = RandomForestClassifier(random_state = 1,\n                                  n_estimators = 300,\n                                  max_depth = 8, \n                                  min_samples_split = 15,  min_samples_leaf = 1) \nmodel_rf = model_rf.fit(X_train, y_train)\npred_X = model_rf.predict(X_train)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, pred_X)))\npred_rf= model_rf.predict(X_test)\nprint('Testing accuracy is {}'.format(accuracy_score(y_test, pred_rf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGBM- Baseline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\n\nlgbm_clf = lgbm.LGBMClassifier(n_estimators=3000, cat_feature = [0,1,2,3,7,8,9,10,11,12,13,14,15,16,17,18], label_gain = [5], num_leaves=8, max_depth=20, \n                               learning_rate=0.01, random_state=42)\nlgbm_clf.fit(X_train, y_train)\npred_X = lgbm_clf.predict(X_train)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, pred_X)))\npred_lgbm= lgbm_clf.predict(X_test)\nprint('Testing accuracy is {}'.format(accuracy_score(y_test, pred_lgbm)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### These are the following pr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### LGBM - RandomCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'n_estimators':[1000,2000,3000,5000,10000], \n             'num_leaves':[15,25,30],\n             'learning_rate':[0.001,0.003,0.01,0.03],\n             'max_depth':[8,12,18,25],\n             'min_data_in_leaf':[40,50,60],\n             'reg_alpha':[i for i in np.arange(1,2,0.2)],\n             'reg_lambda':[i for i in np.arange(1,2,0.2)],\n             'subsample':[0.5,0.7,1]}\nlgbm_grid = RandomizedSearchCV(estimator=lgbm_clf1, param_grid=parameters, n_jobs=-1, cv = 5, scoring='accuracy', verbose=10)\n#Implement the lgbm_grid in the above lgbm baseline model to obtain better accuracy!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output=pd.DataFrame(columns=['ID','Segmentation'])\noutput['ID']=test['ID']\noutput['Segmentation']=pred_lgbm\noutput.to_csv('output.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}