{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## M6W1 Assignment","metadata":{}},{"cell_type":"markdown","source":"*Q0: Have a quick overview of the features and implement a “cleaning process”. Make sure this part of the code is well organised, if possible make this an object-oriented exercise.*","metadata":{}},{"cell_type":"code","source":"#Import necessary dependencies\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport re\nimport numpy as np\nfrom sklearn.metrics import roc_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","metadata":{"id":"DaWqPBEfyn63","execution":{"iopub.status.busy":"2021-05-30T19:59:19.943579Z","iopub.execute_input":"2021-05-30T19:59:19.944337Z","iopub.status.idle":"2021-05-30T19:59:21.261373Z","shell.execute_reply.started":"2021-05-30T19:59:19.944213Z","shell.execute_reply":"2021-05-30T19:59:21.26024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","metadata":{"id":"QwNqlBuvyn64","outputId":"5bacf603-fc8f-4ff3-cfd6-af1ddfd00963","execution":{"iopub.status.busy":"2021-05-30T19:59:21.262909Z","iopub.execute_input":"2021-05-30T19:59:21.263283Z","iopub.status.idle":"2021-05-30T19:59:21.364537Z","shell.execute_reply.started":"2021-05-30T19:59:21.263247Z","shell.execute_reply":"2021-05-30T19:59:21.36357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df.copy()\ndf.info()","metadata":{"id":"CPVZZoFpyn66","outputId":"b3ac9ff2-3e0c-40bc-9828-138da4381edd","execution":{"iopub.status.busy":"2021-05-30T19:59:21.366102Z","iopub.execute_input":"2021-05-30T19:59:21.366403Z","iopub.status.idle":"2021-05-30T19:59:21.400919Z","shell.execute_reply.started":"2021-05-30T19:59:21.366373Z","shell.execute_reply":"2021-05-30T19:59:21.399868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for null values in the dataset\ndf.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:21.402414Z","iopub.execute_input":"2021-05-30T19:59:21.402738Z","iopub.status.idle":"2021-05-30T19:59:21.421447Z","shell.execute_reply.started":"2021-05-30T19:59:21.402708Z","shell.execute_reply":"2021-05-30T19:59:21.420545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique().sort_values()","metadata":{"id":"W6XDP8tLyn67","outputId":"39b338bf-faf0-4d86-f4e9-444a733649c9","execution":{"iopub.status.busy":"2021-05-30T19:59:21.422676Z","iopub.execute_input":"2021-05-30T19:59:21.423179Z","iopub.status.idle":"2021-05-30T19:59:21.467095Z","shell.execute_reply.started":"2021-05-30T19:59:21.423137Z","shell.execute_reply":"2021-05-30T19:59:21.466389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on the above, all columns are categorical except for Tenure, Monthly Charges, Total Charges\ncols = ['Churn', 'gender', 'SeniorCitizen','Partner', 'Dependents', 'PaperlessBilling', 'PhoneService','Contract','StreamingMovies','StreamingTV','TechSupport','OnlineBackup','OnlineSecurity','InternetService', 'MultipleLines', 'DeviceProtection', 'PaymentMethod']\nfor col in cols:\n    print (col,':', df[col].unique())","metadata":{"id":"rmmccBnlyn67","outputId":"08d2fedd-f8ed-42f8-fc5d-ec4ae559a5ef","execution":{"iopub.status.busy":"2021-05-30T19:59:21.468039Z","iopub.execute_input":"2021-05-30T19:59:21.468444Z","iopub.status.idle":"2021-05-30T19:59:21.491899Z","shell.execute_reply.started":"2021-05-30T19:59:21.468412Z","shell.execute_reply":"2021-05-30T19:59:21.490957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classification variables can divided into 3 groups:\n\n1) Yes/No classification variables (Partner, dependent, ...etc) \n\n2) Yes/No/Other classification variables (StreamingMovies, TechSupport, ..etc)\n\n3) Classification variables with other values (gender, contract, etc.) \n","metadata":{}},{"cell_type":"code","source":"# we convert SeniorCitizen to Yes/No in order to plot it with the other Yes/No columns \ndf['SeniorCitizen'] = df['SeniorCitizen'].map({0:'No',1:'Yes'}) \n\n# For these columns we expect yes/no values only \ncols1 = ['Churn', 'SeniorCitizen', 'Partner', 'Dependents', 'PaperlessBilling', 'PhoneService']  \n\n# For these columns we expect yes/no or a different value such as 'no internet service', or 'special package'\ncols2 = ['StreamingMovies','StreamingTV','TechSupport','OnlineBackup','OnlineSecurity', \n         'MultipleLines', 'DeviceProtection']\n\n# For these columns, we expect values other than yes/no\ncols3 = ['gender','Contract','InternetService','PaymentMethod']","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:21.493278Z","iopub.execute_input":"2021-05-30T19:59:21.493558Z","iopub.status.idle":"2021-05-30T19:59:21.501095Z","shell.execute_reply.started":"2021-05-30T19:59:21.49353Z","shell.execute_reply":"2021-05-30T19:59:21.499972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_chart(cols,fz=(12,10), rot=0):\n    fig, axes = plt.subplots(nrows=2, ncols=(len(cols)+1)//2, figsize=fz)\n    for i, col in enumerate(cols):\n        sns.countplot(x=col, data=df, ax=axes[i%2,i//2], order=df[col].value_counts().index)\n        axes[i%2,i//2].set_title(col)\n        axes[i%2,i//2].set_xlabel(None)\n        axes[i%2,i//2].set_ylabel(None)\n        xlabels = axes[i%2,i//2].get_xticklabels()\n        axes[i%2,i//2].set_xticklabels(xlabels, rotation=rot)\n \n    for i in range(len(cols), len(axes.flatten()) ):\n        fig.delaxes(axes.flatten()[i])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:21.503782Z","iopub.execute_input":"2021-05-30T19:59:21.504123Z","iopub.status.idle":"2021-05-30T19:59:21.51821Z","shell.execute_reply.started":"2021-05-30T19:59:21.504057Z","shell.execute_reply":"2021-05-30T19:59:21.517113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the first type - accept only Yes/No\nplot_chart(cols1)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:21.520261Z","iopub.execute_input":"2021-05-30T19:59:21.520639Z","iopub.status.idle":"2021-05-30T19:59:22.273339Z","shell.execute_reply.started":"2021-05-30T19:59:21.520583Z","shell.execute_reply":"2021-05-30T19:59:22.272261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot bar charts \nplot_chart(cols2,(18,10))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:22.276035Z","iopub.execute_input":"2021-05-30T19:59:22.276511Z","iopub.status.idle":"2021-05-30T19:59:23.063421Z","shell.execute_reply.started":"2021-05-30T19:59:22.276466Z","shell.execute_reply":"2021-05-30T19:59:23.062189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot bar charts \nplot_chart(cols3,(10,10),45)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:23.065174Z","iopub.execute_input":"2021-05-30T19:59:23.065625Z","iopub.status.idle":"2021-05-30T19:59:23.54523Z","shell.execute_reply.started":"2021-05-30T19:59:23.06557Z","shell.execute_reply":"2021-05-30T19:59:23.544156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TotalCharges is showing blank when the balance is Null so will replace it to '0' then convert the column to float \ndf[df['TotalCharges'] == ' '].shape","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:23.54688Z","iopub.execute_input":"2021-05-30T19:59:23.54731Z","iopub.status.idle":"2021-05-30T19:59:23.558449Z","shell.execute_reply.started":"2021-05-30T19:59:23.547265Z","shell.execute_reply":"2021-05-30T19:59:23.557236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert empty values to 0 and convert TotalCharges to Numeric\ndf['TotalCharges'] = df['TotalCharges'].str.replace (' ','0')\ndf['TotalCharges'] = df['TotalCharges'].astype(float)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:23.560124Z","iopub.execute_input":"2021-05-30T19:59:23.560569Z","iopub.status.idle":"2021-05-30T19:59:23.576561Z","shell.execute_reply.started":"2021-05-30T19:59:23.560525Z","shell.execute_reply":"2021-05-30T19:59:23.575549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:23.577774Z","iopub.execute_input":"2021-05-30T19:59:23.57828Z","iopub.status.idle":"2021-05-30T19:59:23.604535Z","shell.execute_reply.started":"2021-05-30T19:59:23.578238Z","shell.execute_reply":"2021-05-30T19:59:23.60346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we look at the numeric values \nncols = ['tenure', 'MonthlyCharges', 'TotalCharges']\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,4))\nfor i, col in enumerate(ncols): \n    sns.histplot(df[col], kde=True, ax=axes[i])","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:23.60581Z","iopub.execute_input":"2021-05-30T19:59:23.606137Z","iopub.status.idle":"2021-05-30T19:59:24.307522Z","shell.execute_reply.started":"2021-05-30T19:59:23.6061Z","shell.execute_reply":"2021-05-30T19:59:24.306401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Q1: Explain the process that needs to happen for each feature before you train your model. Also, think about how future observations might be different from the ones you have! Be creative.*","metadata":{}},{"cell_type":"markdown","source":"We need to convert all the classification variables to numbers to be used in the prediction models. This can be done by assigning numerical values for each of the categorical feature we have in the dataset. \n\n1) Yes/No classification variables - cannot accept values other than Yes/No.\n\n2) Yes/No/'Others' classification variables - can accept values other than Yes/No\n\n3) Classification variables with other values (gender, contract, etc.) - can accept any value\n","metadata":{}},{"cell_type":"code","source":"#df= df1.copy()\n\ndef mapping_dict(col, yes_no=True):\n    md = {}\n    if (yes_no == True):\n        md = {'No': 0, 'Yes': 1}\n    val = col.unique()\n    if len(md)==0:\n        cnt=0\n    else:\n        cnt=max(md.values())+1\n    for i in val:\n        if not(i in md.keys()):\n            md[i] = cnt\n            cnt+=1\n    return md","metadata":{"id":"jMX0nnS7yn68","execution":{"iopub.status.busy":"2021-05-30T19:59:24.309182Z","iopub.execute_input":"2021-05-30T19:59:24.309579Z","iopub.status.idle":"2021-05-30T19:59:24.318289Z","shell.execute_reply.started":"2021-05-30T19:59:24.309537Z","shell.execute_reply":"2021-05-30T19:59:24.31704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For first group : convert category values to numbers \nfor col in cols1:\n    md = mapping_dict(df[col])\n    df[col] = df[col].map(md)\n\n    # for any value other than Yes/No, identify these values, flag and remove them \n    lst = [x for x in md.keys() if x not in ['Yes','No']]\n    if (len(lst)>0):\n        print ('The following values in',col,'cannot be accepted and needs to be revised : ', lst)\n        print (df[df[col]>1].shape[0], 'rows removed from the dataset' )\n        df = df[df[col]<=1]\n","metadata":{"id":"LDwSeqwIyn68","outputId":"f3f02766-130e-427a-b4db-81388ec63b07","execution":{"iopub.status.busy":"2021-05-30T19:59:24.319729Z","iopub.execute_input":"2021-05-30T19:59:24.320057Z","iopub.status.idle":"2021-05-30T19:59:24.356164Z","shell.execute_reply.started":"2021-05-30T19:59:24.320027Z","shell.execute_reply":"2021-05-30T19:59:24.355146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for second group : convert category values to numbers and accept any value for these columns\nfor col in cols2:\n    md = mapping_dict(df[col])\n    df[col] = df[col].map(md)","metadata":{"id":"QTzZQXAJ4Ski","outputId":"047a0aed-9b09-440c-e75a-4e7675b1a14b","execution":{"iopub.status.busy":"2021-05-30T19:59:24.357758Z","iopub.execute_input":"2021-05-30T19:59:24.358181Z","iopub.status.idle":"2021-05-30T19:59:24.384781Z","shell.execute_reply.started":"2021-05-30T19:59:24.358136Z","shell.execute_reply":"2021-05-30T19:59:24.383728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We use patterns to identify males & females based on first letter - covert it to Male / Female  \nprint (df['gender'].value_counts())\ndf.loc[df['gender'].str.contains(r'^[Mm]'),'gender']='Male'\ndf.loc[df['gender'].str.contains(r'^[Ff]'),'gender']='Female'\n\n# for any value other than Male/Female, itdentify these values, flag it and remove them \nlst = [x for x in list(df['gender'].unique()) if x not in ['Male','Female']]\nif (len(lst)>0):\n    print ('The following values in gender to be revised : ', lst)\n    print (df[~df['gender'].isin(['Male','Female'])].shape[0], 'rows removed from the dataset' )\n    df = df[df['gender'].isin(['Male','Female'])]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:24.386092Z","iopub.execute_input":"2021-05-30T19:59:24.386398Z","iopub.status.idle":"2021-05-30T19:59:24.414163Z","shell.execute_reply.started":"2021-05-30T19:59:24.386367Z","shell.execute_reply":"2021-05-30T19:59:24.41304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for third group : convert category values to numbers and accept any value for these columns\nfor col in cols3:\n    md = mapping_dict(df[col], yes_no=False)\n    df[col] = df[col].map(md)\n\ndf['gender'].value_counts()    ","metadata":{"id":"vMRhOYTI4WCa","outputId":"381f4ba0-905b-4250-d434-a9d25b11cda8","execution":{"iopub.status.busy":"2021-05-30T19:59:24.415402Z","iopub.execute_input":"2021-05-30T19:59:24.415674Z","iopub.status.idle":"2021-05-30T19:59:24.435195Z","shell.execute_reply.started":"2021-05-30T19:59:24.415648Z","shell.execute_reply":"2021-05-30T19:59:24.433924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols:\n    print (col,': before -', list(df1[col].value_counts()), '&  after -', list(df[col].value_counts()))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:24.436474Z","iopub.execute_input":"2021-05-30T19:59:24.436761Z","iopub.status.idle":"2021-05-30T19:59:24.50653Z","shell.execute_reply.started":"2021-05-30T19:59:24.436733Z","shell.execute_reply":"2021-05-30T19:59:24.505566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize Numeric Value\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,4))\nfor i, col in enumerate(ncols): \n    df[col] = (df[col] - df[col].mean()) / (df[col].std())\n    sns.histplot(df[col], kde=True, ax=axes[i])","metadata":{"id":"TreDDvpAyn6-","outputId":"876e5186-6c30-450c-a477-85578495a9af","execution":{"iopub.status.busy":"2021-05-30T19:59:24.508876Z","iopub.execute_input":"2021-05-30T19:59:24.509228Z","iopub.status.idle":"2021-05-30T19:59:25.186208Z","shell.execute_reply.started":"2021-05-30T19:59:24.509197Z","shell.execute_reply":"2021-05-30T19:59:25.185222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# customerID is not required for the prediction model\ndf = df.drop(['customerID'],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:25.192417Z","iopub.execute_input":"2021-05-30T19:59:25.192766Z","iopub.status.idle":"2021-05-30T19:59:25.201237Z","shell.execute_reply.started":"2021-05-30T19:59:25.192734Z","shell.execute_reply":"2021-05-30T19:59:25.199845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"ULIy3tfGyn6_","outputId":"ac3a1abe-5edb-4c12-bcc2-14ef4f87b558","execution":{"iopub.status.busy":"2021-05-30T19:59:25.203823Z","iopub.execute_input":"2021-05-30T19:59:25.204189Z","iopub.status.idle":"2021-05-30T19:59:25.221296Z","shell.execute_reply.started":"2021-05-30T19:59:25.204153Z","shell.execute_reply":"2021-05-30T19:59:25.219918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T19:59:25.222678Z","iopub.execute_input":"2021-05-30T19:59:25.223019Z","iopub.status.idle":"2021-05-30T19:59:25.242763Z","shell.execute_reply.started":"2021-05-30T19:59:25.222984Z","shell.execute_reply":"2021-05-30T19:59:25.241371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.heatmap(df.corr())","metadata":{"id":"rM45Ak9Zyn7D","outputId":"ff9899fa-34a2-49b8-e0f7-ffab686ed71a","execution":{"iopub.status.busy":"2021-05-30T19:59:25.244389Z","iopub.execute_input":"2021-05-30T19:59:25.244916Z","iopub.status.idle":"2021-05-30T19:59:25.954936Z","shell.execute_reply.started":"2021-05-30T19:59:25.244867Z","shell.execute_reply":"2021-05-30T19:59:25.954202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the heatmap above, judging from the features with significant correlation coefficients of higher than 0.8, customer with internet access will normally have other online services as well such as online security, backup, tech support, streaming movies, etc.","metadata":{}},{"cell_type":"markdown","source":"*Q2: Choose one metric to evaluate the different models you will train and explain why you are choosing that instead of other metrics. You can try a few base models but model performance is not of prime importance yet.*","metadata":{}},{"cell_type":"markdown","source":"Precision measures how precise/accurate the model is out of those predicted positive, how many of them are actual positive. Precision is a good measure when the costs of False Positive is high.\n\nWhile recall calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.\n\n\nIn our model, the cost of not identifying the clients who will churn is the lost revenue after losing them. While the cost of flagging customers who are not planning to leave incorrectly could be calling them to check on their satisfaction level and perphaps offering them some incetives to keep their services\n\nAssuming that the cost of losing clients is higher, we will use recall to evaluate our model. ","metadata":{}},{"cell_type":"code","source":"# Def X and Y\ny = df['Churn']\nX = df.drop('Churn', axis=1)","metadata":{"id":"xPGRE0Q1yn7E","execution":{"iopub.status.busy":"2021-05-30T19:59:25.955837Z","iopub.execute_input":"2021-05-30T19:59:25.956103Z","iopub.status.idle":"2021-05-30T19:59:25.965332Z","shell.execute_reply.started":"2021-05-30T19:59:25.956075Z","shell.execute_reply":"2021-05-30T19:59:25.964227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset to train and test the model \nX_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nprint (X_train.shape)\nprint (X_test.shape)","metadata":{"id":"Af8xVrxzyn7F","outputId":"0bbccba3-4cca-49ce-d950-b801c3b995ec","execution":{"iopub.status.busy":"2021-05-30T19:59:25.966757Z","iopub.execute_input":"2021-05-30T19:59:25.967092Z","iopub.status.idle":"2021-05-30T19:59:25.98338Z","shell.execute_reply.started":"2021-05-30T19:59:25.967024Z","shell.execute_reply":"2021-05-30T19:59:25.982414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Using K Neighbors Classifier","metadata":{}},{"cell_type":"code","source":"# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n         ('k_neighbors', KNeighborsClassifier())]\n\n# Create the pipeline: pipeline \npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'k_neighbors__n_neighbors':np.arange(5,50)}\n\n# Create the GridSearchCV object: knn\nknn_cv = GridSearchCV(pipeline,param_grid=parameters,cv=5)\n\n# Fit to the training set\nknn_cv.fit(X_train,y_train)\n\n# Compute and print the metrics\nprint('Best Score: %s' % knn_cv.best_score_)\nprint('Best Hyperparameters: %s' % knn_cv.best_params_)","metadata":{"id":"h-hwwYsVyn7I","execution":{"iopub.status.busy":"2021-05-30T19:59:25.984788Z","iopub.execute_input":"2021-05-30T19:59:25.98515Z","iopub.status.idle":"2021-05-30T20:00:10.145465Z","shell.execute_reply.started":"2021-05-30T19:59:25.985115Z","shell.execute_reply":"2021-05-30T20:00:10.144356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the mean_test_score values (after 5-fold CV) versus k_neighbors from 5 to 50 curve\nx1 = np.arange(5,50)\ny1 = knn_cv.cv_results_['mean_test_score']\nplt.plot(x1,y1)\nplt.xlabel('K_neighbors_value')\nplt.ylabel('Mean Test Score')\nplt.title('Mean Test Score vs. K_Neighbors value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:10.146853Z","iopub.execute_input":"2021-05-30T20:00:10.147216Z","iopub.status.idle":"2021-05-30T20:00:10.289867Z","shell.execute_reply.started":"2021-05-30T20:00:10.147182Z","shell.execute_reply":"2021-05-30T20:00:10.288815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the model using the best paramter\n#knn = KNeighborsClassifier(n_neighbors=knn_cv.best_params_['k_neighbors__n_neighbors'])\nknn = knn_cv.best_estimator_\n\n#Make the prediction:\ny_pred1 = knn.predict (X_test)\n\n#Classification report\nprint (classification_report(y_test, y_pred1))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:10.291405Z","iopub.execute_input":"2021-05-30T20:00:10.291805Z","iopub.status.idle":"2021-05-30T20:00:10.563196Z","shell.execute_reply.started":"2021-05-30T20:00:10.291763Z","shell.execute_reply":"2021-05-30T20:00:10.561935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob1 = knn.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_prob1)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr1, tpr1)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of KNN Model')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:10.565838Z","iopub.execute_input":"2021-05-30T20:00:10.566213Z","iopub.status.idle":"2021-05-30T20:00:10.913203Z","shell.execute_reply.started":"2021-05-30T20:00:10.56618Z","shell.execute_reply":"2021-05-30T20:00:10.911918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred1)\nprint (cm)\nax = plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:10.914578Z","iopub.execute_input":"2021-05-30T20:00:10.914954Z","iopub.status.idle":"2021-05-30T20:00:11.168624Z","shell.execute_reply.started":"2021-05-30T20:00:10.914922Z","shell.execute_reply":"2021-05-30T20:00:11.167438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since our objective is to imporve the recall ratio - we will change the threshold to see the impact \ny.value_counts()/len(y)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:11.169867Z","iopub.execute_input":"2021-05-30T20:00:11.170169Z","iopub.status.idle":"2021-05-30T20:00:11.178353Z","shell.execute_reply.started":"2021-05-30T20:00:11.17014Z","shell.execute_reply":"2021-05-30T20:00:11.177221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assuming the threshold of 0.26\ny_pred_new1 = np.where(y_pred_prob1 >=0.26, 1, 0)\n\ncm1 = confusion_matrix(y_test, y_pred_new1)\nax = plt.subplot()\nsns.heatmap(cm1, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:11.179624Z","iopub.execute_input":"2021-05-30T20:00:11.179927Z","iopub.status.idle":"2021-05-30T20:00:11.424686Z","shell.execute_reply.started":"2021-05-30T20:00:11.179896Z","shell.execute_reply":"2021-05-30T20:00:11.423669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We improved the recall ratio but the percision is lower \nprint(classification_report(y_test, y_pred_new1))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:11.426039Z","iopub.execute_input":"2021-05-30T20:00:11.426465Z","iopub.status.idle":"2021-05-30T20:00:11.440617Z","shell.execute_reply.started":"2021-05-30T20:00:11.426432Z","shell.execute_reply":"2021-05-30T20:00:11.439441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Using Logistic Regression ","metadata":{}},{"cell_type":"code","source":"# Setup the pipeline steps: steps\nsteps2 = [('scaler', StandardScaler()),\n         ('logreg', LogisticRegression())]\n\n# Create the pipeline: pipeline \npipeline2 = Pipeline(steps2)\n\n# Specify the hyperparameter space\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'logreg__C': c_space}\n\n# Create the GridSearchCV object: knn2\nlogreg_cv = GridSearchCV(pipeline2,param_grid,cv=5)\n\n# Fit to the training set\nlogreg_cv.fit(X_train,y_train)\n\n# Compute and print the metrics\nprint('Best Score: %s' % logreg_cv.best_score_)\nprint('Best Hyperparameters: %s' % logreg_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:11.441995Z","iopub.execute_input":"2021-05-30T20:00:11.442351Z","iopub.status.idle":"2021-05-30T20:00:15.349162Z","shell.execute_reply.started":"2021-05-30T20:00:11.442317Z","shell.execute_reply":"2021-05-30T20:00:15.348098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logreg = LogisticRegression(C=logreg_cv.best_params_['logreg__C'])\nlogreg = logreg_cv.best_estimator_\n\n#Make the prediction:\ny_pred2 = logreg.predict (X_test)\n\n#Classification report\nprint (classification_report(y_test, y_pred2))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:15.350807Z","iopub.execute_input":"2021-05-30T20:00:15.351553Z","iopub.status.idle":"2021-05-30T20:00:15.372787Z","shell.execute_reply.started":"2021-05-30T20:00:15.351502Z","shell.execute_reply":"2021-05-30T20:00:15.371702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob2 = logreg_cv.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_prob2)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr2, tpr2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of Logistics Regression')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:15.374523Z","iopub.execute_input":"2021-05-30T20:00:15.375252Z","iopub.status.idle":"2021-05-30T20:00:15.577088Z","shell.execute_reply.started":"2021-05-30T20:00:15.375199Z","shell.execute_reply":"2021-05-30T20:00:15.576038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred2)\n\nax = plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:15.578281Z","iopub.execute_input":"2021-05-30T20:00:15.578581Z","iopub.status.idle":"2021-05-30T20:00:15.825041Z","shell.execute_reply.started":"2021-05-30T20:00:15.578554Z","shell.execute_reply":"2021-05-30T20:00:15.82405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_new2 = np.where(y_pred_prob2 >=0.26, 1, 0)\n\ncm1 = confusion_matrix(y_test, y_pred_new2)\nax = plt.subplot()\nsns.heatmap(cm1, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-30T20:00:15.826263Z","iopub.execute_input":"2021-05-30T20:00:15.826565Z","iopub.status.idle":"2021-05-30T20:00:16.070413Z","shell.execute_reply.started":"2021-05-30T20:00:15.826534Z","shell.execute_reply":"2021-05-30T20:00:16.069706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We improved the recall ratio but the percision is lower \nprint(classification_report(y_test, y_pred_new2))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:16.071366Z","iopub.execute_input":"2021-05-30T20:00:16.071741Z","iopub.status.idle":"2021-05-30T20:00:16.083346Z","shell.execute_reply.started":"2021-05-30T20:00:16.071712Z","shell.execute_reply":"2021-05-30T20:00:16.082343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## M6W2 Assignment","metadata":{}},{"cell_type":"markdown","source":"*Q0: Select three different models that you would like to test your dataset with. Make sure that at least two of them are tree-based models.*\n\n*Q1: Explain why you selected these three models. You might want to discuss their peformance, explainability, complexity, etc.*","metadata":{}},{"cell_type":"markdown","source":"We will start with a simple model which is the Decision Tree Classifier then we will use Random Forest Classifer and Gradient Boosting Classifier.\n\nDecision Tree Classifier simple to understand, easy to explain and provides a clear visual to guide the decision making process. It has some disadvantages including overfitting, error due to bias and error due to variance. \n\nRandom forest is an ensemble model uses a collection of decision trees with a single, aggregated result. Random forests are considered one of the most accurate learning algorithm. It reduces the variance seen in decision trees by using different samples for training, specifying random feature subsets and building & combining small trees.\n\nGradient boosting is another ensemble model that uses a set of decision trees. The two main differences between Random Forest and Gradient Boosting are:\n\n- How trees are built: random forests builds each tree independently while gradient boosting builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners. \n- Combining results: random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way.\n\n","metadata":{}},{"cell_type":"markdown","source":"### 1. First Model : Decision Tree Classifier","metadata":{}},{"cell_type":"markdown","source":"Decision trees are a series of sequential steps designed to answer a question and provide probabilities, costs, or other consequence of making a particular decision. \n\nDecision tree is derived from the independent variables, with each node having a condition over a feature. The nodes decides which node to navigate next based on the condition. Once the leaf node is reached, an output is predicted. The right sequence of conditions makes the tree efficient. Information gain are used as the criteria to select the conditions in nodes. \n\nThey are simple to understand, providing a clear visual to guide the decision making progress. However, this simplicity comes with a few serious disadvantages, including overfitting, error due to bias and error due to variance.  \n\n- Overfitting happens for many reasons, including presence of noise and lack of representative instances. It's possible for overfitting with one large (deep) tree. \n- Bias error happens when you place too many restrictions on target functions. For example, restricting your result with a restricting function (e.g. a linear equation) or by a simple binary algorithm (like the true/false choices in the above tree) will often result in bias.\n- Variance error refers to how much a result will change based on changes to the training set. Decision trees have high variance, which means that tiny changes in the training data have the potential to cause large changes in the final result.\n\n**Advantages :**\n- No preprocessing needed on data.\n- No assumptions on distribution of data.\n- Handles colinearity efficiently.\n- Decision trees can provide understandable explanation over the prediction.\n\n**Disadvantages :**\n- Chances for overfitting the model if we keep on building the tree to achieve high purity. decision tree pruning can be used to solve this issue.\n- Prone to outliers.\n- Tree may grow to be very complex while training complicated datasets.\n- Looses valuable information while handling continuous variables.\n\n**Decision tree vs KNN :**\n- Both are non-parametric methods.\n- Decision tree supports automatic feature interaction, whereas KNN cant.\n- Decision tree is faster due to KNN’s expensive real time execution.\n\n**Decision Tree vs Logistic Regression :**\n- Decision tree handles colinearity better than LR.\n- Decision trees cannot derive the significance of features, but LR can.\n- Decision trees are better for categorical values than LR.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Specify the hyperparameters\nparam_grid = {'max_depth': range(1,10)}#, 'min_samples_leaf':[1,2,3]}\n\n# Create the DecisionTreeClassifier : dt\ndt = DecisionTreeClassifier()\n\n# Create the GridSearchCV object: dt_cv\ndt_cv = GridSearchCV(dt,param_grid,cv=5,return_train_score = True)\n\n# Fit to the training set\ndt_cv.fit(X_train,y_train)\n\n# Compute and print the metrics\nprint('Best Score: %s' % dt_cv.best_score_)\nprint('Best Hyperparameters: %s' % dt_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:16.08467Z","iopub.execute_input":"2021-05-30T20:00:16.085196Z","iopub.status.idle":"2021-05-30T20:00:16.963551Z","shell.execute_reply.started":"2021-05-30T20:00:16.085158Z","shell.execute_reply":"2021-05-30T20:00:16.96256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = dt_cv.best_estimator_\n\n#Make the prediction:\ny_pred3 = dt.predict (X_test)\n\n#Classification report\nprint (classification_report(y_test, y_pred3))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:16.964866Z","iopub.execute_input":"2021-05-30T20:00:16.965207Z","iopub.status.idle":"2021-05-30T20:00:16.980885Z","shell.execute_reply.started":"2021-05-30T20:00:16.965175Z","shell.execute_reply":"2021-05-30T20:00:16.979777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob3 = dt_cv.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_prob3)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr3, tpr3)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of Decision Tree')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:16.982381Z","iopub.execute_input":"2021-05-30T20:00:16.982802Z","iopub.status.idle":"2021-05-30T20:00:17.135831Z","shell.execute_reply.started":"2021-05-30T20:00:16.982756Z","shell.execute_reply":"2021-05-30T20:00:17.134626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred3)\n\nax = plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:17.137439Z","iopub.execute_input":"2021-05-30T20:00:17.137861Z","iopub.status.idle":"2021-05-30T20:00:17.382659Z","shell.execute_reply.started":"2021-05-30T20:00:17.137816Z","shell.execute_reply":"2021-05-30T20:00:17.381646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_new3 = np.where(y_pred_prob3 >=0.26, 1, 0)\n\ncm1 = confusion_matrix(y_test, y_pred_new3)\nax = plt.subplot()\nsns.heatmap(cm1, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:17.383627Z","iopub.execute_input":"2021-05-30T20:00:17.38389Z","iopub.status.idle":"2021-05-30T20:00:17.615738Z","shell.execute_reply.started":"2021-05-30T20:00:17.383863Z","shell.execute_reply":"2021-05-30T20:00:17.615092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We improved the recall ratio but the percision is lower \nprint(classification_report(y_test, y_pred_new3))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:17.616668Z","iopub.execute_input":"2021-05-30T20:00:17.617057Z","iopub.status.idle":"2021-05-30T20:00:17.626848Z","shell.execute_reply.started":"2021-05-30T20:00:17.617028Z","shell.execute_reply":"2021-05-30T20:00:17.6262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.Series(dt.feature_importances_, index=X.columns)\nprint (data)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:17.62791Z","iopub.execute_input":"2021-05-30T20:00:17.628259Z","iopub.status.idle":"2021-05-30T20:00:17.63483Z","shell.execute_reply.started":"2021-05-30T20:00:17.628228Z","shell.execute_reply":"2021-05-30T20:00:17.633924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" data.sort_values(ascending=True).plot.barh(figsize=(8,6))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:17.635951Z","iopub.execute_input":"2021-05-30T20:00:17.636221Z","iopub.status.idle":"2021-05-30T20:00:17.899608Z","shell.execute_reply.started":"2021-05-30T20:00:17.636196Z","shell.execute_reply":"2021-05-30T20:00:17.898538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_tree","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:17.901248Z","iopub.execute_input":"2021-05-30T20:00:17.901623Z","iopub.status.idle":"2021-05-30T20:00:17.908099Z","shell.execute_reply.started":"2021-05-30T20:00:17.901588Z","shell.execute_reply":"2021-05-30T20:00:17.90692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(dt , filled=True, max_depth=3)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:17.909737Z","iopub.execute_input":"2021-05-30T20:00:17.910186Z","iopub.status.idle":"2021-05-30T20:00:19.621033Z","shell.execute_reply.started":"2021-05-30T20:00:17.910141Z","shell.execute_reply":"2021-05-30T20:00:19.619891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Second Model : Random Forest Classifier","metadata":{}},{"cell_type":"markdown","source":"Random Forest is a collection of decision trees with a single, aggregated result. It is an ensemble model that uses a set of decision trees ensembled with “bagging method” to obtain classification and regression outputs. In classification, it calculates the output using majority voting , whereas in regression, mean is calculated. The derived model will be more robust, accurate and handles overfitting better than constituent models\n\nRandom forests reduce the variance seen in decision trees by:\n\n- Using different samples for training\n- Specifying random feature subsets\n- Building and combining small (shallow) trees\n\n**Advantages :**\n- Accurate and powerful model.\n- handles overfitting efficiently.\n- Supports implicit feature selection and derives feature importance.\n\n**Disadvantages :**\n- computationally complex and slower when forest becomes large.\n- Not a well descriptive model over the prediction.\n\n**Decision tree vs Random Forest :**\n- Random Forest is a collection of decision trees and average/majority vote of the forest is selected as the predicted output.\n- Random Forest model will be less prone to overfitting than Decision tree, and gives a more generalized solution.\n- Random Forest is more robust and accurate than decision trees.","metadata":{}},{"cell_type":"code","source":"# Specify the hyperparameters\nparam_grid = {'max_depth': range(1,10)}#, 'min_samples_leaf':[1,2,3]}\n\n# Create the RandomForestClassifier : rf\nrf = RandomForestClassifier()\n\n# Create the GridSearchCV object: rf_cv\nrf_cv = GridSearchCV(rf,param_grid,cv=5,return_train_score = True)\n\n# Fit to the training set\nrf_cv.fit(X_train,y_train)\n\n# Compute and print the metrics\nprint('Best Score: %s' % rf_cv.best_score_)\nprint('Best Hyperparameters: %s' % rf_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:19.622325Z","iopub.execute_input":"2021-05-30T20:00:19.622638Z","iopub.status.idle":"2021-05-30T20:00:39.742349Z","shell.execute_reply.started":"2021-05-30T20:00:19.622603Z","shell.execute_reply":"2021-05-30T20:00:39.741337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = rf_cv.best_estimator_\n\n#Make the prediction:\ny_pred4 = rf.predict (X_test)\n\n#Classification report\nprint (classification_report(y_test, y_pred4))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:39.743917Z","iopub.execute_input":"2021-05-30T20:00:39.744376Z","iopub.status.idle":"2021-05-30T20:00:39.790598Z","shell.execute_reply.started":"2021-05-30T20:00:39.744329Z","shell.execute_reply":"2021-05-30T20:00:39.789567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob4 = rf_cv.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr4, tpr4, thresholds4 = roc_curve(y_test, y_pred_prob4)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr4, tpr4)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of Random Forest ')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:39.79218Z","iopub.execute_input":"2021-05-30T20:00:39.792591Z","iopub.status.idle":"2021-05-30T20:00:40.11893Z","shell.execute_reply.started":"2021-05-30T20:00:39.792546Z","shell.execute_reply":"2021-05-30T20:00:40.118146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred4)\n\nax = plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:40.120043Z","iopub.execute_input":"2021-05-30T20:00:40.120581Z","iopub.status.idle":"2021-05-30T20:00:40.384632Z","shell.execute_reply.started":"2021-05-30T20:00:40.120532Z","shell.execute_reply":"2021-05-30T20:00:40.383641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_new4 = np.where(y_pred_prob4 >=0.26, 1, 0)\n\ncm1 = confusion_matrix(y_test, y_pred_new4)\nax = plt.subplot()\nsns.heatmap(cm1, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:40.386167Z","iopub.execute_input":"2021-05-30T20:00:40.386466Z","iopub.status.idle":"2021-05-30T20:00:40.588643Z","shell.execute_reply.started":"2021-05-30T20:00:40.386437Z","shell.execute_reply":"2021-05-30T20:00:40.587592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.Series(rf.feature_importances_, index=X.columns)\nprint (data)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:40.591201Z","iopub.execute_input":"2021-05-30T20:00:40.591647Z","iopub.status.idle":"2021-05-30T20:00:40.612235Z","shell.execute_reply.started":"2021-05-30T20:00:40.5916Z","shell.execute_reply":"2021-05-30T20:00:40.611137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sort_values(ascending=True).plot.barh(figsize=(8,6))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:40.613673Z","iopub.execute_input":"2021-05-30T20:00:40.614116Z","iopub.status.idle":"2021-05-30T20:00:40.880311Z","shell.execute_reply.started":"2021-05-30T20:00:40.614043Z","shell.execute_reply":"2021-05-30T20:00:40.879171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Third Model : Gradient Boosting Classifier","metadata":{}},{"cell_type":"markdown","source":"Gradient boosting is a also a set of decision trees. The two main differences are:\n\n- How trees are built: random forests builds each tree independently while gradient boosting builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners. \n- Combining results: random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way.\n\nBy carefully tune parameters, gradient boosting can result in better performance than random forests. However, gradient boosting may not be a good choice if we have a lot of noise, as it can result in overfitting. They also tend to be harder to tune than random forests.\n\nRandom forests and gradient boosting each excel in different areas. Random forests perform well for multi-class object detection and bioinformatics, which tends to have a lot of statistical noise. Gradient Boosting performs well when you have unbalanced data such as in real time risk assessment.\n\n\n**Advantages :**\n- Since boosted trees are derived by optimizing an objective function, basically GBM can be used to solve almost all objective function that we can write gradient out. This including things like ranking and poission regression, which RF is harder to achieve.\n\n**Disadvatages :**\n- GBMs are more sensitive to overfitting if the data is noisy.\n- Training generally takes longer because of the fact that trees are built sequentially.\n- GBMs are harder to tune than RF. There are typically three parameters: number of trees, depth of trees and learning rate, and each tree built is generally shallow","metadata":{}},{"cell_type":"code","source":"# Specify the hyperparameters\nparam_grid = {'max_depth': range(1,10)}#, 'min_samples_leaf':range(1,10)}\n\n# Create the GradientBoostingClassifier : gb\ngb =  GradientBoostingClassifier()\n\n# Create the GridSearchCV object: gb_cv\ngb_cv = GridSearchCV(gb,param_grid,cv=5,return_train_score = True)\n\n# Fit to the training set\ngb_cv.fit(X_train,y_train)\n\n# Compute and print the metrics\nprint('Best Score: %s' % gb_cv.best_score_)\nprint('Best Hyperparameters: %s' % gb_cv.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:00:40.881825Z","iopub.execute_input":"2021-05-30T20:00:40.882221Z","iopub.status.idle":"2021-05-30T20:01:37.030655Z","shell.execute_reply.started":"2021-05-30T20:00:40.882182Z","shell.execute_reply":"2021-05-30T20:01:37.029264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb = gb_cv.best_estimator_\n\n#Make the prediction:\ny_pred5 = gb.predict (X_test)\n\n#Classification report\nprint (classification_report(y_test, y_pred5))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:37.032181Z","iopub.execute_input":"2021-05-30T20:01:37.03263Z","iopub.status.idle":"2021-05-30T20:01:37.050507Z","shell.execute_reply.started":"2021-05-30T20:01:37.032584Z","shell.execute_reply":"2021-05-30T20:01:37.049157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob5 = gb_cv.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr5, tpr5, thresholds5 = roc_curve(y_test, y_pred_prob5)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr5, tpr5)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of Gradient Boosting')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:37.051943Z","iopub.execute_input":"2021-05-30T20:01:37.052314Z","iopub.status.idle":"2021-05-30T20:01:37.216327Z","shell.execute_reply.started":"2021-05-30T20:01:37.05228Z","shell.execute_reply":"2021-05-30T20:01:37.214921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the confusion matrix\ncm = confusion_matrix(y_test, y_pred5)\n\nax = plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:37.217993Z","iopub.execute_input":"2021-05-30T20:01:37.218468Z","iopub.status.idle":"2021-05-30T20:01:37.456936Z","shell.execute_reply.started":"2021-05-30T20:01:37.218429Z","shell.execute_reply":"2021-05-30T20:01:37.45622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_new5 = np.where(y_pred_prob5 >=0.26, 1, 0)\n\ncm1 = confusion_matrix(y_test, y_pred_new5)\nax = plt.subplot()\nsns.heatmap(cm1, annot=True, fmt='g', ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:37.457974Z","iopub.execute_input":"2021-05-30T20:01:37.458379Z","iopub.status.idle":"2021-05-30T20:01:37.702474Z","shell.execute_reply.started":"2021-05-30T20:01:37.458344Z","shell.execute_reply":"2021-05-30T20:01:37.70161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.Series(gb.feature_importances_, index=X.columns)\nprint (data)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:37.703499Z","iopub.execute_input":"2021-05-30T20:01:37.703886Z","iopub.status.idle":"2021-05-30T20:01:37.711934Z","shell.execute_reply.started":"2021-05-30T20:01:37.703857Z","shell.execute_reply":"2021-05-30T20:01:37.710698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sort_values(ascending=True).plot.barh(figsize=(8,6))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:37.713384Z","iopub.execute_input":"2021-05-30T20:01:37.713888Z","iopub.status.idle":"2021-05-30T20:01:37.986011Z","shell.execute_reply.started":"2021-05-30T20:01:37.713835Z","shell.execute_reply":"2021-05-30T20:01:37.984704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot ROC curve to compare all models\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr1, tpr1, label='KNN')\nplt.plot(fpr2, tpr2, label='Logistics')\nplt.plot(fpr3, tpr3, label='Decision Tree')\nplt.plot(fpr4, tpr4, label='Random Forest')\nplt.plot(fpr5, tpr5, label='Gradient Boosting')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of All Models')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:37.987417Z","iopub.execute_input":"2021-05-30T20:01:37.987708Z","iopub.status.idle":"2021-05-30T20:01:38.198393Z","shell.execute_reply.started":"2021-05-30T20:01:37.98768Z","shell.execute_reply":"2021-05-30T20:01:38.197407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All models seem to perform very close to each other in terms of accuracy of the predictions when viewing from the ROC curve.","metadata":{}},{"cell_type":"markdown","source":"*Q3: Do you believe you have overfitting? Why?*","metadata":{}},{"cell_type":"code","source":"# overfitting\ntrain_score = dt_cv.cv_results_[\"mean_train_score\"]\ntest_score = dt_cv.cv_results_[\"mean_test_score\"]\n\nx = range (1,10)\nplt.plot(x,train_score)\nplt.plot(x,test_score)\nplt.axvline(dt_cv.best_params_['max_depth'], color='gray',linestyle=\"--\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:38.204055Z","iopub.execute_input":"2021-05-30T20:01:38.204409Z","iopub.status.idle":"2021-05-30T20:01:38.420179Z","shell.execute_reply.started":"2021-05-30T20:01:38.204368Z","shell.execute_reply":"2021-05-30T20:01:38.41911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# overfitting\ntrain_score = rf_cv.cv_results_[\"mean_train_score\"]\ntest_score = rf_cv.cv_results_[\"mean_test_score\"]\n\nx = range (1,10)\nplt.plot(x,train_score)\nplt.plot(x,test_score)\nplt.axvline(rf_cv.best_params_['max_depth'], color='gray',linestyle=\"--\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:38.421696Z","iopub.execute_input":"2021-05-30T20:01:38.42211Z","iopub.status.idle":"2021-05-30T20:01:38.628848Z","shell.execute_reply.started":"2021-05-30T20:01:38.422076Z","shell.execute_reply":"2021-05-30T20:01:38.627735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# overfitting\ntrain_score = gb_cv.cv_results_[\"mean_train_score\"]\ntest_score = gb_cv.cv_results_[\"mean_test_score\"]\n\nx = range (1,10)\nplt.plot(x,train_score)\nplt.plot(x,test_score)\nplt.axvline(gb_cv.best_params_['max_depth'], color='gray',linestyle=\"--\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:38.630242Z","iopub.execute_input":"2021-05-30T20:01:38.630542Z","iopub.status.idle":"2021-05-30T20:01:38.827955Z","shell.execute_reply.started":"2021-05-30T20:01:38.630514Z","shell.execute_reply":"2021-05-30T20:01:38.826799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the chart above, in our opinion, once the parameters used is higher than the best parameters set concluded from the GridSearchCV, each model is going to be overfitted to the training dataset - where the training accuracy increases while the accuracy of the predictions of the test dataset decreases or does not improve (for the two latter models).\n\nWe also can observe that out of the three models: The Decision Tree is the model which suffers the most from overfitting since the accuracy on the test dataset deteriorates significantly while the accuracy remains constant in the Random Forest and drops slighlty in the Gradient Boosting, when the hyperparameter is tuning higher to increase the accuracy on train dataset.\n\nThis tendancy towards overfitting on Decision Tree model is driven by the different issues compared to Random Forest and Gradient Boosting due to the fact that the two latter models are categorized as ensembled models so they can better generalize the prediction logic on unseen data.","metadata":{}},{"cell_type":"markdown","source":"Now, we would like to investigate on the overview of each model performance regarding the training accuracy and testset accuracy when used the best tuned parameters by plotting the accuracy scores.","metadata":{}},{"cell_type":"code","source":"training_accuracy = []\ntesting_accuracy = []\n\nmodels = {knn, logreg, dt, rf, gb}\n\n#Compute the train and test accuracy for each model\nfor model in models:\n    y_pred_train = model.predict(X_train)\n    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n    Y_pred_test = model.predict(X_test)\n    acc_score = accuracy_score(y_test,Y_pred_test)\n    testing_accuracy.append(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:38.829275Z","iopub.execute_input":"2021-05-30T20:01:38.829564Z","iopub.status.idle":"2021-05-30T20:01:40.229333Z","shell.execute_reply.started":"2021-05-30T20:01:38.829535Z","shell.execute_reply":"2021-05-30T20:01:40.228336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(training_accuracy)\nprint(testing_accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:40.230499Z","iopub.execute_input":"2021-05-30T20:01:40.230788Z","iopub.status.idle":"2021-05-30T20:01:40.237361Z","shell.execute_reply.started":"2021-05-30T20:01:40.230752Z","shell.execute_reply":"2021-05-30T20:01:40.236164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'models':[\"KNN\", \"Logistics Regression\", \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"], 'TrainAccuracy':training_accuracy, 'TestAccuracy':testing_accuracy})\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:40.238994Z","iopub.execute_input":"2021-05-30T20:01:40.239432Z","iopub.status.idle":"2021-05-30T20:01:40.261217Z","shell.execute_reply.started":"2021-05-30T20:01:40.239387Z","shell.execute_reply":"2021-05-30T20:01:40.26039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_melted = df.melt(id_vars='models')\ndf_melted","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:40.262568Z","iopub.execute_input":"2021-05-30T20:01:40.262874Z","iopub.status.idle":"2021-05-30T20:01:40.285413Z","shell.execute_reply.started":"2021-05-30T20:01:40.262845Z","shell.execute_reply":"2021-05-30T20:01:40.284189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(x='variable', y='value', hue='models', data=df_melted)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T20:01:40.286952Z","iopub.execute_input":"2021-05-30T20:01:40.287344Z","iopub.status.idle":"2021-05-30T20:01:40.526096Z","shell.execute_reply.started":"2021-05-30T20:01:40.287311Z","shell.execute_reply":"2021-05-30T20:01:40.525115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the chart above, we can conclude that three of the selected models so far in this assignment have the overfitting issues which are the KNN (the worst), Decision Tree and the Gradient Boosting where the test accuracy deteriorates compared to the training accuracy. The Random Forest Regressor model is quite effective in real world data where its accuracy slightly increases and by far the Logistics Regression performs the best where the accuracy increases steeply on the test dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}