{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom scipy.stats import zscore\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nimport pandas as pd\nimport io\nimport requests\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nimport warnings, gc\n# Any results you write to the current directory are saved as output.","execution_count":92,"outputs":[{"output_type":"stream","text":"['2016-soi-tax-stats', 'midterm']\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"939f13eccc00e6f55f3c0f850576f514772dc086"},"cell_type":"code","source":"def multi_merge(left,right,*args):\n    start = args[0]\n    for i in range(1,len(args)):\n        start = start.merge(args[i], how = 'left', left_on = left, right_on = right)\n    return start\n\ndef to_xy(df, target):\n    result = []\n    for x in df.columns:\n        if x != target:\n            result.append(x)\n    # find out the type of the target column.  Is it really this hard? :(\n    target_type = df[target].dtypes\n    target_type = target_type[0] if hasattr(\n        target_type, '__iter__') else target_type\n    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n    if target_type not in (np.int64, np.int32):\n        # Classification\n        dummies = pd.get_dummies(df[target])\n        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n    # Regression\n    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)","execution_count":93,"outputs":[]},{"metadata":{"_uuid":"e8593c0b7d297fdd8b178adca7a9a8c71cd1014a"},"cell_type":"markdown","source":"> **Data Clean Part**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/midterm/train.csv')\ndf_test = pd.read_csv('../input/midterm/test.csv')\ndf_detail = pd.read_csv('../input/2016-soi-tax-stats/16zpallagi.csv')\ndf_detail = df_detail.loc[(df_detail['zipcode'] != 0) & (df_detail['zipcode'] != 99999)].reset_index(drop=True)","execution_count":94,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8da805cfaa7ad18247d31dc1e578f99a182a369b"},"cell_type":"code","source":"import re\ncolumn_list = list(df_detail.columns)\nA_list = [column for column in column_list if re.match(r'^[A][0-9]{2,10}',column)]\nN_list = [column for column in column_list if re.match(r'^[N][0-9]{2,10}',column)]\nO_list = [column for column in column_list if (column not in A_list and column not in N_list)]","execution_count":95,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"special = []\nfor name in A_list:\n    if 'N' + name[1:] not in N_list:\n        special.append(name)\ndf_zipcode = df_detail.loc[:,'zipcode']\ndf_detail_pair = df_detail.iloc[:,18:].drop(special+['SCHF'],axis=1)\ndf_detail_pair = pd.concat([df_zipcode,df_detail_pair],axis = 1)\nfor name in N_list:\n    A_name = 'A' + name[1:]\n    df_detail_pair[A_name] = df_detail_pair[A_name]*df_detail_pair[name]","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_detail_pair = df_detail_pair.groupby('zipcode').sum()\nfor name in N_list:\n    A_name = 'A' + name[1:]\n    df_detail_pair[A_name] = df_detail_pair[A_name]/df_detail_pair[name]\ndf_feature_1 = df_detail_pair.loc[:,['A' + name[1:] for name in N_list]].fillna(0)","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_special_feature_1 = df_detail.loc[:,special+['zipcode','N1']]\ndf_special_feature_1 = df_special_feature_1.groupby(by='zipcode').apply(lambda x: pd.Series({'avg_agi':sum(x['N1']*x[special[0]])/sum(x['N1']),'avg_item_r':sum(x['N1']*x[special[1]])/sum(x['N1'])}))","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_special_feature_2 = df_detail.iloc[:,:18]\nfor column in list(df_special_feature_2.columns)[5:]:\n    df_special_feature_2[column] = df_special_feature_2[column]/df_special_feature_2['N1']\ndf_special_feature_2 = df_special_feature_2.fillna(0).drop(['STATEFIPS','STATE','agi_stub'],axis=1).groupby(by='zipcode').agg('mean')","execution_count":99,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full_detail = multi_merge('zipcode','zipcode',df_special_feature_2,df_special_feature_1,df_feature_1)\ndf_full_detail.head()","execution_count":100,"outputs":[{"output_type":"execute_result","execution_count":100,"data":{"text/plain":"                  N1     mars1     ...            A11901       A11902\nzipcode                            ...                               \n1001     1496.666667  0.434868     ...        747.731544  3429.103933\n1002     1581.666667  0.426821     ...       2165.102679  3328.300000\n1003       34.000000  0.400000     ...          0.000000   127.000000\n1005      395.000000  0.349221     ...        232.511111   942.876344\n1007     1263.333333  0.371875     ...       1088.131387  2862.419795\n\n[5 rows x 78 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N1</th>\n      <th>mars1</th>\n      <th>MARS2</th>\n      <th>MARS4</th>\n      <th>PREP</th>\n      <th>N2</th>\n      <th>NUMDEP</th>\n      <th>TOTAL_VITA</th>\n      <th>VITA</th>\n      <th>TCE</th>\n      <th>VITA_EIC</th>\n      <th>RAL</th>\n      <th>RAC</th>\n      <th>ELDERLY</th>\n      <th>avg_agi</th>\n      <th>avg_item_r</th>\n      <th>A02650</th>\n      <th>A00200</th>\n      <th>A00300</th>\n      <th>A00600</th>\n      <th>A00650</th>\n      <th>A00700</th>\n      <th>A00900</th>\n      <th>A01000</th>\n      <th>A01400</th>\n      <th>A01700</th>\n      <th>A02300</th>\n      <th>A02500</th>\n      <th>A26270</th>\n      <th>A02900</th>\n      <th>A03220</th>\n      <th>A03300</th>\n      <th>A03270</th>\n      <th>A03150</th>\n      <th>A03210</th>\n      <th>A03230</th>\n      <th>A03240</th>\n      <th>A04470</th>\n      <th>A17000</th>\n      <th>A18425</th>\n      <th>A18450</th>\n      <th>A18500</th>\n      <th>A18800</th>\n      <th>A18300</th>\n      <th>A19300</th>\n      <th>A19500</th>\n      <th>A19530</th>\n      <th>A19550</th>\n      <th>A19570</th>\n      <th>A19700</th>\n      <th>A20800</th>\n      <th>A04800</th>\n      <th>A05800</th>\n      <th>A09600</th>\n      <th>A05780</th>\n      <th>A07100</th>\n      <th>A07300</th>\n      <th>A07180</th>\n      <th>A07230</th>\n      <th>A07240</th>\n      <th>A07220</th>\n      <th>A07260</th>\n      <th>A09400</th>\n      <th>A85770</th>\n      <th>A85775</th>\n      <th>A09750</th>\n      <th>A10600</th>\n      <th>A59660</th>\n      <th>A59720</th>\n      <th>A11070</th>\n      <th>A10960</th>\n      <th>A11560</th>\n      <th>A06500</th>\n      <th>A10300</th>\n      <th>A85530</th>\n      <th>A85300</th>\n      <th>A11901</th>\n      <th>A11902</th>\n    </tr>\n    <tr>\n      <th>zipcode</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1001</th>\n      <td>1496.666667</td>\n      <td>0.434868</td>\n      <td>0.479333</td>\n      <td>0.076106</td>\n      <td>0.606140</td>\n      <td>1.977950</td>\n      <td>0.541338</td>\n      <td>0.012041</td>\n      <td>0.002372</td>\n      <td>0.010261</td>\n      <td>0.000000</td>\n      <td>0.002990</td>\n      <td>0.073244</td>\n      <td>0.299588</td>\n      <td>80337.743875</td>\n      <td>37501.525612</td>\n      <td>81330.952116</td>\n      <td>64046.144595</td>\n      <td>515.501639</td>\n      <td>935.430108</td>\n      <td>692.549133</td>\n      <td>294.103960</td>\n      <td>2734.028302</td>\n      <td>1053.503876</td>\n      <td>2653.745763</td>\n      <td>8489.688995</td>\n      <td>663.387755</td>\n      <td>3433.469799</td>\n      <td>1939.941176</td>\n      <td>1094.099099</td>\n      <td>20.206897</td>\n      <td>430.000000</td>\n      <td>202.933333</td>\n      <td>173.400000</td>\n      <td>252.967742</td>\n      <td>91.600000</td>\n      <td>0.0</td>\n      <td>12004.932476</td>\n      <td>1093.792453</td>\n      <td>3271.446735</td>\n      <td>34.750000</td>\n      <td>2292.811847</td>\n      <td>162.421739</td>\n      <td>5676.422581</td>\n      <td>3443.579592</td>\n      <td>135.000000</td>\n      <td>20.615385</td>\n      <td>155.342105</td>\n      <td>29.00</td>\n      <td>1205.532520</td>\n      <td>775.687500</td>\n      <td>59757.267568</td>\n      <td>9143.719132</td>\n      <td>296.000000</td>\n      <td>42.352941</td>\n      <td>589.733333</td>\n      <td>6.076923</td>\n      <td>54.666667</td>\n      <td>158.733333</td>\n      <td>30.488372</td>\n      <td>361.652893</td>\n      <td>40.086957</td>\n      <td>453.287356</td>\n      <td>198.120000</td>\n      <td>197.875000</td>\n      <td>37.611111</td>\n      <td>10884.271879</td>\n      <td>1015.785714</td>\n      <td>872.860759</td>\n      <td>283.943396</td>\n      <td>102.039216</td>\n      <td>30.818182</td>\n      <td>8849.053237</td>\n      <td>9130.414365</td>\n      <td>61.0</td>\n      <td>89.0</td>\n      <td>747.731544</td>\n      <td>3429.103933</td>\n    </tr>\n    <tr>\n      <th>1002</th>\n      <td>1581.666667</td>\n      <td>0.426821</td>\n      <td>0.483702</td>\n      <td>0.075928</td>\n      <td>0.543996</td>\n      <td>1.945985</td>\n      <td>0.512028</td>\n      <td>0.021110</td>\n      <td>0.013754</td>\n      <td>0.007356</td>\n      <td>0.004028</td>\n      <td>0.002014</td>\n      <td>0.057039</td>\n      <td>0.397310</td>\n      <td>110209.094837</td>\n      <td>76528.651212</td>\n      <td>112762.510011</td>\n      <td>68397.574440</td>\n      <td>1365.477273</td>\n      <td>7195.936170</td>\n      <td>5828.594855</td>\n      <td>491.852041</td>\n      <td>9359.559633</td>\n      <td>7644.710884</td>\n      <td>5950.027027</td>\n      <td>17160.873832</td>\n      <td>357.482759</td>\n      <td>5472.768786</td>\n      <td>14372.985075</td>\n      <td>2988.672840</td>\n      <td>22.864865</td>\n      <td>2014.000000</td>\n      <td>709.490196</td>\n      <td>345.607143</td>\n      <td>230.739130</td>\n      <td>171.045455</td>\n      <td>478.0</td>\n      <td>26671.457584</td>\n      <td>1876.096386</td>\n      <td>8241.715493</td>\n      <td>45.900000</td>\n      <td>7165.760446</td>\n      <td>200.776978</td>\n      <td>15064.363402</td>\n      <td>4553.858268</td>\n      <td>203.333333</td>\n      <td>5.200000</td>\n      <td>50.571429</td>\n      <td>226.75</td>\n      <td>3884.156156</td>\n      <td>1680.297030</td>\n      <td>90529.577519</td>\n      <td>17208.319790</td>\n      <td>2920.580645</td>\n      <td>50.500000</td>\n      <td>795.161473</td>\n      <td>250.188406</td>\n      <td>55.558140</td>\n      <td>183.547368</td>\n      <td>36.613636</td>\n      <td>233.418605</td>\n      <td>279.500000</td>\n      <td>1371.350000</td>\n      <td>343.864865</td>\n      <td>313.536585</td>\n      <td>28.466667</td>\n      <td>18077.309922</td>\n      <td>1077.425743</td>\n      <td>951.927711</td>\n      <td>262.408163</td>\n      <td>143.475410</td>\n      <td>75.666667</td>\n      <td>17565.600846</td>\n      <td>18141.400534</td>\n      <td>478.0</td>\n      <td>913.0</td>\n      <td>2165.102679</td>\n      <td>3328.300000</td>\n    </tr>\n    <tr>\n      <th>1003</th>\n      <td>34.000000</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.026667</td>\n      <td>0.493333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1144.470588</td>\n      <td>0.000000</td>\n      <td>1178.882353</td>\n      <td>1140.823529</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>39.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>420.285714</td>\n      <td>58.571429</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>11.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>124.375000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n      <td>50.571429</td>\n      <td>52.857143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>127.000000</td>\n    </tr>\n    <tr>\n      <th>1005</th>\n      <td>395.000000</td>\n      <td>0.349221</td>\n      <td>0.563934</td>\n      <td>0.072222</td>\n      <td>0.588392</td>\n      <td>2.197465</td>\n      <td>0.666593</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.095656</td>\n      <td>0.234669</td>\n      <td>22167.666667</td>\n      <td>10815.708861</td>\n      <td>22489.105485</td>\n      <td>18147.203883</td>\n      <td>101.540230</td>\n      <td>255.414634</td>\n      <td>190.341463</td>\n      <td>79.781818</td>\n      <td>1259.212121</td>\n      <td>431.709677</td>\n      <td>595.120000</td>\n      <td>2172.040816</td>\n      <td>249.210526</td>\n      <td>775.472222</td>\n      <td>866.888889</td>\n      <td>390.523077</td>\n      <td>11.333333</td>\n      <td>0.000000</td>\n      <td>184.000000</td>\n      <td>217.000000</td>\n      <td>64.428571</td>\n      <td>42.000000</td>\n      <td>0.0</td>\n      <td>3605.024390</td>\n      <td>196.333333</td>\n      <td>944.545455</td>\n      <td>22.000000</td>\n      <td>705.000000</td>\n      <td>48.034483</td>\n      <td>1668.731707</td>\n      <td>1168.338028</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>49.545455</td>\n      <td>0.00</td>\n      <td>262.964912</td>\n      <td>287.000000</td>\n      <td>16397.639594</td>\n      <td>2451.994924</td>\n      <td>92.000000</td>\n      <td>30.000000</td>\n      <td>215.521127</td>\n      <td>2.333333</td>\n      <td>28.000000</td>\n      <td>52.764706</td>\n      <td>11.727273</td>\n      <td>131.500000</td>\n      <td>48.000000</td>\n      <td>177.384615</td>\n      <td>64.857143</td>\n      <td>78.666667</td>\n      <td>32.000000</td>\n      <td>2912.877193</td>\n      <td>232.360000</td>\n      <td>205.100000</td>\n      <td>81.692308</td>\n      <td>33.642857</td>\n      <td>25.000000</td>\n      <td>2333.698925</td>\n      <td>2440.118557</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>232.511111</td>\n      <td>942.876344</td>\n    </tr>\n    <tr>\n      <th>1007</th>\n      <td>1263.333333</td>\n      <td>0.371875</td>\n      <td>0.535202</td>\n      <td>0.074259</td>\n      <td>0.549149</td>\n      <td>2.118798</td>\n      <td>0.646966</td>\n      <td>0.010649</td>\n      <td>0.001536</td>\n      <td>0.009113</td>\n      <td>0.000000</td>\n      <td>0.002304</td>\n      <td>0.067025</td>\n      <td>0.274557</td>\n      <td>89435.886544</td>\n      <td>60582.518470</td>\n      <td>90681.472296</td>\n      <td>75133.359133</td>\n      <td>430.725076</td>\n      <td>1191.135294</td>\n      <td>893.522013</td>\n      <td>396.790698</td>\n      <td>3282.663793</td>\n      <td>1659.638462</td>\n      <td>1956.922078</td>\n      <td>7871.321212</td>\n      <td>554.785714</td>\n      <td>2498.990291</td>\n      <td>7139.078947</td>\n      <td>1441.467811</td>\n      <td>28.102564</td>\n      <td>1019.285714</td>\n      <td>242.095238</td>\n      <td>244.600000</td>\n      <td>234.410526</td>\n      <td>93.818182</td>\n      <td>461.0</td>\n      <td>18382.015198</td>\n      <td>645.000000</td>\n      <td>5421.671975</td>\n      <td>32.461538</td>\n      <td>4345.236593</td>\n      <td>274.600000</td>\n      <td>10080.288754</td>\n      <td>5682.074733</td>\n      <td>109.142857</td>\n      <td>18.333333</td>\n      <td>107.185185</td>\n      <td>36.00</td>\n      <td>1468.026923</td>\n      <td>850.129032</td>\n      <td>70909.868012</td>\n      <td>11772.092622</td>\n      <td>658.142857</td>\n      <td>44.166667</td>\n      <td>800.430279</td>\n      <td>9.166667</td>\n      <td>90.562500</td>\n      <td>208.063492</td>\n      <td>26.387097</td>\n      <td>381.086207</td>\n      <td>169.000000</td>\n      <td>591.606383</td>\n      <td>186.000000</td>\n      <td>170.560000</td>\n      <td>22.500000</td>\n      <td>12582.720770</td>\n      <td>677.312500</td>\n      <td>591.716981</td>\n      <td>157.342857</td>\n      <td>137.808511</td>\n      <td>43.636364</td>\n      <td>11473.818482</td>\n      <td>11726.790476</td>\n      <td>160.0</td>\n      <td>121.0</td>\n      <td>1088.131387</td>\n      <td>2862.419795</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"16658f0dede6af4401da1c918c9b492d36b38a17"},"cell_type":"code","source":"for name in df_full_detail.columns:\n    if df_full_detail[name].dtype in ('float64','int64'):\n        df_full_detail[name] = zscore(df_full_detail[name])\ndf_train = multi_merge('zipcode','zipcode',df_train,df_full_detail).drop(['id','zipcode'],axis=1)\ndf_test = multi_merge('zipcode','zipcode',df_test,df_full_detail).drop(['id','zipcode'],axis=1)\nx,y = to_xy(df_train,'score')","execution_count":101,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tree Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nimport warnings, gc\nfeatures = [c for c in df_train.columns if c not in ['score']]\ndf_target = df_train['score']\ndef lgb_cv(max_depth,\n          num_leaves,\n          min_data_in_leaf,\n          feature_fraction,\n          bagging_fraction,\n          lambda_l1):\n    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    oof = np.zeros(df_train.shape[0])\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_target.values)):\n        print(\"fold n°{}\".format(fold_))\n        trn_data = lgb.Dataset(df_train.iloc[trn_idx][features],\n                               label=df_target.iloc[trn_idx]\n                              )\n        val_data = lgb.Dataset(df_train.iloc[val_idx][features],\n                               label=df_target.iloc[val_idx]\n                              )\n        param = {\n            'num_leaves': int(num_leaves),\n            'min_data_in_leaf': int(min_data_in_leaf), \n            'objective':'regression',\n            'max_depth': int(max_depth),\n            'learning_rate': 0.005,\n            \"boosting\": \"gbdt\",\n            \"feature_fraction\": feature_fraction,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": bagging_fraction ,\n            \"bagging_seed\": 11,\n            \"metric\": 'rmse',\n            \"lambda_l1\": lambda_l1,\n            \"verbosity\": -1\n        }\n        clf = lgb.train(param,\n                        trn_data,\n                        10000,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=200,\n                        early_stopping_rounds = 100)\n        oof[val_idx] = clf.predict(df_train.iloc[val_idx][features],\n                                   num_iteration=clf.best_iteration)\n        del clf, trn_idx, val_idx\n        gc.collect()\n    return -metrics.mean_squared_error(oof, df_target)**0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO = BayesianOptimization(lgb_cv, {\n    'max_depth': (4, 10),\n    'num_leaves': (5, 130),\n    'min_data_in_leaf': (10, 150),\n    'feature_fraction': (0.7, 1.0),\n    'bagging_fraction': (0.7, 1.0),\n    'lambda_l1': (0, 6)\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-'*126)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=2, n_iter=4, acq='ei', xi=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in df_train.columns if c not in ['score']]\ndf_target = df_train['score']\nparam = {\n            'num_leaves': int(52),\n            'min_data_in_leaf': int(12), \n            'objective':'regression',\n            'max_depth': int(6),\n            'learning_rate': 0.005,\n            \"boosting\": \"gbdt\",\n            \"feature_fraction\": 0.7066,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": 0.8086,\n            \"bagging_seed\": 11,\n            \"metric\": 'rmse',\n            \"lambda_l1\": 3.651,\n            \"random_state\": 11,\n            \"verbosity\": -1\n        }\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_target.values)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features],\n                           label=df_target.iloc[trn_idx]\n                          )\n    val_data = lgb.Dataset(df_train.iloc[val_idx][features],\n                           label=df_target.iloc[val_idx]\n                          )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(metrics.mean_squared_error(oof, df_target)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_csv = pd.read_csv('../input/midterm/test.csv')\nsubmit_csv = submit_csv.drop('zipcode',axis=1)\nsubmit_csv['score'] = predictions\nsubmit_csv.to_csv('csv_to_submit_80.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11cee547dc103e43a660487c8649e73cbea47c83"},"cell_type":"markdown","source":"**Model Part (Neural)**"},{"metadata":{"trusted":true,"_uuid":"658ff4fb69b415cdafd73afe92c69a43586707b8"},"cell_type":"code","source":"#features = list(cols)[:10]\n#features_train = features + ['score']\n#df_train = df_train.loc[:,features_train]\n#df_test = df_test.loc[:,features]\n#x,y = to_xy(df_train,'score')\n\nkf = KFold(5)\noos_y = []\noos_pred = []\nfold = 0\nfor train,test in kf.split(x):\n    fold += 1\n    print('Fold #{}'.format(fold))\n    x_train = x[train]\n    y_train = y[train]\n    x_test = x[test]\n    y_test = y[test]\n    \n    model = Sequential()\n    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n    model.add(Dense(75, activation='relu'))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-2, patience=10, verbose=1, mode='auto')\n    checkpointer = ModelCheckpoint(filepath=\"midterm_model1_best\", verbose=0, save_best_only=True)\n    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=0,epochs=250)\n    \n    pred = model.predict(x_test)\n    oos_y.append(y_test)\n    oos_pred.append(pred)\n    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n    print(\"Fold score (RMSE): {}\".format(score))\n\noos_y = np.concatenate(oos_y)\noos_pred = np.concatenate(oos_pred)\nscore = metrics.mean_squared_error(oos_pred,oos_y)\nprint(\"Final score (RMSE): {}\".format(np.sqrt(score)))\n\ndf_test = pd.read_csv('../input/midterm/test.csv')\ndf_test = multi_merge('zipcode','zipcode',df_test,df_full_detail).drop(['id','zipcode'],axis=1)\ntrue_test = df_test.values.astype(np.float32)\npred_test = model.predict(true_test)\npred_test = model.predict(true_test)\nfinal_test_score = np.concatenate(pred_test)\n\ndf_test['score'] = final_test_score\ndf_test = df_test.loc[:,['id','score']]\ndf_test.to_csv('csv_to_submit_80_neural.csv', index = False)\n","execution_count":107,"outputs":[{"output_type":"stream","text":"Fold #1\nEpoch 00029: early stopping\nFold score (RMSE): 5.9513936042785645\nFold #2\nEpoch 00025: early stopping\nFold score (RMSE): 6.886742115020752\nFold #3\nEpoch 00027: early stopping\nFold score (RMSE): 7.139791488647461\nFold #4\nEpoch 00029: early stopping\nFold score (RMSE): 6.25360107421875\nFold #5\nEpoch 00017: early stopping\nFold score (RMSE): 7.392597675323486\nFinal score (RMSE): 6.746560096740723\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \nPassing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\nSee the documentation here:\nhttps://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n  return self._getitem_tuple(key)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}