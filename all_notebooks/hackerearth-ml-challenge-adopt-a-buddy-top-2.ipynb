{"cells":[{"metadata":{"id":"Snuh_QDoTVmR"},"cell_type":"markdown","source":"# Hackerearth ML Challenge : Adopt a Buddy \n\nProblem Description: [Hackerearth Link](https://www.hackerearth.com/challenges/competitive/hackerearth-machine-learning-challenge-pet-adoption/machine-learning/pet-adoption-9-5838c75b/) \n\nLeaderboard [Link](https://www.hackerearth.com/challenges/competitive/hackerearth-machine-learning-challenge-pet-adoption/leaderboard/pet-adoption-9-5838c75b/)\n\nRank: 115th\n\nFinal Score: 90.4\n\nWe have been given training and testing dataset which has columns like the Pet_Id , Condition , Color , Issue and Listing Date. The target variables are the breed_category and pet_category which we need to predict \n\nAs there are two classes , the approach taken is training two models for each classes and then testing seperately and appending the final result. ","execution_count":null},{"metadata":{"id":"ap_kN2NOTTaQ"},"cell_type":"markdown","source":"## Importing Dataset","execution_count":null},{"metadata":{"id":"D94FwmO59neb","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf_train=pd.read_csv('../input/hackerearth-ml-challenge-pet-adoption/train.csv')\ndf_test=pd.read_csv('../input/hackerearth-ml-challenge-pet-adoption/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"RysK5ZSe_vME","outputId":"6d0f6fd6-abeb-45b0-9e03-eef40b428aaa","trusted":true},"cell_type":"code","source":"print(df_train.head())\nprint(df_train.tail())","execution_count":null,"outputs":[]},{"metadata":{"id":"yWxypyDAU-XL"},"cell_type":"markdown","source":"# Viewing the columns","execution_count":null},{"metadata":{"id":"dVvW9px9Am18","outputId":"c51a3f93-5b16-473d-d889-a929697b254a","trusted":true},"cell_type":"code","source":"print(df_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"DAlONxcsVH0e"},"cell_type":"markdown","source":"# Displaying the Unique values","execution_count":null},{"metadata":{"id":"AaWN7mtTADhe","outputId":"ba03ebc6-cb34-4ec9-8314-961a232160b5","trusted":true},"cell_type":"code","source":"print(df_train['pet_category'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"EL2jW6X8_7ZP","outputId":"bfbe4d94-83fc-4384-faac-33b1bc1ae211","trusted":true},"cell_type":"code","source":"y_train=df_train['pet_category'].values\nprint(y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"P98WaBX_AQkw","outputId":"d805a7d4-d550-4d47-8c8e-5147920cabc1","trusted":true},"cell_type":"code","source":"print(df_train['condition'].unique())\nprint(df_train['color_type'].unique())\nprint(df_train['breed_category'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"XK7X_tKgBD4v","outputId":"e1f1b780-e9c7-4190-a5f8-b6255094f300","trusted":true},"cell_type":"code","source":"print(df_train['length(m)'].unique())\nprint(df_train['height(cm)'].unique())\nprint(df_train['X1'].unique())\nprint(df_train['X2'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"qFN8zbDbVgtR"},"cell_type":"markdown","source":"# Calculating the no. of NAN values","execution_count":null},{"metadata":{"id":"zNOL2CKVVnqE"},"cell_type":"markdown","source":"Training dataset stats","execution_count":null},{"metadata":{"id":"3F67IEq7Bn9z","outputId":"a4612d4e-0d9f-43c3-f65e-8aede6bb3ec8","trusted":true},"cell_type":"code","source":"print(df_train['length(m)'].isna().sum())\nprint(df_train['height(cm)'].isna().sum())\nprint(df_train['X1'].isna().sum())\nprint(df_train['X2'].isna().sum())\n\nprint(df_train['condition'].isna().sum())\nprint(df_train['color_type'].isna().sum())\nprint(df_train['breed_category'].isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"ubCYxa04VtAx"},"cell_type":"markdown","source":"Testing dataset stats","execution_count":null},{"metadata":{"id":"fZb5ncaMeqZz","outputId":"d1e2d99b-3fce-4517-b3b4-2c2972ad23e2","trusted":true},"cell_type":"code","source":"print(df_test['length(m)'].isna().sum())\nprint(df_test['height(cm)'].isna().sum())\nprint(df_test['X1'].isna().sum())\nprint(df_test['X2'].isna().sum())\n\nprint(df_test['condition'].isna().sum())\nprint(df_test['color_type'].isna().sum())\n#print(df_test['breed_category'].isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"Uyv-IqXsV5gh"},"cell_type":"markdown","source":"**Observation:** The column 'condition' is having many Nan values","execution_count":null},{"metadata":{"id":"_ssT4uAJB_zX","outputId":"07ac4d6b-79ba-4fb6-d358-fb90a9182e57","trusted":true},"cell_type":"code","source":"print(df_train.groupby(['condition']).size())\n\nprint(df_train[df_train['condition'].isnull()])","execution_count":null,"outputs":[]},{"metadata":{"id":"avy0k7s5FXRQ","outputId":"e1bccba4-fb8c-4f86-c2ff-abc1a4c55c0d","trusted":true},"cell_type":"code","source":"print(df_train[df_train['condition'].isnull()]['breed_category'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"5c5WlfUgFcNQ"},"cell_type":"markdown","source":"**Observation:** Only For breed_category 2.0 , condition value is null","execution_count":null},{"metadata":{"id":"rS_jM7NyFspO","outputId":"c48fc3c2-3c82-4535-da9b-d143d12974cf","trusted":true},"cell_type":"code","source":"df_train[df_train['breed_category']==2].count()","execution_count":null,"outputs":[]},{"metadata":{"id":"obbQ49QAGA9J"},"cell_type":"markdown","source":"So, we can give a unique value for condition where it is null","execution_count":null},{"metadata":{"id":"Vtw0vJOvXA93"},"cell_type":"markdown","source":"# Clearing NAN value in 'condition' column","execution_count":null},{"metadata":{"id":"8y5y928mGHRK","trusted":true},"cell_type":"code","source":"import numpy as np\ndf_train['condition']=df_train['condition'].replace(np.nan,3)","execution_count":null,"outputs":[]},{"metadata":{"id":"6UAuhyy5fHm9","trusted":true},"cell_type":"code","source":"df_test['condition']=df_test['condition'].replace(np.nan,3)","execution_count":null,"outputs":[]},{"metadata":{"id":"ez8CpnEKHBth","outputId":"7fe6c2bd-5c3b-42fb-b414-7991b509f391","trusted":true},"cell_type":"code","source":"print(df_train.groupby(['condition']).size())\n\nprint(df_train[df_train['condition'].isnull()])","execution_count":null,"outputs":[]},{"metadata":{"id":"4_BvkXboYPZ_"},"cell_type":"markdown","source":"# Finding difference between issue_date and listing_date in days","execution_count":null},{"metadata":{"id":"Qka8ilgcXWYi"},"cell_type":"markdown","source":"Calculating Difference and adding feature for training data","execution_count":null},{"metadata":{"id":"Tq5i_WieSO9B","outputId":"402b945e-9afb-4d98-d2a7-41f646451ce3","trusted":true},"cell_type":"code","source":"df_train['diff_days']=np.abs((pd.to_datetime(df_train['listing_date'].values)-pd.to_datetime(df_train['issue_date'].values)).days)\n\nprint(df_train['diff_days'].values)","execution_count":null,"outputs":[]},{"metadata":{"id":"P9bXvb8QXbAA"},"cell_type":"markdown","source":"Calculating Difference and adding feature for testing data","execution_count":null},{"metadata":{"id":"QF4DSJHIfTmh","outputId":"779e0aa3-99b1-41d0-f01a-f886e97d0f7d","trusted":true},"cell_type":"code","source":"df_test['diff_days']=np.abs((pd.to_datetime(df_test['listing_date'].values)-pd.to_datetime(df_test['issue_date'].values)).days)\n\nprint(df_test['diff_days'].values)","execution_count":null,"outputs":[]},{"metadata":{"id":"sGYfLlv0XfnP"},"cell_type":"markdown","source":"Checking the correctness of difference (in days)","execution_count":null},{"metadata":{"id":"WXS2kfhfVYKQ","outputId":"47f0fa02-f8ff-4f36-cf79-8c72d8c26f9a","trusted":true},"cell_type":"code","source":"print(df_train['issue_date'][5], \" \", df_train['listing_date'][5], \" \",df_train['diff_days'][5])","execution_count":null,"outputs":[]},{"metadata":{"id":"NeCeyi-IXeuq"},"cell_type":"markdown","source":"So, we have added new feature 'diff_days' which describes difference in days between listing date and issue date. \\\n","execution_count":null},{"metadata":{"id":"fFygx4pmZN3U"},"cell_type":"markdown","source":"# Removing date-time columns","execution_count":null},{"metadata":{"id":"X1PokKC6ZUL1"},"cell_type":"markdown","source":"Dropping columns from training dataset","execution_count":null},{"metadata":{"id":"KRvWQNYuXmYU","outputId":"453fdd62-8ba7-4c8d-90e5-9d95f36ebc55","trusted":true},"cell_type":"code","source":"df_train_new=df_train.drop(columns=['issue_date','listing_date'])\n\nprint(df_train_new.head())\nprint(df_train_new.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"cjgWcxtoZXOq"},"cell_type":"markdown","source":"Dropping columns from testing dataset","execution_count":null},{"metadata":{"id":"JwxDIdRnfaDQ","outputId":"69545d6c-b027-448e-8975-887ae30cb0a1","trusted":true},"cell_type":"code","source":"df_test_new=df_test.drop(columns=['issue_date','listing_date'])\n\nprint(df_test_new.head())\nprint(df_test_new.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"jy-4kF40HMDK"},"cell_type":"markdown","source":"# Encoding columns of categorical names with numbers\n\nEncoding the color_type column in training dataset","execution_count":null},{"metadata":{"id":"8V0Ciy9NYT6j","outputId":"75267cee-0a02-427d-cbae-7c47de144858","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\ndf_train_new[\"color_type_code\"] = lb_make.fit_transform(df_train_new[\"color_type\"])\ndf_train_new[[\"color_type\", \"color_type_code\"]].head(11)","execution_count":null,"outputs":[]},{"metadata":{"id":"9eGNm0viaIRc"},"cell_type":"markdown","source":"Encoding the color_type column in testing dataset","execution_count":null},{"metadata":{"id":"-yE4R7r_fhpQ","outputId":"679e263c-68f5-4e8f-d133-730a882ae334","trusted":true},"cell_type":"code","source":"df_test_new[\"color_type_code\"] = lb_make.transform(df_test_new[\"color_type\"])\ndf_test_new[[\"color_type\", \"color_type_code\"]].head(11)","execution_count":null,"outputs":[]},{"metadata":{"id":"Myz5yE5vbn3N"},"cell_type":"markdown","source":"Dropping the column from both the training and testing dataset","execution_count":null},{"metadata":{"id":"WCtuVsZjZ6fe","outputId":"f53fff0d-f7a7-4665-98a5-bef46f8a59d4","trusted":true},"cell_type":"code","source":"df_train_new['color_type_code'].unique()\n\ndf_train_new=df_train_new.drop(columns=['color_type'])\n\nprint(df_train_new.head(25))","execution_count":null,"outputs":[]},{"metadata":{"id":"UP5G_uGWfs3t","outputId":"dbd22b21-81e3-4161-9445-6a4601d1a254","trusted":true},"cell_type":"code","source":"df_test_new['color_type_code'].unique()\n\ndf_test_new=df_test_new.drop(columns=['color_type'])\n\nprint(df_test_new.head(25))","execution_count":null,"outputs":[]},{"metadata":{"id":"fYdL1xSMTD6Z"},"cell_type":"markdown","source":"# Distribution of values in some features","execution_count":null},{"metadata":{"id":"Q6IYURedTIjk","outputId":"030d6307-1e83-4cff-a233-44fb6ce8d087","trusted":true},"cell_type":"code","source":"print(df_train_new.columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"DnGluyHTb_C3"},"cell_type":"markdown","source":"Checking skewness of Length(m)","execution_count":null},{"metadata":{"id":"j0kS0bFLThoW","outputId":"a51411f2-8a25-410c-8cc8-ec1e20e95fd0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['length(m)'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Length\")\nax.set(title=\"Length distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['length(m)']))","execution_count":null,"outputs":[]},{"metadata":{"id":"PjPW6e_7cP9f"},"cell_type":"markdown","source":"Checking skewness of Height(cm)","execution_count":null},{"metadata":{"id":"Vh6rKcIATzPa","outputId":"83967d1b-85f5-407e-e9bd-4073661e85a6","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['height(cm)'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Height\")\nax.set(title=\"Height distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['height(cm)']))","execution_count":null,"outputs":[]},{"metadata":{"id":"kNm_xZEHcgNL"},"cell_type":"markdown","source":"Checking skewness of X1","execution_count":null},{"metadata":{"id":"__jAkdEmUE7O","outputId":"c66b90b7-d845-4423-b22a-26e9ffedca12","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['X1'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X1\")\nax.set(title=\"X1 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['X1']))","execution_count":null,"outputs":[]},{"metadata":{"id":"lWy4DHCjcoPq"},"cell_type":"markdown","source":"Checking skewness of X2","execution_count":null},{"metadata":{"id":"hluA3UyMUPmp","outputId":"3c3d1eed-245f-43f8-f125-b4ac13b55446","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(df_train_new['X2'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X2\")\nax.set(title=\"X2 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(df_train_new['X2']))","execution_count":null,"outputs":[]},{"metadata":{"id":"sRyTeR-oW98-"},"cell_type":"markdown","source":"We noticed that only in column X1, there is high skewness, so we take the log transform of that column both in training and testing dataset","execution_count":null},{"metadata":{"id":"l45HUKS5c2et"},"cell_type":"markdown","source":"For Training Dataset","execution_count":null},{"metadata":{"id":"HCA9BFv3UV7z","outputId":"d8f7835c-cf1e-4d2f-a0c1-9a400e2a3146","trusted":true},"cell_type":"code","source":"# to check skewness of X1 Score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nX1_trans=np.log(1+df_train_new['X1'].values)\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(X1_trans, color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X1\")\nax.set(title=\"X1 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(X1_trans))","execution_count":null,"outputs":[]},{"metadata":{"id":"sVaQszkec5vC"},"cell_type":"markdown","source":"For Testing Dataset","execution_count":null},{"metadata":{"id":"36ZutRJzVuUo","outputId":"4043b1b1-4d64-4970-e0f1-d1d7b5a36dc9","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew \nX1_trans_test=np.log(1+df_test_new['X1'].values)\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the distribution \nsns.distplot(X1_trans, color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"X1\")\nax.set(title=\"X1 distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\nprint(\"skew value: \", skew(X1_trans_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"eyUqgTwGVi1E","trusted":true},"cell_type":"code","source":"df_train_norm=df_train_new\ndf_train_norm['X1']=X1_trans\n\ndf_test_norm=df_test_new\ndf_test_norm['X1']=X1_trans_test","execution_count":null,"outputs":[]},{"metadata":{"id":"5CFww-H5dBQM"},"cell_type":"markdown","source":"# Creating features on basis of magnitude of Length and Height","execution_count":null},{"metadata":{"id":"wfpW1SLb3ePq"},"cell_type":"markdown","source":"* Length: low (0 to 0.3) , medium (0.3 to 0.6) , high( 0.6 to 1.0)\n* Height: low (0 to 15) , medium (15 to 30) , high(30 to 45)","execution_count":null},{"metadata":{"id":"hsWD57iy39jA","trusted":true},"cell_type":"code","source":"df_train_norm['Low_Height']=np.where(df_train_norm['height(cm)']<=15,1,0)\ndf_train_norm['Medium_Height']=np.where(((df_train_norm['height(cm)']>15) & (df_train_norm['height(cm)']<=30)),1,0)\ndf_train_norm['High_Height']=np.where(df_train_norm['height(cm)']>30,1,0)\n\ndf_test_norm['Low_Height']=np.where(df_test_norm['height(cm)']<=15,1,0)\ndf_test_norm['Medium_Height']=np.where(((df_test_norm['height(cm)']>15) & (df_test_norm['height(cm)']<=30)),1,0)\ndf_test_norm['High_Height']=np.where(df_test_norm['height(cm)']>30,1,0)","execution_count":null,"outputs":[]},{"metadata":{"id":"ATCgFuj44f9x","trusted":true},"cell_type":"code","source":"df_train_norm['Low_Length']=np.where(df_train_norm['length(m)']<=0.3,1,0)\ndf_train_norm['Medium_Length']=np.where((df_train_norm['length(m)']>0.3) & (df_train_norm['length(m)']<=0.6),1,0)\ndf_train_norm['High_Length']=np.where(df_train_norm['length(m)']>0.6,1,0)\n\ndf_test_norm['Low_Length']=np.where(df_test_norm['length(m)']<=0.3,1,0)\ndf_test_norm['Medium_Length']=np.where((df_test_norm['length(m)']>0.3) & (df_test_norm['length(m)']<=0.6),1,0)\ndf_test_norm['High_Length']=np.where(df_test_norm['length(m)']>0.6,1,0)","execution_count":null,"outputs":[]},{"metadata":{"id":"GdZSh3KbK7Lt","outputId":"5bf5bbf2-cfa4-47ce-928b-21cab9e61641","trusted":true},"cell_type":"code","source":"print(df_train_norm.head(20))","execution_count":null,"outputs":[]},{"metadata":{"id":"wzbDmhl4dc5d"},"cell_type":"markdown","source":"# XGBoost training and validation","execution_count":null},{"metadata":{"id":"HUekaeOzWlx3","outputId":"fa7c8190-c67d-491d-a659-a46cdef2a66b","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nY=df_train_norm['pet_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel=XGBClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_norm['breed_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel2=XGBClassifier()\nmodel2.fit(X_train,y_train)\ny_pred=model2.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"EwZsWtAsXl1f"},"cell_type":"markdown","source":"XGBoost without any parameter tuning gave performance f1 score of 89.71 on submission ","execution_count":null},{"metadata":{"id":"5xs-tWqHfdis"},"cell_type":"markdown","source":"# XGBoost Parameter tuning","execution_count":null},{"metadata":{"id":"sw2BuoWBZM3Y"},"cell_type":"markdown","source":"Observations of performance with respect to learning rate:\n\n* learning rate=0.01 (89.75)\n* learning rate=0.1 (90.04)\n* learning rate=0.4 (90.17)\n* learning rate=0.6 (90.14)","execution_count":null},{"metadata":{"id":"gKcw8-N1sr8z"},"cell_type":"markdown","source":"Now the 'max_depth' parameter is decreased from 4 and tried with 3, 2 and 1. \n\nIt is found that 4 is optimal","execution_count":null},{"metadata":{"id":"p3rC3gH3gJDK"},"cell_type":"markdown","source":"The subsample is increased from 0.8 but we found that it is optimal\n","execution_count":null},{"metadata":{"id":"Dd6ekNB4geYD"},"cell_type":"markdown","source":"Performance with respect to 'gamma' parameter original \n* gamme= 5 (90.57)\n* gamma= 4 (90.7)\n\ngamma=4 is optimal","execution_count":null},{"metadata":{"id":"0Gloz9ElXqfy","outputId":"3664e080-5d35-4f7e-c311-917b8ba608a9","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nY=df_train_norm['pet_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\n\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_norm['breed_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\nX_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodel2=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\nmodel2.fit(X_train,y_train)\ny_pred=model2.predict(X_test)\nprint(f1_score(y_pred,y_test,average='weighted'))\nprint(accuracy_score(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"vbtwXcQDg4wG"},"cell_type":"markdown","source":"# Training the final XGBoost tuned model","execution_count":null},{"metadata":{"id":"ff1vpl7pxOjI","outputId":"63fbfae7-1d06-40af-91cc-bf016deee96c","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nY=df_train_norm['pet_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\n\nmodelx.fit(X,Y)\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_norm['breed_category'].values\nX=df_train_norm.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx2=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4)\nmodelx2.fit(X,Y)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_N8-BnldiGxY"},"cell_type":"markdown","source":"# Observations","execution_count":null},{"metadata":{"id":"itEwJryYx_ko"},"cell_type":"markdown","source":"90.81 performance score without train_test_split, i.e training on whole training Dataset","execution_count":null},{"metadata":{"id":"S8hVqEw4hIF6"},"cell_type":"markdown","source":"After this it is checked the diff_days column had negative values for **two cases** in the training dataset. So used np.abs() for calculation of difference. \n\nAfter retraining the model and testing it, final f1 score acheived on submission is 90.83","execution_count":null},{"metadata":{"id":"h9eF-mIQU4HB"},"cell_type":"markdown","source":"Apart from XgBoost classifier, I had tried with RandomForest, LGBM and CatBoost Classifier too, but it was found that XgBoost performed the best","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Further Feature Engineering\n\nI did not stop at 90.83, but tried to add more features to train the model more efficiently and check whether any improvement is made.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Converting Length to cm units, same as height","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_norm['length(cm)']=df_test_norm['length(m)']*100\ndf_train_norm['length(cm)']=df_train_norm['length(m)']*100\n\n\ndf_test_today=df_test_norm.drop(columns=['length(m)'])\ndf_train_today=df_train_norm.drop(columns=['length(m)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding ratio of X2 : X1 and adding it as a feature\n\n* X1 , X2 columns were removed but the performance decreased, so we include those","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_today['ratio']=df_train_today['X2']/(1+df_train_today['X1'])\ndf_test_today['ratio']=df_test_today['X2']/(1+df_test_today['X1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding ratio of Height(cm) : Length(cm) and adding it as a feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_today['lhratio']=df_train_today['height(cm)']/(1+df_train_today['length(cm)'])\ndf_test_today['lhratio']=df_test_today['height(cm)']/(1+df_test_today['length(cm)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Arriving at the final model\n\nAfter tuning parameters like 'learning_rate' and 'max_depth' , I arrived at the final model which gave the final Public score of **91.06 (an improvement of 0.23)** ! Eureka!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nY=df_train_today['pet_category'].values\nX=df_train_today.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.47,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1500, \n                      reg_alpha = 0.3,\n                      max_depth=7, \n                      gamma=4,\n                     random_state=42)\n\nmodelx.fit(X,Y)\n\n\nfrom sklearn.model_selection import train_test_split\n\nY=df_train_today['breed_category'].values\nX=df_train_today.drop(columns=['pet_category','pet_id','breed_category'])\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nmodelx2=XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.4,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=4,\n                      random_state=42)\nmodelx2.fit(X,Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Y_test_fin=df_test_new['pet_category'].values\nidx=df_test_today['pet_id'].values\nX_test_fin=df_test_today.drop(columns=['pet_id'])\n\n\ny_pred_fin=modelx.predict(X_test_fin)\n\n\nfrom sklearn.model_selection import train_test_split\n\n#Y_test_fin=df_test_new['pet_category'].values\nidx=df_test_today['pet_id'].values\nX_test_fin=df_test_today.drop(columns=['pet_id'])\n\n\ny_pred_fin2=modelx2.predict(X_test_fin)\n\n\ndf_sub = pd.DataFrame({'pet_id': idx,\n                   'breed_category': y_pred_fin2,\n                   'pet_category': y_pred_fin})\ndf_sub.to_csv('submit.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Points to learn\n\n* Feature Engineering is very important, always try to incorporate new features to see performance improvement.\n\n* How to smartly convert DateTime values into numerical features in order to train your model more efficiently\n\n* Parameter Tuning is equally critical\n\n* Never lose Hope!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}