{"cells":[{"metadata":{},"cell_type":"markdown","source":"> ## Assignment-1 Supervised Learning\n"},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 1 csv file in the current version of the dataset:\n"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code."},{"metadata":{},"cell_type":"markdown","source":"### Let's check 1st file: /kaggle/input/data.csv"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"nRowsRead = None # specify 'None' if want to read whole file\n# data.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf = pd.read_csv('/kaggle/input/data.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'data.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking labels distributions\nimport seaborn as sns\nsns.set(context = 'paper')\n\nplt.figure(figsize = (10,5))\nsns.countplot(df['Bankrupt?'])\nplt.title('Class Distributions \\n (Bankrupt?:FALSE||Bankrupt?:TRUE)', fontsize=14)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split features and classification\nX = df.drop('Bankrupt?', axis = 1)       \ny = df['Bankrupt?']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since we are dealing with imbalanced data, we need to apply the synthetic minority oversampling technique \nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE()\nX, y = smote.fit_resample(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train and test data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data structures for storing best accuracies, training time and test time\nnum_classifiers = 5\nbest_accuracy = np.zeros(num_classifiers)\ntrain_time = np.zeros(num_classifiers)\ntest_time = np.zeros(num_classifiers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithm-1 Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt = tree.DecisionTreeClassifier(random_state=42)\nclf_dt.fit(X_train, y_train)\ny_pred = clf_dt.predict(X_test)\ndt_accuracy = accuracy_score(y_test, y_pred)\nprint(dt_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_path = '/kaggle/output'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depth_range = np.arange(20) + 1\ntrain_scores, test_scores = validation_curve(clf_dt, X_train, y_train, param_name=\"max_depth\", param_range=depth_range, cv=5)\n\nplt.figure()\nplt.xticks(depth_range)\nplt.plot(depth_range, np.mean(train_scores, axis=1), label='Training score')\nplt.plot(depth_range, np.mean(test_scores, axis=1), label='Cross-validation score')\nplt.xlabel('Max Depth')\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\nplt.savefig(fig_path + 'dt_validation_curve_1.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  **Hyperparameter Tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nclf_dt = tree.DecisionTreeClassifier(random_state=42)\nclf_dt.fit(X_train, y_train)\n\ndepth_range = np.arange(10) + 1\ntuned_params = {'max_depth' : depth_range}\nclf_dt = GridSearchCV(clf_dt, param_grid=tuned_params, cv=5)\nt0 = time.time()\nclf_dt.fit(X_train, y_train)\nt1 = time.time()\ntrain_time[0] = t1 - t0\nprint('Completed training in %f seconds' % train_time[0])\nbest_clf_dt = clf_dt\nbest_dt_params = clf_dt.best_params_\nprint(\"Best parameters set for decision tree found on development set:\")\nprint(best_dt_params)\nt0 = time.time()\ny_pred = clf_dt.predict(X_test)\nt1 = time.time()\ntest_time[0] = t1 - t0\nprint('Inference time on test data: %f seconds' % test_time[0])\nbest_accuracy[0] = accuracy_score(y_test, y_pred)\nprint('Accuracy of decision tree is %.2f%%' % (best_accuracy[0] * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes = np.linspace(0.1, 1.0, 10)\n_, train_scores, test_scores = learning_curve(best_clf_dt, X_train, y_train, train_sizes=train_sizes, cv=5)\nplt.figure()\nplt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Decisin Tree Training score')\nplt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Decision Tree CV score')\nplt.xlabel('% Training examples')\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\nplt.savefig(fig_path + 'dt_learning_curve.png')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Algorithm-2 Neural Network**"},{"metadata":{},"cell_type":"markdown","source":"I start Neural Network analysis with feed-forward neural network by using PyTorch."},{"metadata":{},"cell_type":"markdown","source":"# **Preparing the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_nn = MLPClassifier(hidden_layer_sizes=(5, 2), random_state=42, max_iter=1000)\nclf_nn.fit(X_train, y_train)\ny_pred = clf_nn.predict(X_test)\nnn_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy of neural network without hyperparameter tuning is %.2f%%' % (nn_accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_svmlight_file\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6250)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=6250)\n\ny_train=np.array(y_train)\ny_valid=np.array(y_valid)\ny_test=np.array(y_test)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_valid.shape)\nprint(y_valid.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MaxAbsScaler().fit(X_train)\nX_train_transformed = scaler.transform(X_train)\nX_valid_transformed = scaler.transform(X_valid)\nX_test_transformed = scaler.transform(X_test)\n\nprint(\"Train - Min: {}, Max: {}\".format(np.min(X_train_transformed), np.max(X_train_transformed)))\nprint(\"Valid - Min: {}, Max: {}\".format(np.min(X_valid_transformed), np.max(X_train_transformed)))\nprint(\"Test  - Min: {}, Max: {}\".format(np.min(X_test_transformed), np.max(X_test_transformed)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will train a feed-forward neural network by using PyTorch. We will do the following steps in order:\n\n1. Load the training and test datasets using DataLoader\n2. Define a Feedforwad Neural Network\n3. Define a loss function\n4. Train the network on the training data\n5. Test the network on the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# lets fix the random seeds for reproducibility.\ntorch.manual_seed(6250)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(6250)\n\ntrainset = TensorDataset(torch.from_numpy(X_train_transformed.astype('float32')), torch.from_numpy(y_train.astype('float32')).view(-1,1))\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n\nvalidset = TensorDataset(torch.from_numpy(X_valid_transformed.astype('float32')), torch.from_numpy(y_valid.astype('float32')).view(-1,1))\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=4, shuffle=True, num_workers=2)\n\ntestset = TensorDataset(torch.from_numpy(X_test_transformed.astype('float32')), torch.from_numpy(y_test.astype('float32')).view(-1,1))\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FeedForwardNet(nn.Module):\n    def __init__(self, n_input, n_hidden, n_output):\n        super(FeedForwardNet, self).__init__()\n        self.hidden1 = nn.Linear(n_input, n_hidden)\n        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n        self.out = nn.Linear(n_hidden, n_output)\n\n    def forward(self, x):\n        x = F.relu(self.hidden1(x))\n        x = F.relu(self.hidden2(x))\n        x = self.out(x)\n        return x\n\nnet = FeedForwardNet(n_input=95, n_hidden=5, n_output=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = []\ny_scores = []\nfor data in testloader:\n    inputs, labels = data\n    outputs = net(inputs)\n    outputs = torch.sigmoid(outputs)  # since we have no activation at the end of output layer\n    y_true.extend(labels.numpy().flatten().tolist())\n    y_scores.extend(outputs.data.numpy().flatten().tolist())\n    \nfpr, tpr, _ = roc_curve(y_true, y_scores)\nauc_ffnet = auc(fpr, tpr)\nauc_ffnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses = []\nvalid_losses = []\nmodel = net\n\nfor epoch in range(10):  # loop over the dataset multiple times\n\n    # set the model as train mode\n    model.train()\n    train_loss = 0.0\n    train_counter = 0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, targets = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += (loss.item() * inputs.size(0))\n        train_counter += inputs.size(0)\n\n    train_losses.append(train_loss/train_counter)\n    \n    # switch to evaluation mode\n    model.eval()\n    valid_loss = 0.0\n    valid_counter = 0\n    with torch.no_grad():\n        for i, data in enumerate(validloader, 0):\n            # get the inputs\n            inputs, targets = data\n\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            valid_loss += (loss.item() * inputs.size(0))\n            valid_counter += inputs.size(0)\n    \n    valid_losses.append(valid_loss/valid_counter)\n    \nprint('Finished Training')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot(np.arange(len(train_losses)), train_losses, label='Train')\nplt.plot(np.arange(len(valid_losses)), valid_losses, label='Validation')\nplt.ylabel('Loss')\nplt.xlabel('epoch')\nplt.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = []\ny_scores = []\nfor data in testloader:\n    inputs, labels = data\n    outputs = model(inputs)\n    # outputs = torch.sigmoid(outputs) \n    y_true.extend(labels.numpy().flatten().tolist())\n    y_scores.extend(outputs.data.numpy().flatten().tolist())\n\nfprnn, tprnn, _ = roc_curve(y_true, y_scores)\nauc_ffnet = auc(fprnn, tprnn)\nauc_ffnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import *\n\ndef classification_metrics_score(Y_true, Y_score, a):\n    #NOTE: It is important to provide the output in the same order\n    Y_score[Y_score>=a] = 1\n    Y_score[Y_score<a] = 0\n    Y_pred = Y_score\n    acc = accuracy_score(Y_true,Y_pred)\n    # bacc = balanced_accuracy_score(Y_true, Y_pred)\n    auc_ = roc_auc_score(Y_true,Y_score)\n    precision = precision_score(Y_true,Y_pred)\n    recall = recall_score(Y_true,Y_pred)\n    f1score = f1_score(Y_true,Y_pred)\n    cm = confusion_matrix(Y_true,Y_pred)\n    false_positive = cm[1,0]/cm.sum()\n\n    return acc, auc_, precision, recall, f1score, cm, false_positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_metrics_score(np.array(y_true), np.array(y_scores),0.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, class_names):\n    cm = confusion_matrix(y_true, y_pred)\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # Normalize\n    cmap = plt.cm.Blues\n\n    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n           xticklabels=class_names, yticklabels=class_names,\n           title=\"Normalized Confusion Matrix\",\n           ylabel='True', xlabel='Predicted')\n\n    # Rotate the tick labels and set alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    # Loop over data dimensions and create text annotations\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = []\ny_scores = []\nfor data in trainloader:\n    inputs, labels = data\n    outputs = model(inputs)\n    # outputs = torch.sigmoid(outputs) \n    y_true.extend(labels.numpy().flatten().tolist())\n    y_scores.extend(outputs.data.numpy().flatten().tolist())\n\nfprnn, tprnn, _ = roc_curve(y_true, y_scores)\nauc_ffnet = auc(fprnn, tprnn)\nauc_ffnet\n\nclassification_metrics_score(np.array(y_true), np.array(y_scores),0.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,6))\nlw = 2\nplt.plot(fprnn, tprnn, color='blue',lw=lw, label='MLP (AUC = %0.2f)' % auc_ffnet)\nplt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Multi-layer Perceptron')\nplt.legend(loc=\"lower right\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.array(y_scores.copy())\ny_pred[y_pred>=0.5] = 1\ny_pred[y_pred<0.5] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_true, y_pred, [\"Bankruptcy:False\",\"Bankruptcy:True\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Algorithm-3 Boosting**"},{"metadata":{},"cell_type":"markdown","source":"We use AdaBoostClassifier with decision tree stumps."},{"metadata":{},"cell_type":"markdown","source":"We are now pruning hyperparameters like the number of weak learners and anayze how it affects the performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(X)\ny = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train and test data (revert from MLP data)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_stump = tree.DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\nclf_boosted = AdaBoostClassifier(base_estimator=dt_stump, random_state=7)\nclf_boosted.fit(X_train, y_train)\ny_pred = clf_boosted.predict(X_test)\nboosted_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy of Adaboost without hyperparameter tuning is %.2f%%' % (boosted_accuracy * 100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define AdaBoost learner\nnum_learners = 500\ndt_stump = tree.DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\nclf_boosted = AdaBoostClassifier(base_estimator=dt_stump, n_estimators=num_learners, random_state=42)\n\n# Cross-validation\nnum_folds = 5\nkf = KFold(n_splits=num_folds, random_state=7)\ntrain_scores = np.zeros((num_learners, num_folds))\nval_scores = np.zeros((num_learners, num_folds))\nfor idx, (train_index, test_index) in enumerate(kf.split(X_train)):\n    clf_boosted.fit(X_train[train_index], y_train[train_index])\n    train_scores[:, idx] = np.asarray(list(clf_boosted.staged_score(X_train[train_index], y_train[train_index])))\n    val_scores[:, idx] = np.asarray(list(clf_boosted.staged_score(X_train[test_index], y_train[test_index])))\n\nn_estimators_range = np.arange(num_learners) + 1\nplt.figure(figsize=(7,6))\nplt.plot(n_estimators_range, np.mean(train_scores, axis=1), label='Training score')\nplt.plot(n_estimators_range, np.mean(val_scores, axis=1), label='Cross-validation score')\nplt.xlabel('Number of weak learners')\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_learners_optimal = np.argmax(np.mean(val_scores, axis=1)) + 1\nprint('Optimal number of learners for AdaBoost: %d' % num_learners_optimal)\nbest_clf_boosted = AdaBoostClassifier(base_estimator=dt_stump, n_estimators=num_learners_optimal, random_state=7)\nt0 = time.time()\nbest_clf_boosted.fit(X_train, y_train)\nt1 = time.time()\ntrain_time[2] = t1 - t0\nprint('Completed training in %f seconds' % train_time[2])\nt0 = time.time()\ny_pred = best_clf_boosted.predict(X_test)\nt1 = time.time()\ntest_time[0] = t1 - t0\nprint('Inference time on test data: %f seconds' % test_time[2])\nbest_accuracy[0] = accuracy_score(y_test, y_pred)\nprint('Accuracy of Adaboost with the best hyperparameters is %.2f%%' % (best_accuracy[0] * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes = np.linspace(0.1, 1.0, 5)\nbest_clf_boosted = AdaBoostClassifier(base_estimator=dt_stump, n_estimators=num_learners_optimal, random_state=7)\n_, train_scores, test_scores = learning_curve(best_clf_boosted, X_train, y_train, train_sizes=train_sizes, cv=5)\n\nplt.figure()\nplt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')\nplt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Cross-validation score')\nplt.xlabel('% training examples')\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\n#plt.savefig(fig_path + 'dt_learning_curve.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, y_pred, [\"Bankruptcy:False\",\"Bankruptcy:True\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Algorithm-4 SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MaxAbsScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm_linear = svm.LinearSVC()\nsvm_linear.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = np.logspace(-3, 3, 7)\ntrain_scores, test_scores = validation_curve(svm_linear, X_train, y_train, param_name=\"C\", param_range=C_range, cv=5)\n\nplt.figure()\nplt.semilogx(C_range, np.mean(train_scores, axis=1), label='Training score')\nplt.semilogx(C_range, np.mean(test_scores, axis=1), label='Score')\nplt.xlabel('C')\nplt.ylabel(\"Classification score\")\nplt.legend(loc=\"best\")\nplt.grid()\n#plt.savefig(fig_path + 'dt_validation_curve_1.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = np.logspace(-2, 1, 10)\ntuned_params = {'C' : C_range}\nsvm_linear = GridSearchCV(svm_linear, param_grid=tuned_params, cv=5)\nt0 = time.time()\nsvm_linear.fit(X_train, y_train)\nt1 = time.time()\ntrain_time[3] = t1 - t0\nprint('Completed training in %f seconds' % train_time[3])\nbest_clf_svm = svm_linear\nbest_params = svm_linear.best_params_\nprint(\"Best parameters set found on development set:\")\nprint(best_params)\nt0 = time.time()\ny_pred = best_clf_svm.predict(X_test)\nt1 = time.time()\ntest_time[3] = t1 - t0\nprint('Inference time on test data: %f seconds' % test_time[3])\nbest_accuracy[3] = accuracy_score(y_test, y_pred)\nprint('Best accuracy with SVM (linear kernel) is %.2f%%' % (best_accuracy[3] * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes = np.linspace(0.1, 1.0, 5)\n_, train_scores, test_scores = learning_curve(best_clf_svm, X_train, y_train, train_sizes=train_sizes, cv=5)\n\nplt.figure()\nplt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')\nplt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Cross-validation score')\nplt.xlabel('% training examples')\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\n#plt.savefig(fig_path + 'dt_learning_curve.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, y_pred, [\"Bankruptcy:False\",\"Bankruptcy:True\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Algorithm-5 kNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = np.arange(1, 15)\ntrain_scores, test_scores = validation_curve(KNeighborsClassifier(), X_train, y_train, param_name=\"n_neighbors\", \n                                             param_range=k_range, cv=5)\n\nplt.figure()\nplt.plot(k_range, np.mean(train_scores, axis=1), label='Training score')\nplt.plot(k_range, np.mean(test_scores, axis=1), label='Cross-validation score')\nplt.xlabel('k')\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\n#plt.savefig(fig_path + 'dt_validation_curve_1.png')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time \n\nk_optimal = np.argmax(np.mean(test_scores, axis=1)) + 1\nprint('Optimal value of k: %d' % k_optimal)\nbest_clf_knn = KNeighborsClassifier(n_neighbors=k_optimal)\nt0 = time.time()\nbest_clf_knn.fit(X_train, y_train)\nt1 = time.time()\ntrain_time[4] = t1 - t0\nprint('Completed training in %f seconds' % train_time[4])\nt0 = time.time()\ny_pred = best_clf_knn.predict(X_test)\nt1 = time.time()\ntest_time[4] = t1 - t0\nprint('Inference time on test data: %f seconds' % test_time[4])\nbest_accuracy[4] = accuracy_score(y_test, y_pred)\nprint('Accuracy of kNN with k = %d is %.2f%%' % (k_optimal, best_accuracy[4] * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes = np.linspace(0.1, 1.0, 5)\n_, train_scores, test_scores = learning_curve(best_clf_knn, X_train, y_train, train_sizes=train_sizes, cv=5)\n\nplt.figure()\nplt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')\nplt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Cross-validation score')\nplt.xlabel('% training examples')\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\n#plt.savefig(fig_path + 'dt_learning_curve.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, y_pred, [\"Bankruptcy:False\",\"Bankruptcy:True\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Fork Notebook\" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}