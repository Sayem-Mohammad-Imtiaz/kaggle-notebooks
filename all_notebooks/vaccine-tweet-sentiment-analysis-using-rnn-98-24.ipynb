{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import library\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import Data**","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/covid19-vaccine-tweets/file.csv')\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Only Englsih language tweet**","metadata":{}},{"cell_type":"code","source":"df = df[df['language']=='en']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df[['tweet']]\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data= data.reset_index(drop=True)\ndata['process_tweet'] = data['tweet'].copy()\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import library**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers as ppb\n\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\n\n\n#import library\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder   ###########\nfrom sklearn.metrics import confusion_matrix , classification_report , accuracy_score\nfrom sklearn.manifold import TSNE ######\nfrom sklearn.feature_extraction.text import TfidfVectorizer #############\n\nfrom keras.preprocessing.text import Tokenizer         #######\nfrom keras.preprocessing.sequence import pad_sequences #######\nfrom keras.models import Sequential\nfrom keras.layers import Dropout , Conv1D ,MaxPool1D,Activation , Dense , Flatten , Embedding , LSTM ####\n\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau , EarlyStopping\n\n\n#nltk\n\nimport nltk\nfrom nltk.corpus import stopwords ########\nfrom nltk.stem import SnowballStemmer ############\n\n\n#word to vec\nimport gensim\n\nimport re #####\nimport os\nfrom collections import Counter #######\nimport logging ###\nimport time\nimport pickle ######\nimport itertools ######\n\n\nfrom textblob import TextBlob # TextBlob - Python library for processing textual data\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data procesing**","metadata":{}},{"cell_type":"code","source":"def clean_data(txt):\n  txt = txt.lower()  # lowercase\n  txt = re.sub(r'@[A-Za-z0-9_]+' , '' , txt)   #remove mentions\n  txt = re.sub(r'#' , '' , txt) #remove hashtags\n  txt = re.sub(r'RT : ','' , txt) # remove retweets\n  txt = re.sub(r'https?:\\/\\/[A-Za-z0-9\\./\\/]+' , '' , txt) #removes url\n  txt = re.sub('\\[.*?\\]' , '' , txt) #remove square brackets\n  txt = re.sub(r'[^\\w\\s]' , '' , txt) #remove puntuations\n  txt = re.sub('\\w*\\d\\w*' , '' ,txt) #removes words containig numbers\n  txt = re.sub('\\n' , '' ,txt) #remove new lines\n\n  return txt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['process_tweet'] = data['process_tweet'].apply(clean_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove Stopwords**","metadata":{}},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\n\ndef remove_stopwords(txt):\n  rmv_stpwords = [i for i in txt.split() if i not in stopwords]\n  rmv_stpwords_join = ' '.join(rmv_stpwords)\n  return rmv_stpwords_join\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['process_tweet'] = data['process_tweet'].apply(remove_stopwords)\n\ndata.head()#after removing stopwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PorterStemmer**","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nst = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemming(txt):\n  txt =[st.stem(word) for word in txt.split()]\n  txt_join = ' '.join(txt)\n  return txt_join","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['process_tweet'] = data['process_tweet'].apply(stemming)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using TextBlob calculate the polarity**","metadata":{}},{"cell_type":"code","source":"def get_text_polarity(txt):\n  return TextBlob(txt).sentiment.polarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Polarity'] = data['process_tweet'].apply(get_text_polarity)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Labeling using the polarity score**","metadata":{}},{"cell_type":"code","source":"#Labeling\ndef get_text_analysis(i):\n  if (i<-0.5):\n    return 'Strongly Negative'\n  elif ((i<0 ) and (i >= -0.5)):\n    return 'Negative'\n  elif (i == 0):\n    return 'Neutral'\n  elif (i>0 and i<=0.5):\n    return 'Positive'\n  else:\n    return 'Strongly Positive'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Sentiment'] = data['Polarity'].apply(get_text_analysis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for deep learning we need to only 2 colums(sentiment and process tweet)\ndata = data[['Sentiment','process_tweet' ]]\n\npossible_labels = data.Sentiment.unique()\n\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels): \n#The enumerate() function takes a collection (e.g. a tuple) and returns it as an enumerate object.\n    label_dict[possible_label] = index\n\ndata['label'] = data.Sentiment.replace(label_dict)\n\n\nprint(label_dict)\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nax = data['Sentiment'].value_counts(sort=False).plot(kind='barh')\nax.set_xlabel('Number of Samples in training Set')\nax.set_ylabel('Label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Total Unique words after pre processing**","metadata":{}},{"cell_type":"code","source":"unique_words = set(data['process_tweet'])\ncount = 0\nfor word in unique_words:\n  count += 1\nprint(count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokenization**","metadata":{}},{"cell_type":"code","source":"#tokenization\ntokenizer = Tokenizer(num_words=163967, split=' ')\n#num_words: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n\n\ntokenizer.fit_on_texts(data['process_tweet'].values)\n\nX = tokenizer.texts_to_sequences(data['process_tweet'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[:3] #before padding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Padding**","metadata":{}},{"cell_type":"code","source":"#padding to make all text vector to same length\n\nX = pad_sequences(X)\n\nX[:3] #after padding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RNN Model**","metadata":{}},{"cell_type":"code","source":"#creating models\n\nmodel= Sequential()\nmodel.add(Embedding(163967, 256 , input_length=X.shape[1]))\nmodel.add(Dropout(0.3))\n\n\nmodel.add(LSTM(128 , return_sequences=True , dropout=0.3 , recurrent_dropout=0.3))\nmodel.add(LSTM(128,dropout=0.3 , recurrent_dropout=0.3))\n\n\nmodel.add(Dense(5 , activation='softmax'))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam' , loss = 'categorical_crossentropy' , metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#one hot encoding\n\ny = pd.get_dummies(data['Sentiment']).values\n\n[print(data['Sentiment'][i] ,y[i]) for i in range(0,5)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data split**","metadata":{}},{"cell_type":"code","source":"x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 ,random_state = 22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trannig model\nbatch_size = 128\nepochs = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tranning the model**","metadata":{}},{"cell_type":"code","source":"history = model.fit(x_train , y_train ,\n          epochs = epochs,\n          batch_size = batch_size,\n          validation_split=0.1,\n          verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nscore = model.evaluate(x_test, y_test , batch_size = batch_size)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(x_test)\npred1 = np.argmax(predictions , axis=1)\npred1[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.argmax(y_test , axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test , pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Classification Report**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test , pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Accuracy**","metadata":{}},{"cell_type":"code","source":"accuracy_score(y_test , pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(30,10)\n\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\nax[0].set_title('Training & Validation Accuracy')\n\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Training & Validation Loss\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}