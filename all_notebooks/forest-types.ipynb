{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A work in progress\n## Forest Type Classification by Andy Hedberg\nhttps://www.linkedin.com/in/andy-hedberg-24b01511b/\n\nI am expanding the exploratory analysis started within the Forest Types dataset starter kernel: https://www.kaggle.com/nagendeak/foresttypes. \n\nModel objective: Classification of forest types\n\nLearning objectives:\n    1. try out different ways to code EDA tasks\n    2. compare feature selection techniques\n    3. compare manually selected models to AutoML (h2o)\n    4. integrate my knowledge about data science with my past background in conservation biology\n    5. explore my budding interest in remote sensing"},{"metadata":{},"cell_type":"markdown","source":"### set-up - imports, reads, splits, functions, seed value"},{"metadata":{},"cell_type":"markdown","source":"imports"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport os # accessing directory structure\n\nfrom scipy.stats import randint as sp_randint\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport h2o\nfrom h2o.automl import H2OAutoML\n\nfrom scipy import stats\n\nfrom random import seed\nfrom random import randint\n\nimport pickle\n\nfrom subprocess import check_output\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning) # suppress \"future warning\" messages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"file checkPoint\n"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"define function - collect useful information and stats about a dataframe"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def preProc_df(df,cutoff_catg_conv,cutoff_num,cutoff_excess,cutoff_sparse):\n    \"\"\"    \n    Returns\n    -------\n    DataFrame with the following fields for each variable:\n        Basic Statistics : Mean, Median, Min, Max, Quantiles, Count\n        % Missing and # of Missing values\n        # of distinct values\n        % Zero and # of Zero values\n        Binary Indicator indicating high cardinality or low based on cutoff value provided\n        Binary indicator indicating singular value for predictor\n    Parameters\n    ----------\n    df : Any Pandas DataFrame\n    cutoff : int, optional (default = 10)\n          Threshold of distinct values to classify a feature as having\n          high or low ordinality.  \n\n    \"\"\"\n    #Adding 1 & 99th quantiles\n    stats = df.describe().T.reset_index()\n    p1 = df.quantile(0.01)\n    p99 = df.quantile(0.99)\n    stats1 = pd.concat([p1,p99],axis=1)\n    stats1.columns = ['1%','99%']\n    stats2 = stats.merge(stats1,left_on='index',right_index=True)\n    stats2 = stats2.drop('count',axis=1)\n    stats2.rename(columns={'index':'feature'},inplace=True)\n\n    #Number of unique values per column\n    distinct = []\n    for col in df.columns:\n        n_unique = df[col].nunique()\n        distinct.append([col,n_unique])\n    #distinct\n\n    # add count, nzero, pct zero, pct missing, combine distinct and dtypes,\n    length = len(df)\n    count = df.count()\n    nzero = df.apply(lambda x: (x == 0).sum())\n    nmiss = length-count\n    pctzero = nzero/length*100\n    pctmiss = nmiss/length*100\n    sparse=pd.Series()\n    for i in df.columns:\n        sparse[i]=1 if df[i].astype(bool).sum() < cutoff_sparse else 0\n    base = pd.concat([count,nmiss,nzero,pctzero,pctmiss,df.dtypes,sparse],axis=1)\n    base.columns = ['count','Missing','Zero','%Zero','%Missing','Dtype','Sparse']\n    distinct = pd.DataFrame(distinct,columns=['Var','Distinct'])\n    merge1 = base.merge(distinct,left_index=True,right_on='Var')\n    merged = merge1.merge(stats2,left_on='Var',right_on='feature',how='left', suffixes=('','_1'))\n    merged = merged.drop('feature',axis=1)\n    merged = merged[['Var','count','Distinct','Dtype','Sparse','Missing','%Missing','Zero','%Zero','mean','std','min','1%','25%','50%','75%','99%','max']]\n    merged['Low_Cardinality'] = (merged['Distinct'] <= cutoff_catg_conv)*1\n    merged['Medium_Cardinality'] = ((merged['Distinct'] > cutoff_catg_conv) & (merged['Distinct'] < cutoff_num)) *1\n    merged['High_Cardinality'] = (merged['Distinct'] >= cutoff_num)*1\n    merged['Excess_Cardinality'] = (merged['Distinct'] >= cutoff_excess)*1\n    merged['String'] = (merged['Dtype'] == 'object')*1\n    merged['Float'] = (merged['Dtype'] == 'float64')*1\n    merged['All_Missing'] = (merged['Missing'] == len(df))*1\n    merged['Some_Missing'] = (merged['Missing'] > 0)*1\n    merged['Singular_Value'] = (merged['Distinct'] <= 1)*1\n    merged['Sparse'] = (merged['Sparse'] == 1)*1\n    merged['Outlier']= ((merged['max']>=1.5*(merged['75%'] - merged['25%'])) | (merged['min']<=1.5*(merged['75%'] - merged['25%'])))*1\n    \n    #merged['Sparse']=[1 if df[merged['Var']].astype(bool).sum()<cutoff_sparse else 0] #Number of non zero elements in each column\n#    merged['D_type']=df.columns.dtypes\n    return merged","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"define function - histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_histo(df,var):\n    g = sns.FacetGrid(df, col=\"class\", margin_titles=True)\n    g.map(sns.distplot, var, color=\"steelblue\", kde=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"define function - boxplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_box(df,var):\n    g = sns.FacetGrid(df, col=\"class\", margin_titles=True)\n    g.map(sns.boxplot, var, color=\"steelblue\", order=['d ' 'h ' 's ' 'o '])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"define functiob - scatter and density plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.5, 0.8), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"define function - unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique(df,var):\n    unique = pd.unique(df[[var]].values.ravel('K'))\n    print(var)\n    print(unique)\n    print(\"\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"generate random number for seed\nhttps://machinelearningmastery.com/how-to-generate-random-numbers-in-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate random integer values\nfrom random import seed\nfrom random import randint\n# seed random number generator\nseed(57720)  # run random generator once without a seed value here and swap in this random seed for the seed value to use in future steps\n\n# generate some integers\nfor _ in range(1):\n    seed_value = randint(0, 100000)\n    print(seed_value)\n    print('CheckPoint - Last random number was: 80683')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### combine testing.csv + training.csv and do a new random 80/20 train/test split\nI am not sure why the testing file has more rows than the training file (testing = 325 rows vs training = 198). I am recreating training and testing files by appending the two original files and doing an 80/20 split (train/test) on the newly appended df."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/testing.csv', delimiter=',')\nnRow, nCol = df1.shape\nclasses = df1['class'].unique()\nprint(f'There are {nRow} rows and {nCol} columns in the testing.csv dataset')\nprint('')\nprint(df1.head(5))\nprint('')\nprint(f'Classes: {classes}')\nprint('')\nprint(df1.groupby(['class']).describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv('../input/training.csv', delimiter=',')\nnRow, nCol = df2.shape\nclasses = df2['class'].unique()\nprint(f'There are {nRow} rows and {nCol} columns in the training.csv dataset')\nprint('')\nprint(df2.head(5))\nprint('')\nprint(f'Classes: {classes}')\nprint('')\nprint(df2.groupby(['class']).describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df2.append(df1, ignore_index=True)\nnRow, nCol = df3.shape\nclasses = df3['class'].unique()\nprint(f'There are {nRow} rows and {nCol} columns in the combined dataset')\nprint('')\nprint(df3.head(5))\nprint('')\nprint(f'Classes: {classes}')\nprint('')\nprint(df3.groupby(['class']).describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"convert df to numpy array"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels are the target values we want to predict\nlabels = np.array(df3['class'])\n\n# Remove the target from the features\ndf = df3.drop('class', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"split the data into training and testing sets using scikit-learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features, train_labels, test_labels = model_selection.train_test_split(df, labels, test_size = 0.20, random_state = seed_value)\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### explore data - EDA\n\nexplore using training data, post split, to help avoid data leakage and avoid creating bias\n\nidentify categoricals"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns that are categorical:', train_features.columns[train_features.select_dtypes(include=['object']).any()].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns with missing values:', train_features.columns[train_features.isna().any()].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check distinct values for class variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('distinct target values:')\nunique(df=df3, var='class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"stats - target"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tgt = pd.concat([train_features, df3['class']], join='inner', axis=1, ignore_index=False)\nstats = df_tgt['class'].value_counts()\nprint('Frequency for target:')\nprint(stats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = pd.DataFrame(preProc_df(df=train_features, cutoff_catg_conv=20, cutoff_num=1000, cutoff_excess=22500, cutoff_sparse=0))\nstats.to_csv('stats_train_df.csv', index = False) # save to csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num = df3.drop('class',axis=1)\n\nn=len(df_num.columns)\n\nfor i in range(n):\n    col = df_num.columns[i]\n    pred_histo(df=df3,var=col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num = df3.drop('class',axis=1)\n\nn=len(df_num.columns)\n\nfor i in range(n):\n    col = df_num.columns[i]\n    pred_box(df=df3,var=col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df3.loc[:, ['b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9']]\nplotScatterMatrix(df_tmp, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df3.loc[:, ['pred_minus_obs_H_b1', 'pred_minus_obs_H_b2', 'pred_minus_obs_H_b3',\n       'pred_minus_obs_H_b4', 'pred_minus_obs_H_b5', 'pred_minus_obs_H_b6',\n       'pred_minus_obs_H_b7', 'pred_minus_obs_H_b8', 'pred_minus_obs_H_b9']]\nplotScatterMatrix(df_tmp, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df3.loc[:, ['pred_minus_obs_S_b1', 'pred_minus_obs_S_b2', 'pred_minus_obs_S_b3',\n       'pred_minus_obs_S_b4', 'pred_minus_obs_S_b5', 'pred_minus_obs_S_b6',\n       'pred_minus_obs_S_b7', 'pred_minus_obs_S_b8', 'pred_minus_obs_S_b9']]\nplotScatterMatrix(df_tmp, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"identify near-zero variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"nzv_tmp = stats.query(\"Sparse == 1\")\nnzv_list = nzv_tmp['Var'].tolist()\nprint('Variables with near zero variance:')\nprint(nzv_list)\n\n# with open('nzv_list.txt', 'w') as filehandle:  \n#     filehandle.writelines(\"%s\\n\" % place for place in nzv_list)\n\n# remove nzv list (none - not needed)\n# train_features = train_features.drop(nzv_list, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"identify high correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find correlation\ncorr_df = df3.corr(method='pearson') # choosing pearson since data appear normally distributed\n\nc1 = corr_df.abs().unstack()\nc1 = pd.DataFrame(c1.sort_values(ascending = False))\n\nc1.to_csv('stats_corr_pairs.csv', index = True)\n\nprint(c1.head())\nprint(c1.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# start work here next!\n## under construction"},{"metadata":{},"cell_type":"markdown","source":"After manually reviewing the correlation pairs in the above output file (stats_corr_pairs.csv), \n\npred_minus_obs_H_b1\npred_minus_obs_H_b2\npred_minus_obs_H_b5\npred_minus_obs_H_b7\npred_minus_obs_H_b8","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.init()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = h2o.H2OFrame(df3)\n\ny = \"class\"\nx = ['pred_minus_obs_H_b1', 'pred_minus_obs_H_b2', 'pred_minus_obs_H_b3']\n\nsplits = df.split_frame(ratios = [0.8], seed = value )\ntrain = splits[0]\ntest = splits[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train.as_data_frame()\ny_train = y_train.loc[:, ['class']]\ny_train.squeeze()\n# C = np.delete(y_train, 1, 1)  # delete second column of C\n# abc = np.delete(y_train)\n# y_train = y_train.reset_index().values\n\ny_test = test.as_data_frame()\ny_test = y_test.loc[:, ['class']]\ny_test.squeeze()\n# y_test = y_test.reset_index().values\n\nx_train = train.as_data_frame()\nx_train = x_train.drop([\"class\"], axis = 1)\nx_train = x_train.reset_index().values\n\nx_test = test.as_data_frame()\nx_test = x_test.drop([\"class\"], axis = 1)\nx_test = x_test.reset_index().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.shape)\n# print(abc.shape)\n# print(y_train.dtype.names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('y_train shape is:')\nprint(y_train.shape)\nprint('')\nprint('x_train shape is:')\nprint(x_train.shape)\nprint('')\nprint('y_test shape is:')\nprint(y_test.shape)\nprint('')\nprint('x_test shape is:')\nprint(x_test.shape)\nprint('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature selection:\nhttps://libraries.io/pypi/ReliefF"},{"metadata":{"trusted":true},"cell_type":"code","source":"fs = ReliefF(n_neighbors=100, n_features_to_keep=5)\n# X_train = fs.fit_transform(x_train, y_train)\nX_test_subset = fs.transform(x_test)\n# print(x_test.shape, X_test_subset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n# from sklearn.cross_validation import train_test_split\nfrom ReliefF import ReliefF\n\ndigits = load_digits(2)\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(digits.data)\n# print(digits.target)\n# print(X_train)\n# print(X_train.shape)\n# print(X_test)\n# print(X_test.shape)\n# print(y_train)\n# print(y_train.shape)\n# print(y_test)\n# print(y_test.shape)\nprint('y_train shape is:')\nprint(y_train.shape)\nprint('')\nprint('X_train shape is:')\nprint(X_train.shape)\nprint('')\nprint('y_test shape is:')\nprint(y_test.shape)\nprint('')\nprint('X_test shape is:')\nprint(X_test.shape)\nprint('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fs = ReliefF(n_neighbors=100, n_features_to_keep=5)\nX_train = fs.fit_transform(X_train, y_train)\nX_test_subset = fs.transform(X_test)\nprint(X_test.shape, X_test_subset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### h20 AutoML\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"aml = H2OAutoML(max_runtime_secs = 300, seed = value)\naml.train(y = y,\n         training_frame = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb = aml.leaderboard\nlb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\nglm = H2OGeneralizedLinearEstimator(family = 'multinomial')\nglm.train(x = x, y = y, training_frame = train, validation_frame = test)\n\n# print the auc for the validation data\n# glm.auc(valid = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = aml.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Boruta"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels are the target values we want to predict\nlabels = np.array(df3['class'])\n\n# Remove the target from the features\ndf = df3.drop('class', axis = 1)\n\n# # Save feature names for later use\nfeature_list = np.array(df.columns)\n\n# # Convert to numpy array\ndf = np.array(df)\n\ntrain_features, test_features, train_labels, test_labels = model_selection.train_test_split(df, labels, test_size = 0.20, random_state = seed_value)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\n\nrf = RandomForestClassifier(n_jobs=-1, class_weight=None, max_depth=7, random_state=seed_value)\n# Define Boruta feature selection method\nfeat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=seed_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_selector.fit(train_features, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check selected features\nprint(feat_selector.support_)\n# Select the chosen features from our dataframe.\nselected = train_features[:, feat_selector.support_]\nprint (\"\")\nprint (\"Selected Feature Matrix Shape\")\nprint (selected.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = [feature_list[i] for i, x in enumerate(feat_selector.support_) if x]\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = []\ndrop_list.append('b1')\ndrop_list.append('b2')\ndrop_list.append('b3')\ndrop_list.append('b4')\ndrop_list.append('b5')\ndrop_list.append('b6')\ndrop_list.append('b7')\ndrop_list.append('b8')\ndrop_list.append('b9')\ndrop_list.append('pred_minus_obs_H_b1')\ndrop_list.append('pred_minus_obs_H_b2')\ndrop_list.append('pred_minus_obs_H_b3')\ndrop_list.append('pred_minus_obs_H_b4')\ndrop_list.append('pred_minus_obs_H_b5')\ndrop_list.append('pred_minus_obs_H_b6')\ndrop_list.append('pred_minus_obs_H_b7')\ndrop_list.append('pred_minus_obs_H_b8')\ndrop_list.append('pred_minus_obs_H_b9')\ndrop_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4 = df3.drop(drop_list)\n\n# Labels are the target values we want to predict\nlabels = np.array(df4['class'])\n\n# Remove the target from the features\ndf = df4.drop('class', axis = 1)\n\n# # Save feature names for later use\nfeature_list = np.array(df.columns)\n\n# # Convert to numpy array\ndf = np.array(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features, test_features, train_labels, test_labels = model_selection.train_test_split(df, labels, test_size = 0.20, random_state = seed_value)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from forest types kernel: https://www.kaggle.com/nagendeak/kernelc80b0b4462"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# import sklearn\n\n# from sklearn.model_selection import train_test_split\n# from sklearn.linear_model import LogisticRegression\n\n# training=pd.read_csv('../input/training.csv')\n# testing=pd.read_csv('../input/testing.csv')\n\n# X_train=training.drop(['class'],axis=1)\n# y_train=training['class']\n# clf=LogisticRegression(random_state=42)\n\n# clf.fit(X_train,y_train)\n# X_test=testing.drop(['class'],axis=1)\n# y_test=testing['class']\n\n# y_pred=clf.predict(X_test)\n# print(\"Classification Report:\")\n# print(sklearn.metrics.classification_report(y_test,y_pred))\n# print(\"Confusion Matrix\")\n# print(sklearn.metrics.confusion_matrix(y_test,y_pred))\n# print(\"Accuracy Score: \",sklearn.metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"multiclass roc\nhttps://stackoverflow.com/questions/45332410/sklearn-roc-for-multiclass-classification#45335434"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport sklearn\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# training=pd.read_csv('../input/training.csv')\n# test=pd.read_csv('../input/testing.csv')\n\ntrain = train.as_data_frame() # convert h20 training frame to pandas frame\ntest = test.as_data_frame() # convert h20 testing frame to pandas frame\n\nX_train=train.drop(['class'],axis=1)\ny_train=train['class']\nclf=LogisticRegression(random_state=42)\n\nclf.fit(X_train,y_train)\nX_test=test.drop(['class'],axis=1)\ny_test=test['class']\n\ny_pred=clf.predict(X_test)\nprint(\"Classification Report:\")\nprint(sklearn.metrics.classification_report(y_test,y_pred))\nprint(\"Confusion Matrix\")\nprint(sklearn.metrics.confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score: \",sklearn.metrics.accuracy_score(y_test,y_pred))\n# print(\"AUC: \", roc_auc_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importance = clf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plots\nDistribution graphs (histogram/bar graph) of sampled columns:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(train, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(train, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter and density plots:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df1, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check 2nd file: ../input/training.csv"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf2 = pd.read_csv('../input/training.csv', delimiter=',', nrows = nRowsRead)\ndf2.dataframeName = 'training.csv'\nnRow, nCol = df2.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df2['class'].unique())\nprint(df2.groupby(['class']).describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df2, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"b4\", y=\"class\", data=df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=\"b1\", y=\"b2\", data=df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corr_train = df2.drop(['class'], axis=1).corr(method='pearson').reset_index()\ncorr_train = df2.select_dtypes([np.number]).corr(method='pearson')\ncorr_train = corr_train.reset_index()\nprint(corr_train.head())\nprint('The shape of corr_train is:', corr_train.shape)\nprint(corr_train.info())\nprint(corr_train)\ncorr_train.to_csv('corr_train.csv', index = True)\n# hi_corr = corr_train.abs().unstack()\n# hi_corr = pd.DataFrame(hi_corr.sort_values(ascending = False))\n# print(hi_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n*to be added*"},{"metadata":{},"cell_type":"markdown","source":"## Archive / To be deleted"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n#     filename = df.dataframeName\n    filename = [x for x in globals() if globals()[x] is df][0]\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n#     plt.title(f'Correlation Matrix for %s' % df, fontsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter and density plots:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df2, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(df2, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}