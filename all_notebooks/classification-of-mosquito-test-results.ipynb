{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Hello everyone\n#### Thank you for viewing this kernel. With the use of various data preprocessing techniques as well as various machine learning algorithms, I was able to build various models in a bid to predict mosquitoes test results (negative or positive). The dataset used is \"Chicago West Nile Virus Mosquito Test Results\" maintained by kaggle.com. Have a look and share your thoughts on this. Thank you again!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\npd.set_option('display.max_rows', 10)\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fetching the data using pandas "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"wnvData = pd.read_csv(\"../input/west-nile-virus-wnv-mosquito-test-results.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the datatypes of the dataframe variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if there are missing values in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.isnull().values.any()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we know we do, let's see where they are and how many are present"},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fetch all the columns with object datatypes as well as drop the irrelvant columns in this mix"},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (wnvData.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nwnvData.drop( wnvData.columns [[3,12, 4, 6]], axis=1, inplace=True)\n\n#wnvData.drop( wnvData.columns [\"BLOCK, \"LOCATION\", 'TRAP', 'TEST DATE'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (wnvData.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the use of LabelEncoder, we can transform all the object datatype into a categorical variables says Result from Negative to 0 and Positive to 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing \nle = preprocessing.LabelEncoder()\n\nle.fit(wnvData['RESULT'])\nwnvData['RESULT'] = le.transform(wnvData['RESULT'])\n\nle.fit(wnvData['TRAP_TYPE'])\nwnvData['TRAP_TYPE'] = le.transform(wnvData['TRAP_TYPE'])\n\nle.fit(wnvData['SPECIES'])\nwnvData['SPECIES'] = le.transform(wnvData['SPECIES'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the use of heatmap, let's inspect the correlation between all the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_corr (wnvData, size =14):\n    corr =wnvData.corr()\n    fig, ax= plt.subplots(figsize =(size,size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns)     #draw x tick marks\n    plt.yticks(range(len(corr.columns)), corr.columns) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_corr(wnvData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap, there is a high correlation between SeasonYear and Test ID. we will drop Test ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData.drop( wnvData.columns [[2]], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_corr(wnvData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnvData['RESULT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_col_name = [ 'SEASON YEAR', 'WEEK', 'SPECIES', 'TRAP_TYPE',\n       'NUMBER OF MOSQUITOES', 'Wards', 'Census Tracts', \n       'Community Areas', 'Historical Wards 2003-2015']\npredicted_class_name= [ 'RESULT']\n\nX = wnvData[feature_col_name].values\ny = wnvData[predicted_class_name].values\nsplit_test_size = 0.1\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=split_test_size, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's handle the missing values in the train and test data set by using SimpleImputer function. You can learn more by visiting this link https://www.kaggle.com/alexisbcook/missing-values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nX_train = my_imputer.fit_transform(X_train)\nX_test = my_imputer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing a function to automate fitting the classifiers and evaluate the algorithms\n\ndef classifier(model,train_independent,train_dependent,test_independent,true):\n    model.fit(train_independent,train_dependent)\n    prediction = model.predict(X_test)\n    print(classification_report(true,prediction))\n    \n    # Confusion Matrix plot\n    \n    cm = confusion_matrix(y_test,prediction)\n    fig= plot_confusion_matrix(conf_mat=cm,figsize=(4,4),cmap=plt.cm.Reds,hide_spines=True)\n    plt.title('Confusion Matrix',fontsize=14)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.grid('off')\n    plt.show()\n\n\n    # 10-fold Cross Validation\n    accuracies = cross_val_score(estimator= model,X= X_train,y=y_train,cv=10)\n    print(\"The average model accuracy score is : %s\" % \"{0:.2%}\".format(accuracies.mean()))\n    print(\"The average accuracy score standard deviation is : %s\" % \"{0:.3%}\".format(accuracies.std()))\n    \n    # Values of the ROC Curve as a probabilistic approach to classification\n    roc_predict = model.predict_proba(X_test)\n    roc_predict = [p[1] for p in roc_predict]\n    area = roc_auc_score(y_test,roc_predict)\n    float(area)\n    print (\"The area under the Reciver Operating Characteristic curve is: \", (round(area,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We will start the process by introducing LogisticRegression algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nclassifier(log_reg,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How about Decision Tree algorithm?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\nclassifier (dtree,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ..... Random Forest Please!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=50)\nclassifier(rfc,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNeighbors were invited!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=20)\nclassifier(knn, X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## .... Finally we will use XGBooster"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Importing the libraries for the XGBoost algorithm\n\nfrom xgboost import XGBClassifier\nxgb_classifier = XGBClassifier()\nclassifier(xgb_classifier,X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's make an attempt to cross validate our models"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nModel_Scores ={'Logistic Regression':{'10 Fold Cross Validation Score':\n                                      \"{0:.2%}\".format((cross_val_score(estimator= log_reg,X= X_train,y=y_train,cv=10)).mean()), \n                                      'Standard Deviation':\"{0:.2%}\".format((cross_val_score(estimator= log_reg,X= X_train,y=y_train,cv=10)).std())},\n                'Decision Trees':{'10 Fold Cross Validation Score':\"{0:.2%}\".format((cross_val_score(estimator= dtree,X= X_train,y=y_train,cv=10)).mean()),\n                                     'Standard Deviation':\"{0:.2%}\".format(((cross_val_score(estimator= dtree,X= X_train,y=y_train,cv=10)).std()))},\n               'Random Forest':{'10 Fold Cross Validation Score':\"74.86%\", \n                                 'Standard Deviation':\"5.85%\"},\n               'K Nearest Neighbors':{'10 Fold Cross Validation Score':\"{0:.2%}\".format((cross_val_score(estimator= knn,X= X_train,y=y_train,cv=10)).mean()), \n                                       'Standard Deviation':\"{0:.2%}\".format(((cross_val_score(estimator= knn,X= X_train,y=y_train,cv=10)).std()))},\n               'XGBoost':{'10 Fold Cross Validation Score':\"{0:.2%}\".format((cross_val_score(estimator= xgb_classifier,X= X_train,y=y_train,cv=10)).mean()), \n                           'Standard Deviation':\"{0:.2%}\".format(((cross_val_score(estimator= xgb_classifier,X= X_train,y=y_train,cv=10)).std()))}\n              }\nModel_Scores= pd.DataFrame(Model_Scores)\nModel_Scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\n\nIt looks like most of these algorithms performed performed similarly when comparing the 10 fold cross validation accuracy scores, with the exceptions being the Decision Trees and Random forest algorithms achieving 80.03% and 74.86% accuracy score respectively which is lower then the rest of the algortims. Other techniques will be used to see if the parameters for some of these algorithms can be improved.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}