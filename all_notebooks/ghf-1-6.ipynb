{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hello and Welcome to the start of Ayan's Attempt to Geofferey Hinton's Fellowship\n\nI am honored to participate in such an event and present my work.\n\nLet's begin with the notebook then.","metadata":{}},{"cell_type":"markdown","source":"# Contents\n## 1. [Libraries](#libraries)\n## 2. [Data Visualization](#data_visualization)\n## 3. [Feature Engineering](#feature_engineering)\n## 4. [Data Modelling](#data_modelling)\n## 5. [Output](#output)\n","metadata":{}},{"cell_type":"markdown","source":"# Libraries<a id='libraries'></a>\nLet's start with all the imports and get done with the boring part","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns',None)\n\nimport re\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import OrdinalEncoder # Not Needed Right now\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport optuna\nimport lightgbm\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 2)\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization<a id='data_visualization'></a>\n## Let's look with what data we will be working with in this Problem","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/ghf1-hackathon/Training Data.csv')\ntest=pd.read_csv('/kaggle/input/ghf1-hackathon/Test Data.csv')\nsub=pd.read_csv('/kaggle/input/ghf1-hackathon/Sample Prediction Dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## So the data has 2.5 lakh entires, that's a lot of credit card data\n\n## Let's see what is the distribution of the target column in this data","metadata":{}},{"cell_type":"code","source":"train['risk_flag'].value_counts()/len(train)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looks like the data is highly skewed.\n\n## Check for null values in train and test set","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## So no null values, the next step is to go for getting info from train set\n\n## As see if we can find anything solid","metadata":{}},{"cell_type":"code","source":"train.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Id column looks a bit redundant here, dropping it will be the best thing to do","metadata":{}},{"cell_type":"code","source":"total_columns=['age', 'experience', 'married', 'house_ownership','car_ownership', 'profession', 'city',\n               'state', 'current_job_years','current_house_years',]\n\nfor i in total_columns:\n    print(f'Column :{i} New Elements :{set(test[i].unique())-set(train[i].unique())}')\n    print(f'Number of New Elements {len(set(test[i].unique())-set(train[i].unique()))}')\n    print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is interesting, we can see that we have a lot of entries with '-' , 'upper case' , '_' or anything else creating a new entry\n\n### Before we proceed any further, let's fix this error","metadata":{}},{"cell_type":"code","source":"def convert_profession(x):\n    prof = x\n    prof = re.sub(r'\\-',' ',prof)\n    prof = re.sub(r'\\_',' ',prof)\n    prof = re.sub(r'\\[[0-9]\\]',' ',prof)\n    prof = prof.split()\n    prof = ' '.join(prof)\n    \n    prof = prof.lower()\n    return prof\n\ndef convert_state(x):\n    state = x\n    state = re.sub(r'\\-',' ',state)\n    state = re.sub(r'\\_',' ',state)\n    state = re.sub(r'\\[[0-9]\\]',' ',state)\n    state = state.split()\n    state = ' '.join(state)\n    \n    state = state.lower()\n    return state\n\ndef convert_city(x):\n    city = x\n    city = re.sub(r'\\-',' ',city)\n    city = re.sub(r'\\_',' ',city)\n    city = re.sub(r'\\[[0-9][0-9]*\\]',' ',city)\n    city = city.split()\n    city = ' '.join(city)\n    \n    city = city.lower()\n    return city\n\ntrain['profession'] = train['profession'].apply(convert_profession)\ntest['profession'] = test['profession'].apply(convert_profession)\n\ntrain['state'] = train['state'].apply(convert_state)\ntest['state'] = test['state'].apply(convert_state)\n\ntrain['city'] = train['city'].apply(convert_city)\ntest['city'] = test['city'].apply(convert_city)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confirm it for surety, sice being extra carefull is never bad ","metadata":{}},{"cell_type":"code","source":"total_columns=['age', 'experience', 'married', 'house_ownership','car_ownership', 'profession', 'city',\n               'state', 'current_job_years','current_house_years',]\n\nfor i in total_columns:\n    print(f'Column :{i} New Elements :{set(test[i].unique())-set(train[i].unique())}')\n    print(f'Number of New Elements {len(set(test[i].unique())-set(train[i].unique()))}')\n    print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Done and Done\n\n### Moving on now, some basic visualisations for inferences ","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['income'],bins=1000,norm_hist=False,kde=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train['age'],kde=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train['experience'],y=train['age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='experience',y='age',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='experience',y='income',hue='married',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looks like married people are paid less as they gather experience \n\n### TAKE THAT SOCIETY AND SOCIAL NORMS!","metadata":{}},{"cell_type":"code","source":"sns.barplot(x='house_ownership',y='income',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('experience')['age'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now, let's be honest with ourselves \n\n### This looks very beautiful but not a lot can actually be inferred from it, so let's jump right into feature engineering ","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering <a id='feature_engineering'></a>\n\n### Time to get our hands dirty and play with the data \n\n### First things first let's check for duplicate entries","metadata":{}},{"cell_type":"code","source":"train.drop(columns=['Id'],inplace=True)\ntest.drop(columns=['id'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For this question, as my model was becoming biased I made synthetic data using the available columns","metadata":{}},{"cell_type":"code","source":"cat_col = ['married','house_ownership','car_ownership','profession','city','state']\n\nfor col in cat_col:\n    for col1 in cat_col:\n        if col != col1:\n            train[col + '_+_'+col1] = train[col] + train[col1]\n            test[col + '_+_'+col1] = test[col] + test[col1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ended up dropping Profession, State and City because of their high VIF factor and dropping them gave me a better result","metadata":{}},{"cell_type":"code","source":"train.drop(columns=['profession','state','city'],inplace=True)\ntest.drop(columns=['profession','state','city'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_col=['house_ownership','married','car_ownership',\n       'married_+_house_ownership', 'married_+_car_ownership','married_+_profession', 'married_+_city',\n    'married_+_state','house_ownership_+_car_ownership','house_ownership_+_profession',\n    'house_ownership_+_city','house_ownership_+_state', 'car_ownership_+_profession','car_ownership_+_city',\n    'car_ownership_+_state', 'profession_+_city', 'profession_+_state','city_+_state', \n        ]\ndrop_cols_cat=['house_ownership_+_married','car_ownership_+_married','profession_+_married','city_+_married',\n           'state_+_married','car_ownership_+_house_ownership','profession_+_house_ownership',\n           'city_+_house_ownership','state_+_house_ownership','profession_+_car_ownership',\n           'city_+_car_ownership','state_+_car_ownership','city_+_profession','state_+_profession',\n           'state_+_city']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing the Duplicate entries","metadata":{}},{"cell_type":"code","source":"train.drop(columns=drop_cols_cat,inplace=True)\ntest.drop(columns=drop_cols_cat,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Doing the same for numerical variables ","metadata":{}},{"cell_type":"code","source":"num_copy = train.select_dtypes(exclude='object')\nnum_copy = num_copy.drop(['risk_flag'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_copy.columns:\n    for col1 in num_copy.columns:\n        if col != col1:\n            train[col + '_add_' + col1] = train[col] + train[col1]\n            train[col + '_minus_' + col1] = train[col] - train[col1]\n            test[col + '_add_' + col1] = test[col] + test[col1]\n            test[col + '_minus_' + col1] = test[col] - test[col1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols=['income', 'age', 'experience', 'current_job_years',\n       'current_house_years', 'income_add_age',\n       'income_minus_age', 'income_add_experience', 'income_minus_experience',\n       'income_add_current_job_years', 'income_minus_current_job_years',\n       'income_add_current_house_years', 'income_minus_current_house_years',\n       'age_add_experience',\n       'age_minus_experience', 'age_add_current_job_years',\n       'age_minus_current_job_years', 'age_add_current_house_years','age_minus_current_house_years',\n       'experience_add_current_job_years','experience_minus_current_job_years',\n       'experience_add_current_house_years','experience_minus_current_house_years',  \n       'current_job_years_add_current_house_years','current_job_years_minus_current_house_years']\n\ndrop_num_cols=['age_add_income','age_minus_income','experience_add_income','experience_minus_income',\n           'current_job_years_add_income','current_job_years_minus_income','current_house_years_add_income',\n           'current_house_years_minus_income', 'experience_add_age', 'experience_minus_age',\n          'current_job_years_add_age','current_job_years_minus_age','current_house_years_add_age',\n          'current_house_years_minus_age','current_job_years_add_experience',\n          'current_job_years_minus_experience','current_house_years_add_experience',\n          'current_house_years_minus_experience','current_house_years_add_current_job_years',\n          'current_house_years_minus_current_job_years']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=drop_num_cols,inplace=True)\ntest.drop(columns=drop_num_cols,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns[train.dtypes=='object']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols=['married', 'house_ownership', 'car_ownership',\n       'married_+_house_ownership', 'married_+_car_ownership',\n       'married_+_profession', 'married_+_city', 'married_+_state',\n       'house_ownership_+_car_ownership', 'house_ownership_+_profession',\n       'house_ownership_+_city', 'house_ownership_+_state',\n       'car_ownership_+_profession', 'car_ownership_+_city',\n       'car_ownership_+_state', 'profession_+_city', 'profession_+_state',\n       'city_+_state']\n\n# loo = LeaveOneOutEncoder()\n# train[cat_cols] = loo.fit_transform(train[cat_cols],train['risk_flag'])\n# test[cat_cols] = loo.transform(test[cat_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.drop_duplicates(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_drop = ['married','car_ownership','house_ownership','married_+_car_ownership',\n                'married_+_house_ownership','house_ownership_+_car_ownership','married_+_state',\n               'car_ownership_+_state','married_+_profession']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.drop(columns=cols_to_drop,inplace=True)\n# test.drop(columns=cols_to_drop,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# next_drop=['married_+_profession']\n\n# train.drop(columns=next_drop,inplace=True)\n# test.drop(columns=next_drop,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X=train.drop(columns=['risk_flag'])\n# vif_data = pd.DataFrame()\n# vif_data[\"feature\"] = X.columns\n  \n# # calculating VIF for each feature\n# vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n#                           for i in range(len(X.columns))]\n  \n# print(vif_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns[train.dtypes!='object']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing Standard Scaler on numerical columns so that they are all within the same range","metadata":{}},{"cell_type":"code","source":"nums = ['income', 'age', 'experience', 'current_job_years',\n       'current_house_years', 'income_add_age',\n       'income_minus_age', 'income_add_experience', 'income_minus_experience',\n       'income_add_current_job_years', 'income_minus_current_job_years',\n       'income_add_current_house_years', 'income_minus_current_house_years',\n       'age_add_experience', 'age_minus_experience',\n       'age_add_current_job_years', 'age_minus_current_job_years',\n       'age_add_current_house_years', 'age_minus_current_house_years',\n       'experience_add_current_job_years',\n       'experience_minus_current_job_years',\n       'experience_add_current_house_years',\n       'experience_minus_current_house_years',\n       'current_job_years_add_current_house_years',\n       'current_job_years_minus_current_house_years']\n\nsc = StandardScaler()\ntrain[nums] = sc.fit_transform(train[nums])\ntest[nums] = sc.transform(test[nums])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlated_features = set()\n# correlation_matrix = train.drop('risk_flag', axis=1).corr()\n\n# for i in range(len(correlation_matrix.columns)):\n#     for j in range(i):\n#         if abs(correlation_matrix.iloc[i, j]) > 0.8:\n#             colname = correlation_matrix.columns[i]\n#             correlated_features.add(colname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlated_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.drop(columns=correlated_features,inplace=True)\n# test.drop(columns=correlated_features,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features=['income', 'age', 'experience', 'married', 'house_ownership',\n       'car_ownership', 'current_job_years', 'current_house_years',\n       'married_+_house_ownership', 'married_+_car_ownership',\n       'married_+_profession', 'married_+_city', 'married_+_state',\n       'house_ownership_+_car_ownership', 'house_ownership_+_profession',\n       'house_ownership_+_city', 'house_ownership_+_state',\n       'car_ownership_+_profession', 'car_ownership_+_city',\n       'car_ownership_+_state', 'profession_+_city', 'profession_+_state',\n       'city_+_state', 'income_add_age', 'income_minus_age',\n       'income_add_experience', 'income_minus_experience',\n       'income_add_current_job_years', 'income_minus_current_job_years',\n       'income_add_current_house_years', 'income_minus_current_house_years',\n       'age_add_experience', 'age_minus_experience',\n       'age_add_current_job_years', 'age_minus_current_job_years',\n       'age_add_current_house_years', 'age_minus_current_house_years',\n       'experience_add_current_job_years',\n       'experience_minus_current_job_years',\n       'experience_add_current_house_years',\n       'experience_minus_current_house_years',\n       'current_job_years_add_current_house_years',\n       'current_job_years_minus_current_house_years']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #import libraries\n# X=train.drop(columns=['risk_flag'])\n# # from sklearn.svm import SVC\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.feature_selection import RFECV\n# import matplotlib.pyplot as plt\n# #Fit the model\n# # svc = SVC(kernel=\"linear\")\n# lgb = XGBClassifier(eval_metric='auc')\n# rfecv = RFECV(estimator=lgb, step=1,min_features_to_select=20, cv=StratifiedKFold(5), scoring='roc_auc')\n# rfecv.fit(X, train['risk_flag'])\n# #Selected features\n# print(X.columns[rfecv.get_support()])\n# print(\"Optimal number of features : %d\" % rfecv.n_features_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X=train.drop(columns=['risk_flag'])\n# vif_data = pd.DataFrame()\n# vif_data[\"feature\"] = X.columns\n  \n# # calculating VIF for each feature\n# vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n#                           for i in range(len(X.columns))]\n  \n# print(vif_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features=['married', 'house_ownership', 'car_ownership', 'current_job_years',\n#        'current_house_years', 'married_+_house_ownership',\n#        'married_+_car_ownership', 'married_+_profession', 'married_+_city',\n#        'married_+_state', 'house_ownership_+_car_ownership',\n#        'house_ownership_+_state', 'car_ownership_+_profession',\n#        'car_ownership_+_state', 'income_minus_experience',\n#        'income_add_current_job_years', 'income_minus_current_job_years',\n#        'income_add_current_house_years', 'income_minus_current_house_years',\n#        'age_add_experience']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat_features=['married', 'house_ownership', 'car_ownership','married_+_house_ownership',\n#        'married_+_car_ownership', 'married_+_profession', 'married_+_city',\n#        'married_+_state', 'house_ownership_+_car_ownership',\n#        'house_ownership_+_state', 'car_ownership_+_profession',\n#        'car_ownership_+_state']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=train.drop(columns=['risk_flag'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l=[]\n# for i in X.columns:\n#     if i not in features:\n#         l.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X.drop(columns=l,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test.drop(columns=l,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features_indices = np.where(X.dtypes != np.float)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=X\ntarget=train['risk_flag']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## This was the final model which helped me secure a score of 0.74166 on the public leader-board ","metadata":{}},{"cell_type":"code","source":"# preds=np.zeros(test.shape[0])\n# oof_predictions=np.zeros(len(data))\n# kf=StratifiedKFold(n_splits=10,random_state=42,shuffle=True)\n# roc=[]\n# n=0\n# for trn_idx,val_idx in kf.split(data,target):\n#     train_x = data.iloc[trn_idx]\n#     train_y = target.iloc[trn_idx]\n#     val_x = data.iloc[val_idx]\n#     val_y = target.iloc[val_idx]\n    \n#     cb_model = CatBoostClassifier(iterations=5000, learning_rate=0.001,\n#                                   custom_metric=['AUC:hints=skip_train~false'], verbose=1000,\n# #                                   task_type='GPU'\n#                                  )\n\n#     cb_model.fit(train_x, train_y,eval_set=[(val_x,val_y)],\n#                      cat_features=categorical_features_indices)\n    \n# #     lgb_model = LGBMClassifier(\n# #         random_state=42,\n# #         cat_l2=25.999876242730252,\n# #         cat_smooth=89.2699690675538,\n# #         colsample_bytree=0.37861069156854954,\n# #         early_stopping_round=200,\n# #         learning_rate=0.015559547182527743,\n# #         max_bin=788,\n# #         max_depth=16,\n# #         metric=\"auc\",\n# #         min_data_per_group=177,\n# #         n_estimators=9482,\n# #         n_jobs=-1,\n# #         num_leaves=244,\n# #         reg_alpha=1.8401832133621817e-05,\n# #         reg_lambda=0.11900311978519747,\n# #         subsample=0.9753842843857927,\n# #         subsample_freq=1,\n# #         verbose=-1,\n# #     )\n# #     lgb_model.fit(train_x,train_y,eval_set=[(val_x,val_y)],early_stopping_rounds=100,verbose=False)\n#     preds+=cb_model.predict_proba(test[features])[:,1]/kf.n_splits\n#     oof_predictions += cb_model.predict_proba(data[features])[:,1]/kf.n_splits\n#     roc.append(roc_auc_score( val_y ,cb_model.predict_proba(val_x)[:,1]))\n#     print(f\"ROC {n+1}: {roc[n]}\")\n#     n=n+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred = cb_model.predict_proba(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cb_model.get_best_score()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub['risk_flag']=[1 if x>0.5 else 0 for x in pred[:,1]]\n# sub.to_csv('cb_45.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sum(sub['risk_flag'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature_imp = pd.DataFrame(sorted(zip(cb_model.feature_importances_,X.columns)), columns=['Value','Feature'])\n\n# plt.figure(figsize=(20, 20))\n# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n# plt.title('CatBoost Features (avg over folds)')\n# plt.tight_layout()\n# plt.show()\n# plt.savefig('lgbm_importances-01.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}