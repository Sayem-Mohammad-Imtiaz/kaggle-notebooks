{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Chargement des librairies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport pandas as pd\nimport numpy as np\n\n#Visualisation\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n#Modelisation\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,cross_validate,GridSearchCV,learning_curve,cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\n\n#Performances\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score,roc_curve,f1_score,precision_score,recall_score,make_scorer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fonctions de visualisation des métriques de performances"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Affichage de la matrice de confusion\ndef plot_confusion_matrix(cm:np.array,classes:tuple,\n                          normalize:bool = False,title:str ='Matrice de Confusion',\n                          cmap = plt.cm.YlGn):\n    '''\n        Fonction permettant d'afficher une matrice de confusion\n    '''\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    #print(cm)\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('Classes réelles')\n    plt.xlabel('Classes prédites')\n    plt.tight_layout()\n    \ndef plot_roc_curve(y_true: pd.Series,\n             y_proba: pd.Series,\n             y_pred: pd.Series) :\n    '''\n        Fonction permettant d'afficher les courbes Roc des deux classes à prédire\n    '''\n    col = ['r','b'] #Couleur des courbes\n    labels = ['Non embauché','embauché']\n    roc_auc = roc_auc_score(y_true, y_pred)\n    for i in range(len(col)):\n        #Calcul de la courbe ROC des deux classes\n        fpr, tpr, thr = roc_curve(y_true, y_proba[:,i], pos_label=i)\n        #Affichage des courbes avec la couleur correspondante et la classe\n        plt.plot(fpr, tpr,'-',c=col[i], label=labels[i])\n        print(f\"Moldèle (auc = {round(roc_auc,2)})\")\n    #Courbe ROC par défaut\n    plt.plot([0,1],[0,1],'k--', label='hasard') \n    plt.xlabel(f\"FPR Specificty\")\n    plt.ylabel(f\"TPR (Sensitivity)\")\n    plt.legend(loc=4)\n    plt.show()\n    \n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5),minus=True):\n    '''\n        Fonction permettant d'afficher la courbe d'apprentissage et de validation en fonction du nombre d'observations qui varient\n    '''\n    if axes is None:\n        _, axes = plt.subplots(1, 2, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    \n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True,shuffle=True,random_state=3)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n    \n    # Courbe d'apprentissage et de validation basé sur l'accuracy\n    axes[0].grid()\n    axes[0].fill_between(train_sizes,\n                            train_scores_mean + train_scores_std,\n                            train_scores_mean - train_scores_std,alpha=0.1,\n                            color=\"r\")\n    axes[0].fill_between(train_sizes,test_scores_mean + test_scores_std,\n                         test_scores_mean - test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n        \n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n    \n    # Courbe d'apprentissage et de validation basé sur 1 - l'accuracy\n    axes[1].set_title(title+' cost')\n    axes[1].grid()\n    axes[1].fill_between(train_sizes,\n                         1 - train_scores_mean + train_scores_std,\n                         1 - train_scores_mean - train_scores_std,alpha=0.1,\n                         color=\"r\")\n    axes[1].fill_between(train_sizes, 1 - test_scores_mean + test_scores_std,\n                         1 - test_scores_mean - test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[1].plot(train_sizes, 1 - train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[1].plot(train_sizes, 1 - test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[1].legend(loc=\"best\")\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"Cost = 1 - Score\")\n    #plt.show()\n        \n    df = pd.DataFrame({'train_sizes':train_sizes,\n                        'train_score':train_scores_mean,\n                        'train_std':train_scores_std,\n                        'test_score':test_scores_mean,\n                        'test_std':test_scores_std,\n                        'train_size_per':np.round(train_sizes/X.shape[0],2)})\n    \n    return plt,df\n\ndef plot_performances(df: pd.DataFrame,\n                      col: str,\n                      metric_name: str,\n                      color: str):\n    '''\n        Fonction permettant d'afficher un graphe avec l'ensemble des métriques calculées par modèle éxécutés\n    '''\n    bar_plot = go.Bar(y = df[col],\n                      x = df[metric_name],\n                      orientation = \"h\",\n                      name = metric_name.replace('test_',''),\n                      marker = dict(line = dict(width =.7),\n                                    color = color)\n                     )\n    return bar_plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fonctions d'évaluations des performances des modèles"},{"metadata":{"trusted":true},"cell_type":"code","source":"def precision_custom(y_true: pd.Series, y_pred:pd.Series):\n    '''\n        Définition d'une métrique de precision personnalisé\n    '''\n    with warnings.catch_warnings():\n        warnings.filterwarnings('error')\n        try: \n            pr = precision_score(y_true, y_pred, average='weighted')\n        except Warning:\n            # Cas ou l'estimateur ne prédiraient aucun candidat embauché donc se retrouverait\n            # à faire une division par 0 pour calculer la précision et le f1\n            pr = precision_score(y_true, y_pred, average='weighted',zero_division=1)\n        \n    return pr\n\ndef recall_custom(y_true: pd.Series, y_pred:pd.Series):\n    '''\n        Définition d'une métrique de rappel personnalisé\n    '''\n    with warnings.catch_warnings():\n        warnings.filterwarnings('error')\n        try :\n            re = recall_score(y_true, y_pred, average='weighted')\n        except Warning: \n            re = recall_score(y_true, y_pred, average='weighted',zero_division=1)\n        \n    return re\n\ndef f_custom(y_true: pd.Series, y_pred:pd.Series):\n    '''\n        Définition d'une métrique de f1_score personnalisé\n    '''\n    with warnings.catch_warnings():\n        warnings.filterwarnings('error')\n        try:\n            f_score = f1_score(y_true, y_pred, average='weighted')\n        except Warning:\n            f_score = f1_score(y_true, y_pred, average='weighted',zero_division=1)\n        \n    return f_score\n\n\ndef cross_val_monitor(model_tuple:tuple,\n                      X: pd.DataFrame,\n                      y: pd.Series,\n                     folds: int):\n    '''\n        Fonction permettant d'éxécuter des modèles en se basant sur la méthode de cross-validation\n    '''\n    \n    #Définition d'une grille de scoring custom\n    scoring_custom = {'precision': make_scorer(precision_custom), 'recall': make_scorer(recall_custom),\n           'f1_weighted': make_scorer(f_custom), 'accuracy': 'accuracy','roc_auc':'roc_auc'}\n    #Extraction du nom du modèle ainsi que de l'estimateur\n    model_name,model = model_tuple\n    print(f\"Cross-validation sur : {X.shape[0]} observations partagé en 5 partition avec le modèle : {model_name}\")\n    #Méthode de cross validation\n    scores = cross_validate(model,X,y,\n                            cv=folds,\n                            scoring=scoring_custom,\n                            return_train_score=True,\n                            verbose=0,error_score=1)\n    print(f\"\\n Affichage des scores: {model_name}\")\n    scoring = save_scores(scores,model_name)\n    title = f\"Courbe d'apprentissage et de validation : {model_name}\"\n    plot_learning_curve(model, title, X, y,\n                        cv=folds, n_jobs=1)\n    plt.show()\n    \n    print(\"Evaluation sur la totalité du jeu de données à l'aide de la fonction Cross val predict\")\n    model_pred = cross_val_predict(model,X,y)\n    conf = confusion_matrix(y,model_pred)\n    plot_confusion_matrix(conf,classes=['Non embauché','Embauché'])\n    print('\\n\\n\\n')\n    \n    return scoring\ndef save_scores(scores:dict,\n                model_name:str,\n                verbose:bool=True)->pd.DataFrame:\n    '''\n        Fonction permettant de sauvegarder les scores des différents classifiers éxécutés dans un dataframe\n    '''\n    if verbose:\n        print(f\"Scores training f1 weighted : {round(scores['train_f1_weighted'].mean(),3)} (+/- {round(scores['train_f1_weighted'].std(),3)})\")\n        print(f\"Scores test f1 weighted : {round(scores['test_f1_weighted'].mean(),3)} (+/- {round(scores['test_f1_weighted'].std(),3)})\")\n    \n        print(f\"Scores training accuracy : {round(scores['train_accuracy'].mean(),3)} (+/- {round(scores['train_accuracy'].std(),3)})\")\n        print(f\"Scores test accuracy : {round(scores['test_accuracy'].mean(),3)} (+/- {round(scores['test_accuracy'].std(),3)})\")\n    \n        print(f\"Scores training roc_auc : {round(scores['train_roc_auc'].mean(),3)} (+/- {round(scores['train_roc_auc'].std(),3)})\")\n        print(f\"Scores test roc_auc : {round(scores['test_roc_auc'].mean(),3)} (+/- {round(scores['test_roc_auc'].std(),3)})\")\n    \n        print(f\"Scores training precision : {round(scores['train_precision'].mean(),3)} (+/- {round(scores['train_precision'].std(),3)})\")\n        print(f\"Scores test precision : {round(scores['test_precision'].mean(),3)} (+/- {round(scores['test_precision'].std(),3)})\")\n    \n        print(f\"Scores training recall : {round(scores['train_recall'].mean(),3)} (+/- {round(scores['train_recall'].std(),3)})\")\n        print(f\"Scores test recall : {round(scores['test_recall'].mean(),3)} (+/- {round(scores['test_recall'].std(),3)})\")\n    \n    return pd.DataFrame({'test_f1_weighted':round(scores['test_f1_weighted'].mean(),2),\n                       'test_f1_weighted_std':round(scores['test_f1_weighted'].std(),3),\n                       'test_roc_auc':round(scores['test_roc_auc'].mean(),2),\n                       'test_roc_auc_std':round(scores['test_roc_auc'].std(),3),\n                       'test_precision':round(scores['test_precision'].mean(),2),\n                       'test_precision_std':round(scores['test_precision'].std(),3),\n                       'test_recall':round(scores['test_recall'].mean(),2),\n                       'test_recall_std':round(scores['test_recall'].std(),3),\n                       'test_accuracy':round(scores['test_accuracy'].mean(),2),\n                       'test_accuracy_std':round(scores['test_accuracy'].std(),3),\n                        'Model_name':pd.Series(model_name)})\n\n\ndef compute_performances(lstClf: list,X_train: pd.Series,\n                         y_train: pd.Series,X_test: pd.Series,y_test: pd.Series):\n    '''\n        Fonction permettant de run une liste de classifieurs et de les évaluer selon certaines métriques de performances\n    '''\n    for model_name, clf in lstClf:\n        clf.fit(X_train,y_train)\n        print(\"\\n\" + model_name)\n        y_pred = clf.predict(X_test)\n        conf = confusion_matrix(y_test, y_pred)\n        try:\n            y_proba = clf.predict_proba(X_test)\n            plot_roc_curve(y_true=y_test,y_proba=y_proba,y_pred=y_pred)\n        except:\n            print(f\"L'estimateur n'a pas de probabilités\\n\")\n        print (\"Matrice de confusion :\\n\")\n        plot_confusion_matrix(conf,classes=['Non Embauché','Embauché'])\n        plt.show()\n        print (\"Resultats :\\n\")\n        print (classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dans ce script nous allons entreprendre la partie modélisation du problème (Machine Learning)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/preprocessing-output/data_prepro.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comme nous l'avons vu, les classes que nous cherchons à prédire sont non balancées (Classe non-embauchés : 88% / Classe embauchés 11%) </br>\nCela pourrait induire en erreur les modèles de ML du fait qu'ils risquent de prédire tout candidats comme un candidat non embauché. </br>\nCependant certains algorithmes comme ceux basés sur des arbres de décisions ont cette capacité à pouvoir prédire des classes minoritairement représentées. </br>  Il existe notamment des méthodes de rééchantillonnage du jeu de données mais nous y reviendrons."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['embauche'],axis=1)\ny = data.embauche.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Méthodologie\n\nPour ce type de problématique, se baser uniquement sur un seul modèle serait une erreur car on pourrait potentiellement passer à coté</br> \ndes modèles ayant des meilleures performances sur nos données. C'est pour cela que quelques modèles seront exécutés comme suit :\n\n- Chaque modèle sera exécuté en utilisant la méthode de cross-validation. Cette méthode nous sera très utile afin de vérifier si notre modèle n'est pas en sur ou sous apprentissage. Mais en théorie, dans notre cas nous seront plus confrontés à des cas de surapprentissage due à la faible représentation de la classe des candidats embauchés.\n- Pour chaque modèle, des métriques de performances (Précision, Recall, f1 score, roc_auc et l'accuracy) seront calculées et affichées.\nLe choix de ces métriques s’est basé sur notre problématique de jeu de données non balancé. \n- Pour chaque modèle, une matrice de confusion sera calculée en se basant sur la méthode de cross-validation à l'aide de la fonction cross val predict. Cependant cette méthode n'est pas appropriée pour quantifier l'erreur de généralisation de notre modèle du fait qu'elle exige au modèle d'apprendre et prédire sur l'ensemble des données. Cela permet d'avoir une idée de la classification de notre modèle. </br>\n"},{"metadata":{},"cell_type":"markdown","source":"#### Définition de notre stratégie de cross validation"},{"metadata":{},"cell_type":"markdown","source":"Cette stratégie nous permet de partitionner notre jeu de données en 5 partitions et de s'entraîner sur 4 partitions représentant 80% des données et évaluer le modèle sur 1 partition représentant 20% des données. Ce processus est répété 5 fois avec 5 partitions différentes. De plus, chacune de ces partitions est stratifiée, c'est à dire qu’elles contiennent en termes de pourcentage environ le même échantillonnage que nos classes soient (88/11)."},{"metadata":{"trusted":true},"cell_type":"code","source":"skfolds = StratifiedKFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exemple de réflexion et de choix réalisé sur le paramétrage des algorithmes"},{"metadata":{},"cell_type":"markdown","source":"Dans un premier temps nous testons une simple régression logistique, avec les hyperparamètres par défaut de sklearn. </br>\nRemarques : \n- Le modèle a prédit l'ensemble des observations comme étant des candidats non embauchés. \n- Les mesures de précision, recall et f1 sont erronées, car la valeur du paramètre \"zero_division\" est fixé à 1. En faisant cela, toute division par 0 n'est pas soulevée comme un avertissement.\n- Les graphique de gauche et de droite nous montre les courbes d'apprentissage et de validation du modèle. Celle de gauche nous montre le taux de bonne prédiction en fonction des différentes tailles de partitions sur lesquelles notre modèle s'entraine. Tandis que le graphe de droite nous montre le taux d'erreur (1 - accuracy). Certains sont plus à l'aise avec la lecture du graphique de droite c'est le plus représenté lorsque l'on parle de courbe d'apprentissage."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=2)\nLr_tuple = (\"LR\",lr)\nlr_res = cross_val_monitor(Lr_tuple,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous sommes dans un cas de sur-apprentissage, il existe de nombreuses méthodes pour y palier. </br> \nL'une d'entre elle est de régulariser notre modèle. C'est à dire que nous allons contraindre notre modèle à trouver les valeurs des paramètres d'apprentissage dans un espace réduit. Ces mêmes paramètres qui dans notre cas tentent de maximiser le f1_score. </br> \nPour ce faire nous utilisons la méthode de gridsearch nous permettant de tester différentes combinaisons de valeurs de paramètres définit dans une grille."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [1.0,10.0,100.0],\n              'max_iter':[30,50,100],\n              'class_weight': ['balanced']\n             }\nrg_cv = GridSearchCV(LogisticRegression(), param_grid, cv=skfolds, scoring='f1_weighted',verbose=0)\nrg_cv.fit(X, y)\nprint(rg_cv.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\" le f1-score régularisé : {round(rg_cv.best_score_,2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remarques : \n- Après régularisation, nous obtenons un score f1 score de 63% ce qui dans un contexte de jeu de données non balancés n'est pas déplorable. Cependant en regardant l'accuracy , l'auc et le recall on s'aperçoit que les résultats sont très faible. Cela est surement dû au fait de notre régularisation sur le paramètre \"class_weight\" qui a pour but d'attribuer un poids plus lourd dans la prédiction de la classe minoritairement représentée. Dans notre cas les poids sont de : 0.56 pour la classe des candidats non embauchées et de 4.30 pour la classe des candidats embauchés.\n- Nous voyons très bien, l'agissement de ce paramètre sur la matrice de confusion ou le nombre de FP est considérablement élevé. \n- Lorsqu'on regarde les courbes d'apprentissage elles traduisent un sous apprentissage évident. Du fait que le biais est très élevé environ 44%. Rajouter de la donnée n'est pas une solution car l'on voit que les deux courbes convergent à la dernière itération. Il faudrait soit alléger la régularisation de par un choix de poids de classe empiriques (class_weight) et/ou jouer sur les autres paramètres ou choisir un modèle plus complexe."},{"metadata":{"trusted":true},"cell_type":"code","source":"Lr_tuple = (\"LR\",rg_cv.best_estimator_)\nlr_res = cross_val_monitor(Lr_tuple,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scikit learn ne fournit pas beaucoup d'informations en sorties d'une régression logistique. </br>\nLes coefficients ne sont donc pas très utiles car nous ne savons pas s'ils sont significatifs dans la prédiction d'un candidat embauché ou non. (Non présence de la p-value) </br>\nNous pouvons juste émettre l'hypothèse que si le coefficient de la variable est positif alors elle pousse notre modèle à prédire que notre candidat est embauché</br>\nUne alternative consiste à utiliser la fonction logit du package stats.models qui fournit plus de sorties ou utiliser des packages aidant à interpréter les sorties de modèle de ML tels que lime,eli5 ou shap.</br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's interpret the results\nlr_coef = pd.DataFrame(np.concatenate([Lr_tuple[1].intercept_.reshape(-1,1),\n                             Lr_tuple[1].coef_],axis=1),\n             index = [\"coef\"],\n             columns = [\"constante\"]+list(X.columns)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_coef.sort_values(by='coef',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Choisissons un modèle plus complexe : L'arbre de décision"},{"metadata":{},"cell_type":"markdown","source":"Ici nous avons régularisé le modèle en pénalisant la profondeur de l'arbre. Plus l'arbre est profond plus il prend des décision complexe et risque de faire du sur apprentissage. Mais cela n'était pas suffisant car nous avions des résultats trop justes en ce qui concerne notre classe minoritaire."},{"metadata":{},"cell_type":"markdown","source":"Remarques : \n\n- Les courbes d'apprentissage et de validation sont bonnes. Nous voyons bien sur le graphique de droite qu'au fur et à mesure que la partition réservée à l'entrainement augmente l'erreur sur le jeu de validation et d'entrainement décroît. Il y a encore un légère sur-apprentissage (expliqué par la variance entre les deux courbes) et l'épaisseur de la courbe de validation nous montre que les résultats sont assez dispersés mais le modèle donne de bonnes performances. \n- Cependant lorsqu'on jette un œil sur la matrice de confusion, on voit qu'on a toujours du mal à classifier les candidats embauchés. Nous pourrions mieux faire en pénalisant le poids de la classe minoritaire mais cela aura un effet immédiat sur les performances."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dt_tuple = (\"DT\",tree.DecisionTreeClassifier(max_depth=8,min_samples_leaf=7,min_samples_split=50,random_state=3))\nDt_res = cross_val_monitor(Dt_tuple,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dans cet exemple nous avons ajouté une pénalité sur le poids des classes lors des prédictions. </br> \nAu final nous perdons en précision car nous avons plus de FP, nos métriques de performances ont aussi baissés et nous observons que les résultats varient fortement ce qui montre une instabilité quant aux prédictions du modèle."},{"metadata":{"trusted":true},"cell_type":"code","source":"Dt_regul = (\"DT\",tree.DecisionTreeClassifier(max_depth=8,min_samples_leaf=1,min_samples_split=10,class_weight='balanced',random_state=3))\nDt_res = cross_val_monitor(Dt_regul,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNN\nPlus nous réduisons le nombre de voisins 'n_neighbors' plus le modèle sur-apprend"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclf_knn = ('Knn',KNeighborsClassifier(n_neighbors=20))\nknn_res = cross_val_monitor(clf_knn,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuple = ('Rf',RandomForestClassifier(random_state=3,max_depth=9,min_samples_leaf=3,\n                                        min_samples_split=10,class_weight='balanced'))\nrf_res = cross_val_monitor(rf_tuple,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extra Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"et_tuple = ('ET',ExtraTreesClassifier(random_state=3,max_depth=12,min_samples_leaf=5,\n                                     min_samples_split=30,class_weight='balanced'))\net_res = cross_val_monitor(et_tuple,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb_tuple = ('Gb',GradientBoostingClassifier(random_state=4,max_depth=3))\ngb_res = cross_val_monitor(gb_tuple,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGboost\nscale_pos_weight = total_negative_examples / total_positive_examples </br>\nscale_pos_weight = 14856/1959 = 7.58"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxg_tuple = ('Xgb',xgb.XGBClassifier(random_state=19,scale_pos_weight=7.58))\nxg_res = cross_val_monitor(xg_tuple,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\nclf_ct = ('CatB',CatBoostClassifier(random_state=5,logging_level='Silent',depth= 3,l2_leaf_reg = 3,learning_rate = 0.04))\nct_res = cross_val_monitor(clf_ct,X,y,skfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ce tableau nous permet de résumer les performances de chacun des modèles précédemment exécutés. </br>\nLe choix va se faire sur le modèle qui maximise la classification des candidats embauchés</br>\nNous allons voir de manière plus détaillée ce qu'ils valent sur un jeu de données splitté en train et test."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.float_format', '{:,.3f}'.format)\ncm = sns.light_palette(\"green\", as_cmap=True)\ndf_res = pd.concat([lr_res,Dt_res,knn_res,rf_res,gb_res,xg_res,ct_res,et_res],axis=0).reset_index(drop=True)\ndf_res = df_res.sort_values(by='test_f1_weighted',ascending=False)\ns = df_res.style.background_gradient(cmap=cm)\ns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import chart_studio.plotly as py\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\n#initate plotly\npyoff.init_notebook_mode()\n\nlayout = go.Layout(dict(title = \"Performances des modèles\",\n                        plot_bgcolor = \"whitesmoke\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"Metriques\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        height = 900\n                       )\n                  )\n\n\nf1_score_w  = plot_performances(df_res,'Model_name',\"test_f1_weighted\",\"darkcyan\")\nroc_auc_s  = plot_performances(df_res,'Model_name','test_roc_auc',\"darkgreen\")\nprecision_s  = plot_performances(df_res,'Model_name','test_precision',\"rebeccapurple\")\nrecall_s  = plot_performances(df_res,'Model_name','test_recall',\"slategray\")\naccuracy_s  = plot_performances(df_res,'Model_name','test_accuracy',\"chocolate\")\ndata = [f1_score_w,roc_auc_s,precision_s,recall_s,accuracy_s]\nfig = go.Figure(data=data,layout=layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split du jeu de données en train et test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,stratify=y,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_clf =  [clf_ct,Dt_tuple,gb_tuple,et_tuple,xg_tuple,clf_knn,rf_tuple]\n\ncompute_performances(lst_clf,\n            X_train,\n            y_train,\n            X_test,\n            y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### And the winner is ... Random Forest !\nCritère de choix du modèle :\n- Le choix du modèle s'est réalisé entre Catboost et Random Forest. Catboost à l'avantage d'avoir des prédictions stable et une courbe d'apprentissage et de validations qui laisseraient entendre qu'avec plus de données les courbes pourraients \"fitté\" et palier ce légère sur-apprentissage sur les données d'entrainement.\n- Intéressons nous aux performances des deux modèles sur la classe minoritaire. On s'aperçoit que Random Forest performe mieux à différents seuil de la courbe ROC (auc = 0.79) et qu'il a un meilleure score F1 (f1-score 0.52). De plus, il nous est possible d'améliorer ce score en trouvant le seuil optimal de décision pour la courbe de précision recall afin que le f1-score soit maximiser"},{"metadata":{},"cell_type":"markdown","source":"Pour cela nous allons tracer la courbe de precision-recall du modèle Random Forest. Avant cela il nous faut fitté notre modèle sur le jeu de données d'entrainement et prédire les probabilités d'appartenir à la classe embauché sur le jeu de données Test."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Modèle random forest\nclf_final = rf_tuple[1] \nclf_final.fit(X_train, y_train)\n#On ne retiens que les proba d'appartenir à la classe embauché\ny_proba = clf_final.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage de la courbe precision-recall du modèle Random Forest\ndefault = len(y_test[y_test==1]) / len(y_test)\nplt.plot(recall, precision, marker='.', label='Precision-Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calcul du f1 score à différents seuils\nfscore = (2 * precision * recall) / (precision + recall)\n# localisation de l'index au score f1 le plus élevé\nind_max = np.argmax(fscore)\nprint(f\"Seuil optimale = {round(thresholds[ind_max],3)}, F1-Score= {round(fscore[ind_max],3)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage de la courbe precision-recall du modèle Random Forest avec le point\nplt.plot(recall, precision, marker='.', label='Precision-Recall')\nplt.scatter(recall[ind_max], precision[ind_max], marker='o', color='black', label='Best',s=90)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous recalculons une matrice de confusion avec le seuil optimale et nous obtennons les résultats ci-dessous"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_final.fit(X_train, y_train)\n#On ne retiens que les proba d'appartenir à la classe embauché\ny_pred_bool = (clf_final.predict_proba(X_test)[:,1]>= thresholds[ind_max]).astype(bool)\ny_pred = np.where(y_pred_bool==True,1,0)\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = tp/(tp+fp)\nrappel = tp/(tp+fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"rappel classe embauché = {rappel}\\n précision classe embauché = {precision}\\n f1-score classe embauché = {2*((precision*rappel)/(precision+rappel))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recherchons les variables qui discriminent le plus nos candidats embauchés des non embauchés"},{"metadata":{},"cell_type":"markdown","source":"Il est possible à partir d'un arbre de décision de calculer dans quelle mesure chaque caractéristique contribue à discriminer nos classes. </br>\nScikit learn nous pourvoit la méthode \"features_importances\" pour le cas de Random Forest. \nRemarques : \n- Les variables Note, salaire et disponibilité sont celles qui discrimiment au mieux nos classes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = clf_final.fit(X_train, y_train)\nfeature_imp = pd.Series(model.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(10,6))\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Score importance des variables')\nplt.ylabel('Features')\nplt.title(\"Importance des variables\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Enregistrement du modèle Random Forest en prenant en compte le seuil optimisé"},{"metadata":{},"cell_type":"markdown","source":"#### Piste d'améliorations :\n- Utiliser des méthodes de réechantillonage. Ces méthodes d'undersampling ou d'oversampling permettent d'équilibrer les individus de chaque classes en y dupliquant des individus de la classe minoritaire pour le cas de l'oversampling ou en y supprimant des individus de la classe majoritaire pour le cas de l'undersampling. Mais il y a des méthodes plus avancés tels que SMOTE ou ADASYN qui permettent en autre de créer des données synthétiqueq basées sur des mesures de distances entre individus.\n- Passer plus de temps sur la recherche de paramètres, qui pourraient réajuster les prédictions de la classe minoritaire pour le modèle Random Forest\n- Utiliser des packages tels que LIME, ELI5 ou SHAP afin d'interprêter au mieux les résultats de notre modèle"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}