{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Content\n1. [Intoduction](#Intoduction)\n1. [Initializing](#Initializing)\n1. [Describing the data](#Describing-the-data)\n1. [Preprocessing the text](#Preprocessing-the-text)\n1. [Making ngram model](Making-ngram-model)\n1. [Making the generator](Making-the-generator)\n1. [Testing the generator](#Testing-the-generator)\n1. [Conclusion](#Conclusion)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Intoduction\nHello, here I whant to show you my attempt to generate covid tweets using **ngram models**, trained on [COVID19 Tweets](https://www.kaggle.com/gpreda/covid19-tweets) dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Initializing\nIn this project we will need\n* **pandas** - to read the data\n* **re** - to preprocess the text \n* **defaultdict** from collections - to create ngram models \n* **random** - to generate tweets\n\nLet's import packages and read csv:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\nfrom collections import defaultdict\nimport random\n\ndf = pd.read_csv('/kaggle/input/covid19-tweets/covid19_tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Describing the data\nThe dataset has information about tweets and their authors. It's author [Gabriel Preda](https://www.kaggle.com/gpreda) searched tweets with hashtags refering to covid.\n\nTo generate tweets we will need only \"text\" column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Helper function\nTo create ngram models we need to clean the sentences. Let's create function, which has **\"text\"** parameter and wich returns list of clean sentences.\n\nIt wil:\n1. lower the case\n1. delete links\n1. delete special symbols, e.g. %, ^, &\n1. delete line breaks\n1. remove unnecessary whitespaces\n1. split tweet text by punctuation marks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(text: str):\n    \"\"\"\n    \n    :param text: - tweet text\n    :returns: - list of preprocessed sentences of a tweet.\n    \"\"\"\n    \n    # lower capital letters\n    text = text.lower()\n    \n    # delete links\n    text = re.sub(r'https.+?', '', text)\n\n    \n    # delete everything except punctuation marks\n    text = re.sub(r'[^a-z !?.\\n]', '', text)\n    \n    # remove whitespace before punctuation mark/whitespace/end of line\n    text = re.sub(r' (\\?|\\!|\\.|\\n| |$)', r'\\1', text)\n\n    \n    # remove whitespace at the begining of the line or after punctuation mark\n    text = re.sub(r'(\\?|\\!|\\.|\\n|^) ', r'\\1', text)\n    \n    # spliting by puncuation mark or new line\n    texts = re.split('\\?|\\.|\\!|\\n', text)\n    \n    # deleting empty sentences\n    texts = [t for t in texts if t]\n    \n\n    return texts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test of preprocessing:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tweet = 'BiG, brother is 100% watching  you .Are You scared???'\npreprocess_text(test_tweet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating text corpus","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\nfor t in df['text']:\n    corpus.extend(preprocess_text(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in corpus[:10]:\n    print(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making ngram model\nSo, here we will make ngram model using default dict. It's a normal dict, but if you try to call undefined key from it, it defines it for you with the default value.\n\nThe keys of our ngram dict will be a tuple of tokens - list of N words, which go in their order.\n\nAlso i whanted to set some parameters:\n1. **N** - This will create ngrams of N size\n1. **min_count** - it will exclude such ngrams, which were found in text less than than min_count value\n1. **min_tokens** - it will add ngram only if there are more than *\"min_tokens\"* count of words in sentence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_ngrams(N: int, min_count: int = 5, min_tokens: int = 5):\n    \n    \"\"\"\n    :param N: ngram size\n    :param min_count: minimum acceptable count of ngram founds\n    :param min_tokens: minimum acceptable count of tokens in sentence\n    \n    :returns: Dict[Tuple[str], int] \n    \"\"\"\n    ngram = defaultdict(int)\n    \n    for line in corpus:\n        tokens = line.split()\n        \n        if len(tokens) > min_tokens:\n            for i in range(len(tokens) - N + 1):\n                ngram[tuple(tokens[i : i + N])] += 1\n                \n            ngram[tuple(['^'] + tokens[0 : N - 1])] += 1\n            \n            ngram[tuple(tokens[-N:-1] + ['$'])] += 1\n            \n    ngram = {key: value for key, value in ngram.items() if value > min_count}\n    \n    return ngram\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets create a trigram model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams = create_ngrams(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(trigrams.items())[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You might can see special symbols:\n* **^** - is used to mark the begining of a sentence \n* **\\$** - is used to mark the end of a sentence ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Making the generator","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's create function, which will give us the nex token (word) by previous_tokens.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_next_token(previous_tokens: tuple, ngrams: dict, method: str = 'random'):\n    \"\"\"\n    Returns the next token if found one\n    \n    :param previous_tokens: previous tokens\n    :param ngrams: ngrams - ngrams, where to search for the next token\n    :param method: method of search. \n        'random' - searches for the random token. \n        'weighted' - searches for random token, but token wich was found more times will have more probability to be searched\n        'most_common' - searches for the most common token\n        \n    :returns: str\n    \"\"\"\n    matching_ngrams = {key: value for key, value in ngrams.items() if key[:2] == previous_tokens}\n    \n    if matching_ngrams:\n        \n        if method == 'random':\n            return random.choice(list(matching_ngrams.keys()))[-1]\n        \n        elif method == 'weighted':\n            return random.choices(list(matching_ngrams.keys()), weights=list(matching_ngrams.values()))[0][-1]\n        \n        elif method == 'most_common':\n            return sorted(matching_ngrams.items(), key=lambda x: x[1])[-1][0][-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_tokens = ('in', 'the')\n\nprint('random:')\nfor _ in range(3):\n    print('\\t', get_next_token(previous_tokens, trigrams, method='random'))\n    \nprint('weighted:')\nfor _ in range(3):\n    print('\\t', get_next_token(previous_tokens, trigrams, method='weighted'))\n\nprint('most_common:')\nfor _ in range(3):\n    print('\\t', get_next_token(previous_tokens, trigrams, method='most_common'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Starting the tweet\nNow, let's create function, which will start the tweet. It will search for ngrams starting with \"^\" symbol and return the ngram due to \"method\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_starter(ngrams, method='random'):\n    starters = {key: value for key, value in ngrams.items() if key[0] == '^'}\n\n    if method == 'random':\n        return random.choice(list(starters.keys()))\n    \n    elif method == 'weighted':\n        return random.choices(list(starters.keys()), weights=list(starters.values()))[0]\n    \n    elif method == 'most_common':\n        return sorted(starters.items(), key=lambda x: x[1])[-1][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('random:')\nfor _ in range(3):\n    print('\\t', get_starter(trigrams, method='random'))\n    \nprint('weighted:')\nfor _ in range(3):\n    print('\\t', get_starter(trigrams, method='weighted'))\n\nprint('most_common:')\nfor _ in range(3):\n    print('\\t', get_starter(trigrams, method='most_common'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a tweet\nNow, let's write a function, which will create a tweet. If the length of created tweet is less than \"min_length\", it regeneretes it untill everything is ok.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_tweet(ngrams, min_length: int = 10, starter_method='random', next_method='most_common'):\n    \n    N = len(list(ngrams.keys())[0])\n    \n    starter = get_starter(ngrams, starter_method)\n    tokens = list(starter)[1:]\n    \n    next_token = get_next_token(tuple(list(starter)[1:]), ngrams, next_method)\n    while next_token:\n        tokens.append(next_token)\n        \n        last_tokens = tuple(tokens[-N+1:])\n        next_token = get_next_token(last_tokens, ngrams, next_method)\n    \n    if len(tokens) < min_length or tokens[-1] != '$':\n        tweet = generate_tweet(ngrams, min_length, starter_method, next_method)\n    else:\n        tweet = ' '.join(tokens)\n    return tweet ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the generator","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here are some generated tweets:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(43)\ntweets = [generate_tweet(trigrams) for _ in range(10)]\nfor tweet in tweets:\n    print('*', tweet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look if your generated tweets have matches","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for tweet in tweets:\n    print(tweet)\n    for text in df['text']:\n        if tweet[:-1] in text:\n            print('\\t', text.replace('\\n', ' ').replace('\\t', ''))\n            \n    else:\n        print('\\t--No matches')\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe have created a text generator working with ngram models and trained it on covid tweets.\nThe result is not so overwhelming, there is a lack of sence in generated tweets. I think, the dataset is too small for this task and ngram model is not the thing we need.\nBut we had fun :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}