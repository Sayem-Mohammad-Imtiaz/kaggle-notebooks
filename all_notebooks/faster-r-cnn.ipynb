{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset,DataLoader\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom torchvision.transforms import transforms\nimport torchvision\nimport torch.nn as nn\nfrom math import sqrt\nfrom torchvision.ops import nms,roi_pool\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T18:17:56.257395Z","iopub.execute_input":"2021-07-10T18:17:56.257811Z","iopub.status.idle":"2021-07-10T18:17:56.630103Z","shell.execute_reply.started":"2021-07-10T18:17:56.257773Z","shell.execute_reply":"2021-07-10T18:17:56.62881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VOC_dataset(Dataset):\n    def __init__(self,file_dir,img_dir,label_dir,img_size,transforms=None):\n        #file_dir - Path of the file containing 2 columns\n        #1st - image file name, 2nd - Label file name\n        self.annotations=pd.read_csv(file_dir)\n        #img_dir - Path of the images folder\n        self.img_dir=img_dir\n        #label_dir - Path of the labels folder\n        #Each label file contains class_number, x_center, y_center, width, height\n        self.label_dir=label_dir\n        self.img_size=img_size\n        self.transforms=transforms\n    def __getitem__(self,idx):\n        img=Image.open(path.join(self.img_dir,self.annotations.iloc[idx,0]))\n        if (self.transforms):\n            img=self.transforms(img)\n        labels_path=path.join(self.label_dir,self.annotations.iloc[idx,1])\n        file_ref=open(labels_path,\"r\")\n        final=[]\n        for lst in file_ref.readlines():\n            example=[]\n            for label in lst.strip().split(\" \"):\n                example.append(float(label))\n            example=[int(example[0])]+[item*self.img_size for item in example[1:]]\n            final.append(example)\n        return img,final\n    def __len__(self):\n        return 10\n        #return len(self.annotations)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:56.632074Z","iopub.execute_input":"2021-07-10T18:17:56.632498Z","iopub.status.idle":"2021-07-10T18:17:56.642817Z","shell.execute_reply.started":"2021-07-10T18:17:56.632461Z","shell.execute_reply":"2021-07-10T18:17:56.642146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_collate(batch):\n    #batch - A list of tuples representing (images,bboxes)\n    images=torch.stack([item[0] for item in batch])\n    bboxes=[item[1] for item in batch]\n    return [images,bboxes]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:56.644321Z","iopub.execute_input":"2021-07-10T18:17:56.644833Z","iopub.status.idle":"2021-07-10T18:17:56.65958Z","shell.execute_reply.started":"2021-07-10T18:17:56.644782Z","shell.execute_reply":"2021-07-10T18:17:56.658748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"settings={\n    \"img_size\": 800,\n    \"pre_nms_top_n\": 12000,\n    \"nms_threshold\": 0.7,\n    \"post_nms_top_n\": 2000,\n    \"pos_threshold\": 0.7,\n    \"neg_threshold\": 0.3,\n    \"num_RPN_samples\": 256,\n    \"fg_threshold\": 0.5,\n    \"bg_threshold_hi\": 0.5,\n    \"bg_threshold_lo\": 0.1,\n    \"num_classification_samples\": 1000, #Change\n    \"pool_size\": 7,\n    \"ss_ratio\": 16,\n    \"num_classes\": 20,\n    \"feature_map_height\": 50,\n    \"feature_map_width\": 50,\n    \"feature_map_depth\": 512,\n    \"anchor_ratios\": [0.5,1,2],\n    \"anchor_scales\": [8,16,32],\n    \"beta_rpn_loss\": 1, #Change\n    \"lambd_rpn_loss\": 1, #Change\n    \"beta_classifier_loss\": 1, #Change\n    \"lambd_classifier_loss\": 1, #Change\n    \"lr\": 0.001 #Change\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:56.660855Z","iopub.execute_input":"2021-07-10T18:17:56.661266Z","iopub.status.idle":"2021-07-10T18:17:56.67338Z","shell.execute_reply.started":"2021-07-10T18:17:56.661237Z","shell.execute_reply":"2021-07-10T18:17:56.672385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=1\nmean_list=[0.485,0.456,0.406]\nstd_list=[0.229, 0.224, 0.225]\n\ncomposed=transforms.Compose([transforms.ToTensor(),\n                             transforms.Normalize(mean_list,std_list),\n                             transforms.Resize((settings[\"img_size\"],settings[\"img_size\"]))])\n\ntrain_set=VOC_dataset(\"../input/pascalvoc-yolo/100examples.csv\",\n                      \"../input/pascalvoc-yolo/images\",\n                      \"../input/pascalvoc-yolo/labels\",\n                      settings[\"img_size\"],\n                      composed)\ntrain_loader=DataLoader(train_set,batch_size=batch_size,shuffle=False,collate_fn=my_collate)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:56.674998Z","iopub.execute_input":"2021-07-10T18:17:56.675621Z","iopub.status.idle":"2021-07-10T18:17:56.700306Z","shell.execute_reply.started":"2021-07-10T18:17:56.675572Z","shell.execute_reply":"2021-07-10T18:17:56.699219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_image(image,bboxes,mean_tensor,std_tensor,classes):\n    image=image*std_tensor+mean_tensor\n    np_image=np.transpose(np.array(image),(1,2,0))\n    plt.imshow(np_image)\n    ax=plt.gca()\n    #img_height,img_width,_=np_image.shape\n    for bbox in bboxes:\n        x_center,y_center,width,height=bbox[1],bbox[2],bbox[3],bbox[4]\n        x_tl,y_tl=(x_center-width/2),(y_center-height/2)\n        ax.add_patch(matplotlib.patches.Rectangle((x_tl,y_tl),width,height,linewidth=1,edgecolor='r',facecolor='none'))\n        plt.text(x_tl,y_tl,classes[int(bbox[0])],backgroundcolor='r')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:56.701808Z","iopub.execute_input":"2021-07-10T18:17:56.702251Z","iopub.status.idle":"2021-07-10T18:17:56.709328Z","shell.execute_reply.started":"2021-07-10T18:17:56.702215Z","shell.execute_reply":"2021-07-10T18:17:56.708256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check out the training data\n\nnum_classes=20\nmean_tensor=torch.tensor(mean_list).view(-1,1,1)\nstd_tensor=torch.tensor(std_list).view(-1,1,1)\nclasses=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n         'bus', 'car', 'cat', 'chair','cow',\n         'diningtable', 'dog', 'horse', 'motorbike', 'person',\n         'pottedplant','sheep', 'sofa', 'train', 'tvmonitor',\n         'background')\n\n#idx can have values [0,batch_size)\nidx=0\nexample=iter(train_loader).next()\nimage=example[0][idx]\nbboxes=example[1][idx]\nplot_image(image,bboxes,mean_tensor,std_tensor,classes)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:56.710539Z","iopub.execute_input":"2021-07-10T18:17:56.710935Z","iopub.status.idle":"2021-07-10T18:17:57.067767Z","shell.execute_reply.started":"2021-07-10T18:17:56.710905Z","shell.execute_reply":"2021-07-10T18:17:57.066807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anchors_loc(x,y,anchor_ratios,anchor_scales,ss_ratio):\n    '''\n        Inputs:\n            x,y - Coordinate from the feature map\n            anchor_ratios - A list of height/width ratios\n            anchor_scales - A list of scales\n            ss_ratio - Subsampling ratio, for VGG16 it is 16\n        Output:\n            anchors - len(anchor_scales)*len(anchor_ratios) x 4\n    '''\n    num_ratios=len(anchor_ratios)\n    s=ss_ratio//2\n    x_ctr=(2*x+1)*s\n    y_ctr=(2*y+1)*s\n    anchors=[]\n    for ratio_idx,ratio in enumerate(anchor_ratios):\n        for scale_idx,scale in enumerate(anchor_scales):\n            current=num_ratios*ratio_idx+scale_idx\n            h=ss_ratio*scale*sqrt(ratio)\n            w=ss_ratio*scale*sqrt(1/ratio)\n            anchors.append([x_ctr,y_ctr,w,h])\n    return torch.tensor(anchors)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.070261Z","iopub.execute_input":"2021-07-10T18:17:57.070573Z","iopub.status.idle":"2021-07-10T18:17:57.077416Z","shell.execute_reply.started":"2021-07-10T18:17:57.070534Z","shell.execute_reply":"2021-07-10T18:17:57.076718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anchor_generation(h,w,device,anchor_ratios=[0.5,1,2],anchor_scales=[8,16,32],ss_ratio=16):\n    '''\n        Inputs: \n            h,w - Height,width of the feature map\n            anchor_ratios - A list of height/width ratios\n            anchor_scales - A list of scales\n            ss_ratio - Subsampling ratio, for VGG16 it is 16\n        Output:\n            anchors - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n    '''\n    anchors=torch.zeros((h,w,len(anchor_ratios)*len(anchor_scales),4))\n    for y in range(h):\n        for x in range(w):\n            anchors[y,x]=anchors_loc(x,y,anchor_ratios,anchor_scales,ss_ratio)\n    return anchors.reshape(-1,4).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.078823Z","iopub.execute_input":"2021-07-10T18:17:57.079234Z","iopub.status.idle":"2021-07-10T18:17:57.094739Z","shell.execute_reply.started":"2021-07-10T18:17:57.079203Z","shell.execute_reply":"2021-07-10T18:17:57.093743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bbox_transform_inv(anchors,rpn_bboxes):\n    '''\n        Inputs:\n            anchors - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n            rpn_bboxes - Similar to anchors but contains regression coefficients\n        Output:\n            transformed bboxes - Target bboxes similar type and shape as that of anchors,\n                                 of the form [x_tl,y_tl,x_br,y_br]\n    '''\n    x_ctrs,delta_x=anchors[:,0],rpn_bboxes[:,0]\n    y_ctrs,delta_y=anchors[:,1],rpn_bboxes[:,1]\n    w,delta_w=anchors[:,2],rpn_bboxes[:,2]\n    h,delta_h=anchors[:,3],rpn_bboxes[:,3]\n    pred_x_ctrs=(delta_x*w+x_ctrs).unsqueeze(-1)\n    pred_y_ctrs=(delta_y*h+y_ctrs).unsqueeze(-1)\n    pred_w=(torch.exp(delta_w)*w).unsqueeze(-1)\n    pred_h=(torch.exp(delta_h)*h).unsqueeze(-1)\n    pred_x_tls=pred_x_ctrs-0.5*pred_w\n    pred_y_tls=pred_y_ctrs-0.5*pred_h\n    pred_x_brs=pred_x_tls+pred_w\n    pred_y_brs=pred_y_tls+pred_h\n    return torch.cat([pred_x_tls,pred_y_tls,pred_x_brs,pred_y_brs],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.096221Z","iopub.execute_input":"2021-07-10T18:17:57.096686Z","iopub.status.idle":"2021-07-10T18:17:57.112413Z","shell.execute_reply.started":"2021-07-10T18:17:57.096651Z","shell.execute_reply":"2021-07-10T18:17:57.111629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clip_bboxes(transformed,img_size,device):\n    '''\n        Inputs: \n            transformed - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n            img_size - A integer value representing width and height of the input image\n        Output:\n            Clipped_bboxes - Similar type and shape as that of transformed\n    '''\n    zero_tensor=torch.tensor([0],device=device)\n    size_tensor=torch.tensor([img_size],device=device)\n    transformed[:,0]=torch.maximum(zero_tensor,transformed[:,0])\n    transformed[:,1]=torch.maximum(zero_tensor,transformed[:,1])\n    transformed[:,2]=torch.minimum(size_tensor,transformed[:,2])\n    transformed[:,3]=torch.minimum(size_tensor,transformed[:,3])\n    return transformed","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.113432Z","iopub.execute_input":"2021-07-10T18:17:57.113833Z","iopub.status.idle":"2021-07-10T18:17:57.127545Z","shell.execute_reply.started":"2021-07-10T18:17:57.113803Z","shell.execute_reply":"2021-07-10T18:17:57.126795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def select_anchors(anchors,img_size):\n    '''\n     Inputs: \n        anchors - A tensor of shape (-1 x 4)\n        img_size - An integer\n    Outputs:\n        selected_anchors - A tensor of shape (-1 x 4)\n        a_idx - A tensor of shape [-1] \n            where,\n                Every value is the index of selected anchors\n    '''\n    a_idx=torch.where(torch.logical_and(torch.logical_and(anchors[:,0]>=0,anchors[:,1]>=0),torch.logical_and(anchors[:,2]<img_size,anchors[:,3]<img_size)))[0]\n    return anchors[a_idx,:],a_idx ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.128553Z","iopub.execute_input":"2021-07-10T18:17:57.128932Z","iopub.status.idle":"2021-07-10T18:17:57.147034Z","shell.execute_reply.started":"2021-07-10T18:17:57.128904Z","shell.execute_reply":"2021-07-10T18:17:57.146237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def proposal_layer(anchors,rpn_scores,rpn_bboxes,img_size,pre_nms_top_n,nms_threshold,post_nms_top_n,device,mode='train'):\n    '''\n        Inputs: \n            anchors - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n            rpn_scores - A tensor of shape (-1 x 1)\n            rpn_bboxes - Similar to anchors but contains regression coefficients\n            img_size - An integer representing width and height of the input image\n            pre_nms_top_n,nms_threshold,post_nms_top_n - Integers \n        Output:\n            scores - A tensor of shape (-1,)\n            proposals - A tensor of shape (-1 x 4)\n            \n    '''\n    transformed=bbox_transform_inv(anchors,rpn_bboxes)\n    #if (mode=='test'):\n    clipped_bboxes=clip_bboxes(transformed,img_size,device)\n    #else:\n    #    clipped_bboxes,selected_idx=select_anchors(transformed,img_size)\n    scores,indices=rpn_scores.view(-1).sort(descending=True)\n    #Keep only those scores and indices of the crops whose tl and br coordinates lie inside the image\n    #scores=scores[selected_idx]\n    #indices=indices[selected_idx]\n    indices=indices[:pre_nms_top_n]\n    scores=scores[:pre_nms_top_n]\n    bboxes=clipped_bboxes[indices,:]\n    keep=nms(bboxes,scores,nms_threshold)\n    m=min(keep.shape[0],post_nms_top_n)\n    keep=keep[:m]\n    proposals=bboxes[keep,:]\n    scores=scores[keep]\n    return scores,proposals ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.148089Z","iopub.execute_input":"2021-07-10T18:17:57.14855Z","iopub.status.idle":"2021-07-10T18:17:57.161414Z","shell.execute_reply.started":"2021-07-10T18:17:57.148518Z","shell.execute_reply":"2021-07-10T18:17:57.160189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tl_br(anchors):\n    '''\n        Inputs: \n            anchors - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n        Output:\n            anchors - Similar to \"input\" but each row represents x_tl,y_tl,x_br,y_br  \n    '''\n    x=anchors[:,0]\n    y=anchors[:,1]\n    w=anchors[:,2]\n    h=anchors[:,3]\n    x_tl=x-w/2\n    y_tl=y-h/2\n    x_br=x_tl+w\n    y_br=y_tl+h\n    return torch.cat([x_tl.unsqueeze(-1),y_tl.unsqueeze(-1),x_br.unsqueeze(-1),y_br.unsqueeze(-1)],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.163097Z","iopub.execute_input":"2021-07-10T18:17:57.16365Z","iopub.status.idle":"2021-07-10T18:17:57.178612Z","shell.execute_reply.started":"2021-07-10T18:17:57.163605Z","shell.execute_reply":"2021-07-10T18:17:57.177345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_iou_matrix(anchors,gt_bboxes,device):\n    '''\n        Inputs: \n            anchors - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n                    and each row in the tensor is of the form [x_tl,y_tl,x_br,y_br]\n            gt_bboxes - A list of lists\n                where,\n                    Every inner list is of the form [class_number,x_ctr,y_ctr,width,height]\n        Output:\n            iou_matrix - A tensor of shape (anchors.shape[0] x len(gt_bboxes))\n    '''\n    #anchors=tl_br(anchors)\n    a_x1=anchors[:,0]\n    a_y1=anchors[:,1]\n    a_x2=anchors[:,2]\n    a_y2=anchors[:,3]\n    iou_matrix=torch.zeros((anchors.shape[0],len(gt_bboxes)),device=device)\n    zero_tensor=torch.tensor([0],device=device)\n    for gt_idx,gt_bbox in enumerate(gt_bboxes):\n        _,gt_x_ctr,gt_y_ctr,gt_w,gt_h=gt_bbox\n        gt_x1=torch.tensor(gt_x_ctr-gt_w/2,device=device)\n        gt_y1=torch.tensor(gt_y_ctr-gt_h/2,device=device)\n        gt_x2=gt_x1+gt_w\n        gt_y2=gt_y1+gt_h\n        x1=torch.maximum(a_x1,gt_x1)\n        y1=torch.maximum(a_y1,gt_y1)\n        x2=torch.minimum(a_x2,gt_x2)\n        y2=torch.minimum(a_y2,gt_y2)\n        intersection=torch.maximum(zero_tensor,(x2-x1))*torch.maximum(zero_tensor,(y2-y1))\n        union=(a_x2-a_x1)*(a_y2-a_y1)+(gt_x2-gt_x1)*(gt_y2-gt_y1)-intersection\n        iou_matrix[:,gt_idx]=intersection/(union+1e-8)\n    return iou_matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.180176Z","iopub.execute_input":"2021-07-10T18:17:57.180536Z","iopub.status.idle":"2021-07-10T18:17:57.194558Z","shell.execute_reply.started":"2021-07-10T18:17:57.180482Z","shell.execute_reply":"2021-07-10T18:17:57.193616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_select_anchors(anchors,img_size):\n    '''\n        Converts to x_tl,y_tl,x_br,y_br then,\n        if x_tl,y_tl or x_br,y_br are outside the img_size, remove that particular anchor\n        Inputs: \n            anchors - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n            img_size - An integer\n        Outputs:\n            selected_anchors - A tensor of shape (-1 x 4)\n            a_idx - A tensor of shape [-1] \n                where,\n                    Every value is the index of selected anchors\n    '''\n    anchors=tl_br(anchors)\n    a_idx=torch.where(torch.logical_and(torch.logical_and(anchors[:,0]>=0,anchors[:,1]>=0),torch.logical_and(anchors[:,2]<img_size,anchors[:,3]<img_size)))[0]\n    return anchors[a_idx,:],a_idx","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.195749Z","iopub.execute_input":"2021-07-10T18:17:57.196064Z","iopub.status.idle":"2021-07-10T18:17:57.214539Z","shell.execute_reply.started":"2021-07-10T18:17:57.196032Z","shell.execute_reply":"2021-07-10T18:17:57.213531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anchor_target_layer(anchors,gt_bboxes,pos_threshold,neg_threshold,num_samples,img_size,rpn_scores,rpn_bboxes,device):\n    '''\n        Inputs: \n             anchors - A tensor of shape (h*w*num_anchors x 4)\n                where,\n                    num_anchors - len(anchor_ratios)*len(anchor_scales)\n            gt_bboxes - A list of lists\n                where,\n                    Every inner list is of the form [class_number,x_ctr,y_ctr,width,height]\n            pos_threshold - A floating point value for classifying anchor boxes as foreground\n            neg_threshold - A floating point value for classifying anchor boxes as background\n            num_samples - An integer that represents the size of the mini-batch\n            img_size - A integer that represents the width and height of the input image\n            rpn_scores - A tensor of shape (-1 x 1)\n        Outputs:\n            foreground_samples - A tensor of shape (-1 x 10)\n                where,\n                    Each row is of the form [1,foreground_prob,gt_tx,gt_ty,gt_tw,gt_th,tx,ty,tw,th]\n            background_samples - A tensor of shape (-1 x 2)\n                where,\n                        Each row is of the form [0,foreground_prob]\n    '''\n    #Convert list to tensor\n    tensor_gt=torch.tensor(gt_bboxes,device=device)\n    #num_class foreground and num_class background samples\n    num_class_samples=num_samples//2\n    #Remove anchors that are outside the image\n    s_anchors,s_idx=convert_select_anchors(anchors,img_size)\n    #Get rpn_scores only for the selected anchors\n    s_rpn_scores=rpn_scores[s_idx,:]\n    #Get rpn_bboxes only for selected anchors\n    s_rpn_bboxes=rpn_bboxes[s_idx,:]\n    #Create a frame to store class information and actual regression coefficients\n    actual=-1*torch.ones((s_anchors.shape[0],10),device=device)\n    #Get the overlap information of every selected anchor with every gt bbox\n    iou_matrix=get_iou_matrix(s_anchors,gt_bboxes,device)\n    #Find type A anchors\n    max_overlap_values=torch.max(iou_matrix,axis=0)[0].unsqueeze(0)\n    anchor_idx,gt_idx=torch.where(iou_matrix==max_overlap_values)\n    #Construct actual tensor for type A foregrounds\n    actual[anchor_idx,0]=1\n    actual[anchor_idx,1]=s_rpn_scores[anchor_idx,0]\n    actual[anchor_idx,2]=(tensor_gt[gt_idx,1]-s_anchors[anchor_idx,0])/s_anchors[anchor_idx,2]\n    actual[anchor_idx,3]=(tensor_gt[gt_idx,2]-s_anchors[anchor_idx,1])/s_anchors[anchor_idx,3]\n    actual[anchor_idx,4]=torch.log(tensor_gt[gt_idx,3]/s_anchors[anchor_idx,2])\n    actual[anchor_idx,5]=torch.log(tensor_gt[gt_idx,4]/s_anchors[anchor_idx,3])\n    actual[anchor_idx,6:]=s_rpn_bboxes[anchor_idx,:]\n    #Find type B anchors\n    s_anchor_overlaps=torch.max(iou_matrix,axis=1)[0].unsqueeze(-1)\n    anchor_idx,gt_idx=torch.where(torch.logical_and(s_anchor_overlaps==iou_matrix,s_anchor_overlaps>pos_threshold))\n    #Construct actual tensor for type B foregrounds\n    actual[anchor_idx,0]=1\n    actual[anchor_idx,1]=s_rpn_scores[anchor_idx,0]\n    actual[anchor_idx,2]=(tensor_gt[gt_idx,1]-s_anchors[anchor_idx,0])/s_anchors[anchor_idx,2]\n    actual[anchor_idx,3]=(tensor_gt[gt_idx,2]-s_anchors[anchor_idx,1])/s_anchors[anchor_idx,3]\n    actual[anchor_idx,4]=torch.log(tensor_gt[gt_idx,3]/s_anchors[anchor_idx,2])\n    actual[anchor_idx,5]=torch.log(tensor_gt[gt_idx,4]/s_anchors[anchor_idx,3])\n    actual[anchor_idx,6:]=s_rpn_bboxes[anchor_idx,:]\n    #Find background anchors\n    background_mask=(s_anchor_overlaps.squeeze(-1)<neg_threshold)\n    anchor_idx=torch.where(background_mask==True)[0]\n    #Construct actual tensor for backgrounds\n    actual[anchor_idx,0]=0\n    actual[anchor_idx,1]=s_rpn_scores[anchor_idx,0]\n    #Create indices for foreground and background samples\n    (pos_idx,)=torch.where(actual[:,0]==1)\n    (neg_idx,)=torch.where(actual[:,0]==0)\n    pos=min(pos_idx.shape[0],num_class_samples)\n    neg=min(neg_idx.shape[0],num_class_samples)\n    pos_idx=pos_idx[:pos]\n    neg_idx=neg_idx[:neg]\n    foreground_samples=actual[pos_idx,:]\n    background_samples=actual[neg_idx,:2]\n    return foreground_samples,background_samples","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.215956Z","iopub.execute_input":"2021-07-10T18:17:57.216578Z","iopub.status.idle":"2021-07-10T18:17:57.239181Z","shell.execute_reply.started":"2021-07-10T18:17:57.216531Z","shell.execute_reply":"2021-07-10T18:17:57.238103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nanchors=torch.tensor([[2,2,2,2],[3,3,2,2],[-3,2,2,2],[501,0,3,4],[4,6,2,2],[2,2,2,2]])\ngt_bboxes=[[0,2,2,2,2],[1,3,3,2,2]]\npos_threshold=0.7\nneg_threshold=0.2\nnum_samples=256\nimg_size=500\nrpn_scores=torch.tensor([[0.8],[0.44],[0.3],[0.2],[0.7],[0.9]])\nrpn_bboxes=torch.tensor([[1,2,3,4],[4,3,2,1],[9,8,7,6],[3,5,8,9],[6,7,8,9],[1,8,0,1]],dtype=torch.float)\nfs,bs=anchor_target_layer(anchors,gt_bboxes,pos_threshold,neg_threshold,num_samples,img_size,rpn_scores,rpn_bboxes,'cpu')\nprint(fs)\nprint(bs)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.240536Z","iopub.execute_input":"2021-07-10T18:17:57.240816Z","iopub.status.idle":"2021-07-10T18:17:57.255569Z","shell.execute_reply.started":"2021-07-10T18:17:57.24079Z","shell.execute_reply":"2021-07-10T18:17:57.254585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RPN(nn.Module):\n    def __init__(self,in_channels,num_anchors):\n        super().__init__()\n        self.conv=nn.Conv2d(in_channels,512,3,padding=1)\n        self.s1=nn.Conv2d(512,num_anchors,1)\n        self.s2=nn.Conv2d(512,num_anchors*4,1)\n        self.relu=nn.ReLU()\n    def forward(self,x):\n        x=self.relu(self.conv(x))\n        scores=torch.sigmoid(self.s1(x))\n        offsets=self.s2(x)\n        return scores,offsets","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.256845Z","iopub.execute_input":"2021-07-10T18:17:57.257308Z","iopub.status.idle":"2021-07-10T18:17:57.265905Z","shell.execute_reply.started":"2021-07-10T18:17:57.257269Z","shell.execute_reply":"2021-07-10T18:17:57.264944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    if (type(m)==nn.Conv2d):\n        nn.init.normal_(m.weight,mean=0,std=0.001)\n        nn.init.zeros_(m.bias)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.267367Z","iopub.execute_input":"2021-07-10T18:17:57.267773Z","iopub.status.idle":"2021-07-10T18:17:57.277475Z","shell.execute_reply.started":"2021-07-10T18:17:57.267741Z","shell.execute_reply":"2021-07-10T18:17:57.276321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_center(proposals):\n    x_tl=proposals[:,0]\n    y_tl=proposals[:,1]\n    x_br=proposals[:,2]\n    y_br=proposals[:,3]\n    x_ctr=((x_tl+x_br)/2).unsqueeze(-1)\n    y_ctr=((y_tl+y_br)/2).unsqueeze(-1)\n    w=(x_br-x_tl).unsqueeze(-1)\n    h=(y_br-y_tl).unsqueeze(-1)\n    return torch.cat([x_ctr,y_ctr,w,h],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.278761Z","iopub.execute_input":"2021-07-10T18:17:57.279069Z","iopub.status.idle":"2021-07-10T18:17:57.292962Z","shell.execute_reply.started":"2021-07-10T18:17:57.279031Z","shell.execute_reply":"2021-07-10T18:17:57.292243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def proposal_target_layer(proposals,gt_bboxes,fr_threshold,bg_threshold_hi,bg_threshold_lo,batch_size,num_classes,device):\n    '''\n        Inputs:\n            proposals - Transformed and clipped boxes of the form [x_tl,y_tl,x_br,y_br], \n                shape - (-1 x 4)\n             gt_bboxes - A list of lists\n                where,\n                    Every inner list is of the form [class_number,x_ctr,y_ctr,width,height]\n            fr_threshold - threshold for foreground samples (Default: 0.5)\n            bg_threshold_hi - Upper threshold for background samples (Default: 0.5)\n            bg_threshold_lo - Lower threshold for background samples (Default: 0.1)\n            batch_size - Number of samples to create for Classification network (Default: 128)\n            num_classes - An integer value\n    '''\n    num_class_samples=batch_size//2\n    gt_tensor=torch.tensor(gt_bboxes,dtype=torch.float,device=device)\n    samples=-1*torch.ones((proposals.shape[0],6),device=device)\n    iou_matrix=get_iou_matrix(proposals,gt_bboxes,device)\n    max_overlaps,gt_idx=torch.max(iou_matrix,axis=1)\n    #Find foreground samples\n    fg_idx=torch.where(max_overlaps>fr_threshold)[0]\n    samples[fg_idx,:]=0\n    samples[fg_idx,0]=1\n    associated_gts_fg=gt_idx[fg_idx]\n    samples[fg_idx,1]=gt_tensor[associated_gts_fg,0]\n    #find background samples\n    bg_idx=torch.where(torch.logical_and(max_overlaps<bg_threshold_hi,max_overlaps>bg_threshold_lo))[0]\n    samples[bg_idx,:]=0\n    associated_gts_bg=gt_idx[bg_idx]\n    samples[bg_idx,1]=num_classes\n    #Concatenate indices\n    indices=torch.cat([fg_idx,bg_idx],axis=0)\n    associated_gts=torch.cat([associated_gts_fg,associated_gts_bg],axis=0)\n    #Convert proposals\n    converted_proposals=convert_to_center(proposals)\n    #Calculate regression coefficients\n    samples[indices,2]=(gt_tensor[associated_gts,1]-converted_proposals[indices,0])/converted_proposals[indices,2]\n    samples[indices,3]=(gt_tensor[associated_gts,2]-converted_proposals[indices,1])/converted_proposals[indices,3]\n    samples[indices,4]=torch.log(gt_tensor[associated_gts,3]/converted_proposals[indices,2])\n    samples[indices,5]=torch.log(gt_tensor[associated_gts,4]/converted_proposals[indices,3])\n    #Create foreground and background samples\n    fg_samples_idx=torch.where(samples[:,0]==1)[0]\n    bg_samples_idx=torch.where(samples[:,0]==0)[0]\n    num_fgs=min(fg_samples_idx.shape[0],num_class_samples)\n    num_bgs=min(bg_samples_idx.shape[0],num_class_samples)\n    final_fgs_idx=fg_samples_idx[:num_fgs]\n    final_bgs_idx=bg_samples_idx[:num_bgs]\n    a=torch.cat([proposals[final_fgs_idx,:],proposals[final_bgs_idx,:]],axis=0)\n    b=torch.cat([torch.arange(end=a.shape[0],dtype=torch.float).view(-1,1),a],axis=1)\n    return torch.cat([samples[final_fgs_idx,1:],samples[final_bgs_idx,1:]],axis=0),b","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.294013Z","iopub.execute_input":"2021-07-10T18:17:57.294426Z","iopub.status.idle":"2021-07-10T18:17:57.311607Z","shell.execute_reply.started":"2021-07-10T18:17:57.294383Z","shell.execute_reply":"2021-07-10T18:17:57.310458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nproposals=torch.tensor([[1,1,3,3],[2,1,4,3],[5,1,6,3],[0,1,3,3]])\ngt_bboxes=[[1,2,2,2,2]]\nsamples,ROI=proposal_target_layer(proposals,gt_bboxes,0.5,0.5,0.1,128,20,'cpu')\nprint(samples)\nprint(ROI)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.317201Z","iopub.execute_input":"2021-07-10T18:17:57.31752Z","iopub.status.idle":"2021-07-10T18:17:57.32612Z","shell.execute_reply.started":"2021-07-10T18:17:57.317491Z","shell.execute_reply":"2021-07-10T18:17:57.324815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_with_mask(samples,num_classes,device):\n    '''\n        Input:\n            samples - A tensor of shape - (-1 x 5)\n        Outputs:\n            mask - A tensor of shape - (-1 x 80)\n            gts - A tensor of shape - (-1 x 81)\n                where,\n                    1st value is the class number and the other 80 are the actual regression coefficients\n    '''\n    class_idx=samples[:,0].type(torch.long)\n    num_rows=samples.shape[0]\n    gts=torch.zeros((num_rows,1+4*(num_classes+1)),device=device)\n    mask=torch.zeros((num_rows,4*(num_classes+1)),device=device)\n    gts[:,0]=class_idx\n    rows=torch.tensor(range(num_rows))\n    start_idx=class_idx*4\n    gts[rows,start_idx+1]=samples[:,1]\n    mask[rows,start_idx]=1\n    gts[rows,start_idx+2]=samples[:,2]\n    mask[rows,start_idx+1]=1\n    gts[rows,start_idx+3]=samples[:,3]\n    mask[rows,start_idx+2]=1\n    gts[rows,start_idx+4]=samples[:,4]\n    mask[rows,start_idx+3]=1\n    return gts[:,:4*num_classes+1],mask[:,:4*num_classes]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.328027Z","iopub.execute_input":"2021-07-10T18:17:57.32845Z","iopub.status.idle":"2021-07-10T18:17:57.338815Z","shell.execute_reply.started":"2021-07-10T18:17:57.328406Z","shell.execute_reply":"2021-07-10T18:17:57.337774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self,in_channels,pool_size,num_classes):\n        super().__init__()\n        self.flat=in_channels*pool_size*pool_size\n        self.lin1=nn.Linear(in_channels*pool_size*pool_size,4096)\n        self.lin2=nn.Linear(4096,4096)\n        self.relu=nn.ReLU()\n        self.score_layer=nn.Linear(4096,num_classes+1)\n        self.offset_layer=nn.Linear(4096,4*num_classes)\n    def forward(self,x):\n        x=x.view(-1,self.flat)\n        x=self.relu(self.lin1(x))\n        x=self.relu(self.lin2(x))\n        class_scores=self.score_layer(x)\n        offsets=self.offset_layer(x)\n        return class_scores,offsets","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.340253Z","iopub.execute_input":"2021-07-10T18:17:57.340636Z","iopub.status.idle":"2021-07-10T18:17:57.354122Z","shell.execute_reply.started":"2021-07-10T18:17:57.3406Z","shell.execute_reply":"2021-07-10T18:17:57.352981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassifierLoss(nn.Module):\n    def __init__(self,beta,lambd):\n        super().__init__()\n        self.CEL=nn.CrossEntropyLoss()\n        self.SmoothL1L=nn.SmoothL1Loss(beta=beta)\n        self.lambd=lambd\n    def forward(self,gts,mask,class_scores,offsets):\n        class_loss=self.CEL(class_scores,gts[:,0])\n        offset_loss=self.SmoothL1L(gts[:,1:],offsets*mask)\n        return class_loss+self.lambd*offset_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.355782Z","iopub.execute_input":"2021-07-10T18:17:57.356196Z","iopub.status.idle":"2021-07-10T18:17:57.367046Z","shell.execute_reply.started":"2021-07-10T18:17:57.356148Z","shell.execute_reply":"2021-07-10T18:17:57.365984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(self,settings,num_anchors,device):\n        super().__init__()\n        self.RPN=RPN(settings[\"feature_map_depth\"],num_anchors)\n        self.RPN.apply(weights_init)\n        self.img_size=settings[\"img_size\"]\n        self.pre_nms_top_n=settings[\"pre_nms_top_n\"] \n        self.nms_threshold=settings[\"nms_threshold\"] \n        self.post_nms_top_n=settings[\"post_nms_top_n\"]  \n        self.pos_threshold=settings[\"pos_threshold\"] \n        self.neg_threshold=settings[\"neg_threshold\"] \n        self.num_RPN_samples=settings[\"num_RPN_samples\"] \n        self.fg_threshold=settings[\"fg_threshold\"]\n        self.bg_threshold_hi=settings[\"bg_threshold_hi\"]\n        self.bg_threshold_lo=settings[\"bg_threshold_lo\"]\n        self.num_classification_samples=settings[\"num_classification_samples\"]\n        self.pool_size=settings[\"pool_size\"]\n        self.ss_ratio=settings[\"ss_ratio\"]\n        self.num_classes=settings[\"num_classes\"]\n        self.classifier=Classifier(settings[\"feature_map_depth\"],settings[\"pool_size\"],\n                                   self.num_classes)\n        self.generated_anchors=anchor_generation(settings[\"feature_map_height\"],\n                                                 settings[\"feature_map_width\"],\n                                                 device,\n                                                 settings[\"anchor_ratios\"],\n                                                 settings[\"anchor_scales\"],\n                                                 self.ss_ratio)\n        self.device=device\n    def forward(self,feature_map,gt_bboxes):\n        rpn_scores,rpn_bboxes=self.RPN(feature_map)\n        rpn_scores=rpn_scores.view(-1,1)\n        rpn_bboxes=rpn_bboxes.view(-1,4)\n        foreground_samples,background_samples=anchor_target_layer(self.generated_anchors,gt_bboxes,\n                                                                  self.pos_threshold,self.neg_threshold,\n                                                                  self.num_RPN_samples,self.img_size,\n                                                                  rpn_scores,rpn_bboxes,self.device)\n        _,proposals=proposal_layer(self.generated_anchors,rpn_scores,\n                                   rpn_bboxes,self.img_size,\n                                   self.pre_nms_top_n,\n                                   self.nms_threshold,self.post_nms_top_n,\n                                   self.device)\n        samples,variable_sized_ROIs=proposal_target_layer(proposals,gt_bboxes,\n                                              self.fg_threshold,self.bg_threshold_hi,\n                                              self.bg_threshold_lo,self.num_classification_samples,\n                                              self.num_classes,self.device)\n        gts,mask=create_labels_with_mask(samples,self.num_classes,self.device)\n        fix_sized_ROIs=roi_pool(feature_map,variable_sized_ROIs,\n                                (self.pool_size,self.pool_size),\n                                self.ss_ratio)\n        class_scores,final_offsets=self.classifier(fix_sized_ROIs)\n        return foreground_samples,background_samples,gts,mask,class_scores,final_offsets\n    def inference(self):\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.368637Z","iopub.execute_input":"2021-07-10T18:17:57.369216Z","iopub.status.idle":"2021-07-10T18:17:57.385194Z","shell.execute_reply.started":"2021-07-10T18:17:57.36918Z","shell.execute_reply":"2021-07-10T18:17:57.384408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RPN_loss(nn.Module):\n    def __init__(self,beta,lambd,device):\n        super().__init__()\n        self.lambd=lambd\n        self.smooth_l1=nn.SmoothL1Loss(beta=beta)\n        self.BCELoss=nn.BCELoss()\n        self.device=device\n    def forward(self,foreground_samples,background_samples):\n        foreground_scores=torch.ones(foreground_samples.shape[0],device=self.device)\n        background_scores=torch.zeros(background_samples.shape[0],device=self.device)\n        gt_scores=torch.cat([foreground_scores,background_scores],axis=0)\n        pred_scores=torch.cat([foreground_samples[:,1].view(-1),background_samples[:,1].view(-1)],axis=0)\n        loss_cls=self.BCELoss(pred_scores,gt_scores)\n        loss_coords=self.smooth_l1(foreground_samples[:,2:6],foreground_samples[:,6:])\n        return loss_cls+self.lambd*loss_coords","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.386453Z","iopub.execute_input":"2021-07-10T18:17:57.386937Z","iopub.status.idle":"2021-07-10T18:17:57.401537Z","shell.execute_reply.started":"2021-07-10T18:17:57.3869Z","shell.execute_reply":"2021-07-10T18:17:57.40052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:17:57.402898Z","iopub.execute_input":"2021-07-10T18:17:57.403378Z","iopub.status.idle":"2021-07-10T18:17:57.415933Z","shell.execute_reply.started":"2021-07-10T18:17:57.403344Z","shell.execute_reply":"2021-07-10T18:17:57.415147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_anchors=len(settings[\"anchor_ratios\"])*len(settings[\"anchor_scales\"])\n\nfeature_extractor=torchvision.models.vgg16(pretrained=True).features[:30] #Used till conv5_3\nfeature_extractor.eval()\nfeature_extractor=feature_extractor.to(device)\nnet=Network(settings,num_anchors,device).to(device)\nget_rpn_loss=RPN_loss(settings[\"beta_rpn_loss\"],settings[\"lambd_rpn_loss\"],device)\nget_classifier_loss=ClassifierLoss(settings[\"beta_classifier_loss\"],settings[\"lambd_classifier_loss\"])\noptimizer=torch.optim.Adam(net.parameters(),lr=settings[\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:18:19.035637Z","iopub.execute_input":"2021-07-10T18:18:19.036039Z","iopub.status.idle":"2021-07-10T18:18:21.752459Z","shell.execute_reply.started":"2021-07-10T18:18:19.035999Z","shell.execute_reply":"2021-07-10T18:18:21.751374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_examples=len(train_loader)\nnum_epochs=1\n\nfor epoch in range(num_epochs):\n    avg_loss=0\n    for x,y in tqdm(train_loader,leave=False):\n        gt_bboxes=y[0]\n        feature_map=feature_extractor(x.to(device))\n        fp_result=net(feature_map,gt_bboxes)\n        rpn_loss=get_rpn_loss(fp_result[0],fp_result[1])\n        classifier_loss=get_classifier_loss(fp_result[2],fp_result[3],fp_result[4],fp_result[5])\n        loss=rpn_loss+classifier_loss\n        optimizer.zero_grad()\n        loss.backward()\n        avg_loss+=loss.item()/num_examples\n        optimizer.step()\n    print(f\"Epoch {epoch+1}, Average loss: {avg_loss:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-10T18:18:57.170985Z","iopub.execute_input":"2021-07-10T18:18:57.171453Z","iopub.status.idle":"2021-07-10T18:18:57.248924Z","shell.execute_reply.started":"2021-07-10T18:18:57.171356Z","shell.execute_reply":"2021-07-10T18:18:57.247652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}