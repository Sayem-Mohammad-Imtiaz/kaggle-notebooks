{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport PIL\nimport PIL.Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nfrom pathlib import Path\nimport pandas as pd\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# !ls -al /kaggle/input/iris\np = Path('/kaggle/input/iris/Iris.csv')\ndf = pd.read_csv(p)\ndf = df.drop(columns='Id')\ndf['Species'] = df['Species'].astype('category').cat.codes\nfeature = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ntarget = df[['Species']]\nfeature","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class IrisDataset(torch.utils.data.Dataset):\n    def __init__(self, path, feature_columns, target_columns, transform=None):\n        self.path = Path(path)\n        self.dframe = pd.read_csv(self.path)\n        self._do_normalizer()\n        self.feature_columns = feature_columns\n        self.target_columns = target_columns\n        self.transform = transform\n\n    def _do_normalizer(self):\n        self.dframe = self.dframe.drop(columns='Id')\n        self.dframe['Species'] = self.dframe['Species'].astype('category').cat.codes\n        \n    def __len__(self):\n        return len(self.dframe)\n    \n    def __getitem__(self, idx):\n        feature = self.dframe[self.feature_columns].iloc[idx].values\n        target = self.dframe[self.target_columns].iloc[idx].values\n        \n        if self.transform:\n            feature = self.transform(feature)\n            target = self.transform(target)\n\n        return feature, target\n\nclass NumpyToTensor(object):\n    def __call__(self, param):\n        return torch.from_numpy(param.astype(np.float32))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"p = '/kaggle/input/iris/Iris.csv'\nfc = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\ntc = ['Species']\ntmft = transforms.Compose([NumpyToTensor()])\niris = IrisDataset(path=p,feature_columns=fc, target_columns=tc, transform=tmft )\nloader = torch.utils.data.DataLoader(iris, batch_size=16, shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"[1]\n[0,1,0]\n\n[2]\n[0,0,1]\n\n[0]\n[1,0,0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"load_iter = iter(loader)\nx,y = load_iter.next()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch.nn as nn\nlinear = nn.Linear(4,1)\nlinear.weight\n# out = linear(x)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# class NumpyToTensor(object):\n#     def __call__(self, param):\n#         return torch.from_numpy(param.astype(np.float32))\n    \n# class TensorToNumpy(object):\n#     def __call__(self, param):\n#         return param.numpy()\n    \n# a = np.array([1,5,6])\n# tmft = transforms.Compose([\n#     NumpyToTensor(),\n#     TensorToNumpy()\n# ])\n# x = tmft(a)\n\n# ntt = NumpyToTensor(),\n# ttn = TensorToNumpy()\n# x = ntt(a)\n# x = ttn(x)\n# x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac984fe71eacfd2808fdada2a20652a2f1e0e094"},"cell_type":"code","source":"from pathlib import Path\npath = Path('/kaggle/input/flower_data/flower_data/')\nlist(path.iterdir())\ntrain_path = path.joinpath('train')\nlist(train_path.glob('*'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"ac984fe71eacfd2808fdada2a20652a2f1e0e094"},"cell_type":"code","source":"trainset = torchvision.datasets.ImageFolder(train_path)\ntrainset.__len__()\nimage, label = trainset.__getitem__(0)\ntrainset.class_to_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c596f23850a1eaa06a6d6fc07b7ec3b637f4379"},"cell_type":"code","source":"class FandiDataset(torch.utils.data.Dataset):\n    def __init__(self, path, transform=None):\n        super(FandiDataset, self).__init__()\n        self.path = path\n        self.transform = transform\n        self.dirfiles = sorted(glob.glob(self.path+'/*'))\n        self.files =  sorted(glob.glob(self.path+'/*/*.jpg'))\n        self.class_to_idx = self._class_to_idx()\n    \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        fpath = self.files[idx]\n        img = PIL.Image.open(fpath)\n        label_key = fpath.split('/')[-2]\n        label_idx = self.class_to_idx[label_key]\n        \n        if self.transform:\n            img = self.transform(img)\n#         img = torch.from_numpy(np.array(img))\n            \n        return img, label_idx\n\n    def _class_to_idx(self):\n        data = {}\n        for idx in range(len(self.dirfiles)):\n            label = self.dirfiles[idx].split('/')[-1]\n            data.update({label:idx})\n        return data\n\npath= '/kaggle/input/flower_data/flower_data/'\ntrain_path = os.path.join(path, 'train')\n\ntmft = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n])\n\ntrain_set = FandiDataset(train_path, transform=tmft)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c596f23850a1eaa06a6d6fc07b7ec3b637f4379"},"cell_type":"code","source":"class ToufanDataset(torch.utils.data.Dataset):\n    def __init__(self, path, transform=None):\n        self.path = Path(path)\n        self.dirfiles = sorted(list(train_path.glob('*')))\n        self.files = sorted(list(train_path.glob('*/*.jpg')))\n        self.class_to_idx = self._class_to_idx()\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def _class_to_idx(self):\n        data = {}\n        for idx in range(len(self.dirfiles)):\n            dirname = str(self.dirfiles[idx]).split('/')[-1]\n            data.update({dirname:idx})\n        return data\n    \n    def __getitem__(self, idx):\n        pf = str(self.files[idx])\n        dirname = pf.split('/')[-2]\n        label = data[dirname]\n        img = PIL.Image.open(pf)\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n        \ntp = '/kaggle/input/flower_data/flower_data/'\ntrain_dataset = ToufanDataset(tp)\nimg, label = train_dataset.__getitem__(0)\nnp.array(img).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c596f23850a1eaa06a6d6fc07b7ec3b637f4379"},"cell_type":"code","source":" \ntmft = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n]) \ntp = '/kaggle/input/flower_data/flower_data/'\ntrain_dataset = ToufanDataset(tp, transform=tmft)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\ntrain_iter = iter(train_loader)\nimg, label = train_iter.next()\nplt.imshow(img[0].permute(1,2,0), cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c596f23850a1eaa06a6d6fc07b7ec3b637f4379"},"cell_type":"code","source":"path = Path('/kaggle/input/flower_data/flower_data/')\ntrain_path = path.joinpath('train')\ndirfiles = sorted(list(train_path.glob('*')))\nfiles = sorted(list(train_path.glob('*/*.jpg')))\n\ndata = {}\nfor idx in range(len(dirfiles)):\n    dirname = str(dirfiles[idx]).split('/')[-1]\n    data.update({dirname:idx})\n\n# data['1']\nidx = 0\npf = str(files[idx])\ndirname = pf.split('/')[-2]\nlabel = data[dirname]\nimg = PIL.Image.open(pf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c596f23850a1eaa06a6d6fc07b7ec3b637f4379"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c596f23850a1eaa06a6d6fc07b7ec3b637f4379"},"cell_type":"code","source":"# class HitamPutih(object):\n#     def __call__(self, img):\n#         return img.convert('L')\n\n\n# tmft = transforms.Compose([\n#     transforms.Resize((224,224)),\n#     transforms.ToTensor(),\n# ])\n\n# x,y = train_set.__getitem__(0)\n# x = tmft(x)\n# x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"965360669c0c3ecfc93dbbc2c016fbcdc0d160bb"},"cell_type":"code","source":"# path = '/data/flower_data'\n# train_path = os.path.join(path, 'train')\n# dirfiles = sorted(glob.glob(train_path+'/*'))\n# data = {}\n# for idx in range(len(dirfiles)):\n#     label = dirfiles[idx].split('/')[-1]\n#     data.update({label:idx})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a960c276687c504ea76f16b54c6257d0dd91dfb"},"cell_type":"code","source":"import torch.nn as nn\nlinear1 = nn.Linear(3,4)\nlinear2 = nn.Linear(4,1)\n\ninp = torch.FloatTensor([[1,0,1]])\nx = linear1(inp)\nx = linear2(x)\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edc1d8a13df5598f5dfde8dc5eed155587816eb2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}