{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"markdown","source":"输入当前的时间","metadata":{}},{"cell_type":"markdown","source":"为使结果易于保存，这里直接将current time的值赋为9000，实际可以输入","metadata":{}},{"cell_type":"code","source":"#current_time = int(input(\"Please enter the current time(from 1):\"))\ncurrent_time = 9000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DGIM算法求出近似解","metadata":{}},{"cell_type":"code","source":"bucket_list = []\n\ndef del_bucket(t):\n    #t 代表当前时刻\n    if len(bucket_list)>0 and t-1000==bucket_list[0]['timestamp']:\n        del bucket_list[0]\n\ndef merge_bucket():\n    for i in range(len(bucket_list)-1,1,-1):\n        if bucket_list[i]['size'] == bucket_list[i-2]['size']:\n            bucket_list[i-2]['size']+=bucket_list[i-1]['size']\n            bucket_list[i-2]['timestamp']=bucket_list[i-1]['timestamp']\n            del bucket_list[i-1]\n        \ndef DGIM(current_time):\n    count = 0\n    \n    with open('../input/coding2/stream_data.txt','r') as f:\n        data = f.read().split()\n        \n    #根据原则更新bucket列表\n    for i in range(current_time):\n        del_bucket(i+1)#去除最左边可能已经过期的bucket\n        if int(data[i])==1:#若当前的数为1\n            bucket = {\"timestamp\":i+1,\"size\":1}\n            bucket_list.append(bucket)\n            merge_bucket()#向后更新bucket 确保每种大小的bucket数量为1个或者两个\n    \n    #计算最终的估计count\n    for i in range(len(bucket_list)):\n        count += bucket_list[i]['size']\n    count -= bucket_list[0]['size']/2 #最后一个bucket的size只加一半\n    \n    return count if len(bucket_list)>0 else 0\n\ncount = DGIM(current_time)\nprint(\"The estimated count by DGIM at current time is:\",count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"markdown","source":"求精确解","metadata":{}},{"cell_type":"code","source":"def accurate_count(current_time):\n    with open('../input/coding2/stream_data.txt','r') as f:\n        data=f.read().split()\n    left_side = 0 if current_time<=1000 else current_time-1000\n    total = 0\n    for i in range(current_time if current_time<=1000 else 1000):\n        total += int(data[left_side+i])\n    return total\nacc_count = accurate_count(current_time)\nprint(\"The accurate count at current time is:\",acc_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The differences in running time and space:**\n\nFor the accurate function: it needs to store the whole stream window, so the running space is O(N), and for a data item comes at current time, it will only take O(1) running time to update the window and count the number of 1s.\n\nAnd for the DGIM algorithm: for each stream, it needs to store O($log^2N$) bits and for each query, it will need O(logN) to estimate the number.","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"markdown","source":"csv数据的读取和预处理","metadata":{}},{"cell_type":"code","source":"import csv\nimport numpy as np\n\ndataSet =[]\nwith open(\"../input/coding2/docs_for_lsh.csv\") as f:\n    reader = csv.reader(f)\n    headers = next(reader)\n    for row in reader:\n        data_row = row[1:]\n        dataSet.append([float(item) for item in data_row])\nquery_matrix = np.array(dataSet)\n#query_matrix = query_matrix[0:10000]\ninput_matrix = query_matrix.T #使得列代表document","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"确定参数 取n=100 b=20","metadata":{}},{"cell_type":"code","source":"n = 100\nb = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"minhash主体部分","metadata":{}},{"cell_type":"code","source":"import random\nimport hashlib\n\ndef sigGen(matrix):\n    seqSet = [i for i in range(matrix.shape[0])] #200\n    result = [-1 for i in range(matrix.shape[1])] #1000000\n    \n    count = 0\n    \n    while len(seqSet) > 0:\n        randomSeq = random.choice(seqSet)\n        for i in range(matrix.shape[1]):\n            if matrix[randomSeq][i] != 0 and result[i] == -1:\n                result[i] = randomSeq\n                count += 1\n        if count == matrix.shape[1]:\n            break\n        seqSet.remove(randomSeq)\n    \n    return result\n\ndef sigMatrixGen(input_matrix, n):\n\n    result = []\n\n    for i in range(n):\n        sig = sigGen(input_matrix)\n        result.append(sig)\n\n\n    return np.array(result)\n\n\ndef minHash(input_matrix, n, b):\n\n    hashBuckets = {}\n    r = int(n / b)\n    sigMatrix = sigMatrixGen(input_matrix, n)\n    begin, end = 0, r\n    count = 0\n\n    while end <= sigMatrix.shape[0]:\n\n        count += 1\n\n        for colNum in range(sigMatrix.shape[1]):\n            hashObj = hashlib.md5()\n            band = str(sigMatrix[begin: begin + r, colNum]) + str(count)\n            hashObj.update(band.encode())\n            tag = hashObj.hexdigest()\n            if tag not in hashBuckets:\n                hashBuckets[tag] = [colNum]\n            elif colNum not in hashBuckets[tag]:\n                hashBuckets[tag].append(colNum)\n        begin += r\n        end += r\n\n    return hashBuckets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nhashBuckets = minHash(input_matrix,n,b)\nprint(\"To show the correctness of the hashBuckets, here shows its first item:\",list(hashBuckets.items())[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"markdown","source":"给定document0搜索前30相似的文档","metadata":{}},{"cell_type":"markdown","source":"搜索函数","metadata":{}},{"cell_type":"code","source":"def nn_search(queryCol):\n    result = set()\n    for key in hashBuckets:\n        if queryCol in hashBuckets[key]:\n            for i in hashBuckets[key]:\n                result.add(i)\n\n    result.remove(queryCol)\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"自己定义的计算相似度函数","metadata":{}},{"cell_type":"code","source":"def Jaccard(a,b):\n    a = list(a)\n    b = list(b)\n    n = 0\n    d = 0\n    for i in range(len(a)):\n        if a[i] == b[i] == 1:\n            n += 1\n            d += 1\n        elif not (a[i] == b[i] == 0):\n            d += 1\n    return n / d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"用自己定义的Jaccard相似度函数基于hashbuckets计算","metadata":{}},{"cell_type":"code","source":"search_result = nn_search(0)\nsearch_result = list(search_result)\nscore = []\nfrom sklearn import metrics\nfor i in range(len(search_result)):\n    score.append(Jaccard(query_matrix[0],query_matrix[search_result[i]]))\nscore = np.array(score)\nindex = np.argsort(-score)\nindex = list(index)\nresult = []\nfor i in index[0:30]:\n    result.append(search_result[i])\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"用sklearn.metrics.jaccard_score()基于整个数据集验证","metadata":{}},{"cell_type":"code","source":"score_v = []\nfrom sklearn import metrics\nfor i in range(query_matrix.shape[0]-1):\n    score_v.append(metrics.jaccard_score(query_matrix[0],query_matrix[i+1]))\nscore_v = np.array(score_v)\nindex_v = np.argsort(-score_v)\nindex_v = list(index_v)\nresult_v = []\nfor i in index_v[0:30]:\n    result_v.append(i+1)\nprint(result_v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"计算hit rate进行验证","metadata":{}},{"cell_type":"code","source":"count = 0\nfor i in result:\n    if i in result_v:\n        count += 1\nhit_rate = count/30\nprint(hit_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}