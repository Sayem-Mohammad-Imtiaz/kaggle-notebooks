{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LSTM with pre-trained Word2Vec for Stack Overflow questions "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# загружаем необходимые библиотеки\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport re\nimport string\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport nltk\nimport torchtext\nimport random\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nfrom tqdm import tqdm\nfrom torchtext.data import BucketIterator\nfrom IPython.display import clear_output\nfrom nltk import word_tokenize\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download, explore and preprocess data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/60k-stack-overflow-questions-with-quality-rate/data.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Body'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# удаляем колонки с ненужной информацией\ndf = data.drop(['Id', 'CreationDate'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# объединяем колокнки с текстовыми признаками в одну\ndf['text']= df['Title'] + ' ' + df['Body'] + ' ' + df['Tags']\ndf = df.drop(['Title', 'Body', 'Tags'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# заменяем значения в колонке с целевой переменной\ndf['Y'] = df['Y'].replace(['HQ'], 0)\ndf['Y'] = df['Y'].replace(['LQ_EDIT'], 1)\ndf['Y'] = df['Y'].replace(['LQ_CLOSE'], 2)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Y'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# выделим текстовый признак и целевую перевенную в отдельные списки\ntext, target = list(df['text']), list(df['Y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# функция для предобработки текста\ndef preprocess(doc):\n    prepdoc = []\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    urlptr = r'((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)'\n    alhptr = '[^a-zA-Z0-9]'\n    sqcptr = r'(.)\\1\\1+'\n    rplptr = r'\\1\\1'\n    \n    for text in doc:\n        # приводим весь текст к нижнему регистру\n        text = text.lower()\n        # заменяем ссылки на 'URL'\n        text = re.sub(urlptr, ' URL', text)      \n        # убираем все символы, отличные от буквенных или цифровых\n        text = re.sub(alhptr, ' ', text)\n        # обрезаем последовательности из трёх и более одинаковых букв\n        text = re.sub(sqcptr, rplptr, text)\n        \n        words = ' '\n        for word in text.split():\n            # проверяем короткие слова и приводим словоформы к лемме (словарной форме)\n            if len(word) > 1:\n                word = lemmatizer.lemmatize(word)\n                words += (word + ' ')\n            \n        prepdoc.append(words)\n        \n    return prepdoc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# обработаем список признака 'text' и отобразим часть сообщений\n%time preptext = preprocess(text)\npreptext[:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec & words clasterization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# подготовим данные для обработки в Word2Vec\ndf_w2v = [sentence.split() for sentence in preptext]\ndf_w2v[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\n# обучим Word2Vec на основе нашего датасета\nw2v = Word2Vec(sentences=df_w2v, size=100, window=5, \n               min_count=5, workers=2, sg=1, iter=50)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сохраним и загрузим обратно модель Word2Vec\nw2v.save('../working/w2v.model')\nw2v = Word2Vec.load('../working/w2v.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на похожие слова на основе векторного представления\nw2v.wv.most_similar('javascript')[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v.wv.most_similar('console')[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# функция для отображения кластеризации схожих слов\ndef display_closestwords_tsnescatterplot(model, word, size):\n    sns.set_style('darkgrid')\n    mpl.rcParams.update({'font.size': 15})\n    \n    arr = np.empty((0, size), dtype='f')\n    word_labels = [word]\n    \n    close_words = model.wv.most_similar([word])\n    arr = np.append(arr, model.wv.__getitem__([word]), axis=0)\n    \n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, wrd_vector, axis=0)\n        \n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    plt.figure(figsize=(16, 10))\n    plt.scatter(x_coords, y_coords)\n    \n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()-50, x_coords.max()+50)\n    plt.ylim(y_coords.min()-50, y_coords.max()+50)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_closestwords_tsnescatterplot(w2v,'git', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_closestwords_tsnescatterplot(w2v,'access', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# векторное представление отдельного слова\nw2v.wv['git']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# получившийся словарь слов\n# w2v.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим заново датасет из предобработанной ранее текстовой информации\ndict = {'Text': preptext, 'Target': target}    \ndf_rnn = pd.DataFrame(dict)\ndf_rnn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на распределение длин вопросов из датасета на гистограмме \ndf_rnn['Text'].map(len).hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dowloading pre-trained Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"# сохраним изначальный датасет в .csv файл\ndf.to_csv('df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# инициализиурем объекты для предобработки датасета при дальнейшей загурзки в torch\ndescription = torchtext.data.Field(tokenize=word_tokenize, lower=True, batch_first=True)\ny = torchtext.data.Field(sequential=False, is_target=True, use_vocab=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# загрузим данные в torch с помощью TabularDataset\ndata = torchtext.data.TabularDataset(path='../working/df', format='csv', \n                                     fields={\n                                         'text': ('text', description),\n                                         'Y': ('target', y)\n                                     })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим словарь на основе текстового признака\ndescription.build_vocab(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# загрузим обученный Word2Vec\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# распакуем загруженный Word2Vec\n!unzip '../working/wiki-news-300d-1M.vec.zip' -d '../working'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# загурзим ветора из обученного Word2Vec\ndescription.vocab.load_vectors(torchtext.vocab.Vectors('../working/wiki-news-300d-1M.vec'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"description.vocab.vectors.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# разобьём датасет на тренировочную и тестовую выборку\ntrain, val = data.split(split_ratio=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training LSTM with pre-trained Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим сеть LSTM\nclass lstm(nn.Module):\n    def __init__(self, w2v, padding_inx, dropout, hidden_size):\n        super(lstm, self).__init__()\n        \n        self.embedding = nn.Embedding.from_pretrained(w2v)\n        self.embedding.padding_inx = padding_inx\n\n        self.embedding.weight.requires_grad = True\n\n        self.dropout = nn.Dropout(p = dropout)\n        self.lstm = nn.LSTM(input_size = self.embedding.embedding_dim,\n                            hidden_size = hidden_size,\n                            num_layers = 2,\n                            dropout = dropout,\n                            bidirectional = True)\n        self.label = nn.Linear(hidden_size*2*2, 1)\n\n    def forward(self, sentence):\n        x = self.embedding(sentence)\n        x = torch.transpose(x, dim0 = 1, dim1 = 0)\n        out, (hidden, c) = self.lstm(x)\n        x = self.dropout(torch.cat([c[i,:,:] for i in range(c.shape[0])], dim=1))\n        x = self.label(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# с помощью BucketIterator создадим объекты загрузки данных в сеть\nbatch_size = 16\ntrain_i = torchtext.data.BucketIterator(dataset=train,\n                                        batch_size=batch_size,\n                                        shuffle=True,\n                                        sort = False,\n                                        train =True)\n\n\nval_i = torchtext.data.BucketIterator(dataset=val,\n                                      batch_size=batch_size,\n                                      shuffle=True,\n                                      sort = False,\n                                      train = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# определим нашу созданнуть сеть LSTM\nmodel = lstm(description.vocab.vectors, description.vocab.stoi[description.pad_token], \n             dropout=0.2, hidden_size=128).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# оптимизатор и функция потерь\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nloss = nn.BCEWithLogitsLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# функция для обучения сети\ndef train(epochs, model, eval_time, loss_f, optimizer, train_i, val_i):\n    sns.set_style('white')\n    mpl.rcParams.update({'font.size': 10})\n    \n    step = 0\n    losses = []\n    val_losses = []\n    accuracy = []\n    val_accuracy = []\n    train_i.init_epoch()\n    \n    for epoch in range(epochs):\n        for batch in iter(train_i):\n            step += 1\n            model.train()\n            x = batch.text.cuda()\n            y = batch.target.type(torch.Tensor).cuda()\n            model.zero_grad()\n            preds = model.forward(x).view(-1)\n            loss = loss_f(preds, y)\n            losses.append(loss.cpu().data.numpy())\n            accuracy.append(accuracy_score(batch.target.data.numpy().tolist(), \n                                           np.round(np.array(torch.sigmoid(preds).cpu().data.numpy().tolist()))\n                                          ))\n            loss.backward()\n            optimizer.step()\n\n            if step % eval_time == 0:\n                clear_output(True)\n                model.eval()\n                model.zero_grad()\n\n                for batch in iter(val_i):\n                    x = batch.text.cuda()\n                    y = batch.target.type(torch.Tensor).cuda()\n                    preds = model.forward(x).view(-1)\n                    val_losses.append(loss_f(preds, y).cpu().data.numpy())\n                    val_accuracy.append(accuracy_score(batch.target.data.numpy().tolist(), \n                                                   np.round(np.array(torch.sigmoid(preds).cpu().data.numpy().tolist()))\n                                                      ))\n                    \n                fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n                fig.suptitle('Accuracy & Loss')\n                \n                axs[0, 0].set_title('train cross-entropy loss')\n                axs[0, 1].set_title('test cross-entropy loss')\n                axs[1, 0].set_title('train accuracy')\n                axs[1, 1].set_title('test accuracy')\n\n                axs[0, 0].plot(losses)\n                axs[0, 0].plot(pd.Series(losses).rolling(400).mean().values)\n                axs[0, 1].plot(val_losses)\n                axs[0, 1].plot(pd.Series(val_losses).rolling(400).mean().values)\n                axs[1, 0].plot(accuracy)\n                axs[1, 0].plot(pd.Series(accuracy).rolling(400).mean().values)\n                axs[1, 1].plot(val_accuracy)\n                axs[1, 1].plot(pd.Series(val_accuracy).rolling(400).mean().values)\n            \n                for ax in axs.flat:\n                    ax.set(xlabel='step')\n\n                plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# переносим модель на GPU\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# обучаем и выводим графики с метриками loss и accurancy\ntrain(3, model, 250, loss, optimizer, train_i, val_i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# оцениваем обученную модель и выводим метрики качества\nmodel.eval()\n\nreal = []\npreds = []\n\nfor batch in iter(val_i):\n    x = batch.text.cuda()\n    real += batch.target.data.numpy().tolist()\n    preds += torch.sigmoid(model.forward(x).view(-1)).cpu().data.numpy().tolist()\n\nprint(classification_report(real, np.round(np.array(preds))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* В целом вино, что точность модели пока неудовлетворительная, и один из классов не распознаётся вовсе. Требуется ещё поработать с настройками самой сети (в процессе)..."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}