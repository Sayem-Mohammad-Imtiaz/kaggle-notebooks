{"cells":[{"metadata":{},"cell_type":"markdown","source":"Uses the prepared [melspectrograms](https://librosa.github.io/librosa/generated/librosa.feature.melspectrogram.html) and [Mel-frequency cepstral coefficients (MFCCs)](https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html) to train and evaluate a CNN-based classifier.\n\nThis version includes MFCCs in the feature space. Instead of adding this array as a separate input, or stacking it on top, it is simply overwrites the bottom 20 frequency bands of the spectrogram. Since they share the same `time` axis, this should be a good approach. This is also a convenient means of filtering out the lower frequencies.\n\nThis version also include Dropout(rate=0.2) after each MaxPooling layer. \n\nThis version does **not** filter out quiet frames."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa as lr\nfrom librosa.display import specshow\nfrom glob import glob\nimport os\nfrom IPython.display import Audio\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport keras\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_full_path(sample): return os.path.join(sounds_dir, sample['file_name'])\nsounds_dir = \"../input/xenocanto-avian-vocalizations-canv-usa/xeno-canto-ca-nv/\"\n# sounds_dir = \"../input/xeno-canto-ca-nv/\"\n\nmelspec_dir = \"../input/avian-vocalizations-melspectrograms-log-norm/\"\nmelspec_features_dir = melspec_dir + \"/melspectrograms_logscaled_normalized/features\"\n\ndf = pd.read_csv(\"../input/xenocanto-avian-vocalizations-canv-usa/xeno-canto_ca-nv_index.csv\", index_col='file_id' )\ndisplay(df.head(2))\n# df = pd.read_csv(\"../input/xeno-canto_ca-nv_index.csv\")\nfiles_list = glob(os.path.join(sounds_dir,\"*.mp3\"))\nprint(\"%i mp3 files in %s\"%(len(files_list), sounds_dir))\nprint(\"%i samples in index.\"%len(df))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapes_df = pd.read_csv(\"../input/avian-vocalizations-spectrograms-and-mfccs/feature_shapes.csv\",index_col=0 )\ndisplay(shapes_df.head(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/avian-vocalizations-partitioned-data/train_file_ids.csv\",index_col=0)\ndisplay(\"Training data:\",train_df.head(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/avian-vocalizations-partitioned-data/test_file_ids.csv\",index_col=0)\ndisplay(\"Test data:\",test_df.head(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef parse_shape(shape_str):\n    \"\"\"Shape was saved in feature_shapes as a string. Woops. Parse out the values. \"\"\"\n    a,b = re.search('\\((\\d+), (\\d+)\\)',shape_str).groups()\n    return int(a), int(b)\n\ndef log_clipped(a):\n    \"\"\"Convenience function to clip the input to positive values then return the log.\"\"\" \n    return np.log(np.clip(a,.0000001,a.max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_ids(generator, batch_number, batch_index):\n#     idxs = generator.indexes[\n#         batch_number*generator.batch_size+batch_index:(batch_number+1)*generator.batch_size]\n#     return [generator.list_IDs[k] for k in idxs]\n# def show_batch_item(generator, batch_number, batch_index):\n#     X_batch, y_batch = generator.__getitem__(batch_number)\n#     ids = get_ids(generator, batch_number, batch_index)\n#     file_id = X_train[ids[batch_index]]\n#     sg = X_batch[batch_index].reshape(generator.dim)\n# #     mm_scaled_log_sg = np.memmap('features/XC%s_log_scaled_melspectrogram.dat'%file_id))\n# #     specshow(mm_scaled_log_sg, x_axis='time', y_axis='mel' )\n# #     specshow(np.log(np.clip(sg,.0000001,sg.max())), x_axis='time', y_axis='mel' )\n#     specshow(sg, x_axis='time', y_axis='mel' )\n# #     specshow(mfcc, x_axis='time', y_axis='mel' )\n#     sample = df[df.file_id==file_id].to_dict(orient='records')[0]\n#     print(file_id,label_encoder.classes_[y_train[ids][batch_index]],\n#           \"contributed by\",sample['recordist'], sample['recordist_url'])\n#     species_name = sample['english_cname']\n#     plt.title(species_name+\" - \"+sample['file_name'])\n#     plt.colorbar()\n#     plt.show()\n#     wav, sr = lr.load(get_full_path(sample))\n#     return Audio(wav, rate=sr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_english_labels_entire_dataset = [s['english_cname'] for i,s in df.iterrows()]\nlabel_encoder = LabelEncoder().fit(y_english_labels_entire_dataset)\nn_classes = len(label_encoder.classes_)\n\n# X_train, X_test, y_train, y_test = train_test_split(\n#     np.array([s['file_id'] for i,s in df.iterrows()]), \n#     y_encoded_entire_dataset, \n#     test_size=1/3, \n#     stratify=y_encoded_entire_dataset, \n#     shuffle=True, \n#     random_state=37,\n# )\nX_train = list(train_df.index)\ny_train = list(train_df.label)\nprint(\"Training data len:\",len(X_train), len(y_train))\nX_test = list(test_df.index)\ny_test = list(test_df.label)\nprint(\"Test data len:    \",len(X_test), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Training File IDs: \\n['+', '.join([str(x) for x in X_train])+']')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Test File IDs: \\n['+', '.join([str(x) for x in X_test])+']')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a Data Generator to generate fixed-length samples from random windows within clips"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pickle\n# with open('../input/avian-vocalizations-pickled-spectrograms-and-mfcc/features.pickle','rb') as f:\n#     features = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mfccs_mean_px = np.mean([features[file_id]['mfcc'].mean() for file_id in features])\n# mfccs_std_px = np.mean([features[file_id]['mfcc'].std() for file_id in features])\n# mfccs_mean_px, mfccs_std_px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nmfcc_scaler = StandardScaler()\nfor file_id in shapes_df.file_id:\n    mfcc = np.memmap('../input/avian-vocalizations-spectrograms-and-mfccs/mfccs/features/XC%s_mfcc.dat'%file_id, \n        shape=parse_shape(shapes_df[shapes_df.file_id==file_id]['mfcc_shapes'].values[0]),  dtype='float32', mode='readonly')\n    mfcc_scaler.partial_fit(mfcc.flatten().reshape(-1, 1))\nprint(\"MFCC scaler:\",mfcc_scaler.mean_, mfcc_scaler.var_, np.sqrt(mfcc_scaler.var_))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapes_df[shapes_df.file_id==file_id]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.utils import to_categorical\n\n# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\nclass AudioFeatureGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size, n_frames=128, n_channels=1,\n                 n_classes=10, shuffle=False, seed=37):\n        'Initialization'\n        self.n_frames = n_frames\n        self.dim = (128, self.n_frames)\n        self.batch_size = batch_size\n        self.labels = {list_IDs[i]:l for i,l in enumerate(labels)}\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.seed = seed\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.seed(self.seed)\n            self.seed = self.seed+1 # increment the seed so we get a different batch.\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        #X = np.empty((self.batch_size, 128+20, self.dim[1], self.n_channels))\n        y = np.empty((self.batch_size, self.n_classes), dtype=int) # one-hot encoded labels\n\n        for i, ID in enumerate(list_IDs_temp):\n            sg_lognorm = np.memmap(os.path.join(melspec_features_dir,'XC%s_melspectrogram_logscaled_normalized.dat'%ID), \n                    shape=parse_shape(shapes_df[shapes_df.file_id==ID]['melspectrogram_shapes'].values[0]),  dtype='float32', mode='readonly')\n#             sg = np.memmap('../input/avian-vocalizations-spectrograms-and-mfccs/melspectrograms/features/XC%s_melspectrogram.dat'%file_id, \n#                     shape=parse_shape(shapes_df[shapes_df.file_id==file_id]['melspectrogram_shapes'].values[0]),  dtype='float32', mode='readonly')\n            mfcc = np.memmap('../input/avian-vocalizations-spectrograms-and-mfccs/mfccs/features/XC%s_mfcc.dat'%ID, \n                    shape=parse_shape(shapes_df[shapes_df.file_id==ID]['mfcc_shapes'].values[0]),  dtype='float32', mode='readonly')\n            # Normalize MFCCs\n            mfcc = mfcc_scaler.transform(mfcc)\n            \n            # Filter out quiet frames, thanks to https://www.kaggle.com/fleanend/extract-features-with-librosa-predict-with-nb\n            # Take mean amplitude M from frame with highest energy\n#             m = sg[:,np.argmax(sg.mean(axis=0))].mean()\n#             # Filter out all frames with energy less than 5% of M\n#             mask = sg.mean(axis=0)>=m/20\n#             sg = sg[:,mask]\n#             sg_lognorm = sg_lognorm[:,mask]\n#             mfcc = mfcc[:,mask]\n            \n            d_len = mfcc.shape[1] - self.dim[1]\n            if d_len<0: # Clip is shorter than window, so pad with mean value.\n                n = int(np.random.uniform(0, -d_len))\n                pad_range = (n, -d_len-n) # pad with n values on the left, clip_length - n values on the right \n#                 sg_cropped = np.pad(sg, ((0,0), pad_range), 'constant', constant_values=sg.mean())\n                sg_lognorm_cropped = np.pad(sg_lognorm, ((0,0), pad_range), 'constant', constant_values=0)\n                mfcc_cropped = np.pad(mfcc, ((0,0), pad_range), 'constant', constant_values=0)\n            else: # Clip is longer than window, so slice it up\n                n = int(np.random.uniform(0, d_len))\n#                 sg_cropped = sg[:, n:(n+self.dim[1])]\n                sg_lognorm_cropped = sg_lognorm[:, n:(n+self.dim[1])]\n                mfcc_cropped = mfcc[:, n:(n+self.dim[1])]\n                \n            #X[i,] = np.concatenate([sg_lognorm_cropped.reshape(1,128,self.dim[1],1), mfcc_cropped.reshape(1,20,self.dim[1],1)], axis=1)\n            X[i,] = sg_lognorm_cropped.reshape(1,128,self.dim[1],1)\n            # Overwrite the bottom of X with MFCCs (we don't need the low frequency bands anyway) \n            X[i,:20] = mfcc_cropped.reshape(1,20,self.dim[1],1)\n            y[i,] = to_categorical(self.labels[ID], num_classes=self.n_classes)\n\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import islice\ngenerator = AudioFeatureGenerator(X_test, y_test, batch_size=1, shuffle=True, seed=37, n_frames=128, n_classes=n_classes)\nfor g in islice(generator,0,4): # show a few examples\n    for i,spec in enumerate(g[0]): \n        plt.figure(figsize=(10,4))\n        spec_ax = specshow(spec.squeeze(), x_axis='time', y_axis='mel')\n        plt.title(label_encoder.classes_[np.argmax(g[1][i])])\n        plt.colorbar()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vis_learning_curve(learning):\n    train_loss = learning.history['loss']\n    train_acc = learning.history['acc']\n    val_loss = learning.history['val_loss'] if hasattr(learning.history,'val_loss') else None\n    val_acc = learning.history['val_acc'] if hasattr(learning.history,'val_acc') else None\n\n    fig, axes = plt.subplots(1, 2, figsize=(20,4), subplot_kw={'xlabel':'epoch'} )\n    axes[0].set_title(\"Accuracy\")\n    axes[0].plot(train_acc, label='training')\n    if val_acc is not None: axes[0].plot(val_acc, label='validation')\n    axes[0].legend()\n    axes[1].set_title(\"Loss\")\n    axes[1].plot(train_loss, label='training')\n    if val_acc is not None: axes[1].plot(val_loss, label='validation')\n    axes[1].legend()\n\n    # Plot a line to indicate the best epoch \n    best_training_epoc = np.argmin(val_loss) if val_acc is not None else np.argmin(train_loss)\n    axes[0].axvline(x=best_training_epoc, color='red')\n    axes[1].axvline(x=best_training_epoc, color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy by random guess: %.4f\"%(1/len(df['english_cname'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 150\nparams = {#'dim': (128,256),\n          'n_frames': 128,\n          'n_classes': n_classes,\n          'n_channels': 1}\ntraining_generator = AudioFeatureGenerator(X_train, y_train, batch_size=64, shuffle=True, seed=37, **params)\n# validation_generator = AudioFeatureGenerator(cv_val_index, y_cv_val, batch_size=len(cv_val_index), **params)\ndim = training_generator.dim\n\nsnapshot_filename = \"weights.best.hdf5\"\n\ncheckpointer = ModelCheckpoint(filepath=snapshot_filename, verbose=1)#, save_best_only=True)\n\n# From My Udacity Dog Project Image Classifier\nmodel = Sequential()\n#model.add(Conv2D(16,(1,4),input_shape=(20+dim[0], dim[1], 1),padding='valid',activation=\"relu\"))\nmodel.add(Conv2D(64,3,input_shape=(dim[0], dim[1], 1),padding='valid',activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=3))\nmodel.add(Dropout(rate=.2))\nmodel.add(Conv2D(64,3,padding='valid',activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=3))\nmodel.add(Dropout(rate=.2))\nmodel.add(Conv2D(64,3,padding='valid',activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=3))\nmodel.add(Dropout(rate=.2))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(n_classes, activation=\"softmax\"))\nmodel.summary()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nlearning = model.fit_generator(\n            training_generator, \n#             validation_data=validation_generator,\n            epochs=n_epochs, \n            callbacks=[checkpointer], \n#             use_multiprocessing=True, workers=4,\n            verbose=0, )\npd.DataFrame(learning.history).to_csv('training_history.csv', index_label='epoch')\nvis_learning_curve(learning)\nplt.savefig(\"learning_curve.png\")\nplt.show()\nacc_at_min_loss = learning.history['acc'][np.argmin(learning.history['loss'])]\nprint(\"Min training loss: %.5f, Training accuracy at min loss: %.5f\"%(np.min(learning.history['loss']), acc_at_min_loss ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(snapshot_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_batch, y_batch = AudioFeatureGenerator(X_test, y_test, batch_size=len(X_test), **params)[0]\npredictions = model.predict(X_batch) \ny_predicted = [np.argmax(p) for p in predictions]\ny_true = [np.argmax(y) for y in y_batch]\ntest_score = accuracy_score(y_true, y_predicted)\nprint(\"Test accuracy score: %.5f\"%test_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw a confusion matrix\nconf_matrix = confusion_matrix(y_true, y_predicted, labels=range(n_classes))\nplt.figure(figsize=(20,20))\nplt.imshow(conf_matrix)\nplt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical')\nplt.yticks(range(n_classes), label_encoder.classes_)\nplt.colorbar(shrink=.25);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show which species are correctly classified\nplt.figure(figsize=(15,4))\nplt.title(\"Percent Correctly Classified, by Species\")\npct_correct_by_class = np.zeros(n_classes)\ncounts = np.sum(conf_matrix,axis=1)\nnp.divide(np.array([conf_matrix[i,i] for i in range(n_classes)]), counts, \n          out=pct_correct_by_class, where=counts!=0)*100\nplt.bar(range(n_classes), pct_correct_by_class, .75)\nplt.xlim(-1,91)\nplt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show which species are incorrectly classified\nplt.figure(figsize=(15,4))\nplt.title(\"Percent Correct, by Predicted Class\")\npct_correct_by_predicted_class = np.zeros(n_classes)\ncounts = np.sum(conf_matrix,axis=0)\nnp.divide(np.array([conf_matrix[i,i] for i in range(n_classes)]), counts, \n          out=pct_correct_by_predicted_class, where=counts!=0)*100\nplt.bar(range(n_classes), pct_correct_by_predicted_class, .75)\nplt.xlim(-1,91)\nplt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}