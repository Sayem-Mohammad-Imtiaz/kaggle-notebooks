{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I'll pick the best algorithm from automated model training. This is a classification problem and I'll try automating 3 algorithms, which are K-Nearest Neighbors, SVM and Random Forest. First, import common packages. For training result reporting, also install `beautifultable`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install beautifultable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import argparse\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom beautifultable import BeautifulTable\nfrom pathlib import Path\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# classifier algorithms \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick Look & Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Look at the head preview to get some sense of the datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mobile-price-classification/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also we know that we need to avoid missing values in datasets. We'll need to (i) remove the feature columns if it contains too much missing values, or (ii) impute those missing values whenever possible. Get the quick resume this way:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nMissing values percentage in each column: \")\nprint(df.isnull().sum() / len(df) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool, we see no missing value in all columns.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Target & Config","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the purpose of generalization in automating the process, I find it easier to create a config dictionary as follows. The dict contains all the parameters we'll need to pass to the autoML function.\n* target: The target column name we want to predict. Type: str\n* irrelevant_cols: The column name(s) irrelevant to the target column, that may have been better removed. In this case, I passed a column example `phone_wallpaper` that won't be relevant to predict our target column, which is `price_range`. The column doesn't exist anyway, so comment that out. Type: list of str\n* num_features: Feature columns containing numerical values. Type: list of str\n* cat_features: Feature columns containing categorical values. Type: list of str\n* num_imputer: Imputation strategy for num_features. Type: str\n* cat_imputer: Imputation strategy for cat_features. Type: str\n* scaler: In some cases we might need to add scaling function for the num_features. Type: function\n* encoder: We need to encode cat_features due to computational reason. Type: function\n\nIn this dataset, the feature columns are quite straight forward. We can quickly infer whether or not each of them relate to `price_range`. I personally would use all features available, as follows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {\n    'target': 'price_range',\n    #'irrelevant_cols' : [\"phone_wallpaper\"],\n    'num_features': [\"battery_power\", \"clock_speed\", \"fc\", \"int_memory\", \"m_dep\", \"mobile_wt\", \"n_cores\", \"pc\", \"px_height\", \\\n                     \"px_width\", \"ram\", \"sc_h\", \"sc_w\", \"talk_time\"],\n    'cat_features': [\"blue\", \"dual_sim\", \"four_g\", \"three_g\", \"touch_screen\", \"wifi\"],\n    'num_imputer': 'mean',\n    'cat_imputer': 'most_frequent',\n    'scaler': RobustScaler(),\n    'encoder': OneHotEncoder(handle_unknown='ignore')\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithms and Parameters to Automate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can use a simple dictionary like this one to feed the algorithms and paramaters to the function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"algos_params_classifier = {\n    'KNNClassifier': {\n        'algo': KNeighborsClassifier(),  # change parameter values here\n        'parameter': {  # change tuning parameter values here\n            'algo__n_neighbors': np.arange(1, 51, 2),\n            'algo__weights': ['uniform', 'distance'],\n            'algo__p': [1, 2]\n        }\n    },\n    'SVMClassifier': {\n        'algo': SVC(),  # common max_iter=500\n        'parameter': {\n            'algo__gamma': np.array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n            'algo__C': np.array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n        }\n\n    },\n    'RandomForestClassifier': {\n        'algo': RandomForestClassifier(),  # scaling wont help RF. to explore, use feature importance instead\n        'parameter': {\n            'algo__n_estimators': [100, 150, 200],\n            'algo__max_depth': [20, 50, 80],\n            'algo__max_features': [0.3, 0.6, 0.8],\n            'algo__min_samples_leaf': [1, 5, 10]\n        }\n    }  \n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Define a generic preprocessing function we can use later in the autoML pipeline.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessor(num_features, cat_features, num_imputer=None, cat_imputer=None, scaler=None, encoder=None):\n        '''\n        :param imputer: 'mean', 'median', 'most_frequent', 'constant'\n        :param scaler: MinMaxScaler(), StandardScaler(), RobustScaler()\n        :param encoder: OneHotEncoder(handle_unknown='ignore')\n        :return: preprocessed dataset (imputed, scaled, etc)\n        '''\n        numerical_pipeline = Pipeline([\n            (\"imputer\", SimpleImputer(strategy=num_imputer)),\n            (\"scaler\", scaler)\n        ])\n\n        categorical_pipeline = Pipeline([\n            (\"imputer\", SimpleImputer(strategy=cat_imputer)),\n            (\"encoder\", encoder)\n        ])\n\n        preprocessor = ColumnTransformer([\n            ('numeric', numerical_pipeline, num_features),\n            ('categoric', categorical_pipeline, cat_features),\n        ])\n        \n        return preprocessor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AutoML","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Define the function that takes the dictionary `algos_params_classifier` as an argument. The function will then loop over the algorithms, fit it on the train dataset, outputs:\n* Train and test score from CV score calculations\n* `diff`, which is a method I usually use to quick check the fit. `diff` calculates the absolute difference between train and test score. Hypothetically, the smaller the `diff`, the better the model fit. Hence, the better the model generalizes on unseen data.\n* Best parameters results from RandomizedSearchCV.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def automl_train(algos_params_dict):\n        '''\n        :param algos_params_dict: ML algorithms and parameters to train the dataset\n        :return: saved models from all trainings in .pickle\n        '''\n        # dataset_splitting\n        X = df.drop(columns=config['target'])\n        y = df[config['target']]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n        \n        \n        # add report table\n        table = BeautifulTable(max_width=150)\n        table.column_headers = [\"model\", \"train score\", \"test score\", \"diff\"]\n        \n        \n        # autoML\n        print(f'Train test results\\n')\n        global saved_models\n        saved_models = []\n        for algo, algo_params in algos_params_dict.items():\n            # key, value = algo, algo_params; algo = algo type, algo_params = (i) algo (ii) parameters\n            pipeline = Pipeline([\n                (\"prep\", preprocessor(num_imputer=config['num_imputer'], cat_imputer=config['cat_imputer'], \\\n                         num_features=config['num_features'], cat_features=config['cat_features'], \\\n                         scaler=config['scaler'], encoder=config['encoder'])),\n                (\"algo\", algo_params['algo'])\n            ])\n\n            parameter = algo_params['parameter']\n \n            # selection mode: RandomizedSearchCV\n            model = RandomizedSearchCV(pipeline, param_distributions=parameter, cv=3, n_iter=50, n_jobs=-1, verbose=1,\n                                       random_state=42) # for more verbosity: verbose=degree\n            model.fit(X_train, y_train)\n            model_name = f'model_{algo}.pickle'\n            model_path = Path.cwd() / model_name\n            pickle.dump(model, open(model_path, 'wb'))\n            saved_models.append(model_path)\n            diff = abs(model.score(X_train, y_train) - model.score(X_test, y_test))\n            table.append_row(\n                [algo, model.score(X_train, y_train), model.score(X_test, y_test), diff])\n        print(table)\n        \n        return saved_models\n    \nautoml_train(algos_params_classifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict on Unseen Test Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the result table, I personally would pick SVM model to proceed with prediction and further improvements as it has the smallest `diff` value. Hypothetically speaking, I assume models with smallest `diff` would be the ones having good fit, not overfitting or underfitting. Here we load the test data and the SVM model dumped from automl_train function. Predict the `price_range` on test set and recheck the test dataframe (here `test_df`).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_testset(testset_path, pick_model, models):\n    df_test = pd.read_csv(testset_path)\n    for model in saved_models:\n        if pick_model in str(model):\n            load_model = pickle.load(open(model, 'rb'))\n            pred = load_model.predict(df_test)\n    \n    df_test['prediction'] = pred\n    print(df_test.head())\n    \n    \npredict_testset(testset_path='../input/mobile-price-classification/test.csv', pick_model='SVMClassifier', models=saved_models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"That's it! We've predict the price range in column `prediction`. Unfortunately, this test set doesn't have `price_range` column in it for us to compare and calculate the prediction accuracy score to evaluate the fit.  ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}