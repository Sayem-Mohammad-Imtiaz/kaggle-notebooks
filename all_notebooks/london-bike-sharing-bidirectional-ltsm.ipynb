{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Predicting number of bikes shared, without using number from previous timeframe as feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For interactive plots\n%matplotlib notebook\n# Requires javascript for jupyter lab\n#import matplotlib.ipympl\n#%matplotlib widget\n# Fallback\n#%matplotlib inline\n\n# Imports\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\n\n# Imports \nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout , LSTM , Bidirectional \nfrom keras.regularizers import L1L2\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom matplotlib.gridspec import GridSpec\n\n# for use with kaggle, directory for dataset\nos.chdir(\"/kaggle/input/london-bike-sharing-dataset\")\n\n# Read dataset from file\ndf = pd.read_csv(\n    \"london_merged.csv\", #for use with kaggle and built in dataset\n    #\"LSTMdataset.csv\", #for use with LSTM dataset\n    parse_dates=['timestamp'],\n    index_col=\"timestamp\"\n)\n\ndf.head()\n\n# Expanded index, to put information in separate columns\ndf['hour'] = df.index.hour\ndf['day_of_month'] = df.index.day\ndf['day_of_week'] = df.index.dayofweek\ndf['month'] = df.index.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate and plot Pearsons correlation between variables, investigating linear correlations\ncorr_mat = df.corr()\ncorr_mat.style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Various plots between features and target\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nsns.regplot(ax=axes[0,0], x=\"t1\", y=\"hum\", data=df)\nsns.regplot(ax=axes[0,1], x=\"t1\", y=\"cnt\", data=df)\nsns.regplot(ax=axes[0,2], x=\"t2\", y=\"cnt\", data=df)\nsns.regplot(ax=axes[1,0], x=\"hum\", y=\"cnt\", data=df)\nsns.boxplot(ax=axes[1,1], x=\"weather_code\", y=\"cnt\", data=df)\nsns.boxplot(ax=axes[1,2], x=\"is_holiday\", y=\"cnt\", data=df)\nsns.boxplot(ax=axes[2,0], x=\"is_weekend\", y=\"cnt\", data=df)\nsns.regplot(ax=axes[2,1], x=\"wind_speed\", y=\"cnt\", data=df)\nsns.regplot(ax=axes[2,2], x=\"hour\", y=\"cnt\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Various plots between features and target\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nsns.pointplot(ax=axes[0,0], x='t1',y='cnt',data=df, color=\"red\")\nsns.pointplot(ax=axes[0,1], x='t2',y='cnt', data=df, color=\"green\")\nsns.pointplot(ax=axes[0,2], x=\"hour\", y=\"cnt\", data=df, color=\"blue\")\nsns.pointplot(ax=axes[1,0], x=\"weather_code\", y=\"cnt\", data=df, color=\"cyan\")\nsns.pointplot(ax=axes[1,1], x=\"day_of_week\", y=\"cnt\", data=df, color=\"yellow\")\nsns.pointplot(ax=axes[1,2], x=\"hum\", y=\"cnt\", data=df, color=\"magenta\")\nsns.pointplot(ax=axes[2,0], x=\"wind_speed\", y=\"cnt\", data=df, color=\"black\")\nsns.boxplot(ax=axes[2,1], x=\"is_weekend\", y=\"cnt\", data=df)\nsns.boxplot(ax=axes[2,2], x=\"is_holiday\", y=\"cnt\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If not enough memory, reduce max_depth\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nplt.figure(figsize=(12,12))\nmodel = ExtraTreesClassifier(max_depth=12, bootstrap=True)\nmodel.fit(df.iloc[:,1:],df['cnt'])\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=(df.iloc[:,1:]).columns)\nfeat_importances.nlargest(12).plot(kind='barh')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter out which columns to use\n# Trying to predict without using 'cnt' as feature\n\n# seems to work well with low validation error.\ncolumns = ['cnt', 't1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_weekend', 'is_holiday', 'hour']\n\n# Dataframe with only chosen columns\ndf = df.loc[:, columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build model and precict.\n\n# Input to model\nhours_to_predict = 24\nwindow_in = 5\nbatch_in = 64\nepoch_in = 500\nprint(\"Predicting number of bike shared for \",hours_to_predict,\" hours. \\n\")\nprint(\"Please enter starting hour to predict from: \\n\")\nstartar = input()\nstartar = int(startar)\n\n# Split data into train set (first 80%) and validation set (last 20%)\ntrain_df_size = int(len(df) * 0.8)\nvalidate_df_size = int((len(df) - train_df_size))\ntrain_df = df.iloc[:train_df_size]\nvalidate_df = df.iloc[train_df_size:]\nprint(train_df.iloc[(startar+1):(startar+1+hours_to_predict),:])  # Inspect the dataframe\n\n# Separate the data frames into x and y values, with Y set to predict the next step\ny_train = train_df.iloc[1:, 0].values  # Separate the cnt column as y, start at index 1 (what we want to predict)\nx_train = train_df.iloc[:-1, 1:].values  # Do not include the last element, to match size, remove target\ny_validate = validate_df.iloc[1:, 0].values  # Separate the cnt column as y, start at index 1 (what we want to predict)\nx_validate = validate_df.iloc[:-1, 1:].values  # Do not include the last element, to match size, remove target\n\n# Scalers, for x and y separate, choose one\n# MinMaxscaler\nscaler_y = MinMaxScaler()\nscaler_x = MinMaxScaler()\n# Standardscaler\n#scaler_x = StandardScaler()\n#scaler_y = StandardScaler()\n# Robustscaler\n#scaler_y = RobustScaler()\n#scaler_x = RobustScaler()\n\n# Scaling\ny_train = scaler_y.fit_transform(y_train.reshape(-1, 1))\ny_validate = scaler_y.transform(y_validate.reshape(-1, 1))\nx_train = scaler_x.fit_transform(x_train)\nx_validate = scaler_x.transform(x_validate)\n\n# Create the batch datasets with moving windows\nwindow_size = window_in\ntrain_batch = tf.keras.preprocessing.timeseries_dataset_from_array(\n    data=x_train,\n    targets=y_train,\n    sequence_length=window_size,\n    sequence_stride=1,\n    shuffle=False,\n    batch_size=batch_in)\nvalidate_batch = tf.keras.preprocessing.timeseries_dataset_from_array(\n    data=x_validate,\n    targets=y_validate,\n    sequence_length=window_size,\n    sequence_stride=1,\n    shuffle=False,\n    batch_size=batch_in)\n\n# Regularization parameters\nl1 = 0.0001\nl2 = 0.0001\ndropout_param = 0.5\n\n# Define the LSTM model\nlstm_model = tf.keras.models.Sequential([\n    #tf.keras.layers.LSTM(units=10),\n    #tf.keras.layers.Bidirectional(LSTM(64, activation='relu', kernel_regularizer=L1L2(l1, l2), return_sequences=True), merge_mode='concat'),\n    #tf.keras.layers.Dropout(dropout_param),\n    tf.keras.layers.Bidirectional(LSTM(64, activation='relu', kernel_regularizer=L1L2(l1, l2)), merge_mode='concat'), \n    tf.keras.layers.Dropout(dropout_param),\n    tf.keras.layers.Dense(units=1)\n])\n    \nlstm_model.compile(loss='mean_squared_error', optimizer='adam')\n\n# Train the LSTM model\nhistory = lstm_model.fit(\n    train_batch,\n    epochs=epoch_in,\n    batch_size=batch_in,\n    validation_data=validate_batch,\n    shuffle=False,\n)\n\n# Create the prediction dataset, without the known Y target values\npredict_batch = tf.keras.preprocessing.timeseries_dataset_from_array(\n    data=x_validate,\n    targets=None,\n    sequence_length=window_size,\n    sequence_stride=1,\n    shuffle=False,\n    batch_size=batch_in)\n\n# Predict the future\ny_predicted = lstm_model.predict(predict_batch)\n\n# Plot the loss/val_loss and the predicted values\nplt.figure(figsize=(12, 8))\ngs = GridSpec(1, 4)\nplt.subplot(gs[0, 0])\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.subplot(gs[0, 1:])\nplt.plot(np.arange(0, len(y_train)), y_train, color='r', label=\"history\")  # Uncomment to see training data as well\nplt.plot(np.arange(len(y_train), len(y_train) + len(y_validate)), y_validate, color='b', marker='.',\n         label=\"true validate set\")\nplt.plot(np.arange(len(y_train), len(y_train) + len(y_predicted)), y_predicted, color='g', marker='.',\n         label=\"prediction of validation set\")\nplt.ylabel('Bike Count')\nplt.xlabel('Time Step')\nplt.legend()\nplt.show()\n\n# DT069A Laboration 2 TestPrediction\n\n# Part of x_train to use for prediction selected in for-loop\nx_predict = x_train\n\n# List for predictions\nlistan =  np.array([])\n\nfor i in range(0,hours_to_predict):\n    # Create a working batch\n    window_size = window_in\n    predict_batch_part = tf.keras.preprocessing.timeseries_dataset_from_array(\n        data=x_predict[i+startar:(i+startar+window_size),:],\n        targets=None,\n        sequence_length=window_size,\n        sequence_stride=1,\n        shuffle=False,\n        batch_size=batch_in)\n\n    # Predict using model\n    y_predicted_part = lstm_model.predict(predict_batch_part)\n    \n    # Inverse transform\n    y_predicted_part = scaler_y.inverse_transform(y_predicted_part)\n    \n    # Append list\n    listan = np.append(listan, y_predicted_part)\n\n# Interpret the results\nprint(\"Estimated number of bike shared for next \",hours_to_predict,\" hours: \", listan.astype('int'))\ndf_out = train_df.iloc[(startar+1):(startar+1+hours_to_predict),0]\nprint(\"(Should be around: \",df_out.to_list(),\" for the example data)\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nscaler_y.inverse_transform(y_validate)\nscaler_y.inverse_transform(y_predicted)\n\n#RMSE_minmax = sqrt(mean_squared_error(y_validate[0:-4], y_predicted)) \n#RMSE_standard = sqrt(mean_squared_error(y_validate[0:-4], y_predicted)) \n#RMSE_robust = sqrt(mean_squared_error(y_validate[0:-4], y_predicted)) \n\nprint(RMSE_minmax)\nprint(RMSE_standard)\nprint(RMSE_robust)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nscaler_y.inverse_transform(y_validate)\nscaler_y.inverse_transform(y_predicted)\n\nRMSE_64 = sqrt(mean_squared_error(y_validate[0:-4], y_predicted)) \nprint(RMSE_)\nprint(RMSE_20)\nprint(RMSE_10)\nprint(RMSE_32)\nprint(RMSE_64) # 32 best so far, have not tried 64 yet\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}