{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p align=\"center\">\n<img align=\"center\" src=\"https://image.freepik.com/free-photo/physician-noting-down-symptoms-patient_53876-63308.jpg\">\n\n</p>\n\n[Image source](https://www.freepik.com/photos/medical)","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"text-align: center; color:#01872A; font-size: 80px;\nbackground:#daf2e1; border-radius: 20px;\n\">Simple <br>  Breast Cancer analysis.</h1>\n<h2 style=\"padding: 10px; text-align: center; color:#01872A; font-size: 40px;\nbackground:#daf2e1; border-radius: 20px;\n\">Contents</h2>\n\n## 1. [Overview of data.](#step1)\n## 2. [Univariate analysis of features](#Step2)\n## 3. [Bivariate analysis](#Step3)\n## 4. [Model selection.](#Step4)\n## 5. [Choosing metric.](#Step5)\n## 6. [Fitting model.](#Step6)\n## 7. [Feature engineering.](#Step7)\n## 8. [Hyperparameter tuning.](#Step8)\n## 9. [Metrics analysis.](#Step9)\n## 10. [Conclusion.](#Step10)","metadata":{}},{"cell_type":"code","source":"# Imports.\n# Basic libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.patches as patches\nimport matplotlib.colors\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Sklearn\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, \\\n    GridSearchCV, cross_val_predict\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import MinMaxScaler, power_transform\nimport warnings","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:39.676126Z","iopub.execute_input":"2021-05-21T19:56:39.676829Z","iopub.status.idle":"2021-05-21T19:56:40.958942Z","shell.execute_reply.started":"2021-05-21T19:56:39.676697Z","shell.execute_reply":"2021-05-21T19:56:40.957737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(action='ignore')\npd.options.display.max_columns = None","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:40.961042Z","iopub.execute_input":"2021-05-21T19:56:40.961338Z","iopub.status.idle":"2021-05-21T19:56:40.966246Z","shell.execute_reply.started":"2021-05-21T19:56:40.961311Z","shell.execute_reply":"2021-05-21T19:56:40.965072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configure Matplotlib and seaborn\nplt.style.use('seaborn-muted')\nsns.set_palette(\"muted\")\nplt.rcParams['figure.figsize'] = (16,5);\nplt.rcParams['figure.facecolor'] = '#daf2e1'\nplt.rcParams['axes.facecolor'] = '#daf2e1'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['lines.linewidth'] = 5\nplt.rcParams['figure.titlesize'] = 30\nplt.rcParams['axes.titlesize'] = 25\nplt.rcParams['image.cmap']=cm.tab10\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['xtick.labelsize']=14\nplt.rcParams['ytick.labelsize']=14\ncmap = cm.tab10\nblue = cmap.colors[0]\nblue_hex = matplotlib.colors.to_hex(blue)\norange = cmap.colors[1]\norange_hex = matplotlib.colors.to_hex(orange)\ngreen = cmap.colors[2]\ngreen_hex = matplotlib.colors.to_hex(green)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:40.967569Z","iopub.execute_input":"2021-05-21T19:56:40.967883Z","iopub.status.idle":"2021-05-21T19:56:40.981064Z","shell.execute_reply.started":"2021-05-21T19:56:40.967855Z","shell.execute_reply":"2021-05-21T19:56:40.980018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [blue, orange,\n                                                             green])\nmatplotlib.cm.register_cmap(\"mycolormap\", cmap)\ncpal = sns.color_palette(\"mycolormap\", n_colors=60)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:40.982516Z","iopub.execute_input":"2021-05-21T19:56:40.982848Z","iopub.status.idle":"2021-05-21T19:56:40.996896Z","shell.execute_reply.started":"2021-05-21T19:56:40.98281Z","shell.execute_reply":"2021-05-21T19:56:40.995909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"step1\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 1. Overview of data.</h2>\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv',\n                 index_col='id')","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:40.999958Z","iopub.execute_input":"2021-05-21T19:56:41.000547Z","iopub.status.idle":"2021-05-21T19:56:41.046365Z","shell.execute_reply.started":"2021-05-21T19:56:41.000505Z","shell.execute_reply":"2021-05-21T19:56:41.045575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Get overall statistics of DataFrame (selected features).</h3>","metadata":{}},{"cell_type":"code","source":"cmap2 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [orange, blue],\n                                                 N=2)\ndf_t = df.describe().T\nselected_features = ['area_worst', 'perimeter_worst', 'texture_worst',\n                     'radius_worst', 'fractal_dimension_worst',\n                     'symmetry_worst', 'Unnamed: 32']\nselected_columns =  ['count', 'mean', 'max']\ndf_stats = df_t.loc[selected_features, selected_columns]\ndf_stats.sort_values('mean', inplace=True, ascending=False)\ndf_stats.loc['diagnosis'] = [df['diagnosis'].count(), np.nan, np.nan]\ndf_stats['dtype'] = df[selected_features + ['diagnosis']].dtypes.astype('str')\ndef background_color(s):\n    is_special = s.isin([0, 'object'])\n    return [f'background-color: {orange_hex}' if v else '' for v in is_special]\nformat_dict = {selected_column: \"{:10,.2f}\" for selected_column in selected_columns}\ndf_stats.style.apply(background_color).format(format_dict)\\\n                 .bar(subset=[\"mean\"], color=green_hex)\\\n                 .bar(subset=[\"max\"], color=blue_hex)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:41.048547Z","iopub.execute_input":"2021-05-21T19:56:41.049109Z","iopub.status.idle":"2021-05-21T19:56:41.199374Z","shell.execute_reply.started":"2021-05-21T19:56:41.049061Z","shell.execute_reply":"2021-05-21T19:56:41.19868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 1 results:</h2>\n\n### 1. **Feature 'Unnamed: 32' should be removed** as it contains only empty values.\n### 2. **Dependent variable is a categorical one (type: object)**, so we need to encode it. The M(malignant) label should be 1 and B(benign) should be 0.\n### 3. There are features that have different magnitude (mean values differ, for example, from 0.08 to 880), so we will **need to do scaling for linear models**.\n### 4. There are no missing values, so **no imputing needed**.\n### 5. The dataset is very small, so no need to divide into the train/validation/test sets, for scoring **standard 5 fold cross-validation** will be used.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df.drop(['Unnamed: 32'], axis=1, inplace=True)\ndf.sort_index(axis=1, inplace=True)\n\nencoder = LabelEncoder()\ndf['diagnosis'] = encoder.fit_transform(df['diagnosis'])\nencoder_mapping = dict(zip(encoder.classes_,\n                           encoder.transform(encoder.classes_)))","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:41.200487Z","iopub.execute_input":"2021-05-21T19:56:41.200975Z","iopub.status.idle":"2021-05-21T19:56:41.208764Z","shell.execute_reply.started":"2021-05-21T19:56:41.200932Z","shell.execute_reply":"2021-05-21T19:56:41.207808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id=\"Step2\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 2. Univariate analysis <br> (Individual feature statistics).</h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Histogram plots of selected features.</h3>","metadata":{}},{"cell_type":"code","source":"def feature_distibutions(df, log=False):\n    columns = ['area_worst', 'concave points_worst', 'concavity_worst',\n               'perimeter_worst', 'radius_worst']\n    plot_columns = 2\n    plot_rows = int(np.ceil(len(columns) / plot_columns))\n    fig, axes = plt.subplots(plot_rows, plot_columns, figsize=(16, 8))\n    plot_axes = df[columns].hist(ax=axes.ravel()[:len(columns)], legend=True)\n    for ax in plot_axes.ravel():\n        ax.legend(prop={'size': 15})\n    df[columns].plot(kind='kde', secondary_y=True, subplots=True,\n                     ax=axes.ravel()[:len(columns)], color=orange, legend=False)\n    last_ax = axes.ravel()[-1]\n    last_ax.axis('off')\n    if log:\n        text = 'Feature distributions seem\\na bit closer to normal.'\n    else:\n        text = 'Feature distributions are\\nnot normal.'\n    last_ax.text(0.1, 0.4, text,\n           transform = last_ax.transAxes, size=25, fontweight='bold')\n    plt.suptitle('Feature distributions')\n    plt.tight_layout();\nfeature_distibutions(df)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:41.209943Z","iopub.execute_input":"2021-05-21T19:56:41.210264Z","iopub.status.idle":"2021-05-21T19:56:42.947256Z","shell.execute_reply.started":"2021-05-21T19:56:41.210233Z","shell.execute_reply":"2021-05-21T19:56:42.945784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Compare feature distributions of both classes.</h3>\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"columns = ['area_worst', 'concave points_worst', 'concavity_worst',\n           'perimeter_worst', 'radius_worst']\nmalignant = df[df['diagnosis'] == 1][columns]\nbenign = df[df['diagnosis'] == 0][columns]\nplot_columns = 2\nplot_rows = int(np.ceil(len(columns) / plot_columns))\nfig, axes = plt.subplots(plot_rows, plot_columns, figsize=(16, 8))\nfor column, ax in zip(columns, axes.ravel()):\n    ax.hist(malignant[column], label='Malignant', density=True, alpha = 0.5,\n            color=blue)\n    ax.hist(benign[column], label='Benign', density=True, alpha = 0.5,\n            color=orange)\n    ax.legend(fontsize=14)\n    ax.set_title(column)\nplt.suptitle('Feature distributions of classes')\nlast_ax = axes.ravel()[-1]\nlast_ax.axis('off')\nlast_ax.text(0.1, 0.4, 'Features can separate classes \\nas there is little'\n                       ' overlap in\\nfeature distributions.',\n       transform=last_ax.transAxes, size=25, fontweight='bold')\nplt.tight_layout();","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:42.949031Z","iopub.execute_input":"2021-05-21T19:56:42.949495Z","iopub.status.idle":"2021-05-21T19:56:44.383461Z","shell.execute_reply.started":"2021-05-21T19:56:42.949436Z","shell.execute_reply":"2021-05-21T19:56:44.380714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Check dependent variable distributions.</h3>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def autolabel(rects, ax, decimals=2):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        value = round(height, decimals)\n        ax.annotate('{}'.format(value),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  size=18,# 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\ndiagnosis_count = df['diagnosis'].value_counts()\ndiagnosis_percent = \\\n    df['diagnosis'].value_counts(normalize=True).apply(round, args=(2,))\nfig, ax = plt.subplots(figsize=(16, 7))\nfig.suptitle('Dependent variable distribution')\nrects = ax.bar(diagnosis_count.index, diagnosis_count.values,\n               color=[blue, orange], zorder=3)\nax.set_xticks([0, 1])\nax.set_ylim(0, 400)\nax.set_xticklabels(['Benign', 'Malignant'])\nax.set_title(f'Benign: {diagnosis_percent.loc[0] * 100}%\\n'\n              f'Malignant: {diagnosis_percent.loc[1] * 100}%\\n', color=orange)\nax.text(0.65, 0.70, f'Imbalanced classes\\n',\n             transform = ax.transAxes, size=18, fontweight='bold')\nautolabel(rects, ax, decimals=2)\nplt.tight_layout();","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:44.384947Z","iopub.execute_input":"2021-05-21T19:56:44.385351Z","iopub.status.idle":"2021-05-21T19:56:44.628842Z","shell.execute_reply.started":"2021-05-21T19:56:44.38531Z","shell.execute_reply":"2021-05-21T19:56:44.627884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_scores(score_df, modelname, score):\n    if modelname not in score_df['ModelName'].values:\n        model_dict = dict()\n        model_dict['ModelName'] = modelname\n        model_dict['Score'] = score\n        score_df = score_df.append(model_dict, ignore_index=True)\n    return score_df\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:44.630121Z","iopub.execute_input":"2021-05-21T19:56:44.630411Z","iopub.status.idle":"2021-05-21T19:56:44.636308Z","shell.execute_reply.started":"2021-05-21T19:56:44.630381Z","shell.execute_reply":"2021-05-21T19:56:44.635168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 2 results:</h2>\n\n### 1. **The features can distinguish classes**, e.g. 'radius_worst' feature  has very little overlap between classes.\n### 2. **We got some skewed distributions**, so it is possible to try to change distributions to improve scoring (power transform).\n### 3. **The dependent variable distribution is imbalanced**, so stratification will be useful during cross-validation to balance classes.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div id=\"Step3\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 3. Bivariate analysis <br> (analysis of pairs of feature).</h2>\n","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\nsorted_index = \\\n    corr['diagnosis'].apply(np.abs).sort_values(ascending=False).index\nsorted_corr = corr[sorted_index].reindex(sorted_index)\n\nmost_correlated_index = sorted_corr[(sorted_corr.iloc[:, 0] >= 0.7) |\n                                    (sorted_corr.iloc[:, 0] <= -0.7)].index\nsorted_corr = sorted_corr.loc[most_correlated_index, most_correlated_index]","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:44.638091Z","iopub.execute_input":"2021-05-21T19:56:44.638518Z","iopub.status.idle":"2021-05-21T19:56:44.657393Z","shell.execute_reply.started":"2021-05-21T19:56:44.638478Z","shell.execute_reply":"2021-05-21T19:56:44.65633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Create the correlation DataFrame and sort it according to the 'Diagnosis'\ncorrelation by absolute values. Choose only correlations with dependent variable with 0.7 or higher or with\n -0.7 or lower.</h3>","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 8))\nsns.heatmap(sorted_corr, ax=ax, annot=True, fmt='.2f',\n            cmap='YlGnBu',\n            cbar=False,\n            square=True)\nfig.suptitle('Most correlated features.')\nrect = patches.Rectangle((4, 4), 4.95, 4.95, linewidth=5, edgecolor=orange,\n                         facecolor='none')\nax.add_patch(rect)\nax.text(-0.4, -0.2, 'High correlation\\nwith dependent\\nvariable',\n       transform = ax.transAxes, size=15, fontweight='bold')\nrect = patches.Rectangle((0.05, 1), 1, 7.95, linewidth=5, edgecolor=orange,\n                         facecolor='none')\nax.add_patch(rect)\nax.text(1.05, 0.25, 'Features are\\nhighly correlated\\nwith each other',\n       transform = ax.transAxes, size=15, fontweight='bold')\nplt.tight_layout();","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:44.659119Z","iopub.execute_input":"2021-05-21T19:56:44.659911Z","iopub.status.idle":"2021-05-21T19:56:45.427121Z","shell.execute_reply.started":"2021-05-21T19:56:44.659867Z","shell.execute_reply":"2021-05-21T19:56:45.426054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Pairplots plots for most correlated features.</h3>\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"columns = ['radius_worst', 'area_worst', 'concave points_worst',\n          'perimeter_worst', 'diagnosis']","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:45.428333Z","iopub.execute_input":"2021-05-21T19:56:45.428651Z","iopub.status.idle":"2021-05-21T19:56:45.432903Z","shell.execute_reply.started":"2021-05-21T19:56:45.428623Z","shell.execute_reply":"2021-05-21T19:56:45.431913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with sns.plotting_context(\"notebook\", font_scale=1.2):\n    axes = sns.pairplot(df[columns], hue='diagnosis', )\naxes.fig.suptitle('Pairplots for most correlated features')\naxes._legend.remove()\nplt.tight_layout();","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-21T19:56:45.433829Z","iopub.execute_input":"2021-05-21T19:56:45.434099Z","iopub.status.idle":"2021-05-21T19:56:50.014653Z","shell.execute_reply.started":"2021-05-21T19:56:45.434073Z","shell.execute_reply":"2021-05-21T19:56:50.013655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 3 results:</h2>\n\n### 1. **There are some strong feature correlations with dependent variable**. At the same time those features are correlated with each other, e.g. 'concave_points_mean'  and 'concave_points_worst', so **feature selection algorithms can be used**.\n### 2. **All the top correlated features are positively correlated** with dependent  variable. The higher value of independent features, more likely the dependent  feature will be 1.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div id=\"Step4\">\n</div>\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 4. Model selection.</h2>\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">To choose between linear and non-liner models perform a check if the\nproblem may be solved with a linear model with PCA with 2 components.</h3>\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"X = df.drop('diagnosis', axis=1)\ny = df['diagnosis']\n\n# Standardization before using of PCA\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns,\n                        index=X.index)\npca = PCA(n_components=2)\nX_tr = pca.fit_transform(X_scaled, y)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:50.015935Z","iopub.execute_input":"2021-05-21T19:56:50.016237Z","iopub.status.idle":"2021-05-21T19:56:50.047772Z","shell.execute_reply.started":"2021-05-21T19:56:50.016208Z","shell.execute_reply":"2021-05-21T19:56:50.046602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 8))\nsns.scatterplot(X_tr[:, 0], X_tr[:, 1], hue=y, ax=ax, style=y, s=100)\nax.set_title('Two component PCA ')\nax.plot([-5, 10], [-7.4, 12.5], color=green)\nax.legend(['Example line', 'Malignant', 'Benign'], prop={'size': 20})\nax.set_ylim((-5, 10))\nax.set_xlim((-5, 10))\nax.text(0.75, 0.7, 'Classes are mostly\\nlinearly separable',\n       transform = ax.transAxes, size=20, fontweight='bold');","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:50.049852Z","iopub.execute_input":"2021-05-21T19:56:50.050316Z","iopub.status.idle":"2021-05-21T19:56:50.394026Z","shell.execute_reply.started":"2021-05-21T19:56:50.05027Z","shell.execute_reply":"2021-05-21T19:56:50.393226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1;border-radius: 20px;\n\">Step 4 results:</h2>\n\n### 1. Data is **mostly linearly separable**, so linear models could be used. To  keep it simple, basic linear model will be used: LogisticRegression.\n### 2. Linear models can probably achieve **good results**.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div id=\"Step5\">\n</div>\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 5. Choosing metric.</h2>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">As a metric I will use recall, where:</h3>\n$\n\\ \\huge{Recall} = \\frac {True\\ Positives} {True\\ Positives\\ +\\ False\\ Negatives}\n$\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Recall shows how good we can identify positive (M(malignant)) cases and immediately start responding to such serious diagnosis.</h3>\n","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 5 results:</h2>\n\n## 1. **Appropriate** metric selected for model evaluation (Recall).","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Step6\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 6. Fitting model.</h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Create model and check its results with both scaled and unscaled data.</h3>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def score_model(model, X_scored, y, scoring):\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    score = cross_val_score(model, X_scored, y, scoring=scoring,\n                            cv=skf)\n    return score\n\ndef compare_scores(score_1, score_2, score_name_1, score_name_2, description):\n    score_range = range(1, 6)\n    fig, ax = plt.subplots(figsize=(16, 8))\n    ax.plot(score_range, score_1, label=f'{score_name_1} CV')\n    ax.plot(score_range, score_2, label=f'{score_name_2} CV')\n    plt.suptitle('Logistic regression cross-validation recall')\n    ax.set_title('Metric: Recall', color=orange, size=25)\n    ax.set_ylim([0.85,1.05])\n    ax.set_xlim([0.80,5.5])\n    ax.set_xlabel('Results')\n    ax.set_ylabel('Score')\n    ax.set_xticks(score_range)\n    score_1_mean = score_1.mean()\n    ax.axhline(y=score_1_mean, linestyle='--', label=f'{score_name_1} mean',\n               alpha=0.5)\n    ax.annotate(format(score_1_mean, '.3f'), xy=(5, score_1_mean), size=20)\n    score_2_mean = score_2.mean()\n    ax.axhline(y=score_2_mean, linestyle='--', label=f'{score_name_2} mean',\n               color=orange, alpha=0.5)\n    ax.annotate(format(score_2_mean, '.3f'), xy=(5,score_2_mean), size=20)\n    ax.legend(loc='best', prop={'size': 20})\n    ax.text(0.3, 0.85, description,\n       transform = ax.transAxes, size=20, fontweight='bold')\n    ax.set_xlabel('Results', fontsize=18)\n    ax.set_ylabel('Score', fontsize=18)\n    plt.tight_layout();\n\nunscaled_recall = score_model(LogisticRegression(random_state=0), X, y,\n                              'recall')\nscaled_recall = score_model(LogisticRegression(random_state=0), X_scaled, y,\n                            'recall')\ndescription = 'Scaling improves model score'\ncompare_scores(unscaled_recall, scaled_recall, 'Unscaled', 'Scaled', description)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:50.395043Z","iopub.execute_input":"2021-05-21T19:56:50.395427Z","iopub.status.idle":"2021-05-21T19:56:51.250921Z","shell.execute_reply.started":"2021-05-21T19:56:50.395399Z","shell.execute_reply":"2021-05-21T19:56:51.250026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with scores\nscore_df = pd.DataFrame(columns=['ModelName', 'Score'])\nscore_df = update_scores(score_df, 'Baseline', unscaled_recall.mean())\nscore_df = update_scores(score_df, 'Scaled features', scaled_recall.mean())","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:51.252258Z","iopub.execute_input":"2021-05-21T19:56:51.252552Z","iopub.status.idle":"2021-05-21T19:56:51.266341Z","shell.execute_reply.started":"2021-05-21T19:56:51.252523Z","shell.execute_reply":"2021-05-21T19:56:51.265403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 6 results:</h2>\n\n### 1. **Logistic Regression shows pretty good results**: we get over 91% of the Malignant cases right with raw features.\n### 2. **Model fitted with scaled features performs better** than model with raw  features. Recall raises from 91,5% to 96%.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div id=\"Step7\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 7. Improving performance with feature engineering.</h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">To improve performance we can try following ideas:</h3>\n\n### 1. Feature selection techniques.\n### 2. Power transform the data to make distributions more Gaussian.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Feature selection based on feature importance.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nrfecv = RFECV(LogisticRegression(random_state=0), cv=skf, scoring='recall').fit\\\n    (X_scaled, y)\nused_features = rfecv.get_support()\nX_scaled_rfe = X_scaled.loc[:, used_features]\nselected_recall = \\\n    score_model(LogisticRegression(random_state=0), X_scaled_rfe, y, 'recall')\ndescription = 'Feature selection\\nimproves model score'\ncompare_scores(scaled_recall, selected_recall, 'Scaled', 'Selected', description)\nscore_df = update_scores(score_df, 'Feature selection + Scaling',\n                       selected_recall.mean())","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:51.267511Z","iopub.execute_input":"2021-05-21T19:56:51.268043Z","iopub.status.idle":"2021-05-21T19:56:53.964949Z","shell.execute_reply.started":"2021-05-21T19:56:51.267984Z","shell.execute_reply":"2021-05-21T19:56:53.963767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = \\\n    pd.Series(rfecv.estimator_.coef_[0], index=X_scaled.columns[used_features])\nfeatures = features.apply(np.abs).sort_values(ascending=False)[:7]\n\nfig, ax = plt.subplots(figsize=(16, 7))\nrects = ax.bar(x=features.index, height=features,\n               zorder=2, color=[blue, orange, green])\nautolabel(rects, ax, decimals=2)\nplt.suptitle('Top 7 feature importances (absolute)')\nax.set_title(f'Before feature selection: {len(X_scaled.columns)} features\\n'\n             f'After feature selection: {len(X_scaled_rfe.columns)} '\n             f'features\\n',\n             color=orange)\nax.set_ylim(0.0, 1.40)\nax.tick_params(axis='both', which='major', labelsize=16)\nplt.tight_layout();","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:53.966197Z","iopub.execute_input":"2021-05-21T19:56:53.966511Z","iopub.status.idle":"2021-05-21T19:56:54.285031Z","shell.execute_reply.started":"2021-05-21T19:56:53.96648Z","shell.execute_reply":"2021-05-21T19:56:54.28385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Power transform the data</h3>\n","metadata":{}},{"cell_type":"code","source":"X_power_transform \\\n    = pd.DataFrame(power_transform(X_scaled_rfe, standardize=True),\n                   columns=X_scaled_rfe.columns, index=X_scaled_rfe.index)\nfeature_distibutions(X_power_transform, log=True)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:54.286675Z","iopub.execute_input":"2021-05-21T19:56:54.2871Z","iopub.status.idle":"2021-05-21T19:56:56.070562Z","shell.execute_reply.started":"2021-05-21T19:56:54.287059Z","shell.execute_reply":"2021-05-21T19:56:56.069487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"power_transform_recall = \\\n    score_model(LogisticRegression(random_state=0), X_power_transform, y, 'recall')\ndescription = 'Power transform does\\nnot improve model score'\ncompare_scores(selected_recall, power_transform_recall, 'Selected',\n               'Power transform', description);\nscore_df = update_scores(score_df, 'Power transform', power_transform_recall\n                         .mean())","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:56.072228Z","iopub.execute_input":"2021-05-21T19:56:56.072668Z","iopub.status.idle":"2021-05-21T19:56:56.802979Z","shell.execute_reply.started":"2021-05-21T19:56:56.072628Z","shell.execute_reply":"2021-05-21T19:56:56.801786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 7 results:</h2>\n\n### 1. **Feature selection improves score** from 96% to 97% and reduces dataset from  30 features to 23 features.\n### 2. **Power transform doesn't give any score improvements**.\n","metadata":{}},{"cell_type":"markdown","source":"<div id=\"Step8\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 8. Improving performance with hyperparameter tuning.</h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Define the grid and use GridSearch to find the best parameters.</h3>\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"grid = {'penalty': ['l2', 'l1'],\n        'solver': ['lbfgs', 'liblinear', 'sag', 'saga'],\n        'C': [0.0001, 0.01, 0.01, 0.1, 0.8, 1, 1.2, 10, 100, 1000],\n        }\n\ngrid_search =\\\n    GridSearchCV(estimator=LogisticRegression(random_state=0, n_jobs=-1),\n                 param_grid=grid, scoring='recall', n_jobs=-1,\n                 cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0))\ngrid_search.fit(X_scaled_rfe, y)\nbest_params = grid_search.best_params_\ngrid_search_score = \\\n    score_model(LogisticRegression(n_jobs=-1, random_state=0, **best_params),\n                X_scaled_rfe, y, scoring='recall')\ndescription = 'Grid search shows the same results\\n'\ncompare_scores(selected_recall, grid_search_score, 'Selected',\n               'Power transform', description);\ntransformed_dict = {'ModelName': 'Grid search',\n               'Score': grid_search.best_score_}","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:56:56.804512Z","iopub.execute_input":"2021-05-21T19:56:56.804924Z","iopub.status.idle":"2021-05-21T19:57:05.716736Z","shell.execute_reply.started":"2021-05-21T19:56:56.804879Z","shell.execute_reply":"2021-05-21T19:57:05.715651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 8 results:</h2>\n\n### 1. **GridSearch CV doesn't make the results better** with current model.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div id=\"Step9\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 9. Metrics analysis.</h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Basic statistics, confusion matrix, ROC Curve.</h3>","metadata":{}},{"cell_type":"code","source":"def results_plot(model, X, y):\n    fig = plt.figure(figsize=(16, 8))\n    layout = (1, 2)\n    cm_ax = plt.subplot2grid(layout, (0, 0))\n    roc_ax = plt.subplot2grid(layout, (0, 1))\n\n    scoring_metrics = ['accuracy', 'recall', 'precision']\n    title = ''\n    for metric in scoring_metrics:\n        score = score_model(model, X_scaled_rfe, y, scoring=metric).mean()\n        title += f'{metric.capitalize()}: {round(score, 4)}   '\n    plt.suptitle('Results\\n\\n')\n    fig.text(0.25, 0.85, title,\n             transform=fig.transFigure, size=20, fontweight='bold', color=orange)\n    predictions = cross_val_predict(model, X, y, cv=skf)\n    conf = confusion_matrix(y, predictions)\n    sns.heatmap(conf, annot=True, fmt='.0f', ax=cm_ax,\n                cmap=cpal,\n                xticklabels=['Benign', 'Malignant'],\n                annot_kws={'fontsize': 20})\n    cm_ax.set_yticklabels(['Benign', 'Malignant'], va='center')\n    cm_ax.set_xlabel('Predicted', size=20)\n    cm_ax.set_ylabel('Actual', size=20)\n    cm_ax.set_title('Confusion matrix')\n\n    y_probs = cross_val_predict(LogisticRegression(random_state=0), X_scaled_rfe, y,\n                          method='predict_proba', cv=skf)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y, y_probs)\n    roc_auc = auc(fpr, tpr)\n    roc_ax.plot(fpr, tpr, label=f'Logistic regression (AUC: {round(roc_auc, 3)})')\n    roc_ax.plot([0, 1], [0, 1], label='Guessing')\n    roc_ax.set_title('ROC Curve')\n    roc_ax.set_xlabel('False Positive Rate', size=20)\n    roc_ax.set_ylabel('True Positive Rate', size=20)\n    roc_ax.legend(loc='best', prop={'size': 15})\n    plt.tight_layout();\n\nmodel = LogisticRegression(random_state=0)\nresults_plot(model, X_scaled_rfe, y)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:57:05.718421Z","iopub.execute_input":"2021-05-21T19:57:05.718777Z","iopub.status.idle":"2021-05-21T19:57:06.928376Z","shell.execute_reply.started":"2021-05-21T19:57:05.718746Z","shell.execute_reply":"2021-05-21T19:57:06.927613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Prediction analysis.</h3>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\ny_pred = cross_val_predict(model, X_scaled_rfe, y, cv=skf)\ntrue_positive_mask = ((y == y_pred) & (y.isin([1])))\ntrue_negative_mask = ((y == y_pred) & (y.isin([0])))\nfalse_positive_mask = ((y != y_pred) & (y.isin([0])))\nfalse_negative_mask = ((y != y_pred) & (y.isin([1])))\n\nfig, ax = plt.subplots(figsize=(16, 8))\npca = PCA(n_components=2)\nX_pca = pd.DataFrame(pca.fit_transform(X_scaled_rfe, y),\n                    columns=['PCA1', 'PCA2'], index=X_scaled_rfe.index)\n\nax.scatter(X_pca.loc[true_positive_mask, 'PCA1'],\n           X_pca.loc[true_positive_mask, 'PCA2'],\n           color=blue, marker='.', label='True positives', s=120)\nax.scatter(X_pca.loc[true_negative_mask, 'PCA1'],\n           X_pca.loc[true_negative_mask, 'PCA2'],\n           color=orange, marker='.', label='True negatives', s=120)\nax.scatter(X_pca.loc[false_positive_mask, 'PCA1'],\n           X_pca.loc[false_positive_mask, 'PCA2'],\n           color=green, marker='X', label='False positives', s=200)\nax.scatter(X_pca.loc[false_negative_mask, 'PCA1'],\n           X_pca.loc[false_negative_mask, 'PCA2'],\n           c='black', marker='X', label='False negatives', s=200)\nax.text(0.05, 0.10, 'Most of the false negatives\\n'\n                    'are close to the true\\n'\n                    'negatives population',\n        transform = ax.transAxes, size=15, fontweight='bold')\nax.set_title('PCA Prediction analysis')\nax.set_xlim((-5, 10))\nax.set_xlabel('PCA 1', fontsize=18)\nax.set_ylabel('PCA 2', fontsize=18)\nax.legend(prop={'size': 15});\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:57:06.929458Z","iopub.execute_input":"2021-05-21T19:57:06.92992Z","iopub.status.idle":"2021-05-21T19:57:07.456453Z","shell.execute_reply.started":"2021-05-21T19:57:06.929879Z","shell.execute_reply":"2021-05-21T19:57:07.455696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 9 results:</h2>\n\n### 1. **Most of false negative predictions seem to be very close to negative value**  population, so it seems to be impossible to improve recall with current feature set and current model.\n### 2. **Overall results are good enough with current model**.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div id=\"Step10\">\n</div>","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div id=\"step1\">\n</div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 10. Conclusion.</h2>\n","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 7))\nscore_df.sort_values(['Score'], inplace=True, ascending=False)\nrects = ax.bar(x=score_df['ModelName'], height=score_df['Score'],\n               zorder=2, color=[ blue, orange, green])\nautolabel(rects, ax, decimals=4)\nplt.suptitle('Scores')\nax.set_title('Metric: Recall', color=orange)\nax.set_ylim(0.0, 1.40)\nax.text(0.03, 0.8, 'Best feature engineering',\n        transform = ax.transAxes, size=17, fontweight='bold')\nax.tick_params(axis='both', which='major', labelsize=16)\nplt.tight_layout();","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T19:57:07.457534Z","iopub.execute_input":"2021-05-21T19:57:07.457969Z","iopub.status.idle":"2021-05-21T19:57:07.674606Z","shell.execute_reply.started":"2021-05-21T19:57:07.457931Z","shell.execute_reply":"2021-05-21T19:57:07.673567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Logistic regression shows good results on this dataset even if used on raw features.\n### 2. Feature selection + feature scaling allows to create a prediction which is sufficiently better than a baseline.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Further analysis.</h3>\n\n### 1. **Use non-linear models**: K-Nearest Neighbours, Random Forest, SVM, Gradient Boosting.\n### 2. **Use wider feature engineering** to distinguish False Negatives.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<h2 style=\"padding: 30px; text-align: center; color:#01872A; font-size: 40px;\nbackground:#daf2e1; border-radius: 20px;\">\nThank you for reading.<br> Any feedback is highly appreciated. </h2>","metadata":{"pycharm":{"name":"#%% md\n"}}}]}