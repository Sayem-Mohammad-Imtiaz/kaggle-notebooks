{"cells":[{"metadata":{},"cell_type":"markdown","source":"**The main goal of this kernel is to make the results comprensehible**. If we create a plot to show the words most used in the news but we can't understand them because they are in chinese, then our work has been for nothing. Remember: the main goal of Data Analysis is to extract useful conclusions from the data.  \n\nI know a little Chinese so that's why this dataset interested me. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1-Inspection of the dataset","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"#libraries I'm going to use\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('../input/chinese-official-daily-news-since-2016/chinese_news.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns of the dataset are:\n\n* Date. When the news were published.\n\n* Tag. The topic of what are the news about.\n\n* Headline. The headline.\n\n* Content. The text in which the news are detailed explained.\n\nLet's check the possible topics the news can belong to.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.tag.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 国内 (guónèi) : these characters together literally mean 'inside country'. The correct translation is 'domestic'.\n* 国际(guójì) : this means 'international'.\n* 详细全文(xiángxì quánwén): this means 'full text'.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"#I'm going to replace the tags by their English version\ndataset['tag'] = dataset['tag'].str.replace('国内', 'domestic news')\ndataset['tag'] = dataset['tag'].str.replace('国际', 'international news')\ndataset['tag'] = dataset['tag'].str.replace('详细全文', 'detailed news')\n\n#and create a new column year\ndataset['year'] = pd.DatetimeIndex(dataset['date']).year\n#change the type of the column to string\ndataset['year'] = dataset['year'].apply(str)\n#unique values of the new column 'year'\nprint(dataset['year'].unique())\n#make a new dataset counting the number of news by year\nd2=dataset.groupby('year').size().reset_index(name='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"fig = px.bar(d2,x='year',y='count',title='Amount of news by year',color='year',color_discrete_map={'2016': '#e8e3cc', \n                                                   '2017': '#d7a449', '2018': '#db3f29'})\n\nfig.update_layout(\n   paper_bgcolor='#0b1f65',\n   plot_bgcolor='#0b1f65',\n    font_family=\"Arial\",\n    font_color=\"white\",\n    title_font_family=\"Arial\",\n    title_font_color=\"white\",\n    legend_title_font_color=\"white\",\n    xaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    'tickformat': 'd'\n    },\n    yaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    'title': 'Amount of news'\n    }\n    \n)\n\n\n\n\n# plot\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#Amounts of news by tag\nd3=dataset.groupby('tag').size().reset_index(name='count')\n\n\nfig = px.bar(d3,x='tag',y='count',title='News by tag',color='tag',color_discrete_map={'detailed news': '#e8e3cc', \n                                                   'domestic news': '#d7a449', 'international news': '#db3f29'})\n\nfig.update_layout(\n   paper_bgcolor='#0b1f65',\n   plot_bgcolor='#0b1f65',\n    font_family=\"Arial\",\n    font_color=\"white\",\n    title_font_family=\"Arial\",\n    title_font_color=\"white\",\n    legend_title_font_color=\"white\",\n    xaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    'tickformat': 'd'\n    },\n    yaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    'title': 'Amount of news'\n    }\n    \n)\n\n\n\n\n# plot\nfig.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#news by month\ndataset['month'] = pd.DatetimeIndex(dataset['date']).month\ndataset['month'] = dataset['month'].apply(str)\nd4=dataset.groupby('month').size().reset_index(name='count')\n\ncolors = ['crimson',] * 12\n\nfig = go.Figure(data=[go.Bar(\n    x=d4.month,\n    y=d4['count'],\n    marker_color=colors\n)])\n\nfig.update_layout(\n   paper_bgcolor='#0b1f65',\n   plot_bgcolor='#0b1f65',\n    font_family=\"Arial\",\n    font_color=\"white\",\n    title_font_family=\"Arial\",\n    title_font_color=\"white\",\n    legend_title_font_color=\"white\",\n    xaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    \n    },\n    yaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    'title': 'Amount of news'\n    }\n    \n)\n\n\n\n\n# plot\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more news published during May, August, September and March.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2-Analysis of the text\n\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install -U spacy","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy import displacy\nfrom spacy.lang.zh import Chinese\n# Disable jieba to use character segmentation\nChinese.Defaults.use_jieba = False\nnlp = Chinese()\n\n# Disable jieba through tokenizer config options\ncfg = {\"use_jieba\": False}\nnlp = Chinese(meta={\"tokenizer\": {\"config\": cfg}})\n# Load with \"default\" model provided by pkuseg\ncfg = {\"pkuseg_model\": \"default\", \"require_pkuseg\": True}\nnlp = Chinese(meta={\"tokenizer\": {\"config\": cfg}})","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"#filter the dataset to get only the international news\ninternational=dataset[dataset['tag']=='international news']\ninternational = international.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"nlp = Chinese()\nd1=nlp(international['headline'][3])\nd1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_text = pd.DataFrame()\n#describe the words in the sentence before\nfor i, token in enumerate(d1):\n    tokenized_text.loc[i, 'text'] = token.text\n    tokenized_text.loc[i, 'type'] = token.pos_\n    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n    tokenized_text.loc[i, 'is_alphabetic'] = token.is_alpha\n    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n    tokenized_text.loc[i, 'sentiment'] = token.sentiment\n    \n    \n\ntokenized_text[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here comes the problem of tokenize chinese words. Let's look at the first word in the sentence: 以色列 (Yǐsèliè). This means Israel. The tokenizer, as we can see in the table before,is splitting this word in three characters. To make the names of foreign countries, Chinese people usually forms the word with characters that sound similar, in this case: 以(use/by/for),色(colour/color/expression) and 列(list/rank/category). Chinese words are composed by characters and all these characters have an individual meaning by their own. That's the big problem here.\n\nThen, how can we count the most frequent words in the headlines?\n\nI have used Googletrans (a free Python library that implemented Google Translate API) to translate the headlines to English. \n\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install googletrans","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import googletrans\nfrom googletrans import Translator\n\ntranslator = Translator()\n# available languages for translation\nprint(googletrans.LANGUAGES)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"cols=international['headline']\ntranslations = []\nfor column in cols:\n    translations.append(translator.translate(column).text)\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"international['headlineEnglish'] = translations","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import nltk\nfrom collections import Counter\n\n# Create a list of stopwords\nstopwords = nltk.corpus.stopwords.words('english')\n# Create a list of punctuation marks\nRE_stopwords = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n\n\nwords = (international.headlineEnglish\n           .str.lower()\n           .str.cat(sep=' ')\n           .split()\n        )\n\nl=[]\n\nfor i in words:\n    if i not in RE_stopwords:\n        l.append(i)\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mostFrequentWords = pd.DataFrame(Counter(l).most_common(40),\n                    columns=['Word', 'Frequency'])\n\nmostFrequentWords.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"fig = px.bar(mostFrequentWords,\n             x='Frequency',\n             y='Word',\n             title='The 40 words most mentioned in the headlines (International news)',\n             color='Frequency',\n             barmode='stack')\n\nfig.update_layout(\n   paper_bgcolor='#0b1f65',\n   plot_bgcolor='#0b1f65',\n    font_family=\"Arial\",\n    font_color=\"white\",\n    title_font_family=\"Arial\",\n    title_font_color=\"white\",\n    legend_title_font_color=\"white\",\n    xaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    \n    },\n    yaxis = { \n    'showgrid': False, \n    'zeroline': True, \n    'visible': True,\n    \n    }\n    \n)\n\n\n\n\n# plot\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The topics of the international news are United States, Russia, Syria, Korea, Iran, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {}\nfor a, x in mostFrequentWords.values:\n    d[a] = x\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color='#e8e3cc',max_font_size = 86, random_state = 42)\nwordcloud.generate_from_frequencies(frequencies=d)\nplt.figure(figsize=[12, 8])\nfigure_size=(24.0,16.0)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}