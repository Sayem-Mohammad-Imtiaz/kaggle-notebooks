{"cells":[{"metadata":{"_cell_guid":"e1d0b632-9c74-4257-a3ff-c0782d04c47e","_uuid":"3c0dd7fa9d7ab27ae84f311ae0616d7121d7e1d5"},"cell_type":"markdown","source":"# PIMA Dataset Prediction Modelling with KERAS (~80%)\n---\n\nImplement a Deep Neural Network with KERAS on the Pima Indians Diabetes Database (https://www.kaggle.com/uciml/pima-indians-diabetes-database)"},{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom __future__ import print_function\nimport tensorflow as tf\nfrom six.moves import range\nimport numpy as np\nimport os\nimport sys\n\nfrom IPython.display import display, Image\nimport matplotlib.pyplot as plt\n# Config the matlotlib backend as plotting inline in IPython\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nfrom tensorflow.contrib import keras\nfrom keras import models, layers, losses, optimizers, metrics\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"collapsed":true,"_cell_guid":"f251e04c-8ad6-4e1c-86eb-252691af18c4","_uuid":"aaad9d66c2373fe72ee8cbfb4919955fff9b34d3","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1f12b85e-8da6-4d7a-a1a5-423277e20968","_uuid":"c0ebf98f750acd2f6ab0f150a54c84c92bf36fc7","trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7f7b72eb-4c87-406b-9f39-20bcab3bb9b5","_uuid":"7b417453cc8072d1eb1f4483671186a3d807b429","trusted":false},"cell_type":"code","source":"# Splitting into training and testing datasets\n\nX = df.drop(['Outcome'], axis=1)\nY = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f27148be-22ce-475b-8803-457506bed94c","_uuid":"1e1c5a27fc86c753c35ed8574a62325a357d5d13"},"cell_type":"markdown","source":"---"},{"metadata":{"_cell_guid":"1a70fafe-3c80-4010-992c-ac2d49def0ca","_uuid":"85562dae6a8dd32c4bd27ca54d5476e52aad6785"},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"_cell_guid":"ac0f0c45-8a9b-40cf-9bf9-333bf650258f","_uuid":"0216c0277c31aad291142ccd689dbe7dd7e592d8"},"cell_type":"markdown","source":"*I have used the following kernel as reference for Data Visualizations - ML From Scratch-Part 2 by I, Coder *"},{"metadata":{"_cell_guid":"bca40c38-38ae-44a4-8fd1-e1fb95852738","_uuid":"1e1f66ed25fab36c59319e857d7d353ffec64dc7"},"cell_type":"markdown","source":"We first plot a 'histogram' of all data as-is."},{"metadata":{"collapsed":true,"_cell_guid":"bd120a37-073a-4b28-b080-dbb617724035","_uuid":"40450dfe35b20b5e7687eeb5fc88c02033439e62","trusted":false},"cell_type":"code","source":"X.hist(figsize=(16,9), edgecolor=\"black\", bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a33038f5-9d01-47be-bfe3-054bea149f26","_uuid":"8a1708a14ac3dd214c0001b3ea0780c851e3f6b1"},"cell_type":"markdown","source":"Now, we plot the histogram of the Diabetic Outcomes"},{"metadata":{"collapsed":true,"_cell_guid":"080500ed-08fc-461a-8710-fd50f73086dc","_uuid":"2cfc3921ab2b64324a0d527defb10ad7ab4d870c","trusted":false},"cell_type":"code","source":"# Diabetic Outcomes\nx_aff = df[df['Outcome']==1]\nx_aff.hist(figsize=(16,9), edgecolor=\"black\", bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"985aebdd-372a-4f0c-aa28-817dd3be712b","_uuid":"f8e244ef9baf00ce3aa229e34afe6462761b6c7d"},"cell_type":"markdown","source":"Next, we plot the Pair Plots - A plot of all variables against the each other to get an idea about the distribution of diabetic and non-diabetic trend."},{"metadata":{"collapsed":true,"_cell_guid":"fe306de7-6724-4d1c-9660-1cae95e48a38","_uuid":"84f50b2fd96bf654ff7387db0229b89cdc607dcb","trusted":false},"cell_type":"code","source":"sns.pairplot(df, hue='Outcome', markers=['o', 'x'], diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ae96011-d07b-4a4a-a0ce-3a85bf973dbb","_uuid":"e05010f3054a5703a76582a423d1d016053f4249"},"cell_type":"markdown","source":"---"},{"metadata":{"_cell_guid":"30dc3c97-6544-46c1-8046-a29683441eef","_uuid":"723d4d2742a540043b18f9bfcca01681b2dc1be9"},"cell_type":"markdown","source":"## Data Scaling\n\nWe scale the data so as to increase the model accuracy."},{"metadata":{"collapsed":true,"_cell_guid":"09b71ec9-0d7f-44e2-b6ae-002cf2f75ef7","_uuid":"42423677c70cbce8f7c7e085ee36a993b17f08ec","trusted":false},"cell_type":"code","source":"scaler = MinMaxScaler()\n\nscaled_x_train = scaler.fit_transform(X_train)\nscaled_x_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60c2f1ab-b210-48b4-b7aa-800e38a4f883","_uuid":"69f3ebcf7caa97d2d3284b30cc097a91e6a91509"},"cell_type":"markdown","source":"---"},{"metadata":{"_cell_guid":"5c00ef05-b159-4b03-a8d1-7248cefb4031","_uuid":"1037fa2fccccc27dcce40b18c466e2cd16bba394"},"cell_type":"markdown","source":"## KERAS DATA MODEL"},{"metadata":{"_cell_guid":"76939459-7a74-42ee-b525-a2c0f2de970c","_uuid":"b16ce79c01256b5d0251f5ea6b1fc624ae4a8e41"},"cell_type":"markdown","source":"We will use the following parameters in our implementation -\n\n* Hidden Layers - 3 , each consisiting of 8 neurons.\n* Activation - RELU for hidden, SOFTMAX for output layer.\n* Optimizer - SGD\n* Learning Rate Decay - 0.01\n* L2 Regularization\n"},{"metadata":{"collapsed":true,"_cell_guid":"24d41df9-22cc-4332-aa9a-ab834ae064ea","_uuid":"db012f5d72130a391c7d889096f935c2fbef7867","scrolled":true,"trusted":false},"cell_type":"code","source":"# Create Keras DNN Model\n\nmodel = models.Sequential()\n\n# Hyperparameters\nhold_prob = 0.01\nbeta = 1e-8\nalpha = 0.05\nlr_decay = 0.01\niterations = 400\nvalidation_split = 0.5\nopt_momentum = 0.9 # (Use only for SGD)\nbatch_size = 32\n\n# Optimizer\nopt = optimizers.SGD(lr=alpha, decay=lr_decay, momentum=opt_momentum, nesterov=True)\n\n# First Layer\nmodel.add(layers.Dense(input_dim=8, units=8, activation='relu'))\n\n# Hidden Layers\nmodel.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))\nmodel.add(layers.Dropout(hold_prob))\n\nmodel.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))\nmodel.add(layers.Dropout(hold_prob))\n\nmodel.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))\nmodel.add(layers.Dropout(hold_prob))\n\n# Output Layer\nmodel.add(layers.Dense(units=2, activation='softmax'))\n\n# Compiling the Model\nmodel.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train\nmodel.fit(x=scaled_x_train, y=y_train, epochs=iterations, validation_split=validation_split, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2131116-b91b-45ff-a099-ff52c41cedd2","_uuid":"88ceeea3a6440864342c923611c40499ab4b5247"},"cell_type":"markdown","source":"**Training Accuracy ~ 80%**\n\n**Validation Accuracy ~ 74%**\n\n---"},{"metadata":{"_cell_guid":"0a59d0a2-929d-44b0-a290-0840da515f7e","_uuid":"3e9bcd6003e2abf8ec1f8aaeb2c8cd89d4df86a5"},"cell_type":"markdown","source":"Let's check how our model performs on new data."},{"metadata":{"collapsed":true,"_cell_guid":"4973f745-086b-4bd7-bc63-f1aef1b4ccc1","_uuid":"be4f329f2dc4877f735af4b421380875c0904b2d","trusted":false},"cell_type":"code","source":"predictions = model.predict_classes(scaled_x_test)\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24cdd6ee-83c1-47b1-8f44-a160ad86cd42","_uuid":"d77600451953acc735d79f379b43d5065c0a63a0"},"cell_type":"markdown","source":"### Conclusion\n\nThus, with considerable hyperparameter tuning, we can achieve an accuracy of ~76-79% with KERAS, which is a decent accuracy level."}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}