{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Airbnb London - Predict property rating based on sentiment analysis\n\nThis study is divided in two parts.\n\n#### Step 1. Sentiment Analysis\nSentiment analysis is a process of identifying an attitude of the author on a topic that is being written about. With sentiment analysis, text can be categorized into a variety of sentiments. \nIdeally, to perform sentiment analysis in a supervised learning machine learning process, we would have to manually associate each text record with a “sentiment” for training. Based on training, a prediction model associates “positive” or “negative” sentiments to test records.\n\nFor simplicity, this work will be based on only two categories, positive and negative. Also, instead of manually labelling each comment, we will use an automated algorithm for this purpose.\n\n\n#### Step 2. Property rating prediction based on sentiment analysis\n\nBased on the sentiment scores of a set of review comments, for each property, we will try to infer its rating.\n\n\n**Note:** Portions of this notebook were based on the following articles:\n* https://towardsdatascience.com/social-media-sentiment-analysis-49b395771197\n* https://www.kaggle.com/madatpython/python-nltk-sentiment-analysis\n* https://www.kaggle.com/sasikala11/sentiment-analysis-using-python\n* https://algotrading101.com/learn/sentiment-analysis-python-guide/\n* https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/\n* https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n* https://www.kaggle.com/leandrodoze/sentiment-analysis-in-portuguese\n* https://realpython.com/python-nltk-sentiment-analysis/","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.stem.porter import *\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\n\nfrom wordcloud import WordCloud,STOPWORDS\nfrom gensim.models import word2vec\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split # function for splitting data to train and test sets\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.metrics import jaccard_similarity_score\ncv = CountVectorizer()\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n%matplotlib inline\nplt.style.use('bmh')\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Read dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndfdict = dict()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename != 'calendar.csv' and filename.rfind('.csv') > 0:\n            print(os.path.join(dirname, filename))\n            name = Path(os.path.join(dirname, filename)).stem\n            dfdict[name] = pd.read_csv(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1. Convert data types and remove irrelevant columns","metadata":{}},{"cell_type":"code","source":"# Remove columns with almost no values: listings: bathrooms, neighbourhood_group_cleansed, calendar_updated, license\ndfdict['listings'].drop(columns=['bathrooms', 'neighbourhood_group_cleansed', 'calendar_updated', 'license'], inplace=True)\n# Remove irrelevant columns\ndfdict['listings'].drop(columns=['listing_url', 'picture_url', 'host_url', 'host_name', 'host_thumbnail_url', 'host_picture_url'], inplace=True)\ndfdict['listings'].drop(columns=['minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', \n                                 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfdict['reviews']['date'] = pd.to_datetime(dfdict['reviews']['date'])\ndfdict['listings']['last_scraped'] = pd.to_datetime(dfdict['listings']['last_scraped'])\ndfdict['listings']['host_since'] = pd.to_datetime(dfdict['listings']['host_since'])\ndfdict['listings']['calendar_last_scraped'] = pd.to_datetime(dfdict['listings']['calendar_last_scraped'])\ndfdict['listings']['first_review'] = pd.to_datetime(dfdict['listings']['first_review'])\ndfdict['listings']['last_review'] = pd.to_datetime(dfdict['listings']['last_review'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_price(df_column):\n    return df_column.str.replace('$', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfdict['listings']['price'] = convert_price(dfdict['listings']['price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_boolean(df_column):\n    return df_column.replace({'f': 0, 't': 1}).astype('boolean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert t/f fields to boolean\n# calendar dataframe : available\n# listings dataframe : (host_is_superhost, host_has_profile_pic, host_identity_verified, calendar_updated, has_availability, instant_bookable)\ndfdict['listings']['host_is_superhost'] = convert_boolean(dfdict['listings']['host_is_superhost'])\ndfdict['listings']['host_has_profile_pic'] = convert_boolean(dfdict['listings']['host_has_profile_pic'])\ndfdict['listings']['host_identity_verified'] = convert_boolean(dfdict['listings']['host_identity_verified'])\ndfdict['listings']['has_availability'] = convert_boolean(dfdict['listings']['has_availability'])\ndfdict['listings']['instant_bookable'] = convert_boolean(dfdict['listings']['instant_bookable'])\n# Convert 'host_acceptance_rate', 'host_response_rate', removing the %\ndfdict['listings']['host_acceptance_rate'] = dfdict['listings']['host_acceptance_rate'].str.replace('%', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)\ndfdict['listings']['host_response_rate'] = dfdict['listings']['host_response_rate'].str.replace('%', '', regex = 'true').str.replace(',', '', regex = 'true').astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Setup indices","metadata":{}},{"cell_type":"code","source":"dfdict['listings'].set_index('id', inplace=True)\ndfdict['reviews'].set_index('id', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data overview","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Overview of the `reviews` dataset","metadata":{}},{"cell_type":"code","source":"dfdict['reviews']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are 5 attributes present in the dataset, with a total of 1,163,886 reviews. Each review has a date and is associated with a property (`listing_id`), and with a reviewer (`reviewed_id`, `reviewer_name`). Each review has a text field with `comments`.","metadata":{}},{"cell_type":"markdown","source":"### We need to label each comment as positive (1), neutral (0) or negative (-1)\n\n","metadata":{}},{"cell_type":"markdown","source":"## 3. Data preprocessing\n\nWhen performing the sentiment analysis of property comments, our approach will be based on performing individual sentiment analysis on the comment of each reviewer, and then aggregating sentiment scores for each property.","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Merge `reviews` and `listings` dataframes to obtain the `property review score` column\n\nTo perform sentiment analysis on each comment, while preserving property information, we will join the `listings` and `reviews` dataframes. \n\nWe will also sort the records by review date (column `review_date`).","metadata":{}},{"cell_type":"code","source":"# Rename the column from date to review_date (will help recognizing this column after merging with the other dataframe)\ndfdict['reviews'].rename(columns={\"date\": \"review_date\"}, inplace=True)\ndf_merge = dfdict['listings'].merge(dfdict['reviews'], left_on=['id'], right_on=['listing_id']).sort_values(by=['review_date'])\ndf_merge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Reduce dataset size\n\nOur `comments` dataframe has 1,163,886 reviews (too much records!). Let's reduce the size of the dataset, by choosing a subset of reviews. \n\nOur filter criteria will be purely random. Let's select 10% of the records, randomly.\n\nBut, before filtering, let's look at the distribution of the number of reviews.","metadata":{}},{"cell_type":"code","source":"# Get the number of comments per listing\nreview_count = df_merge[['listing_id', 'reviewer_id']].groupby(by=['listing_id']).count()\nreview_count.hist(bins=40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_merge_original = df_merge.copy()\n# Select a random 50% sample of the DataFrame \ndf_merge = df_merge.sample(frac=0.1, random_state=42)\nreview_count = df_merge[['listing_id', 'reviewer_id']].groupby(by=['listing_id']).count()\nreview_count.hist(bins=40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1. Convert `comments` column from object to string","metadata":{}},{"cell_type":"code","source":"df_merge['comments'] = df_merge['comments'].astype(str)\ndf_merge.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Removing Punctuation, Numbers, and Special Characters\n\nPunctuation, numbers and special characters do not help much. In the following function, we will replace some expressions with spaces.\n\n#### NOTE: We will store the modified comments in a new column called `comments_cleaned`. ","metadata":{}},{"cell_type":"code","source":"def cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = re.sub(r'[^\\x00-\\x7f]',r' ',s)  # remove punctuation\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merge['comments_cleaned'] = df_merge['comments'].apply(lambda s: cleaning(s))\ndf_merge.head(8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3. Analyzing in which language the comments are written","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language\n#from langdetect import detect => does not work\n#from textblob import TextBlob  # Requires NLTK package, uses Google => error of too many web requests...\nimport fasttext\nmodel = fasttext.load_model('../input/fasttext-model/fasttext.ftz')\n\nlanguage_list = []\nfor comment in df_merge['comments']:\n    language_list.append(model.predict(comment.replace(\"\\n\",\"\"))[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merge['language'] = language_list\ndf_merge[['comments', 'comments_cleaned', 'language']].head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ops! There are comments in other languages! We will remove all comments that are not in English.","metadata":{}},{"cell_type":"markdown","source":"### 3.4. Filter out non-english reviews","metadata":{}},{"cell_type":"code","source":"df_english = df_merge[df_merge['language'] == '__label__en']\ndf_english[['comments', 'comments_cleaned', 'language']].head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5. Removing stop words\n\nWe will define a function to remove stop words in English.","metadata":{}},{"cell_type":"code","source":"stopword_list = set(stopwords.words(\"english\"))\ndef removeStopwords(x):\n    filtered_words = [word for word in x.split() if word not in stopword_list]\n    return \" \".join(filtered_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_english['comments_cleaned_2'] = df_english['comments_cleaned'].apply(lambda s: removeStopwords(s))\ndf_english[['comments', 'comments_cleaned', 'comments_cleaned_2', 'language']].head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.6. Tokenization\n\nNow we will tokenize all the cleaned comments in our dataset. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n\nWe tokenize our sentences because we will apply Stemming from the “NLTK” package in the next step.","metadata":{}},{"cell_type":"code","source":"tokenized_comments = df_english['comments_cleaned_2'].apply(lambda x: x.split())\ntokenized_comments.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.7. Stemming\n\nStemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\nFor example — “play”, “player”, “played”, “plays” and “playing” are the different variations of the word — “play”","metadata":{}},{"cell_type":"code","source":"from nltk import PorterStemmer\n\nps = PorterStemmer()\ntokenized_comments = tokenized_comments.apply(lambda x: [ps.stem(i) for i in x])\ntokenized_comments.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.8. Merge tokens back together","metadata":{}},{"cell_type":"code","source":"tokenized_comments = tokenized_comments.apply(lambda x: ' '.join(x))\ndf_english['comments_cleaned_3'] = tokenized_comments\ndf_english[['comments', 'comments_cleaned', 'comments_cleaned_2', 'comments_cleaned_3', 'language']].head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Performing automated sentiment analysis on `comments`\n\nSource: https://algotrading101.com/learn/sentiment-analysis-python-guide/, Section 3.\n\nWe will use the `VADER Sentiment Analyzer`\n\nVADER is a sentiment analyser that is trained using social media and news data using a lexicon-based approach. This means that it looks at words, punctuation, phases, emojis etc and rates them as positive or negative.\n\nVADER stands for “Valence Aware Dictionary and sEntiment Reasoner”.","metadata":{}},{"cell_type":"markdown","source":"### 5.1. Download VADER lexicon","metadata":{}},{"cell_type":"code","source":"%%time\nnltk.download('vader_lexicon')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2. Run the automated sentiment analysis\n\nAnd concatenate the output to the original dataframe. Note that we are only interested in the values of the ‘compound’ variable, which is a function of the positive, neutral and negative sentiment scores.","metadata":{}},{"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n\ndef guess_sentiment(comment):\n    pol_score = SIA().polarity_scores(comment) # run analysis\n    return pol_score[\"compound\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Since Vader algorithm takes a lot of time to run (more than 15min), we will store its results in a pickle file to save time. If this result file already exists, we will skip Vader processing.","metadata":{}},{"cell_type":"code","source":"outputpath = '/kaggle/working/'\nvader_sentiment_list_file = os.path.join(outputpath, 'airbnb-vader-sentiment.pkl.gz')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile(vader_sentiment_list_file):\n    print('Reusing existing Vader sentiment results.')\n    vader_sentiment_list = pd.read_pickle(vader_sentiment_list_file)\nelse:\n    print('Existing Vader sentiment results not found. Reprocessing...')\n    #%%time \n    vader_sentiment_list = df_english['comments'].apply(lambda x: guess_sentiment(x))\n    vader_sentiment_list.to_pickle(vader_sentiment_list_file)\n    print('Saved existing Vader sentiment results to file.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vader_sentiment_list.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3. Analyze obtained sentiment scores\n\nFirst, let's merge the obtained sentiment score into the original dataset.","metadata":{}},{"cell_type":"code","source":"df_english['vader_sentiment_score'] = vader_sentiment_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_english[['comments', 'vader_sentiment_score']].describe())\ndisplay(df_english[['comments', 'language', 'vader_sentiment_score']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, sentiment scores range from -1.0 to 1.0.","metadata":{}},{"cell_type":"markdown","source":"## 6. Correlating sentiment score of the reviews against property review scores\n\nSource: https://algotrading101.com/learn/sentiment-analysis-python-guide/, Section 4.\n\nIn this section, we want to compare the relationship between property ratings and our sentiment score. \n\nIf there is a significant relationship, then our sentiment scores might have some predictive value.\n\nHere are the next steps:\n\n1. Aggregate sentiment scores by property, creating the `property review score` column;\n1. Check relationship between `property sentiment score` against `property review score`.","metadata":{}},{"cell_type":"markdown","source":"### 6.1. Scale sentiment scores, so that values lie in the interval `[0, 1]`","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df_english[['vader_sentiment_score']])\ndf_english['sentiment_score'] = scaler.transform(df_english[['vader_sentiment_score']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_english[['comments', 'sentiment_score']].describe())\ndisplay(df_english[['comments', 'language', 'sentiment_score']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's also multiply sentiment scores by 100, truncate the values and convert them to int","metadata":{}},{"cell_type":"code","source":"df_english['sentiment_score'] = np.ceil(df_english['sentiment_score'] * 100).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2. Aggregate sentiment scores by property\n\nLet's take a quick look at a random property (`id = 15400`) and the sentiment scores of its reviews.","metadata":{}},{"cell_type":"code","source":"df_english[df_english['listing_id'] == 15400][['sentiment_score', 'review_scores_rating']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's combine the `sentiment_score` for all comments/reviews concerning each property, to get a unique property `sentiment_score`. \n\n#### Let's investigate the best way to calculate an aggregated property sentiment score by observing some statistics of the scores of the individual comments.","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/17578115/pass-percentiles-to-pandas-agg-function\ndef percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'p%s' % n\n    return percentile_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_stats = df_english[['listing_id', 'sentiment_score', 'review_scores_rating']].groupby(['listing_id'])\\\n.agg({\"sentiment_score\": [np.median, np.var, np.min, np.max, percentile(10), percentile(25), percentile(75),\n                         percentile(90), percentile(95), np.mean], \n      \"review_scores_rating\" : [np.mean]})\ndf_stats.columns = ['_'.join(col).strip() for col in df_stats.columns.values]\ndf_stats\n#reset_index().pivot(index='name', values='score', columns='level_1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3. Correlation heatmap between candidate `property sentiment scores` and `property review score`","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\nplt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(df_stats.corr()[['review_scores_rating_mean']].sort_values(by='review_scores_rating_mean', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with review_scores_rating', fontdict={'fontsize':18}, pad=16);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=df_stats, x=\"sentiment_score_mean\", y=\"review_scores_rating_mean\")\nprint(df_stats[\"sentiment_score_mean\"].corr(df_stats[\"review_scores_rating_mean\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the x-axis, we have our sentiment score. On the y-axis we have our property review score.","metadata":{}},{"cell_type":"markdown","source":"The correlation between the mean sentiment score and the property review score is not so strong (0.36). We may need to improve it.","metadata":{}},{"cell_type":"markdown","source":"### 6.4. Improving the relationship between `property sentiment score` against `property review score`\n","metadata":{}},{"cell_type":"markdown","source":"#### 6.4.1. Feature engineering on `last_scraped` and `review_date` dates\n\nWe do not know the formula used by Airbnb when calculating the property review score. Maybe older reviews receive a smaller weight in the formula when compared to newer / more recent reviews?\n\nLet's create a new field with the date difference between the review date (`review_date`) and the date of the property review score (`last_scraped`). ","metadata":{}},{"cell_type":"code","source":"df_english['days_since_review'] = (df_english['last_scraped'] - df_english['review_date']).astype('timedelta64[D]')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.4.2. Ignoring some neutral-sentiment comments\n\nLet's try to improve the correlation by discarding comments that present neutral sentiment (insignificant comment). \nThe bare minimum is to exclude the data where the score is 0 or insignificant.\n\nWe shall assume that a score of between -0.1 and 0.1 is insignificant for the sake of simplicity. This is an arbitrary figure.\nLet's discard any comment with $\\mid sentiment.score \\mid \\leq 0.1 $.","metadata":{}},{"cell_type":"code","source":"threshold = 0.1\ndf_english2 = df_english[((df_english['vader_sentiment_score'] <= -threshold) | (df_english['vader_sentiment_score'] >= threshold))]\ndf_english2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df_english2[['vader_sentiment_score']])\ndf_english2['sentiment_score'] = scaler.transform(df_english2[['vader_sentiment_score']])\n\ndf_stats2 = df_english2[['listing_id', 'sentiment_score', 'review_scores_rating', 'days_since_review']].groupby(['listing_id'])\\\n.agg({\"sentiment_score\": [np.median, np.var, np.min, np.max, percentile(10), percentile(25), percentile(75),\n                         percentile(90), percentile(95), np.mean], \n      \"review_scores_rating\" : [np.mean],\n      \"days_since_review\" : [np.mean]})\ndf_stats2.columns = ['_'.join(col).strip() for col in df_stats2.columns.values]\ndf_stats2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(df_stats2.corr()[['review_scores_rating_mean']].sort_values(by='review_scores_rating_mean', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with review_scores_rating 2', fontdict={'fontsize':18}, pad=16);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We managed to improve the correlation index from 0.36 to 0.40.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=df_stats2, x=\"sentiment_score_mean\", y=\"review_scores_rating_mean\")\nprint(df_stats2[\"sentiment_score_mean\"].corr(df_stats2[\"review_scores_rating_mean\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Predict `property review score` using `property sentiment score`","metadata":{}},{"cell_type":"markdown","source":"### 7.1. Splitting the Dataset for Training and Testing the Model\n\nWe need to divide our data into training and testing sets. The training set will be used to train the algorithm while the test set will be used to evaluate the performance of the machine learning model.","metadata":{}},{"cell_type":"code","source":"# Checking for NaN values in the dataframe\ndf_stats2.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_stats3 = df_stats2.fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_col = 'review_scores_rating_mean'\nX = df_stats3[[col for col in df_stats3.columns if col != target_col]]\ny = df_stats3[target_col]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the code above we use the `train_test_split` class from the `sklearn.model_selection` module to divide our data into `training` and `testing` set. The method takes the feature set as the first parameter, the label set as the second parameter, and a value for the `test_size` parameter. We specified a value of `0.2` for `test_size` which means that our data set will be split into two sets of `80%` and `20%` data. We will use the `80%` dataset for training and `20%` dataset for testing.","metadata":{}},{"cell_type":"markdown","source":"### 7.2. Define the ML Pipeline and Randomized Hyperparameter Search\n\nThe `XGBClassifier` class implements the Scikit-Learn interface for using `XGBoost` for classification. That means that it has the familiar fit method as well as predict, score and so on.\n\nThe preprocessing methods to use in the pipeline and the parameters to optimize are just for the sake of the example.\n\nIn k-fold cross-validation, note that I have set the number of splits/folds to 3 in order to save time. You should probably put 5 there to get a more reliable result.","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/carlosdg/xgboost-with-scikit-learn-pipeline-gridsearchcv\nmodel = xgb.XGBRegressor(learning_rate=0.02, n_estimators=600, silent=True, nthread=1)\n\npipeline = Pipeline([\n    ('standard_scaler', StandardScaler()), \n    ('model', model)\n])\n\nfolds = 3\nparam_comb = 50\n\nparam_grid = {\n        'model__silent': [False],\n        'model__max_depth': [6, 10, 15, 20],\n        'model__learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'model__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'model__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'model__colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'model__min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'model__gamma': [0, 0.25, 0.5, 1.0],\n        'model__reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'model__n_estimators': [100]}\n\ngrid = RandomizedSearchCV(pipeline, param_grid, random_state=0, n_iter=param_comb, cv=folds, n_jobs=4, verbose=3, \n                          scoring='neg_mean_squared_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ngrid.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.3. CV results\n\nHere are the results of the model that gave the best mean score in the k-fold cross-validation","metadata":{}},{"cell_type":"code","source":"mean_score = grid.cv_results_[\"mean_test_score\"][grid.best_index_]\nstd_score = grid.cv_results_[\"std_test_score\"][grid.best_index_]\n\ngrid.best_params_, mean_score, std_score\n\nprint(f\"Best parameters: {grid.best_params_}\")\nprint(f\"Mean CV score: {mean_score: .6f}\")\nprint(f\"Standard deviation of CV score: {std_score: .6f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.4. Making Predictions and Evaluating the Model\n\nOnce the model has been trained, the last step is to make predictions on the model. To do so, we need to call the predict method on the `grid` object that we used for training. ","metadata":{}},{"cell_type":"code","source":"y_pred = grid.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now evaluate the regressor performance.","metadata":{}},{"cell_type":"code","source":"# https://medium.com/acing-ai/how-to-evaluate-regression-models-d183b4f5853d\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, max_error\n\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MedAE: ', median_absolute_error(y_test, y_pred))\nprint('MaxError: ', max_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The XGBoostRegressor presented a mean absolute error (MAE) of `4.18` when trying to predict the `property review score`. Note, however, that the maximum absolute error observed in prediction is equal to 95.12, a really large error!","metadata":{}},{"cell_type":"markdown","source":"We can also plot the test values (`y_test` vs. `y_pred`). ","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/65539013/how-to-plot-a-graph-of-actual-vs-predict-values-in-python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plotGraph(y_test,y_pred,regressorName):\n    if max(y_test) >= max(y_pred):\n        my_range = int(max(y_test))\n    else:\n        my_range = int(max(y_pred))\n    plt.scatter(range(len(y_test)), y_test, color='blue')\n    plt.scatter(range(len(y_pred)), y_pred, color='red')\n    plt.title(regressorName)\n    plt.show()\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotGraph(y_test, y_pred, \"XGBRegressor\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observe that the regressor has problemas to predict smaller preperty review scores (e.g., from 0 until 80). No property review score below ~60 is returned by the regressor.","metadata":{}},{"cell_type":"markdown","source":"## 8. Trying to improve the prediction results by formulating a classification problem\n\nWhat if, instead of trying to predict the `property review score` as a number (from `0` to `100`), we tried to predict the `property review score` as a class (e.g., `0` == low, `1` == medium and `3` == high)? Would it be possible to obtain improved results?\n\n**Let's begin by analyzing the distribution of the target column.** I suspect that these values are not i.i.d. distributed. If this is true, the regression model may be overfitting on high property scores (the majority of the scores is high), leaving behind the low scores.\n","metadata":{}},{"cell_type":"code","source":"sns.displot(df_stats3, x=\"review_scores_rating_mean\", bins=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.1. Converting the target colum from numerical to categorical\n\nEven though some websites explain how Statified K-fold CV could be performed on non-categorial targets (https://scottclowe.com/2016-03-19-stratified-regression-partitions/), there is no source code available on that subject.\n\nOur approach here will be the conversion of the target column to catagorical (property score ranges) in order to apply `StratifiedKFold` cross-validation. \n\nTo discretize the target column values, we will apply the `KBinsDiscretizer`. Here is a page that illustrates the different stratagies of discretization: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html\nWe will discretize the column into 4 groups using `quantile` criteria (i.e., all bins in each feature have the same number of points) to define the width of the bins.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer\nenc = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\nenc.fit(y_train.to_numpy().reshape(-1, 1))\ny_train_cat = enc.transform(y_train.to_numpy().reshape(-1, 1)).astype(int)\ny_train_cat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_target = 'review_scores_rating_bin'\nsns.displot(pd.DataFrame(y_train_cat, columns=[new_target]), x=new_target, bins=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The new values of `new_target` column a much better distributed than the original numerical column. \n\nLet's apply the same transformation to the `y_test` data.","metadata":{}},{"cell_type":"code","source":"y_test_cat = enc.transform(y_test.to_numpy().reshape(-1, 1)).astype(int)\nsns.displot(pd.DataFrame(y_test_cat, columns=[new_target]), x=new_target, bins=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now run a new Machine Learning algorithm, now based on a classification task on target column `new_target`.\n\n### 8.2. ML pipeline for classification\n\nWe'll define a new Machine Learning pipeline for classification, this time using `XGBClassifier` and `RandomizedSearchCV` with the `f1_weighted` evaluation metric.\n\nTo properly define the `f1_weighted` metric, first we need to define the following metrics:\n\n**Accuracy**: Accuracy is an evaluation metric that allows you to measure the total number of predictions a model gets right. The formula for accuracy is below:\n\n![Accuracy_formula](https://miro.medium.com/max/700/1*sVuthxNoz09nzzJTDN1rww.png)\n\n**Precision**: It tells you what fraction of predictions as a positive class were actually positive. To calculate precision, use the following formula: \n\n$TP/(TP+FP)$.\n\n**Recall**: It tells you what fraction of all positive samples were correctly predicted as positive by the classifier. It is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: \n\n$TP/(TP+FN)$.\n\n**F1-score**: It combines precision and recall into a single measure. Mathematically it’s the harmonic mean of precision and recall. It can be calculated as follows:\n\n![F1-score](https://miro.medium.com/max/700/1*wUdjcIb9J9Bq6f2GvX1jSA.png)\n\nFinally, `f1_weighted` is the weighted average of the f1-scores of each class.","metadata":{}},{"cell_type":"code","source":"# Machine Learning pipeline definition\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nmodel = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, silent=True, nthread=1)\n\npipeline = Pipeline([\n    #('standard_scaler', StandardScaler()), \n    ('model', model)\n])\n\nfolds = 5\nparam_comb = 30\n\nparam_grid = {\n        'model__silent': [False],\n        'model__max_depth': [6, 10, 15, 20],\n        'model__learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'model__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'model__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'model__colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'model__min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'model__gamma': [0, 0.25, 0.5, 1.0],\n        'model__reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'model__n_estimators': [100]}\n\n#cross_val = StratifiedKFold(n_splits=folds)\ncross_val = KFold(n_splits=folds, random_state=42, shuffle=True)\n\nclass_grid = RandomizedSearchCV(pipeline, param_grid, random_state=0, n_iter=param_comb, cv=cross_val, n_jobs=4, verbose=3, \n                          scoring='f1_weighted')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclass_grid.fit(X_train, y_train_cat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.3. Making Predictions and Evaluating the new model\n\nOnce the model has been trained, the last step is to make predictions on the model. To do so, we need to call the predict method on the `class_grid` object that we used for training. ","metadata":{}},{"cell_type":"code","source":"y_pred_cat = class_grid.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's now evaluate the classifier performance.","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/65618137/confusion-matrix-for-multiple-classes-in-python\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() / 2.\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n# importing confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test_cat, y_pred_cat)\nprint('Confusion Matrix\\n')\nprint(cm)\nplot_confusion_matrix(cm, classes=[0, 1, 2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://towardsdatascience.com/hackcvilleds-4636c6c1ba53\n# Generating a report to extract the measure of interest using built-in sklearn function\n# First experiment obtained a weighted_f1 = 0.351\nfrom sklearn.metrics import classification_report\nreport = classification_report(y_test_cat, y_pred_cat, digits=3)\nprint(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no definitive answer to the question of \"what's a good accuracy value\". It depends on the problem and its context. \nIn other words, the classification accuracy mainly depends on the application domain.\n\nIn this problem of property review classification based on sentiment scores, we obtained a weighted average accuracy of `71%` in the score class prediction.\n\nOn average, considering all 3 classes, `29%` of the predictions of `property review score class` were incorrect.\n\nHowever, it we take into account only the intermediate class `1` (medium score), the classification model is able to correctly predict only `17%` of the records (`383 / (6 + 383 + 1856)`). \n\nThis percentage rises to `96%` when trying to predict the upper class `2` (high score). ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}