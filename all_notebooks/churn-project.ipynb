{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem: \n\n**Bankayı terk edecek müşteriler önceden belirlenmek isteniyor.**\n\n- Projenin amacı bir müşterinin bankayı terk edip etmeyeceğini tahmin etmektir.\n\n- Banka hesaplarının kapatılması müşteri terkini tanımlayan durumdur.\n\n**Veri Seti Hikayesi:**\n\n- Veri seti 10.000 gözlemden ve 13 değişkenden oluşmaktadır. Bu değişkenlerden bir tanesi bağımlı değişken.\n- Bağımsız değişkenler müşterilere ilişkin bilgilerdir.\n- Bağımlı değişken müşteri terk durumunu ifade etmektedir.\n\n**Değişkenler:**\n\n- Surname : Müşterinin Soy Adı\n- CreditScore : Müşterinin Kredi skoru\n- Geography : Müşterinin ikamet ettiği ülke (Almanya/Fransa/İspanya)\n- Gender : Müşterinin Cinsiyeti (Kadın/Erkek)\n- Age : Müşterinin Yaş\n- Tenure : Kaç yıldır bankayla çalıştığı\n- Balance : Hesap Bakiyesi\n- NumOfProducts : Kullanılan banka ürünü (Kredi kartı,maaş hesabı vs.)\n- HasCrCard : Kredi kartı durumu (0=Yok,1=Var)\n- IsActiveMember : Aktif üyelik durumu (0=Aktif Değil,1=Aktif)\n- EstimatedSalary : Müşterinin Tahmin edilen maaşı\n- Exited : Müşteri terk olacak mı? (0=Hayır,1=Evet)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"   class color:\n       BOLD = '\\033[1m'\n       UNDERLINE = '\\033[4m'\n       END = '\\033[0m'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\ndef read_data():\n    \"\"\"Reads the dataframe, assigns the column CustomerId as index, and the dataframe is returned.\"\"\"\n    return pd.read_csv(\"../input/bank-churn-modelling/Churn_Modelling.csv\",index_col=\"CustomerId\")\n#2\ndef split_features():\n    \"\"\"Separates variables as categorical,numeric and outcome. It prints them on the screen and returns them.\"\"\"\n    categorical_features=[\"HasCrCard\",\"IsActiveMember\",\"Gender\",\"Geography\"]\n    numerical_features=[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"]\n    Target=\"Exited\"\n    print(categorical_features,numerical_features,Target,sep=\"\\n\")\n    return categorical_features,numerical_features,Target\n#3\ndef cat_vis():\n    \"\"\"Visualizes churn status by category\"\"\"\n    df.groupby(\"Gender\").agg({\"Exited\":\"count\"}).plot.bar(color=\"blue\");\n    plt.title(\"Churn by Gender\")\n    df.groupby(\"Geography\").agg({\"Exited\":\"count\"}).plot.bar(color=\"black\")\n    plt.title(\"Churn by Geography\")\n    df.groupby(\"HasCrCard\").agg({\"Exited\":\"count\"}).plot.bar(color=\"green\");\n    plt.title(\"Churn by Credit Card\")\n    df.groupby(\"IsActiveMember\").agg({\"Exited\":\"count\"}).plot.bar(color=\"green\");\n    plt.title(\"Churn by activity status\")\n#4\ndef stats(num_data):\n    \"\"\"It gives descriptive statistics according to the determined percentiles\"\"\"\n    return df[num_data].describe([0.05,0.25,0.50,0.75,0.95]).T\n#5\ndef missing_values():\n    \"\"\"It examines the missing data in the data set visually and numerically.\"\"\"\n    import missingno as msno\n    msno.bar(df); \n    print(df.isnull().sum())\n#6\ndef data_prep():\n    \"\"\"Drops the Surname and Rownumber columns, transforms into one hot encoding for categorical variables, and drops dummy columns.\"\"\"\n    df.drop([\"Surname\",\"RowNumber\"],axis=1,inplace=True)\n    return pd.get_dummies(df,columns = cat_ft, drop_first = True)\n    \n#7    \ndef handle_outliers(df,q1=0.05,q3=0.95,method=\"quantiles\",\n                    inplace=False):\n    \"\"\"Analyze outliers with LOF or quantiles method. Optionally, it drops outliers in LOF and suppresses to the limits in quantiles method.\"\"\"\n    if method==\"quantiles\":\n        for feature in df:\n            Q1 = df[feature].quantile(q1)\n            Q3 = df[feature].quantile(q3)\n            IQR = Q3-Q1\n            lower = Q1- 1.5*IQR\n            upper = Q3 + 1.5*IQR\n            if df[(df[feature] > upper)].any(axis=None):\n                print(color.BOLD+color.UNDERLINE+feature+\":\"+color.END,\"OUTLIERS\"+\" \",sep=\"\\n\")\n                print(df[(df[feature] > upper)])\n                if inplace==True:\n                    df.loc[df[feature] > upper,feature] = upper\n                    return df\n                print(\"*******************************O*******************************\")\n            else:\n                print(color.BOLD+color.UNDERLINE+feature+color.END+\": There aren't outliers in this feature\"+color.END+\" \")\n                print(\"*******************************O*******************************\")\n    elif method==\"LOF\":\n        from sklearn.neighbors import LocalOutlierFactor\n        n_neighbors=int(input(\"n_neighbors(default=20): \"))\n        clf = LocalOutlierFactor(n_neighbors=n_neighbors)\n        clf.fit_predict(df)\n        df_scores = clf.negative_outlier_factor_\n        print(np.sort(df_scores)[0:30])\n        threshold=int(input(\"threshold: \"))\n        threshold=np.sort(df_scores)[threshold-1]\n        print(threshold)\n        print(df[df_scores< threshold])\n        if inplace==True:\n            print(df[df_scores< threshold])\n            df=df.drop(index=df[df_scores< threshold].index,inplace=True)\n            return df\n\ndef var_target():\n    y=df[[\"Exited\"]]\n    X=df.drop(\"Exited\",axis=1)\n    return y,X\n#8\ndef scale(num_data):\n    \"\"\"Uses Robust Scaler to standardize numerical variables.\"\"\"\n    from sklearn.preprocessing import RobustScaler\n    num_df=pd.DataFrame(df[num_ft])\n    scaler = RobustScaler() \n    data_scaled = scaler.fit_transform(df[num_ft])\n    df_scaled=pd.DataFrame(data_scaled,columns=num_ft,index=df.index)\n    cat_df=df[X.columns.difference(df_scaled.columns)]\n    \n    return df_scaled.merge(cat_df,left_index=True,right_index=True)\n#9\ndef ml_simple_models(X,y):\n    \n    \"\"\"It takes the independent variables(X) and outcome(y) as parameters, and prints the prediction\n    success of the models within it after the train test separation.\"\"\"\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\n    \n    names = [\"LogisticRegression\",\"GaussianNB\",\"KNeighborsClassifier\",\"LinearSVC\",\"SVC\",\n         \"DecisionTreeClassifier\",\"RandomForestClassifier\",\"GradientBoostingClassifier\",\n         \"XGBClassifier\",\"LGBMClassifier\"]\n    \n    \n    classifiers = [LogisticRegression(), GaussianNB(), KNeighborsClassifier(), LinearSVC(), SVC(),\n               DecisionTreeClassifier(),RandomForestClassifier(), GradientBoostingClassifier(),\n               XGBClassifier(), LGBMClassifier()]\n\n    for name, clf in zip(names, classifiers):\n\n        model = clf.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        msg = \"%s: %f\" % (name, acc)\n        print(msg)\n\n    \n#10\ndef tuned_ml_models(X,y):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\n    \n    gb_params = {\"learning_rate\": np.linspace(0,1,5),\n            \"max_depth\": [2,6,8,10],\n            \"n_estimators\": [50,100,250,500],\n            \"min_samples_split\": [2,7,10]}\n\n    gb_model = GradientBoostingClassifier()\n\n    gb_cv_model = GridSearchCV(gb_model, \n                               gb_params, \n                               cv = 3, \n                               n_jobs = -1, \n                               verbose = 1) \n\n    gb_cv_model.fit(X_train, y_train)\n    \n    gb_tuned=gb_cv_model.best_estimator_\n    gb_tuned.fit(X_train,y_train)\n    y_pred=gb_tuned.predict(X_test)\n    print(\"Best params: \",gb_cv_model.best_params_)\n    print(\"Tuned Gradient Boosting Classifier: \",accuracy_score(y_test,y_pred))\n    \n    cm = confusion_matrix( y_test,y_pred, [1,0] )\n    sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [\"1\", \"0\"] , \n    yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('ACTUAL')\n    plt.xlabel('PREDICTED')\n    plt.show()\n    \n    print(\"********************************************0**************************************************\")\n    \n    rf_params = {\"max_depth\":[2,4,8], \n            \"max_features\": [2,5,8],\n            \"n_estimators\": [50,150,300,500],\n            \"min_samples_split\": [2,5,9]}\n\n    rf_model = RandomForestClassifier()\n\n    rf_cv_model = GridSearchCV(rf_model, \n                               rf_params, \n                               cv = 3, \n                               n_jobs = -1, \n                               verbose = 1) \n\n    rf_cv_model.fit(X_train, y_train)\n    \n    rf_tuned=rf_cv_model.best_estimator_\n    rf_tuned.fit(X_train,y_train)\n    y_pred=rf_tuned.predict(X_test)\n    print(\"Best params: \",rf_cv_model.best_params_)\n    print(\"Tuned Random Forests: \",accuracy_score(y_test,y_pred))\n    \n    cm = confusion_matrix( y_test,y_pred, [1,0] )\n    sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [\"1\", \"0\"] , \n    yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('ACTUAL')\n    plt.xlabel('PREDICTED')\n    plt.show()\n    \n    print(\"********************************************0**************************************************\")\n    \n    lgbm_params = {\"learning_rate\":np.linspace(0,1,5), \n            \"max_features\": [2,5,7],\n            \"n_estimators\": [10,50,150,300,500],\n            \"min_samples_split\": [2,5,7]}\n\n    lgbm_model = LGBMClassifier()\n\n    lgbm_cv_model = GridSearchCV(lgbm_model, \n                               lgbm_params, \n                               cv = 3, \n                               n_jobs = -1, \n                               verbose = 3) \n\n    lgbm_cv_model.fit(X_train, y_train)\n    \n    lgbm_tuned=lgbm_cv_model.best_estimator_\n    lgbm_tuned.fit(X_train,y_train)\n    y_pred=lgbm_tuned.predict(X_test)\n    print(\"Best params: \",lgbm_cv_model.best_params_)\n    print(\"Tuned LGBM: \",accuracy_score(y_test,y_pred))\n    \n    cm = confusion_matrix( y_test,y_pred, [1,0] )\n    sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [\"1\", \"0\"] , \n    yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('ACTUAL')\n    plt.xlabel('PREDICTED')\n    plt.show()\n    \n#12\ndef conf_matrix(X,y):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\n    \n        names = [\"RandomForestClassifier\",\"GradientBoostingClassifier\",\"LGBMClassifier\"]\n\n\n        classifiers = [RandomForestClassifier(), GradientBoostingClassifier(),LGBMClassifier()]\n\n        for name, clf in zip(names, classifiers):\n\n            model = clf.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n            acc = accuracy_score(y_test, y_pred)\n            msg = \"%s: %f\" % (name, acc)\n            print(msg)\n            cm = confusion_matrix( y_test,y_pred, [1,0] )\n            sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [\"1\", \"0\"] , \n                    yticklabels = [\"1\", \"0\"] )\n            plt.ylabel('ACTUAL')\n            plt.xlabel('PREDICTED')\n            plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Reading and Understanding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# first 5 rows\ndf=read_data()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting features into 3 categories\ncat_ft,num_ft,outcome=split_features()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_vis()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is noteworthy that those with credit cards lose more. This may indicate a dissatisfaction with credit cards.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stats(num_ft)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can have general information about distributions, means,medians, standard deviations and even outliers(very generally) by looking at descriptive statistics.Looking at the variables alone, there does not seem to be an anomaly. That's why we will look at it later according to the LOF method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#we're checking if there is missing data in the dataframe\nmissing_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is no missing value in the dataframe","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping \"Surname\" and \"Rownumber\" columns\n# One hot encoding for categorical variables\ndf=data_prep()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"handle_outliers(df,method=\"LOF\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y,X=var_target()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=scale(num_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_simple_models(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Tuning","execution_count":null},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Hyperparameter optimizations of the 3 algorithms that give the highest score in the primitive test error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_ml_models(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SONUÇ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Yapılan hiperparametre testleri sonucundan ilkel modellere göre daha iyi bir sonuç elde edilemedi. \n- En yüksek yüzdeyle tahmini %86 doğru tahmin yüzdesiyle Gradient Boosting Classifier optimizasyonsuz modeli ile elde ettik.\n- Daha yüksek tahmin başarısı elde etmek için yapılabilecekler.\n   * Outlier'lar üzerinde herhangi bir işlem yapmamıştık. Bazı değerleri outlier olarak belirleyip drop etme,ortalama veya medyan ile doldurma, baskılama gibi yöntemler denenebilir.\n   * Farklı bir standardizasyon yöntemi kullanılabilir. (Biz Robust Scaler kullanmıştık)\n   * Tüm veri setine standardizasyon uygulanabilir. (Biz numerik değerlere uygulamıştık.)\n   * Feature Engineering ile yeni değişkenler türetilebilir, var olan değişkenler dönüştürülebilir.\n   * Optimum hiperparametreleri bulmak için mutlaka farklı hiperparametre uzayları test edilmeli.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}