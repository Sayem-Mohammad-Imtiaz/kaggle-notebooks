{"cells":[{"metadata":{},"cell_type":"markdown","source":"# *Summary*\n\n## This notebook is an extension of the credit risk modelling from credit log modelling.\n\n### In this notebook, the credit risk modelled with gradient boosted trees for predicting loan defaults with probability. This can be used to automate approving and declining loan applcations more accurately.\n\n### An 93.3% accuracy level was achieved in predicting the loan defaults on 32,576 loans and 12 benchmarks with a probability threshold of 60%. With this model, the default rate would decrease from 21.8% to 6.7%, resulting in minimized risk for both the lender and applicant.\n   \n### The top 5 most important features of determining a loan default depends on the applicant's income, age, and employment length and the loan's interest rate and amount.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection,linear_model, metrics\nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cr_data = pd.read_csv(\"/kaggle/input/credit-risk-dataset/credit_risk_dataset.csv\")\n\nemp_len_null = cr_data[cr_data['person_emp_length'].isnull()].index\nint_rate_null = cr_data[cr_data['loan_int_rate'].isnull()].index\n\ncr_data['person_emp_length'].fillna((cr_data['person_emp_length'].median()), inplace=True)\ncr_data['loan_int_rate'].fillna((cr_data['loan_int_rate'].median()), inplace = True)\n\ncr_data = cr_data.rename(columns = {\"cb_person_default_on_file\":\"default_hist\", \"cb_person_cred_hist_length\": \"cr_hist_len\"})\n\ncr_clean1 = cr_data[cr_data['person_age']<=100]\n\n# one hot encoding categorical variables\nnum_col = cr_clean1.select_dtypes(exclude = 'object')\nchar_col = cr_clean1.select_dtypes(include = 'object')\n\nencoded_char_col = pd.get_dummies(char_col)\n\ncr_clean2 = pd.concat([num_col, encoded_char_col], axis=1)\ncr_clean2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split Train and Test Sets\nY = cr_clean2['loan_status']\nX = cr_clean2.drop('loan_status',axis=1)\n \n\n\nx_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, random_state=2020, test_size=.30)\n\n#Start of gradient boosted tree\n\nxgb_model = xgb.XGBClassifier() # initialize tree\n\nxgb_model.fit(x_train, np.ravel(y_train)) # train tree\n\npredict_xgb = xgb_model.predict_proba(x_test) # 1st col = pred val, 2nd col = pred prob\n\npredict_xgb_prob = pd.DataFrame(predict_xgb[:,1],columns = ['Default Probability'])\n\npd.concat([predict_xgb_prob, y_test.reset_index(drop=True)],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} features in cr_clean2\".format(cr_clean2.shape[1]))\nround(xgb_model.score(x_test,y_test),3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display feature and their importance\nfeat_imp = xgb_model.get_booster().get_score(importance_type='weight')\n\nfeat_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns not used\nset(x_train.columns) - set(feat_imp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only feture not used was whether an applicant had a history of default."},{"metadata":{"trusted":true},"cell_type":"code","source":"# display top 5 most import features\nsorted(feat_imp.items(), key=lambda kv: kv[1],reverse=True)[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top 5 most important features of determining a loan default depends on the applicant's income, age, and employment length and the loan's interest rate and amount."},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator as op\nsorted_feat_imp = dict(sorted(feat_imp.items(), key=op.itemgetter(1),reverse=True))\nsorted_feat_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(xgb_model,importance_type='weight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nremove_feats = []\n\nfor key in feat_imp.keys():\n    if feat_imp[key] < 40:    # tried \n        remove_feats.append(key)\n        \nimp_data = X\nfor key in remove_feats:\n    imp_data = imp_data.drop(key,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has been reduced to 15 important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_most_imp_feat ={}\nfor key in list(imp_data.columns):\n    if key  in feat_imp.keys():\n        display_most_imp_feat[key] = feat_imp[key]\n\nsorted(display_most_imp_feat.items(), key=lambda kv: kv[1],reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = model_selection.train_test_split(imp_data, Y, random_state=2020, test_size=.30)\n\n#Start of gradient boosted tree\n\nxgb_model = xgb.XGBClassifier() # initialize tree\n\nxgb_model.fit(x_train, np.ravel(y_train)) # train tree\n\npredict_xgb = xgb_model.predict_proba(x_test) # 1st col = pred val, 2nd col = pred prob\n\npredict_xgb_prob = pd.DataFrame(predict_xgb[:,1],columns = ['Default Probability'])\n\npd.concat([predict_xgb_prob, y_test.reset_index(drop=True)],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(xgb_model.score(x_test,y_test),3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = np.linspace(0,1,21)\nthresh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_opt_thresh(predict,thr =thresh, y_true = y_test):\n    data = predict\n    \n    def_recalls = []\n    nondef_recalls = []\n    accs =[]\n\n    \n    for threshold in thr:\n        # predicted values for each threshold\n        data['loan_status'] = data['Default Probability'].apply(lambda x: 1 if x > threshold else 0 )\n        \n        accs.append(metrics.accuracy_score(y_true, data['loan_status']))\n        \n        stats = metrics.precision_recall_fscore_support(y_true, data['loan_status'])\n        \n        def_recalls.append(stats[1][1])\n        nondef_recalls.append(stats[1][0])\n        \n        \n    return accs, def_recalls, nondef_recalls\n\naccs, def_recalls, nondef_recalls= find_opt_thresh(predict_xgb_prob)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(thresh,def_recalls)\nplt.plot(thresh,nondef_recalls)\nplt.plot(thresh,accs)\nplt.xlabel(\"Probability Threshold\")\nplt.xticks(thresh, rotation = 'vertical')\nplt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\n#plt.axvline(x=0.45, color='pink')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim_threshold = accs.index(max(accs))\n\nprint(round(accs[optim_threshold],3))\n\nthresh[optim_threshold]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cr_clean2.shape[1] - imp_data.shape[1]\n# num of total features (including dummies) subtract important features with F score >= 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# original loan defaults by previous default history\ndefault_hist_status_tab= pd.crosstab(cr_clean1['default_hist'], cr_clean1['loan_status'])\ndefault_hist_status_tab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(default_hist_status_tab.iloc[:,1].sum() /cr_data.shape[0],3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The accuracy of the model was 93.3% with 15 important features and 12 feature removed once the data had been cleaned and encoded.**\n\n**The gradient boosted tree performed 7.3% better than the logistics regression and had a higher probability threshold for default by 15%.**\n\n**The initial loan default rate was 21.8%. With the `xgb_model`, the loan default rate should decrease to 6.7%. With this 15.1% improvement in default rate the lenders and applicants are even more so protected from risk.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}