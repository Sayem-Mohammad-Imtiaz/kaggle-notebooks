{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bs4\n!pip install seqeval\n!pip install conllu\n!pip install vncorenlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/huggingface/transformers.git\n%cd transformers\n!pip install .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp /kaggle/input/ner-dataset/ner_dev.txt ./dev.txt\n!cp /kaggle/input/ner-dataset/ner_dev.txt ./test.txt\n!cp /kaggle/input/ner-dataset/ner_train.txt ./train.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/vncorenlp/VnCoreNLP\n%cd /kaggle/working/transformers/examples/legacy/token-classification\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vncorenlp import VnCoreNLP\n\nannotator = VnCoreNLP(\"/kaggle/working/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx2g') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile labels.txt\nB-team\nI-team\nO","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEAM_NAMES = [\n    [\"Barcelona\", \"Barca\", \"BARCELONA\"],\n    [\"Man United\", \"M.U\", \"MU\", \"Man Utd\", \"United\", \"Manchester United\"],\n    [\"B.Bình Dương\", \"Bình Dương\"],\n    [\"AC Milan\", \"Milan\"],\n    [\"Sociedad\", \"Real Sociedad\"], \n    [\"SHB.Đà Nẵng\", \"SHB Đà Nẵng\"],\n    [\"Than Quảng Ninh\", \"Than.QN\"],\n    [\"Bayern Munich\", \"Bayern\"],\n    [\"Bayer Leverkusen\", \"Leverkusen\"],\n    [\"Inter Milan\", \"Inter\"],\n    [\"Real Madrid\", \"Real\", \"REAL MADRID\"],\n    [\"Atletico Madrid\", \"Atletico\"],\n    [\"Stoke City\", \"Stoke\"],\n    [\"LA GALAXY\", \"LA Galaxy\"],\n    [\"Manchester City\", \"Man City\"],\n    [\"U15 SLNA\", \"SLNA\"],\n    [\"U15 HAGL\", \"HAGL\"],\n    [\"Mokpo City FC\", \"CLB Mokpo\"],\n    [\"Athletic Bilbao\", \"Athletic\"],\n    [\"U22 VN\", \"U22 Việt Nam\", \"Việt Nam\", \"U22 ViệtNam\"],\n    [\"U.22 Lào\", \"U22 Lào\"],\n    [\"U22 Campuchia\", \"Campuchia\"],\n    [\"Futsal Thái Lan\", \"Thái Lan\", \"futsal Thái Lan\"],\n    [\"U23 Manchester City\", \"U23 Man City\"],\n    [\"U23 Arsenal\", \"Arsenal\"],\n    [\"U22 Thái Lan\", \"Thái Lan\", \"U 22 Thái Lan\"],\n    [\"Tuyển nữ Việt Nam\", \"đội tuyển Việt Nam\", \"đội tuyển nữ Việt Nam\", \"tuyển nữ Việt Nam\", \"tuyển Việt Nam\", \"ĐT nữ Việt Nam\", \"nữ Việt Nam\", \"VN\", \"Việt Nam\", \"bóng đá nữ Việt Nam\"],\n    [\"nữ futsal Việt Nam\", \"futsal nữ Việt Nam\", \"Việt Nam\"],\n    [\"Burton Albion\", \"Burton\"],\n    [\"Chelsea\", \"The Blues\"],\n    [\"FLC Thanh Hóa\", \"FLC THANH HÓA\"],\n    [\"Olympic Bahrain\", \"Bahrain\"],\n    [\"RB Leipzig\", \"Leipzig\"],\n]\n\n\ndef my_concatenate(str1, str2):\n    if (not str1.endswith('.')) and (not str2.startswith('.')) and (str1!='') :\n        return str1 + ' . ' + str2\n    return str1 + ' ' +str2\n\ndef create_context_from_mi_mr(json_obj, match_info = True, match_result = True):\n    result  = {'match_info': [], 'match_result': []}\n    for html in json_obj['html_annotation']:\n        soup = BeautifulSoup(html)\n\n        if match_info:\n            match_info_ls = soup.find_all('span', {'class':'match_info'})\n            for i in match_info_ls:\n                result['match_info'].append(i.text)\n        if match_result:\n            match_result_ls = soup.find_all('span', {'class':'match_result'})\n            for i in match_result_ls:\n                result['match_result'].append(i.text)\n    return result\n\ndef get_match_summary_as_text(json_obj, label = 'players'):\n    team1  = json_obj['match_summary']['players']['team1']\n    team2 = json_obj['match_summary']['players']['team2']\n    return team1, team2\ndef get_team_name_alias(team_name):\n    for ls in TEAM_NAMES:\n        if team_name in ls:\n            return ls\n    return [team_name]\ndef team_name_appear_in_text_filter(text, team_name_ls):\n    result = []\n    #full_text = ' '.join([i for i in text_list])\n    #print('full_text',full_text)\n    for name in team_name_ls:\n        if name in text:\n            result.append(name)\n    return result\ndef nomalize1(text):\n    result = text.strip('\\n').strip(' ').strip('.')\n    if not result.endswith('.'):\n        result += ' .'\n    return result\n\ndef do_segmentation(text):\n    #return annotator.tokenize(text)\n    result = ' '.join([' '.join(i) for i in annotator.tokenize(text)])\n    #result = nomalize1(result)\n    return result\n\n\n# \ndef create_team_ner_label(text, name_list):\n    text_split = text.split()\n    ner_list = ['O']*len(text_split)\n\n    for name in name_list:\n        if name in text:\n            name_split = name.split()\n            start_index = text_split.index(name_split[0])\n            if len(name_split) == 1:\n                ner_list[start_index] = 'B-team'     \n            else:\n                end_index = start_index + len(name_split) - 1\n                for i in range(start_index, end_index + 1):\n                    if i==end_index:\n                        ner_list[i] = 'I-team'\n                    else:\n                        ner_list[i] = 'B-team'                    \n    return (text_split, ner_list)\n\n\n\n# util\ndef find_sub_list(sub_ls, ls):\n    sub_ls_len = len(sub_ls)\n    result_ls = []\n    for i in [i for i, v in enumerate(ls) if v==sub_ls[0]]:\n        if ls[i:i+sub_ls_len] == sub_ls:\n            result_ls.append((i,i+sub_ls_len-1))\n    return result_ls\n\n\ndef ibo_annotate(ls, begin, end, ent = 'team'):\n    if begin == end:\n        ls[begin]='B-' + ent\n    else:\n        ls[begin]='B-' + ent\n        for i in range(begin+1,end+1):\n                ls[i] = 'I-' + ent\n\ndef create_team_ner_label2(text, name_list):\n    text_split = text.split()\n    ner_list = ['O']*len(text_split)\n    \n    for name in name_list:\n        name = name.split()\n        #print(f'sub ls:{name}')\n        ids = find_sub_list(name, text_split)\n        #print(ids)\n        for item in ids:\n            begin, end = item\n            if ner_list[begin] == 'O':\n                ibo_annotate(ner_list, begin, end, 'team')\n    return text_split, ner_list\n            \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Fine-tuning the library models for named entity recognition on CoNLL-2003. \"\"\"\nimport logging\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom importlib import import_module\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nfrom seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom torch import nn\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    set_seed)\nfrom transformers.trainer_utils import is_main_process\nfrom utils_ner import Split, TokenClassificationDataset, TokenClassificationTask\n\n\nlogger = logging.getLogger(__name__)\nSEED = 42\nDO_PREDICT = True\nOUTPUT_DIR = './'\nDO_TRAIN = False\nLOCAL_RANK = -1\nUSE_FAST = False\nFP16 = False\nLABELS = 'labels.txt'\nDEVICE = 'cuda'\nOVERWRITE_OUTPUT_DIR = False\nMODEL_NAME_OR_PATH = '/kaggle/input/pretrain-model-news-summ/ner_team_pretrained_model'\nMAX_SEQ_LENGTH = 128\nTASK_TYPE = 'NER'\nOVERWRITE_CACHE= True\nCACHE_DIR = None\nDATA_DIR = './'\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    task_type: Optional[str] = field(\n        default=\"NER\", metadata={\"help\": \"Task type to fine tune in training (e.g. NER, POS, etc)\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n    # or just modify its tokenizer_config.json.\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    data_dir: str = field(\n        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n    )\n    labels: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n    )\n    max_seq_length: int = field(\n        default=128,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n\n\n        \n        \n\nif (\n    os.path.exists(OUTPUT_DIR)\n    and os.listdir(OUTPUT_DIR)\n    and DO_TRAIN\n    and not OVERWRITE_OUTPUT_DIR\n):\n    raise ValueError(\n        f\"Output directory ({OUTPUT_DIR}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n    )\n\nmodule = import_module(\"tasks\")\ntry:\n    token_classification_task_clazz = getattr(module, TASK_TYPE)\n    token_classification_task: TokenClassificationTask = token_classification_task_clazz()\nexcept AttributeError:\n    raise ValueError(\n        f\"Task {TASK_TYPE} needs to be defined as a TokenClassificationTask subclass in {module}. \"\n        f\"Available tasks classes are: {TokenClassificationTask.__subclasses__()}\"\n    )\n\n# Setup logging\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    level=logging.INFO if LOCAL_RANK in [-1, 0] else logging.WARN,\n)\n\n# Set the verbosity to info of the Transformers logger (on main process only):\nif is_main_process(LOCAL_RANK):\n    transformers.utils.logging.set_verbosity_info()\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n#logger.info(\"Training/evaluation parameters %s\", training_args)\n\n# Set seed\nset_seed(SEED)\n\n# Prepare CONLL-2003 task\nlabels = token_classification_task.get_labels(LABELS)\nlabel_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\nnum_labels = len(labels)\n\n# Load pretrained model and tokenizer\n#\n# Distributed training:\n# The .from_pretrained methods guarantee that only one local process can concurrently\n# download model & vocab.\n\nconfig = AutoConfig.from_pretrained(\n    MODEL_NAME_OR_PATH,\n    num_labels=num_labels,\n    id2label=label_map,\n    label2id={label: i for i, label in enumerate(labels)},\n    cache_dir=CACHE_DIR,\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME_OR_PATH,\n    cache_dir=CACHE_DIR,\n    use_fast=USE_FAST,\n)\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_NAME_OR_PATH,\n    from_tf=bool(\".ckpt\" in MODEL_NAME_OR_PATH),\n    config=config,\n    cache_dir=CACHE_DIR,\n)\n\n# Get datasets\n\n\ndef align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n    preds = np.argmax(predictions, axis=2)\n\n    batch_size, seq_len = preds.shape\n\n    out_label_list = [[] for _ in range(batch_size)]\n    preds_list = [[] for _ in range(batch_size)]\n\n    for i in range(batch_size):\n        for j in range(seq_len):\n            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n                out_label_list[i].append(label_map[label_ids[i][j]])\n                preds_list[i].append(label_map[preds[i][j]])\n\n    return preds_list, out_label_list\n\ndef compute_metrics(p: EvalPrediction) -> Dict:\n    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n    return {\n        \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n        \"precision\": precision_score(out_label_list, preds_list),\n        \"recall\": recall_score(out_label_list, preds_list),\n        \"f1\": f1_score(out_label_list, preds_list),\n    }\n\n# Data collator\ndata_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) if FP16 else None\n\n# Initialize our Trainer\ntrainer = Trainer(\n    model=model,\n    #args=training_args,\n    #train_dataset=train_dataset,\n    #eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\n\n# # Predict\n# if DO_PREDICT:\n#     test_dataset = TokenClassificationDataset(\n#         token_classification_task=token_classification_task,\n#         data_dir=DATA_DIR,\n#         tokenizer=tokenizer,\n#         labels=labels,\n#         model_type=config.model_type,\n#         max_seq_length=MAX_SEQ_LENGTH,\n#         overwrite_cache=OVERWRITE_CACHE,\n#         mode=Split.test,\n#     )\n\n#     predictions, label_ids, metrics = trainer.predict(test_dataset)\n#     preds_list, _ = align_predictions(predictions, label_ids)\n\n#     output_test_results_file = os.path.join(OUTPUT_DIR, \"test_results.txt\")\n#     if trainer.is_world_process_zero():\n#         with open(output_test_results_file, \"w\") as writer:\n#             for key, value in metrics.items():\n#                 logger.info(\"  %s = %s\", key, value)\n#                 writer.write(\"%s = %s\\n\" % (key, value))\n\n#     # Save predictions\n#     output_test_predictions_file = os.path.join(OUTPUT_DIR, \"test_predictions.txt\")\n#     if trainer.is_world_process_zero():\n#         with open(output_test_predictions_file, \"w\") as writer:\n#             with open(os.path.join(DATA_DIR, \"test.txt\"), \"r\") as f:\n#                 token_classification_task.write_predictions_to_file(writer, f, preds_list)\n\n\n\ndef _mp_fn(index):\n    # For xla_spawn (TPUs)\n    main()\n\n\n# if __name__ == \"__main__\":\n#     main()\n    \n\n    \ndef predict():\n    # Predict\n        test_dataset = TokenClassificationDataset(\n            token_classification_task=token_classification_task,\n            data_dir=DATA_DIR,\n            tokenizer=tokenizer,\n            labels=labels,\n            model_type=config.model_type,\n            max_seq_length=MAX_SEQ_LENGTH,\n            overwrite_cache=OVERWRITE_CACHE,\n            mode=Split.test,\n        )\n\n        predictions, label_ids, metrics = trainer.predict(test_dataset)\n        preds_list, _ = align_predictions(predictions, label_ids)\n\n        output_test_results_file = os.path.join(OUTPUT_DIR, \"test_results.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_test_results_file, \"w\") as writer:\n                for key, value in metrics.items():\n                    logger.info(\"  %s = %s\", key, value)\n                    writer.write(\"%s = %s\\n\" % (key, value))\n\n        # Save predictions\n        output_test_predictions_file = os.path.join(OUTPUT_DIR, \"test_predictions.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_test_predictions_file, \"w\") as writer:\n                with open(os.path.join(DATA_DIR, \"test.txt\"), \"r\") as f:\n                    token_classification_task.write_predictions_to_file(writer, f, preds_list)\n                    \ndef predict2():\n    # Predict\n        test_dataset = TokenClassificationDataset(\n            token_classification_task=token_classification_task,\n            data_dir=DATA_DIR,\n            tokenizer=tokenizer,\n            labels=labels,\n            model_type=config.model_type,\n            max_seq_length=MAX_SEQ_LENGTH,\n            overwrite_cache=OVERWRITE_CACHE,\n            mode=Split.test,\n        )\n\n        predictions, label_ids, metrics = trainer.predict(test_dataset)\n        preds_list, _ = align_predictions(predictions, label_ids)\n#         print(preds_list)\n#         for i in preds_list:\n#             print(len(i))\n        output_test_results_file = os.path.join(OUTPUT_DIR, \"team_test_results.txt\")\n        if trainer.is_world_process_zero():\n            print('is_world_process_zero')\n            with open(output_test_results_file, \"w\") as writer:\n                for key, value in metrics.items():\n                    logger.info(\"  %s = %s\", key, value)\n                    writer.write(\"%s = %s\\n\" % (key, value))\n\n        # Save predictions\n        output_test_predictions_file = os.path.join(OUTPUT_DIR, \"team_test_predictions.txt\")\n        if trainer.is_world_process_zero():\n            with open(output_test_predictions_file, \"w\") as writer:\n                with open(os.path.join(DATA_DIR, \"test.txt\"), \"r\") as f:\n                    token_classification_task.write_predictions_to_file(writer, f, preds_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only 1 sentence\ndef extract_ne_from_ner_prediction(txt_predict_file, ne = 'team'):\n    with open(txt_predict_file, 'r') as f:\n        results= []\n        ls = list(map(lambda x:tuple(x.strip('\\n').split()), f))\n        #print(ls,'\\n')\n        i=0\n        while True:\n            if i>=len(ls): break\n            w, p = ls[i]\n            if p=='B-'+ne:\n                name = ''\n                name += w\n                j = i + 1\n                while True:\n                    if j>=len(ls): break\n                    if ls[j][1] == ('I-'+ ne):\n                        name += ' ' + ls[j][0]\n                    else: break\n                    j+=1\n                #print(f'\\\"{name}\\\"')\n                results.append(name)\n            i+=1\n    return results\n            \n                \n#extract_ne_from_ner_prediction('team_test_predictions.txt', 'team')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cell này chạy thử tìm 2 đội (giả sử bước classsify đúng match_info và match_result)\n\nimport json\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n# predict team\n\ntrain_path = '/kaggle/input/zacdata/train/train.jsonl'\nwith open('label_predict.txt', 'w') as f_pred:\n    with open(train_path,'r') as f:\n\n        ls = list(f)\n        for item in ls[600:650]: # đã train trên 0:600 bài, bên chạy thử từ 600: cuối\n            team_predict_ls = []\n\n\n            json_obj = json.loads(item)\n            train_id = json_obj['train_id']\n            # create context from match_info, match_result\n            context_dict = create_context_from_mi_mr(json_obj)\n            # get team label (as text)\n            team1, team2 = get_match_summary_as_text(json_obj, label = 'players')\n            # get team name alias\n            team1_ls = get_team_name_alias(team1)\n            team2_ls = get_team_name_alias(team2)\n            #print(team1_ls, '-' ,team2_ls)\n            #print(context_dict)\n            #print('')\n            full_text = ' '.join(context_dict['match_info']) + ' ' + ' '.join(context_dict['match_result'])\n            #print(full_text)\n            team1_ls = team_name_appear_in_text_filter(full_text, team1_ls)\n            team2_ls = team_name_appear_in_text_filter(full_text, team2_ls)\n            #print(context_dict)\n            team1_ls = [do_segmentation(i) for i in team1_ls]\n            team2_ls = [do_segmentation(i) for i in team2_ls]\n\n            print('TEAM NAME LABEL:',team1_ls, '-' ,team2_ls)\n            #ner_data = []\n            #print(context_dict)\n            for field in ['match_info', 'match_result']:\n                for item in context_dict[field]:\n                    item_segmented = do_segmentation(item)\n                    #print(item_segmented)\n\n                    text_split, ner_list = create_team_ner_label2(item_segmented, team1_ls + team2_ls)\n                    #print(text_split, [-1]*len(ner_list))\n                    with open('test.txt', 'w') as f_ner:\n                        #print('heeeee')\n                        for t in  text_split:\n                            #print(t)\n                            f_ner.write(f'{t} O\\n')\n                    #print(pd.read_csv('test.txt', sep = ' '))\n                    predict2()\n                    #print(pd.read_csv('team_test_predictions.txt', sep = ' '))\n                    #print('--------------')\n                    team_predict = extract_ne_from_ner_prediction('team_test_predictions.txt', 'team')\n                    team_predict_ls.extend(team_predict)\n                    f_pred.write(f'train_id: {train_id} groundtruth:{team1_ls}-{team2_ls} predict:{team_predict_ls}\\n')\n                    print('team predict:',team_predict_ls)\n                    print('--------------')\n            print('=================================')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat label_predict.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}