{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"import time\n\n# merge buckets if the number of the type is greater than 2\ndef drop(arr):\n    for i in range(len(arr)-1, 1, -1):\n        if arr[i][1]==arr[i-2][1]:\n            # arr[i-2][0] = arr[i-1][0]\n            arr[i-2][1] += 1\n            del arr[i-1]\n\n# get the data stream\ndata = open(\"/kaggle/input/coding2/stream_data.txt\",'r')\nf = data.readline()\ndata.close()\nact_list = f.strip().split('\\t')\n\n# my DGIM\nstamp = 0\nN = 1000\ntime_list = []\ntime1 = time.time()\nnum = 0\nfor i in act_list:\n    stamp += 1\n    if int(i)==0:\n        continue\n    else:        \n        if len(time_list) > 0 and time_list[0][0] + N <= stamp:\n            del time_list[0]\n        time_list.append([stamp, 0])\n        drop(time_list)\nfor i in time_list:\n    num += 2**(i[1])\nnum -= 2**(time_list[0][1]-1)\nprint(\"Execution time of my DGIM: \",time.time()-time1)\nprint(\"The buckets: \",time_list)\nprint(\"The number of 1-bits: \",num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"import time\n\n# get the data stream\ndata = open(\"/kaggle/input/coding2/stream_data.txt\",'r')\nf = data.readline()\ndata.close()\nact_list = f.strip().split('\\t')\n\n# get the actual number of 1-bits in the windows\nnum = 0\nN = 1000\ntime1 = time.time()\nfor i in range(len(act_list) - N, len(act_list)):\n    if int(act_list[i])==1:\n        num +=1\nprint(\"Execution time of actual count: \",time.time()-time1)\nprint(\"Actual number of 1-bits:\",num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running time: Actual count(0.00051) is less than my DGIM(0.07903). I refer the reason is that DGIM spends more time on drop buckets.\nSpace: My DGIM(loglogN) is less than actual count(N), which is its advantage.\nRelative error: |316-391|/391=19.2%<50%","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"# # read the shingles of documents and stores in the list row after processing.\n# import csv\n\n# with open('/kaggle/input/coding2/docs_for_lsh.csv','r') as csvfile:\n#     reader = csv.reader(csvfile)\n#     rows_ori = [row for row in reader]\n#     rows = []\n#     del rows_ori[0]\n#     for i in range(0,len(rows_ori)):\n#         row = rows_ori[i]\n#         row = [int(x) for x in row]\n#         del row[0]\n#         rows.append(row)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # define the hash fuction\n# import random\n\n# # generate minhash function\n# def get_minhash(factor, constant, size, x):\n#     return (factor*x+constant)%size\n\n# # generate hash function for LSH\n# def hash1(x,length):\n#     s = 0;\n#     for i in range(len(x)):\n#         s = ((int(s)<<2)+(int(x[i])>>4))^(int(x[i])<<10)\n#     s = s % length\n#     if s<0:\n#         s += length\n#     return s\n\n# # generate specific number of minhash functions and return the list of parameters(prime number) of functions.\n# # parameters are randomly generated.\n# def get_hashlist(sigsize=100):\n#     hlist=[]\n#     num1 = [3,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,233,239,241,251,257,263,269,271,277,281,283,293]\n#     num2 = [97,101,103,107,109,113,127,131,137,139,149,151,157,163,167,173,179,181,191,193,197,199,307,311,313,317,331,337,347,349,353,359,367,373,379,383,389,397,401,409]\n#     for i in range(sigsize):\n#         k1 = random.randint(0,len(num1)-1)\n#         k2 = random.randint(0,len(num2)-1)\n#         factor=num1[k1]\n#         constant=num2[k2]\n#         hlist.append((factor,constant))\n#     return hlist \n\n# hlist=get_hashlist(150)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # generate my signature matrix\n# import pandas as pd\n\n# # for each document shingles, generate signature list of the document\n# def get_signature(shingle, hashlist):\n#     signature=[]\n#     size=len(shingle)\n#     sigsize = len(hashlist)\n#     for i in range(sigsize):\n#         sig=float(\"inf\")\n#         for k in range(0,size):\n#             if shingle[k]==1:\n#                 value = get_minhash(hashlist[i][0], hashlist[i][1], size, k)\n#                 if value<sig:\n#                     sig=value               \n#             else:\n#                 pass\n#         signature.append(sig)\n#     return signature\n\n\n# # my jaccard similariry fuction\n# def jaccard_sig(signature1, signature2):\n#     k=0\n#     for i in range(len(signature1)):\n#         if signature1[i]==signature2[i]:\n#             k+=1;\n#     k = k/len(signature1)\n#     return k\n\n\n# sigall=[]\n# if os.path.exists('./mysignature_test1.csv'):\n#     #get signatures from preprocess file\n#     with open('./mysignature_test1.csv','r') as csvfile:\n#         reader = csv.reader(csvfile)\n#         rows_ori = [row for row in reader]\n#         del rows_ori[0]\n#         for i in range(0,len(rows_ori)):\n#             row = rows_ori[i]\n#             row = [int(x) for x in row]\n#             del row[0]\n#             sigall.append(row)\n# else:\n#     # generate signatures and store them as file\n#     for i in range(len(rows)):\n#         sigall.append(get_signature(rows[i],hlist))\n#     mysig = pd.DataFrame(data=sigall)\n#     mysig.to_csv('./mysignature_test1.csv')\n#     print(\"finish\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # LSH\n# import csv\n# import random\n# import os\n# from operator import itemgetter\n# import numpy as np\n\n# def choose_similiar(signature, bands=10, row=10, bucsize=5000000,s=0.8):\n#     candidates={}\n#     for i in range(1):\n#         buckets=[]\n#         for j in range(bucsize):\n#             buckets.append([])\n#         for k in range(len(signature)):\n#             band0=signature[k]\n#             band1=band0[i*row:(i+1)*row]\n#             band=\"\".join('%s' %l for l in band1)\n#             buckets[hash1(band,bucsize)].append(k)\n#         #judge the similarity if they are in the same bucket for at least one band\n#         for item in buckets:\n#             if len(item)>1:\n#                 for i1 in range(len(item)):\n#                     for j1 in range(i1+1,len(item)):\n#                         pair = (item[i1],item[j1])\n#                         if pair not in candidates:\n#                             A = signature[item[i1]]\n#                             B = signature[item[j1]]\n#                             sim = jaccard_sig(A, B)\n#                             if sim >= s:\n#                                  candidates[pair] = sim\n#         print(\"band-finish\",i) #hint print\n#     #ordered by the similarity\n#     sort = sorted(candidates.items(),key=itemgetter(1), reverse=True)\n#     return candidates, sort\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# # Your code here, you can add cells if necessary\n# from sklearn.metrics import jaccard_score\n\n# # c,s=choose_similiar(sigall)\n# # for item in s:\n# #     if item[0][0]==0:\n# #         print(item)\n# # print(s)\n\n# #calculate the actual result\n# same_act=[]\n# for k in range(len(rows)):\n#     same_act.append([jaccard_score(rows[0],rows[k]),k])\n# same_act.sort()\n# same_act.reverse()\n# print(\"The actually 30 most similiar document ids\", same_act[1:31])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The actually 30 most similiar document ids [[0.9090909090909091, 91300], [0.9090909090909091, 89833], [0.9090909090909091, 32681], [0.8333333333333334, 89825], [0.8333333333333334, 84306], [0.8333333333333334, 81480], [0.8333333333333334, 81379], [0.8333333333333334, 62080], [0.8333333333333334, 40298], [0.8333333333333334, 39310], [0.8333333333333334, 28910], [0.8333333333333334, 20854], [0.8181818181818182, 99370], [0.8181818181818182, 84520], [0.8181818181818182, 81289], [0.8181818181818182, 73681], [0.8181818181818182, 72156], [0.8181818181818182, 69724], [0.8181818181818182, 68730], [0.8181818181818182, 67032], [0.8181818181818182, 58852], [0.8181818181818182, 58694], [0.8181818181818182, 52076], [0.8181818181818182, 48131], [0.8181818181818182, 46220], [0.8181818181818182, 39784], [0.8181818181818182, 26980], [0.8181818181818182, 23585], [0.8181818181818182, 2575], [0.8181818181818182, 1331]]","metadata":{}},{"cell_type":"markdown","source":"**Since kaggle and jupyter notebook can't keep running for a long time, I used the server to get the following result with the same code.**\n\nmy result:((0, 67032), 0.91),((0, 89833), 0.91),((0, 23585), 0.9),((0, 2575), 0.89),((0, 48131), 0.89),((0, 32681), 0.89),((0, 91300), 0.89),((0, 39784), 0.86),((0, 1331), 0.85),((0, 28910), 0.85),((0, 78531), 0.84),((0, 58852), 0.83),((0, 69724), 0.83),((0, 62080), 0.83),((0, 66125), 0.83),((0, 72156), 0.82),((0, 52076), 0.82),((0, 58694), 0.82),((0, 61193), 0.82),((0, 44663), 0.82),((0, 40298), 0.81),((0, 81480), 0.81),((0, 20854), 0.81),((0, 72078), 0.81),((0, 14134), 0.81),((0, 26261), 0.81),((0, 81289), 0.8),((0, 89825), 0.8),((0, 81379), 0.8),((0, 39310), 0.8). The number of intersetion is 23. 78531,66125,61193,44663,72078,14134 and 26261 are misjudged. Excepted 14131(0.42 similarity), these similiarity score is all 0.75, which means my method is right but the accuracy can be improved.","metadata":{}}]}