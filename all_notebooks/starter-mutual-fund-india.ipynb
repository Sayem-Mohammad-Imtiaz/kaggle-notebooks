{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook uses internet to install pyspark\n\nAdvantage of pyspark: We can load all csv file at once and use sql style query. This will enable us to easily filter the data and efficient from the point of view of updation.  \n\nPackage Documentation: https://spark.apache.org/docs/latest/api/python/index.html"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"!pip install -q pyspark\n!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from IPython.core.display import HTML\ndisplay(HTML(\"<style>pre { white-space: pre !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport time\nimport os\n\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n\n!java -version","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, IntegerType, DoubleType, StructField, StructType, DateType\nfrom pyspark.sql.functions import array, col, explode, lit, create_map\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n\nfrom itertools import chain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_na(data, normalize=False):\n    if normalize:\n        count = data.count()\n        return data.select([(F.count(F.when(F.isnan(c) | F.col(c).isNull() if t not in ('date', 'timestamp') else F.col(c).isNull(), c))/count).alias(c) for c, t in data.dtypes])\n    return data.select([(F.count(F.when(F.isnan(c) | F.col(c).isNull() if t not in ('date', 'timestamp') else F.col(c).isNull(), c))).alias(c) for c, t in data.dtypes])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Printing few file name\nfor dirname, _, filenames in os.walk('/kaggle/input/DailyNAV'):\n    for filename in filenames[:10]:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\nsc = spark.sparkContext\nsc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_schema = StructType(\n    [StructField('Scheme_Code', IntegerType(), False), \n     StructField('Scheme_Name', StringType(), False), \n     StructField('ISIN Div Payout/ISIN Growth', StringType(), False),\n     StructField('ISIN Div Reinvestment', StringType(), False),\n     StructField('NAV', DoubleType(), False),\n     StructField('Repurchase Price', DoubleType(), False), \n     StructField('Sale Price', DoubleType(), False), \n     StructField('Date', DateType(), False)]\n)\n\ndata = spark.read.csv('/kaggle/input/DailyNAV', header = True, schema=data_schema)\ndata.cache()\ndata.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.rdd.getNumPartitions()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validating All Days"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nunique_date = data.select('Date').distinct().orderBy('Date').toPandas()['Date']\nunique_date = pd.to_datetime(unique_date)\nunique_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_date = unique_date.min()\nmax_date = unique_date.max()\n\ndate_range = pd.date_range(min_date, max_date, freq='1D')\nassert set(date_range) == set(unique_date)\n\nmin_date = min_date.strftime(\"%Y-%m-%d\")\nmax_date = max_date.strftime(\"%Y-%m-%d\")\n\nprint(f'Data contains all days between {min_date} and {max_date}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter for a date"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata_single_day = data.where('Date == \"2020-12-05\"')\ndata_single_day.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter on Latest Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlatest_date = data.selectExpr('max(Date) as Date').collect()[0]['Date'].strftime(\"%Y-%m-%d\")\nprint(f'latest_date: {latest_date}')\n\nlatest_data = data.where(f'Date == \"{latest_date}\"')\nlatest_data.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter for a Mutual Fund"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\naxis_long_eq = data.where('Scheme_Name == \"Axis Long Term Equity Fund - Direct Plan - Growth Option\"').orderBy('Date')\naxis_long_eq.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert into Pandas\n\nCAUTION: pyspark doesn't keep data in memory but pandas does. So, below is Memory intensive command"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npandas_data = axis_long_eq.toPandas()\npandas_data['Date'] = pd.to_datetime(pandas_data['Date'])\npandas_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pandas_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pandas_data.query('NAV == 0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Observe for one date NAV is 0. Thus we will drop this obervation\npandas_data.loc[pandas_data['NAV'] == 0, 'NAV'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pandas_data.set_index('Date')['NAV'].plot(figsize=(20, 10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}