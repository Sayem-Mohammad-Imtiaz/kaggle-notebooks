{"cells":[{"metadata":{},"cell_type":"markdown","source":"> This kernel is based on the work of http://hunterheidenreich.com/blog/elmo-word-vectors-in-keras/"},{"metadata":{},"cell_type":"markdown","source":"1. # 1. Kernel Overview"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Defination :"},{"metadata":{},"cell_type":"markdown","source":"In today world** Text Classification/Segmentation/Categorization** (for example ticket categorization in a call centre, email classification, logs category detection etc.) is a common task. With humongous data out there, its nearly impossible to do this manually. Let's try to solve this problem automatically using machine learning and natural language processing tools."},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"BBC articles dataset(2126 records) consist of two features text and the assiciated categories namely \n1. Sport \n2. Business \n3. Politics \n4. Tech \n5. Others\n\n**Our task is to train a multiclass classification model on the mentioned dataset.**"},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Metrics"},{"metadata":{},"cell_type":"markdown","source":"**Accuracy** - Classification accuracy is the number of correct predictions made as a\nratio of all predictions made\n\n**Precision** - precision (also called positive predictive value) is the fraction of\nrelevant instances among the retrieved instances\n\n**F1_score** - considers both the precision and the recall of the test to compute the\nscore\n\n**Recall** â€“ recall (also known as sensitivity) is the fraction of relevant instances that\nhave been retrieved over the total amount of relevant instances\n\n**Why these metrics?** - We took Accuracy, Precision, F1 Score and Recall as metrics\nfor evaluating our model because accuracy would give an estimate of correct prediction. Precision would give us an estimate about the positive category predicted value i.e. how much our model is giving relevant result. F1 Score gives a clubbed estimate of precision and recall.Recall would provide us the relevant positive category prediction to the false negative and true positive category recognition results."},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Machine Learning Model Considered:"},{"metadata":{},"cell_type":"markdown","source":"We will be using **ELMO embeddings with KERAS** for this use case. \n\nELMO and KERAS is not in the scope of this kernal. Kindly refer other external sources."},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"### Step 2.1 Load Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndata=pd.read_csv(r\"../input/bbc-text.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Implementation"},{"metadata":{},"cell_type":"markdown","source":"### Step 2.2 Map Textual labels to numeric using Label Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndf2 = pd.DataFrame()\ndf2[\"text\"] = data[\"text\"]\ndf2[\"label\"] = LabelEncoder().fit_transform(data[\"category\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndf2['text'] = df2['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf2['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(df2['text']).split()).value_counts()[-10:]\ndf2['text'] = df2['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ndf2['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2.3 Import the Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom tqdm import tqdm\nimport re\nimport time\nimport pickle\npd.set_option('display.max_colwidth', 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub\nimport tensorflow as tf\n\nembed = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2.4 Convert Sentence to Elmo Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nfrom sklearn import preprocessing\nimport keras\nimport numpy as np\n\n\ny = list(df2['label'])\nx = list(df2['text'])\n\nle = preprocessing.LabelEncoder()\nle.fit(y)\n\ndef encode(le, labels):\n    enc = le.transform(labels)\n    return keras.utils.to_categorical(enc)\n\ndef decode(le, one_hot):\n    dec = np.argmax(one_hot, axis=1)\n    return le.inverse_transform(dec)\n\n\nx_enc = x\ny_enc = encode(le, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2.5 Divide dataset to test and train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(np.asarray(x_enc), np.asarray(y_enc), test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2.5 Train Keras neural model with ELMO Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, Lambda, Dense\nfrom keras.models import Model\nimport keras.backend as K\n\ndef ELMoEmbedding(x):\n    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n\ninput_text = Input(shape=(1,), dtype=tf.string)\nembedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\ndense = Dense(256, activation='relu')(embedding)\npred = Dense(5, activation='softmax')(dense)\nmodel = Model(inputs=[input_text], outputs=pred)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())  \n    session.run(tf.tables_initializer())\n    history = model.fit(x_train, y_train, epochs=1, batch_size=16)\n    model.save_weights('./elmo-model.h5')\n\nwith tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('./elmo-model.h5')  \n    predicts = model.predict(x_test, batch_size=16)\n\ny_test = decode(le, y_test)\ny_preds = decode(le, predicts)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(metrics.confusion_matrix(y_test, y_preds))\n\nprint(metrics.classification_report(y_test, y_preds))\n\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy of ELMO is:\",accuracy_score(y_test,y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">** Past Work mentioned on this dataset at max achieved 95.22 accuracies. keras with ELMO embeddings achieved 95.5. But still BERT base model without any preprocessing and achieved 97.75 accuracies.\n\n[bert model]https://www.kaggle.com/sarthak221995/textclassification-97-77-accuracy-bert\n**"},{"metadata":{},"cell_type":"markdown","source":"# 5. Future Improvements on this kernel:"},{"metadata":{},"cell_type":"markdown","source":"* Explore preprocessing steps on data.\n* Explore other models as baseline.\n* Make this notebook more informative and illustrative.\n* Explaination on ELMO Embeddings Model.\n* More time on data exploration\nand many more..."},{"metadata":{},"cell_type":"markdown","source":"# 6. References"},{"metadata":{},"cell_type":"markdown","source":"> This kernel is based on the work of http://hunterheidenreich.com/blog/elmo-word-vectors-in-keras/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}