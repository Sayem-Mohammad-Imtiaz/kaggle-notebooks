{"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimizing the prediction of NHL player salary:\n## Blending machine learning algorithms to reduce predction error.\n   In previous explorations of this dataset, I have employed extreme gradient boosting and random forest regression to try to make accurate predictions of NHL player salary. These models have proven interesting, showing that player's draft year along with numerous advanced stats are some of the best predictors of the amount of money they make. The default XGBoost model had a fairly high root mean square error though: \\$1,574,073. This is a definite improvement over just guessing the median salary, which provides an rmse of \\$2,878,624. However, here I am looking to improve this error and create a more accurate predictor of player salary. To do this, I will be attempting to train several optimized models (via cross validation of hyperparamaters) and then blend the results to create a more accurate predition. I have chosen to proceed with the two previously employed models, Random Forest and XGBoost, and also add in a support vector machine to see if I can reach a blended and optimized ensemble model.\n\nAs a tangental goal, I am also looking to streamline the data munging process using some of the built in functions of Scikit Learn to clean the data and make dummy variables where needed.","metadata":{"_uuid":"0d75d095aafbb71043c39f3b396bd2e299af304f","_cell_guid":"c3046b3d-5d90-4ede-b166-1bf815558137"}},{"cell_type":"markdown","source":"First up, a big old pile of imports! This is definitely the downside to using the SciKit built-ins, you have to remember the imports for all the functions you need!","metadata":{"_uuid":"38c7a77d2cfcf9aadf65fdbc30346f841cca1910","_cell_guid":"5d535f6b-5389-4d64-9ab3-832f31277189"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import FeatureUnion\nfrom datetime import datetime\nimport gc\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n","metadata":{"_uuid":"24d32eed129dd7a72fce06b20c67357e03be5d43","collapsed":true,"_cell_guid":"6371828f-e84a-4026-8b52-cce198f8c282"}},{"cell_type":"markdown","source":"## Load the data","metadata":{"_uuid":"d0267f51b5a2128ad0906b49a0a3ae0287344d39","_cell_guid":"aed3e8ce-c8a9-419a-84cb-7da48cd80731"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train = pd.read_csv('../input/train.csv', encoding = \"ISO-8859-1\")\ntest_x = pd.read_csv('../input/test.csv', encoding = \"ISO-8859-1\")\ntest_y = pd.read_csv('../input/test_salaries.csv') ","metadata":{"_uuid":"c8ad3f7c07cc447a919253796bd30de6a7577108","collapsed":true,"_cell_guid":"305887bb-03b0-4970-bdc4-5ccaa164f981"}},{"cell_type":"markdown","source":"set up the train and test variables","metadata":{"_uuid":"640fe379a94f8c6f00cc315422766c6972dfb036","_cell_guid":"be31a3eb-d2f4-4af7-bee7-84409b6104b7"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\ntest_y=list(test_y['Salary'].values)\n\ntrain_x = train.drop('Salary',axis=1)\ntrain_y = list(train['Salary'])\n\ntrain=[]\ngc.collect()","metadata":{"_uuid":"e462ba2f5f2267b5b1ade37fab4518812caf8386","collapsed":true,"_cell_guid":"eaa38439-30d5-4277-b056-8d3d7ac7e7e1"}},{"cell_type":"markdown","source":"## clean the data\n\nBased on my previous experience with the dataset, I've seen that the team of the player, along with the country of origin are poor predictors of salary, so I am electing to remove these outright. Below I use the python datetime functions to take the date of birth column, and turn it into the age (in days) at season's start.","metadata":{"_uuid":"d6e42da1a14e3b91f29d8822493fdfc1197591e9","_cell_guid":"4052e2b5-3c63-4b37-83e3-d22210335efa"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n#Born - datetime needs to be changed to days since a set date\n#days form birth to season start\n\ndef elapsed_days(start, end=datetime(2016,10,12)):\n\t\"\"\" calcualte the number of days start and end dates\"\"\"\n\tx = (end - start)\n\treturn x.days\n\n#\ntrain_x['age_season_start'] = train_x.apply(lambda x: \n\telapsed_days(datetime.strptime(x['Born'], '%y-%m-%d')) ,axis=1)\n\ntest_x['age_season_start'] = test_x.apply(lambda x: \n\telapsed_days(datetime.strptime(x['Born'], '%y-%m-%d')) ,axis=1)\n\n","metadata":{"_uuid":"99d81536ada692ba50bcee972105c56020eff750","collapsed":true,"_cell_guid":"7fe7af71-50b1-434c-9b60-f03b9087ed21"}},{"cell_type":"markdown","source":"With the age columns altered, we can drop the unneeded information","metadata":{"_uuid":"734d2eddbabaf90a5bd0a9d77dea130cce7884ac","_cell_guid":"cc3bd832-6725-44ae-964d-eede2ca25591"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n# Drop the city, province and Cntry cols, will include nationality but all these\n# seemed redundant on the initial rf and XGBoost models\n\ndrop_cols = ['City', 'Pr/St', 'Cntry', 'Last Name', 'First Name', 'Team', 'Born']\n\ntest_x.drop(drop_cols, axis = 1, inplace = True)\n\ntrain_x.drop(drop_cols, axis = 1, inplace = True)\n\n","metadata":{"_uuid":"4faa3b5932c9f66e9e3d4643e6a89476ffc1ef3b","collapsed":true,"_cell_guid":"b68c1cbc-e332-421c-81a6-73191609f5be"}},{"cell_type":"markdown","source":"## Use of sckikit pipelines for data processing\n\nWith the data cleaned, I can now use parallel data processing pipelines to:\n - impute the median for missing numerical values\n - binarize (one-hot encode) each of the categorical columns\n - merge the numerical and categorical arrays into a single input\n \nThis process is shown below. I first identify the numerical and categorical columns, then write a DataFrameSelector class to pull these columns out of the input and use them in the processsing pipelines.","metadata":{"_uuid":"0b475555433473d9ea6f291f0251fddca17e3bec","_cell_guid":"a7288e43-ab8b-4408-b3ef-24db1beb7331"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n\n#check the data types of the remaining columns\ntrain_x.dtypes\nfor i in train_x.dtypes:\n\tprint(i)\n\n\n#Categoricals:\ncat_attribs = ['Nat', 'Hand', 'Position']\n\nnum_attribs = list(train_x.drop(cat_attribs,axis=1).columns)\n","metadata":{"_uuid":"5f4d6d586b72b9c509e44d3a66d418551ddb71ba","collapsed":true,"_cell_guid":"24a40cef-5fdb-4f51-9cc7-1f1e7118a3d6"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n\t\"\"\" this class will select a subset of columns,\n\t\tpass in the numerical or categorical columns as \n\t\tattribute names to get just those columns for processing\"\"\"\n\tdef __init__(self, attribute_names):\n\t\tself.attribute_names = attribute_names\n\tdef fit(self, X, y=None):\n\t\treturn self\n\tdef transform(self, X):\n\t\treturn X[self.attribute_names]\n\n","metadata":{"_uuid":"2218348e2fe0442835313999031f948b632749ca","collapsed":true,"_cell_guid":"22213a11-e714-450a-a837-ae21bee3cbb3"}},{"cell_type":"markdown","source":"Below is a class to employ the LabelBinarizer() function for multiple categorical columns at once. It returns a single binary array, and also has the self.classes_ variable that keeps track of which variables are stored in which columns.","metadata":{"_uuid":"732d66ea95ec9c887da8ecfb43be8566316f819c","_cell_guid":"68011737-a925-4e38-8604-9d78268dd0c5"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n\nclass MultiColBinarize(BaseEstimator, TransformerMixin):\n\t\"\"\" take a df with multiple categoricals\n\t\tone hot encode them all and return the numpy array\"\"\"\n\tdef __init__(self, alter_df= True):\n\t\tself.alter_df = alter_df\n\tdef fit(self, X, y=None):\n\t\t\"\"\"load the data in, initiate the binarizer for each column\"\"\"\n\t\tself.X = X\n\t\tself.cols_list = list(self.X.columns)\n\t\tself.binarizers = []\n\t\tfor i in self.cols_list:\n\t\t\tencoder = LabelBinarizer()\n\t\t\tencoder.fit(self.X[i])\n\t\t\tself.binarizers.append(encoder)\n\t\treturn self\n\tdef transform(self, X):\n\t\t\"\"\" for each of the columns, use the existing binarizer to make new cols \"\"\"\n\t\tself.X = X\n\t\tself.binarized_cols = self.binarizers[0].transform(self.X[self.cols_list[0]])\n\t\tself.classes_ = list(self.binarizers[0].classes_)\n\t\tfor i in range(1,len(self.cols_list)):\n\t\t\tbinarized_col = self.binarizers[i].transform(self.X[self.cols_list[i]])\n\t\t\tself.binarized_cols = np.concatenate((self.binarized_cols , binarized_col), axis = 1)\n\t\t\tself.classes_.extend(list(self.binarizers[i].classes_))\n\t\treturn self.binarized_cols\n","metadata":{"_uuid":"7f238e2cb951eb182f04f169a6fc0aa02febdb32","collapsed":true,"_cell_guid":"7dfacf98-e1ae-4ad4-9a59-c6f843e14642"}},{"cell_type":"markdown","source":"The numerical processing and categorical processing functions are then deployed on the respective data subsets using the following pipelines","metadata":{"_uuid":"8fc39b3361fd7f7b1751c0ff2b8020f2f3cf3e39","_cell_guid":"5c342df8-4e8f-4bc5-9dc0-6e1c664778be"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\nnum_pipeline = Pipeline([\n\t\t('selector', DataFrameSelector(num_attribs)),\n\t\t('imputer', Imputer(strategy=\"median\")),\n\t\t('std_scaler', StandardScaler()),\n\t])\n","metadata":{"_uuid":"8c0a6c8abc5cf8c73113e8b2dd181468c06f124b","collapsed":true,"_cell_guid":"a9abd909-beca-493f-bfe7-b26ab5750376"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n# select the categorical columns, binarize them \ncat_pipeline = Pipeline([\n\t\t('selector', DataFrameSelector(cat_attribs)),\n\t\t('label_binarizer', MultiColBinarize()),\n\t])\n\n","metadata":{"_uuid":"e15b018152250c57ce3d14ccf34e7cab858cb439","collapsed":true,"_cell_guid":"04bdbf8f-3d02-469b-b409-8df1063e247a"}},{"cell_type":"markdown","source":"The two pipelines are called on the train data, and the output is concatenated into a single array","metadata":{"_uuid":"906691f6703b9f9ccababa58af3016a75f184713","_cell_guid":"69ed16c6-d0cf-4605-8f9d-e5677e96076f"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\ntrain_num_processed = num_pipeline.fit_transform(train_x)\ntrain_cat_processed = cat_pipeline.fit_transform(train_x)\n\ntrain_x_clean =  np.concatenate((train_num_processed,train_cat_processed),axis=1)\n","metadata":{"_uuid":"79ddecef8de8d0277f5abe29e4926cf5838fcd82","collapsed":true,"_cell_guid":"6b402add-c25e-4524-8733-a34657bd9d16"}},{"cell_type":"markdown","source":"The test data is just transformed (not fit!), this is so we impute based on the training data, and so the binarized columns match across the datasets.","metadata":{"_uuid":"0fb6b05ff4167b970caa1ae470a609db2bcc6502","_cell_guid":"03fefdef-031f-4669-8970-62ac8010c56f"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\ntest_num_processed = num_pipeline.transform(test_x)\ntest_cat_processed = cat_pipeline.transform(test_x)\n\ntest_x_clean =  np.concatenate((test_num_processed,test_cat_processed),axis=1)\n\n","metadata":{"_uuid":"583618869d545ad6774221a6666ec4bbdbf18ee2","collapsed":true,"_cell_guid":"3e895478-5f6f-41fc-96a8-2dd04c8add5c"}},{"cell_type":"markdown","source":"Sanity check that the number of columns are the same for both","metadata":{"_uuid":"e2a230092f6726984866fa692bcfe273799c29ba","_cell_guid":"517a7c25-b6f2-4960-b0df-404eb6dc45ac"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\ntrain_x_clean.shape","metadata":{"_uuid":"f3f66e6ee8f1e05200c3098ee5d951d91cc868f2","collapsed":true,"_cell_guid":"96e6e6b6-fe8f-414a-a6a9-8c8b0ebacc92"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\ntest_x_clean.shape","metadata":{"_uuid":"c616b6c33e0ec9f62040172603633b95f30de811","collapsed":true,"_cell_guid":"08725fd7-71ce-4770-847b-4c79466e5555"}},{"cell_type":"markdown","source":"## Training the predictors\n\nA simple grid search of hyperparamaters is performed below to optimize the three models we are deploying\n\nThese are commented out for kaggle time restrictions. try them yourself!\n### support vector machine","metadata":{"_uuid":"481a79808014920142f942351f178372303d2f84","_cell_guid":"aa5d0a60-14ec-482c-990e-8e7daca94c3f"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\"\"\"\nsvm_reg = SVR(kernel=\"linear\")\n\n\nsvr_param_grid = [\n\t\t{'kernel': ['rbf','linear'], 'C': [1.0, 10., 100., 1000.0],\n\t\t'gamma': [0.01, 0.1,1.0]}\n\t]\n\n\nsvm_grid_search = GridSearchCV(svm_reg, svr_param_grid, cv=5,\n\t\t\t\t\t\tscoring='neg_mean_squared_error')\n\nsvm_grid_search.fit(train_x_clean, train_y)\n\nsvm_grid_search.best_params_\n\nsvm_grid_search.best_estimator_\n\ncvres = svm_grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\tprint(np.sqrt(-mean_score), params)\n\n\"\"\"\n","metadata":{"_uuid":"b50893bfad143d2bd091f15f2878dfdeeede058a","collapsed":true,"_cell_guid":"1e04fc20-7723-4ad8-bb0a-c34dfb32f25c"}},{"cell_type":"markdown","source":"### Random forest regression","metadata":{"_uuid":"0d4b7cb350b2a069e6e67b540ba43c073c2eda30","_cell_guid":"e51f8d9b-b42b-4db6-85ea-ad96843818cf"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\"\"\"\nforest_reg = RandomForestRegressor(random_state=42)\n\nrf_param_grid = [\n\t{'n_estimators': [3, 10, 30,100,300,1000], 'max_features': [2, 4, 6, 8]},\n\t{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \nrf_grid_search = GridSearchCV(forest_reg, rf_param_grid, cv=5,\n\t\t\t\t\t\t   scoring='neg_mean_squared_error')\nrf_grid_search.fit(train_x_clean, train_y)\n\n\nrf_grid_search.best_params_\nrf_grid_search.best_estimator_\n\ncvres = rf_grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\tprint(np.sqrt(-mean_score), params)\n\n\"\"\"\n\n","metadata":{"_uuid":"9d4657f7c26bcf3ab98ee2943d86bc94c3185269","collapsed":true,"_cell_guid":"bbce3d28-745d-421c-9121-f9ed40b69984"}},{"cell_type":"markdown","source":"### XG Boost","metadata":{"_uuid":"44d685279973868a01077aa32fc8011128c6e36d","_cell_guid":"e3efbff5-b2e9-4e75-bd11-ea1a5980a909"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\"\"\"\nXGBoost_reg = xgb.XGBRegressor()\n\n#note all the params below must be wrapped in lists\nxgb_param_grid  = [{'min_child_weight': [20,25,30], \n\t\t\t\t\t'learning_rate': [0.1, 0.2, 0.3], \n\t\t\t\t\t'colsample_bytree': [0.9], \n\t\t\t\t\t'max_depth': [5,6,7,8], \n\t\t\t\t\t'reg_lambda': [1.], \n\t\t\t\t\t'nthread': [-1], \n\t\t\t\t\t'n_estimators': [100,1000,2000],\n\t\t\t\t\t'early_stopping_rounds':50,\n\t\t\t\t\t'objective': ['reg:linear']}]\n\n\nxgb_grid_search = GridSearchCV(XGBoost_reg, xgb_param_grid, cv=5,\n\t\t\t\t\tscoring='neg_mean_squared_error', n_jobs=1)\n\nxgb_grid_search.fit(train_x_clean, train_y)\n\n\n\nxgb_grid_search.best_params_\n\nxgb_grid_search.best_estimator_\n\ncvres = xgb_grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\tprint(np.sqrt(-mean_score), params)\"\"\"","metadata":{"_uuid":"3a98711c93a4cb349cd1a1b7a00e8b185679de26","collapsed":true,"_cell_guid":"f1fa3cc2-6678-47d3-b73c-0050661b7465"}},{"cell_type":"markdown","source":"### test the above 3 models to see their predictive abilities","metadata":{"_uuid":"0be4fdc039195ab5080d6c3ffc313526eab5e668","_cell_guid":"610ea83e-e4fe-468a-a76e-a20013e16bb3"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n#SVM\nopt_svm_params = {'C': 1000.0, \n\t\t\t\t'gamma': 0.01, \n\t\t\t\t'kernel': 'linear'}\n\n#need the ** to unpack the dictonary so all the params don't get assigned to one\nopt_svm_reg = SVR(**opt_svm_params)\n\nopt_svm_reg.fit(train_x_clean, train_y)\n\n\ny1 = opt_svm_reg.predict(test_x_clean)\n\ny1_mse = mean_squared_error(test_y, y1)\ny1_rmse = np.sqrt(y1_mse)\ny1_rmse\n","metadata":{"_uuid":"2a7b87958a5e3079fbcf54f41c98f9b1dd00d747","collapsed":true,"_cell_guid":"fa5e7007-2043-43be-a3ca-e5552238c075"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n#RF\nopt_rf_params= {'max_features': 8, 'n_estimators': 100}\n\nopt_forest_reg = RandomForestRegressor(**opt_rf_params, random_state=42)\n\nopt_forest_reg.fit(train_x_clean, train_y)\n\ny2 = opt_forest_reg.predict(test_x_clean)\n\ny2_mse = mean_squared_error(test_y,y2 )\ny2_rmse = np.sqrt(y2_mse)\ny2_rmse","metadata":{"_uuid":"d48f149c48fff6fc65ee4e3e8583a6e5b1541fc6","collapsed":true,"_cell_guid":"fdd6f21c-1496-4a74-b676-89afecb9e231"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"#XGB\nopt_xgb_params = {'colsample_bytree': 0.9,\n\t\t\t\t'learning_rate': 0.1,\n\t\t\t\t'max_depth': 7,\n\t\t\t\t'min_child_weight': 30,\n\t\t\t\t'n_estimators': 1000,\n\t\t\t\t'nthread': -1,\n\t\t\t\t'objective': 'reg:linear',\n\t\t\t\t'reg_lambda': 1.0}\n\n\nopt_XGBoost_reg = xgb.XGBRegressor(**opt_xgb_params)\n\nopt_XGBoost_reg.fit(train_x_clean, train_y)\n\ny3 = opt_XGBoost_reg.predict(test_x_clean)\n\ny3_mse = mean_squared_error(test_y, y3)\ny3_rmse = np.sqrt(y3_mse)\ny3_rmse","metadata":{"_uuid":"c229f0530a4ea3c82cd5772a64d1b913f5c63faa","collapsed":true,"_cell_guid":"28ba2ea8-e836-4cdd-9bc1-aa71eab55149"}},{"cell_type":"markdown","source":"# combine optimal models into a single model\n\nI developed a skeleton class that can be altered to combine multiple predictors into a single sklearn class that spits\nout predicted values. Below this is used on the three models we have developed here.\nThe class has a tuning param that changes the weights of the models and uses cross validation function to get the rmse scores. Running this class for multiple weights can provide an approximation of the best model weights to use in the combined predictions.\n","metadata":{"_uuid":"d47e67881858143fef9a8e1e4eaae3d3461af5ac","_cell_guid":"18ce52d3-b075-4be4-abc6-ee4aac562ef7"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\nclass ensemble_predictor(BaseEstimator, TransformerMixin):\n\t\"\"\" take in a dataset and train it with three models,\n\t\tcombining the outputs to make predictions\"\"\"\n\tdef __init__(self, weights= { 'xgb': 0.33, 'rf': 0.33, 'svm' : 0.34}):\n\t\tself.weights = weights\n\t\tself.opt_xgb_params = {'colsample_bytree': 0.9,\n\t\t\t\t\t'learning_rate': 0.1,\n\t\t\t\t\t'max_depth': 7,\n\t\t\t\t\t'min_child_weight': 30,\n\t\t\t\t\t'nthread': -1,\n\t\t\t\t\t'objective': 'reg:linear',\n\t\t\t\t\t'reg_lambda': 1.0}\n\t\tself.opt_svm_params = {'C': 1000.0, \n\t\t\t\t'gamma': 0.01, \n\t\t\t\t'kernel': 'linear'}\n\t\tself.opt_rf_params= {'max_features': 8, 'n_estimators': 100}\n\n\tdef fit(self, X, y):\n\t\t\"\"\"load the data in, initiate the models\"\"\"\n\t\tself.X = X\n\t\tself.y = y\n\t\tself.opt_XGBoost_reg = xgb.XGBRegressor(**self.opt_xgb_params)\n\t\tself.opt_forest_reg = RandomForestRegressor(**self.opt_rf_params)\n\t\tself.opt_svm_reg = SVR(**self.opt_svm_params)\n\t\t\"\"\" fit the models \"\"\"\n\t\tself.opt_XGBoost_reg.fit(self.X ,self.y)\n\t\tself.opt_forest_reg.fit(self.X ,self.y)\n\t\tself.opt_svm_reg.fit(self.X ,self.y)\n\tdef predict(self, X2):\n\t\t\"\"\" make the predictions for the models, combine based on weights \"\"\"\n\t\tself.y_xgb = self.opt_XGBoost_reg.predict(X2)\n\t\tself.y_rf = self.opt_forest_reg.predict(X2)\n\t\tself.y_svm = self.opt_svm_reg.predict(X2)\n\t\t\"\"\" multiply the predictions by their weights, return optimal \"\"\"\n\t\tself.prediction = self.y_xgb * self.weights['xgb'] \\\n\t\t\t\t\t\t+ self.y_rf * self.weights['rf'] \\\n\t\t\t\t\t\t+ self.y_svm * self.weights['svm']\n\t\treturn self.prediction\n","metadata":{"_uuid":"7c6b8ca5c9d2b244e998b13c5eae9664105b073b","collapsed":true,"_cell_guid":"4b14dd54-30fb-4e1e-b517-95fade6ff5ba"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"weight_variants = [\n{ 'xgb': 0.33, 'rf': 0.33, 'svm' : 0.34},\n{ 'xgb': 0.9, 'rf': 0.05, 'svm' : 0.05},\n{ 'xgb': 0.8, 'rf': 0.1, 'svm' : 0.1},\n{ 'xgb': 0.5, 'rf': 0.3, 'svm' : 0.2},\n{ 'xgb': 0.3, 'rf': 0.2, 'svm' : 0.5},\n{ 'xgb': 0.3, 'rf': 0.5, 'svm' : 0.2}\n]\n","metadata":{"_uuid":"664bdd4eaeec19757aea9b7724e932971443d96e","collapsed":true,"_cell_guid":"f19008f8-1726-40d4-8dde-22afff1de5e2"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\n#determine the optimal weights for the different models via cross validation\nw_results = []\nfor params in weight_variants:\n    model = ensemble_predictor(weights = params)\n    ensemble_score = cross_val_score(model, train_x_clean, train_y,\n                                    scoring=\"neg_mean_squared_error\", cv=5)\n    ensemble_rmse = np.sqrt(-ensemble_score)\n    print('%s\\t %s'% (params, ensemble_rmse.mean()))\n    w_results.append(ensemble_rmse.mean())\n","metadata":{"_uuid":"6f865dbe4febfc81d0a0966652d28cafa9fac243","collapsed":true,"_cell_guid":"495fb28b-88c3-43eb-a631-a7e6154d5d22"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\nimport matplotlib.pyplot as plt\n\ny_pos = np.arange(len(w_results))\n\nweight_variant_names = [\"{ 'xgb': 0.33, 'rf': 0.33, 'svm' : 0.34}\",\n                         \"{ 'xgb': 0.9, 'rf': 0.05, 'svm' : 0.05}\",\n                         \"{ 'xgb': 0.8, 'rf': 0.1, 'svm' : 0.1}\",\n                         \"{ 'xgb': 0.5, 'rf': 0.3, 'svm' : 0.2}\",\n                         \"{ 'xgb': 0.3, 'rf': 0.2, 'svm' : 0.5}\",\n                         \"{ 'xgb': 0.3, 'rf': 0.5, 'svm' : 0.2}\"]\nplt.bar(y_pos, w_results, align='center', alpha=0.5)\nplt.xticks(y_pos, weight_variant_names, rotation=90)\nplt.ylabel('rmse')\nplt.ylim(1300000,1450000)\nplt.title('RMSE of different ensemble model weights')\n \nplt.show()","metadata":{"_uuid":"a2768c99ec227b040f98a81a27e42771f5aae5ef","collapsed":true,"_cell_guid":"f9c6d1f2-45a3-48f9-8c69-e2196fb781ff"}},{"cell_type":"markdown","source":"winner: {'xgb': 0.8, 'rf': 0.1, 'svm': 0.1} 1322950.1668\nTry again with the new weight variants, tuned in towards the optimal numbers\n","metadata":{"_uuid":"41648184f5055032133216c46e229e0d7eb14f60","_cell_guid":"41b95514-45aa-4c93-a535-060f714427a3"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"weights ={'xgb': 0.8, 'rf': 0.15, 'svm': 0.05} #SUB IN OPTIMAL WEIGHTS","metadata":{"_uuid":"2d8b48a3034cc63b00110d535543171fc933185f","collapsed":true,"_cell_guid":"04c09dc2-9b4f-46f1-9901-a7bba1a0453b"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"opt_model = ensemble_predictor(weights)\nopt_model.fit(train_x_clean, train_y)\nfinal_predictions = opt_model.predict(test_x_clean)","metadata":{"_uuid":"eabf6f4dabf4ab4b32682a0f68bdcbb74bc23658","collapsed":true,"_cell_guid":"d72ab192-fd02-4423-be86-33e635691319"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"opt_mean_squared_error = mean_squared_error(test_y,final_predictions)\n\nopt_rmse = np.sqrt(opt_mean_squared_error)\nopt_rmse","metadata":{"_uuid":"4be5336606f868dca82bd0356fdb1d95acd03338","collapsed":true,"_cell_guid":"46bd67e9-9ca8-4fcc-9196-3007ca9652f5"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"meadian_guess = [np.median(test_y) for x in test_y]\nmeadian_guess\n\n\nmedian_mse= mean_squared_error(test_y,meadian_guess)\n\nmedian_rmse = np.sqrt(median_mse)\nmedian_rmse","metadata":{"_uuid":"256813899051ff9306a6c7729d24803042430939","collapsed":true,"_cell_guid":"681cf0d3-25c9-4b09-9e80-dccd7237fe01"}},{"cell_type":"markdown","source":"When I ran this locall the ensemble model was about 1.3 million dollars closer on average than guessing by just the median alone.\n\nThis mixed model was better then previous iterations, when we ran Random Forest regression the model was off by an average of \\$1,578,497 and with XGBoost alone the model was only slightly improved at \\$1,574,073 When we combined models here, we see that we are about \\$25,000 closer on average, which is a slight improvement, but an improvement nonetheless! Any suggestions on ways to further tweak the predictive model are welcomed in the comments.","metadata":{"_uuid":"0ec8442b6413ae1959e0b85c90ee4ba4a0d4e546","_cell_guid":"ece97870-4c8d-49a8-9bec-1132cd9577fb"}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"4c7a678f610697bc2a3ea5977bd64e551eb019fc","_cell_guid":"95bda7a9-f55b-451d-b729-5e63022bc434"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"name":"python","version":"3.6.1","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":1}