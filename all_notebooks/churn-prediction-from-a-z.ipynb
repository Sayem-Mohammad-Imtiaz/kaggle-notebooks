{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS"},{"metadata":{},"cell_type":"markdown","source":"## GENERAL INFO"},{"metadata":{"trusted":true},"cell_type":"code","source":"telco = pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ntelco.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telco.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 7043 rows of customer data.\n* Each data has 19 features (excluding the CustomerID), 3 of them is numerical and all others are categorical variable.\n* Features can be divided by 3 categories:\n    * Customer demographic info – gender, senior citizen, partner and dependent\n    * Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, streaming TV and streaming movies\n    * Customer account information – tenure(how long they’ve been a customer), contract, payment method, paperless billing, monthly charges, and total charges\n* Our target is the Churn column which shows whether the customer left within the last month."},{"metadata":{},"cell_type":"markdown","source":"## MISSING DATA"},{"metadata":{},"cell_type":"markdown","source":"* It seems this dataset doesn't have any missing values. But there is a problem in **TotalCharges** column, the values are numeric but it is of type object. If we look deep, we can see that there are 11 rows that are empty, so we will drop these 11 rows and convert this column to float."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(telco[telco['TotalCharges'] == \" \"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"telco.drop(telco[telco['TotalCharges'] == \" \"].index, inplace=True)\ntelco['TotalCharges'] = telco['TotalCharges'].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the missing value counts for each column\nmissing = telco.isnull().sum()\n# Show the number of columns that have missing values\nmissing[missing > 0].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TARGET VARIABLE (CHURN)"},{"metadata":{},"cell_type":"markdown","source":"* For simplicity, let's call the customers who left within the last month as **Churning Customers** and others as **Non-Churning Customers**\n* Let's look at Churning and Non-Churning Customer counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=telco, x=\"Churn\")\nplt.title('Churn Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Churn Percentage\")\nprint(telco['Churn'].value_counts(normalize=True)* 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Only 26.5% of customers have left within the last month which shows us that we are dealing with **imbalanced data**. So we should be aware of this situation especially in choosing right evaluation metric, i.e. Accuracy may not be sufficient metric and may even mislead us."},{"metadata":{},"cell_type":"markdown","source":"## RELATIONSHIP BETWEEN TARGET VARIABLE AND FEATURES"},{"metadata":{},"cell_type":"markdown","source":"### NUMERICAL FEATURES"},{"metadata":{},"cell_type":"markdown","source":"* Let's look at the probability density distribution of these features."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\nfig, ax = plt.subplots(figsize=(15, 9))\nax = [plt.subplot(221), plt.subplot(222), plt.subplot(223)]\nplt.subplots_adjust(hspace=0.3)\nfor i in range(0, len(ax)):\n    ax[i].set_title(\"Distribution of \" + num_features[i] + \" by Churn\")\n    ax[i].set_ylabel('Density')\n    ax[i].set_xlabel(num_features[i])\n    sns.kdeplot(telco[telco['Churn'] == 'Yes'][num_features[i]], color= 'red', label= 'Churn: Yes', ax=ax[i])\n    sns.kdeplot(telco[telco['Churn'] == 'No'][num_features[i]], color= 'blue', label= 'Churn: No', ax=ax[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBSERVATIONS\n* Most of the Churning Customers leaves in early months of the contract.\n* The churning rate increases as the monthly charges increase.\n* Totalcharge distribution is the result of the 2 observations above, i.e. it is the result of Tenure and monthly charges."},{"metadata":{},"cell_type":"markdown","source":"### CATEGORICAL FEATURES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the churn percentage for each category of given feature \ndef plotFeature(feature, xoffset, yoffset):\n    df = telco.groupby(feature)[\"Churn\"].value_counts().to_frame()\n    df = df.rename({\"Churn\": \"Count\"}, axis=1).reset_index()\n    df[\"Percentage\"] = df[\"Count\"]/len(telco)*100\n    \n    ax = sns.barplot(x=feature, y=\"Percentage\", hue=\"Churn\", data=df)\n    ax.set_title(\"Churn percentages for each \" + feature)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width()/2 - xoffset,\n                height - yoffset,\n                '{:1.1f}%'.format(height),\n                weight='bold', color='white', size = 12) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GENDER - SENIORCITIZEN"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13,5))\nplt.subplot(121)\nplotFeature(\"gender\", 0.12, 4.5)\nplt.subplot(122)\nplotFeature(\"SeniorCitizen\", 0.11, 4.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Gender distribution is homogenious and their churn rate is also same, so it is not an indicative of churn.\n* Although Senior citizens are only the 16% of all customers their churn rate is much higher than non-seniors."},{"metadata":{},"cell_type":"markdown","source":"#### PARTNER - DEPENDENTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13,5))\nplt.subplot(121)\nplotFeature(\"Partner\", 0.12, 4)\nplt.subplot(122)\nplotFeature(\"Dependents\", 0.12, 3.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Customers that doesn't have partners have moderately higher churn rate\n* Customers without dependents have much higher churn rate"},{"metadata":{},"cell_type":"markdown","source":"#### PHONE, INTERNET SERVICE, MULTIPLE LINES"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,5))\nplt.subplot(131)\nplotFeature(\"PhoneService\", 0.12, 3)\nplt.subplot(132)\nplotFeature(\"InternetService\", 0.18, 2)\nplt.subplot(133)\nplotFeature(\"MultipleLines\", 0.18, 2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Customers without internet have a low churn rate and customers with fiberoptic internet have high churn rate\n* Having a Phone Service or Multiple Lines doesn't seem to influence churn"},{"metadata":{},"cell_type":"markdown","source":"#### ONLINE SECURITY, ONLINE BACKUP, DEVICE PROTECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,5))\nplt.subplot(131)\nplotFeature(\"OnlineSecurity\", 0.18, 2)\nplt.subplot(132)\nplotFeature(\"OnlineBackup\", 0.18, 2)\nplt.subplot(133)\nplotFeature(\"DeviceProtection\", 0.18, 2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Customers that doesn't have these 3 services (Online Security, Online Backup and Device Protection) are morely likely to churn"},{"metadata":{},"cell_type":"markdown","source":"#### TECH SUPPORT, STREAMING TV, STREAMING MOVIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,5))\nplt.subplot(131)\nplotFeature(\"TechSupport\", 0.18, 1.5)\nplt.subplot(132)\nplotFeature(\"StreamingTV\", 0.18, 1.5)\nplt.subplot(133)\nplotFeature(\"StreamingMovies\", 0.18, 1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Customers that doesn't have a tech support are more likely to churn\n* Streaming service is not predictive for churn"},{"metadata":{},"cell_type":"markdown","source":"#### CONTRACT - PAPERLESS BILLING - PAYMENT METHOD"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,10))\nplt.subplots_adjust(hspace=0.2)\nplt.subplot(221)\nplotFeature(\"Contract\", 0.16, 1.5)\nplt.subplot(222)\nplotFeature(\"PaperlessBilling\", 0.1, 2)\nplt.subplot(223)\nplotFeature(\"PaymentMethod\", 0.18, 1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Intuitively short term contracts have much higher churn rate\n* Customers with paperless billing have high churn rate\n* Payment method of electronic check shows much higher churn rate than others"},{"metadata":{},"cell_type":"markdown","source":"### OUTLIER CHECK"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the IQR for each numerical feature\nfor i in num_features:\n    Q1 = telco[i].quantile(0.25)\n    Q3 = telco[i].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_limit = Q1 - 1.5*IQR\n    upper_limit = Q3 + 1.5*IQR\n    print(((telco[i] < lower_limit) | (telco[i] > upper_limit)).any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Using IQR method, we didn't detect any outliers in numerical features"},{"metadata":{},"cell_type":"markdown","source":"### FEATURE ENGINEERING"},{"metadata":{},"cell_type":"markdown","source":"* Drop the ID column as we don't need it"},{"metadata":{"trusted":true},"cell_type":"code","source":"telco.drop('customerID', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Generate Dummy variables from categorial features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dummy variable for gender but use only one of the column\ntelco = pd.get_dummies(telco, columns=['gender'], drop_first=True)\n# For the features below, we can map Yes/No to binary variables of 0/1\nvars1 = ['Churn', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\nmap1 = {'No': 0, 'Yes': 1}\nfor i in vars1:\n    telco[i] = telco[i].map(map1).astype('int')\n# The features below have more than 2 category so we used one-hot encoding\nvars2 = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n         'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\ntelco = pd.get_dummies(telco, columns=vars2)\ntelco.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Apply Scaling to numerical features as some of the Machine Learning models need it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntelco[num_features] = scaler.fit_transform(telco[num_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Correlation of Churn with all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.title(\"Correlation of Features with the Churn\")\ntelco.corr()['Churn'].drop(index='Churn').sort_values(ascending=False).plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we observe above, the monthly contracts, absence of some services (online security, techsupport) are positively correlated with Churn. On the other hand, tenure, yearly contracts and absence of internet service are negatively correlated."},{"metadata":{},"cell_type":"markdown","source":"# MODELING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score, precision_recall_fscore_support\nfrom sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = telco.drop([\"Churn\"], axis=1)\ny = telco[\"Churn\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applied stratified sampling since our data is imbalanced\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate a dataframe that keeps track of the model results\nall_scores = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HELPER FUNCTIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function takes the model, features and actual target values, makes prediction and then prints different metric scores\ndef model_predictions(name, model, X, y, title):\n    y_pred = model.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    precision, recall, f1, support = precision_recall_fscore_support(y, y_pred, average='binary')\n    scores = pd.DataFrame(columns=['Accuracy','Precision','Recall','F1'])\n    scores.loc[name] = [accuracy, precision, recall, f1]\n    print('----------------------------------------')\n    print(title)\n    print(scores)\n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the Confusion matrix with/without normalization + ROC Curve + Precision-Recall Curve\ndef model_plots(model, X, y, title):\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    fig.suptitle(title, fontsize=18)\n    plt.subplots_adjust(hspace=0.3)\n    axes[0,0].set_title('Confusion Matrix')\n    plot_confusion_matrix(model, X, y, display_labels=['No Churn','Churn'], ax=axes[0,0])\n    axes[0,1].set_title('Normalized Confusion Matrix')\n    plot_confusion_matrix(model, X, y, normalize='true', display_labels=['No Churn','Churn'], ax=axes[0,1])\n    axes[1,0].set_title('ROC Curve')\n    plot_roc_curve(model, X, y, name='Log.Reg.', ax=axes[1,0])\n    axes[1,1].set_title('Precision-Recall Curve')\n    plot_precision_recall_curve(model, X, y, name='Log.Reg.', ax=axes[1,1])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function takes the model and it's related parameters as input and returns the best parameters \n# from given set using Grid Search Algorithm (hyperparameter tuning) \ndef hyperparam_gridcv(name, model, params):\n    gsearch = GridSearchCV(estimator=model, \n                           param_grid=params, \n                           scoring='f1',\n                           cv=5,\n                           n_jobs = -1)\n    gsearch.fit(X_train, y_train)\n    print(name, \" - Best Parameters: \", gsearch.best_params_)\n    #return gsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function takes the model and it's related parameters as input and returns the best parameters \n# from given set using Randomized Search Algorithm\ndef hyperparam_randcv(name, model, params):\n    gsearch = RandomizedSearchCV(estimator=model, \n                                 param_distributions=params, \n                                 n_iter = 100,\n                                 scoring='f1',\n                                 cv=5,\n                                 n_jobs = -1)\n    gsearch.fit(X_train, y_train)\n    print(name, \" - Best Parameters: \", gsearch.best_params_)\n    #return gsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOGISTIC REGRESSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the base model\nlogreg = LogisticRegression(max_iter=1000, random_state=10)\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the training and test set's scores\nmodel_predictions('LOGREG', logreg, X_train, y_train, 'Logistic Reg. Training Set Scores')\nmodel_predictions('LOGREG', logreg, X_test, y_test, 'Logistic Reg. Test Set Scores')\n# Plot the confusion matrix, ROC curve and PRC curve\nmodel_plots(logreg, X_test, y_test, \"Logistic Regression Results (Base Model)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As our data is imbalanced, we will focus on the metrics other than the accuracy. \n* Looking at the results, base model is very poor at predicting Churning Customers as it has a very low precision, recall and f1 score. Looking at the confusion matrix, the model predicts almost half of the Churning customers as wrong and AUC of the Precision-Recall curve is very low."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning (it is commented as it takes time)\nparams_logreg = {'penalty':['l1','l2'], 'C':np.arange(0.1, 3, 0.1)}\n#hyperparam_gridcv('Logistic Regression', logreg, params_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(max_iter=1000, C=2.7, penalty='l2', random_state=10)\nlogreg.fit(X_train, y_train)\n\nlogreg_results = model_predictions('LOGREG', logreg, X_test, y_test, 'Logistic Reg. Test Set Scores')\nall_scores = all_scores.append(logreg_results)\n\nmodel_plots(logreg, X_test, y_test, \"Logistic Regression Results (Tuned Model)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is a slight improvement on the predictions but not that much. "},{"metadata":{},"cell_type":"markdown","source":"### KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the base model\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the training and test set's scores\nmodel_predictions('KNN', knn, X_train, y_train, 'KNN Training Set Scores')\nmodel_predictions('KNN', knn, X_test, y_test, 'KNN Test Set Scores')\n# Plot the confusion matrix, ROC curve and PRC curve\nmodel_plots(knn, X_test, y_test, \"KNN Results (Base Model)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning\nparams_knn = {'n_neighbors':np.arange(3, 25, 1), \n              'metric':['euclidean','manhattan','minkowski']}\n#hyperparam_gridcv('K-Nearest Neighbors', knn, params_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=21, metric='manhattan')\nknn.fit(X_train, y_train)\n\nknn_results = model_predictions('KNN', knn, X_test, y_test, 'KNN Test Set Scores')\nall_scores = all_scores.append(knn_results)\n\nmodel_plots(knn, X_test, y_test, \"KNN Results (Tuned Model)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Hyperparameter tuning improved results of the base model but it is not satisfactory and below the results of Logistic regression. Our model is good at predicting non-churning customers but still poor at churning customers."},{"metadata":{},"cell_type":"markdown","source":"### RANDOM FOREST"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the base model\nrf = RandomForestClassifier(random_state=10)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the training and test set's scores\nmodel_predictions('RND.FOR.', rf, X_train, y_train, 'Random Forest Training Set Scores')\nmodel_predictions('RND.FOR.', rf, X_test, y_test, 'Random Forest Test Set Scores')\n# Plot the confusion matrix, ROC curve and PRC curve\nmodel_plots(rf, X_test, y_test, \"Random Forest Results (Base Model)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning\nparams_rf = {'n_estimators':[100, 500, 1000, 1500],\n             'max_depth':np.append(np.arange(10, 60, 10), None),\n             'class_weight':[None, 'balanced', 'balanced_subsample'],\n             'min_samples_split': [2, 5, 10],\n             'min_samples_leaf': [1, 2, 5]\n            }\n\n# hyperparam_randcv('Random Forest', rf, params_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, \n                            max_depth=10, \n                            max_features='auto', \n                            min_samples_split=5,\n                            min_samples_leaf=2, \n                            class_weight='balanced_subsample',\n                            random_state=10)\nrf.fit(X_train, y_train)\n\nrf_results = model_predictions('RND.FOR.', rf, X_test, y_test, 'Random Forest Test Set Scores')\nall_scores = all_scores.append(rf_results)\n\nmodel_plots(rf, X_test, y_test, \"Random Forest Results (Tuned Model)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = pd.Series(rf.feature_importances_*100, index=X.columns)\nimp.sort_values()[-10:].plot(kind='barh', figsize=(10,6))\nplt.title('Top 10 Feature Importances of Base Random Forest Model')\nplt.xlabel('Percentage Contribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### OBSERVATIONS\n* Adjusted model performed slightly better than the base and previous two models in terms of general scores. \n\n\n* The most significant improvement is in predicting churning customers. Previous model can predict only half of the churning customers but this model predicts 72% of them correctly.\n\n\n* From the plot, tenure,total charges, monthly contracts and monthly charges are the most important predictor variables to predict churn which is similar to our observations made before."},{"metadata":{},"cell_type":"markdown","source":"### SUPPORT VECTOR MACHINES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the base model\nsvm = SVC(random_state=10)\nsvm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the training and test set's scores\nmodel_predictions('SVM', svm, X_train, y_train, 'SVM Training Set Scores')\nmodel_predictions('SVM', svm, X_test, y_test, 'SVM Test Set Scores')\n# Plot the confusion matrix, ROC curve and PRC curve\nmodel_plots(svm, X_test, y_test, \"SVM Results (Base Model)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning\nparams_svm = {'C':np.arange(0.1, 3, 0.1), 'kernel':['linear', 'poly', 'rbf']}\n#hyperparam_gridcv('SVM', svm, params_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(C=0.3,\n          kernel='linear',\n          random_state=10)\nsvm.fit(X_train, y_train)\n\nsvm_results = model_predictions('SVM', svm, X_test, y_test, 'SVM Test Set Scores')\nall_scores = all_scores.append(svm_results)\n\nmodel_plots(svm, X_test, y_test, \"SVM Results (Tuned Model)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The tuned model has slightly better results than base model but overall it is poor at predicting churning customers like the logistic regression and knn model."},{"metadata":{},"cell_type":"markdown","source":"### XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the base model\nxgb = XGBClassifier(random_state=10)\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the training and test set's scores\nmodel_predictions('XGB', xgb, X_train, y_train, 'XGB Training Set Scores')\nmodel_predictions('XGB', xgb, X_test, y_test, 'XGB Test Set Scores')\n# Plot the confusion matrix, ROC curve and PRC curve\nmodel_plots(xgb, X_test, y_test, \"XGB Results (Base Model)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning\nparams_xgb = {'learning_rate': [0.01, 0.05, 0.1],\n              'max_depth': [3, 5, 7, 10], \n              'min_child_weight': [1, 5, 10],\n              'subsample': [0.5, 0.7, 0.9],\n              'colsample_bytree': [0.6, 0.8, 1.0],\n              'gamma': [0, 0.05, 0.1],\n              'scale_pos_weight':[1,2,3]}\n#hyperparam_randcv('XGB', xgb, params_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(n_estimators=100,\n                    learning_rate=0.05,\n                    max_depth=5,\n                    min_child_weight=10,\n                    subsample=0.7,\n                    colsample_bytree=0.8,\n                    gamma=0.05,\n                    scale_pos_weight=2,\n                    random_state=10)\nxgb.fit(X_train, y_train)\n\nxgb_results = model_predictions('XGB', xgb, X_test, y_test, 'XGB Test Set Scores')\nall_scores = all_scores.append(xgb_results)\n\nmodel_plots(xgb, X_test, y_test, \"XGB Results (Tuned Model)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This model both has a better accuracy and also predicting the churning customers better than previous models."},{"metadata":{},"cell_type":"markdown","source":"### LIGHT GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the base model\nlgbm = LGBMClassifier()\nlgbm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the training and test set's scores\nmodel_predictions('LGBM', lgbm, X_train, y_train, 'LGBM Training Set Scores')\nmodel_predictions('LGBM', lgbm, X_test, y_test, 'LGBM Test Set Scores')\n# Plot the confusion matrix, ROC curve and PRC curve\nmodel_plots(lgbm, X_test, y_test, \"LGBM Results (Base Model)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning\nparams_lgbm = {'learning_rate': [0.01, 0.05, 0.1],\n               'n_estimators' : [100, 1000, 2000],\n               'max_depth': [-1, 3, 5, 7],\n               'min_child_weight': [1, 10, 100],\n               'subsample': [0.5, 0.7, 0.9],\n               'colsample_bytree': [0.6, 0.8, 1.0],\n               'reg_alpha': [0, 1, 5, 10],\n               'scale_pos_weight':[1, 1.5, 2]}\n#hyperparam_randcv('LGBM', lgbm, params_lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(n_estimators=2000,\n                    learning_rate=0.05,\n                    min_child_weight=100,\n                    max_depth=-1,\n                    subsample=0.9,\n                    colsample_bytree=1,\n                    scale_pos_weight=2,\n                    reg_alpha=5,\n                    random_state=10)\nlgbm.fit(X_train, y_train)\n\nlgbm_results = model_predictions('LGBM', lgbm, X_test, y_test, 'LGBM Test Set Scores')\nall_scores = all_scores.append(lgbm_results)\n\nmodel_plots(lgbm, X_test, y_test, \"LGBM Results (Tuned Model)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The results of LGBM is similar to XGBoost."},{"metadata":{},"cell_type":"markdown","source":"# RESULTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_scores.sort_values(by='F1', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 6 Classification Models are selected, base model is built for each and then these models are improved by hyperparameter tuning.\n\n\n* Since data is mildly imbalanced (has a churning rate of 26%), most of the models predicted non-churning customers well but couldn't perform same at predicting churning customers. \n\n\n* To avoid imbalance problem,\n    * Stratified Sampling is applied while splitting\n    * Relevent parameters (giving weight to classes) are used while hyperparameter tuning\n    \n    \n* While taking Accuracy score into consideration, F1 score is chosen as the first performance evaluation metric since our data is imbalanced. There was a trade-off between the two, if i tried to increase accuracy of the model, it gave unacceptible low prediction of churning customers, on the other hand if i give weight to churning customer prediction, accuracy score and non-churning customer prediction decreased much.\n\n\n* In conclusion, the winner of this classification problem is **Light GBM Model** as it has the best F1 score and a good accuracy."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}