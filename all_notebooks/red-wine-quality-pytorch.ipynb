{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read the csv file**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find number of columns and rows**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It means there are 1599 rows and 12 columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now plot graph to see it's distribution and property**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_figure(index,column):\n    plt.subplot(6,2,index)\n    plt.title(column)\n    plt.plot(df[column])\n    \nplt.figure(figsize=(10,10))\n\nfor index , column in enumerate(df.columns):\n    if index+1<=len(df.columns):\n        plot_figure(index+1, column)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check datatypes of dataframe's columns**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking if there is any missing value exists**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As every column name returned false it means that there is not any null value.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**check correlation between features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n#correlation matrix\ncorrmat = df.corr()\nk = 12 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'quality')['quality'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1)\nplt.figure(figsize=(8,8))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have considered highly correlated if correaltion value is >0.7 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_cols = list(df.columns)[:-1]\ninput_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_cols = ['quality']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convert dataframe to numpy arrays**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataframe_to_arrays(df):\n    # Make a copy of the original dataframe\n    df1 = df.copy(deep=True)\n    # Extract input & outupts as numpy arrays\n    inputs_array = df1[input_cols].to_numpy()\n    targets_array = df1[output_cols].to_numpy()\n    return inputs_array, targets_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs_array, targets_array = dataframe_to_arrays(df)\ninputs_array, targets_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs_array.shape,targets_array.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convert numpy array to torch tensor**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ninputs = torch.Tensor(inputs_array)\ntargets = torch.Tensor(targets_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a TensorDataset. **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset, random_split\ndataset = TensorDataset(inputs, targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the datasets into train ,validation and test datasets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_rows = len(df)\nval_percent = 0.01 # between 0.1 and 0.2\nval_size = int(num_rows * val_percent)\ntrain_size = num_rows - val_size\n\n\ntrain_df, val_df = random_split(dataset, [train_size, val_size]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pick a batch size for data loader**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_df, batch_size, shuffle=True)\nval_loader = DataLoader(val_df, batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Model skeleton**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = len(input_cols)\noutput_size = len(output_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nclass WineModel(nn.Module):\n    def __init__(self):\n        super().__init__()     \n        self.linear = nn.Linear(input_size, output_size) # fill this (hint: use input_size & output_size defined above)\n        #model initialized with random weight\n        \n    def forward(self, xb):\n        out = self.linear(xb)             # batch wise forwarding\n        return out\n    \n    def training_step(self, batch):\n        inputs, targets = batch \n        # Generate predictions\n        out = self(inputs)         \n        # Calcuate loss\n        loss = F.l1_loss(out, targets)  # batch wise training step and loss\n        return loss\n    \n    def validation_step(self, batch):\n        inputs, targets = batch\n        # Generate predictions\n        out = self(inputs)\n        # Calculate loss\n        loss =F.l1_loss(out, targets)       # batch wise validation and loss    \n        return {'val_loss': loss.detach()}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine val losses of all batches as average\n        return {'val_loss': epoch_loss.item()}\n    \n    def epoch_end(self, epoch, result, num_epochs):\n        # Print result every 20th epoch\n        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model =  WineModel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result, epochs)\n        history.append(result)  #appends total validation loss of whole validation set epoch wise\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Use the evaluate function to calculate the loss on the validation set before training.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision\nimport torch.nn as nn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torchvision.datasets.utils import download_url\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = evaluate(model,val_loader) # Use the the evaluate function\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train the model 4-5 times with different learning rates & for different number of epochs to see what works**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nlr = 1e-2\nhistory1 = fit(epochs, lr, model, train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nlr = 1e-3\nhistory2 = fit(epochs, lr, model, train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nlr = 1e-4\nhistory3 = fit(epochs, lr, model, train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nlr = 1e-5\nhistory3 = fit(epochs, lr, model, train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1000\nlr = 1e-6\nhistory3 = fit(epochs, lr, model, train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now calculate final validation loss **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loss = evaluate(model,val_loader)\nval_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Make predictions using the trained model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_single(input, target, model):\n    inputs = input.unsqueeze(0) \n    predictions = model(inputs)\n    prediction = predictions[0].detach()\n    print(\"Input:\", input)\n    print(\"Target:\", target)\n    print(\"Prediction:\", prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: I am doing predictions for validation set. But ideally you should seperate some of datasaets for test. **\n\nHere it has been done for learning purpose only.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input, target = val_df[0]\npredict_single(input, target, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input, target = val_df[10]\npredict_single(input, target, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input, target = val_df[5]\npredict_single(input, target, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hoorrah..... It's performing well .<br>\nAnd this is how we trained our first pytorch model with linear regression on **Wine quality dataset.**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}