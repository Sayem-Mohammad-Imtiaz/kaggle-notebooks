{"cells":[{"metadata":{},"cell_type":"markdown","source":"## BIGMART SALES \n\nBigmart is a big supermarket chain , with stores all around the country, The management of the shop had set out a challenge to all Data Scientist to help create a model thatcan predict the **sales per product** for each store.\n\nBreakdown of the Data(Goal):\nThis is a supervised machine learning problem with a target label as: **(Item_Outlet_Sales)**\nAlso we are predicting the sale price, it becomes the **regression task.**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom scipy import stats\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Importing Datasets\ntrain_data =  pd.read_csv('../input/big-mart-sales-prediction/train_v9rqX0R.csv')\ntest_data = pd.read_csv('../input/big-mart-sales-prediction/test_AbJTz2l.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['source'] = 'train'\ntest_data['source'] = 'test'\ndf = pd.concat([train_data,test_data], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train_data.describe().columns:\n    sns.distplot(train_data[i].dropna())\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train_data.describe().columns:\n    sns.boxplot(train_data[i].dropna())\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(train_data.Item_Type)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.Item_Type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of the Outlet_Size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(train_data.Outlet_Size)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of the Outlet_Location_Type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(train_data.Outlet_Location_Type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.Outlet_Location_Type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of the Outlet_Type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(train_data.Outlet_Type)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.Outlet_Type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.xlabel(\"Item_Weight\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Itam Weight and Item Outlet Sales\")\nsns.scatterplot(x='Item_Weight', y='Item_Outlet_Sales', hue='Item_Type',size='Item_Weight',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,9))\nplt.xlabel(\"Item_Visibility\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Item Visibility and Item Outlet Sales\",fontsize=15)\nsns.scatterplot(x=\"Item_Visibility\", y=\"Item_Outlet_Sales\", hue=\"Item_Type\", size= 'Item_Weight',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nplt.xlabel(\"Item_visibility\")\nplt.ylabel(\"Maximum Retail Price\")\nplt.title(\"Item_visibility and Maximum Retail Price\")\nplt.plot(train_data.Item_Visibility, train_data.Item_MRP, \".\", alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Outlet_Type_pivot = train_data.pivot_table(index='Outlet_Type',values='Item_Outlet_Sales', aggfunc=np.median)\n\nOutlet_Type_pivot.plot(kind='bar', color='red', figsize=(12,8))\nplt.xlabel(\"Outlet_Type\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Outlet_type on Item_Outlet_Sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Item_Fat_Content_pivot = train_data.pivot_table(index='Item_Fat_Content', values='Item_Outlet_Sales', aggfunc=np.median)\n\nItem_Fat_Content_pivot.plot(kind='bar',color='blue', figsize=(12,7))\nplt.xlabel(\"Item_Fat_Content\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Item_Fat_Content on Item_outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Fat_Content'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'LF':'Low Fat','reg':'Regular','low fat':'Low Fat'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Fat_Content'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Item_Fat_Content'] = train_data['Item_Fat_Content'].replace({'LF':'Low Fat','reg':'Regular','low fat':'Low Fat'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Item_Fat_Content_pivot = train_data.pivot_table(index='Item_Fat_Content', values='Item_Outlet_Sales', aggfunc=np.median)\n\nItem_Fat_Content_pivot.plot(kind='bar',color='blue', figsize=(12,7))\nplt.xlabel(\"Item_Fat_Content\")\nplt.ylabel(\"Item_Outlet_Sales\")\nplt.title(\"Impact of Item_Fat_Content on Item_outlet_Sales\")\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35,15))\nsns.heatmap(train_data.corr(), vmax=1,square=True, cmap='viridis')\nplt.title(\"Correlation between different attributes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Transformation\n\n### Treating the missing values\n\n#### Item_Weight\n\nFrom the boxplot we plotted at beginning , we noticed that the item_weight column is approximately the \nmean of the column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Weight'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Weight'].fillna(df['Item_Weight'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlet_Size\n\nWe will be replacing the NaN values in the Outlet_Size column with Medium since we don't pass large and small so it is ideal to consider it that way and also the model(i.e outlet size that appears).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outlet_Size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outlet_Size'].fillna(\"Medium\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()    ## now we dont have any null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Item_Visibility\n\nthe Item_Visibility had the minimum value of 0 from our earlier descriptive analysis , but all the items needs to be visible to the customers. Means these items that were not available obviously did not have visibility and were captured as 0. This is an indication that this is a missing value..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Item_Visibility']==0]['Item_Visibility'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Visibility'].fillna(df['Item_Visibility'].median(), inplace=True)                ## 0 is replaced by Medium","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlet Years","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outlet_Establishment_Year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Outlet_Years'] = 2009 - df['Outlet_Establishment_Year']\ndf['Outlet_Years'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Item Type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Type'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### These items are either Food, Drinks, or Non-Consumable..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Item_Identifier'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A closer look at each of the Item_identifier shows that they either starts with either \"FD\"(Food), \"DR\"(Drinks), \"NC\"(Non-Consumable) 3 categories.\n\nFor  better analysis, we will be creating 3 categories as pointed out instead of the already existed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Changing only the first 2 characters (i,e the category ID)\ndf['New_Item_Type'] = df['Item_Identifier'].apply(lambda x: x[0:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Rename them to more intuitive categories::\ndf['New_Item_Type'] = df['New_Item_Type'].map({'FD':'Food','NC':'Non_Consumable','DR':'Drinks'})\n\ndf['New_Item_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If a product is non-consumable then why associate a fat-content to that? we will get rid of this**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Mark non-consumable as separate category in Low-fat.\n\ndf.loc[df['New_Item_Type']==\"Non_Consumable\",\"Item_Fat_Content\"] = \"Non-Edible\"\ndf['Item_Fat_Content'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Under normal circumstances , if a product more visible, then it's likely it eill be getting higher sales. We can based on that hypothesis and importance given to a product in a given store according to the mean of significance given to same product in all other stores.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_visib_avg = df.pivot_table(values='Item_Visibility', index='Item_Identifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_visib_avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"function = lambda x: x['Item_Visibility']/item_visib_avg['Item_Visibility'][item_visib_avg.index==x['Item_Identifier']][0]\n\ndf['item_visib_avg'] = df.apply(function, axis=1).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing with  Categorical Variables\n\n## Label Encoder\nWe will be converting all categorical variables into Numerical types[values of 0 and 1] using the LabelEncoder function since we  cannot build an algorithm based on the object type Variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ndf['Outlet'] = label.fit_transform(df['Outlet_Identifier'])\nvarib = ['Item_Fat_Content','Outlet_Location_Type', 'Outlet_Size','New_Item_Type','Outlet_Type','Outlet']\n\nfor i in varib:\n    df[i] = label.fit_transform(df[i])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to create dummy Variables for these Label encoded variables in order to avoid our algorithm ranking these labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type','New_Item_Type','Outlet'])\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Item_Type','Outlet_Establishment_Year'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = df.loc[df['source']=='train']\ntest_data = df.loc[df['source']=='test']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(['source'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.drop(['Item_Outlet_Sales','source'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_data.drop(['Item_Outlet_Sales','Item_Identifier','Outlet_Identifier'],axis=1).copy()\ny_train = train_data['Item_Outlet_Sales']\nX_test = test_data.drop(['Item_Identifier','Outlet_Identifier'], axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression(normalize=True)\n\nlr.fit(X_train , y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_accuracy = round(lr.score(X_train,y_train) * 100)\nlr_accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DecisionTreeRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\n\ntree.fit(X_train,y_train)\n\ntree_pred = tree.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_accuracy = round(tree.score(X_train, y_train)*100)\ntree_accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomForestRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=400, max_depth=6, min_samples_leaf = 100,n_jobs=4)\n\nrf.fit(X_train,y_train)\n\nrf_pred = rf.predict(X_test)\n\nrf_accuracy = round(rf.score(X_train,y_train) * 100)\nrf_accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(n_estimators=1000, learning_rate = 0.05)\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test)\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_train,y_train)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Suggestions\n\nHere, I have used a simple way of building the algorithm , I did not use train_test_split .\nWe can also consider dealing with Outliers\nWe acn also scale our dataset since all the features are not having the same scale\nThere are several other things which we can use:::\n\n1. Ridge Regression\n2. LASSO\n3. Support Vector Regressor\n4. Bagging Regression\n5. Gradient Boosting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feel free to add more points regarding this data and if you enjoy reading the kernel and gain Value from it, Please do Upvote it as a token of motivation and Appreciation.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}