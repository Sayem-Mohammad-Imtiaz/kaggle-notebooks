{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first read all data\n\ndf2015 = pd.read_csv(\"../input/world-happiness/2015.csv\")\ndf2016 = pd.read_csv(\"../input/world-happiness/2016.csv\")\ndf2017 = pd.read_csv(\"../input/world-happiness/2017.csv\")\ndf2018 = pd.read_csv(\"../input/world-happiness/2018.csv\")\ndf2019 = pd.read_csv(\"../input/world-happiness/2019.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check shape of all data frame\n\nprint(\"2015: \", df2015.shape)\nprint(\"2016: \", df2016.shape)\nprint(\"2017: \", df2017.shape)\nprint(\"2018: \", df2018.shape)\nprint(\"2019: \", df2019.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we can see that 2018 and 2019 data frame have minimum column compare to other data frame\n# let's first clean and preprocess data of 2019 data frame\n\n# checking NaN values\ndf2019.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking dtypes\n\n\ndf2019.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2015.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so here dtypes are perfect according to it's values\n# here let's rename the column for ease to access\n\ndf2019.columns = df2019.columns.str.lower()\ndf2019.rename(columns = {\"overall rank\": \"rank\", \"country or region\": \"country\",\n                         \"freedom to make life choices\": \"freedom\", \"perceptions of corruption\": \"corruption\",\n                        \"gdp per capita\": \"gdp\", \"healthy life expectancy\": \"health\",\n                        \"social support\": \"family\"}, inplace=True)\ndf2019.columns = df2019.columns.str.replace(\" \", \"_\")\ndf2019.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df2019.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so here 2019 data frame it's our main common column among all year data set\n# let's look at to the 2018 data\n\n# let's check NaN values\ndf2018.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2018[df2018.isna().values]\n# here we have 1 NaN value in corruption for United Arab Emirates we can not drop it because\n# this country data important for use we will handle it later on","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check dtypes\n\ndf2018.info()\n\n# dtypes are perfect respect to it's values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's rename the columns and make column as 2019 data frame\n\ndf2018.columns = df2018.columns.str.lower()\ndf2018.rename(columns = {\"overall rank\": \"rank\", \"country or region\": \"country\",\n                         \"freedom to make life choices\": \"freedom\", \"perceptions of corruption\": \"corruption\",\n                        \"gdp per capita\": \"gdp\", \"healthy life expectancy\": \"health\",\n                        \"social support\": \"family\"}, inplace=True)\ndf2018.columns = df2018.columns.str.replace(\" \", \"_\")\ndf2018.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2018.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at to the 2017 data frame\n# let's first check NaN values\n\ndf2017.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we don't have any NaN value let's check dtypes\n\ndf2017.info()\n# dtypes are perfect respect to it's values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's rename the columns and make it same as in 2019 data frame\n\n# let's reamove the dots \ndf2017.columns = df2017.columns.str.replace(\".\", \"\")\n# let's convert into lower case\ndf2017.columns = df2017.columns.str.lower()\ndf2017.rename(columns = {\"happinessrank\": \"rank\", \"happinessscore\": \"score\", \"healthlifeexpectancy\": \"health\",\n                        \"economygdppercapita\": \"gdp\", \"trustgovernmentcorruption\": \"corruption\",\n                        \"dystopiaresidual\": \"dystopia\"}, inplace = True)\ndf2017.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here in 2017 data frame we have some extra columns which are not present in 2019 so let's drop it\n\ndf2017.drop(columns = ['whiskerhigh', 'whiskerlow'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2017.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2017.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at to the 2016 data frame\n# let's first check NaN values\n\ndf2016.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we don't have any NaN values\n# let's look at the dtyps\n\ndf2016.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we have perfect dtypes respect to it's values\n# let's rename the column base on 2019 data frame\n\ndf2016.columns = df2016.columns.str.lower()\ndf2016.rename(columns={'happiness rank':'rank', 'happiness score':'score', 'economy (gdp per capita)': 'gdp',\n                      'health (life expectancy)':'health', 'trust (government corruption)': 'corruption',\n                      \"dystopia residual\": \"dystopia\"},\n                       inplace = True)\ndf2016.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we have some extra columns compare to 2019 data frame so let's drop it\n\ndf2016.drop(columns = ['lower confidence interval','upper confidence interval',\n                      'region'],\n           inplace = True)\ndf2016.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's look at to the 2015 data frame\n\n# checking NaN values\ndf2015.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we don't have any NaN values\n# let's check dtypes \n\ndf2015.info()\n# here all dtypes are perfect according to it's value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's rename the columns base on 2019 columns\n\ndf2015.rename(columns={'Happiness Rank':'rank', 'Happiness Score':'score', 'Economy (GDP per Capita)': 'gdp',\n                      'Health (Life Expectancy)': 'health', 'Trust (Government Corruption)': 'corruption',\n                      \"Dystopia Residual\": \"dystopia\"},\n                        inplace = True)\ndf2015.columns = df2015.columns.str.lower()\ndf2015.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we have some extra columns compare to 2019 so let's remove it\n\ndf2015.drop(columns = ['region', 'standard error'], inplace = True)\ndf2015.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check all columns of all data frame\n\ndef print_all_dataframe_columns():\n    print(\"2015:\\n\", df2015.columns.to_list())\n    print(\"\\n2016:\\n\", df2016.columns.to_list())\n    print(\"\\n2017:\\n\", df2017.columns.to_list())\n    print(\"\\n2018:\\n\", df2018.columns.to_list())\n    print(\"\\n2019:\\n\", df2019.columns.to_list())\n    \nprint_all_dataframe_columns()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so here we don't have a column dystopia in year 2018 and 2019 \n# so i will predict dystopia value for 2018 and 2019 by making a model base on other year data\n# so let's first make a empty column of dystopia with value 0.0 in year 2018 and 2019 for making balanced data fram\n\ndf2018[\"dystopia\"] = 0.0\ndf2019[\"dystopia\"] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's print again columns\n\nprint_all_dataframe_columns()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding year column to each data frame.\n# here year column indicate year of particular year\n\ndf2015[\"year\"] = 2015\ndf2016[\"year\"] = 2016\ndf2017[\"year\"] = 2017\ndf2018[\"year\"] = 2018\ndf2019[\"year\"] = 2019","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# let's add a year column to particular data set.\n# here year column will indicate the year of particular record\n\nprint_all_dataframe_columns()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's combine all the data frame into one \n\nmain_data = pd.concat([df2015, df2016, df2017, df2018, df2019]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first convert country name to lower cases\n\nmain_data.country = main_data.country.str.lower().str.replace(\" \", \"_\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's find the NaN values in main_data\n\nmain_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so here in the corruption column we have one NaN value let's check it out\n\nmain_data[main_data.isna().values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so here we have NaN value in year 2018 for corruption column in country united arab emirates\n\nmain_data.loc[main_data.country == \"united_arab_emirates\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here i'm going to fill that NaN value by average of other years value of corruption for united arab emirates\n\navg_corruption = main_data.loc[main_data.country == \"united_arab_emirates\", \"corruption\"].mean()\n\nmain_data.corruption.fillna(avg_corruption, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data.isna().sum().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filling dysopia value for year 2018 and 2019 by creating model base on 2015, 2016 and 2017"},{"metadata":{"trusted":true},"cell_type":"code","source":"# le's combine 2016, 2017 and 2018 data set from main_data data frame\n# because in these data sets we have dystopia values which will be help full to create a model to predict\n# a dystopia value for year 2018 and 2019\n\ndf_dystopia = main_data.loc[(main_data.year == 2015) | (main_data.year == 2016) | (main_data.year == 2017)].copy().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the data of dystopia \ndf_dystopia.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we have a text data in country column so let's check how many unique country we have\ndf_dystopia.country.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can't pass text data to ML model because ML model work on numeric data for calculating stuff\n# so let's convert text data into numerice\n# here i'm use One Hot Ecoding to convert text data into numeric\n\nencoded_country = pd.get_dummies(df_dystopia.country, drop_first=True) # afghanistan was first column\n# here i have drop first column for preventing dummy variable trap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's merge encoded country columns with our main df_dystopia data frame\ndf_dystopia = pd.concat([df_dystopia.copy(), encoded_country], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dystopia.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we have encoded country column so we don't need country column so let's drop it\n\ndf_dystopia.drop(columns = \"country\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dystopia.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create feature matrix X and target vector y\n\nX = df_dystopia.drop(columns = \"dystopia\")\ny = df_dystopia.dystopia","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X Train: \", X_train.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"X Test: \", X_test.shape)\nprint(\"y test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Scalling using Standard Scaler\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train.iloc[:, :9] = scaler.fit_transform(X_train.iloc[:, :9])\nX_test.iloc[:, :9] = scaler.transform(X_test.iloc[:, :9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\nalgos = {\n    \"Ridge\": {\n        \"model\": Ridge(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30 , 40]\n        }\n    },\n    \"Lasso\": {\n        \"model\": Lasso(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 40]\n        }\n    },\n    \"Random Forest\": {\n        \"model\": RandomForestRegressor(),\n        \"params\": {\n            \"n_estimators\": [int(x) for x in np.linspace(100, 1200, 12)],\n            \"max_features\": [\"auto\", \"sqrt\"],\n            \"max_depth\": [int(x) for x in np.linspace(5, 30, 6)],\n            \"min_samples_split\": [2, 5, 10, 15, 100],\n            \"min_samples_leaf\": [1, 2, 5, 10]\n        }\n    }\n}\n\nbest_models = {}\nscore = []\nfor model_name, values in algos.items():\n    rscv = RandomizedSearchCV(values[\"model\"], values[\"params\"], cv = 5, n_jobs = -1)\n    rscv.fit(X_train, y_train)\n    best_models[model_name] = rscv\n    score.append({\n        \"Model\": model_name,\n        \"Best Parameters\": rscv.best_params_,\n        \"Best Score\": rscv.best_score_\n    })\n    \npd.DataFrame(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's test ridge lasso and random forest regressor model on our test data set\n\nprint(\"Ridge: \", best_models[\"Ridge\"].score(X_test, y_test))\nprint(\"Lasso: \", best_models[\"Lasso\"].score(X_test, y_test))\nprint(\"Random Forest\", best_models[\"Random Forest\"].score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here as we know ridge regression perform better\n# let's predict test data using ridge regression model\n\ndystopia_pred = best_models[\"Ridge\"].predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's find difference between actual value and predicted value\n\nsns.distplot(y_test - dystopia_pred)\n# here curve look like normal distribution so our model trained value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### final model for dystopia"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we know Ridge regression work better on our data for prediction dystopia\n# so let's create final model using ridge regression on base on all data i mean full X and y data\n\n# so let's first scale the full X data using Standard Scaler\n# scaling X data\n\nscaler = StandardScaler()\nX.iloc[:, :9] = scaler.fit_transform(X.iloc[:, :9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's use Lasso Regression for building final model with alpha = 1e-0.8\n\nfinal_model_dystopia = Lasso(alpha = 1e-08)\nfinal_model_dystopia.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(X.columns == \"ok\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create predict function to predict dystopia\n# in the function we perform scalling and prediction using final_model_dystopia\n\ndef predict_dystopia(record):\n    X_new = np.zeros((1, X.shape[1]), dtype=\"int\")\n    other_columns = [[record[\"rank\"], record[\"score\"], record[\"gdp\"], record[\"family\"], record[\"health\"],\n                    record[\"freedom\"], record[\"generosity\"], record[\"corruption\"], record[\"year\"]]]\n    other_columns = scaler.transform(other_columns).flatten()\n    for i in range(0, 9):\n        X_new[0][i] = other_columns[i]\n    country_name = record[\"country\"]\n    if country_name != \"afghanistan\": # bcz we dropped afghanistan column so it can not find it.\n        country_index = np.where(X.columns == record[\"country\"])\n        X_new[0][country_index] = 1\n    \n    predicted_value = final_model_dystopia.predict(X_new)\n    return predicted_value[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create a loop and put a predicted value to dystopia column where year 2018 and 2019\n\nfor i in main_data.loc[(main_data.year == 2018) | (main_data.year == 2019)].index:\n    main_data.loc[i, \"dystopia\"] = predict_dystopia(main_data.loc[i])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# now our dystopia value has been filled for year 2018 and 2019 using lasso regression model\nmain_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 10 Happiness country in year 2015 and 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"year_wise_data = main_data.groupby([\"year\", \"country\"])[\"score\"].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_wise_data.loc[year_wise_data.country == \"india\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Top 10 Hapinees Country in 2015 and 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_2015 = year_wise_data.loc[year_wise_data.year==2015].sort_values(\"score\", ascending = False).reset_index(drop=True).head(10)\ntop_10_2019 = year_wise_data.loc[year_wise_data.year==2019].sort_values(\"score\", ascending = False).reset_index(drop=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"top_10_2015.style.background_gradient(cmap=\"Reds\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"top_10_2019.style.background_gradient(cmap=\"Greens\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's encode the text data to numeric format using one hot encoding\n\nencoded_country = pd.get_dummies(main_data.country, drop_first=True) # first column was afghanistan\n\n# concate encoded country to main data\nmain_data = pd.concat([main_data, encoded_country], axis = 1)\n\n# drop country column\nmain_data.drop(columns = \"country\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create a feature matrix X and response vactor y\n\nX = main_data.drop(columns = \"score\")\ny = main_data.score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's divide the data into train and test part\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X Train: \", X_train.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"X Test: \", X_test.shape)\nprint(\"y test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale the features\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train.iloc[:, 0:9] = scaler.fit_transform(X_train.iloc[:, 0:9])\nX_test.iloc[:, 0:9] = scaler.transform(X_test.iloc[:, 0:9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\nalgos = {\n    \"Ridge\": {\n        \"model\": Ridge(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30 , 40]\n        }\n    },\n    \"Lasso\": {\n        \"model\": Lasso(),\n        \"params\": {\n            \"alpha\": [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 40]\n        }\n    },\n    \"Random Forest\": {\n        \"model\": RandomForestRegressor(),\n        \"params\": {\n            \"n_estimators\": [int(x) for x in np.linspace(100, 1200, 12)],\n            \"max_features\": [\"auto\", \"sqrt\"],\n            \"max_depth\": [int(x) for x in np.linspace(5, 30, 6)],\n            \"min_samples_split\": [2, 5, 10, 15, 100],\n            \"min_samples_leaf\": [1, 2, 5, 10]\n        }\n    }\n}\n\nbest_models = {}\nscore = []\nfor model_name, values in algos.items():\n    rscv = RandomizedSearchCV(values[\"model\"], values[\"params\"], cv = 5, n_jobs = -1)\n    rscv.fit(X_train, y_train)\n    best_models[model_name] = rscv\n    score.append({\n        \"Model\": model_name,\n        \"Best Parameters\": rscv.best_params_,\n        \"Best Score\": rscv.best_score_\n    })\n    \npd.DataFrame(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's test ridge lasso and random forest regressor model on our test data set\n\nprint(\"Ridge: \", best_models[\"Ridge\"].score(X_test, y_test))\nprint(\"Lasso: \", best_models[\"Lasso\"].score(X_test, y_test))\nprint(\"Random Forest\", best_models[\"Random Forest\"].score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction = best_models[\"Random Forest\"].predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y_test - y_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE: \", mean_absolute_error(y_test, y_prediction))\nprint(\"MSE: \", mean_squared_error(y_test, y_prediction))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}