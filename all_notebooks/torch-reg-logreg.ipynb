{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Linear Regression in PyTorch\nHere we demonstrate how to use PyTorch's automatic differentiation features.\n\n`theta` is our variable containing learnable parameters for the regression coefficients. Notice that when we instantiate `theta`, we set `requires_grad` to `True`. This signifies that we want to keep track of all the gradients of these parameters.","metadata":{}},{"cell_type":"code","source":"X = torch.rand(1000, 2)\nw_true = 10 * torch.rand(2, 1)\ny = X@w_true\n\ntheta = torch.rand(2, 1)\ntheta.requires_grad = True\nloss_fn =  torch.nn.MSELoss()\nalpha = 0.001\ntol = 1e-4\n\nfor i in range(100):\n    y_pred = X @ theta\n    loss = (y - y_pred).square().sum()\n    loss.backward()\n    \n    theta.data -= alpha * theta.grad.data\n    theta.grad.zero_()\n    \n    if i % 10 == 0:\n        print('Iter: {:2d} | Loss: {:.3f}'.format(i, loss.item()))\n    if loss < tol:\n        print('Stopping after', i, 'iterations')\n        break\nprint(\"True weights;\")\nprint(w_true)\nprint(\"Learned weights:\")\nprint(theta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The key steps in each iteration are:\n* evaluate the model to get `y_pred`\n* compute the mean squared error loss between `y_pred` and `y_true`\n* call `loss.backward()`, which tells PyTorch to compute the gradients of your parameters, which will now be accessible via `theta.grad`\n* update theta with its gradients (note that you have to do this via `theta.data`)\n* zero out the gradients","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression with PyTorch\nNow that we have seen the basics of gradient descent with PyTorch, we can try to do gradient descent for logistic regression.\n\nTo setup the model, we just need to have a parameter vector `theta`. Our loss function will be `torch.nn.CrossEntropyLoss`, which will compute the log softmax and negative log-likelihood steps in one function. This allows us to avoid doing any of the logistic function steps like computing $e^{-\\theta^\\top x}$ and computing the log of these terms since they all happen in the `torch.nn.CrossEntropy` function.","metadata":{}},{"cell_type":"code","source":"def sample(X, y, batch_size):\n    indices = torch.randint(low=0, high=len(X), size=(batch_size,))\n    return X[indices], y[indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of doing the full multi-class classification, we'll just do 0/1 classification for this demo.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\nX = df.drop('label', axis=1).to_numpy()\ny = df['label'].to_numpy()\n\n# only use 1s and 0s, convert to torch Tensor\nX_th = torch.from_numpy(X[y <= 1]).float()\ny_th = torch.from_numpy(y[y <= 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_digits(X, y):\n    plt.subplots(2, 5)\n    for d in range(10):\n        plt.subplot(2, 5, d+1)\n        digits = y==d\n        plt.imshow(X[digits, :][0, :].reshape(28, 28), cmap='gray')\n        plt.xticks([])\n        plt.yticks([])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_digits(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_iters = 100\nbatch_size = 100\nalpha = 0.001 # learning rate\nloss_fn = torch.nn.CrossEntropyLoss()\ntheta = torch.rand(784, 2, requires_grad=True)\nlosses = []\naccs = []\n\nfor i in range(max_iters):\n    X_batch, y_batch = sample(X_th, y_th, batch_size)\n    logits = X_batch @ theta\n    \n    loss = loss_fn(logits, y_batch)\n    loss.backward()\n    theta.data -= alpha * theta.grad.data\n    theta.grad.zero_()\n    \n    y_pred = (X_batch @ theta).max(1)[1]\n    batch_acc = (y_pred == y_batch).sum() / len(y_batch)\n    losses.append(loss.item())\n    accs.append(batch_acc.item())\n    if i % 10 == 0:\n        print(\"Iter {} | Last 10 avg loss: {:.3f} | acc: {:.3f}\".format(\n            i, np.mean(losses[-10:]),np.mean(accs[-10:])\n        ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As in the linear regression case, our gradient descent procedure breaks into a few crucial steps\n* evaluating the model on our minibatch of examples\n* computing the loss function between our model output and the true labels\n* calling `loss.backward()`\n* updating our weights with its gradients\n* zeroing out the gradient","metadata":{}}]}