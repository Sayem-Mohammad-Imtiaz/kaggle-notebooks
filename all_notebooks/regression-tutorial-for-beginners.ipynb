{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Regression\n\n*Supervised* machine learning techniques involve training a model to operate on a set of *features* and predict a *label* using a dataset that includes some already-known label values. The training process *fits* the features to the known labels to define a general function that can be applied to new features for which the labels are unknown, and predict them. You can think of this function like this, in which ***y*** represents the label we want to predict and ***x*** represents the features the model uses to predict it.\n\n$$y = f(x)$$\n\nIn most cases, *x* is actually a *vector* that consists of multiple feature values, so to be a little more precise, the function could be expressed like this:\n\n$$y = f([x_1, x_2, x_3, ...])$$\n\nThe goal of training the model is to find a function that performs some kind of calculation to the *x* values that produces the result *y*. We do this by applying a machine learning *algorithm* that tries to fit the *x* values to a calculation that produces *y* reasonably accurately for all of the cases in the training dataset.\n\nIn this notebook, we'll focus on *regression*, using an example based on a real study in which data for a bicycle sharing scheme was collected and used to predict the number of bike share counts based on seasonality and weather conditions.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Explore the Data\n\nThe first step in any machine learning project is to explore the data that you will use to train a model. The goal of this exploration is to try to understand the relationships between its attributes; in particular, any apparent correlation between the *features* and the *label* your model will try to predict. This may require some work to detect and fix issues in the data (such as dealing with missing values, errors, or outlier values), deriving new feature columns by transforming or combining existing features (a process known as *feature engineering*), *normalizing* numeric features (values you can measure or count) so they're on a similar scale, and *encoding* categorical features (values that represent discrete categories) as numeric indicators.\n\nLet's start by loading the bicycle sharing data as a **Pandas** DataFrame and viewing the first few rows.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n# load the training dataset\nbike_data = pd.read_csv('/kaggle/input/london-bike-sharing-dataset/london_merged.csv')\nbike_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:47:40.321056Z","iopub.execute_input":"2021-07-20T03:47:40.321426Z","iopub.status.idle":"2021-07-20T03:47:40.377125Z","shell.execute_reply.started":"2021-07-20T03:47:40.321398Z","shell.execute_reply":"2021-07-20T03:47:40.376211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data consists of the following columns:\n\n- **timestamp**: Representing timestamp of bike share\n- **cnt**: Representing total number of bike shares\n- **t1**: The temperature in celsius.\n- **t2**: The apparent (\"feels-like\") temperature in celsius.\n- **hum**: The humidity level\n- **wind_speed**: The windspeed\n- **weather_code**: A categorical value indicating the weather situation (1:clear, 2:mist/cloud, 3:light rain/snow, 4:heavy rain/hail/snow/fog)\n- **is_holiday**: A binary value indicating whether or not the day is a holiday\n- **is_weekend**: A binary value indicating whether or not the day is a weekend\n- **season**: A numerically encoded value indicating the season (1:spring, 2:summer, 3:fall, 4:winter)\n\n\nIn this dataset, **cnt** represents the label (the *y* value) our model must be trained to predict. The other columns are potential features (*x* values).","metadata":{}},{"cell_type":"code","source":"bike_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T02:50:58.735083Z","iopub.execute_input":"2021-07-20T02:50:58.735513Z","iopub.status.idle":"2021-07-20T02:50:58.762406Z","shell.execute_reply.started":"2021-07-20T02:50:58.735477Z","shell.execute_reply":"2021-07-20T02:50:58.761213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above results it is evident that there are no null values and total 17413 records in the dataset","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering\nWe can perform some *feature engineering* to combine or derive new features. For example, let's add new columns named **day**, **month**, **hour** to the dataframe by extracting the day component from the existing **timestamp** column.","metadata":{}},{"cell_type":"code","source":"# as the datatype of timestamp is object we need to convert it into datetime\nbike_data[\"timestamp\"] = pd.to_datetime(bike_data[\"timestamp\"])\nbike_data[\"month\"] = bike_data[\"timestamp\"].apply(lambda x:x.month)\nbike_data[\"day\"] = bike_data[\"timestamp\"].apply(lambda x:x.day)\nbike_data[\"hour\"] = bike_data[\"timestamp\"].apply(lambda x:x.hour)\nbike_data = bike_data.drop(\"timestamp\", axis=1) ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T02:55:16.600986Z","iopub.execute_input":"2021-07-20T02:55:16.601382Z","iopub.status.idle":"2021-07-20T02:55:16.904622Z","shell.execute_reply.started":"2021-07-20T02:55:16.601351Z","shell.execute_reply":"2021-07-20T02:55:16.903619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bike_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T02:55:24.257963Z","iopub.execute_input":"2021-07-20T02:55:24.258315Z","iopub.status.idle":"2021-07-20T02:55:24.276862Z","shell.execute_reply.started":"2021-07-20T02:55:24.258274Z","shell.execute_reply":"2021-07-20T02:55:24.276204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking unique values in each column\n{column: len(bike_data[column].unique()) for column in bike_data.columns}","metadata":{"execution":{"iopub.status.busy":"2021-07-20T02:56:21.66432Z","iopub.execute_input":"2021-07-20T02:56:21.664663Z","iopub.status.idle":"2021-07-20T02:56:21.679424Z","shell.execute_reply.started":"2021-07-20T02:56:21.664634Z","shell.execute_reply":"2021-07-20T02:56:21.678319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we know `weather_code` and `season` are categorical variable so we need to handle them using **One Hot Encoding**","metadata":{}},{"cell_type":"code","source":"# One-hot encoding\n## 1. encoding weather_code\ndummies_w = pd.get_dummies(bike_data[\"weather_code\"], prefix=\"weather\")\nbike_data = pd.concat([bike_data,dummies_w], axis=1)\nbike_data = bike_data.drop(\"weather_code\", axis=1)\n\n## 2. encoding season\ndummies_s = pd.get_dummies(bike_data[\"season\"], prefix=\"season\")\nbike_data = pd.concat([bike_data,dummies_s], axis=1)\nbike_data = bike_data.drop(\"season\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:00:11.231713Z","iopub.execute_input":"2021-07-20T03:00:11.232102Z","iopub.status.idle":"2021-07-20T03:00:11.251425Z","shell.execute_reply.started":"2021-07-20T03:00:11.232071Z","shell.execute_reply":"2021-07-20T03:00:11.250409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bike_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:00:18.189971Z","iopub.execute_input":"2021-07-20T03:00:18.190366Z","iopub.status.idle":"2021-07-20T03:00:18.219176Z","shell.execute_reply.started":"2021-07-20T03:00:18.190332Z","shell.execute_reply":"2021-07-20T03:00:18.218471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, let's start our analysis of the data by examining a few key descriptive statistics. We can use the dataframe's **describe** method to generate these for the numeric features as well as the **cnt** label column.","metadata":{}},{"cell_type":"code","source":"numeric_features = ['t1', 't2', 'hum', 'wind_speed','day','hour','month']\nbike_data[numeric_features + ['cnt']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:02:28.179065Z","iopub.execute_input":"2021-07-20T03:02:28.179568Z","iopub.status.idle":"2021-07-20T03:02:28.225941Z","shell.execute_reply.started":"2021-07-20T03:02:28.179536Z","shell.execute_reply":"2021-07-20T03:02:28.224922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"he statistics reveal some information about the distribution of the data in each of the numeric fields, including the number of observations (there are 731 records), the mean, standard deviation, minimum and maximum values, and the quartile values (the threshold values for 25%, 50% - which is also the median, and 75% of the data). From this, we can see that the mean number of daily bike share count is around 1143; there's a slight difference in standard deviation, indicating a slight variance in the number of bike shares per day.\n\nWe might get a clearer idea of the distribution of bike share values by visualizing the data. Common plot types for visualizing numeric data distributions are *histograms* and *box plots*, so let's use Python's **matplotlib** library to create one of each of these for the **cnt** column.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# This ensures plots are displayed inline in the Jupyter notebook\n%matplotlib inline\n\n# Get the label column\nlabel = bike_data['cnt']\n\n\n# Create a figure for 2 subplots (2 rows, 1 column)\nfig, ax = plt.subplots(2, 1, figsize = (9,12))\n\n# Plot the histogram   \nax[0].hist(label, bins=100)\nax[0].set_ylabel('Frequency')\n\n# Add lines for the mean, median, and mode\nax[0].axvline(label.mean(), color='magenta', linestyle='dashed', linewidth=2)\nax[0].axvline(label.median(), color='cyan', linestyle='dashed', linewidth=2)\n\n# Plot the boxplot   \nax[1].boxplot(label, vert=False)\nax[1].set_xlabel('bike shares')\n\n# Add a title to the Figure\nfig.suptitle('Bike Share Distribution')\n\n# Show the figure\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:05:54.67745Z","iopub.execute_input":"2021-07-20T03:05:54.677821Z","iopub.status.idle":"2021-07-20T03:05:55.249406Z","shell.execute_reply.started":"2021-07-20T03:05:54.677789Z","shell.execute_reply":"2021-07-20T03:05:55.248405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plots show that the number of daily rentals ranges from 0 to just over 5000. However, the mean (and median) number of daily rentals is closer to the low end of that range, with most of the data between 0 and around 1800 bike shares. The few values above this are shown in the box plot as small circles, indicating that they are **outliers** - in other words, unusually high or low values beyond the typical range of most of the data.\n\nWe can do the same kind of visual exploration of the numeric features. Let's create a histogram for each of these.","metadata":{}},{"cell_type":"code","source":"# Plot a histogram for each numeric feature\nfor col in numeric_features:\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    feature = bike_data[col]\n    feature.hist(bins=100, ax = ax)\n    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n    ax.set_title(col)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:07:59.098614Z","iopub.execute_input":"2021-07-20T03:07:59.099001Z","iopub.status.idle":"2021-07-20T03:08:01.590464Z","shell.execute_reply.started":"2021-07-20T03:07:59.09897Z","shell.execute_reply":"2021-07-20T03:08:01.589518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"he numeric features seem to be more *normally* distributed, with the mean and median nearer the middle of the range of values except humidity where mean and median is towards right side, coinciding with where the most commonly occurring values are.\n\n> **Note**: The distributions are not truly *normal* in the statistical sense, which would result in a smooth, symmetric \"bell-curve\" histogram with the mean and mode (the most common value) in the center; but they do generally indicate that most of the observations have a value somewhere near the middle.\n\nFor the numeric features, we can create scatter plots that show the intersection of feature and label values. We can also calculate the *correlation* statistic to quantify the apparent relationship.","metadata":{}},{"cell_type":"code","source":"for col in numeric_features:\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    feature = bike_data[col]\n    label = bike_data['cnt']\n    correlation = feature.corr(label)\n    plt.scatter(x=feature, y=label)\n    plt.xlabel(col)\n    plt.ylabel('Bike Shares')\n    ax.set_title('Bike Shares vs ' + col + '- correlation: ' + str(correlation))\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:11:38.259772Z","iopub.execute_input":"2021-07-20T03:11:38.26023Z","iopub.status.idle":"2021-07-20T03:11:39.901174Z","shell.execute_reply.started":"2021-07-20T03:11:38.260193Z","shell.execute_reply":"2021-07-20T03:11:39.900278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results aren't conclusive, but if you look closely at the scatter plots for **t1** and **t2**, you can see a vague diagonal trend showing that higher Bike share counts tend to coincide with higher temperatures; and a correlation value of just over 0.5 for both of these features supports this observation. Conversely, the plots for **hum** and **windspeed** show a slightly negative correlation, indicating that there are fewer rentals on days with high humidity or windspeed.\n\nNow let's compare the categorical features to the label. We'll do this by creating box plots that show the distribution of share counts for each category.","metadata":{}},{"cell_type":"code","source":"# plot a bar plot for each categorical feature count\ncategorical_features = ['is_weekend','is_holiday','day','month','hour']\n# plot a boxplot for the label by each categorical feature\nfor col in categorical_features:\n    fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    bike_data.boxplot(column = 'cnt', by = col, ax = ax)\n    ax.set_title('Bike Shares by ' + col)\n    ax.set_ylabel(\"Bike Share Counts\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:17:29.246052Z","iopub.execute_input":"2021-07-20T03:17:29.24656Z","iopub.status.idle":"2021-07-20T03:17:31.079165Z","shell.execute_reply.started":"2021-07-20T03:17:29.246528Z","shell.execute_reply":"2021-07-20T03:17:31.078282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plots show some variance in the relationship between some category values and rentals. For example, there's a similar distribution of bike shares on weekends and those during the holiday. There's a noticeable trend that shows different bike share distributions in day time from 10 to 15 compared to evening and night time. ","metadata":{}},{"cell_type":"markdown","source":"## Train a Regression Model\n\nNow that we've explored the data, it's time to use it to train a regression model that uses the features we've identified as potentially predictive to predict the **cnt** label.  The first thing we need to do is to separate the features we want to use to train the model from the label we want it to predict.","metadata":{}},{"cell_type":"code","source":"# Splitting & scaling the data\n\nX = bike_data.drop(\"cnt\", axis=1)\ny = bike_data[\"cnt\"]\n\nprint('Features:',X[:10], '\\nLabels:', y[:10], sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:25:17.177976Z","iopub.execute_input":"2021-07-20T03:25:17.178408Z","iopub.status.idle":"2021-07-20T03:25:17.199099Z","shell.execute_reply.started":"2021-07-20T03:25:17.178373Z","shell.execute_reply":"2021-07-20T03:25:17.198065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After separating the dataset, we now have numpy arrays named **X** containing the features, and **y** containing the labels.\n\nWe *could* train a model using all of the data; but it's common practice in supervised learning to split the data into two subsets; a (typically larger) set with which to train the model, and a smaller \"hold-back\" set with which to validate the trained model. This enables us to evaluate how well the model performs when used with the validation dataset by comparing the predicted labels to the known labels. It's important to split the data *randomly* (rather than say, taking the first 80% of the data for training and keeping the rest for validation). This helps ensure that the two subsets of data are statistically comparable (so we validate the model with data that has a similar statistical distribution to the data on which it was trained).\n\nTo randomly split the data, we'll use the **train_test_split** function in the **scikit-learn** library. This library is one of the most widely used machine learning packages for Python.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data 70%-30% into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\nprint ('Training Set: %d rows\\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:27:22.222754Z","iopub.execute_input":"2021-07-20T03:27:22.223143Z","iopub.status.idle":"2021-07-20T03:27:23.460247Z","shell.execute_reply.started":"2021-07-20T03:27:22.223112Z","shell.execute_reply":"2021-07-20T03:27:23.459101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have the following four datasets:\n\n- **X_train**: The feature values we'll use to train the model\n- **y_train**: The corresponding labels we'll use to train the model\n- **X_test**: The feature values we'll use to validate the model\n- **y_test**: The corresponding labels we'll use to validate the model\n\nNow we're ready to train a model by fitting a suitable regression algorithm to the training data. We'll use a *linear regression* algorithm, a common starting point for regression that works by trying to find a linear relationship between the *X* values and the *y* label. The resulting model is a function that conceptually defines a line where every possible X and y value combination intersect.","metadata":{}},{"cell_type":"markdown","source":"## Feature Scaling\nScaling is a technique often applied as part of data preparation for machine learning. The goal of scaling is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For standardization we will use standrad scaler. Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.","metadata":{}},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:33:26.217494Z","iopub.execute_input":"2021-07-20T03:33:26.217862Z","iopub.status.idle":"2021-07-20T03:33:26.226452Z","shell.execute_reply.started":"2021-07-20T03:33:26.217832Z","shell.execute_reply":"2021-07-20T03:33:26.225224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler= StandardScaler()\nX_train[['t1', 't2', 'hum', 'wind_speed','month','day', 'hour']] = scaler.fit_transform(X_train[['t1', 't2', 'hum', 'wind_speed','month','day', 'hour']])\n\nX_test[['t1', 't2', 'hum', 'wind_speed','month','day', 'hour']] = scaler.transform(X_test[['t1', 't2', 'hum', 'wind_speed','month','day', 'hour']])","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:35:24.014297Z","iopub.execute_input":"2021-07-20T03:35:24.014656Z","iopub.status.idle":"2021-07-20T03:35:24.076725Z","shell.execute_reply.started":"2021-07-20T03:35:24.014627Z","shell.execute_reply":"2021-07-20T03:35:24.075811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nfrom sklearn.linear_model import LinearRegression\n\n# Fit a linear regression model on the training set\nmodel = LinearRegression().fit(X_train, y_train)\nprint (model)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:36:07.104401Z","iopub.execute_input":"2021-07-20T03:36:07.104786Z","iopub.status.idle":"2021-07-20T03:36:07.39504Z","shell.execute_reply.started":"2021-07-20T03:36:07.104754Z","shell.execute_reply":"2021-07-20T03:36:07.393926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Trained Model\nNow that we've trained the model, we can use it to predict bike share counts for the features we held back in our validation dataset. Then we can compare these predictions to the actual label values to evaluate how well (or not!) the model is working.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\npredictions = model.predict(X_test)\nnp.set_printoptions(suppress=True)\nprint('Predicted labels: ', np.round(predictions)[:10])\nprint('Actual labels   : ' ,y_test[:10])","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:36:58.91487Z","iopub.execute_input":"2021-07-20T03:36:58.915576Z","iopub.status.idle":"2021-07-20T03:36:58.940609Z","shell.execute_reply.started":"2021-07-20T03:36:58.915525Z","shell.execute_reply":"2021-07-20T03:36:58.93901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n%matplotlib inline\n\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Daily Bike Share Predictions')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:37:29.154025Z","iopub.execute_input":"2021-07-20T03:37:29.154404Z","iopub.status.idle":"2021-07-20T03:37:29.36312Z","shell.execute_reply.started":"2021-07-20T03:37:29.154372Z","shell.execute_reply":"2021-07-20T03:37:29.362229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a definite diagonal trend, and the intersections of the predicted and actual values are generally following the path of the trend line; but there's a fair amount of difference between the ideal function represented by the line and the results. This variance represents the *residuals* of the model - in other words, the difference between the label predicted when the model applies the coefficients it learned during training to the validation data, and the actual value of the validation label. These residuals when evaluated from the validation data indicate the expected level of *error* when the model is used with new data for which the label is unknown.\n\nYou can quantify the residuals by calculating a number of commonly used evaluation metrics. We'll focus on the following three:\n\n- **Mean Square Error (MSE)**: The mean of the squared differences between predicted and actual values. This yields a relative metric in which the smaller the value, the better the fit of the model\n- **Root Mean Square Error (RMSE)**: The square root of the MSE. This yields an absolute metric in the same unit as the label (in this case, numbers of rentals). The smaller the value, the better the model (in a simplistic sense, it represents the average number of rentals by which the predictions are wrong!)\n- **Coefficient of Determination (usually known as *R-squared* or R<sup>2</sup>)**: A relative metric in which the higher the value, the better the fit of the model. In essence, this metric represents how much of the variance between predicted and actual label values the model is able to explain.\n\n> **Note**: You can find out more about these and other metrics for evaluating regression models in the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n\nLet's use Scikit-Learn to calculate these metrics for our model, based on the predictions it generated for the validation data.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\n\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:37:53.699941Z","iopub.execute_input":"2021-07-20T03:37:53.700732Z","iopub.status.idle":"2021-07-20T03:37:53.712747Z","shell.execute_reply.started":"2021-07-20T03:37:53.700666Z","shell.execute_reply":"2021-07-20T03:37:53.711557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now we've quantified the ability of our model to predict the number of rentals. It definitely has *some* predictive power, but we can probably do better!\n\n## Experiment with Algorithms\n\nThe linear regression algorithm we used to train the model has some predictive capability, but there are many kinds of regression algorithm we could try, including:\n\n- **Linear algorithms**: Not just the Linear Regression algorithm we used above (which is technically an *Ordinary Least Squares* algorithm), but other variants such as *Lasso* and *Ridge*.\n- **Tree-based algorithms**: Algorithms that build a decision tree to reach a prediction.\n- **Ensemble algorithms**: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n\n> **Note**: For a full list of Scikit-Learn estimators that encapsulate algorithms for supervised machine learning, see the [Scikit-Learn documentation](https://scikit-learn.org/stable/supervised_learning.html). There are many algorithms to choose from, but for most real-world scenarios, the [Scikit-Learn estimator cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) can help you find a suitable starting point. \n\n### Try Another Linear Algorithm\n\nLet's try training our regression model by using a **Lasso** algorithm. We can do this by just changing the estimator in the training code.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\n# Fit a lasso model on the training set\nmodel = Lasso().fit(X_train, y_train)\nprint (model, \"\\n\")\n\n# Evaluate the model using the test data\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Daily Bike Share Counts')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:38:40.580725Z","iopub.execute_input":"2021-07-20T03:38:40.581071Z","iopub.status.idle":"2021-07-20T03:38:40.874322Z","shell.execute_reply.started":"2021-07-20T03:38:40.581041Z","shell.execute_reply":"2021-07-20T03:38:40.873666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Try a Decision Tree Algorithm\n\nAs an alternative to a linear model, there's a category of algorithms for machine learning that uses a tree-based approach in which the features in the dataset are examined in a series of evaluations, each of which results in a *branch* in a *decision tree* based on the feature value. At the end of each series of branches are leaf-nodes with the predicted label value based on the feature values.\n\nIt's easiest to see how this works with an example. Let's train a Decision Tree regression model using the bike rental data. After training the model, the code below will print the model definition and a text representation of the tree it uses to predict label values.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import export_text\n\n# Train the model\nmodel = DecisionTreeRegressor().fit(X_train, y_train)\nprint (model, \"\\n\")\n\n# Visualize the model tree\ntree = export_text(model)\nprint(tree)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:39:09.976136Z","iopub.execute_input":"2021-07-20T03:39:09.976716Z","iopub.status.idle":"2021-07-20T03:39:10.335467Z","shell.execute_reply.started":"2021-07-20T03:39:09.976668Z","shell.execute_reply":"2021-07-20T03:39:10.334411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now we have a tree-based model; but is it any good? Let's evaluate it with the test data.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model using the test data\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Daily Bike Share Predictions')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:39:41.507996Z","iopub.execute_input":"2021-07-20T03:39:41.508371Z","iopub.status.idle":"2021-07-20T03:39:41.685723Z","shell.execute_reply.started":"2021-07-20T03:39:41.50834Z","shell.execute_reply":"2021-07-20T03:39:41.684752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect now our R2 score is more than 90% which means our model has higher predictive power.","metadata":{}},{"cell_type":"markdown","source":"### Try an Ensemble Algorithm\n\nEnsemble algorithms work by combining multiple base estimators to produce an optimal model, either by applying an aggregate function to a collection of base models (sometimes referred to a *bagging*) or by building a sequence of models that build on one another to improve predictive performance (referred to as *boosting*).\n\nFor example, let's try a Random Forest model, which applies an averaging function to multiple Decision Tree models for a better overall model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Train the model\nmodel_rf = RandomForestRegressor().fit(X_train, y_train)\nprint (model_rf, \"\\n\")\n\n# Evaluate the model using the test data\npredictions = model_rf.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Daily Bike Share Predictions')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:44:56.446963Z","iopub.execute_input":"2021-07-20T03:44:56.447354Z","iopub.status.idle":"2021-07-20T03:45:03.743631Z","shell.execute_reply.started":"2021-07-20T03:44:56.447322Z","shell.execute_reply":"2021-07-20T03:45:03.742689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, this is more accurate than decision tree having R2 score more than 95% and MSE RMSE are also quite lower than decision tree algorithm","metadata":{}},{"cell_type":"markdown","source":"For good measure, let's also try a *boosting* ensemble algorithm. We'll use a Gradient Boosting estimator, which like a Random Forest algorithm builds multiple trees, but instead of building them all independently and taking the average result, each tree is built on the outputs of the previous one in an attempt to incrementally reduce the *loss* (error) in the model.","metadata":{}},{"cell_type":"code","source":"# Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Fit a lasso model on the training set\nmodel = GradientBoostingRegressor().fit(X_train, y_train)\nprint (model, \"\\n\")\n\n# Evaluate the model using the test data\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\nrmse = np.sqrt(mse)\nprint(\"RMSE:\", rmse)\nr2 = r2_score(y_test, predictions)\nprint(\"R2:\", r2)\n\n# Plot predicted vs actual\nplt.scatter(y_test, predictions)\nplt.xlabel('Actual Labels')\nplt.ylabel('Predicted Labels')\nplt.title('Daily Bike Share Predictions')\n# overlay the regression line\nz = np.polyfit(y_test, predictions, 1)\np = np.poly1d(z)\nplt.plot(y_test,p(y_test), color='magenta')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:42:55.292986Z","iopub.execute_input":"2021-07-20T03:42:55.293362Z","iopub.status.idle":"2021-07-20T03:42:57.13694Z","shell.execute_reply.started":"2021-07-20T03:42:55.29333Z","shell.execute_reply":"2021-07-20T03:42:57.136144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, from the above results it is evident that random forest is best model in comparison to others having R2 score of 0.954.","metadata":{}},{"cell_type":"markdown","source":"### Use the Trained Model\n\nFirst, let's save the model.","metadata":{}},{"cell_type":"code","source":"import joblib\n\n# Save the model as a pickle file\nfilename = 'bike-share-rf.pkl'\njoblib.dump(model, filename)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:45:30.838142Z","iopub.execute_input":"2021-07-20T03:45:30.838685Z","iopub.status.idle":"2021-07-20T03:45:30.982351Z","shell.execute_reply.started":"2021-07-20T03:45:30.838631Z","shell.execute_reply":"2021-07-20T03:45:30.981405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can load it whenever we need it, and use it to predict labels for new data. This is often called *scoring* or *inferencing*.","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:45:40.664816Z","iopub.execute_input":"2021-07-20T03:45:40.665146Z","iopub.status.idle":"2021-07-20T03:45:40.671342Z","shell.execute_reply.started":"2021-07-20T03:45:40.665118Z","shell.execute_reply":"2021-07-20T03:45:40.670176Z"}}},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:47:59.163846Z","iopub.execute_input":"2021-07-20T03:47:59.164223Z","iopub.status.idle":"2021-07-20T03:47:59.186441Z","shell.execute_reply.started":"2021-07-20T03:47:59.164189Z","shell.execute_reply":"2021-07-20T03:47:59.185423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model from the file\nloaded_model = joblib.load(filename)\n\n# Create a numpy array containing a new observation (for example tomorrow's seasonal and weather forecast information)\nX_new = np.array([[2.5,2.04,-3.2,1.4,0,1.0,1.3,1.6,1.08,1,0,0,0,0,0,0,1,0,0,0]]).astype('float64')\nprint ('New sample: {}'.format(list(X_new[0])))\n\n# Use the model to predict tomorrow's rentals\nresult = loaded_model.predict(X_new)\nprint('Prediction: {:.0f} shares'.format(np.round(result[0])))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T03:50:54.279195Z","iopub.execute_input":"2021-07-20T03:50:54.279752Z","iopub.status.idle":"2021-07-20T03:50:54.299058Z","shell.execute_reply.started":"2021-07-20T03:50:54.279691Z","shell.execute_reply":"2021-07-20T03:50:54.298036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please Upvote if you like the kernel. Happy Learning.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}