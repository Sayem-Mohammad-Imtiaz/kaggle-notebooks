{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of Content\n\n1. **[Header Files](#lib)**\n2. **[About Data Set](#about)**\n3. **[Data Preparation](#prep)**\n    - 3.1 - **[Read Data](#read)**\n    - 3.2 - **[Analysing Missing Values](#miss)**\n    - 3.3 - **[Analysing Outliers](#outliers)**\n    - 3.4 - **[Analysing the data set](#dt)**\n    - 3.5 - **[Scaling](#scale)**   \n    - 3.6 - **[Encoding](#encode)** \n    \n4. **[Determining Optimal Linkage Method](#ol)**\n5. **[Visualizing the clusters](#vis)**\n6. **[Agglomerative Clustering](#ag)**\n7. **[KMeans Clustering](#kmeans)**\n8. **[Principal Component Analysis](#PCA)**\n9. **[Kernel PCA](#kpca)**\n10. **[Density Based Clustering](#dbscan)**","metadata":{}},{"cell_type":"markdown","source":"<a id='lib'></a>\n## 1. Header Files","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Header Files for Data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Header Files for finding optimal linkage for clustering\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist\n\n# Header files for visualizing the clusters\nfrom scipy.cluster.hierarchy import linkage,dendrogram,cut_tree\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\n# Header Files for Agglomerative Clustering\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Header Files for KMeans Custering\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Header files for dimensionality Reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\n\n# Header files for DBSCan\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='about'></a>\n## 2.About Data Set\n","metadata":{}},{"cell_type":"markdown","source":"Customer ID - Unique identification of customer\n\nGender - Sex of the customer\n\nAge - Age of customer\n\nAnnual Income - Income of salary in 1000's unit Dollars\n\nSpending Score - Readiness of customer to spend money","metadata":{}},{"cell_type":"markdown","source":"<a id='prep'></a>\n## 3.Data Preperation","metadata":{}},{"cell_type":"markdown","source":"<a id='read'></a>\n### 3.1 Read the data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='miss'></a>\n### 3.2 Analysing Missing Values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note- No missing Values in data","metadata":{}},{"cell_type":"markdown","source":"<a id='outliers'></a>\n### 3.3 Analysing Outliers","metadata":{}},{"cell_type":"code","source":"for x in df.select_dtypes(np.number).columns:\n    sns.boxplot(x=df[x])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: Very Few Outliers","metadata":{}},{"cell_type":"markdown","source":"<a id='dt'></a>\n### 3.4 Analysing the data set","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysing range of numerical columns\ndf.describe().T[['min','max']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysing categorical Variables\nfor x in df.select_dtypes(exclude=np.number):\n    print(df[x].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.drop('CustomerID',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Maintaining a copy of the data\ndata=df.copy()\ndf_num=df.select_dtypes(np.number)\ndf_cat=df.select_dtypes(exclude=np.number)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <a id='scale'></a>\n### 3.5 Scaling","metadata":{}},{"cell_type":"code","source":"ss=StandardScaler()\ndf_nums=pd.DataFrame(ss.fit_transform(df_num),columns=df_num.columns)\ndf_nums","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='encode'></a>\n### 3.6 Encoding","metadata":{}},{"cell_type":"code","source":"df1=pd.concat([df_nums,df_cat],axis=1)\ndf1.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed=pd.get_dummies(df1,columns=['Gender'])\ndf_processed.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='ol'></a>\n## 4.Determining Optimal Linkage Method","metadata":{}},{"cell_type":"code","source":"# Method with highest cophenetic score is the optimal linkage method\n\ncoph=dict()\nfor method in ['ward','average','complete','single']:\n    mergings=linkage(df_processed,method=method)\n    c,d=cophenet(mergings,pdist(df_processed))\n    coph[method]=c\nprint(coph)\n\nprint('\\nOptimal Linkage Method:',max(coph))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"<a id='vis'></a>\n## 5.Visualizing the clusters","metadata":{}},{"cell_type":"code","source":"df_cluster=df_processed.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Done to find an approx value of k\nmergings=linkage(df_processed,method='ward',metric='euclidean')\ndendrogram(mergings,truncate_mode='lastp')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note : \n\n1. Determining optimal number of clusters using dendrogram is confusing\n    \n2. High time complexity","metadata":{}},{"cell_type":"code","source":"df_cluster['cluster']=cut_tree(mergings,n_clusters=4)\ndf_cluster.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Analysing The Cluster\ndf_cluster.cluster.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='ag'></a>\n## 6.Agglomerative Clustering","metadata":{}},{"cell_type":"markdown","source":"Logic - Each Observation is a unique cluster at the initial step then iteratively moves to add more similar points to the cluster.This process is continued till all observations are fused to a single cluster\n\nNote - Doesnt work well with very large data(Computational Cost is very high)","metadata":{}},{"cell_type":"code","source":"model=AgglomerativeClustering(n_clusters=4)\nmodel.fit(df_processed)\ndf_cluster['ag_cluster']=model.labels_\nmodel.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cluster.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cluster.ag_cluster.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the cluster\nsns.scatterplot(x=df['Annual Income (k$)'],y=df['Spending Score (1-100)'],hue=df_cluster.ag_cluster)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='kmeans'></a>\n## 7. KMeans Clustering (Lloyds Algorithm)","metadata":{}},{"cell_type":"markdown","source":"Logic: Clusters data by seperating data into groups of equal variance.","metadata":{}},{"cell_type":"markdown","source":"Note: A cluster is said to be a good cluster when\n\n1.Clusters are well packed\n\n2.Clusters are well seperated","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Optimal Value of K for Kmeans clustering","metadata":{}},{"cell_type":"markdown","source":"There are two methods to calculate the optimal value of K \n\n1. Elbow Plot\n\n2. Silhoutte Method","metadata":{}},{"cell_type":"markdown","source":"### 7.1.1 Elbow Plot","metadata":{}},{"cell_type":"markdown","source":"The aim of ploting an elbow plot is to find an optimal value of k such that varience within clusters is lowest and the number of clusters is not too large to interpret.","metadata":{}},{"cell_type":"code","source":"#wcss is within cluster sum of squared errors.\nwcsse=[]\n\nfor k in np.arange(2,8):\n    model=KMeans(n_clusters=k,random_state=5)\n    model.fit(df_processed)\n    wcsse.append(model.inertia_)\n    \nplt.plot(np.arange(2,8),wcsse)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('wcsse')\nplt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The value of k is selelected at the point where an elbow is formed, hence the name elbow plot.(Here 4)","metadata":{}},{"cell_type":"markdown","source":"### 7.1.2 Silhoutte Method","metadata":{}},{"cell_type":"code","source":"score=[]\nfor k in np.arange(2,6):\n    model=KMeans(n_clusters=k,random_state=5)\n    cluster=model.fit_predict(df_processed)\n    score.append(silhouette_score(df_processed,cluster))\n\nplt.plot(np.arange(2,6),score)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('Silhoutte Score')\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The point at which silhouette score is highest is considered as the optimal value for number of clusters.","metadata":{}},{"cell_type":"markdown","source":"### 7.1.3 Silhoutte Visualizer","metadata":{}},{"cell_type":"code","source":"clust_mod=KMeans(n_clusters=3,random_state=5)\nviz=SilhouetteVisualizer(clust_mod)\nviz.fit(df_processed)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clust_mod=KMeans(n_clusters=4,random_state=5)\nviz=SilhouetteVisualizer(clust_mod)\nviz.fit(df_processed)\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clust_mod=KMeans(n_clusters=5,random_state=5)\nviz=SilhouetteVisualizer(clust_mod)\nviz.fit(df_processed)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.1.4 KMeans Model","metadata":{}},{"cell_type":"code","source":"model=KMeans(n_clusters=4,random_state=5)\ncluster=model.fit_predict(df_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the cluster\nsns.scatterplot(x=df['Annual Income (k$)'],y=df['Spending Score (1-100)'],hue=cluster)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the middle there are is no clear seperation in the clusters(Could be because the two features selected at random doesnot explain maximum variance).\n\nDimensionality reduction techniques like PCA or KPCA can be used to find the best vectors to represent clusters in lower dimensions.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='PCA'></a>\n## 8. Principal Component Analysis","metadata":{}},{"cell_type":"markdown","source":"PCA is a method used to represent the data in lower dimensions by creating new features that capture maximum variance.","metadata":{}},{"cell_type":"code","source":"pca=PCA()\npca.fit(df_processed)\nnp.cumsum(pca.explained_variance_ratio_)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca=pd.DataFrame(pca.transform(df_processed))\ncol=['PC'+str(x) for x in np.arange(1,6)]\ndf_pca.columns=col\ndf_pca.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding optimal number of clusters\nwcsse=[]\n\nfor k in np.arange(2,8):\n    model=KMeans(n_clusters=k,random_state=5)\n    model.fit(df_pca)\n    wcsse.append(model.inertia_)\n    \nplt.plot(np.arange(2,8),wcsse)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('wcsse')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=KMeans(n_clusters=4,random_state=5)\ncluster=model.fit_predict(df_pca)\n\n# Visualizing the cluster\n\nsns.scatterplot(x=df_pca['PC1'],y=df_pca['PC2'],hue=cluster)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='kpca'></a>\n## 9. Kernel PCA","metadata":{}},{"cell_type":"markdown","source":"Kernel PCA uses a function to project non linear data onto a higher dimension inorder to make it linearly seperable and then uses PCA.","metadata":{}},{"cell_type":"code","source":"kpca= KernelPCA(n_components=2)\nkpca.fit(df_processed)\ndf_kpca=pd.DataFrame(kpca.fit_transform(df_processed),columns=['PC1','PC2'])\ndf_kpca.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding optimal number of clusters\nwcsse=[]\n\nfor k in np.arange(2,8):\n    model=KMeans(n_clusters=k,random_state=5)\n    model.fit(df_kpca)\n    wcsse.append(model.inertia_)\n    \nplt.plot(np.arange(2,8),wcsse)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('wcsse')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=KMeans(n_clusters=4,random_state=5)\ncluster=model.fit_predict(df_kpca)\nsns.scatterplot(x=df_kpca['PC1'],y=df_kpca['PC2'],hue=cluster)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When compared to the results of pca at the edges the clusters are not overlapping . But the main application of KPCA is when the points are not linearly seperable.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='dbscan'></a>\n## 9. Density Based Clustering (DBScan)","metadata":{}},{"cell_type":"markdown","source":"DBScan forms clusters of non linear shapes. The main application of DBScan is in outlier detection. The regions are not densely populated are considered to be outliers.","metadata":{}},{"cell_type":"markdown","source":"DBScan has 2 main parameters to be considered : \n\n1. eps - Radius of neighbourhood of a data point\n\n2. min_samples - Number of points inside epsilon neighborhood to be considered as a core point","metadata":{}},{"cell_type":"markdown","source":"### 9.1 Finding optimal value of epsilon","metadata":{}},{"cell_type":"code","source":"nn=NearestNeighbors(n_neighbors=4)\nnn.fit(df_processed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distance,index=nn.kneighbors(df_processed)\nplt.plot(np.sort(distance[:,3]))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.2 DBScan ","metadata":{}},{"cell_type":"code","source":"df_db=df_kpca.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=DBSCAN(eps=0.9,min_samples=5)\ndf_db['cluster']=model.fit_predict(df_processed)\ndf_db.cluster.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-1 represents outliers","metadata":{}},{"cell_type":"code","source":"# Visualizing the cluster to identify outliers\nsns.scatterplot(x=df_db['PC1'],y=df_kpca['PC2'],hue=df_db['cluster'],palette=['Red','Yellow','Pink'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Points in Light Red are the outliers","metadata":{}},{"cell_type":"code","source":"# Removing the identified outliers\ndf_db=df_db[df_db.cluster != -1]\nsns.scatterplot(x=df_db['PC1'],y=df_kpca['PC2'],hue=df_db['cluster'],palette=['Yellow','Pink'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}