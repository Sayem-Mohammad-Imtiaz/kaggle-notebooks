{"cells":[{"metadata":{},"cell_type":"markdown","source":"# For Beginners - Lets take a look at that rain dataset"},{"metadata":{},"cell_type":"markdown","source":"Any exploratory analysis should start with a clear understanding about the goals that you wish to achieve. So lets define our key objectives..\n\n-  Clean up the data set\n-  Feature selection\n-  Establish various models to predict rain tomorrow\n-  Visualize the best models and results\n\n### Lets check what we have and clean the data set:  \" Data Pre-processing\" "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the required modules for you analysis\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn import preprocessing # To get MinMax Scaler function\nfrom sklearn.model_selection import train_test_split # required to split the data\nfrom sklearn.feature_selection import SelectKBest # to select best feature\nfrom sklearn.feature_selection import chi2 # for feature Selection\nfrom sklearn.metrics import classification_report # to extract classsification report\nfrom sklearn.metrics import confusion_matrix # to extract confusion matrix\nfrom sklearn.metrics import accuracy_score # to get the scores\nfrom sklearn.linear_model import LogisticRegression # for logistic regression\nfrom sklearn.ensemble import RandomForestClassifier # for random forrest \nfrom sklearn.naive_bayes import GaussianNB # for Naive Bayes\nfrom sklearn.feature_selection import RFE # Recursive Feature Selection\nfrom sklearn.feature_selection import RFECV # Recursive Feature Selection with Cross Validation\nfrom sklearn.decomposition import PCA # To apply PCA\n\n# To plot inline\n%matplotlib inline\n\n# Load the csv to a dataframe\ndf = pd.read_csv('../input/weatherAUS.csv')\n\n# Let us see whats inside..\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, that we have a glimspe of the dataset lets drop the **RISK_MM** column as recommended by the author of the dataset. Using **RISK_MM** in our analysis might leak unwanted noise to our data..\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop RISK_MM\ndf = df.drop(columns=['RISK_MM'],axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also drop columns with location infromation and non-decisive data which contributes very little to the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping columns\ndf = df.drop(columns=['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'],axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a look at the columns with missing info"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for any null\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its quite clear that columns like Sunshine, Evaporation, Cloud3pm and Cloud9am has considerable amount missing information. It wont be a good idea to use these columns in our analysis. Lets drop them "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns=['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am'],axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a few catagoerical columns, lets convert and assign integer values --> Yes will be 1 and No will be 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# substituting 1 and 0 inplace of yes and no\ndf['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndf['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most important piece of any analysis is handling missing data. When done correctly it will contribute in training a much more accurate model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets take a look at the values from each column\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Since only less than 1% of following columns are missing values, lets just replace these with the mean.\n# (You can also use the imput pre-processing class to do this)\ndf.MinTemp.fillna(df.MinTemp.mean(),inplace=True)\ndf.MaxTemp.fillna(df.MaxTemp.mean(),inplace=True)\ndf.Rainfall.fillna(df.Rainfall.mean(),inplace=True)\ndf.WindSpeed9am.fillna(df.WindSpeed9am.mean(),inplace=True)\ndf.WindSpeed3pm.fillna(df.WindSpeed3pm.mean(),inplace=True)\ndf.Humidity9am.fillna(df.Humidity9am.mean(),inplace=True)\ndf.Temp9am.fillna(df.Temp9am.mean(),inplace=True)\ndf.Temp3pm.fillna(df.Temp3pm.mean(),inplace=True)\ndf.RainToday.fillna(df.RainToday.mean(),inplace=True)\ndf.Humidity3pm.fillna(df.Humidity3pm.mean(),inplace=True)\n\n# And for columns with close to 5% missing values lets randomly fill them with values close to the mean value but \n# within one standard deviation.\nWindGustSpeed_avg = df['WindGustSpeed'].mean()\nWindGustSpeed_std = df['WindGustSpeed'].std()\nWindGustSpeed_null_count = df['WindGustSpeed'].isnull().sum()\nWindGustSpeed_null_random_list = np.random.randint(WindGustSpeed_avg - WindGustSpeed_std, WindGustSpeed_avg + WindGustSpeed_std, size=WindGustSpeed_null_count)\ndf['WindGustSpeed'][np.isnan(df['WindGustSpeed'])] = WindGustSpeed_null_random_list\n\nPressure9am_avg = df['Pressure9am'].mean()\nPressure9am_std = df['Pressure9am'].std()\nPressure9am_null_count = df['Pressure9am'].isnull().sum()\nPressure9am_null_random_list = np.random.randint(Pressure9am_avg - Pressure9am_std, Pressure9am_avg + Pressure9am_std, size=Pressure9am_null_count)\ndf['Pressure9am'][np.isnan(df['Pressure9am'])] = Pressure9am_null_random_list\n\nPressure3pm_avg = df['Pressure3pm'].mean()\nPressure3pm_std = df['Pressure3pm'].std()\nPressure3pm_null_count = df['Pressure3pm'].isnull().sum()\nPressure3pm_null_random_list = np.random.randint(Pressure3pm_avg - Pressure3pm_std, Pressure3pm_avg + Pressure3pm_std, size=Pressure3pm_null_count)\ndf['Pressure3pm'][np.isnan(df['Pressure3pm'])] = Pressure3pm_null_random_list\n\n\n#replace negative values with 0\ndf.clip(lower=0)\ndf[df < 0] = 0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the best coulmns that conrtibute to rain tomorrow : 'Feature Selection'\n\nFrom sklearn we can use the SelectKBest to find the best features available to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From selectkbest find the best score and plot it\nfrom sklearn.feature_selection import SelectKBest, chi2\nX = df.loc[:,df.columns!='RainTomorrow']\ny = df[['RainTomorrow']]\nselector = SelectKBest(chi2, k=3)\nselector.fit(X, y)\nX_new = selector.transform(X)\nscores = selector.scores_\nprint(X.columns[selector.get_support(indices=True)]) #top 3 columns\n\n# Plot the scores. Lets see if we can visualise which are the best?\nplt.bar(range(len(X.columns)), scores)\nplt.xticks(range(len(X.columns)), X.columns, rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also visualize the correlation between the columns. Based on the resulting heat map, we can also try to find the varriables which might have a direct or partial relation with our target varriable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the corealation heat map \nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(df.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training through varrious models : \"Model Selection\"\n\nAt this point since we have an idea of the features that contribute to our target value lets train and fit our data through various models.\n\n#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the required columns for testing and training\nX = df.loc[:, ['Rainfall', 'Humidity3pm', 'Humidity9am', 'WindGustSpeed', 'Temp3pm', 'MaxTemp']].shift(-1).iloc[:-1].values\ny = df.iloc[:-1, -1:].values.astype('int')\n\n# Logistic Regression \n# Split the data to appropriate testing sample size\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\nmodel_lr = LogisticRegression(random_state=0)\nmodel_lr.fit(X_train,y_train)\nprediction_lr = model_lr.predict(X_test)\nscore = accuracy_score(y_test,prediction_lr)\nprint('Accuracy - Logistic Regression:',score)\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, prediction_lr)\n\n# Ploting the Confusion Matrix\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(cm,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forrest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n\n# Random Forrest\nmodel_rf = RandomForestClassifier(n_estimators= 25, max_depth= None,max_features = 0.4,random_state= 11 )\n# Fitting the data\nmodel_rf.fit(X_train, y_train)\n# Making a prediction\nprediction_rf =model_rf.predict(X_test)\nscore = accuracy_score(y_test,prediction_rf)\nprint('Accuracy - Random Forrest:',score)\nprint(classification_report(y_test, prediction_rf))\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, prediction_rf)\n\n# Plotting the data\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(cm,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data    \nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n\n# Gaussian Navie Bayes\nmodel_gNB = GaussianNB()\nmodel_gNB.fit(X_train,y_train)\nprediction_gNB = model_gNB.predict(X_test)\nscore = accuracy_score(y_test,prediction_gNB)\nprint('Accuracy - GaussianNB:',score)\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, prediction_gNB)\n\n# Plotting the matrix\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(cm,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}