{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.isri import ISRIStemmer\n\n!pip install Tashaphyne\nimport pyarabic.arabrepr\nfrom tashaphyne.stemming import ArabicLightStemmer\n\nimport random\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words =['من',\n 'في',\n 'على',\n 'و',\n 'فى',\n 'يا',\n 'عن',\n 'مع',\n 'ان',\n 'هو',\n 'علي',\n 'ما',\n 'اللي',\n 'كل',\n 'بعد',\n 'ده',\n 'اليوم',\n 'أن',\n 'يوم',\n 'انا',\n 'إلى',\n 'كان',\n 'ايه',\n 'اللى',\n 'الى',\n 'دي',\n 'بين',\n 'انت',\n 'أنا',\n 'حتى',\n 'لما',\n 'فيه',\n 'هذا',\n 'واحد',\n 'احنا',\n 'اي',\n 'كده',\n 'إن',\n 'او',\n 'أو',\n 'عليه',\n 'ف',\n 'دى',\n 'مين',\n 'الي',\n 'كانت',\n 'أمام',\n 'زي',\n 'يكون',\n 'خلال',\n 'ع',\n 'كنت',\n 'هي',\n 'فيها',\n 'عند',\n 'التي',\n 'الذي',\n 'قال',\n 'هذه',\n 'قد',\n 'انه',\n 'ريتويت',\n 'بعض',\n 'أول',\n 'ايه',\n 'الان',\n 'أي',\n 'منذ',\n 'عليها',\n 'له',\n 'ال',\n 'تم',\n 'ب',\n 'دة',\n 'عليك',\n 'اى',\n 'كلها',\n 'اللتى',\n 'هى',\n 'دا',\n 'انك',\n 'وهو',\n 'ومن',\n 'منك',\n 'نحن',\n 'زى',\n 'أنت',\n 'انهم',\n 'معانا',\n 'حتي',\n 'وانا',\n 'عنه',\n 'إلي',\n 'ونحن',\n 'وانت',\n 'منكم',\n 'وان',\n 'معاهم',\n 'معايا',\n 'وأنا',\n 'عنها',\n 'إنه',\n 'اني',\n 'معك',\n 'اننا',\n 'فيهم',\n 'د',\n 'انتا',\n 'عنك',\n 'وهى',\n 'معا',\n 'آن',\n 'انتي',\n 'وأنت',\n 'وإن',\n 'ومع',\n 'وعن',\n 'معاكم',\n 'معاكو',\n 'معاها',\n 'وعليه',\n 'وانتم',\n 'وانتي',\n '¿',\n '|']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"   \ndef normalize(sentence):\n    '''\n    Argument:\n        string of words\n    return:\n        string of words but standardize the words\n    '''\n    sentence = re.sub(\"[إأآا]\", \"ا\", sentence)\n    sentence = re.sub(\"ى\", \"ي\", sentence)\n    sentence = re.sub(\"ؤ\", \"ء\", sentence)\n    sentence = re.sub(\"ئ\", \"ء\", sentence)\n    sentence = re.sub(\"ة\", \"ه\", sentence)\n    sentence = re.sub(\"گ\", \"ك\", sentence)\n    return sentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removing_ar_stopwords(text):\n    \"\"\"\n        Here we remove all Arabic stop words\n        \n    \"\"\"\n      # if read it from file\n#     ar_stopwords_list = open(\"arabic_stopwords.txt\", \"r\") \n#     stop_words = ar_stopwords_list.read().split(\"\\n\")\n#     stop_words = []\n    original_words = []\n    words = word_tokenize(text) # it works on one hadith not list\n    for word in words:\n        if word not in stop_words:\n            original_words.append(word)\n    filtered_sentence = \" \".join(original_words)\n    return filtered_sentence\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clearReg(text):\n    \"\"\"\n        This function for getting the normal values of out of lemmatization function\n        that takse a string of dict as a \n        takes  : '{\"result\":[\"امر\",\"ب\",\"أخذ\",\"ما\",\"نهى\",\"ه\",\"انتهى\"]}'\n        return : ['امر أخذ ما نهى انتهى']\n    \"\"\"\n    each_lemma_word = []\n    each_lemma_sentence = []\n    for hadith in text:\n        matches = re.findall(r'\\\"(.+?)\\\"',hadith)\n        for word in matches:\n            if len(word) >= 2 and word !='result':\n                each_lemma_word.append(word)\n        each_lemma_sentence.append(\" \".join(each_lemma_word))\n        each_lemma_word.clear()\n    return each_lemma_sentence\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndef stemming_1(text):\n    \"\"\"\n        This is first functoin for stemming and it's looks not good accurac, NLTK by ISRIStemmer.\n    \"\"\"\n    st = ISRIStemmer()\n    stemmend_words = []\n    words = word_tokenize(text)\n    for word in words:\n        stemmend_words.append(st.stem(word))\n    stemmed_sentence = \" \".join(stemmend_words)\n    return stemmed_sentence\n        \n    \n    \ndef stemming_2(text):\n    \"\"\"\n        This is Second functoin for stemming and it's looks good results, with built in library called Tashaphyne.\n        The documentation here ==> https://pypi.org/project/Tashaphyne/\n    \n    \"\"\"\n    import pyarabic.arabrepr\n    arepr = pyarabic.arabrepr.ArabicRepr()\n    repr = arepr.repr\n\n    from tashaphyne.stemming import ArabicLightStemmer\n    ArListem = ArabicLightStemmer()\n\n    hadiths_without_stop_words_and_with_normalization_and_with_stemming = []\n\n    for hadith in hadiths_without_stop_words_and_with_normalization:\n        words = word_tokenize(hadith)\n        new_list = []\n        for word in words:\n            stem = ArListem.light_stem(word)\n            stem = ArListem.get_stem()\n            new_list.append(stem)\n\n        hadith_sentence_with_stemming = \" \".join(new_list)\n        hadiths_without_stop_words_and_with_normalization_and_with_stemming.append(hadith_sentence_with_stemming)\n        \n    return hadiths_without_stop_words_and_with_normalization_and_with_stemming\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(text):\n    \"\"\"\n        This function for lemma Arabic words by API, and it getting best result of the previous functions\n        return a string dictinary like exactly '{\"result\":[\"امر\",\"ب\",\"أخذ\",\"ما\",\"نهى\",\"ه\",\"انتهى\"]}'\n    \"\"\"\n    import http.client\n    conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\")\n    hadith_dict = {}\n    list_pyload_input = []\n    list_pyload_out = []\n    length = len(text)\n    for h in text[:length]:\n        q = '{\"text\":'+'\"{}\"'.format(h)+'}'\n        list_pyload_input.append(q)\n    headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\", }\n    for h in list_pyload_input:\n        conn.request(\"POST\", \"/msa/webapi/lemma\", h.encode('utf-8'), headers)\n        res = conn.getresponse()\n        data = res.read()\n        list_pyload_out.append(data.decode(\"utf-8\"))\n        final_result = clearReg(list_pyload_out)     # call clearReg for clean the text\n    return final_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **`Stemming`** : algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.\n* **`Lemmatization`** : takes into consideration the morphological analysis of the words. \n\n**Lemmatization is typically seen as much more informative than simple stemming, because stem may not be an actual word whereas lemma is an actual language word.**"},{"metadata":{},"cell_type":"markdown","source":"#### Upove some functions for like:\n* stemming_1 by `ISRIStemmer from NLTK`.\n* stemming_2 by `Tashaphyne` is an Arabic light stemmer(removing prefixes and suffixes) and give all possible segmentations. \n* lemmatization by [Farasa API](https://alt.qcri.org/farasa/)\n\n**By the experimental: lemmatization by Farasa have a good results.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_1 = pd.read_csv(dirname + '/Maliks Muwatta Without_Tashkel.csv')\ndata_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_hadiths_1 = []\n\nfor hadith in data_1['Maliks Muwatta Without_Tashkel']:\n    all_hadiths_1.append(hadith)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Maliks Muwatta\ncleared_Hadith_1 = []           # Removing stopwords\ncleared_Hadith_1_2 = []         # Normalization\ncleared_Hadith_1_2_3 = []       # Lematization\n\nfor hadith in all_hadiths_1:\n    cleared_Hadith_1.append(removing_ar_stopwords(hadith))         # Removing stopwords\nfor hadith in cleared_Hadith_1:\n    cleared_Hadith_1_2.append(normalize(hadith))                   # Normalization\ncleared_Hadith_1_2_3 = lemmatization(cleared_Hadith_1_2)           # Lematization\n\nprint('The size of data:')\nlen(cleared_Hadith_1), len(cleared_Hadith_1_2), len(cleared_Hadith_1_2_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It takes ~3min.\n\nI make just one Hadith book to showing you the steps, so you can make the same for others books."},{"metadata":{"trusted":true},"cell_type":"code","source":"cleared_Hadith_1_2_3[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make it as a DataFram \nMaliks_Muwatta_preprosessing_1 = pd.DataFrame(cleared_Hadith_1_2_3, columns=['Maliks_Muwatta_Preprosessing_Cleaned'])\nMaliks_Muwatta_preprosessing_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}