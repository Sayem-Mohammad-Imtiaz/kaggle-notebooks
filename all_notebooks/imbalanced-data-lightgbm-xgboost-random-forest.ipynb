{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Data Description \n\n* emp_length_int : Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n* home_ownership : The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.\n* income_category : Categorized Income (Low, Medium, High) \n* annual_inc : The self-reported annual income provided by the borrower during registration.\n\n* loan_amount : The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n* term : The number of payments on the loan. Values are in months and can be either 36 or 60.\n\n* application_type : Indicates whether the loan is an individual application or a joint application with two co-borrowers\n* purpose : A category provided by the borrower for the loan request.\n* interest_payments : ;;\n* loan_condition : Condition of the Loan [TARGET] (Good Loan = 0 , Bad Loan = 1)\n* interest_rate :  Interest Rate on the loan\n* grade : LC assigned loan grade\n* dti : A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, - - - excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n* total_pymnt : Payments received to date for total amount funded\n* total_rec_prncp : Principal received to date\n* recoveries : post charge off gross recovery\n* installment : The monthly payment owed by the borrower if the loan originates.\n* region : region of Loan being executed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data Description (Korean)\n* year : 대출 발생 연도\n* issue_d : 대출 발생 일자\n* final_d : 마지막 거래일자\n* emp_length_int : \"근속년수. 0은 1년 미만, 10은 10년 이상\"\n* home_ownership : \"등록 시 대출자에게서 제공된 집 보유 상태. RENT(대여) = 1, OWN(소유) = 2, MORTAGE(담보대출) = 3 \"\n* home_ownership_cat : ;;\n* income_category : \"수익 Low = 1, Medium = 2, High = 3 으로 분류\"\n* annual_inc : 등록시 대출자에게서 제공된 연간 소득\n* income_cat : ;;\n* loan_amount : 대출금액(달러)\n* term : \"대출기간(36개월 = 1, 60개월 = 2)\"\n* term_cat : ;;\n* application_type : \"개인 대출 신청(=1) 인지, 2명의 대출자에 의해 공동으로 신청된 대출 신청 (=2)인지 여부\"\n* application_type_cat : ;;\n* purpose : 대출이유\n* purpose_cat : \"대출용도(빛 청산, 카드 대금 결제, 집 개발 등등) \n[credit_card = 1, car = 2, small_business = 3, other = 4, wedding = 5, debt_consolidation =6, \nhome_improvement = 7, major_purchase = 8, medical = 9, moving = 10, vacation = 11, house =12,\nrenewable_energy = 13,  educational = 14]\"\n* interest_payments : \"이자 지불? (Low = 1, High =2 로 분류)\"\n* interest_payments_cat : ;;\n* loan_condition : 대출의 상태(TARGET) (Good Loan = 0 , Bad Loan = 1)\n* loan_condition_cat : ;;\n* interest_rate : 대출의 이자율\n* grade : 대출 등급 ( A ~ G , 1~7)\n* grade_cat : ;;\n* dti : 금융부채 상환능력을 소득으로 따져서 대출한도를 정하는 계산비율\n* total_pymnt : 총 상환금액\n* total_rec_prncp : ???\n* recoveries : 회수\n* installment : 분할 불입금\n* region : 거래지역","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score \nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report \nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport shap\nimport joblib\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold , cross_val_score\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"fork from previous EDA kernel https://www.kaggle.com/possiblemanjr/handling-imbalanced-data-eda-small-fe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_pickle(\"../input/handling-imbalanced-data-eda-small-fe/df_for_use.pkl\")\ndf_fe = pd.read_pickle(\"../input/handling-imbalanced-data-eda-small-fe/df_fe.pkl\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Utilities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}'.format(accuracy , precision, recall, f1))\n    print('------------------------------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = {0.1,0.15, 0.2,0.25, 0.3,0.35, 0.4 , 0.45 , 0.5}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train_test_split (Stratify)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NGboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install --upgrade git+https://github.com/stanfordmlgroup/ngboost.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from ngboost import NGBRegressor, NGBClassifier\n# from ngboost.ngboost import NGBoost\n# from ngboost.learners import default_tree_learner\n# from ngboost.scores import CRPS, MLE , LogScore\n# from ngboost.distns import LogNormal, Normal\n# from ngboost.distns import k_categorical, Bernoulli","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start = time.time()\n\n# ngb_clf = NGBClassifier(Dist=k_categorical(2), verbose=10 ,Score=LogScore, random_state = 2020 )\n# ngb_clf.fit(X_train, y_train , early_stopping_rounds=100)\n\n\n# ngb_runtime = time.time() - start\n\n\n\n# # test roc_auc_score\n# get_eval_by_threshold(y_test, ngb_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\n# NGb_roc_score = roc_auc_score(y_test, ngb_clf.predict_proba(X_test)[:,1], average = 'macro')\n\n\n\n# print( 'NGboost_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(NGb_roc_score ,ngb_runtime ))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##RandomForest with stratified 5 Fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = 10\nmax_features = 'auto'\nmax_depth = None\nmin_samples_split = 2\nmin_samples_leaf = 1\nmin_weight_fraction_leaf = 0.0\nmax_leaf_nodes = None\nbootstrap = True\noob_score = False\nn_jobs = -1\nrandom_state = 2018\nclass_weight = 'balanced'\n\nRFC = RandomForestClassifier(n_estimators=n_estimators, \n        max_features=max_features, max_depth=max_depth,\n        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, \n        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n        class_weight=class_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##RandomForest with stratified 5 Fold\n\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                        index=y_train.index,columns=[0,1])\n\n\nclf = RandomForestClassifier(n_estimators=30, min_samples_leaf=20, max_features=0.7, n_jobs=-1, random_state = 2020, oob_score=True)\ncv = StratifiedKFold(n_splits=5,random_state = 2020)\ny_preds_rf = np.zeros(X_test.shape[0])\nn_iter = 0 \nfor train_index,test_index in cv.split(X_train,y_train):\n    trx , tsx = X_train.iloc[train_index] , X_train.iloc[test_index]\n    vly , vlt = y_train.iloc[train_index] , y_train.iloc[test_index]\n    RFC = RFC.fit(trx,vly)   \n    loglossTraining = log_loss(vly, \\\n                                RFC.predict_proba(trx)[:,1])\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[tsx.index,:] = \\\n        RFC.predict_proba(tsx)[:,1]  \n    loglossCV = log_loss(vlt, \\\n        predictionsBasedOnKFolds.loc[tsx.index,1])\n    cvScores.append(loglossCV)\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \n    n_iter += 1\n    cv_roc_score = roc_auc_score(y_test, RFC.predict_proba(X_test)[:,1], average = 'macro')\n    cv_precision, cv_recall, _ = precision_recall_curve(y_test,RFC.predict_proba(X_test)[:,1])\n    cv_pr_auc = auc(cv_recall, cv_precision)\n    print( '\\n#{0}, CV_ROC_AUC : {1} , RF_CV_PR_AUC : {2} '.format(n_iter ,cv_roc_score, cv_pr_auc))\n    y_preds_rf += RFC.predict_proba(X_test)[:,1]/ cv.n_splits\n\nrf_cv_roc_score = roc_auc_score(y_test, y_preds_rf, average = 'macro')\nrf_cv_precision, rf_cv_recall, _ = precision_recall_curve(y_test,y_preds_rf)\nrf_cv_pr_auc = auc(rf_cv_recall, rf_cv_precision)    \nloglossRandomForestsClassifier = log_loss(y_train, \n                                          predictionsBasedOnKFolds.loc[:,1])\nprint('Random Forests Log Loss: ', loglossRandomForestsClassifier)\n    \nprint( 'RF_cv_ROC_AUC : {0:.4f} , RF_cv_PR_AUC : {1:.4f} '.format(rf_cv_roc_score ,rf_cv_pr_auc ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsRandomForests = preds.copy()\n\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],\n                                                       preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],\n                                            preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n          Area under the curve = {0:0.2f}'.format(\n          areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xGB = {\n    'nthread':16, # 코어 수\n    'gamma': 0, # 감마 : 범위 (0 ~ 무한대) , 디폴트 0\n        # 이 값이 높으면 복잡성이 감소(편향 증가, 변동 감소) \n    'max_depth': 6, # max_depth : 범위 (1 ~ 무한대) , 디폴트 6 ## 트리의 최대 깊이\n    'min_child_weight': 1, # min_child_weight : 범위 (0 ~ 무한대) , 디폴트 1 ## 자식노드에 필요한 가중치의 최소 합계\n    'max_delta_step': 0, # max_delta_step : 범위 (0 ~ 무한대) ,  디폴트 0 ## 각 트리의 가중치 추정을 위한 최대 델타 단계\n    'subsample': 1.0, # subsample : 범위 (0 ~ 1) , 디폴트 1\n        # 훈련 데이터의 샘플링 비율\n    'colsample_bytree': 1.0, # colsample_bytree : 범위 (0 ~ 1) , 디폴트 1\n        # 훈련 피쳐의 샘플링 비율\n    'objective':'binary:logistic',\n    'num_class':1,\n    'eval_metric':'logloss',\n    'seed':2020,\n    'tree_method' : 'gpu_hist',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                    index=y_train.index,columns=['prediction'])\nk_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2020)\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)\n    dCV = xgb.DMatrix(data=X_cv_fold)\n    \n    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000, \n                 nfold=5, early_stopping_rounds=200, verbose_eval=50)\n    \n    # 수정 사항 : np.arrary 로 재정의 하면서 경고 메세지를 지울 수 있음\n    best_rounds = np.argmin(np.array(bst['test-logloss-mean']))\n    bst = xgb.train(params_xGB, dtrain, best_rounds)\n    \n    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        bst.predict(dCV)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossXGBoostGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('XGBoost Gradient Boosting Log Loss: ', loglossXGBoostGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n        Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n#     'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2020,\n    'verbose': 50,\n    'num_threads':16,\n    'random_state ' : 2020\n}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=10000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200 , verbose_eval = 500)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('LightGBM Gradient Boosting Log Loss: ', loglossLightGBMGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsTestSetRandomForests = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetRandomForests.loc[:,'prediction'] = \\\n    RFC.predict_proba(X_test)[:,1]\nlogLossTestSetRandomForests = \\\n    log_loss(y_test, predictionsTestSetRandomForests)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsTestSetXGBoostGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\ndtest = xgb.DMatrix(data=X_test)\npredictionsTestSetXGBoostGradientBoosting.loc[:,'prediction'] = \\\n    bst.predict(dtest)\nlogLossTestSetXGBoostGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetXGBoostGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsTestSetLightGBMGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLightGBMGradientBoosting.loc[:,'prediction'] = \\\n    gbm.predict(X_test, num_iteration=gbm.best_iteration)\nlogLossTestSetLightGBMGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetLightGBMGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Log Loss of Random Forests on Test Set: \", \\\n          logLossTestSetRandomForests)\nprint(\"Log Loss of XGBoost Gradient Boosting on Test Set: \", \\\n          logLossTestSetXGBoostGradientBoosting)\nprint(\"Log Loss of LightGBM Gradient Boosting on Test Set: \", \\\n          logLossTestSetLightGBMGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetRandomForests)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetRandomForests)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(y_test,predictionsTestSetRandomForests)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetXGBoostGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetLightGBMGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 앙상블","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)\npredictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(predictionsBasedOnKFoldsRandomForests['prediction'] \\\n    .astype(float),how='left').join( \\\n    predictionsBasedOnKFoldsXGBoostGradientBoosting['prediction'].astype(float), \\\n    how='left',rsuffix=\"2\").join( \\\n    predictionsBasedOnKFoldsLightGBMGradientBoosting['prediction'].astype(float), \\\n    how='left',rsuffix=\"3\")\npredictionsBasedOnKFoldsFourModels.columns = \\\n    ['predsRF','predsXGB','predsLightGBM']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trainWithPredictions = \\\n    X_train.merge(predictionsBasedOnKFoldsFourModels,\n                  left_index=True,right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2018,\n    'verbose': 0,\n    'num_threads':16\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFoldsEnsemble = \\\n    pd.DataFrame(data=[],index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = \\\n        X_trainWithPredictions.iloc[train_index,:], \\\n        X_trainWithPredictions.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=10000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200 ,verbose_eval = 500)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n        gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossEnsemble = log_loss(y_train, \\\n        predictionsBasedOnKFoldsEnsemble.loc[:,'prediction'])\nprint('Ensemble Log Loss: ', loglossEnsemble)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Feature importances:', list(gbm.feature_importance()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([y_train,predictionsBasedOnKFoldsEnsemble.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterData = predictionsTestSetLightGBMGradientBoosting.join(y_test,how='left')\nscatterData.columns = ['Predicted Probability','True Label']\nax = sns.regplot(x=\"True Label\", y=\"Predicted Probability\", color='k', \n                 fit_reg=False, scatter_kws={'alpha':0.1},\n                 data=scatterData).set_title( \\\n                'Plot of Prediction Probabilities and the True Label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatterDataMelted = pd.melt(scatterData, \"True Label\", \\\n                            var_name=\"Predicted Probability\")\nscatterDataMelted.head()\nax = sns.stripplot(x=\"value\", y=\"Predicted Probability\", \\\n                   hue='True Label', jitter=0.4, \\\n                   data=scatterDataMelted).set_title( \\\n                   'Plot of Prediction Probabilities and the True Label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## XGboost with Stratified 5 Fold\n\nval = np.zeros(X_train.shape[0])\npred = np.zeros(X_test.shape[0])\nfolds = StratifiedKFold(n_splits=5, random_state=2020)\n\nmodel_xgb =  xgb.XGBClassifier(\n                              n_estimators=10000,\n                              objective='binary:logistic', \n\n                              verbosity =1,\n                              eval_metric  = 'aucpr',\n                              tree_method='gpu_hist',\n                              random_state = 2020, \n                              n_jobs=-1)\n\nfor fold_index, (train_index,val_index) in enumerate(folds.split(X_train,y_train)):\n    print('Batch {} started...'.format(fold_index))\n    gc.collect()\n    bst = model_xgb.fit(X_train.iloc[train_index],y_train.iloc[train_index],\n              eval_set = [(X_train.iloc[val_index],y_train.iloc[val_index])],\n              early_stopping_rounds=200,\n              verbose= 100, \n              eval_metric ='aucpr'\n              )\n\n    pred += bst.predict_proba(X_test)[:,1]/folds.n_splits\n\nxgb_cv_roc_score = roc_auc_score(y_test, pred, average = 'macro')\nxgb_cv_precision, xgb_cv_recall, _ = precision_recall_curve(y_test,pred)\nxgb_cv_pr_auc = auc(xgb_cv_recall, xgb_cv_precision)\n\n\n\nprint( 'XGboost_cv_ROC_AUC : {0:.4f} , XGboost_cv_PR_AUC : {1:.4f} '.format(xgb_cv_roc_score ,xgb_cv_pr_auc ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###LightGBM with Stratified 5 Fold\n\nX = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n\n\nfrom lightgbm import LGBMClassifier\n\nfrom time import time\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'binary_logloss',\n            'metric_freq' : 50,\n            'max_depth' :4, \n            'num_leaves' : 31,\n            'learning_rate' : 0.01,\n            'feature_fraction' : 1.0,\n            'bagging_fraction' : 1.0,\n            'bagging_freq' : 0,\n            'bagging_seed' : 2020,\n            'num_threads' : 16\n           }\n\nk_fold=5\nkf=StratifiedKFold(n_splits=k_fold,shuffle=True, random_state=2020)\ntraining_start_time = time()\naucs=[]\ny_preds = np.zeros(X_test.shape[0])\n\nfor fold, (trn_idx,val_idx) in enumerate(kf.split(X_train,y_train)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n    clf = lgb.train(params_lgb, trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds=200)\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    y_preds += clf.predict(X_test) / 5\n    \n    \n    \nprint('-' * 30)\nprint('Training is completed!.')\n# print(\"\\n## Mean CV_AUC_Score : \", np.mean(aucs))\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print(clf.best_params_)\nprint('-' * 30)\n\n\n\nlgbm_cv_roc_score = roc_auc_score(y_test, y_preds, average = 'macro')\nlgbm_cv_precision, lgbm_cv_recall, _ = precision_recall_curve(y_test,y_preds)\nlgbm_cv_pr_auc = auc(lgbm_cv_recall, lgbm_cv_precision)\n\n\n\nprint( 'LightGBM_cv_ROC_AUC : {0:.4f} , LightGBM_cv_PR_AUC : {1:.4f} '.format(lgbm_cv_roc_score ,lgbm_cv_pr_auc ))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results\n\nprint( 'RandomForest_ROC_AUC : {0:.4f} , RandomForest_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(rf_roc_score ,rf_pr_auc, RF_runtime ))\nprint( 'XGboost_gpu_ROC_AUC : {0:.4f} , XGboost_gpu_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(xgb_gpu_roc_score ,xgb_gpu_pr_auc, xgb_gpu_runtime ))\nprint( 'XGboost_cpu_ROC_AUC : {0:.4f} , XGboost_cpu_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(xgb_cpu_roc_score ,xgb_cpu_pr_auc, xgb_cpu_runtime ))\nprint( 'LightGBM_ROC_AUC : {0:.4f} , LightGBM_PR_AUC : {1:.4f} ,Runtime : {2:.4f}'.format(lgbm_roc_score ,lgbm_pr_auc, lgbm_cpu_runtime))\nprint( 'RF_cv_ROC_AUC : {0:.4f} , RF_cv_PR_AUC : {1:.4f} '.format(rf_cv_roc_score ,rf_cv_pr_auc ))\nprint( 'XGboost_cv_ROC_AUC : {0:.4f} , XGboost_cv_PR_AUC : {1:.4f} '.format(xgb_cv_roc_score ,xgb_cv_pr_auc ))\nprint( 'LightGBM_cv_ROC_AUC : {0:.4f} , LightGBM_cv_PR_AUC : {1:.4f} '.format(lgbm_cv_roc_score ,lgbm_cv_pr_auc ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########## Export\n\n\n#save model\njoblib.dump(rf_clf , 'rf_clf.pkl')\njoblib.dump(lgbm_clf , 'lgbm_clf.pkl')\njoblib.dump(xgb_clf , 'xgb_clf.pkl')\n# joblib.dump(ngb_clf , 'ngb_clf.pkl')\n\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}