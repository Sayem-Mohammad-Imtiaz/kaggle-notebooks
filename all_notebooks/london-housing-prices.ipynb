{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement: Predicting London House Prices\nThe housing prices in London are extortionate. As a strapped-for-cash university graduate, it seems financially implausible that I would be able to purchase a house here. If I could just get onto the property ladder, on the other hand, I might be able to begin my climb toward a deluxe property at the foot of Hyde Park.\n\nUsing housing data from as early as 1995, I might be able to predict future prices. I expect either to be filled with hope, or have those hopes smashed to smithereens and condemning me to a life spent living in my parents' basement.\n\n* **Data:** 2 csv files, comprising monthly and yealy variables about the houses in London dating from 1995 to 2020.\n* **Target variable:** Average yearly price","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration and Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Read in the Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nos.symlink('../input', 'input')\n\nmonthly_data = pd.read_csv('input/housing-in-london/housing_in_london_monthly_variables.csv',\n                                  parse_dates=['date'])\nyearly_data = pd.read_csv('input/housing-in-london/housing_in_london_yearly_variables.csv',\n                                 parse_dates=['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearly_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Values\nIt is immediately visible that some columns have missing values. To quantify this, let's see what proportion of each column is _not_ missing (i.e. how much is potentially useful data):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# fraction values that are not null\nyearly_data.notnull().sum()/len(yearly_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_data.notnull().sum()/len(monthly_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notably, roughly 33% of life satisfaction data is potentially useful, and 50% for the number of crimes data. We may want to first try and construct a simple model without using these features, and find a way to include them later if required.\n\nThe data for both area size and number of houses accounts for ~62% of the total dataset. Recycling percentage has ~80% and number of jobs has ~86%. The rest are above ~95% present. We will try imputation for missing data of these features, keeping in mind the percentage of missing values that have to be imputed for each feature in subsequent analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Filtering Data from Outside of London\nIt appears that we have some data for houses outside of London. It is reasonable to expect that if we are predicting *London* housing prices, just the data from London will be sufficient for accurate predictions. Conveniently, the data has a 'borough_flag' indicating whether the house is located in a London borough or not. We will use this to filter out the non-London data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep only the rows corresponding to locations in London\nlondon_yearly = yearly_data.loc[yearly_data['borough_flag'] == 1]\nlondon_monthly = monthly_data.loc[monthly_data['borough_flag'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check to make sure the areas match between the yearly and monthly datasets\nset(london_monthly['area'].unique()) == set(london_yearly['area'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Converting Monthly to Yearly Data\nWe will calculate yearly average prices for each area, using the monthly 'average_price' and 'houses_sold' columns.\nWe will also find the total number of crimes in each year, but must deal with the missing values first.\n\nFirst, lets calculate the average prices, for which we appear to have all the data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the sum of prices over all the houses sold (average times sale count)\nsum_prices = london_monthly['average_price'] * london_monthly['houses_sold']\n# group prices by year and area, and sum for each distinct (year, area) pair\nsum_prices = pd.concat([london_monthly['date'], london_monthly['area'], sum_prices], axis=1)\nsum_prices = sum_prices.groupby([sum_prices['date'].dt.year, sum_prices['area']]).sum()\n\n# total number of houses sold\nsum_sales = london_monthly['houses_sold'].groupby([london_monthly['date'].dt.year, london_monthly['area']]).sum()\n\n# element wise division of the average prices by the number of houses sold\nmonthly_average = sum_prices.div(sum_sales, axis=0)\nmonthly_average","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before naively imputing missing crime data, we might want to see if more data is missing for particular years:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_crimes = london_monthly[london_monthly['no_of_crimes'].isnull()]\nmissing_crimes.groupby(missing_crimes['date'].dt.year).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears years further into the past have more missing values. This may support an argument for considering more recent data. This would also make sense when considering inflation and other changes over time. For now, we will use some simple imputation to have some data available for early years.\n\nIs there a chronological trend to the crime rates?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# total number of crimes by year and area\nsum_crimes = london_monthly['no_of_crimes'].groupby([london_monthly['date'].dt.year, london_monthly['area']]).sum()\nsns.lineplot(sum_crimes.reset_index()['date'], sum_crimes.reset_index()['no_of_crimes'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The drops to zero are in line with our missing data. For the data we have, it appears that there is a small dip, but not too much variation over 20 years. This suggests imputing missing data with mean values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# imputation\nimputer = SimpleImputer(strategy='mean')\nimputed_crimes = pd.DataFrame(imputer.fit_transform(np.array(london_monthly['no_of_crimes']).reshape(-1, 1)))\nimputed_crimes[['date', 'area']] = london_monthly[['date', 'area']]\n\n# sum of crimes by year and area\nsum_crimes = imputed_crimes.groupby([imputed_crimes['date'].dt.year, imputed_crimes['area']]).sum()\n\n# putting it together with our averge prices\nyearly_aggregates = pd.concat([sum_crimes, monthly_average], axis=1)\nyearly_aggregates.columns = ['average_price', 'num_crimes']\nyearly_aggregates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for example: the average house price and number of crimes in Hounslow, in 2014 \nyearly_aggregates.loc[(2014, 'hounslow')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's merge our results with the rest of the yearly data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_use = ['area', 'date', 'median_salary', 'mean_salary', 'recycling_pct', 'population_size', 'number_of_jobs', 'area_size', 'no_of_houses']\ntotal_data = london_yearly[features_to_use]\n# the date is the same day of each year, so we can simplify our values by dropping day and month\ntotal_data['date'] = total_data['date'].dt.year\n\n# join with aggregated monthly data\ntotal_data = total_data.set_index(['date', 'area']).join(yearly_aggregates).reset_index()\ntotal_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualisations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"How does the average price depend on area? Looks like the far right (Westminster) is a clear winner for expensive houses.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(total_data['area'], total_data['average_price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How about mean and median salary? In trying to plot these, I discovered that there are some missing values in mean salary:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_data.loc[total_data['mean_salary'] == '#']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_data['mean_salary'] = total_data.replace('#', 'NaN')['mean_salary'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice the two distinct arms below. \nFor one arm, there is a low median salary, but high housing prices. This appear to correspond to City of London. The other appears to be Barking and Dagenham, Barnet, and Sutton, where the median salary is high, but the prices are low.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(total_data['median_salary'], total_data['average_price'], hue=total_data['area'])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(total_data['mean_salary'], total_data['average_price'], hue=total_data['area'])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Train, Val, Test Splits and Constructing a Pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_data['recycling_pct'] = total_data['recycling_pct'].replace('na', 'NaN').astype(float)\ntotal_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = total_data.drop(['average_price'], axis=1)\ny = total_data['average_price']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_features = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\nnumeric_features = [col for col in X_train.columns if X_train[col].dtype in ['float64']]\n\nnumerical_preprocessor = Pipeline([('imputer', SimpleImputer())])\ncategorical_preprocessor = Pipeline([('encoder', OneHotEncoder(sparse=False))])\n\npreprocessor = ColumnTransformer(\n                    transformers=[\n                        ('num', numerical_preprocessor, numeric_features),\n                        ('cat', categorical_preprocessor, categorical_features)])\n\ndef train_model(model):\n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('model', model)])\n    pipeline.fit(X_train, y_train)\n    predictions = pipeline.predict(X_val)\n\n    score = mean_absolute_error(predictions, y_val)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nxgb_model = XGBRegressor()\nrandom_forest_model = RandomForestRegressor()\n\nrandom_forest_score = train_model(random_forest_model)\nxgb_score = train_model(xgb_model)\nprint(f\"MAE for random forest: {random_forest_score}\\nMAE for XGB Regressor: {xgb_score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explainability and Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance as plot_xgb_importance\n\n# the xgboost importance plots are less versatile than is available from sklearn models, but we can still see what we get\nplot_xgb_importance(xgb_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation Importance: Relative Feature Importances","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\npreprocessed_features = list(X_val.columns[2:]) + list(X_val['area'].unique())\n\npreprocessed = pd.DataFrame(preprocessor.fit_transform(X_val, y_val))\npreprocessed.columns = preprocessed_features\nperm = PermutationImportance(model, random_state=1).fit(preprocessed, y_val)\neli5.show_weights(perm, feature_names=preprocessed.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Partial Dependence Plots: Effect of Individual Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pdpbox import pdp, get_dataset, info_plots\n\npdp_num_jobs = pdp.pdp_isolate(model=model, dataset=preprocessed, model_features=preprocessed_features,\n                                   feature='number_of_jobs')\n\npdp_recycling_pct = pdp.pdp_isolate(model=model, dataset=preprocessed, model_features=preprocessed_features,\n                                   feature='recycling_pct')\n\npdp.pdp_plot(pdp_num_jobs, 'Number of Jobs')\npdp.pdp_plot(pdp_recycling_pct, 'Recycling Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP Values: Feature Influence on Individual Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\n\ndata_for_prediction = preprocessed.iloc[-10]\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(data_for_prediction)\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP Summaries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = explainer.shap_values(preprocessed)\nshap.summary_plot(shap_values, preprocessed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that a small number of houses has the effect of decreasing housing prices, while a small area size increases prices. Perhaps we should be thinking more about the interaction between these two variables, in other words: the house density?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SHAP Dependence Plots\nTo investigate if there is such an interaction, let's try a dependence plot:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('no_of_houses', shap_values, preprocessed, interaction_index='area_size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like for the smaller area sizes, a smaller number of houses makes a big difference to the price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}