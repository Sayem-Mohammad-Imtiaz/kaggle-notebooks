{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is this project about?\n\nOur data was provided by [Barun Kumar](https://www.kaggle.com/barun2104). It contains information on customers of a telecommunication company and whether they left the company or not. In this project we will see which types of customers are most likely to leave the company using classification."},{"metadata":{},"cell_type":"markdown","source":"## Import packages and data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objects as pgo\nimport plotly.express as px\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Import data\ndf = pd.read_csv('../input/telecom-churn/telecom_churn.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How does our data look like? \n\nLet's have a look at the first five rows."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many rows and columns do we have?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's have a closer look at the variables**\n\nThe following table contains all variables with their description, data-type and the number of missing values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Table\nvar = {'Description': ['1 if customer cancelled service, 0 if not', 'number of weeks customer has had active account', '1 if customer recently renewed contract, 0 if not', '1 if customer has data plan, 0 if not', 'gigabytes of monthly data usage', 'number of calls into customer service', 'average daytime minutes per month', 'average number of daytime calls', 'average monthly bill', 'largest overage fee in last 12 months', 'average number of roaming minutes'],\n       'Datatype': list(df.dtypes),\n       '# of NaNs': list(df.isna().sum())}\n\ndf_var = pd.DataFrame(var, columns = ['Description', 'Datatype', '# of NaNs'], index = list(df.columns))\ndf_var","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This looks great!**\n\nWe don't have any categorical variables and no missing values. So our dataset is ready to be analysed. Of course the variable 'Churn' will be our target feature."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nNow we will see what insights and relationships can be discovered through analysing the variables. We will start with our target feature 'Churn'. "},{"metadata":{},"cell_type":"markdown","source":"## Target variable: 'Churn'\n\n**How high is the proprotion of customers which cancelled the service?**\n\nAs described in the table above, this variable is '1' if the customer cancelled the service and '0' if not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new dataframe for better labeling of plots\ndf['churn'] = np.where(df['Churn'] == 1, 'cancelled', 'not cancelled')\ndf['contract renewal'] = np.where(df['ContractRenewal'] == 1, 'renewed', 'not renewed')\ndf['data plan'] = np.where(df['DataPlan'] == 1, 'data plan', 'no data plan')\n\n# Create pie chart\nlabels = ['cancelled', 'not cancelled']\nvalues = [(df.Churn == 1).sum(), (df.Churn == 0).sum()]\nfig = pgo.Figure(data = [pgo.Pie(labels = labels, values = values, hole = .6, marker_colors = ['coral',                                                                                                                       \n                                                                                               'cornflowerblue'])])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Binary Features: 'ContractRenewal' and 'DataPlan'\n\nThe feature 'ContractRenewal' tells us wether the customer recently renewed his contract (1) or not (0). The feature 'DataPlan' is 1 if the customer had a data plan and 0 else.\n\nWe will see how many customers renewed their contract and how many had a data plan in this dataset. Additionaly we will use 'Churn' as a third variable in our plots to hopefully gain first insights of who is more likely to leave the company."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create barchart for 'ContractRenewal'\nsns.set_style(\"darkgrid\")\ndf_plot = df.groupby(['churn', 'contract renewal']).size().reset_index().pivot(columns = 'churn', index = 'contract renewal', values = 0)\ndf_plot.plot(kind = 'bar', stacked = True, figsize = (10, 7))\nplt.xticks(rotation = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In this dataset around 90% renewed their contract recently. \n* We can see that the proportion of customers who cancelled their contract is much higher in the group of customers who haven't renewed their contract recently. \n* In the group with a contract renewal the proportion is only around 10%, whereas in the other group it is more than one third."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create barchart for 'DataPlan'\ndf_plot = df.groupby(['churn', 'data plan']).size().reset_index().pivot(columns = 'churn', index = 'data plan', values = 0)\ndf_plot.plot(kind = 'bar', stacked = True, figsize = (10, 7))\nplt.xticks(rotation = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Only around 27% of customers have a data plan in this dataset.\n* There is no significant difference in the proportion of cancelled contracts in these two groups.\n* In the group with no data plan around 16% and in the other group 11% left the company."},{"metadata":{},"cell_type":"markdown","source":"## All non-binary features\n\nNow we will look at the non-binary features and if they can help us identify the reason for leaving the company."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up figure\nfig = plt.figure(figsize = (20,16))\n\n# Violinplots\nnbin_var = df[['AccountWeeks', 'DataUsage', 'CustServCalls', 'DayMins', 'DayCalls', 'MonthlyCharge', 'OverageFee',\n               'RoamMins']]\nfor i, v in enumerate(nbin_var):\n    axes = fig.add_subplot(4, 2, i+1)\n    sns.violinplot(x = 'churn', y = v, data = df, ax = axes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Half of the non-binary variables show no significant difference regarding the behavior of the two groups.\n* The other half give us some interesting insights.\n\n**DataUsage:**\n* The median in both groups is around 0. This is most probably the case because as we already found out, only 27% have a data plan in this dataset.\n* In the group of cancelled contracts the interquartile range is not nearly as large as in the other group. Also the above whisker is very small in comparison.\n* This means that the data usage is more spread out for customers who then stayed in the company.\n\n**CustServCalls:**\n* Those who cancelled their service have one customer-service call more on average (median) than those who stayed in the company. \n\n**DayMins:**\n* They also had higher average daytime minutes per month.\n\n**MonthlyCharge:**\n* The monthly charge was also higher on average in the group of customers who cancelled."},{"metadata":{},"cell_type":"markdown","source":"## Relationship: 'DataPlan' & 'DataUsage' & 'Churn'\n\nWe will now focus on other relationships and thereby try to better understand our data. We will start with plotting the data usage of customers with data plan vs customers without a data plan. We will keep 'Churn' as a third variable in the plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create boxplots\nsns.catplot(x = 'DataUsage', y = 'data plan', row = 'churn',\n            kind = 'box', orient = 'h', height = 2.5, aspect = 4,\n            data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course, customers with a data plan had higher data usage on average than those without. The more valuable insights here are:\n* Customers with a data plan who cancelled their service used more data on average than those who stayed in the company.\n* 50% of those with no data plan who stayed in the company made use of data whereas only outliers of the group which left the company did so."},{"metadata":{},"cell_type":"markdown","source":"## Relationship: 'DataPlan' & 'MonthlyCharge' & 'Churn'\n\nNow we will look at the differences in the monthly charge between costumers with and without a data plan."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create boxplots\nsns.catplot(x = 'MonthlyCharge', y = 'data plan', row = 'churn',\n            kind = 'box', orient = 'h', height = 2.5, aspect = 4,\n            data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The monthly charge is also higher on average for those with a data plan. The more interesting fact about these boxplots is that the average monthly charge of the customers who had no data plan and left the company and was higher (around 62) than those who stayed in the company (around 47)."},{"metadata":{},"cell_type":"markdown","source":"## Relationship: 'DataPlan' & 'DayMins' & 'Churn' \n\nLastly let's have a look at the differences in their average daytime minutes per month."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create boxplots\nsns.catplot(x = 'DayMins', y = 'data plan', row = 'churn',\n            kind = 'box', orient = 'h', height = 2.5, aspect = 4,\n            data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the customers who stayed in the company the distribution of daytime minutes per month looks almost the same. For those who cancelled their service the difference between the group with and without a data plan is large. The average of those with a data plan is almost as high as the one of the customers who stayed in the company with 170 minutes. But for those who had no data plan is around 225 minutes. "},{"metadata":{},"cell_type":"markdown","source":"## Relationship: 'DataUsage' & 'MonthlyCharge' & 'Churn'\n\nOf course the monthly charge of customers with high data usage will be higher on average. But by adding the variable 'Churn' as a third variable we could observe additional important insights."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create scatterplot\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df, x = 'DataUsage', y = 'MonthlyCharge', hue = 'churn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As predicted, there is a positive correlation between data usage and monthly charge. What we can also observe is that most customers with low to no data usage but a high monthly charge left the company."},{"metadata":{},"cell_type":"markdown","source":"## Relationship: 'Daymins' & 'MonthlyCharge' & 'Churn' \n\nWe will do the same for 'DayMins' and 'MonthlyCharge'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create scatterplot\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df, x = 'DayMins', y = 'MonthlyCharge', hue = 'churn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"There is also a positive relationship between the average daytime minutes per month and the monthly charge. Here we can additionally observe one surprising fact. Almost all customers with a monthly charge between 60 and 80 and average daytime minutes per month above 200 left the company. The other leavers are well distributed in this scatterplot."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nAfter this analysis we now more about relationships of the variables and what kind of customers the company. We will now use Machine-Learning-Models to have a precise classification of the costumers in there leaving-behavior."},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\n\n**Define target variable, features and split the data into train and test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define target and features\ny = df['Churn']\nX = df.iloc[:, 1:11]\n\n# Stratified train-test-split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Resampling**\n\nAs we saw in our first pie chart the target feature 'churn' has a problem of imbalanced classes. It had 2850 customers who stayed in the company and 483 who left. The problem with imbalanced classes is that the data isn't easily seperable. We must make sacrifices to one class or the other.\nThat's why we will try out Upsampling and Downsampling our train-data prior to fitting our machine-learning-models. Then we will see which model and which resampling method worked best on our test-data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge X_train and y_train\ndf_train = X_train.merge(y_train.to_frame(), left_index = True, right_index = True)\n\n# Indicies of each class' observations\ndf_train0 = df_train[df_train['Churn'] == 0]\ndf_train1 = df_train[df_train['Churn'] == 1]\n\n# Number of observations in each class\nn_0 = len(df_train0)\nn_1 = len(df_train1)\n\n# Downsampling\ndf_train0_ds = resample(df_train0, replace = False, n_samples = n_1, random_state = 123)\ndf_train_ds = pd.concat([df_train0_ds, df_train1])\nprint(\"Downsampled Dataset Class Counts:\", df_train_ds.Churn.value_counts(), sep = \"\\n\")\n\n# Upsampling\ndf_train1_us = resample(df_train1, replace = True, n_samples = n_0, random_state = 123)\ndf_train_us = pd.concat([df_train1_us, df_train0])\nprint(\"\\nUpsampled Dataset Class Counts:\", df_train_us.Churn.value_counts(), sep = \"\\n\")\n\n# Seperate y and X\ny_train_ds = df_train_ds['Churn']\nX_train_ds = df_train_ds.iloc[:, 0:10]\n\ny_train_us = df_train_us['Churn']\nX_train_us = df_train_us.iloc[:, 0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML-Models\n\nWe will define a function which lets us use and compare multiple models without having to write the code multiple times. As performence metric we will use the normal accuracy score and \"Area under the ROC-Curve\" as it better represents the performance in a classification problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up function which uses an algo as input and returns the accuracy score and AUROC for the input data\ndef alg_fit(alg, X_train, y_train):\n    \n    # Model selection\n    mod = alg.fit(X_train, y_train)\n    \n    # Prediction\n    y_pred = mod.predict(X_test)\n    \n    # Accuracy Score\n    acc_score = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score: \", acc_score)\n    \n    # AUROC\n    prob_y = mod.predict_proba(X_test)\n    prob_y = [p[1] for p in prob_y]\n    auroc_score = roc_auc_score(y_test, prob_y)\n    print(\"AUROC Score:\", auroc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up function which uses function alg_fit and returns a comparison of the accuracy score and AUROC for imbalanced, downsampled and upsampled data\ndef compare_results(alg):\n    # Unsampled Dataset\n    print(\"IMBALANCED CLASSES\")\n    alg_fit(alg, X_train, y_train)\n\n    # Downsampled majority class\n    print(\"\\nDOWNSAMPLED MAJORITY CLASS\")\n    alg_fit(alg, X_train_ds, y_train_ds)\n\n    # Upsampled minority class\n    print(\"\\nUPSAMPLED MINORITY CLASS\")\n    alg_fit(alg, X_train_us, y_train_us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression \n\nLet's have a look at the first model and how it performs on the data with imbalanced classes, downsampled majority class and upsampled minority class."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\ncompare_results(LogisticRegression(max_iter = 500))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is ok that the accuracy metric is lower for the resampled data as it is expected. We will focus more on the AUROC-score as it is a better precision metric for our classification models. Here, upsampling the minority class yields the best result."},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes\ncompare_results(GaussianNB())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the AUROC-score for the resampled data is even lower than with imbalanced classes. But still the precision is higher than for Logistic Regression."},{"metadata":{},"cell_type":"markdown","source":"## K-nearest Neighbors "},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-nearest Neighbors\ncompare_results(KNeighborsClassifier(n_neighbors = 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the k-nearest neighbor didn't perform that good."},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Classifier\ncompare_results(DecisionTreeClassifier(random_state = 0, max_depth = 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a new accuracy high with the decision tree. "},{"metadata":{},"cell_type":"markdown","source":"## Random Forest "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classifier\nrfc = RandomForestClassifier(random_state = 0, n_estimators = 50, max_depth = 5)\ncompare_results(rfc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As forseeable Random Forest performed even better than a single decision tree."},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting Classifier\ngbc = GradientBoostingClassifier(random_state = 0, n_estimators = 50, max_depth = 3, \n                                 min_samples_leaf = 5)\ncompare_results(gbc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Besides the increase in the AUROC-Score with downsampled majority class nothing has increased with Gradient-Boosting."},{"metadata":{},"cell_type":"markdown","source":"## XG-Boost "},{"metadata":{"trusted":true},"cell_type":"code","source":"# XG-Boost Classifier\nxgb = XGBClassifier(learning_rate = 0.15,\n                    n_estimators = 1000,\n                    max_depth = 20,\n                    min_child_weight = 1,\n                    gamma = 0.5,\n                    subsample = 0.8,\n                    colsample_bytree = 0.8,\n                    reg_alpha = 0.03,\n                    objective = 'binary:logistic',\n                    nthread = 4,\n                    scale_pos_weight = 1,\n                    seed = 27)\ncompare_results(xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is higher but the AUROC-score of the Random-Forest Classifier is still the highest."},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix, Recall and Precision\n\nWe will have a look at the confusion matrices of the last three models (Random Forest, Gradient Boosting & XGBoost) as they performed the best. Additionally, we are going to focus on the models with upsampled minority class as for all it yielded the highest AUROC Score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of best algos with parameter tuned\nbest_algs = [rfc, gbc, xgb]\nnames = [\"RandomForestClassifier\", \"GradientBoostingClassifier\", \"XGBClassifier\"]\n\n# Return confusion matrices (absolute), precision and recall\nj = 0\nfor i in best_algs:\n    mod = i.fit(X_train_us, y_train_us)\n    y_pred = mod.predict(X_test)\n    con_mat = confusion_matrix(y_test, y_pred)\n    prec = (con_mat[0][0])/(con_mat[0][0] + con_mat[0][1])\n    rec = (con_mat[0][0])/(con_mat[0][0] + con_mat[1][0])\n    print(\"\\n\", names[j], \"Confusion Matrix:\\n\", con_mat, \"\\nPrecision: \", prec, \"\\nRecall: \", rec)\n    j = j + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Result\n\nOur Random-Forest Model actually had the highest AUROC-Score and Recall. But the Accuracy and Precision of our XG-Boost Classifier outperformed the Random-Forest Model."},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance\n\nWe will use the XG-Boost Classifier to detect the most important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature importance\ndf_feature_importance = pd.DataFrame(xgb.feature_importances_, index = X_train_us.columns, \n                                     columns = ['feature importance']).sort_values('feature importance', \n                                                                                   ascending = False)\ndf_feature_importance","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}