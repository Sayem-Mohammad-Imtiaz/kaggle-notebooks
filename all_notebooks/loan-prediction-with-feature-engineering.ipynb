{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom scipy.stats import norm, skew \nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/loan-prediction-practice-av-competition/train_csv.csv')\ntest = pd.read_csv('/kaggle/input/loan-prediction-practice-av-competition/test.csv.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data overview**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('Loan_ID',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data summary**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the include parameter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include = ['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We change the target from Yes, No into logical expression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'Loan_Status'\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntrain[target] = encoder.fit_transform(train[target])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = train.dtypes =='object'\ncat_cols = list(cat_cols[cat_cols].index)\nnum_cols = train.dtypes != 'object'\nnum_cols = list(num_cols[num_cols].index)\nnum_cols.remove('Loan_Status')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[cat_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[num_cols].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Get summary of target variable. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[target].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=target, data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relationship with numerical features**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In the following section, plot boxes will be made to see the relationship of target varible and numerical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_boxplot(col,target,train,y = 80000):\n    data =  pd.concat([train[target], train[col]], axis=1)\n    f, ax = plt.subplots(figsize=(8, 6))\n    fig = sns.boxplot(x=str(target), y=col, data=train)\n    fig.axis(ymin=0, ymax=y);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applicant Income and target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Maximum value in ApplicantIncome is 81000.\nnum_boxplot(num_cols[0],target,train,81000)\nnum_boxplot(num_cols[0],target,train,25000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions of applicant income on different loan status are similar. Hence, there're lot of outliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"CoapplicantIncome and target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Maximum value in CoapplicantIncome is 41667.\nnum_boxplot(num_cols[1],target,train,41667)\nnum_boxplot(num_cols[0],target,train,15000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions of applicant income on different loan status are similar. Hence, there're lot of outliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LoanAmount and target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_boxplot(num_cols[2],target,train,700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loan_Amount_Term and target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Maximum loan amount trem is 700. Notice that Q1,Q2 and Q3 are equal\nnum_boxplot(num_cols[3],target,train,1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution are similar.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credit history and traget","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_boxplot(num_cols[4],target,train,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions are differnet here. Creidit history is more vary in loan that were disapproved.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**What are average values of numerical features for each loan status?**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.Loan_Status == 1].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.Loan_Status == 0].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relationship with categorical variables.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Convert target variable into numceric form","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_cols:\n    sns.barplot(col, target, data=train, color=\"darkturquoise\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Data/ Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Join the df together handling the missing data together\nall_df = pd.concat([train,test.drop('Loan_ID',axis =1)],axis = 0)\n#train = all_df.iloc[1:614]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test['Loan_ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the target column, it hasn't dropped in test data set. \nall_df = all_df.drop('Loan_Status',axis = 1)\nall_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_cols = list(all_df.columns)\nmissing_cols = [col for col in all_cols if all_df[col].isnull().any()]\nlen(missing_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There 7 columns with missing values, let's go further.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to create a data frame with number and percentage of missing data in a data frame\ndef missing_to_df(df):\n    #Number and percentage of missing data in training data set for each column\n    total_missing_df = df.isnull().sum().sort_values(ascending =False)\n    percent_missing_df = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending=False)\n    missing_data_df = pd.concat([total_missing_df, percent_missing_df], axis=1, keys=['Total', 'Percent'])\n    return missing_data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = missing_to_df(all_df)\nmissing_df[missing_df['Total'] > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing in credit history might mean the credit history of the clients are not available. Fill the missing data with 2 means the data aren't available.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df['Credit_History'] = all_df['Credit_History'].fillna(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing in self employed can mean a person is not in labor force or retired. So, we give a new categorical to those people.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df['Self_Employed'] = all_df['Self_Employed'].fillna('Other')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outliers in Loan Amount (maximum value is 700 and Q3 is 162), so the missing value in this column will be filled with median. The remaining columns with missing values will be filled by median value as well. There size are relatively small, it's safe to do so.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nnum_missing = ['LoanAmount',  'Loan_Amount_Term']\ncat_missing = ['Gender', 'Married','Dependents']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_imputer = SimpleImputer(strategy = 'median')\nfor col in num_missing:\n    all_df[col] = pd.DataFrame(median_imputer.fit_transform(pd.DataFrame(all_df[col])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_imputer = SimpleImputer(strategy = 'most_frequent')\nfor col in cat_missing:\n    all_df[col] = pd.DataFrame(freq_imputer.fit_transform(pd.DataFrame(all_df[col])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = missing_to_df(all_df)\nmissing_df[missing_df['Total'] > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no more missing data in our data set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Skewed features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = all_df.dtypes[all_df.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Box Cox Transformation of (highly) skewed features**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\\\(1 + x\\\\)**. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_df[feat] = boxcox1p(all_df[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting dummy categorical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature engineering**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding total income by combining applicant's income and coapplicant's income\nall_df['Total_Income'] = all_df['ApplicantIncome'] + all_df['CoapplicantIncome']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label Encoding dependets that contain information in their ordering set.\n\nConvert 3+ in depedents into 3, and convert the column into numeric feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = all_df.replace({'Dependents': r'3+'}, {'Dependents': 3}, regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process column, apply LabelEncoder to categorical features\nfrom sklearn.preprocessing import LabelEncoder\nlbl = LabelEncoder()\nlbl.fit(list(all_df[\"Dependents\"].values))\nall_df[\"Dependents\"] = lbl.transform(list(all_df[\"Dependents\"].values))\n# shape        \nprint('Shape all_data: {}'.format(all_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (all_df.dtypes == 'object')\nobject_cols = list(s[s].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    all_df[col] = label_encoder.fit_transform(all_df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_df = pd.get_dummies(all_df)\nprint(all_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting the new train and test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = all_df.iloc[:614]\nprint(train.shape)\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = all_df[614:]\nprint(test.shape)\ntest.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\nsel_train, valid, _ = get_data_splits(pd.concat([train,y],axis=1))\nfeature_cols = sel_train.columns.drop(target)\n\n# Keep 5 features\nselector = SelectKBest(f_classif, k=8)\n\nX_new = selector.fit_transform(sel_train[feature_cols], sel_train[target])\nX_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get back the features we've kept, zero out all other features\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=sel_train.index, \n                                 columns=feature_cols)\nselected_features.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropped columns have values of all 0s, so var is 0, drop them\nselected_columns = selected_features.columns[selected_features.var() != 0]\n\n# Get the valid dataset with the selected features.\nvalid[selected_columns].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train test split for model building.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Baseline models**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(train,y,random_state = 1)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\nacc_log = round(accuracy_score(y_test,y_pred)*100,2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc  = round(accuracy_score(y_test,y_pred)*100,2)\nacc_svc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_test,y_pred)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(y_test,y_pred)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(y_test,y_pred)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(random_state=1)\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_test)\nacc_decision_tree = round(accuracy_score(y_test,y_pred)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100,random_state = 1)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nacc_random_forest = round(accuracy_score(y_test,y_pred)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBMClassifier\n\nlgbc = lgb.LGBMClassifier()\nlgbc.fit(x_train, y_train)\ny_pred = lgbc.predict(x_test)\nacc_lgbc = round(accuracy_score(y_test,y_pred)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n             acc_decision_tree,acc_lgbc]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Models with selected features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(train[selected_columns],y,random_state = 1)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\nacc_log = round(accuracy_score(y_test,y_pred)*100,2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we compute the coefficient of each features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n* Credit History is highest positive coefficient, implying as the credit history value rises, the probability of Loan Status =1 (Loan approved) increases the most.\n\n* Total income isn't a good artificial feature to model as it is not included in selected features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_df = pd.DataFrame(train[selected_columns].columns.delete(-1))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machines**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machines is a supervised learning models with associated learning algorithms that analyse data used for classification and regression analysis. \n\nNote that the model generates a confidence score which is lower than Logistics Regression model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc  = round(accuracy_score(y_test,y_pred)*100,2)\nacc_svc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**kNeighborsClassifier**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In clustering, the k-Nearest Neighbors algorithm is an unsupervisied learning model. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"KNN confidence score is the lowest.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#kNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_test,y_pred)*100,2)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naive Bayes Classifiers**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n\nNaive Bayes classifiers confidence score is better than SVM but sightly worse than Logisitics Regression.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(y_test,y_pred)*100,2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Perceptron**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is a linear classifier that makes predictions based on a linear predcition function integrating a set of weights with the feature's vector.\n\nThe model generated confidence score is not high.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(y_test,y_pred)*100,2)\nacc_perceptron","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The accuaracy of decision tree is not too high.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(random_state=1)\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_test)\nacc_decision_tree = round(accuracy_score(y_test,y_pred)*100,2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Random foresdt is not bad.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100,random_state = 1)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nacc_random_forest = round(accuracy_score(y_test,y_pred)*100,2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LGBMClassifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBMClassifier\n\nlgbc = lgb.LGBMClassifier()\nlgbc.fit(x_train, y_train)\ny_pred = lgbc.predict(x_test)\nacc_lgbc = round(accuracy_score(y_test,y_pred)*100,2)\nacc_lgbc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Evaluation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n             acc_decision_tree,acc_lgbc]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The models built with selected features are more accurate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Cross valudation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nThe following section is using **cross validation** strategy to compare performance of differnence models. f1 is used as our target is binary.\n[ The scoring parameter](https://scikit-learn.org/stable/modules/model_evaluation.html)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train[selected_columns].values)\n    rmse= (cross_val_score(model, train[selected_columns].values, \n                                   y.values, scoring=\"f1\", cv = kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below show how the models above perform on the data by evaluating the cross-validation rmsle error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_score = rmsle_cv(logreg)\nprint(\"\\nLogistic Regression score: {:.4f} ({:.4f})\\n\".format(log_reg_score.mean(), log_reg_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_score = rmsle_cv(svc)\nprint(\"\\nSupport Vector Machines score: {:.4f} ({:.4f})\\n\".format(svc_score.mean(), svc_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_score = rmsle_cv(knn)\nprint(\"\\nkNN score: {:.4f} ({:.4f})\\n\".format(knn_score.mean(), knn_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_score = rmsle_cv(gaussian)\nprint(\"\\nGaussian Naive Bayes score: {:.4f} ({:.4f})\\n\".format(naive_score.mean(), naive_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perceptron_score = rmsle_cv(perceptron)\nprint(\"\\nPerceptron score: {:.4f} ({:.4f})\\n\".format(perceptron_score.mean(), perceptron_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_score = rmsle_cv(decision_tree)\nprint(\"\\nDecision_tree score: {:.4f} ({:.4f})\\n\".format(decision_tree_score.mean(), decision_tree_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_score = rmsle_cv(random_forest)\nprint(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(random_forest_score.mean(), random_forest_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbc_score = rmsle_cv(lgbc)\nprint(\"\\nLGB Classificier score: {:.4f} ({:.4f})\\n\".format(lgbc_score.mean(), lgbc_score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron',  \n              'Decision Tree','LGBMClassifier'],\n    'Mean_Score': [svc_score.mean(), knn_score.mean(), log_reg_score.mean(), \n              random_forest_score.mean(), naive_score.mean(), perceptron_score.mean(), \n             decision_tree_score.mean(),lgbc_score.mean()]})\nmodels.sort_values(by='Mean_Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Svc , logistic regression and Naive Bayes are always the top 3 highest accurate models in this case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(train[selected_columns], y)\nY_pred = logreg.predict(test[selected_columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"Loan_Id\": test_id,\n        \"Loan_Status\": Y_pred\n    })\nsubmission.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}