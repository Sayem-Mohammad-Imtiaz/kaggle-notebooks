{"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.6.3","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"77350c58-d2ea-4d21-8a65-3141de2125c0","_uuid":"7b8ea00450b46f4d9e5696bbe4a57a2107b2902a"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Load the data\ntrain_df = pd.read_csv('../input/train.csv', header=0)\ntest_df = pd.read_csv('../input/test.csv', header=0)\n\n# We'll impute missing values using the median for numeric columns and the most\n# common value for string columns.\n# This is based on some nice code by 'sveitser' at http://stackoverflow.com/a/25562948\nfrom sklearn.base import TransformerMixin\nclass DataFrameImputer(TransformerMixin):\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n            index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\nfeature_columns_to_use = ['Pclass','Sex','Age','Fare','Parch']\nnonnumeric_columns = ['Sex']\n\n# Join the features from train and test together before imputing missing values,\n# in case their distribution is slightly different\nbig_X = train_df[feature_columns_to_use].append(test_df[feature_columns_to_use])\nbig_X_imputed = DataFrameImputer().fit_transform(big_X)\n\nle = LabelEncoder()\nfor feature in nonnumeric_columns:\n    big_X_imputed[feature] = le.fit_transform(big_X_imputed[feature])\n    \nprint(\"Train DF Size: {:,}\".format(train_df.shape[0]))\nprint(\"Test DF Size: {:,}\".format(test_df.shape[0]))\nprint(\"Train Row 1: {}\".format(big_X.as_matrix()[0]))\nprint(\"Train Row 1 Imputed: {}\".format(big_X_imputed.as_matrix()[0]))","metadata":{"_cell_guid":"08863e68-ecd4-4ef5-ac13-9a0058bf1b43","_kg_hide-input":false,"_uuid":"f7a6f661f90f4e1e75bb92e48e42f909f8c2a636"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Prepare the inputs for the model\n# leave out last 100 samples for second model\ntrain_X = big_X_imputed[0:train_df.shape[0]-100].as_matrix()\ntest_X = big_X_imputed[train_df.shape[0]::].as_matrix()\ntrain_y = train_df['Survived'][0:-100]\n\n# You can experiment with many other options here, using the same .fit() and .predict()\n# methods; see http://scikit-learn.org\n# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\nxgb1 = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(train_X, train_y)\npredictions1 = xgb1.predict(test_X)\nprint(\"Done Training 1st model\")\nsubmission1 = pd.DataFrame({ 'PassengerId': test_df['PassengerId'],\n                            'Survived': predictions1 })\n\ntrain_X2 = big_X_imputed[0:train_df.shape[0]].as_matrix()\ntest_X2 = big_X_imputed[train_df.shape[0]::].as_matrix()\ntrain_y2 = train_df['Survived']\nxgb2 = xgb.XGBClassifier(max_depth=5, n_estimators=300, learning_rate=0.10, ).fit(train_X, train_y, xgb_model=xgb1.get_booster())\nxgb3 = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05, ).fit(train_X, train_y, xgb_model=xgb1.get_booster())\npredictions2 = xgb2.predict(test_X)\npredictions3 = xgb3.predict(test_X)\nprint(\"Done Training 2nd and 3rd models\")\nsubmission2 = pd.DataFrame({ 'PassengerId': test_df['PassengerId'],\n                             'Survived1': predictions1, 'Survived2': predictions2\n                           , 'Survived3': predictions3})\n","metadata":{"_cell_guid":"d493d5ec-6d8e-4b83-ba4b-e0ac2cb24b12","_uuid":"07d8222b041ed3ea443f07299a707ceb9b60f10a"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"print(submission2[0:20])","metadata":{"_cell_guid":"f7292c88-3657-423d-b7ac-d87ca0831f54","_uuid":"5b8de4ca63ba5b3e18f99c9ac69903d12221ae93"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"booster1 = xgb1.get_booster()\nbooster2 = xgb2.get_booster()\nbooster3 = xgb3.get_booster()\nprint(dir(booster1))","metadata":{"_cell_guid":"e0f60989-4d08-4cec-ba6d-e7458c60936f","_uuid":"6f962b48721680edeb8c670effa75e5dbe3f841d"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"from xgboost import plot_importance\nplot_importance(booster1) # first model\nplot_importance(booster2) # deeper model\nplot_importance(booster3) # same as first with more training","metadata":{"_cell_guid":"cff56aa5-bca9-4cd4-b52f-39dd703effa3","_uuid":"08ea1d9b77c67806e77fe5ce0609e0b8566afa4e"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"booster1.save_model('xgb1')\nbooster2.save_model('xgb2')\nbooster3.save_model('xgb3')\nprint(check_output([\"ls\", \"-lh\"]).decode(\"utf8\"))","metadata":{"_cell_guid":"c69957a6-c655-4062-a334-3c64daf2dc2c","_uuid":"14f5dfd071256025cd0ae0f988b328bd340d233b"},"outputs":[]}]}