{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip -qq install focal-loss","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport skopt\nimport gc\nimport time\nfrom skopt.space import Real, Integer, Categorical\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\nimport matplotlib.pyplot as plt\nfrom skopt.plots import plot_convergence,plot_objective,plot_evaluations\nimport warnings\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom focal_loss import SparseCategoricalFocalLoss\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Conv1D, Input, Dropout, BatchNormalization, MaxPooling1D, Flatten, Dense, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom skopt import dump, load\nimport tensorflow.keras.backend as K\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/cmsdata/CMS_trigger.csv').drop(columns = 'Unnamed: 0')\ndf['1/pT'] = df['q/pt'].abs()\ndef label(a):\n    if a<=10:\n        return 0\n    if a>10 and a<=30:\n        return 1\n    if a>30:\n        return 2\n\ndf['pT'] = 1/df['1/pT']\n    \ndf['pT_classes'] = df['pT'].apply(label)\n\nfeatures = ['Phi_'+str(i) for i in [0,2,3,4]] + ['Theta_'+str(i) for i in [0,2,3,4]] + ['Front_'+str(i) for i in [0,2,3,4]]\nlabels_1 = ['pT']\nlabels_2 = ['pT_classes']\nlabels_3 = ['PatternStraightness']\n\nscaler_1 = StandardScaler()\ndf[features] = scaler_1.fit_transform(df[features])\n\nscaler_3 = MinMaxScaler()\ndf[labels_3] = scaler_3.fit_transform(df[labels_3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df[features].iloc[:int(len(df)*0.8)].to_numpy().reshape((-1,4,3))\nY1_train = df[labels_1].iloc[:int(len(df)*0.8)]\nY2_train = df[labels_2].astype('float32').iloc[:int(len(df)*0.8)]\nY3_train = df[labels_3].iloc[:int(len(df)*0.8)]\n\nX_test = df[features].iloc[int(len(df)*0.8):].to_numpy().reshape((-1,4,3))\nY1_test = df[labels_1].iloc[int(len(df)*0.8):]\nY2_test = df[labels_2].astype('float32').iloc[int(len(df)*0.8):]\nY3_test = df[labels_3].iloc[int(len(df)*0.8):]\n\ndf = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, len(Y1_train), len(Y2_train), len(Y3_train), X_test.shape, len(Y1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Real - Dropout, Focal loss weight, SkipFraction\n# Binary - Batchnorm, skipType\n# Integer - number of layers, number of filters, focal loss gamma, number of dense neurons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(y_true,y_pred):\n    y_t = K.cast(y_true<80,K.dtype(y_true))*y_true + K.cast(y_true>=80,K.dtype(y_true))*K.cast(y_true<160,K.dtype(y_true))*y_true*2.4 + K.cast(y_true>=160,K.dtype(y_true))*10 \n    return K.mean(y_t*K.pow((y_pred-y_true)/y_true,2))/250","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space  = [Real(0.0, 0.7, name='Dropout'),\n          Real(0.0, 1.0, name='SkipFraction'),\n          Categorical(['YES','No'], name = 'BatchNorm'),\n          Categorical(['ADD','CONCAT'], name = 'SkipType'),\n          Integer(2, 10, name='# Layers'),\n          Integer(16, 512, name='# Filters'),\n          Integer(128, 512, name='DenseNeurons'),\n          Categorical(['MAX','AVG'], name = 'PoolingType'),\n          Categorical(['relu','swish','tanh','sigmoid'], name = 'Activation')\n         ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnn(values = [0,1,'NO','ADD',5,32,128,'AVG','relu'], X_train = X_train, Y1_train = Y1_train, Y2_train = Y2_train, Y3_train = Y3_train):\n    global strategy\n    dropout = values[0]\n    SkipFraction = values[1]\n    batchNorm = values[2]\n    SkipType = values[3]\n    Layers = values[4]\n    Filters = values[5]\n    DN = values[6]\n    PoolingType = values[7]\n    Activation = values[8]\n    \n    batch_size=1024* strategy.num_replicas_in_sync\n    path = \"model.h5\"\n    \n    with strategy.scope():\n        I = Input(shape=(4,3))\n        x = Conv1D(filters=Filters, kernel_size=3, activation=Activation, padding='same')(I)\n        x_ = x\n        for i in range(Layers-1):\n            x = Dropout(dropout)(x)\n            if batchNorm==\"YES\":\n                x = BatchNormalization()(x)\n            x__ = Conv1D(filters=Filters, kernel_size=3, activation=Activation, padding='same')(x)\n            if SkipType=='CONCAT' and Layers*(1-SkipFraction)<=i:\n                x = tf.concat([x_, x__] ,axis = -1)\n                x_ = x__\n            elif SkipType=='ADD' and Layers*(1-SkipFraction)<=i:\n                x = x_ + x__\n                x_ = x__\n            else:\n                x_ = x__\n                x = x__\n        if PoolingType == 'AVG':\n            x = GlobalAveragePooling1D()(x)\n        if PoolingType == 'MAX':\n            x = GlobalMaxPooling1D()(x)\n        x1 = Dense(DN, activation=Activation)(x)\n        O1 = Dense(1, activation='linear')(x1)\n#         x2 = Dense(DN, activation=Activation)(x)\n#         O2 = Dense(3, activation='softmax')(x2)\n#         x3 = Dense(DN, activation=Activation)(x)\n#         O3 = Dense(1, activation='sigmoid')(x3)\n\n#         model = Model(inputs=I, outputs=[O1,O2, O3])\n        model = Model(inputs=I, outputs=O1)\n\n        checkpoint = ModelCheckpoint(path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n        early_stop = EarlyStopping(monitor='val_loss',patience=3,verbose=0)\n        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=0,verbose=0)\n\n#         model.compile(optimizer = 'adam', loss=[loss,SparseCategoricalFocalLoss(gamma=2),'mse'],loss_weights = [1,0.05,1] )\n        model.compile(optimizer = 'adam', loss=loss)\n#     model.summary()\n#     print(X_train.shape, Y1_train.shape, Y2_train.shape, Y3_train.shape, Y4_train.shape )\n#     model.fit(x = X_train, y = [Y1_train, Y2_train, Y3_train], batch_size=batch_size, epochs=55, verbose=0, validation_split=0.1, callbacks=[checkpoint,early_stop,reduce_lr])\n    model.fit(x = X_train, y = Y1_train, batch_size=batch_size, epochs=55, verbose=0, validation_split=0.1, callbacks=[checkpoint,early_stop,reduce_lr])\n    \n    model.load_weights(path)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score(true, preds):\n    S = []\n    count = 0\n    for i in range(120):\n        try:\n            if i<25:\n                S.append(2*mae(true[(true>i)&(true<=i+1)],preds[(true>i)&(true<=i+1)])/(i+0.5))\n            if i>=25 and i<50:\n                S.append(1.5*mae(true[(true>i)&(true<=i+1)],preds[(true>i)&(true<=i+1)])/(i+0.5))\n            else:\n                S.append(mae(true[(true>i)&(true<=i+1)],preds[(true>i)&(true<=i+1)])/(i+0.5))\n        except:\n            count+=1\n            continue\n    print(count)\n    return sum(S)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def score(true, preds):\n#     true = 1/true\n#     preds = 1/(preds+pow(10,-6))\n#     return mae(true,preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iteration__ = 0\ndef objective(values = [0,1,'NO','ADD',5,32,128,'AVG','relu'], X_test = X_test, Y1_test = Y1_test, Y2_test = Y3_test, Y3_test = Y3_test):\n    global iteration__\n    start = time.time()\n    iteration__ += 1\n    gc.collect()\n    model = cnn(values)\n#     test_preds = model.predict(X_test)[0]\n    test_preds = model.predict(X_test)\n    if len(set(list(test_preds.reshape((-1)))))==1:\n        model = cnn(values)\n        test_preds = model.predict(X_test)[0]\n    loss = score(Y1_test.to_numpy(), test_preds)\n    print(iteration__, \"iteration loss = \", loss)\n    print('Time-taken = ', time.time()-start)\n    print()\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x0 = []\ny0 = np.array([])\nfor i in range(1):\n    res = load('../input/newlossbayesianopt2/result.pkl')\n    x0 = x0 + res.x_iters\n    y0 = np.concatenate([y0,res.func_vals])\n    \n# # for i in range(2):\n# #     res = load('../input/bayesian3/result'+str(i)+'.pkl')\n# #     x0 = x0 + res.x_iters\n# #     y0 = np.concatenate([y0,res.func_vals])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_gp = gp_minimize(objective, space, n_calls=40, x0=x0, y0=y0, n_random_starts=10)\n\n\"Best score=%.4f\" % res_gp.fun","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# res_gp = gp_minimize(objective, space, n_calls=40, n_random_starts=10)\n\n# \"Best score=%.4f\" % res_gp.fun","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x0 = res_gp.x_iters\ny0 = res_gp.func_vals\n\nfor i in sorted(y0):\n    print(i, x0[np.where(y0==i)[0][0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(res_gp, 'result.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best parameters:\")\nprint(\"- Dropout=\",res_gp.x[0])\nprint(\"- SkipFraction=\",res_gp.x[1])\nprint(\"- BatchNorm=\",res_gp.x[2])\nprint(\"- SkipType=\",res_gp.x[3])\nprint(\"- # Layers=\",res_gp.x[4])\nprint(\"- # Filters=\",res_gp.x[5])\nprint(\"- DenseNeurons=\",res_gp.x[6])\nprint(\"- PoolingType=\",res_gp.x[7])\nprint(\"- Activation=\",res_gp.x[8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_convergence(res_gp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_objective(res_gp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_evaluations(res_gp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_loaded = load('result.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_loaded.fun","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}