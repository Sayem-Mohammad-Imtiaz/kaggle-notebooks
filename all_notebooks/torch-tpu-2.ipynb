{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install efficientnet_pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm \nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\nimport gc\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage import io, transform\nfrom torchvision import models, transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time\nimport copy \nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split as ttp\nfrom skimage.filters import threshold_otsu\nfrom skimage.color import rgb2gray\nimport cv2 as cv \nimport pickle\nimport random \nimport albumentations\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class melanoma_dataset(Dataset):\n    def __init__(self, root_dir, transform, df = pd.DataFrame() , csv_file = False, train = True):\n        \n        \n        self.df = df\n        \n        if csv_file:\n            self.csv = pd.read_csv(csv_file)\n        \n        self.directory = root_dir\n        \n        self.transform = transform\n        \n        self.train = train\n        \n        \n        \n    def __getitem__(self,idx):\n        \n        if not self.df.empty:\n            tab = self.df\n        else:\n            tab = self.csv\n        \n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        if tab.iloc[idx,1] == '-1':\n            directory = self.directory[1]\n        else:\n            directory = self.directory[0]\n        \n        if self.train == False:\n            directory = TEST_FOLDER\n        img_name = os.path.join(directory, tab.iloc[idx, 0]) + '.jpg'\n        img = cv.imread(img_name)\n\n        target= tab.iloc[idx, 3] if self.train else 0\n        \n        if self.transform:\n            #sample= self.transform(image = self['image'], target = self['target'])\n            image = self.transform(image = img)\n            flipped = image['image']\n            image = np.transpose(flipped, (2, 0, 1)).astype(np.float32)\n        \n        if self.train:\n            return image, target\n        else:\n            return image\n        \n        \n        \n        \n    \n    def __len__(self):\n        if not self.df.empty:\n            return len(self.df)\n        else:\n            return len(self.csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mx = EfficientNet.from_pretrained('efficientnet-b6')\nmx._fc = nn.Linear(2304, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\ntransform_train = albumentations.Compose([\n    albumentations.Normalize(mean, std, always_apply = True),\n    albumentations.ShiftScaleRotate(),\n    albumentations.Flip(p=0.5)\n])\n\ntransform_valid = albumentations.Compose([\n    albumentations.Normalize(mean, std, always_apply = True),\n])\n\ntransform_test = albumentations.Compose([\n    albumentations.Normalize(mean, std, always_apply = True),\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR0 = '../input/jpeg-melanoma-384x384/train'\nDIR1 = '../input/jpeg-isic2019-384x384/train'\nFOLD_CSVS ={0:'../input/combined-train/train0.csv',1:'../input/combined-train/train1.csv',2:'../input/combined-train/train2.csv',\n            3:'../input/combined-train/train3.csv',4:'../input/combined-train/train4.csv'}\nTEST_FOLDER = '../input/jpeg-melanoma-384x384/test'\nTEST_CSV = '../input/combined-train/test.csv'\nMODELS_PATH = '../input/tpu-models/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#have to add in datasets/for fold line ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_fn(vals):\n    return sum(vals) / len(vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\nlr_start   = 0.000005\nlr_max     = 0.000000125 * 8 * 4\nlr_min     = 0.00000005\nlr_ramp_ep = 5\nlr_sus_ep  = 0\nlr_decay   = 0.8\n\ndef get_lr(epoch):\n    if epoch == 0:\n        lr = 0.001\n        \n    elif epoch < lr_ramp_ep:\n        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n\n    elif epoch < lr_ramp_ep + lr_sus_ep:\n        lr = lr_max\n\n    else:\n        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n\n    return lr\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop_fn(dataloader, model, optimizer, device, scheduler):\n    model.train()\n    #torch.set_grad_enabled(True)\n    for bi, d in enumerate(dataloader):\n    \n        inputs = d[0]\n        targets = d[1]\n        \n        inputs = inputs.to(device, dtype = torch.float32)\n        targets = targets.to(device, dtype = torch.float32)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n \n        loss = loss_fn(outputs, targets)\n        \n        if bi % 50 == 0:\n            loss_reduced = xm.mesh_reduce('loss_reduce', loss, reduce_fn)\n            xm.master_print(f'bi ={bi}, TRAIN_loss = {loss_reduced}')\n            \n            loss.backward()\n            \n            xm.optimizer_step(optimizer)\n            if scheduler is not None:\n                scheduler.step()\n    model.eval()\n    \ndef eval_loop_fn(data_loader, model, device,scheduler = None):\n    fin_targets = []\n    fin_outputs = []\n    val_loss_avg= []\n    #torch.set_grad_enabled(False)\n    for bi, d in enumerate(data_loader):\n\n        inputs = d[0] \n        targets = d[1] \n\n        \n        inputs = inputs.to(device, dtype=torch.float32)\n        targets = targets.to(device, dtype=torch.float32)\n        \n        outputs = model(inputs)\n        ''''if bi % 10 == 0:\n            val_loss = loss_fn(outputs, targets)\n            val_loss_reduced = xm.mesh_reduce('loss_reduce', val_loss, reduce_fn)\n            xm.master_print(f'bi ={bi}, VAL_loss = {val_loss_reduced}')\n            val_loss_avg.append(val_loss_reduced.numpy())'''\n            \n        \n        \n        targets_np = targets.cpu().detach().numpy().tolist()\n        outputs_np = outputs.sigmoid().cpu().detach().numpy().tolist()\n        fin_targets.extend(targets_np)\n        fin_outputs.extend(outputs_np)    \n        del targets_np, outputs_np\n        gc.collect() \n        #scheduler.step(np.mean(val_loss_avg))\n    return fin_outputs, fin_targets    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"mx = EfficientNet.from_pretrained('efficientnet-b6')\nmx._fc = nn.Linear(2304, 1) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = EfficientNet.from_pretrained('efficientnet-b6')\nmodel._fc = nn.Linear(2304, 1)\nmodel.load_state_dict(torch.load('../input/tpu-models-pretrained/model_0.pth'))\nmx = model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"BATCH_SIZE = 4\ndef get_lr(epoch):\n    dic = {0:0.00001,1:0.00001,2:0.00001,3:0.00001,4:0.00001,5:0.00005, 6:0.000001,7:0.000005,8:0.000005,9:0.000001, 10:0.000001,11:0.000001,12:0.000001}\n    lr = dic[epoch]\n    return lr*xm.xrt_world_size()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def get_lr(epoch):\n    dic = {0:0.000005, 1:0.00001, 2:0.0001,3:0.0001,4: 0.001, 5: 0.0001, 6:0.00005, 7: 0.00001}\n    lr = dic[epoch]\n    return lr","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def _run(fold):\n\n    \n    EPOCHS = 50\n    model_path = f'model_{fold}.pth'\n    CSV = pd.read_csv(FOLD_CSVS[fold])\n    train_df, valid_df, _,_ =  ttp(CSV, np.zeros(len(CSV)), random_state = 0)\n    train_dataset = melanoma_dataset([DIR0,DIR1], transform_train, train_df)\n    valid_dataset = melanoma_dataset([DIR0,DIR1], transform_valid, valid_df)\n    \n    # defining data samplers and loaders \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(), \n          rank=xm.get_ordinal(), \n          shuffle=True)\n\n\n        \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=2\n    )\n    \n\n    device = xm.xla_device()\n    model = mx.to(device) \n    xm.master_print('done loading model')\n\n\n    xm.master_print('training on train dataset')\n    \n\n    lr =1e-7 *xm.xrt_world_size()\n    num_train_steps = int(len(train_dataset) / BATCH_SIZE / xm.xrt_world_size() * EPOCHS) \n    #optimizer = optim.SGD(model.parameters(), lr = .0001, momentum = 0.9)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=.000005*num_train_steps,\n        num_training_steps=num_train_steps\n    )\n    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,get_lr, verbose=False)\n    #scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    \n    xm.master_print(f'num_training_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n\n    best_auc = 0\n    aucs = []\n    count = 0\n    for epoch in range(EPOCHS):\n        #lr = get_lr(epoch)\n        xm.master_print(f'learning rate for epoch : {epoch} is {lr}')\n        #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n        ts = time.time()\n        gc.collect() \n        \n        train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=2,\n        )\n        para_loader = pl.ParallelLoader(train_data_loader, [device]) \n        #xm.master_print('parallel loader created... training now')\n        gc.collect()\n        # call training loop:\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n        del para_loader\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        gc.collect()\n        # call evaluation loop\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        del para_loader\n        gc.collect()\n        # report AUC at the end\n        auc = roc_auc_score(np.array(t) >= 0.5, o)\n        auc_reduced = xm.mesh_reduce('auc_reduce',auc,reduce_fn)\n        auc_reduced\n        aucs.append(auc_reduced)\n        if auc_reduced > best_auc:\n            best_auc = auc_reduced\n            best_model_wts = copy.deepcopy(model.state_dict())\n        xm.master_print(f'{time.time()-ts}----AUC = {auc_reduced}')\n        #if epoch > 9:\n        #    break\n        gc.collect()\n    xm.save(best_model_wts, model_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\n# Start training processes\ndef _mp_fn(rank, flags):\n        a = _run(fold)\n\nFLAGS={}\nstart_time = time.time()\nfor fold in range(5):\n    model = EfficientNet.from_pretrained('efficientnet-b6')\n    model._fc = nn.Linear(2304, 1)\n    model.load_state_dict(torch.load('../input/tpu-models-pretrained/model_0.pth'))\n    mx = model\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}