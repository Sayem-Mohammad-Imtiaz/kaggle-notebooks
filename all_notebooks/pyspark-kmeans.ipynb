{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Installing PySpark"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pyspark.ml.evaluation import ClusteringEvaluator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Starting Spark Session with Context"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = SparkContext('local')\nspark = SparkSession(sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To read the data, we first need to desin an scheme with appropriate data types, matching with CSV file inputs. Our dataset has three fields namely userID, songID, and rating."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_schema = StructType([\n    StructField('userID',IntegerType(), False),\n    StructField('songID',IntegerType(), False),\n])\ndata = spark.read.csv(\n    '../input/dataset-for-collaborative-filters/songsDataset.csv', header=True, schema=input_schema\n).cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We transform our features and update our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"vecAssembler = VectorAssembler(inputCols=['userID', 'songID'], outputCol=\"features\")\ndata = vecAssembler.transform(data).select('features')\ndata.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have vectorised form of dataset. The first step of kmeans clustering is to exact the number of clusters. We need to know the value of k. For that, we, keep minimum number to 2 and check the cost incurred with various various of k. We check for range [2, 22]"},{"metadata":{"trusted":true},"cell_type":"code","source":"cost = np.zeros(22)\nfor k in range(2,22):\n    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n    model = kmeans.fit(data.sample(False,0.1))\n    cost[k] = model.computeCost(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cost is initialized as an array with 22 values. Then, we fit and compute cost to build a list of costs with varying number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize =(10,7))\nax.plot(range(2,22),cost[2:22])\nax.set_xlabel('k')\nax.set_ylabel('cost')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that there isn't much change after 20. So, we take k as 20."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}