{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analyze Product Sentiment\n\nHere, I'm going to analyze 'Amazon Baby' product' on the basis of the reviews given by consumers.   \nI'm going to use Logistic Regression Model of Machine Learning to classify sentiments with two approaches as below:\n1. Sentiment Classifier Model using all the words in reviews.\n2. Sentiment Classifier Model using the selected words from the reviews.\nAnd at the end, I'll compare both the approaches of analysing sentiments."},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install turicreate\nimport turicreate\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import HoverTool\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the data and creating an SFrame of the data\nproducts = turicreate.SFrame.read_csv('../input/amazon-baby-sentiment-analysis/amazon_baby.csv')\n\n# Exploring dataset\nproducts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 10 Amazon Baby Product's Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"products.groupby('name',operations={'count':turicreate.aggregate.COUNT()}).sort('count', ascending= False).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Ratings of the most popular Amazon Baby Product"},{"metadata":{"trusted":true},"cell_type":"code","source":"giraffe_reviews = products[products['name']=='Vulli Sophie the Giraffe Teether']\ngiraffe_reviews['rating'].show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing data for Sentiment Analysis\n\n* We will build 'word_count' vector. "},{"metadata":{"trusted":true},"cell_type":"code","source":"products['word_count'] = turicreate.text_analytics.count_words(products['review'])\nproducts.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I'm creating a subset of words to create a classifier. Often, ML practitioners will throw out words they consider “unimportant” before training their model. This procedure can often be helpful in terms of accuracy. Here, I'm going to throw out all words except for the very few which indicate sentiments as below. Using so few words in our model will hurt our accuracy, but help us interpret what our classifier is doing.\n* I'll build columns for selected words using 'word_count' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_words = ['awesome', 'great', 'fantastic', 'amazing', 'love', 'horrible', 'bad', 'terrible', 'awful', 'wow', 'hate']\n# Loop through word counts to create a classifier for only a few words \n# Created an individual column for each item \nfor word in selected_words:\n    products[word] = products['word_count'].apply(lambda counts: counts.get(word, 0))\n\nproducts.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define what is positive and negative sentiment\n\nLet's see ratings distribution of Amazon Baby Products"},{"metadata":{"trusted":true},"cell_type":"code","source":"products['rating'].show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ignore all 3*  reviews\nproducts = products[products['rating']!= 3]\n\n#positive sentiment = 4-star or 5-star reviews\nproducts['sentiment'] = products['rating'] >= 4\n\nproducts.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of Sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"products['sentiment'].show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train and Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data,test_data = products.random_split(.8,seed=0)                  # using 80% data for trainning and the rest for Testing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a sentiment classifier using all words as Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Model using all words\nsentiment_model = turicreate.logistic_classifier.create(train_data,target='sentiment', features=['word_count'], validation_set=test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = sentiment_model.classify(test_data)\nprint (predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of Sentiment Model using all words as Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"roc = sentiment_model.evaluate(test_data, metric= 'roc_curve')\nroc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = figure(title= 'ROC Curve for all words Sentiment Model', plot_width=600, plot_height=400)\n\np.line(x= roc['roc_curve']['fpr'], y= roc['roc_curve']['tpr'], line_width=2 , legend_label=\"ROC Curve Class\")\np.line([0, 1], [0, 1], line_dash=\"dotted\", line_color=\"indigo\", line_width=2)\np.add_tools(HoverTool(tooltips=[(\"False Positive Rate\", \"@x\"), (\"True Positive Rate\", \"@y\")])) \np.xaxis.axis_label = 'False Positive Rate'\np.yaxis.axis_label = 'True Positive Rate'\np.legend.location = 'bottom_right'\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = sentiment_model.evaluate(test_data)\nprint (\"Accuracy             : {}\".format(result['accuracy']))\nprint (\"Area under ROC Curve : {}\".format(result['auc']))\nprint (\"Confusion Matrix     : \\n{}\".format(result['confusion_matrix']))\nprint (\"F1_score             : {}\".format(result['f1_score']))\nprint (\"Precision            : {}\".format(result['precision']))\nprint (\"Recall               : {}\".format(result['recall']))\nprint (\"Log_loss             : {}\".format(result['log_loss']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply the sentiment classifier to better understand the most popular Amazon Baby Product"},{"metadata":{"trusted":true},"cell_type":"code","source":"productsdata = products.copy()\nproductsdata['predicted_sentiment'] = sentiment_model.predict(productsdata, output_type = 'probability')\n# As above identified the most popular Amazon Baby Product is 'Vulli Sophie the Giraffe Teether'\ngiraffe_reviews = productsdata[productsdata['name']== 'Vulli Sophie the Giraffe Teether']\ngiraffe_reviews = giraffe_reviews.sort('predicted_sentiment', ascending=False)\n\n# Most positive review for most popular Amazon Baby Product\nprint('Most Positive review for Vulli Sophie the Giraffe Teether:\\n\\n ', giraffe_reviews[0]['review'])\nprint('\\n\\n')\n# Most negative review for most popular Amazon Baby Product\nprint('Most Negative review for Vulli Sophie the Giraffe Teether:\\n\\n ', giraffe_reviews[-1]['review'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# Building a sentiment classifier using Selected Words as Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features to be trained on selected words Model\nselected_words_feat = selected_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Model using selected words\nselected_words_model = turicreate.logistic_classifier.create(train_data,target='sentiment', features= selected_words_feat, validation_set=test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = selected_words_model.classify(test_data)\nprint (predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of Sentiment Model using selected words as Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_swm = selected_words_model.evaluate(test_data, metric= 'roc_curve')\nroc_swm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = figure(title= 'ROC Curve for selected words Sentiment Model', plot_width=600, plot_height=400)\n\np.line(x= roc_swm['roc_curve']['fpr'], y= roc_swm['roc_curve']['tpr'], line_width=2 , legend_label=\"ROC Curve Class\")\np.line([0, 1], [0, 1], line_dash=\"dotted\", line_color=\"indigo\", line_width=2)\np.add_tools(HoverTool(tooltips=[(\"False Positive Rate\", \"@x\"), (\"True Positive Rate\", \"@y\")])) \np.xaxis.axis_label = 'False Positive Rate'\np.yaxis.axis_label = 'True Positive Rate'\np.legend.location = 'bottom_right'\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_swm = selected_words_model.evaluate(test_data)\nprint (\"Accuracy             : {}\".format(result_swm['accuracy']))\nprint (\"Area under ROC Curve : {}\".format(result_swm['auc']))\nprint (\"Confusion Matrix     : \\n{}\".format(result_swm['confusion_matrix']))\nprint (\"F1_score             : {}\".format(result_swm['f1_score']))\nprint (\"Precision            : {}\".format(result_swm['precision']))\nprint (\"Recall               : {}\".format(result_swm['recall']))\nprint (\"Log_loss             : {}\".format(result_swm['log_loss']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"---\n---\n# Comparing the two Models\nAlso finding answers to some queries."},{"metadata":{},"cell_type":"markdown","source":"Using the .sum() method on each of the new columns you created, answer the following questions: Out of the selected_words, which one is most used in the dataset? Which one is least used?"},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in selected_words:\n    print(\"\\nThe number of times {} appears: {}\".format(word, products[word].sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above, out of the selected_words, the **most used word** in the dataset is <span style=\"color:blue\">'great'</span> and the **least used word** in the dataset is <span style=\"color:blue\">'wow'</span>."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"### Analysing selected words on the basis of weights learned in selected words classifier Model\nOut of the 11 words in selected_words, which one got the most positive weight? Which one got the most negative weight? Do these values make sense for you?"},{"metadata":{"trusted":true},"cell_type":"code","source":"swm_weights= selected_words_model.coefficients.sort(key_column_names='value', ascending=False)\nswm_weights.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Out of the 11 words in selected_words, Most Positive: ', \n      swm_weights[swm_weights['value'] == swm_weights['value'].max()]['name'][0])\nprint('\\n')\nprint('Out of the 11 words in selected_words, Most Negative: ', \n      swm_weights[swm_weights['value'] == swm_weights['value'].min()]['name'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of the 11 words in selected_words,\n**Most Positive**: <span style=\"color:blue\">'love'</span> and\n**Most Negative**: <span style=\"color:blue\">'horrible'</span> \n\nThese values make total sense because love is a great word and horrible is a bad descriptor.    \n\n---"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Interpreting the difference in performance between the models: \nTo understand which of the two models performs better, I'll now examine the reviews for a particular product.\n\n* I'll investigate a product named ‘Baby Trend Diaper Champ’. (This is a trash can for soiled baby diapers, which keeps the smell contained.)\n\n* Again, just like 'Vulli Sophie the Giraffe Teether', I'll use the sentiment_model to predict the sentiment of each review in diaper_champ_reviews and then sort the results according to their ‘predicted_sentiment’.     \n    \n   \n* Now I'll find out the ‘predicted_sentiment’ for the most positive and most negative reviews with their reviews  for ‘Baby Trend Diaper Champ’ according to the sentiment_model from the Jupyter Notebook from lecture? Save this result to answer the quiz at the end.   \n    \n    \n* Then I'll use the selected_words_model learned using just the selected_words to predict the sentiment most positive and negative review. Then Compare the value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For sentiment_model\ndiaper_champ_reviews = products[products['name'] == 'Baby Trend Diaper Champ']            # extracts data only product named 'diaper_champ_reviews'\ndiaper_champ_reviews['predicted_sentiment'] = sentiment_model.predict(diaper_champ_reviews, output_type = 'probability')\ndiaper_champ_reviews = diaper_champ_reviews.sort('predicted_sentiment', ascending=False)\ndiaper_champ_reviews.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted Sentiment for the most positive review \nprint('Predicted Sentiment for Most Positive review:  ', diaper_champ_reviews[0]['predicted_sentiment'])\n# Most positive review for ‘Baby Trend Diaper Champ’\nprint('Most positive review for ‘Baby Trend Diaper Champ’:\\n\\n ', diaper_champ_reviews[0]['review'])\nprint('\\n\\n')\n\n# Predicted Sentiment for the most negative review\nprint('Predicted Sentiment for Most Negative review:  ', diaper_champ_reviews[-1]['predicted_sentiment'])\n# Most negative review for ‘Baby Trend Diaper Champ’\nprint('Most negative review for ‘Baby Trend Diaper Champ’:\\n\\n ', diaper_champ_reviews[-1]['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For selected_words_model\ndcr_swm = products[products['name'] == 'Baby Trend Diaper Champ']            # extracts data only product named 'diaper_champ_reviews'\ndcr_swm['predicted_sentiment'] = selected_words_model.predict(dcr_swm, output_type = 'probability')\ndcr_swm = dcr_swm.sort('predicted_sentiment', ascending=False)\ndcr_swm.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicted Sentiment for the most positive review \nprint('Predicted Sentiment for Most Positive review:  ', dcr_swm[0]['predicted_sentiment'])\n# Most positive review for ‘Baby Trend Diaper Champ’\nprint('Most positive review for ‘Baby Trend Diaper Champ’:\\n\\n ', dcr_swm[0]['review'])\nprint('\\n\\n')\n\n# Predicted Sentiment for the most negative review\nprint('Predicted Sentiment for Most Negative review:  ', dcr_swm[-1]['predicted_sentiment'])\n# Most negative review for ‘Baby Trend Diaper Champ’\nprint('Most negative review for ‘Baby Trend Diaper Champ’:\\n\\n ', dcr_swm[-1]['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dcr_swm[dcr_swm['word_count'] == diaper_champ_reviews['word_count'][0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ‘predicted_sentiment’ for the most positive review for ‘Baby Trend Diaper Champ’, according to the <span style=\"color:green\">sentiment_model</span> is <span style=\"color:blue\">0.9999</span> where as the predicted sentiment for the same ‘Baby Trend Diaper Champ’ review, according to  <span style=\"color:green\">selected_words_model</span> is <span style=\"color:blue\">0.7919</span>.      \nAccording to me, the value of the predicted_sentiment for the most positive review found using the the sentiment_model is much more positive than the value predicted using_selected_words_model because none of the selected words appeared in the text of this review.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"## Accuracy of Majority Class Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Calculate_y_hat(scores):\n    y_hat = []\n    for score in scores:\n        if score>0:\n            y_hat.append(1)\n        else:y_hat.append(-1)\n    return y_hat\n\ndef get_classification_accuracy(model, data, true_labels):\n    # First get the predictions\n    scores = model.predict(data, output_type='margin')\n    \n    # Compute the number of correctly classified examples\n    count_correct_classified_samples = 0\n    y_hat =  Calculate_y_hat(scores)\n    \n    for i in range(len(scores)):\n        if y_hat[i] == true_labels[i]:\n            count_correct_classified_samples+=1\n\n    # Then compute accuracy by dividing num_correct by total number of examples\n    accuracy = count_correct_classified_samples/(len(scores))\n    \n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_classification_accuracy(sentiment_model, test_data, test_data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_classification_accuracy(selected_words_model, test_data, test_data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline: Majority class prediction\nIt is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, we should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n\nWhile comparing just the Majority Class Classifier, I compare the different learned models with baseline approach as model with selected_word_model performed better than the all word model.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Comparison of Accuracy\nAs the accuracy score for <span style=\"color:green\">sentiment_model</span> is <span style=\"color:blue\">0.9177</span> where as the accuracy score for <span style=\"color:green\">selected_words_model</span> is <span style=\"color:blue\">0.8464</span>. Definately, 'sentiment_model' means model with all words have better accuracy than 'selected_words_model' means model with selected words.       \nAlso, the total error (i.e. sum of False Positive and False Negative) for sentiment model is 2741 and for selected_words_model is 5116. Hence, error count in sentiment_model is less than selected_words_model.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\nAfter all those comparision we come to conclusion that, the sentiment_model with all word is better sentiment classifer than the selected_words_model with some selected words. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}