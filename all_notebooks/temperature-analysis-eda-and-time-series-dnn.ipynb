{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Packages","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing data and cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data is imported as .csv from Kaggle and imported with pandas library","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/daily-temperature-of-major-cities/city_temperature.csv', low_memory = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Number of unique members belonging to different dimensions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('No. of Regions: %i' %data['Region'].nunique())\nprint('No. of Countries: %i' %data['Country'].nunique())\nprint('No. of States: %i' %data['State'].nunique())\nprint('No. of Cities: %i' %data['City'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Members of various dimensions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"Region\"].unique())\nprint(data[\"Country\"].unique())\nprint(data[\"Month\"].unique())\nprint(data[\"Day\"].unique())\nprint(data[\"Year\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Displaying basic information about data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Descriptive statistics of measures","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we find that the minimum Average temperaure temperature is -99 degrees which is unusual","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Counting N/As in data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Counting the number of N/As, we find that the states is only defined for the U.S.A. Hence we need to drop the column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Removing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The following procedures are performed to clean the data\n\n* The rows with values '201' and '200' are removed from the Years column.\n* The rows with day as '0' is removed.\n* The States column with data only for the US is removed.\n* The rows with temperature -99 degress Fahrenheit is removed ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing '201' and '200' from Year column\ndf = data[~data['Year'].isin(['201','200'])]\n\n# Removing '0' from Date column\ndf = df[df['Day'] != 0]\n\n# Removing 'State' column\ndf = df.drop(columns=['State'])\n\n# Dropping the rowns with temperature -99\ndf = df.drop(df[df['AvgTemperature'] == -99.0].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding data/columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The following columns are added\n\n* Average Temperature in Celcius is added.\n* Combining year, month and day to make the Date column in the format YYYY-MM-DD\n* Introduce column Period in the format YYYY-MM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a row with average temperatures in Celcius\ndf['AvgTempCelcius'] = round((((df.iloc[:,6] - 32) * 5) / 9),2)\n\n# Adding the Date column in the format YYYY-MM-DD\ndf['Date'] = df.iloc[:,5].astype(str) + '-' + df.iloc[:,3].astype(str) + '-' + df.iloc[:,4].astype(str)\n\n# Coverting the Date column into Pandas Date type datetime64[ns]\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Introducing the Period column in format YYYY-MM\ndf['Month/Year'] = pd.to_datetime(df['Date']).dt.to_period('M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization and Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Mean Average temperature of different continents in Fahrenheit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.groupby(['Region'])['AvgTemperature'].mean())\navg_temp_world = pd.Series(round(df.groupby('Region')['AvgTemperature'].mean().sort_values(),2))\navg_temp_world.plot(kind='bar', figsize = (10,6), color='yellow', alpha=0.5)\nplt.xlabel('Mean Average Temperature')\nplt.ylabel('Regions')\nplt.title('Mean Average Temperatures by Region')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Average temperature in the world from 1995 to 2019","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"world_temp_date = pd.DataFrame(pd.Series(round(df.groupby('Date')['AvgTempCelcius'].mean(),2))[:-1])\nworld_temp_year = pd.DataFrame(pd.Series(round(df.groupby('Year')['AvgTempCelcius'].mean(),2))[:-1])\n\nplt.subplot(2,1,1)\nsns.set_style(\"darkgrid\")\nsns.lineplot(data = world_temp_date, color = 'blue')\nplt.xlabel('Time')\nplt.ylabel('Temperature (in Celcius)')\nplt.title('Mean Avg. Temperature Over Time (Date) in the world')\nplt.show()\n\nplt.subplot(2,1,2)\nsns.set_style(\"darkgrid\")\nsns.lineplot(data = world_temp_year, color = 'blue')\nplt.xlabel('Time')\nplt.ylabel('Temperature (in Celcius)')\nplt.title('Mean Avg. Temperature Over Time (Years) in the world')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average Temperatures for various countries over time from 1995 to 2019","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating a function to plot the temperature over the Periods in different countries\ndef plot_temp_country_month(country, format = '-', temp = 'Celcius'):\n    dat = df[df['Country'] == country]\n    if temp == 'Celcius':\n        dat_temp = pd.Series(round(dat.groupby('Date')['AvgTempCelcius'].mean().sort_values(),2))\n    else:\n        dat_temp = pd.Series(round(dat.groupby('Date')['AvgTemperature'].mean().sort_values(),2))\n    sns.set_style(\"darkgrid\")\n    sns.lineplot(data = dat_temp, color = 'red')\n    plt.xlabel('Time (Periods)')\n    plt.ylabel('Temperature (in %s)' %temp)\n    plt.title('Mean Avg. Temperature Over Time in %s' %country)\n    plt.show()\n\n    \n## Creating a function to plot the temperature over the Years in different countries\ndef plot_temp_country_year(country, format = '-', temp = 'Celcius'):\n    dat = df[df['Country'] == country]\n    if temp == 'Celcius':\n        dat_temp = pd.DataFrame(pd.Series(round(df[df['Country'] == country].groupby('Year')['AvgTempCelcius'].mean(),2))[:-1])\n    else:\n        dat_temp = pd.DataFrame(pd.Series(round(df[df['Country'] == country].groupby('Year')['AvgTemperature'].mean(),2))[:-1])\n    sns.set_style(\"darkgrid\")\n    sns.lineplot(data = dat_temp, color = 'red' , style = 'event', hue = 'cue')\n    plt.xlabel('Time (Years)')\n    plt.ylabel('Temperature (in %s)' %temp)\n    plt.title('Mean Avg. Temperature Over Time in %s' %country)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_temp_country_year('India')\nplot_temp_country_month('India')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Temperature Fluctuation over time from 1995 to 2019","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In the section the temperature fluctuations from 1996 to 2019 relative to the year 1995 are calculated and visualised. Below are the functions defined:\n\nNote: The analysis has been carried out in Celcius scale ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to calculate the temperature fluctuation\ndef calculate_fluctuation(series):\n    fluctuation = np.zeros((len(series),))\n    for i in range(1,len(series)):\n        fluctuation[i] = series[i] - series[0]\n    return fluctuation\n\n## Function to plot the temperature fluctuation Lineplot\ndef plot_change(years, fluctuation, entity):\n    change_df = pd.DataFrame(np.column_stack((years, fluctuation)), columns = ['Year', 'Change'])\n    change_df['Year'] = change_df['Year'].astype(int)\n    sns.lineplot(x = \"Year\", y = \"Change\", data = change_df, err_style=\"bars\", ci=68, label = entity)\n    x = np.zeros((len(change_df['Year']),1))\n    plt.plot(change_df['Year'], x, '--')\n    plt.title('Temperature fluctuations over the years')\n    plt.ylabel('Change in Temperature')\n\n## Function to plot the temperature fluctuation in the world \ndef world_change_temp():\n    dat = np.array(pd.Series(round(df.groupby('Year')['AvgTempCelcius'].mean(),2)))[:-1]\n    years = np.arange(1995,2020)\n    fluctuation = calculate_fluctuation(dat)\n    plot_change(years, fluctuation, 'World')\n\n## Function to plot the temperature fluctuation in every country\ndef country_change_temp(country):\n    dat = np.array(pd.Series(round(df[df['Country'] == country].groupby('Year')['AvgTempCelcius'].mean(),2)))[:-1]\n    years = np.arange(1995,2020)\n    fluctuation = calculate_fluctuation(dat)\n    plot_change(years, fluctuation, country)\n\n## Function to plot the temperature fluctuation in every continent\ndef region_change_temp(region):\n    dat = np.array(pd.Series(round(df[df['Region'] == region].groupby('Year')['AvgTempCelcius'].mean(),2)))[:-1]\n    years = np.arange(1995,2020)\n    fluctuation = calculate_fluctuation(dat)\n    plot_change(years, fluctuation, region)\n\n## Function to plot the average temperature and temperature fluctutaion distribution for every country\ndef temperature_histogram(country):\n    hist1_s = np.array(pd.Series(round(df.groupby('Year')['AvgTempCelcius'].mean(),2)))[:-1]\n    hist2_s = np.array(pd.Series(round(df[df['Country'] == country].groupby('Year')['AvgTempCelcius'].mean(),2)))[:-1]\n    hist1_fluctuation = calculate_fluctuation(hist1_s)\n    hist2_fluctuation = calculate_fluctuation(hist2_s)\n    print('Skewness for Temperature in %s: ' %country, df[df['Country'] == country]['AvgTempCelcius'].skew())\n    print('Kurtosis for Temperature in %s: ' %country, df[df['Country'] == country]['AvgTempCelcius'].kurt())\n    plt.figure(figsize = (10,5))\n    plt.subplot(1,2,1)\n    sns.distplot(df[df['Country'] == country]['AvgTempCelcius'], label = country)\n    sns.distplot(df['AvgTempCelcius'], label = 'World')\n    plt.legend()\n    plt.subplot(1,2,2)\n    sns.distplot(hist1_fluctuation , label = 'World')\n    sns.distplot(hist2_fluctuation, label = country)\n    plt.xlabel('Temperature Fluctuations')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Temperature fluctuation in the world","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The Average world temperature has continually increased after 1995. There have been ups and downs. One can also find that the temperature has dropped during the 2008 economic crisis. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"world_change_temp()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Temperature fluctuation in a country","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the below graphs, we can find that there is a gradual increase in the temprerature from 1995 in the United States and China, but temperature has dropped in Canada. Also, we can find that the temperature dropped during the 2008 economic crisis. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country_change_temp('US')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_change_temp('Canada')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_change_temp('China')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Temperature fluctuation in a continent","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"region_change_temp('Africa')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Temperature and fluctuation distribution in a country","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temperature_histogram('US')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time - Series Forecasting Model with DNN, LSTM and CNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to create a forecasting model, the data has been considered as a time-series data and Neural networks are used to recognise and predict time series","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the series\nseries = np.array(list(pd.Series(round(df.groupby('Date')['AvgTempCelcius'].mean(),2))))\n\n# Creating time intervals\ntime = np.array(np.arange(0,len(series)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have 7000 training examples and rest as training examples\nsplit_time = 7000\n\n# Defining the Training set and test set\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\n# Initialising the Hyperparamenters\nwindow_size = 60\nbatch_size = 100\nshuffle_buffer_size = 1000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining functions to feed into the training Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to create a line plot\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(False)\n\n## Function to prepare data to be fed into the Tensorflow model\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    dp = tf.data.Dataset.from_tensor_slices(series)\n    dp = dp.window(window_size + 1, shift=1, drop_remainder=True)\n    dp = dp.flat_map(lambda w: w.batch(window_size + 1))\n    dp = dp.shuffle(shuffle_buffer)\n    dp = dp.map(lambda w: (w[:-1], w[1:]))\n    return dp.batch(batch_size).prefetch(1)\n\n## Function to prepare validation data into the model for prediction\ndef model_forecast(model, series, window_size):\n    dp = tf.data.Dataset.from_tensor_slices(series)\n    dp = dp.window(window_size, shift=1, drop_remainder=True)\n    dp = dp.flat_map(lambda w: w.batch(window_size))\n    dp = dp.batch(32).prefetch(1)\n    forecast = model.predict(dp)\n    return forecast","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Time series","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the time series\nplot_series(time, series)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining and running the model with callbacks to tune Learning rate for SGD optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## The model contains 1 ConV1D filter, 2 LSTMs, 3 Dense layers and 1 Lambda layer\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\n## Defining window sizes for hyperparameter tuning \nwindow_size = 64\nbatch_size = 256\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n\n# Defining the model\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n# Create a callback function to get optimal learning rate\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 20))\n\n# Defining the optimizer\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n\n# Compiling the model with Huber loss function \nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n\n# Running the model\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting learning rate vs loss","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Plotting a line plot of Learning rate vs loss to find the learning rate with minimal loss. Here we can see that at e-6 the loss is very low. Hence the learning rate is chosen to be 1e-6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0, 60])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the model and training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\ntrain_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\n\n# Defining the model with 1 ConV1D filter, 2 LSTMs, 3 Dense layers and 1 Lambda layer\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n# Defining the optimizer with learning rate 1e-6 and momentum 0.9\noptimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\n\n# Compiling model with Huber loss function\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n\n# Training with 50 epochs\nhistory = model.fit(train_set,epochs=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a forecast for the validation data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting the validation data and the forecast","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the mean absolute error of the validation set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can find that the MAE for the validation set is pretty low and hence the model is a good forcasting model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}