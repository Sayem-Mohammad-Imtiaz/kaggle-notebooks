{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings \n\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T10:21:55.145865Z","iopub.execute_input":"2021-07-22T10:21:55.146452Z","iopub.status.idle":"2021-07-22T10:21:55.164786Z","shell.execute_reply.started":"2021-07-22T10:21:55.146387Z","shell.execute_reply":"2021-07-22T10:21:55.163789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tweets-with-sarcasm-and-irony/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:55.561125Z","iopub.execute_input":"2021-07-22T10:21:55.561483Z","iopub.status.idle":"2021-07-22T10:21:55.954024Z","shell.execute_reply.started":"2021-07-22T10:21:55.561453Z","shell.execute_reply":"2021-07-22T10:21:55.952971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/tweets-with-sarcasm-and-irony/test.csv')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:55.955884Z","iopub.execute_input":"2021-07-22T10:21:55.956241Z","iopub.status.idle":"2021-07-22T10:21:56.010326Z","shell.execute_reply.started":"2021-07-22T10:21:55.956211Z","shell.execute_reply":"2021-07-22T10:21:56.00948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's visualize the classes \ntrain['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:56.011772Z","iopub.execute_input":"2021-07-22T10:21:56.01205Z","iopub.status.idle":"2021-07-22T10:21:56.043102Z","shell.execute_reply.started":"2021-07-22T10:21:56.012024Z","shell.execute_reply":"2021-07-22T10:21:56.041976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:56.087282Z","iopub.execute_input":"2021-07-22T10:21:56.087666Z","iopub.status.idle":"2021-07-22T10:21:56.097987Z","shell.execute_reply.started":"2021-07-22T10:21:56.087631Z","shell.execute_reply":"2021-07-22T10:21:56.097031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's import some visualization Library\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:56.240223Z","iopub.execute_input":"2021-07-22T10:21:56.240624Z","iopub.status.idle":"2021-07-22T10:21:56.244816Z","shell.execute_reply.started":"2021-07-22T10:21:56.240587Z","shell.execute_reply":"2021-07-22T10:21:56.243629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(22, 6))\nsns.countplot(train['class'], ax = ax[0])\nsns.countplot(test['class'], ax = ax[1])\nax[0].set_title('Training', size=19)\nax[1].set_title('Testing', size=19)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:56.367215Z","iopub.execute_input":"2021-07-22T10:21:56.367618Z","iopub.status.idle":"2021-07-22T10:21:56.748812Z","shell.execute_reply.started":"2021-07-22T10:21:56.367581Z","shell.execute_reply":"2021-07-22T10:21:56.747832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's check the shape of the dataset \n\nprint(f'Shape of the training dataset is : {train.shape}')\nprint(f'Shape of the testing dataset is : {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:56.751044Z","iopub.execute_input":"2021-07-22T10:21:56.751499Z","iopub.status.idle":"2021-07-22T10:21:56.756537Z","shell.execute_reply.started":"2021-07-22T10:21:56.751459Z","shell.execute_reply":"2021-07-22T10:21:56.755794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's check for the null values in dataste \nprint(f'Null values : \\n{train.isna().sum()}')\nprint(f'Null values : \\n{test.isna().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:57.132857Z","iopub.execute_input":"2021-07-22T10:21:57.133226Z","iopub.status.idle":"2021-07-22T10:21:57.159582Z","shell.execute_reply.started":"2021-07-22T10:21:57.133195Z","shell.execute_reply":"2021-07-22T10:21:57.158801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# two null values present in the test dataset .. we will drop these null values \n\ntest.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:57.302374Z","iopub.execute_input":"2021-07-22T10:21:57.302943Z","iopub.status.idle":"2021-07-22T10:21:57.326527Z","shell.execute_reply.started":"2021-07-22T10:21:57.302908Z","shell.execute_reply":"2021-07-22T10:21:57.325512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's check again the null values\nprint(f'Null values : \\n{test.isna().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:57.472203Z","iopub.execute_input":"2021-07-22T10:21:57.47257Z","iopub.status.idle":"2021-07-22T10:21:57.480485Z","shell.execute_reply.started":"2021-07-22T10:21:57.472539Z","shell.execute_reply":"2021-07-22T10:21:57.479754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing ","metadata":{}},{"cell_type":"code","source":"s = train['tweets'][0]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:57.992206Z","iopub.execute_input":"2021-07-22T10:21:57.992711Z","iopub.status.idle":"2021-07-22T10:21:57.996021Z","shell.execute_reply.started":"2021-07-22T10:21:57.992679Z","shell.execute_reply":"2021-07-22T10:21:57.99522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install emoji","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:21:58.163218Z","iopub.execute_input":"2021-07-22T10:21:58.163567Z","iopub.status.idle":"2021-07-22T10:22:06.763924Z","shell.execute_reply.started":"2021-07-22T10:21:58.163538Z","shell.execute_reply":"2021-07-22T10:22:06.762832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport re \nfrom lxml import html\nfrom emoji import demojize\n\nimport nltk\nnltk.download('stopwords')\n\n\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n\nstemmer = PorterStemmer()\nstop = stopwords.words('english')\n\ndef clean_text(text):\n    \n    # Remove Hyperlinks\n    text = re.sub('http\\S+', ' ', text)\n\n    # Remove non alphabets\n    text = re.sub('[^a-zA-Z ]+', ' ', text)\n\n    # Lowercase and split\n    text = text.lower().split()\n\n    # Remove stopwords and short words\n    text = [stemmer.stem(word) for word in text if word not in stop and len(word) > 2]\n\n    # Join and Return\n    return ' '.join(text)\n\ndef clean_text2(tweet):\n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    #emojis\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    tweet =  emoji_pattern.sub(r'', tweet)\n    \n    # usernames mentions like \"@abc123\"\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n    tweet =  ment.sub(r'', tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # html tags\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    tweet = re.sub(html, '', tweet)\n    \n    # Urls\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n    tweet = re.sub(r'https?://\\S+|www\\.\\S+','', tweet)\n        \n    #Punctuations and special characters\n    \n    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n    \n    tweet = tweet.lower()\n    \n    splits = tweet.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    tweet = ' '.join(splits)\n    \n    return tweet\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:45:12.515806Z","iopub.execute_input":"2021-07-22T10:45:12.516504Z","iopub.status.idle":"2021-07-22T10:45:12.562114Z","shell.execute_reply.started":"2021-07-22T10:45:12.516437Z","shell.execute_reply":"2021-07-22T10:45:12.561373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Original String: {s}')\nprint(f'Cleaned String: {clean_text2(s)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:45:14.469833Z","iopub.execute_input":"2021-07-22T10:45:14.47032Z","iopub.status.idle":"2021-07-22T10:45:14.477304Z","shell.execute_reply.started":"2021-07-22T10:45:14.470288Z","shell.execute_reply":"2021-07-22T10:45:14.47653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = train['tweets'].apply(clean_text2)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:45:25.595613Z","iopub.execute_input":"2021-07-22T10:45:25.596118Z","iopub.status.idle":"2021-07-22T10:48:19.60202Z","shell.execute_reply.started":"2021-07-22T10:45:25.596087Z","shell.execute_reply":"2021-07-22T10:48:19.600823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:19.651108Z","iopub.execute_input":"2021-07-22T10:48:19.651553Z","iopub.status.idle":"2021-07-22T10:48:19.665565Z","shell.execute_reply.started":"2021-07-22T10:48:19.651516Z","shell.execute_reply":"2021-07-22T10:48:19.664541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nle = le.fit(train['class'])","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:19.666985Z","iopub.execute_input":"2021-07-22T10:48:19.667304Z","iopub.status.idle":"2021-07-22T10:48:19.680423Z","shell.execute_reply.started":"2021-07-22T10:48:19.667276Z","shell.execute_reply":"2021-07-22T10:48:19.678977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = le.transform(train['class'])","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:19.681747Z","iopub.execute_input":"2021-07-22T10:48:19.682043Z","iopub.status.idle":"2021-07-22T10:48:19.715144Z","shell.execute_reply.started":"2021-07-22T10:48:19.682015Z","shell.execute_reply":"2021-07-22T10:48:19.714308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text[:10], labels[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:19.716457Z","iopub.execute_input":"2021-07-22T10:48:19.716988Z","iopub.status.idle":"2021-07-22T10:48:19.733138Z","shell.execute_reply.started":"2021-07-22T10:48:19.716954Z","shell.execute_reply":"2021-07-22T10:48:19.731941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFIDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:19.735554Z","iopub.execute_input":"2021-07-22T10:48:19.735939Z","iopub.status.idle":"2021-07-22T10:48:19.747306Z","shell.execute_reply.started":"2021-07-22T10:48:19.735893Z","shell.execute_reply":"2021-07-22T10:48:19.74633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_train = tfidf.fit_transform(text).todense()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:19.749226Z","iopub.execute_input":"2021-07-22T10:48:19.749722Z","iopub.status.idle":"2021-07-22T10:48:25.219266Z","shell.execute_reply.started":"2021-07-22T10:48:19.749674Z","shell.execute_reply":"2021-07-22T10:48:25.218157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same for test data\ntest_text = test['tweets'].apply(clean_text)\ntest_labels = le.transform(test['class'])\n\ntexts_test = tfidf.transform(test_text).todense()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:25.220368Z","iopub.execute_input":"2021-07-22T10:48:25.220666Z","iopub.status.idle":"2021-07-22T10:48:28.343793Z","shell.execute_reply.started":"2021-07-22T10:48:25.220638Z","shell.execute_reply":"2021-07-22T10:48:28.342798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:28.345141Z","iopub.execute_input":"2021-07-22T10:48:28.345449Z","iopub.status.idle":"2021-07-22T10:48:28.349785Z","shell.execute_reply.started":"2021-07-22T10:48:28.345422Z","shell.execute_reply":"2021-07-22T10:48:28.348711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(texts_train, labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:28.351035Z","iopub.execute_input":"2021-07-22T10:48:28.351371Z","iopub.status.idle":"2021-07-22T10:48:50.71833Z","shell.execute_reply.started":"2021-07-22T10:48:28.351339Z","shell.execute_reply":"2021-07-22T10:48:50.717346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(test_labels, clf.predict(texts_test)))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:50.71979Z","iopub.execute_input":"2021-07-22T10:48:50.720345Z","iopub.status.idle":"2021-07-22T10:48:52.69825Z","shell.execute_reply.started":"2021-07-22T10:48:50.720301Z","shell.execute_reply":"2021-07-22T10:48:52.697156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looks like our model is able to predict class 2 better than any other class \n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:52.699761Z","iopub.execute_input":"2021-07-22T10:48:52.700356Z","iopub.status.idle":"2021-07-22T10:48:52.705017Z","shell.execute_reply.started":"2021-07-22T10:48:52.700313Z","shell.execute_reply":"2021-07-22T10:48:52.703682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(test_labels, clf.predict(texts_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:48:52.70665Z","iopub.execute_input":"2021-07-22T10:48:52.70751Z","iopub.status.idle":"2021-07-22T10:48:53.474214Z","shell.execute_reply.started":"2021-07-22T10:48:52.707427Z","shell.execute_reply":"2021-07-22T10:48:53.473178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# COUNT VECTORIZER ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(text)\n\n# transform the training and test data using count vectorizer object\ntextcv =  count_vect.transform(text)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:56:36.265549Z","iopub.execute_input":"2021-07-22T10:56:36.265942Z","iopub.status.idle":"2021-07-22T10:56:38.917481Z","shell.execute_reply.started":"2021-07-22T10:56:36.265909Z","shell.execute_reply":"2021-07-22T10:56:38.916374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textcv.todense()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:56:53.544659Z","iopub.execute_input":"2021-07-22T10:56:53.545023Z","iopub.status.idle":"2021-07-22T10:56:55.74761Z","shell.execute_reply.started":"2021-07-22T10:56:53.544992Z","shell.execute_reply":"2021-07-22T10:56:55.746513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testcv = count_vect.transform(test_text)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:58:05.750557Z","iopub.execute_input":"2021-07-22T10:58:05.750951Z","iopub.status.idle":"2021-07-22T10:58:05.870575Z","shell.execute_reply.started":"2021-07-22T10:58:05.750916Z","shell.execute_reply":"2021-07-22T10:58:05.869609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(textcv, labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:58:23.231227Z","iopub.execute_input":"2021-07-22T10:58:23.231746Z","iopub.status.idle":"2021-07-22T10:58:23.269741Z","shell.execute_reply.started":"2021-07-22T10:58:23.231713Z","shell.execute_reply":"2021-07-22T10:58:23.268876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_labels, clf.predict(testcv)))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:59:17.310471Z","iopub.execute_input":"2021-07-22T10:59:17.31106Z","iopub.status.idle":"2021-07-22T10:59:17.346287Z","shell.execute_reply.started":"2021-07-22T10:59:17.311013Z","shell.execute_reply":"2021-07-22T10:59:17.345229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(test_labels, clf.predict(testcv))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T10:59:42.74987Z","iopub.execute_input":"2021-07-22T10:59:42.750464Z","iopub.status.idle":"2021-07-22T10:59:42.772941Z","shell.execute_reply.started":"2021-07-22T10:59:42.750411Z","shell.execute_reply":"2021-07-22T10:59:42.771804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}