{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification practice\nPractice using \n* Random forest\n* SVM\n* Convnet\n* Precompose with dimensional reduction e.g. PCA\n\n# Summary of results so far:\n* Random forest is not bad considering simplicity and lack of concern\n\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"chinese_mnist=pd.read_csv('../input/chinese-mnist/chinese_mnist.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chinese_mnist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_PATH = '..//input//chinese-mnist//data//data//'\nIMAGE_WIDTH = 64\nIMAGE_HEIGHT = 64\nIMAGE_CHANNELS = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_files = list(os.listdir(IMAGE_PATH))\nprint(\"# of image files:{}\".format(len(image_files)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_file_name(x):\n    file_name=f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\ndata_df=copy.copy(chinese_mnist)\ndata_df[\"file\"]=data_df.apply(create_file_name,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image_sizes(path,file_name):\n    image = skimage.io.imread(path+file_name)\n    return list(image.shape)\nm=np.stack(data_df['file'].apply(lambda f:read_image_sizes(IMAGE_PATH,f)))\ndf = pd.DataFrame(m,columns=['w','h'])\ndata_df=pd.concat([data_df,df],axis=1,sort=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of suites: {data_df.suite_id.nunique()}\")\nprint(f\"Samples: {data_df.sample_id.unique()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting up data for model\nThe data we have is a pandas dataset. We wish to form a predictive map\n64x64 np.array -> code\n\nFor the first project. Let's split the dataset into train and test.\n\nWe will now load the images into a 3d np array \n\nnum_images x 64 x 64."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_images=len(data_df)\nimage_dim=64\n\nimage_array=np.zeros((num_images,image_dim,image_dim))\nfor im_dex in range(num_images):\n    image_array[im_dex,:,:]=\\\n    skimage.io.imread(IMAGE_PATH+data_df['file'][im_dex])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code_array=np.array(data_df['code'])\n#code_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(image_array[1,:,:])\nimage_array[1,:,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random forest approach: up to 64.6%. You can probably need more but then you will need a lot of trees and computation time.\n\n# Apparently random forest is >90% accurate on Arabic MNIST. Can we find out how to do this in a tractable manner for this data-set."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrand_perm=np.random.permutation(len(image_array))\nimage_array=image_array[rand_perm,:,:]\ncode_array=code_array[rand_perm]\ntrain_im=image_array[:int(np.floor(len(image_array)/2)),:,:]\ntest_im=image_array[int(np.floor(len(image_array)/2)):,:,:]\ntrain_code=code_array[:int(np.floor(len(image_array)/2))]\ntest_code=code_array[int(np.floor(len(image_array)/2)):]\ntrain=np.reshape(train_im,\n                 (int(np.floor(len(image_array)/2)),64**2))\ntest=np.reshape(test_im,\n                 (int(np.ceil(len(image_array)/2)),64**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_jobs=-1, n_estimators=300)\nrfc.fit(train,train_code)\nprint('test result: ',rfc.score(test,test_code),'train result: ', rfc.score(train,train_code))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convnet approach: 97.8% accurate on test"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrand_perm=np.random.permutation(len(image_array))\nimage_array=image_array[rand_perm,:,:]\ncode_array=code_array[rand_perm]\ntrain_im=image_array[:int(np.floor(len(image_array)/2)),:,:]\ntest_im=image_array[int(np.floor(len(image_array)/2)):,:,:]\ntrain_code=code_array[:int(np.floor(len(image_array)/2))]\ntest_code=code_array[int(np.floor(len(image_array)/2)):]\n\nfrom keras.utils import to_categorical\ntrain_images=train_im.reshape((7500,64,64,1))\ntrain_images=train_images.astype('float32')\ntrain_code=train_code-1\ntrain_labels=to_categorical(train_code)\n\ntest_images=test_im.reshape((7500,64,64,1))\ntest_images=test_images.astype('float32')\ntest_code=test_code-1\ntest_labels=to_categorical(test_code)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models, layers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32,(9,9),activation='relu', \n                        input_shape=(64,64,1)))\n\n\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64,(9,9),activation='relu'))\n\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64,(9,9),activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(15,activation='softmax'))\n#model.add(layers.Flatten())\nmodel.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nmodel.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_images,train_labels,epochs=7,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss,test_acc=model.evaluate(test_images,test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means- a sparse non-negative matrix factorization\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_perm=np.random.permutation(len(image_array))\nimage_array=image_array[rand_perm,:,:]\ncode_array=code_array[rand_perm]\ntrain_im=image_array[:int(np.floor(len(image_array)/2)),:,:]\ntest_im=image_array[int(np.floor(len(image_array)/2)):,:,:]\ntrain_code=code_array[:int(np.floor(len(image_array)/2))]\ntest_code=code_array[int(np.floor(len(image_array)/2)):]\ntrain=np.reshape(train_im,\n                 (int(np.floor(len(image_array)/2)),64**2))\ntest=np.reshape(test_im,\n                 (int(np.ceil(len(image_array)/2)),64**2))\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nprint('start kmeans')\nKM_model=KMeans(init='k-means++',n_clusters=15,n_init=10)\nKM_model.fit(train) \nprint('end kmeans')\nprint('start PCA')\nreduced_train=PCA(n_components=2).fit_transform(train)\nprint('end PCA')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(reduced_train[:, 0], reduced_train[:, 1], 'k.', markersize=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nh=2\nx_min, x_max = reduced_train[:, 0].min() - 1, reduced_train[:, 0].max() + 1\ny_min, y_max = reduced_train[:, 1].min() - 1, reduced_train[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\nPCA_train=PCA(n_components=2).fit(train)\nproj_train=PCA_train.inverse_transform(np.c_[xx.ravel(),yy.ravel()])\n# Obtain labels for each point in mesh. Use last trained model.\nZ = KM_model.predict(proj_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nZ = Z.reshape(xx.shape)\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\nfor l in range(1,16):\n    plt.plot(reduced_train[train_code==l, 0], reduced_train[train_code==l, 1], 'x', markersize=5)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}