{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data = pd.read_csv('../input/spam.csv', encoding = \"ISO-8859-1\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data = email_data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis = 1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"     v1                                                 v2\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apostrophe Dictionary\napostrophe_dict = {\n\"ain't\": \"am not / are not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is\",\n\"i'd\": \"I had / I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I shall / I will\",\n\"i'll've\": \"I shall have / I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"short_word_dict = {\n\"121\": \"one to one\",\n\"a/s/l\": \"age, sex, location\",\n\"adn\": \"any day now\",\n\"afaik\": \"as far as I know\",\n\"afk\": \"away from keyboard\",\n\"aight\": \"alright\",\n\"alol\": \"actually laughing out loud\",\n\"b4\": \"before\",\n\"b4n\": \"bye for now\",\n\"bak\": \"back at the keyboard\",\n\"bf\": \"boyfriend\",\n\"bff\": \"best friends forever\",\n\"bfn\": \"bye for now\",\n\"bg\": \"big grin\",\n\"bta\": \"but then again\",\n\"btw\": \"by the way\",\n\"cid\": \"crying in disgrace\",\n\"cnp\": \"continued in my next post\",\n\"cp\": \"chat post\",\n\"cu\": \"see you\",\n\"cul\": \"see you later\",\n\"cul8r\": \"see you later\",\n\"cya\": \"bye\",\n\"cyo\": \"see you online\",\n\"dbau\": \"doing business as usual\",\n\"fud\": \"fear, uncertainty, and doubt\",\n\"fwiw\": \"for what it's worth\",\n\"fyi\": \"for your information\",\n\"g\": \"grin\",\n\"g2g\": \"got to go\",\n\"ga\": \"go ahead\",\n\"gal\": \"get a life\",\n\"gf\": \"girlfriend\",\n\"gfn\": \"gone for now\",\n\"gmbo\": \"giggling my butt off\",\n\"gmta\": \"great minds think alike\",\n\"h8\": \"hate\",\n\"hagn\": \"have a good night\",\n\"hdop\": \"help delete online predators\",\n\"hhis\": \"hanging head in shame\",\n\"iac\": \"in any case\",\n\"ianal\": \"I am not a lawyer\",\n\"ic\": \"I see\",\n\"idk\": \"I don't know\",\n\"imao\": \"in my arrogant opinion\",\n\"imnsho\": \"in my not so humble opinion\",\n\"imo\": \"in my opinion\",\n\"iow\": \"in other words\",\n\"ipn\": \"I’m posting naked\",\n\"irl\": \"in real life\",\n\"jk\": \"just kidding\",\n\"l8r\": \"later\",\n\"ld\": \"later, dude\",\n\"ldr\": \"long distance relationship\",\n\"llta\": \"lots and lots of thunderous applause\",\n\"lmao\": \"laugh my ass off\",\n\"lmirl\": \"let's meet in real life\",\n\"lol\": \"laugh out loud\",\n\"ltr\": \"longterm relationship\",\n\"lulab\": \"love you like a brother\",\n\"lulas\": \"love you like a sister\",\n\"luv\": \"love\",\n\"m/f\": \"male or female\",\n\"m8\": \"mate\",\n\"milf\": \"mother I would like to fuck\",\n\"oll\": \"online love\",\n\"omg\": \"oh my god\",\n\"otoh\": \"on the other hand\",\n\"pir\": \"parent in room\",\n\"ppl\": \"people\",\n\"r\": \"are\",\n\"rofl\": \"roll on the floor laughing\",\n\"rpg\": \"role playing games\",\n\"ru\": \"are you\",\n\"shid\": \"slaps head in disgust\",\n\"somy\": \"sick of me yet\",\n\"sot\": \"short of time\",\n\"thanx\": \"thanks\",\n\"thx\": \"thanks\",\n\"ttyl\": \"talk to you later\",\n\"u\": \"you\",\n\"ur\": \"you are\",\n\"uw\": \"you’re welcome\",\n\"wb\": \"welcome back\",\n\"wfm\": \"works for me\",\n\"wibni\": \"wouldn't it be nice if\",\n\"wtf\": \"what the fuck\",\n\"wtg\": \"way to go\",\n\"wtgp\": \"want to go private\",\n\"ym\": \"young man\",\n\"gr8\": \"great\"\n}","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_words = []\nremove_words += stopwords.words('english')\nremove_words += list(string.punctuation)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utlity to simplify the lemmatization\ndef get_simple_pos_tag(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADJ\n    else:\n        return wordnet.NOUN\n\n# Utility to remove stopwords\ndef remove_patterns(pattern, data):\n    pattern_words = re.findall(pattern, data) \n    for word in pattern_words:\n            data = re.sub(word, '', data)\n    return data\n\n# Utlity to perform lemmatization\ndef word_lemma_(data):\n    lemmatizer = WordNetLemmatizer()\n    text = ''\n    for word in data:\n        pos = pos_tag([word])\n        clean_word = lemmatizer.lemmatize(word, pos = get_simple_pos_tag(pos[0][1]))\n        text += clean_word+' '\n    return text","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all email ID's from data\nemail_data['v2'] = np.vectorize(remove_patterns)('\\w+[@]\\w+[.]\\w+', email_data['v2'])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manipulate Data\ndef manipulate_text_(dictionary, text):\n    for word in text.split(' '):\n        if word.lower() in dictionary:\n            text = text.replace(word, dictionary[word.lower()])\n    return text","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all apostrophes from data\nemail_data['clean_apostrophe'] = email_data['v2'].apply(lambda x: manipulate_text_(apostrophe_dict, x))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update all short words in data\nemail_data['clean_short_words'] = email_data['clean_apostrophe'].apply(lambda x: manipulate_text_(short_word_dict, x))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update all emoticons in data\n#email_data['clean_emoticons'] = email_data['clean_short_words'].apply(lambda x: manipulate_text_(emoticon_dict, x))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all extra characters in data\nemail_data['clean_emails'] = email_data['clean_short_words'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all stopwords and punctuations from data\nemail_data['clean_emails'] = email_data['clean_emails'].apply(lambda x: [word for word in x.split(' ') if len(word) > 1 if word not in remove_words])","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"     v1                        ...                                                               clean_emails\n0   ham                        ...                          [Go, jurong, point, crazy, Available, bugis, g...\n1   ham                        ...                                                [Ok, lar, Joking, wif, oni]\n2  spam                        ...                          [Free, entry, wkly, comp, win, FA, Cup, final,...\n3   ham                        ...                                       [dun, say, early, hor, already, say]\n4   ham                        ...                             [Nah, think, goes, usf, lives, around, though]\n\n[5 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>clean_apostrophe</th>\n      <th>clean_short_words</th>\n      <th>clean_emails</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>[Go, jurong, point, crazy, Available, bugis, g...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>Ok lar... Joking wif you oni...</td>\n      <td>[Ok, lar, Joking, wif, oni]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>[Free, entry, wkly, comp, win, FA, Cup, final,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>you dun say so early hor... you c already then...</td>\n      <td>[dun, say, early, hor, already, say]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>Nah I do not think he goes to usf, he lives ar...</td>\n      <td>Nah I do not think he goes to usf, he lives ar...</td>\n      <td>[Nah, think, goes, usf, lives, around, though]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatize the Data\nemail_data['lemmatized_emails'] = email_data['clean_emails'].apply(lambda x: word_lemma_(x))","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.1, max_features=1500)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=0.9, max_features=1500, min_df=0.1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n        stop_words=None, strip_accents=None, sublinear_tf=False,\n        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n        vocabulary=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"apostrphe_transformed = tfidf_vectorizer.fit_transform(email_data['clean_apostrophe'])\na_x_train, a_x_test, a_y_train, a_y_test = train_test_split(apostrphe_transformed, email_data['v1'])\n\napostrphe_clf = LogisticRegression()\napostrphe_clf.fit(a_x_train, a_y_train)\na_y_pred = apostrphe_clf.predict(a_x_test)\n\na_accuracy = accuracy_score(a_y_test, a_y_pred)\nprint('Accuracy is: ',a_accuracy)\n\na_cm_ = confusion_matrix(a_y_test, a_y_pred)\nprint('Confusion Matrix: ',a_cm_)","execution_count":20,"outputs":[{"output_type":"stream","text":"Accuracy is:  0.9002153625269204\nConfusion Matrix:  [[1220   28]\n [ 111   34]]\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_text_transformed = tfidf_vectorizer.fit_transform(email_data['lemmatized_emails'])\nx_train, x_test, y_train, y_test = train_test_split(email_text_transformed, email_data['v1'])\n\nclf = LogisticRegression()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy is: ',accuracy)\n\ncm_ = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix: ',cm_)","execution_count":21,"outputs":[{"output_type":"stream","text":"Accuracy is:  0.8765254845656856\nConfusion Matrix:  [[1142   73]\n [  99   79]]\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}