{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Diabetes 1999-2008 Dataset","metadata":{}},{"cell_type":"code","source":"# Requiered Installations:\n# pip install category_encoders\n# pip install missingno\n\n# Essential\nimport numpy as np\nimport pandas as pd\nimport re\n\n# Visualization\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n# preprocessing\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom category_encoders import BinaryEncoder\nfrom sklearn.utils import resample\nfrom imblearn.under_sampling import NearMiss\n\n# Metrics\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score,recall_score, roc_auc_score, roc_curve, classification_report,precision_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom mlxtend.evaluate import bias_variance_decomp\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the data\n\n**The data is splitted in 2 csv files so, we had to read both of them seperatly**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"diabetic_data.csv\")\nIDs_ds = pd.read_csv(\"IDs_mapping.csv\")\n\n# First (Main) Dataset\nwith pd.option_context(\"display.max_row\", 100, \"display.max_columns\", 100):\n        display(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second Dataset\nIDs_ds.head(len(IDs_ds))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA on the Dataset\n\n**In this section, we explore the data to gain more knowledge of what are we dealing with in the dataset.**","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Most of the dataset is non-numerical values and some of the attributes are number stored in a \"String\" form.**","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transforming the target variable to numerical and refining the nan values to be able to deal with later","metadata":{}},{"cell_type":"code","source":"data.readmitted = [1 if each=='<30' else 0 for each in data.readmitted]\ndata.replace('?', np.nan , inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the balance of the dataset\nax=data['readmitted'].value_counts().plot(kind='bar',color = ['#BB0000', '#0000BB'],\n                                          title='Number of readmitted and not readmitted',figsize=(8, 6))\nax.set_xlabel(\"readmitted\",fontsize=14)\nax.set_ylabel(\"Number of instaces\",fontsize=14)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looking at the barchart above, we can see that the dataset is heavily imbalanced with the class of not readmitted having most of the dataset.**","metadata":{}},{"cell_type":"markdown","source":"**Visualising the missing values in the dataset.**","metadata":{}},{"cell_type":"code","source":"msno.matrix(data)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(data,sort='descending',color='#66a9bc')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<strong>looking at the plots above</strong> we see that the missing values are in the columns: weight, medical_speciality, payer_code, race, and diag_1,2,3.","metadata":{}},{"cell_type":"code","source":"# This function helps giving more insights \n# about the nature of the values in the dataset like:\n# the missing values, unique values, the percentage of values missing in each column\ndef Missing_Values(data):\n    variable_name = []\n    total_value = []\n    total_missing_value = []\n    missing_value_rate = []\n    unique_value_list = []\n    total_unique_value = []\n    data_type = []\n    \n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()/data[col].shape[0],4))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n        \n    missing_data=pd.DataFrame({\"Variable\":variable_name,\\\n                               \"#_Total_Value\":total_value,\\\n                               \"#_Total_Missing_Value\":total_missing_value,\\\n                               \"%_Missing_Value_Rate\":missing_value_rate,\\\n                               \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value\n                              })\n    \n    missing_data = missing_data.set_index(\"Variable\")\n    return missing_data.sort_values(\"#_Total_Missing_Value\",ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_info = Missing_Values(data)\ndata_info","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the useless non-impactful columns\ndrop_list = ['examide' , 'citoglipton', 'weight','encounter_id','patient_nbr','payer_code','medical_specialty']  \ndata.drop(drop_list,axis=1, inplace=True)\ndata_info.drop(drop_list, axis=0,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting the numerical and non-numerical attributes for better preprocessing**","metadata":{}},{"cell_type":"code","source":"cols_num = ['time_in_hospital','num_lab_procedures', 'num_procedures', 'num_medications',\n       'number_outpatient', 'number_emergency', 'number_inpatient','number_diagnoses']\ndata[cols_num].isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_cat = ['race', 'gender', \n       'max_glu_serum', 'A1Cresult',\n       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n       'tolazamide', 'insulin',\n       'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n       'metformin-pioglitazone', 'change', 'diabetesMed']\ndata[cols_cat].isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 10))\ncorr = data.corr()\nsns.heatmap(corr,mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, annot = True, ax=ax)\nplt.title('The Correlation Between Features')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After looking at the coorelation map, we notice that the coorelations between the feature and the target class are low.**","metadata":{}},{"cell_type":"code","source":"data.hist(figsize=(25, 10))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the barchart of some of the features of the dataset, we see that there is some outliers which is going to be dealt with using clustring techniques.","metadata":{}},{"cell_type":"code","source":"# updating the NULL value to be able to get rid of them\ndata.gender.replace('Unknown/Invalid', np.nan , inplace=True)\ndata.dropna(subset=['gender'], how='all', inplace = True)\ndata.gender.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualising the distribution of the males vs femals in the dataset\nlabels=['Female','Male']\ndata.gender.value_counts().plot.pie(autopct=\"%1.2f%%\", colors=['#66a3ff','#facc99'], \n                                        labels=labels, explode = (0, 0.05), startangle=120,\n                                        textprops={'fontsize': 12, 'color':'#0a0a00'})\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the impact of each of 'gender','age','race' on the target class\nvisual_list = ['gender','age','race']\nfig, ax =plt.subplots(nrows=1,ncols=3,figsize=(24,8))\nax_col =0\nfor i in visual_list:\n    sns.countplot(x=data[i], hue=data.readmitted, ax=ax[ax_col]);\n    ax_col = ax_col+1","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looking at the figures above we see that females that are between 80-90 years old and their race is caucasin are the most readmitted among all the types of people.**","metadata":{}},{"cell_type":"code","source":"data.groupby(by = \"insulin\").readmitted.mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising the impact of insulin intake on the target class\nfig =plt.subplots(figsize=(8,4))\nsns.countplot(x=\"insulin\", hue=\"readmitted\", data=data, )\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The figure indicates that the people who does not take insulin are the most readmitted**","metadata":{}},{"cell_type":"code","source":"age_list = list(data.age.unique())\nsns.catplot(x=\"insulin\", hue=\"age\", data=data, kind=\"count\", height=6, aspect=2, palette=\"gnuplot\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The figure indicates that the people who does not take insulin and their age is between 70-80 are the most readmitted, and people with age between 70-80 are the most readmitted in all types of insulin intake**","metadata":{}},{"cell_type":"code","source":"# checking for null values in the race feature\ndata[\"race\"].fillna(data[\"race\"].mode()[0], inplace = True)\ndata[\"race\"].isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here we perform some function and technique to clean up the diagnosis columns 'diag_1','diag_2','diag_3' and make it more clear**","metadata":{}},{"cell_type":"code","source":"diag_list = ['diag_1','diag_2','diag_3']\n\nfor col in diag_list:\n    data[col].fillna('NaN', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformFunc(value):\n    value = re.sub(\"V[0-9]*\", \"0\", value) # V \n    value = re.sub(\"E[0-9]*\", \"0\", value) # E \n    value = re.sub('NaN', \"-1\", value) # Nan \n    return value\n\ndef transformCategory(value):\n    if value>=390 and value<=459 or value==785:\n        category = 'Circulatory'\n    elif value>=460 and value<=519 or value==786:\n        category = 'Respiratory'\n    elif value>=520 and value<=579 or value==787:\n        category = 'Digestive'\n    elif value==250:\n        category = 'Diabetes'\n    elif value>=800 and value<=999:\n        category = 'Injury'          \n    elif value>=710 and value<=739:\n        category = 'Musculoskeletal'   \n    elif value>=580 and value<=629 or value==788:\n        category = 'Genitourinary'\n    elif value>=140 and value<=239 :\n        category = 'Neoplasms'\n    elif value==-1:\n        category = 'NAN'\n    else :\n        category = 'Other'\n\n    return category","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in diag_list:\n    data[col] = data[col].apply(transformFunc)\n    data[col] = data[col].astype(float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in diag_list:\n    data[col] = data[col].apply(transformCategory)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(nrows=3,ncols=1,figsize=(15,12))\nrow_ax =0\nfor i in diag_list:\n    sns.countplot(x= data[i], hue=data.readmitted, ax=ax[row_ax], order = data[i].value_counts().index);\n    row_ax = row_ax+1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After Cleaning the diagnosis columns and plotting the distribution of it, we see that the people diagnosed with Circulatory are the most readmitted**","metadata":{}},{"cell_type":"markdown","source":"### Data Cleaning and Encoding","metadata":{}},{"cell_type":"code","source":"# Custom encoding for the 21 Drug Features\ndrugs = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', 'pioglitazone',\n        'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', 'tolazamide', 'metformin-pioglitazone',\n        'metformin-rosiglitazone', 'glimepiride-pioglitazone', 'glipizide-metformin', 'troglitazone', 'tolbutamide', 'acetohexamide']\n\nfor col in drugs:\n    data[col] = data[col].replace(['No','Steady','Up','Down'],[0,1,1,1])\n    data[col] = data[col].astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A1Cresult and max_glu_serum attributes cleanning\ndata['A1Cresult'] = data['A1Cresult'].replace(['>7','>8','Norm','None'],[1,1,0,-99])\ndata['max_glu_serum'] = data['max_glu_serum'].replace(['>200','>300','Norm','None'],[1,1,0,-99])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One hot Encoding Race and Id's \none_hot_data = pd.get_dummies(data, columns=['race'], prefix=[\"enc\"])\n\ncolumns_ids = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n\none_hot_data[columns_ids] = one_hot_data[columns_ids].astype('str')\none_hot_data = pd.get_dummies(one_hot_data, columns=columns_ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding the Rest of the categorical attributes\nbinary = ['change', 'diabetesMed', 'gender']\ndf = one_hot_data.copy()\n\nfor col in diag_list:\n    df[col] = LabelEncoder().fit_transform(df[col])\n\nfor col in binary:\n    df[col] = LabelEncoder().fit_transform(df[col])\n    \ndf['age'] = OrdinalEncoder().fit_transform(df['age'].values.reshape(-1, 1))\n\nX = df.drop(columns=\"readmitted\", axis=1)\nY = df.readmitted\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After Encoding all the attributes, now the dataset is ready for the preprocessing phase**","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Outlier Removal Using Clustring\n<strong>DBSCAN technique </strong> is used to deal with outliers**","metadata":{}},{"cell_type":"code","source":"model = DBSCAN(eps = 3,metric = 'manhattan',n_jobs=-1).fit(X)\noutliers = df[model.labels_ != -1]\ncluster = df[model.labels_ == -1]\nprint(\"The number of outliers that has been removed \", outliers.shape[0],\"which is: \", \"{:.2f}\".format(outliers.shape[0]/ X.shape[0] *100), \"%\")\nprint(\"The number of data without outliers \",cluster.shape[0],\"which is: \", \"{:.2f}\".format(cluster.shape[0]/ X.shape[0] *100),\"%\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the new Dataset after clustring\nX = cluster.drop(columns=\"readmitted\", axis=1)\nY = cluster.readmitted","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster['readmitted'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Undersampling (NearMiss) and Downsampling","metadata":{}},{"cell_type":"code","source":"# Using NearMiss method to undersample the data\nsampler = NearMiss(version=1,sampling_strategy={0: 20000, 1: 11356})\nx_under, y_under = sampler.fit_sample(X, Y)\n\n# Creating the new train and validation after the upsampling\nXu_train, Xu_val, yu_train, yu_val = train_test_split(x_under, y_under, test_size = 0.20, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=y_under.value_counts().plot(kind='bar',color = ['#BB0000', '#0000BB'],\n                                          title='Number of readmitted and not readmitted',figsize=(8, 6))\nax.set_xlabel(\"readmitted\",fontsize=14)\nax.set_ylabel(\"Number of instaces\",fontsize=14)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After performing the undersampling we see that the data is more balanced now**","metadata":{}},{"cell_type":"code","source":"X = pd.concat([X_train, y_train], axis=1)\n\nnot_readmitted = X[X.readmitted==0]\nreadmitted = X[X.readmitted==1]\n\nnot_readmitted_sampled = resample(not_readmitted,\n                                replace = False, \n                                n_samples = len(readmitted),\n                                random_state = 42)\n\ndownsampled = pd.concat([not_readmitted_sampled, readmitted])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=downsampled['readmitted'].value_counts().plot(kind='bar',color = ['#BB0000', '#0000BB'],\n                                          title='Number of readmitted and not readmitted',figsize=(8, 6))\nax.set_xlabel(\"readmitted\",fontsize=14)\nax.set_ylabel(\"Number of instaces\",fontsize=14)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After performing the downampling we see that the data is more balanced but we lost so many instances.**","metadata":{}},{"cell_type":"code","source":"# creating the train and validation after the downsampling\ny_down = downsampled.readmitted\nX_down = downsampled.drop('readmitted', axis=1)\nXd_train, Xd_val, yd_train, yd_val = train_test_split(X_down, y_down, test_size = 0.2, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Scaler to finish the preprocessing phase\nsc1= StandardScaler()\nXu_train = sc1.fit_transform(Xu_train)\nXu_val = sc1.transform(Xu_val)\n\nXd_train = sc1.fit_transform(Xd_train)\nXd_val = sc1.transform(Xd_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying Machine Learning Models\n**In this section, different machine learning models were implemented to see which is the best model.**\n<strong>Below are the models implemented:</strong>\n<ul>\n    <li>Logistic Regression</li>\n    <li>Support Vector Machine</li>\n    <li>Random Forest</li>\n    <li>Ensmble Model 1: (SVM, Logistic Regression)</li>\n    <li>Ensmble Model 2: (SVM, Random Forest)</li>\n    <li>Ensmble Model 3: (SVM, Logistic Regression, Logistic Regression)</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# This function outputs all the metrics needed to evaluate the models.\ndef print_report(x_val, y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_pred, y_actual)\n    accuracy = accuracy_score((y_pred > thresh), y_actual)\n    recall = recall_score((y_pred > thresh), y_actual)\n    precision = precision_score((y_pred > thresh),y_actual)\n    fscore = f1_score((y_pred > thresh), y_actual)\n    \n    print('AUC: %.3f'%auc)\n    print('Accuracy: %.3f'%accuracy)\n    print('Recall: %.3f'%recall)\n    print('Precision: %.3f'%precision)\n    print('F-Score: %.3f'%fscore)\n    print(' ')\n        \n    return auc\nthresh = 0.5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Models Implemented**","metadata":{}},{"cell_type":"code","source":"lr=LogisticRegression(random_state = 42)\nsvc = SVC(probability=True)\nrf = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=100, max_depth=3)\nensemble1 = VotingClassifier(estimators=[('svc',svc),('lr',lr)], voting ='soft', n_jobs=-1)\nensemble2 = VotingClassifier(estimators=[('svc',svc),('rf',rf)], voting ='soft', n_jobs=-1)\nensemble3 = VotingClassifier(estimators=[('svc',svc),('lr',lr),('rf',rf)], voting ='soft', n_jobs=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementing the models on the dataset created using the undersampling technique","metadata":{}},{"cell_type":"code","source":"lrus = lr.fit(Xu_train, yu_train)\nlrus_preds = lrus.predict(Xu_val)\n\nSVCus = svc.fit(Xu_train, yu_train)\nSVCus_preds = SVCus.predict(Xu_val)\n\nrfus = rf.fit(Xu_train, yu_train)\nrfus_preds = rfus.predict(Xu_val)\n\nens1Us = ensemble1.fit(Xu_train, yu_train)\nens1Us_preds = ens1Us.predict(Xu_val)\n\nens2Us = ensemble2.fit(Xu_train, yu_train)\nens2Us_preds = ens2Us.predict(Xu_val)\n\nens3Us = ensemble3.fit(Xu_train, yu_train)\nens3Us_preds = ens3Us.predict(Xu_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation of the models**","metadata":{}},{"cell_type":"code","source":"string='\\033[1m'+'Logistic Regression Undersample'+'\\033[0m'\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nlrus_auc = print_report(Xu_val, yu_val, lrus_preds, thresh)\n\nstring='\\033[1m'+'Support Vector Machine Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nSVCus_auc = print_report(Xu_val, yu_val, SVCus_preds, thresh)\n\nstring='\\033[1m'+'Random Forest Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nrfus_auc = print_report(Xu_val, yu_val, rfus_preds, thresh)\n\nstring='\\033[1m'+'Ensmble Model1 Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nens1us_auc = print_report(Xu_val, yu_val, ens1Us_preds, thresh)\n\nstring='\\033[1m'+'Ensmble Model2 Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nen2us_auc = print_report(Xu_val, yu_val, ens2Us_preds, thresh)\n\nstring='\\033[1m'+'Ensmble Model3 Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nens3us_auc = print_report(Xu_val, yu_val, ens3Us_preds, thresh)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After looking at the results of the model, Random Forest Scored the best AUC which is equal = 0.812, and Ensmble model 1 scored the best Accuracy score which is equal = 79.4%.**","metadata":{}},{"cell_type":"code","source":"# Plotting the confusion matrices of the models\nstring='\\033[1m'+'Confusion Matrices for Models Using Undersampling'+\"\\033[0m\"\nprint(string.center(60))\n\nplot_confusion_matrix(lrus, Xu_val, yu_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Linear Regression')\n\nplot_confusion_matrix(SVCus, Xu_val, yu_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the SVM')\n\nplot_confusion_matrix(rfus, Xu_val, yu_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Random Forest')\n# ---------------\nplot_confusion_matrix(ens1Us, Xu_val, yu_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Ensmble Model1')\n\nplot_confusion_matrix(ens2Us, Xu_val, yu_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Ensmble Model2')\n\nplot_confusion_matrix(ens3Us, Xu_val, yu_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Ensmble Model3')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementing the models on the dataset created using the downsampling technique","metadata":{}},{"cell_type":"code","source":"lrds = lr.fit(Xd_train, yd_train)\nlrds_preds = lrds.predict(Xd_val)\n\nSVCds = svc.fit(Xd_train, yd_train)\nSVCds_preds = SVCds.predict(Xd_val)\n\nrfds = rf.fit(Xd_train, yd_train)\nrfds_preds = rfds.predict(Xd_val)\n\nens1Ds = ensemble1.fit(Xd_train, yd_train)\nens1Ds_preds = ens1Ds.predict(Xd_val)\n\nens2Ds = ensemble1.fit(Xd_train, yd_train)\nens2Ds_preds = ens2Ds.predict(Xd_val)\n\nens3Ds = ensemble1.fit(Xd_train, yd_train)\nens3Ds_preds = ens3Ds.predict(Xd_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation of the models**","metadata":{}},{"cell_type":"code","source":"string='\\033[1m'+'Logistic Regression Downsample'+'\\033[0m'\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nlrds_auc = print_report(Xd_val, yd_val, lrds_preds, thresh)\n\nstring='\\033[1m'+'Support Vector Machine Downsample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nSVCds_auc = print_report(Xd_val, yd_val, SVCds_preds, thresh)\n\nstring='\\033[1m'+'Random Forest Downsample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nrfds_auc= print_report(Xd_val, yd_val, rfds_preds, thresh)\n\n\nstring='\\033[1m'+'Ensmble Model1 Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nens1ds_auc = print_report(Xd_val, yd_val, ens1Ds_preds, thresh)\n\nstring='\\033[1m'+'Ensmble Model2 Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nens2ds_auc = print_report(Xd_val, yd_val, ens2Ds_preds, thresh)\n\nstring='\\033[1m'+'Ensmble Model3 Undersample'+\"\\033[0m\"\nprint(string.center(60))\nprint('\\033[1m'+'Validation:'+'\\033[0m')\nens3ds_auc = print_report(Xd_val, yd_val, ens3Ds_preds, thresh)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After looking at the results of the model, All ensmble models Scored the same Auccracy score which is considered the best and it is equal = 62.5%, and Ensmble model 1, 3 scored the best AUC which is equal = 0.626.**","metadata":{}},{"cell_type":"code","source":"# Plotting the confusion matrices of the models\nstring='\\033[1m'+'Confusion Matrices for Models Using Downsampling'+\"\\033[0m\"\nprint(string.center(60))\n\nplot_confusion_matrix(lrds, Xd_val, yd_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Linear Regression')\n\nplot_confusion_matrix(SVCds, Xd_val, yd_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the SVM')\n\nplot_confusion_matrix(rfds, Xd_val, yd_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Random Forest')\n\nplot_confusion_matrix(ens1Ds, Xd_val, yd_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Ensmble Model1')\n\nplot_confusion_matrix(ens2Ds, Xd_val, yd_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Ensmble Model2')\n\nplot_confusion_matrix(ens3Ds, Xd_val, yd_val, cmap='PuBuGn')\nplt.title('Confusion matrix of the Ensmble Model3')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ROC Curves\n\n**In this section we plot the ROC Curves to give us better intuition about the models.**","metadata":{}},{"cell_type":"markdown","source":"### ROC of models implemented on the upsampled dataset ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,6))\nfpr_lrus, tpr_lrus, thresholds_lrus = roc_curve(yu_val, lrus_preds)\nplt.plot(fpr_lrus, tpr_lrus, 'g-',label ='LR AUC:%.3f'%lrus_auc)\n\nfpr_SVCus, tpr_SVCus, thresholds_SVCus = roc_curve(yu_val, SVCus_preds)\nplt.plot(fpr_SVCus, tpr_SVCus, 'b-',label ='SVM AUC:%.3f'%SVCus_auc)\n\nfpr_rfus, tpr_rfus, thresholds_rfus = roc_curve(yu_val, rfus_preds)\nplt.plot(fpr_rfus, tpr_rfus, 'r-',label ='RF AUC:%.3f'%rfus_auc)\n\nfpr_ens1Us, tpr_ens1Us, thresholds_ens1Us = roc_curve(yu_val, ens1Us_preds)\nplt.plot(fpr_ens1Us, tpr_ens1Us, 'c-',label ='Ensmble1 AUC:%.3f'%ens1us_auc)\n\nfpr_ens2Us, tpr_ens2Us, thresholds_ens2Us = roc_curve(yu_val, ens2Us_preds)\nplt.plot(fpr_ens2Us, tpr_ens2Us, 'm-',label ='Ensmble2 AUC:%.3f'%en2us_auc)\n\nfpr_ens3Us, tpr_ens3Us, thresholds_ens3Us = roc_curve(yu_val, ens3Us_preds)\nplt.plot(fpr_ens3Us, tpr_ens3Us, 'y-',label ='Ensmble3 AUC:%.3f'%ens3us_auc)\n\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ROC of models implemented on the downsampled dataset ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,6))\nfpr_lrds, tpr_lrds, thresholds_lrds = roc_curve(yd_val, lrds_preds)\nplt.plot(fpr_lrds, tpr_lrds, 'g-',label ='LR AUC:%.3f'%lrds_auc)\n\nfpr_SVCds, tpr_SVCds, thresholds_SVCds = roc_curve(yd_val, SVCds_preds)\nplt.plot(fpr_SVCds, tpr_SVCds, 'b-',label ='SVM AUC:%.3f'%SVCds_auc)\n\nfpr_rfds, tpr_rfds, thresholds_rfds = roc_curve(yd_val, rfds_preds)\nplt.plot(fpr_rfds, tpr_rfds, 'r-',label ='RF AUC:%.3f'%rfds_auc)\n\nfpr_ens1Ds, tpr_ens1Ds, thresholds_ens1Ds = roc_curve(yd_val, ens1Ds_preds)\nplt.plot(fpr_ens1Ds, tpr_ens1Ds, 'c-',label ='Ensmble1 AUC:%.3f'%ens1ds_auc)\n\nfpr_ens2Ds, tpr_ens2Ds, thresholds_ens2Ds = roc_curve(yd_val, ens2Ds_preds)\nplt.plot(fpr_ens2Ds, tpr_ens2Ds, 'm-',label ='Ensmble2 AUC:%.3f'%ens2ds_auc)\n\nfpr_ens3Ds, tpr_ens3Ds, thresholds_ens3Ds = roc_curve(yd_val, ens3Ds_preds)\nplt.plot(fpr_ens3Ds, tpr_ens3Ds, 'y-',label ='Ensmble3 AUC:%.3f'%ens3ds_auc)\n\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"Through this project, we created a binary classifier to predict the probability that a patient with diabetes would be readmitted to the hospital within 30 days. We compared between the datasets created with different resampling techniques (upsampling and downsampling) using 6 different models which are: logistic regresstion, support vector machine, random forest, Ensmble Model 1: (SVM, Logistic Regression), Ensmble Model 2: (SVM, Random Forest), Ensmble Model 3: (SVM, Logistic Regression, Logistic Regression). The results based on the confusion matrices and different evaulation metrics showed that the dataset created using upsampling technique produced more accurate model implementation than the dataset created using downsampling technique. The overall best model which scored the highest AUC = 0.819 was the Random forest implemented on the upsampled dataset.","metadata":{}}]}