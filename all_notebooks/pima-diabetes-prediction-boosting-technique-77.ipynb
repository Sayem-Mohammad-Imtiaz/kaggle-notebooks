{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pima Indians Diabetes Database\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n* This tutorial is highly recommended for beginners.\n* This is my fourth notebook. Do point out my mistakes in comment section.\n* I achieved accuracy 77% on test data.\n* If you find my work interesting, do upvote it."},{"metadata":{},"cell_type":"markdown","source":"This is default first cell in any kaggle kernel. They import NumPy and Pandas libraries and it also lists the available Kernel files. NumPy is the fundamental package for scientific computing with Python. Pandas is the most popular python library that is used for data analysis."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Necessary Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Plotting Libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\n%matplotlib inline\n\n# Metrics for Classification technique\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n# Scaler\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\n# Cross Validation\n\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n\n# Linear Models\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Ensemble Technique\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n\n# Other model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Model Stacking \n\nfrom mlxtend.classifier import StackingCVClassifier\n\n# Other libraries\n\nfrom datetime import datetime\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.impute import SimpleImputer\nfrom numpy import nan\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Dataset\n\nOur first step is to extract data. We will be extracting data using pandas function read_csv. Specify the location to the dataset and import them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndata.head(6) # Mention no of rows to be displayed from the top in the argument","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the dataset\n\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are 768 rows and 9 columns in the dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are no missing values in the dataset. Two columns are of float type and rest are int type.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"**Let's check the correlation between the features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nsns.set_context('notebook',font_scale = 1.3)\nsns.heatmap(data.corr(),annot=True,cmap='coolwarm')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check whether the dependent variable is balanced or not.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=data['Outcome'],data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It looks like ratio between negative and positive patients is approx 2:1 and actually this is not imbalanced dataset as we have enough values for both 0 and 1.**"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('Outcome',axis = 1)\ny = data['Outcome']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for zero values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"total number of rows : {0}\".format(len(data)))\nprint(\"number of rows missing glucose_conc: {0}\".format(len(data.loc[data['Glucose'] == 0])))\nprint(\"number of rows missing diastolic_bp: {0}\".format(len(data.loc[data['BloodPressure'] == 0])))\nprint(\"number of rows missing insulin: {0}\".format(len(data.loc[data['Insulin'] == 0])))\nprint(\"number of rows missing bmi: {0}\".format(len(data.loc[data['BMI'] == 0])))\nprint(\"number of rows missing diab_pred: {0}\".format(len(data.loc[data['DiabetesPedigreeFunction'] == 0])))\nprint(\"number of rows missing age: {0}\".format(len(data.loc[data['Age'] == 0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling Zero values\n\nfill_values = SimpleImputer(missing_values=0, strategy=\"mean\")\n\nX_train = fill_values.fit_transform(X_train)\nX_test = fill_values.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling and Stacking "},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForestClassifier\n\nrandom_forest_model = RandomForestClassifier(random_state = 42)\n\nrandom_forest_model.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_train_data = random_forest_model.predict(X_test)\n\nprint(\"Accuracy = {0:.3f}\".format(accuracy_score(y_test, predict_train_data)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hyperparameter Optimzation\n\nparams1={\n    \n    \"n_estimators\" : [100, 300, 500, 800, 1200], \n    \"max_depth\" : [5, 8, 15, 25, 30],\n    \"min_samples_split\" : [2, 5, 10, 15, 100],\n    \"min_samples_leaf\" : [1, 2, 5, 10] \n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm = RandomForestClassifier(random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfms = RandomizedSearchCV(rfm,param_distributions=params1,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrfms.fit(X_train,y_train.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfms.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = RandomForestClassifier(max_depth=8, min_samples_split=10, n_estimators=500,\n                       random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(X_train,y_train)\ny_pred1 = model1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred1))\nprint(confusion_matrix(y_test,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So after doing Hyperparameter optimization, we are able to achieve accuracy of 76% approx through RandomForestClassifier model. Let's try XGBoost Classifier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.fit(X_train,y_train.ravel())\ny_pred2 = model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred2))\nprint(confusion_matrix(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So after doing Hyperparameter optimization, we are able to achieve accuracy of 73% approx through XGBoost Classifier. Let's try CatBoostClassifier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = CatBoostClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.fit(X_train,y_train)\ny_pred3 = model3.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred3))\nprint(confusion_matrix(y_test,y_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So after doing Hyperparameter optimization, we are able to achieve accuracy of 75% approx through CatBoostClassifier. Let's try SVC.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params4 = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svcs = RandomizedSearchCV(SVC(),param_distributions=params4,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nsvcs.fit(X_train,y_train.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svcs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = SVC(C=0.1, gamma=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4.fit(X_train,y_train)\ny_pred4 = model4.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred4))\nprint(confusion_matrix(y_test,y_pred4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So after doing Hyperparameter optimization, we are able to achieve accuracy of 70% approx through SVC. Let's try AdaBoost Classifier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params5 = {'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adas = RandomizedSearchCV(AdaBoostClassifier(),param_distributions=params5,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nadas.fit(X_train,y_train.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adas.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model5 = AdaBoostClassifier(learning_rate=0.01, n_estimators=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model5.fit(X_train,y_train)\ny_pred5 = model5.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred5))\nprint(confusion_matrix(y_test,y_pred5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So after doing Hyperparameter optimization, we are able to achieve accuracy of 77% approx through AdaBoost Classifier. Let's try LightGBM Classifier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params6 = {\n    'learning_rate': [ 0.1],\n    'num_leaves': [31],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbs = RandomizedSearchCV(LGBMClassifier(),param_distributions=params6,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nlgbs.fit(X_train,y_train.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model6 = LGBMClassifier(objective='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model6.fit(X_train,y_train)\ny_pred6 = model6.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred6))\nprint(confusion_matrix(y_test,y_pred6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So after doing Hyperparameter optimization, we are able to achieve accuracy of 74% approx through LightGBM Classifier. Let's try GradientBoostingClassifier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model7 = GradientBoostingClassifier(random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model7.fit(X_train,y_train)\ny_pred7 = model7.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred7))\nprint(confusion_matrix(y_test,y_pred7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So after doing Hyperparameter optimization, we are able to achieve accuracy of 74% approx through GradientBoostingClassifier. Now let's try stacking of models.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Stacking of Models\n\nmodel8 = StackingCVClassifier(classifiers=[model1,model2,model3,model5,model6,model7],\n                            meta_classifier=model1,\n                            random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model8.fit(X_train,y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred8 = model8.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred8))\nprint(confusion_matrix(y_test,y_pred8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After stacking the models, we are able to achieve accuracy of 76%. Highest so far is achieved by AdaBoost Classifier which is 77%.**"},{"metadata":{},"cell_type":"markdown","source":"**Note : My next work will be on Malaria Dataset. My aim is to work on atleaat 5 disease dataset and then I will be creating Web app using Flask where user can check whether they are suffering from those diseases or not. After completing the web app, I will deploy it on Heroku and code can be accessed from GitHub.**"},{"metadata":{},"cell_type":"markdown","source":"# Thank You!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}