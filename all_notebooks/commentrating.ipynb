{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **<center> Comment Rating System </center>**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Term Project** \n**Student ID:** 1001780927  \n**Name:** Weixiao Sang  \n**Demo Website:** https://comment-rating-system.herokuapp.com/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Introduction**  \nThe goal is given the review, predict the rating. We use board game geek review data.In this project, I use naive bayes to train my classifier.First step is preprocessing, preprocessing is data cleaning, In this step, we need to remove meaningless parts in comments, such as, punctuation, meaningless word.Those parts is useless for classification, they take up storage space and reduce classification accuracy.After preprocessing, I use three different trainset to train three different naive bayes classifiers.I do this because I want to use Ensemble Methods to improve the classifier performance.In the final, I analyze the classification results.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Library Import**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport re\nimport random\nfrom csv import reader\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Data Analysis**  \n### Step1: Read data  \nWe can see there are many useless data in this data set(no comment rating).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/boardgamegeek-reviews/bgg-13m-reviews.csv')\ndata[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step2: Preprocessing,Remove the punctuation and stopword and no comment data, lowercase. Remove useless row. The rating scores are rounded into 11 different classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_drict = '/kaggle/input/boardgamegeek-reviews/bgg-13m-reviews.csv'\nstopwords = (stopwords.words('english'))\n\nwith open(data_drict,'r',encoding='utf-8') as f:\n    row_data = reader(f)\n    review = []\n    rate = []\n    for row in row_data:\n        if  row[0] != '' and row[3] !='':\n            rate.append(round(float(row[2])))\n            content = row[3].lower()\n            content = content.replace(\"\\r\", \"\").strip()  \n            content = content.replace(\"\\n\", \"\").strip()\n            content = re.sub(\"[%s]+\"%('.,|?|!|:|;\\\"\\-|#|$|%|&|\\|(|)|*|+|-|/|<|=|>|@|^|`|{|}|~\\[\\]'), \"\", content)\n            sentence = content.split(' ')\n            for i in stopwords:\n                while i in sentence:\n                    sentence.remove(i)\n            content = ' '.join(sentence)\n            review.append(content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original data size:'+str(len(data)))\n\nprint('Vaild data size:'+str(len(review)))\ncnt = 11*[0]\nfor i in range(11):\n    cnt[i] = rate.count(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there only about 1/6 useful data, and the useful sample distribution is very uneven.There are only 11 zero Rating score sample. I guess this will result in very unsatisfactory training results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nname = ['K=0','K=1','K=2','K=3','K=4','K=5','K=6','K=7','K=8','K=9','K=10']\nplt.bar(name,cnt)\nplt.xlabel('Rating')\nplt.ylabel('Number')\nfor a, b in zip(name, cnt):\n plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step4: Split train and testing data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test, y_train, y_test = train_test_split(review,rate,test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Trainset_size:')\nprint(len(x_train))\nprint('Testingset_size:')\nprint(len(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We split trainset to three different set to train different classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_0 = x_train[:100000]\ny_0 = y_train[:100000]\nx_1 = x_train[100000:200000]\ny_1 = y_train[100000:200000]\nx_2 = x_train[200000:300000]\ny_2 = y_train[200000:300000]\nx_test = x_test[:100000]\ny_test = y_test[:100000]\n\ntest_y = np.asarray(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step5: Train 3 different naive bayes classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer0 = feature_extraction.text.CountVectorizer()\ntrain0_x = vectorizer0.fit_transform(x_0)\ntrain0_y = np.asarray(y_0)\n\ntest_x = vectorizer0.transform(x_test)\n\nNB0 = MultinomialNB()\nNB0.fit(train0_x,train0_y)\n\npred0 = NB0.predict(test_x)\nacc_0 = accuracy_score(test_y,pred0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The first NB classifier precision is '+ str(acc_0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer1 = feature_extraction.text.CountVectorizer()\ntrain1_x = vectorizer1.fit_transform(x_1)\ntrain1_y = np.asarray(y_1)\n\ntest_x = vectorizer1.transform(x_test)\n\nNB1 = MultinomialNB()\nNB1.fit(train1_x,train1_y)\n\npred1 = NB1.predict(test_x)\nacc_1 = accuracy_score(test_y,pred1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The second NB classifier precision is '+ str(acc_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer2 = feature_extraction.text.CountVectorizer()\ntrain2_x = vectorizer2.fit_transform(x_2)\ntrain2_y = np.asarray(y_2)\n\ntest_x = vectorizer2.transform(x_test)\n\nNB2 = MultinomialNB()\nNB2.fit(train2_x,train2_y)\n\npred2 = NB2.predict(test_x)\nacc_2 = accuracy_score(test_y,pred2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The third NB classifier precision is '+ str(acc_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 6: Ensemble Methods\nThen I use Ensemble Methods to predict the result. I take the most common predicting result among the three classifier results as the final classification result.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = []\nfor i in range(len(test_y)):\n    pred = []\n    pred.append(pred0[i])\n    pred.append(pred1[i])\n    pred.append(pred2[i])\n    tmp = dict((a, pred.count(a)) for a in pred)\n    top = sorted(tmp.items() , key=lambda tmp:tmp[1] ,reverse= True)\n    result.append(top[0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = accuracy_score(test_y,result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The combining NB classifier precision is '+str(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the Ensemble Methods has 2.6% improvement than traditional naive bayes classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Experiment\n\nWe can see that the classifier doesn't work very well, the precision is only 30%. I try to analyze why.\n\nWe can see testing set distribution, most of the data is distributed in 6,7,8 rating score. In the predicting result, we can see most of the predicting is still distributed in 6,7,8 rating score. And the 8 rating score absorb most of the predictions. I analyze why this happened.\n\n- Uneven sample distribution results in uneven distribution of the number of features in each class, the more samples there are, the more features there are, so the class with a large sample has a better chance of extracting good features.\n- 7 rating score class feature words are similar to 6 , 8 rating class, but 6, 8 feature words are different , so part of 7 rating samples are divided to 8 and 9 rating class, this happens to every two adjacent classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y = list(test_y)\ncnt = 11*[0]\nfor i in range(11):\n    cnt[i] = test_y.count(i)\n\nname = ['K=0','K=1','K=2','K=3','K=4','K=5','K=6','K=7','K=8','K=9','K=10']\nplt.bar(name,cnt)\nplt.xlabel('Testing set Rating')\nplt.ylabel('Number')\nfor a, b in zip(name, cnt):\n plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt = 11*[0]\nfor i in range(11):\n    cnt[i] = result.count(i)\n\nname = ['K=0','K=1','K=2','K=3','K=4','K=5','K=6','K=7','K=8','K=9','K=10']\nplt.bar(name,cnt)\nplt.xlabel('Predicting Rating')\nplt.ylabel('Number')\nfor a, b in zip(name, cnt):\n plt.text(a, b, '%d' % b, ha='center', va='bottom', fontsize=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I said, 7 rating score class feature words are similar to 6 , 8 rating class, but 6, 8 feature words are different , so part of 7 rating samples are divided to 8 and 9 rating class, this happens to every two adjacent classes. So, To test this hypothesis, I do the following experiment, I keep the rating error of plus or minus two, it means if 7 is classified as 6 or 8, I think it is right.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result1 = []\nfor i in range(len(test_y)):\n    Min = test_y[i] - 1.1\n    Max = test_y[i] + 1.1\n    if result[i]>Min and result[i]<Max:\n        pd = 1\n    else:\n        pd = 0\n    result1.append(pd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_acc1 = result1.count(1) / len(test_y)\nprint('If we keep the rating error of plus or minus one:')\nprint('Final precision is '+str(final_acc1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the precision is improved. If we keep the rating error of plus or minus two. And the precision is 0.88714. This result confirms my guess.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result2 = []\nfor i in range(len(test_y)):\n    Min = test_y[i] - 2.1\n    Max = test_y[i] + 2.1\n    if result[i]>Min and result[i]<Max:\n        pd = 1\n    else:\n        pd = 0\n    result2.append(pd)\n\nfinal_acc2 = result2.count(1) / len(test_y)\nprint('If we keep the rating error of plus or minus two:')\nprint('Final precision is '+str(final_acc2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step7: Save model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.externals import joblib\njoblib.dump(NB0,  \"/kaggle/working/NB0.pkl\")\njoblib.dump(vectorizer0,  \"/kaggle/working/vectorizer0.pkl\")\njoblib.dump(NB0,  \"/kaggle/working/NB1.pkl\")\njoblib.dump(vectorizer0,  \"/kaggle/working/vectorizer1.pkl\")\njoblib.dump(NB0,  \"/kaggle/working/NB2.pkl\")\njoblib.dump(vectorizer0,  \"/kaggle/working/vectorizer2.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## conclusion\n\n- I preprocessed the data set\n- I trained naive bayes classifier\n- Ensemble Methods can improve the performance of classifier\n- Uneven training sets can have a negative effect on training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## REFERENCES\nhttps://scikit-learn.org/stable/modules/naive_bayes.html  \nhttps://blog.statsbot.co/ensemble-learning-d1dcd548e936","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}