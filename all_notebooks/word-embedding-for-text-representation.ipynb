{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Training word2vec for Item Title\n\nWord2Vec was introduced in two papers between September and October 2013, by a team of researchers at Google. Along with the papers, the researchers published their implementation in C. The Python implementation was done soon after the 1st paper, by Gensim.\n\n\n![](https://miro.medium.com/fit/c/1838/551/0*_j8UK1NpsCY_yUk2)\n\nref: https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial#Training-the-model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom time import time\n#For displaying complete rows info\npd.options.display.max_colwidth=500\nimport tensorflow as tf\nimport spacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport os\nimport seaborn as sns\nimport missingno as msno\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#sns.set_theme(style=\"whitegrid\")\n\ndef read_json(input_file):\n    '''\n    Read Json Lines File\n    '''\n    with open(input_file) as f:\n        lines = f.read().splitlines()    \n    \n    df = pd.DataFrame(lines)\n    df.columns = ['json_element']\n    df = pd.json_normalize(df['json_element'].apply(json.loads))\n    \n    return df\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Texto in Dataset\n\nRead all text dataset from Item and Search interaction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_item = read_json('/kaggle/input/meli-data-challenge-2020/item_data.jl') \ndf_item.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = read_json('/kaggle/input/meli-data-challenge-2020/train_dataset.jl').sample(n=300000)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_search = list()\nfor i in range(df.shape[0]):\n    if i % 10000 == 0:\n        print(i)\n    row = [j['event_info'] for j in df.iloc[i]['user_history'] if isinstance(j['event_info'], str)]\n    df_search.extend(row)\n    \ndf_search = np.unique(df_search)\ndf_search[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gensim Word2Vec Implementation\n\nWe use Gensim implementation of word2vec: https://radimrehurek.com/gensim/models/word2vec.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\n\nimport gensim\nimport string\nimport re\n\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom unidecode import unidecode\n\ndef func_tokenizer(text):\n    # print(text)\n\n    text = str(text)\n\n    # # Remove acentuação\n    text = unidecode(text)\n\n    # # lowercase\n    text = text.lower()\n\n    # #remove tags\n    text = re.sub(\"<!--?.*?-->\", \"\", text)\n\n    # # remove special characters and digits\n    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n\n    # # punk\n    text = re.sub(r'[?|!|\\'|#]', r'', text)\n    text = re.sub(r'[.|,|:|)|(|\\|/]', r' ', text)\n\n    # Clean onde\n    tokens = [t.strip() for t in text.split() if len(t) > 1]\n\n    # remove stopwords\n    #stopwords = self.load_stopwords()\n    #tokens    = [t for t in tokens if t not in stopwords]\n\n    if len(tokens) == 0:\n        tokens.append(\"<pad>\")\n    # print(tokens)\n    # print(\"\")\n    # if len(tokens) < 2:\n    #    print(tokens)\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_item.iloc[4].title, func_tokenizer(df_item.iloc[4].title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nsentences=[]\n\n# make corpus\nfor i in tqdm(range(len(df_item[\"title\"]))):\n    sentences.append(func_tokenizer(df_item.iloc[i]['title']))\n    \nfor i in tqdm(range(len(df_search))):    \n    sentences.append(func_tokenizer(df_search))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = gensim.models.Word2Vec(min_count=50,\n                                 window=2,\n                                 size=100,\n                                 sample=6e-5, \n                                 alpha=0.03, \n                                 min_alpha=0.0007, \n                                 negative=20,\n                                 workers=cores)\n\nmodel.build_vocab(sentences, progress_per=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.corpus_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(model.wv.vocab.keys())[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.callbacks import CallbackAny2Vec\nfrom gensim.models import Word2Vec\n\n# init callback class\nclass callback(CallbackAny2Vec):\n    \"\"\"\n    Callback to print loss after each epoch\n    \"\"\"\n    def __init__(self):\n        self.epoch = 0\n\n    def on_epoch_end(self, model):\n        loss = model.get_latest_training_loss()\n        if self.epoch == 0:\n            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n        else:\n            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n        self.epoch += 1\n        self.loss_previous_step = loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\nepochs = 100\n\nmodel.train(sentences, \n            total_examples=model.corpus_count, \n            epochs=epochs, \n            report_delay=1,\n            compute_loss = True,\n            callbacks=[callback()])\n\nprint('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.similar_by_word('dell')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = gensim.models.Word2Vec(sentences=frase_tokens, min_count=2,size=100,workers=4)\n# model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### t-SNE visualizations\n\nVisualization Similarity Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial#Training-the-model\n    \"\"\"\n    arrays = np.empty((0, 100), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    #reduc = PCA(n_components=50).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(arrays)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = 'dell'\ntsnescatterplot(model, word, [i[0] for i in model.wv.most_similar(negative=[word])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = 'xaomi'\ntsnescatterplot(model, word, [i[0] for i in model.wv.most_similar(negative=[word])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = 'carro'\ntsnescatterplot(model, word, [i[0] for i in model.wv.most_similar(negative=[word])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save Model\n\nSave dictionary and embs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the word2vec model\nmodel.save('/kaggle/working/word2vec.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.save_word2vec_format('/kaggle/working/mercadolivre-100d.bin', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = gensim.models.KeyedVectors.load_word2vec_format('/kaggle/working/mercadolivre-100d.bin', binary=True)\nmodel['celular']","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}