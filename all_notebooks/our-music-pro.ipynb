{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron, SGDClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization, Input\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.regularizers import l2\nfrom keras.layers import LeakyReLU\nimport kerastuner\nfrom kerastuner.tuners import RandomSearch\nfrom keras.callbacks import Callback\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.recurrent import LSTM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/gtzan-dataset-music-genre-classification/Data/features_30_sec.csv'\nfile = pd.read_csv(path)\nfile = file.drop(['filename'], axis = 1)\ny = file.label\nX = file.drop(columns = ['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n\n# pca = PCA(n_components=40)\n# principalComponents = pca.fit_transform(X)\n# cols = [f\"{i+1}\" for i in range(40)]\n\n# X = pd.DataFrame(data = principalComponents, columns = cols)\n# # concatenate with target label\n# file = pd.concat([principalDf, y], axis = 1)\n\n# pca.explained_variance_ratio_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(np.array(X, dtype = float))\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\nlabel_encoder = LabelEncoder()\ntransformed_train_y = label_encoder.fit_transform(train_y)\ntransformed_test_y = label_encoder.transform(test_y)\ntr_y = pd.get_dummies(train_y)\nte_y = pd.get_dummies(test_y)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_shape = (train_X.shape[1], train_X.shape[2])\ntrain_X = tf.reshape(tf.convert_to_tensor(train_X), (800,58,1))\ntest_X = tf.reshape(tf.convert_to_tensor(test_X), (200, 58, 1))\nprint(\"Build LSTM RNN model ...\")\nmodel = Sequential()\nmodel.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True))\nmodel.add(LSTM(units=32,  dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\nmodel.add(Dense(units=10, activation=\"softmax\"))\nmodel.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\nmodel.fit(train_X, tr_y, epochs = 150, batch_size = 32,\\\n          validation_data = (test_X, te_y))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_traincurve(history):\n    colors = {'loss':'r', 'accuracy':'b', 'val_loss':'m', 'val_accuracy':'g'}\n    plt.figure(figsize=(10,6))\n    plt.title(\"Training Curve\") \n    plt.xlabel(\"Epoch\")\n\n    for measure in history.keys():\n        color = colors[measure]\n        ln = len(history[measure])\n        plt.plot(range(1,ln+1), history[measure], color + '-', label=measure)  # use last 2 values to draw line\n\n    plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n\ndef generator(features, labels, batch_size):\n    while True:\n        batch_features = []\n        batch_labels = []\n        \n        for i in range(batch_size):\n            # choose random index in features\n            index = np.random.choice(len(features),1)\n            batch_features.extend(features[index])\n            batch_labels.extend(labels[index])\n        batch_features = np.array(batch_features)\n        batch_labels = np.array(batch_labels)\n        yield batch_features, batch_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# model = Sequential()\n\n# model.add(Conv1D(128, 8,strides = 2,kernel_regularizer=l2(0.02),\\\n#                  input_shape=train_X.shape[1:], activation='sigmoid'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.1))\n\n# model.add(Conv1D(64, 6,strides = 2,kernel_regularizer=l2(0.02), activation='sigmoid'))\n# # model.add(MaxPooling1D(pool_size=4)) \n# model.add(BatchNormalization())\n# model.add(Dropout(0.1))\n\n# model.add(Conv1D(32, 11,strides = 2, kernel_regularizer=l2(0.02), activation='sigmoid'))\n# # model.add(MaxPooling1D(pool_size=4)) \n# model.add(BatchNormalization())\n# model.add(Dropout(0.1))\n\n# # model.add(Conv1D(16, 5, activation=LeakyReLU(0.25)))\n# # model.add(MaxPooling1D(pool_size=4)) \n# # model.add(BatchNormalization())\n# # model.add(Dropout(0.2))\n\n# model.add(Flatten())\n# model.add(Dense(128, activation='sigmoid'))\n# model.add(BatchNormalization())\n\n# model.add(Dense(64, activation='sigmoid')) \n# model.add(BatchNormalization())\n\n# # model.add(Dense(32, activation='sigmoid')) \n# # model.add(Dense(16, activation='sigmoid')) \n# model.add(Dense(10,activation='softmax'))\n\n# model.compile(loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])\n# History = model.fit(train_X, tr_y, batch_size = 20, epochs=150, verbose = 1, validation_data=(test_X, te_y))\n# # batch_size = 20\n# # History = model.fit_generator(generator(train_X, tr_y, batch_size), steps_per_epoch=len(train_X) // batch_size, \n# #                               epochs=50, validation_data=generator(test_X, te_y, batch_size), validation_steps=20)\n\n\n\n\n# # ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # for key in History.history.keys():\n# #     history[key].extend(History.history[key])\n# plot_traincurve(History.history)\n# # print(History.history.keys())\n# # plt.hist(History.history['val_accuracy'])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStoppingByAccuracy(Callback):\n    def __init__(self, monitor='accuracy', value=0.98, verbose=0):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\n        if current >= self.value:\n            if self.verbose > 0:\n                print(\"Epoch %05d: early stopping THR\" % epoch)\n            self.model.stop_training = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# U, S, VT=np.linalg.svd(X, full_matrices=0)\n# fig1 = plt.figure(figsize = [16, 8])\n# ax1 = fig1.add_subplot(121)\n# ax1.semilogy(S, '-o', color = 'k')\n# ax2 = fig1.add_subplot(122)\n# ax2.plot(np.cumsum(S)/np.sum(S), '-o', color = 'k')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import seaborn as sns\n\n# plt.figure(figsize = (16, 9))\n# sns.scatterplot(x = \"principal component 1\", y = \"principal component 2\", data = finalDf, hue = \"label\", alpha = 0.7,\n#                s = 100);\n\n# plt.title('PCA on Genres', fontsize = 25)\n# plt.xticks(fontsize = 14)\n# plt.yticks(fontsize = 10);\n# plt.xlabel(\"Principal Component 1\", fontsize = 15)\n# plt.ylabel(\"Principal Component 2\", fontsize = 15)\n# plt.savefig(\"PCA Scattert.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(hp):\n    model = Sequential()\n    for i in range(hp.Int('layers', 1, 6)):\n        model.add(Dense(hp.Int(f'filters_{i}', 32, 256, 32),\\\n                        kernel_regularizer = l2(hp.Int(f'reg_{i}', 0, 10) * 0.01), activation='relu'))\n        model.add(BatchNormalization())\n        if hp.Choice(f'Dropout_{i}', values = [0, 1]) == 0:\n            model.add(Dropout(hp.Int(f'Dropout_val_{i}',1, 6) * 0.1))\n\n    \n    model.add(Dense(10, activation= 'softmax'))\n\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3,\n    directory='./',\n    project_name='eigth_project')\n# tuner.search_space_summary()\n# model.fit(train_X, transformed_train_y, epochs = 200, batch_size = 10, verbose=1, \\\n#               validation_data=(test_X, transformed_test_y),\\\n#           callbacks=[EarlyStoppingByAccuracy(monitor = 'val_accuracy', value = 0.835)])\n\n# acc = model.evaluate(test_X, transformed_test_y)[0]\n# while acc < 0.835:\n    \n#     acc = model.evaluate(test_X, transformed_test_y)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.search(train_X, transformed_train_y,\n             epochs=200, batch_size = 10,\n             validation_data=(test_X, transformed_test_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save(\"my_h3_model.h5\")\n# models = tuner.get_best_models(num_models=1)\nmodels[0].summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.results_summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import keras\n# model1 = keras.models.load_model('./my_h3_model.h5')\n# # model1.fit(train_X, transformed_train_y, epochs = 130, batch_size = 10, verbose=0, \\\n# #               validation_data=(test_X, transformed_test_y),\\\n# #           callbacks=[EarlyStoppingByAccuracy(monitor = 'val_accuracy', value = 0.779)])\n# model1.evaluate(test_X, transformed_test_y, batch_size = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# batch_size = [10, 20, 40, 16, 32, 64, 128, 265]\n# epochs = [200, 100, 300]\n\n# param_grid = dict(batch_size=batch_size, epochs=epochs)\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2, scoring='accuracy')\n# grid_result = grid.fit(train_X, transformed_train_y)\n# model.evaluate(, batch_size= 10\n\n# bacc = 0\n# idx = []\n# for i in batch_size:\n#     for j in epochs:\n#         model.fit(train_X, transformed_train_y, epochs = j, batch_size = i)\n        #         if acc > bacc:\n#             bacc = acc\n#             idx = [i, j]\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.neural_network import MLPClassifier, MLPRegressor\n\n# # classi = MLPRegressor(max_iter = 100000, activation = 'relu', solver = 'sgd')\n# rf = RandomForestClassifier(random_state = 0)\n# dt = DecisionTreeClassifier(max_depth = 18, random_state = 0)\n# classi = LogisticRegression(max_iter=10000)\n# # max_val = 0\n# # idx = []\n# # classi = XGBClassifier(estimators= 50, learning_rate = 0.7)\n# # for i in range(100, 10000, 100):\n# #     s = LinearSVC(random_state = 0, max_iter=i)\n# # s = KNeighborsClassifier(n_neighbors = i)\n# # gb = GaussianNB()\n# # classi = VotingClassifier(estimators=[('lr', lr),('x', x), ('dt', dt), ('rf', rf)])\n# classi.fit(train_X, transformed_train_y)\n# preds_lr = classi.predict(test_X)\n# np.round(preds_lr)\n# test_acc = round(classi.score(test_X, transformed_test_y) * 100, 2)\n# print(\"Train Accuracy: \", round(classi.score(train_X, transformed_train_y) * 100, 2))\n# print(\"Test Accuracy: \", test_acc)\n# #         print(\"=====================================================================\")\n\n# # print(max_val, idx)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coeff_df = pd.DataFrame(train_X.columns.delete(0))\n# coeff_df.columns = ['Feature']\n# coeff_df[\"Correlation\"] = pd.Series(lr.coef_[0])\n\n# coeff_df.sort_values(by='Correlation', ascending=False)\n# drop_cols = train_X.columns[44:]\n\n# train_X = train_X.drop(columns = drop_cols)\n# test_X = test_X.drop(columns = drop_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}