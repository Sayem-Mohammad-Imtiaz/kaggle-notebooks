{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df  = pd.read_csv('/kaggle/input/fetal-health-classification/fetal_health.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    #print('no of unique vlaues in {} column {}'.format(i,df[i].nunique()))\n    if df[i].nunique() <=3:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['fetal_health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot('fetal_health',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint('Normal',round(df['fetal_health'].value_counts()[1]/len(df) * 100), '% of the data ')\nprint('Suspect',round(df['fetal_health'].value_counts()[2]/len(df) * 100), '% of the data ')\nprint('Pathological',round(df['fetal_health'].value_counts()[3]/len(df) * 100), '% of the data ')\n\nX = df.drop('fetal_health',axis=1)\ny = df['fetal_health']\n\n\nsss = StratifiedKFold(n_splits=6,random_state=1,shuffle=True)\n\nfor train_index,test_index in sss.split(X,y):\n    #print('Train:', train_index)\n    #print('Test:',test_index)\n    \n    original_Xtrain,original_Xtest = X.iloc[train_index],X.iloc[test_index]\n    original_ytrain,original_ytest = y.iloc[train_index],y.iloc[test_index]\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both train and test label distribution are similarly distributed\ntrain_unique_label,train_counts_label = np.unique(original_ytrain,return_counts=True)\ntest_unique_label,test_counts_label = np.unique(original_ytest,return_counts=True)\n\nprint(train_unique_label,train_counts_label)\nprint(test_unique_label,test_counts_label)\n\nprint('Label Distributions:')\nprint(train_counts_label/len(original_ytrain))\nprint(test_counts_label/len(original_ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac=1)\n#amount of pathological classes 176 rows\n\nPath_df = df.loc[df['fetal_health']==3.0]\nsusp_df = df.loc[df['fetal_health']==2.0][:176]\nnorm_df = df.loc[df['fetal_health']==1.0][:176]\n\nnormal_distributed_df = pd.concat([Path_df,susp_df,norm_df])\n#print(normal_distributed_df.head(5))\n\n#Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nprint(new_df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint('Distribution of classes in the sample distribution')\nprint(new_df['fetal_health'].value_counts()/len(new_df))\n\nsns.countplot('fetal_health',data=new_df)\nplt.title('Equally Distirbuted Classes',fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Matrix\n\nf, (ax1,ax2) = plt.subplots(2,1,figsize=(24,20))\n\n#Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr,cmap='coolwarm_r',annot_kws={'size':20},ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (dont use Reference)\", fontsize=14)\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr,cmap='coolwarm_r',annot_kws={'size':20},ax=ax2)\nax2.set_title(\"SubSample Correlation Matrix \\n (use Reference)\",fontsize=14)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Highest correlated pairs\nc = new_df.corr().abs()\ns = c.unstack()\nso = s.sort_values(kind=\"quicksort\")\nso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BoxPlots\n#Negative correlated values are compared with the class(label)(The lower our feature value the more likely it will be apthological):\n\nf, axes = plt.subplots(ncols=4,figsize=(20,4))\n\nsns.boxplot(x='fetal_health',y='accelerations',data=new_df,ax=axes[0])\naxes[0].set_title('accelerations vs fetal_health Negative Corr')\n\nsns.boxplot(x='fetal_health',y='histogram_mean',data=new_df,ax=axes[1])\naxes[1].set_title('histogram_mean vs fetal_health Negative Corr')\n\nsns.boxplot(x='fetal_health',y='histogram_median',data=new_df,ax=axes[2])\naxes[2].set_title('histogram_median vs fetal_health Negative Corr')\n\nsns.boxplot(x='fetal_health',y='histogram_mode',data=new_df,ax=axes[3])\naxes[3].set_title('histogram_mode vs fetal_health Negative Corr')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive correlations (The higher the feature the probability increases that it will be a )\nsns.boxplot(x=\"fetal_health\", y=\"abnormal_short_term_variability\", data=new_df)\nplt.title('abnormal_short_term_variability vs fetal_health Positive Correlation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histogram_mean_fhealth = new_df['histogram_mean'].loc[new_df['fetal_health']==1].values\nq25,q75 = np.percentile(histogram_mean_fhealth,25),np.percentile(histogram_mean_fhealth,75)\n\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25,q75))\nhistogram_mean_iqr = q75 - q25\nprint('iqr: {}'.format(histogram_mean_iqr))\n\nhistogram_mean_cut_off = histogram_mean_iqr * 1.5\nhistogram_mean_lower,histogram_mean_upper = q25 - histogram_mean_cut_off, q75 + histogram_mean_cut_off\n\nprint('cutoff: {}'.format(histogram_mean_cut_off))\nprint('histogram_mean Lower: {}'.format(histogram_mean_lower))\nprint('histogram_mean_upper: {}'.format(histogram_mean_upper))\n\noutliers = [x for x in histogram_mean_fhealth if x < histogram_mean_lower or x > histogram_mean_upper]\nprint('Features histogram_mean outliers for fhealth cases: {}'.format(len(outliers)))\nprint('histogram_mean outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['histogram_mean'] > histogram_mean_upper) | (new_df['histogram_mean'] < histogram_mean_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histogram_median_fhealth = new_df['histogram_median'].loc[new_df['fetal_health']==1].values\nq25,q75 = np.percentile(histogram_median_fhealth,25),np.percentile(histogram_median_fhealth,75)\n\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25,q75))\nhistogram_median_iqr = q75 - q25\nprint('iqr: {}'.format(histogram_median_iqr))\n\nhistogram_median_cut_off = histogram_median_iqr * 1.5\nhistogram_median_lower,histogram_median_upper = q25 - histogram_median_cut_off, q75 + histogram_median_cut_off\n\nprint('cutoff: {}'.format(histogram_median_cut_off))\nprint('histogram_median Lower: {}'.format(histogram_median_lower))\nprint('histogram_median_upper: {}'.format(histogram_median_upper))\n\noutliers = [x for x in histogram_median_fhealth if x < histogram_median_lower or x > histogram_median_upper]\nprint('Features histogram_median outliers for fhealth cases: {}'.format(len(outliers)))\nprint('histogram_median outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['histogram_median'] > histogram_median_upper) | (new_df['histogram_median'] < histogram_median_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## -----> histogram_mode Removing Outliers\n\nhistogram_mode_fhealth = new_df['histogram_mode'].loc[new_df['fetal_health']==1].values\nq25,q75 = np.percentile(histogram_mode_fhealth,25),np.percentile(histogram_mode_fhealth,75)\n\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25,q75))\nhistogram_mode_iqr = q75 - q25\nprint('iqr: {}'.format(histogram_mode_iqr))\n\nhistogram_mode_cut_off = histogram_mode_iqr * 1.5\nhistogram_mode_lower,histogram_mode_upper = q25 - histogram_mode_cut_off, q75 + histogram_mode_cut_off\n\nprint('cutoff: {}'.format(histogram_mode_cut_off))\nprint('histogram_mode Lower: {}'.format(histogram_mode_lower))\nprint('histogram_mode_upper: {}'.format(histogram_mode_upper))\n\noutliers = [x for x in histogram_mode_fhealth if x < histogram_mode_lower or x > histogram_mode_upper]\nprint('Features histogram_mode outliers for fhealth cases: {}'.format(len(outliers)))\nprint('histogram_mode outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['histogram_mode'] > histogram_mode_upper) | (new_df['histogram_mode'] < histogram_mode_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## -----> abnormal_short_term_variability Removing Outliers\n\nabnormal_short_term_variability_fhealth = new_df['abnormal_short_term_variability'].loc[new_df['fetal_health']==1].values\nq25,q75 = np.percentile(abnormal_short_term_variability_fhealth,25),np.percentile(abnormal_short_term_variability_fhealth,75)\n\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25,q75))\nabnormal_short_term_variability_iqr = q75 - q25\nprint('iqr: {}'.format(abnormal_short_term_variability_iqr))\n\nabnormal_short_term_variability_cut_off = abnormal_short_term_variability_iqr * 1.5\nabnormal_short_term_variability_lower,abnormal_short_term_variability_upper = q25 - abnormal_short_term_variability_cut_off, q75 + abnormal_short_term_variability_cut_off\n\nprint('cutoff: {}'.format(abnormal_short_term_variability_cut_off))\nprint('abnormal_short_term_variability Lower: {}'.format(abnormal_short_term_variability_lower))\nprint('abnormal_short_term_variability_upper: {}'.format(abnormal_short_term_variability_upper))\n\noutliers = [x for x in abnormal_short_term_variability_fhealth if x < abnormal_short_term_variability_lower or x > abnormal_short_term_variability_upper]\nprint('Features abnormal_short_term_variability outliers for fhealth cases: {}'.format(len(outliers)))\nprint('abnormal_short_term_variability outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['abnormal_short_term_variability'] > abnormal_short_term_variability_upper) | (new_df['abnormal_short_term_variability'] < abnormal_short_term_variability_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no outliers present\nf,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature histogram_mean\nsns.boxplot(x=\"fetal_health\", y=\"histogram_mean\", data=new_df,ax=ax1, palette=colors)\nax1.set_title(\"histogram_mean Feature \\n Reduction of outliers\", fontsize=14)\n\n\n# Feature histogram_median\nsns.boxplot(x=\"fetal_health\", y=\"histogram_median\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"histogram_median Feature \\n Reduction of outliers\", fontsize=14)\n\n# Feature histogram_mode\nsns.boxplot(x=\"fetal_health\", y=\"histogram_mode\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"histogram_mode Feature \\n Reduction of outliers\", fontsize=14)\n\n\n# Feature abnormal_short_term_variability\nsns.boxplot(x=\"fetal_health\", y=\"abnormal_short_term_variability\", data=new_df, ax=ax4, palette=colors)\nax4.set_title(\"abnormal_short_term_variability Feature \\n Reduction of outliers\", fontsize=14)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#undersampling before cross validating\nX = new_df.drop('fetal_health',axis=1)\ny = new_df['fetal_health']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n#from sklearn import LDA\n#from sklearn import QDA\nimport collections\n\nclassifiers = {\"Logistic Regression\":LogisticRegression(),\n               \"KNearest\":KNeighborsClassifier(),\n               \"DecisionTreeClassifier\":DecisionTreeClassifier(max_depth=5),\n               \"RandomForestClassifier\":RandomForestClassifier(max_depth=5),\n               \"AdaBoostClassifier\":AdaBoostClassifier(),\n               \"GaussianNB\":GaussianNB()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor key,classifier in classifiers.items():\n    classifier.fit(X_train,y_train)\n    training_score = cross_val_score(classifier,X_train,y_train,cv=5)\n    print(\"Models:\",classifier.__class__.__name__,\"has a training score of\",round(training_score.mean(),2)*100,\"accuracy score\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_score = cross_val_score(LogisticRegression(),X_train,y_train,cv=5)\nprint(\"Logistic Regression cross validation score\",round(log_reg_score.mean()*100,2).astype(str)+'%')\n\nknears_score = cross_val_score(KNeighborsClassifier(),X_train,y_train,cv=5)\nprint(\"Knearest neighbors cross vlaidation score\",round(knears_score.mean()*100,2).astype(str)+'%')\n\nRand_score = cross_val_score(RandomForestClassifier(max_depth=5),X_train,y_train,cv=5)\nprint(\"Random Forest cross validation score\",round(Rand_score.mean()*100,2).astype(str)+'%')\n\ntree_score = cross_val_score(DecisionTreeClassifier(max_depth=5),X_train,y_train,cv=5)\nprint(\"Decisiontree Classifiers\",round(tree_score.mean()*100,2).astype(str)+'%')\n\nAda_score = cross_val_score(AdaBoostClassifier(),X_train,y_train,cv=5)\nprint(\"AdaBoost Classifiers\",round(Ada_score.mean()*100,2).astype(str)+'%')\n\nGauss_score = cross_val_score(GaussianNB(),X_train,y_train,cv=5)\nprint(\"GaussianNB Classifiers\",round(Gauss_score.mean()*100,2).astype(str)+'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom collections import Counter\n\n#We will undersample during cross validating\nundersample_X = df.drop('fetal_health',axis=1)\nundersample_y = df['fetal_health']\n\nfor train_index,test_index in sss.split(undersample_X,undersample_y):\n    undersample_Xtrain,undersample_Xtest = undersample_X.iloc[train_index],undersample_X.iloc[test_index]\n    undersample_ytrain,undersample_ytest = undersample_y.iloc[train_index],undersample_y.iloc[test_index]\n\n    #print(sss.get_n_splits(undersample_X,undersample_y))\n    \n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_ytrain = undersample_ytrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytest = undersample_ytest.values\n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# implementing the near miss technique\n# Demonstration of distribution by nearmiss\nX_nearmiss,y_nearmiss = NearMiss().fit_sample(undersample_X.values,undersample_y.values)\nprint(\"nearmiss label distribution: {}\".format(Counter(y_nearmiss)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report,roc_curve\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='auto'), DecisionTreeClassifier(max_depth=5)) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    undersample_prediction_prob = undersample_model.predict_proba(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction,average='weighted'))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction,average='weighted'))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction,average='weighted'))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction_prob,multi_class='ovr'))\n    \n    fprate, tprate, thresholds = roc_curve(original_ytrain[test], undersample_prediction,pos_label=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(undersample_accuracy)\nprint(undersample_precision)\nprint(undersample_recall)\nprint(undersample_f1)\nprint(undersample_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Learning curve helps to visualize the training score and the cross validation score\nIf cross vlaidation score is trending more towards a high training score then a larger dataset is required for cross validation of algorithm'''\n\n# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, estimator5, estimator6, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(3,2, figsize=(20,14), sharey=True)\n    plt.subplots_adjust(wspace=0.4)\n    if ylim is not None:\n        plt.ylim(*ylim)\n\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=10)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=10)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Random Forest Classifier Learning Curve\", fontsize=10)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier Learning Curve\", fontsize=10)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    # Fifth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator5, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax5.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax5.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax5.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax5.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax5.set_title(\"Ada Boost Classifier Learning Curve\", fontsize=10)\n    ax5.set_xlabel('Training size (m)')\n    ax5.set_ylabel('Score')\n    ax5.grid(True)\n    ax5.legend(loc=\"best\")\n    \n    # Sixth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator6, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax6.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax6.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax6.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax6.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax6.set_title(\"Gaussian NB Classifier Learning Curve\", fontsize=10)\n    ax6.set_xlabel('Training size (m)')\n    ax6.set_ylabel('Score')\n    ax6.grid(True)\n    ax6.legend(loc=\"best\")\n    \n    \n    \n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(LogisticRegression(),KNeighborsClassifier(),RandomForestClassifier(max_depth=5),DecisionTreeClassifier(max_depth=5),AdaBoostClassifier(),GaussianNB(), X_train, y_train, (0.60, 1.01), cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality.\n\nROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\n\nThe “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\n'''\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred = cross_val_predict(LogisticRegression(),X_train,y_train,cv=5,method='decision_function')\nknears_pred = cross_val_predict(KNeighborsClassifier(),X_train,y_train,cv=5,method='predict_proba')\nRfc_pred = cross_val_predict(RandomForestClassifier(max_depth=5),X_train,y_train,cv=5,method='predict_proba')\nDec_tree_pred = cross_val_predict(DecisionTreeClassifier(max_depth=5),X_train,y_train,cv=5,method='predict_proba')\nada_pred = cross_val_predict(AdaBoostClassifier(),X_train,y_train,cv=5,method='predict_proba')\ngauss_pred = cross_val_predict(GaussianNB(),X_train,y_train,cv=5,method='predict_proba')\n\nprint((log_reg_pred))\nprint((knears_pred))\nprint((Rfc_pred))\nprint((Dec_tree_pred))\nprint((ada_pred))\nprint((gauss_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score,precision_score,f1_score,accuracy_score\nRandom = RandomForestClassifier(max_depth=5)\nRandom.fit(X_train,y_train)\ny_pred = Random.predict(X_train)\n\n#overfitting case:\n\nprint('----'*45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train,y_pred,average='weighted')))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train,y_pred,average='weighted')))\nprint('f1 score: {:.2f}'.format(f1_score(y_train,y_pred,average='weighted')))\nprint('accuracy score: {:.2f}'.format(accuracy_score(y_train,y_pred)))\n\n#Real work\nprint('----'*45)\n\nprint(\"Accuracy score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"F1 score: {:.2f}\".format(np.mean(undersample_f1)))\nprint(\"Recall score: {:.2f}\".format(np.mean(undersample_recall)))\nprint('---' * 45)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_train, y_pred), annot=True,cmap='Blues', fmt='g');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}