{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting movie ratings and recommendation system","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://)## Overview of this project\n\nI used machine learning to predict a movie's score and cosine similarity for recommendation system.For this project, I will be using the [5000 movies dataset][db], and supervised learning, to predict the user score for a film.\n\n\n\nFor my project, instead of using a cosine-similarity algorithm, I will use machine learning to predict a movie's score. To do this, I will: \n- create a decimal representation of the features: genres, actors, directors, keywords, and production companies\n  - Like Swain has done, I will create a binary array for each feature. Then, I will analyze the array like points in a number line\n    - Looking at the points, I can identify a point that best describes the distribution of 1's, and use that as a decimal representation of the feature\n- normalize the decimal values to be within [0,1]\n- Split data into training and testing sets\n  - 70% for training, 30% for testing\n- Train a regression-based model using Scikit learn's Bayesian Ridge\n- Make predictions and evaluate using an R^2 score\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Setup\n### Import Modules","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import io\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd \n\nimport json #converting JSON to lists for dataframe\nimport warnings\nwarnings.filterwarnings('ignore')\nimport base64\nimport codecs\nfrom IPython.display import HTML\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie = pd.read_csv(\"../input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\ncredit = pd.read_csv(\"../input/tmdb-movie-metadata/tmdb_5000_credits.csv\")\ncredit.columns = ['id','tittle','cast','crew']\nmovies = movie.merge(credit,on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = movies[(movies['vote_average']!=0)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick look at movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Format dataframe\nFeatures: **genres, keywords, cast, crew, and production companies** are all in JSON format. Convert to simple list via following function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef to_list(df, feature_names_list): #df: dataframe, feature_names: list of all features to convert from JSON to list\n    for feature_name in feature_names_list:\n        print(\"Current:\", feature_name)\n        #STEP 1: convert JSON format to a list\n        df[feature_name]=df[feature_name].apply(json.loads)\n        #Two cases here: Feature is crew, or feature is not crew\n        if feature_name == 'crew': #if crew, due to large size, want to limit to most influential members: director, editor, cinematographer, screenplay, and composer\n            for index,i in zip(df.index,df[feature_name]):\n                feature_list_1=[]\n                limit = 10\n                if len(i) < 10:\n                    limit = len(i)\n                for j in range(limit): #top 10 crew members\n                    feature_list_1.append((i[j]['name'])) # the key 'name' contains the name of the a sub-feature (ex: sci-fi in genres)\n                df.loc[index,feature_name]= str(feature_list_1)\n        \n        elif feature_name == 'cast': #Another special case. Only want top 5 cast members (most infulential)\n            for index,i in zip(df.index,df[feature_name]):\n                feature_list_1=[]\n                if len(i) > 5:\n                    limit = 5\n                else:\n                    limit = len(i)\n                for j in range(limit): #top 5 (JSON format already has this sorted)\n                    feature_list_1.append((i[j]['name']))\n                df.loc[index,feature_name]= str(feature_list_1)\n        else:    \n            for index,i in zip(df.index,df[feature_name]):\n                feature_list_1=[]\n                for j in range(len(i)):\n                    feature_list_1.append((i[j]['name']))\n                df.loc[index,feature_name]= str(feature_list_1)\n    \n        #STEP 2: clean up and transform into unsorted list\n        df[feature_name] = df[feature_name].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n        df[feature_name] = df[feature_name].str.split(',')\n        \n        #STEP 3: Sort list elements\n        for i,j in zip(df[feature_name],df.index):\n            features_list_2=i\n            features_list_2.sort()\n            df.loc[j,feature_name]=str(features_list_2)\n        df[feature_name]=df[feature_name].str.strip('[]').str.replace(' ','').str.replace(\"'\",'')\n        lst = df[feature_name].str.split(',')\n        if len(lst) == 0:\n            df[feature_name] = None\n        else:\n            df[feature_name]= df[feature_name].str.split(',')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = to_list(movies, ['genres', 'keywords', 'production_companies', 'cast', 'crew']) #function call","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every dataset contain some feature with empty olumns lets remove them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = []\nfor i in movies.index:\n    if (movies['production_companies'][i] == [''] and movies['cast'][i] == [''] and \n        movies['crew'][i] == ['']):\n        to_drop.append(i)\nprint('Dropping', str(len(to_drop)), 'movies.')\nmovies = movies.drop(to_drop, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove un-needed feature-types in dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened = movies[['id','original_title','genres','cast', 'crew', 'production_companies', 'keywords', 'vote_average']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make an histogram for distribution of movie ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\nn, bins, patches = plt.hist(movies_shortened['vote_average'], 30, density=1, facecolor='g', alpha=0.75)\n\nplt.xlabel('Vote_average')\nplt.ylabel('Occurence')\nplt.title('Distribution of voter average')\nplt.grid(True)\nplt.show()\nprint(\"Minimum of Ratings:\", round(min(movies_shortened['vote_average']),2))\nprint(\"Maximum of Ratings:\", round(max(movies_shortened['vote_average']),2))\nprint(\"Average of Ratings:\", round(np.mean(movies_shortened['vote_average']),2))\nprint(\"Variance of Ratings:\",round(np.var(movies_shortened['vote_average']),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering: Turn lists of features into numerical representations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Step 1: Identify all unique sub-feature for each feature (ex: all unique cast members in the cast category)\n#### Organize sub-features to lowest-rating association to highest-rating association\nBy organizing features in this order, the numerical representation will then also describe the quality of features associated with the movie\n- higher quality feature values should correlate with higher-rated movies, and vice versa","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_list(df, feature_name): #create a list of all unique feature values\n    #Step 1: track all ratings associated with each feature in a dictionary\n    feature_dict = {}\n    for index, row in df.iterrows():\n        feat = row[feature_name]\n        for sub_feat in feat:\n            if sub_feat not in feature_dict:\n                feature_dict[sub_feat] = (df['vote_average'][index], 1) #\n            else:\n                feature_dict[sub_feat] = (feature_dict[sub_feat][0] + (df['vote_average'][index]), feature_dict[sub_feat][1] + 1)\n    #Step 2: calculate average ratings for each feature\n    for key in feature_dict:\n        feature_dict[key] = feature_dict[key][0]/feature_dict[key][1] #average of all vote_averages\n       \n    #Step 3: create and sort a list of tuples (dictionary value, key)\n    lst = list()\n    for name in feature_dict:\n        lst.append((feature_dict[name],name))\n    lst = sorted(lst)\n    #step 4: create a list of only the feature names, from lowest rating to highest rating\n    feature_list = list()\n    ratings_list = list()\n    for element in lst:\n        feature_list.append(element[1])\n        ratings_list.append(element[0])\n    \n    #get the variance of the ratings. This is helpful for determining the usefulness of the information (to be displayed in below plot)\n    var = round(np.var(ratings_list),3)\n    \n    #before returning the list, do a quick visualization to show that generate_list works\n    fig, ax = plt.subplots(figsize=(6,5))\n    if feature_name != 'genres':\n        n = 50 # sample at intervals of n\n    else:\n        n = 1\n    X = [] #sample for associated movie(s) rating average\n    Y = [] #sample for feature names\n    for i in range(0, len(feature_list) - 1, n):\n        X.append(ratings_list[i])\n        Y.append(feature_list[i])\n    \n    y_pos = np.arange(len(Y))\n    ax.barh(y_pos, X, align='center')\n    #ax.set_yticklabels(Y)\n    ax.invert_yaxis()  # labels read top-to-bottom\n    \n    ax.set_xlabel('Overall average movie ratings')\n    ax.set_ylabel(feature_name + ' sample list index')\n    ax.set_title(feature_name + ' to associated movie(s) performance (' + str(int(len(feature_list)/n)) + ' samples), variance: ' + str(var))\n    \n    plt.show()\n    \n    return feature_list\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create lists for each feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"genres_list = generate_list(movies_shortened, 'genres')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_list = generate_list(movies_shortened, 'cast')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crew_list = generate_list(movies_shortened, 'crew')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prod_companies_list = generate_list(movies_shortened, 'production_companies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_list = generate_list(movies_shortened, 'keywords')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis\n\nMost of the features show some variance except genra so lets drop genre from movies_shortened\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened = movies_shortened[['id', 'original_title', 'cast', 'crew', 'production_companies', 'keywords','vote_average']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2: Create a binary representation for each feature\n\n##### Using the lists created, create binary arrays that indicated whether or not feature_name can be found in this movie\n\nnote: each array represents a feature associated with movies with lowest average ratings to highest average ratings\n- this is useful because we can use the array as a gauge for how well the features track record in movies are","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_bin_array(this_list, all_features):\n    bin_list = []\n    for element in all_features:\n        if element in this_list:\n            bin_list.append(1)\n        else:\n            bin_list.append(0)\n    return bin_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened['cast'] = movies_shortened['cast'].apply(lambda x: calculate_bin_array(x, cast_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened['crew'] = movies_shortened['crew'].apply(lambda x: calculate_bin_array(x, crew_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened['production_companies'] = movies_shortened['production_companies'].apply(lambda x: calculate_bin_array(x, prod_companies_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened['keywords'] = movies_shortened['keywords'].apply(lambda x: calculate_bin_array(x, keywords_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Look at distribution of 1's in a number-line format","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bin(mov):\n    cast_bin = mov[2]\n    cast_index = []\n    # create arrays of indeces where bin number is one\n    for i in range(len(cast_bin)):\n        if cast_bin[i] == 1:\n            cast_index.append(i)\n    \n    crew_bin = mov[3]\n    crew_index = []\n    for i in range(len(crew_bin)):\n        if crew_bin[i] == 1:\n            crew_index.append(i)\n    \n    prod_bin = mov[4]\n    prod_index = []\n    for i in range(len(prod_bin)):\n        if prod_bin[i] == 1:\n            prod_index.append(i)\n    \n    keywords_bin = mov[5]\n    keywords_index = []\n    for i in range(len(keywords_bin)):\n        if keywords_bin[i] == 1:\n            keywords_index.append(i)\n    \n    font = {'family': 'serif',\n        'color':  'red',\n        'weight': 'normal',\n        'size': 10,\n        }\n    \n    fig, ax = plt.subplots(4,1,figsize=(5,1))\n    plt.subplots_adjust(hspace = 5)\n    ax[0].scatter(cast_index, np.zeros_like(cast_index), vmin=-2)\n    ax[0].set_title('Cast', loc = 'left', fontdict=font)\n    ax[0].set_xlim(0,len(cast_bin))\n    ax[0].set_yticks([])\n    ax[0].set_xticks([])\n    \n    ax[1].scatter(crew_index, np.zeros_like(crew_index), vmin=-2)\n    ax[1].set_title('Crew', loc = 'left', fontdict=font)\n    ax[1].set_xlim(0,len(crew_bin))\n    ax[1].set_yticks([])\n    ax[1].set_xticks([])\n    \n    ax[2].scatter(prod_index, np.zeros_like(prod_index), vmin=-2)\n    ax[2].set_title('Production companies', loc = 'left', fontdict=font)\n    ax[2].set_xlim(0,len(prod_bin))\n    ax[2].set_yticks([])\n    ax[2].set_xticks([])\n    \n    ax[3].scatter(keywords_index, np.zeros_like(keywords_index), vmin=-2)\n    ax[3].set_title('Keywords', loc = 'left', fontdict=font)\n    ax[3].set_xlim(0,len(keywords_bin))\n    ax[3].set_yticks([])\n    ax[3].set_xticks([])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_sample = movies_shortened.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Movie: ' + movies_sample.iloc[0][1] + '\\nRating: ' + str(movies_sample.iloc[0][-1]) + '\\n')\nplot_bin(movies_sample.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Movie:' + movies_sample.iloc[1][1] + '\\nRating: ' + str(movies_sample.iloc[1][-1]) + '\\n')\nplot_bin(movies_sample.iloc[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Movie:' + movies_sample.iloc[2][1] + '\\nRating: ' + str(movies_sample.iloc[2][-1]) + '\\n')\nplot_bin(movies_sample.iloc[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Movie:' + movies_sample.iloc[3][1] + '\\nRating: ' + str(movies_sample.iloc[3][-1]) + '\\n')\nplot_bin(movies_sample.iloc[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Movie:' + movies_sample.iloc[4][1] + '\\nRating: ' + str(movies_sample.iloc[4][-1]) + '\\n')\nplot_bin(movies_sample.iloc[4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analysis\n - Movies with lower ratings have features leaning towards the left, while movies with higher ratings have features leaning to the right. \n - This shows that there's a relationship between features and ratings if feature names is organized from lowest movie rating associations to highest movie rating associations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Find concentration points in each array\nFind areas where numbers are grouped, and identify points that resemble the centers of binary distribution\n\n    Generic example: [1110001111100101] -> [0300000050000020] -> [(1,3), (8,5), (14,2)] tuple[0] is the index of concentration, tuple[1] is the number of 1's about index","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_arr(arr, n_splits): \n      \n    # looping till length l \n    for i in range(0, len(arr), n_splits):  \n        yield arr[i:i + n_splits] \n\ndef find_concentration(arr, n = 100): # n is the number of concentration points to find\n    #seperate array into batches\n    batches = list(split_arr(arr,int(len(arr)/n)))\n    concentrations = []\n    for i in range(len(batches)):\n        point = 0\n        num_ones = 0\n        for j in range(len(batches[i])):\n            if batches[i][j] == 1:\n                point += j + (i * int(len(arr)/n)) # adding correction for batches\n                num_ones += 1\n        if num_ones > 0:\n            point = point/num_ones\n            concentrations.append((point,num_ones))\n    return concentrations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_concentrations(df, feature_names):\n    for feature_name in feature_names:\n        print('feature: ', feature_name)\n        df[feature_name] = df[feature_name].apply(lambda x: find_concentration(x))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened = to_concentrations(movies_shortened, ['cast', 'crew', 'production_companies', 'keywords'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3.1: Find a decimal value that represents the concentration points\nThe point will represent the weighted average of all points of concentration\nThe weight is the number of ones for each concentration point","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def w_avg(arr):\n    weight = 0 #weight\n    s = 0 # position*weight\n    for element in arr:\n        s += (element[0] * element[1])\n        weight += element[1]\n    return s/weight #weighted average","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_weighted_avg(df, feature_names):\n    for feature_name in feature_names:\n        print('Current: ', feature_name)\n        df[feature_name] = df[feature_name].apply(lambda x: w_avg(x))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened = to_weighted_avg(movies_shortened, ['cast', 'crew', 'production_companies', 'keywords'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened['vote_average'] = movies['vote_average']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_shortened.sample(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4: Normalize the features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, make a dataframe to isolate the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_isolated = movies_shortened[['cast', 'crew', 'production_companies', 'keywords']] #extract only features from df, and scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfeat_scaled = pd.DataFrame(scaler.fit_transform(feat_isolated.astype(float)))\nfeat_scaled.index = feat_isolated.index\nfeat_scaled.columns = feat_isolated.columns\n\n#Seperate dataframe for target\ntarget_df = pd.DataFrame()\ntarget_df['ratings'] =  movies_shortened['vote_average']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_scaled.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising Features in a Scatterplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2, figsize=(24,20))\n\nax[0,0].scatter(target_df['ratings'], feat_scaled['cast'], facecolor='blue')\nax[0,0].set_xlabel('rating')\nax[0,0].set_ylabel('cast normalized')\nax[0,0].set_title('cast')\n\nax[1,0].scatter(target_df['ratings'], feat_scaled['crew'], facecolor='green')\nax[1,0].set_xlabel('rating')\nax[1,0].set_ylabel('crew normalized')\nax[1,0].set_title('crew')\n\nax[0,1].scatter(target_df['ratings'], feat_scaled['production_companies'], facecolor='red')\nax[0,1].set_xlabel('rating')\nax[0,1].set_ylabel('production companies normalized')\nax[0,1].set_title('Production Companies')\n\nax[1,1].scatter(target_df['ratings'], feat_scaled['keywords'], facecolor='orange')\nax[1,1].set_xlabel('rating')\nax[1,1].set_ylabel('keywords normalized')\nax[1,1].set_title('keywords')\n\nfig.suptitle(\"Corrlation between a movie's features and its rating\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, there's a clear correlation between the features and the ratings\nThe straight lines in figures for keywords and production companies represents the absence of keywords and production companies for certain movies","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Split Data into testing and training\nWill be splitting trainting : testing : validation -> (0.7) : (0.15) : (0.15)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef train_test_val_split(df_feat, df_target, train_frac):\n    train_features, test_features, train_target, test_target = train_test_split(df_feat, df_target, test_size = train_frac) #splitting training from rest of the dataset\n    return (train_features, train_target), (test_features, test_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(features_train, target_train), (features_test, target_test) = train_test_val_split(feat_scaled, target_df,0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Import Scikit Learn's [Bayesian Ridge][bayes] Regressor\n\nBayesian ridge is one of many regression models offered by Scikit Learn. I'm choosing this model because it's ideal for dealing with data containing multiple outliers (movies with ratings inconsistent with its features)\n\n\n[//]: # (These are reference links used in the body of this note and get stripped out when the markdown processor does its job. There is no need to format nicely because it shouldn't be seen. Thanks SO - http://stackoverflow.com/questions/4823468/store-comments-in-markdown-syntax)\n\n\n   [bayes]: <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import BayesianRidge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create and train model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = BayesianRidge()\nreg.fit(features_train.values, target_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Make predictions with features_test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_pred = reg.predict(features_test.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation\n#### Plot predictions vs test ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.axis([0,10,0,10])\nplt.scatter(target_test, target_pred)\n\nindex_arr = [n for n in range(11)]\nplt.plot(index_arr,'r--')             \nplt.xlabel(\"Movie Ratings\")\nplt.ylabel(\"Predicted Ratings\")\nplt.title(\"Movie ratings vs Predicted ratings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get R^2 score\n\nAn [r^2 score][r2] is ideal for regression models because it gauges how well the variance of movie ratings can be explained by the features used (cast, crew, production companies, and keywords)\n\n[//]: # (These are reference links used in the body of this note and get stripped out when the markdown processor does its job. There is no need to format nicely because it shouldn't be seen. Thanks SO - http://stackoverflow.com/questions/4823468/store-comments-in-markdown-syntax)\n\n\n   [r2]: <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nscore = r2_score(target_test, target_pred)\n\nprint(\"R^2 Score for predictions:\", score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThough 0.7997 shows that the variance of ratings in can be explained by movie features, Although its not very high.\n    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Now making a recommendation engine**\nMaking recommendation based on plot\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movie = pd.read_csv(\"../input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\ncredit = pd.read_csv(\"../input/tmdb-movie-metadata/tmdb_5000_credits.csv\")\ncredit.columns = ['id','tittle','cast','crew']\nmovies = movie.merge(credit,on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies['overview'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nNow we'll compute Term Frequency-Inverse Document Frequency (TF-IDF) vectors for each overview.\n\nNow if you are wondering what is term frequency , it is the relative frequency of a word in a document and is given as (term instances/total instances). Inverse Document Frequency is the relative count of documents containing the term is given as log(number of documents/documents with term) The overall importance of each word to the documents in which they appear is equal to TF * IDF\n\nThis will give you a matrix where each column represents a word in the overview vocabulary (all the words that appear in at least one document) and each row represents a movie, as before.This is done to reduce the importance of words that occur frequently in plot overviews and therefore, their significance in computing the final similarity score.\n\nFortunately, scikit-learn gives you a built-in TfIdfVectorizer class that produces the TF-IDF matrix in a couple of lines. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Replace NaN with an empty string\nmovies['overview'] = movies['overview'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(movies['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nWe see that over 20,808 different words were used to describe the 4732 movies in our dataset.\n\nWith this matrix in hand, we can now compute a similarity score. There are several candidates for this; such as the euclidean, the Pearson and the cosine similarity scores. There is no right answer to which score is the best. Different scores work well in different scenarios and it is often a good idea to experiment with different metrics.\n\nWe will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. We use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate. Mathematically, it is defined as follows:\n\nSince we have used the TF-IDF vectorizer, calculating the dot product will directly give us the cosine similarity score. Therefore, we will use sklearn's linear_kernel() instead of cosine_similarities() since it is faster.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nWe are going to define a function that takes in a movie title as an input and outputs a list of the 10 most similar movies. Firstly, for this, we need a reverse mapping of movie titles and DataFrame indices. In other words, we need a mechanism to identify the index of a movie in our metadata DataFrame, given its title.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(movies.index, index=movies['title']).drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nWe are now in a good position to define our recommendation function. These are the following steps we'll follow :-\n\n    Get the index of the movie given its title.\n    Get the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples where the first element is its position and the second is the similarity score.\n    Sort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n    Get the top 10 elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\n    Return the titles corresponding to the indices of the top elements.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cosine_sim=cosine_sim):\n    # Get the index of the movie that matches the title\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return movies['title'].iloc[movie_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nWhile our system has done a decent job of finding movies with similar plot descriptions, the quality of recommendations is not that great. \"The Dark Knight Rises\" returns all Batman movies while it is more likely that the people who liked that movie are more inclined to enjoy other Christopher Nolan movies. This is something that cannot be captured by the present system.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nCredits, Genres and Keywords Based Recommender\n\nIt goes without saying that the quality of our recommender would be increased with the usage of better metadata. That is exactly what we are going to do in this section. We are going to build a recommender based on the following metadata: the 3 top actors, the director, related genres and the movie plot keywords.\n\nFrom the cast, crew and keywords features, we need to extract the three most important actors, the director and the keywords associated with that movie. Right now, our data is present in the form of \"stringified\" lists , we need to convert it into a safe and usable structure\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parse the stringified features into their corresponding python object\n\nfrom ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres']\nfor feature in features:\n    movies[feature] = movies[feature].apply(literal_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will write function to extract information from each function\n# Get the director's name from the crew feature. If director is not listed, return NaN\ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns the list top 3 elements or entire list; whichever is more.\ndef get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    #Return empty list in case of missing/malformed data\n    return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define new director, cast, genres and keywords features that are in a suitable form.\nmovies['director'] = movies['crew'].apply(get_director)\n\nfeatures = ['cast', 'keywords', 'genres']\nfor feature in features:\n    movies[feature] = movies[feature].apply(get_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the new features of the first 3 films\nmovies[['title', 'cast', 'director', 'keywords', 'genres']].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to convert all strings to lower case and strip names of spaces\ndef clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        #Check if director exists. If not, return empty string\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    movies[feature] = movies[feature].apply(clean_data)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now in a position to create our \"metadata set\", which is a string that contains all the metadata that we want to feed to our vectorizer (namely actors, director and keywords).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_soup(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\nmovies['soup'] = movies.apply(create_soup, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next steps are the same as what we did with our plot description based recommender. One important difference is that we use the CountVectorizer() instead of TF-IDF. This is because we do not want to down-weight the presence of an actor/director if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(movies['soup'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset index of our main DataFrame and construct reverse mapping as before\nmovies = movies.reset_index()\nindices = pd.Series(movies.index, index=movies['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Dark Knight Rises', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is giving better recommendation than previous systme.Lets try some more movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('The Godfather', cosine_sim2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('Before Sunrise')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_recommendations('Pulp Fiction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}