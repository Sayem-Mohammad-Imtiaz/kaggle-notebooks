{"cells":[{"metadata":{"_cell_guid":"66f404e7-e68d-4d21-9cee-04027e81b6fd","_uuid":"efd3212a5bce716313b62b06695a78f85075a323"},"cell_type":"markdown","source":"## 用决策树来分类贷款是否优良"},{"metadata":{"_cell_guid":"ed4b6684-ad79-41a9-977a-f5f70a8a2ba3","_uuid":"31af9aed35757e34d12ed0a4e8c63c64710d7999"},"cell_type":"markdown","source":"[LendingClub](https://www.lendingclub.com/) 是一家贷款公司. 在本次作业中,我们需要手动实现决策树来预测一份贷款是否安全，并对比不同复杂度下决策树的表现  \n\n![alt text](./notebook_image/lc.jpg)"},{"metadata":{"_cell_guid":"052f9c1b-1577-45c1-8c88-105ba0501863","_uuid":"a1e19726f9370759713d300f4cdb0b0f1f2509f4","trusted":false,"collapsed":true},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import metrics\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29da4347-9518-4eb2-8374-8b8393244d4b","_uuid":"c2534724503df34afdb8773f750b353d1d631e44"},"cell_type":"markdown","source":"## 读取数据"},{"metadata":{"_cell_guid":"892c3516-5373-4c59-b06f-a72e39fdf676","_uuid":"5cd6555e43687f647d174691586eb3f4d66f1f8a","trusted":false,"collapsed":true},"cell_type":"code","source":"#data = pd.read_csv(os.path.join(\"data\", \"loan_sub.csv\"), sep=',')\n#data = pd.read_csv(os.path.join(\"/Users/jiadileng/Desktop/machine learning/jiuzhang/week4/input\", \"loan_sub.csv\"), sep=',')\ndata = pd.read_csv(os.path.join(\"../input\", \"loan_sub.csv\"), sep=',')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"85e9d110-9447-45f2-9669-9679bf96e9fe","_uuid":"788028a69b12157afdc8bc052f82c671d71e6d1a"},"cell_type":"markdown","source":"## 打印可用特征"},{"metadata":{"_cell_guid":"5c11d994-29ca-4fa4-a177-b8aa12d968d1","_uuid":"fcac7a0154d0029f0901d9898e42a75d2433a91f","trusted":false,"collapsed":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"58b5cece54433fd563939e2b0bd4b0d81753deba"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"286be8f7-67d5-4470-9156-50e359efaf58","_uuid":"adbc4064c57c2cbf686ecfbab5eb5fc2bd03a515"},"cell_type":"markdown","source":"## 预处理预测目标\n\n预测目标是一列'bad_loans'的数据。其中**1**表示的是不良贷款，**0**表示的是优质贷款。\n\n将预测目标处理成更符合直觉的标签，创建一列 `safe_loans`. 并且: \n\n* **+1** 表示优质贷款, \n* **-1** 表示不良贷款. "},{"metadata":{"_cell_guid":"a23c6333-55db-40bc-8d6a-29f4ab0334a2","_uuid":"520d19866dcb451b437cdcad8fa21ab5b7ab5524","trusted":false,"collapsed":true},"cell_type":"code","source":"# safe_loans =  1 => safe\n# safe_loans = -1 => risky\n#TODO\ndata['safe_loans'] = data['bad_loans'].apply(lambda x: +1 if x == 0 else -1)\ndata = data.drop('bad_loans', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ced8fab-2d4b-40df-89e8-12254ba8fead","_uuid":"e1160dae5696b268fa08b2ad949edd7ee348a70f"},"cell_type":"markdown","source":"## 打印优质贷款与不良贷款的比例"},{"metadata":{"_cell_guid":"1587083c-5d4c-415e-bc88-c8b36b15f120","_uuid":"3e45e2aa029cb60c92918c898baed106d22a428a","trusted":false,"collapsed":true},"cell_type":"code","source":"data['safe_loans'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c974f4f-6476-448b-80be-b755ba0062d5","_uuid":"b5db243556bdd0007d7f985293c5bc043353ac05"},"cell_type":"markdown","source":"#### 这是一个不平衡数据， 好的贷款远比坏的贷款要多. "},{"metadata":{"_cell_guid":"e7b18f35-448a-47f0-92b4-7d865652e34d","_uuid":"fdcf4d5a0f36c2e3446109a92b2961a72b878c01"},"cell_type":"markdown","source":"## 选取用于预测的特征"},{"metadata":{"_cell_guid":"cfd013e4-de8b-47c1-b28f-487543295915","_uuid":"88652259a0004ce261d64d948b73e84a9485264b","trusted":false,"collapsed":true},"cell_type":"code","source":"cols = ['grade', 'term','home_ownership', 'emp_length']\ntarget = 'safe_loans'\n\ndata = data[cols + [target]]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56c45e9a-e2a3-4aa9-b458-150d0d40d38b","_uuid":"a491ca370cc93d03060d460df03c998cc4753749"},"cell_type":"markdown","source":"## 创建更为平衡的数据集  \n\n* 对占多数的标签进行下采样  \n* 注意有很多方法处理不平衡数据，下采样只是其中之一"},{"metadata":{"_cell_guid":"8074e537-8045-4282-ac71-d76df34a84e2","_uuid":"c6bd4a48d62079be51a771dbe9d047db740ba4f1","trusted":false,"collapsed":true},"cell_type":"code","source":"data['safe_loans'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ffdfd93-d4a4-4105-aa9a-0df057bd2be4","_uuid":"6eeffb2a1c6afcb32ca35a33c35f4157e1cb3cac","trusted":false,"collapsed":true},"cell_type":"code","source":"# use the percentage of bad and good loans to undersample the safe loans.\nbad_ones = data[data[target] == -1]# TODO\nsafe_ones = data[data[target] == 1]# TODO\npercentage = len(bad_ones) / float(len(safe_ones))\n#percentage = float(len(bad_ones) / len(safe_ones))\n\n\nrisky_loans = bad_ones\nsafe_loans = safe_ones.sample(frac=percentage, random_state=33)#根据刚才算出来的比例对好的贷款进行随机抽样\n\n# combine two kinds of loans\ndata_set = pd.concat([risky_loans, safe_loans], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fca93ef-54dc-4aeb-b358-4fe1747ae212","_uuid":"0c620f0f6b574586a8b1679e3d1a8a725c66e6e3"},"cell_type":"markdown","source":"Now, let's verify that the resulting percentage of safe and risky loans are each nearly 50%. 希望能学到每个label下对应的特征，所以两个label数目要平衡一些"},{"metadata":{"_cell_guid":"646bc01e-04be-4be6-933a-10fd869d6a7b","_uuid":"ad42250a12a835fbb5d5212dbbddb36c2f75d9c5","trusted":false,"collapsed":true},"cell_type":"code","source":"data_set[target].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2be23ac5-993e-45c5-929c-b3a52f72ff69","_uuid":"b70e269c90700b8448133556f73ccdfd56e8c4da"},"cell_type":"markdown","source":"## Preprocessing your features"},{"metadata":{"_cell_guid":"c1a5b773-e48a-47ea-ac6f-89370c1df7f1","_uuid":"401c87cf16e667e1b2400143e40465fa08646d27","trusted":false,"collapsed":true},"cell_type":"code","source":"#actually this is not dummies but one-hot encoding\ndef dummies(data, columns=['pclass','name_title','embarked', 'sex']): \n    for col in columns:\n        data[col] = data[col].apply(lambda x: str(x))\n        new_cols = [col + '_' + i for i in data[col].unique()]\n        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)[new_cols]], axis=1)\n        del data[col]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d4d2d4f-622b-4d10-9d59-043191b6cf79","_uuid":"f10e7c09adfc6a6f8e9af5c5916eed63ef8b55c7","trusted":false,"collapsed":true},"cell_type":"code","source":"#grade, home_ownership, target\ncols = ['grade', 'term','home_ownership', 'emp_length']\ndata_set = dummies(data_set, columns=cols)\ndata_set.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84874e9e-2908-4cad-b36c-d043bb01b274","_uuid":"412b4bee68ba3a21ae440119d88ec6f8d35610ac"},"cell_type":"markdown","source":"## 将数据分成训练集和测试集"},{"metadata":{"_cell_guid":"1026e5f8-dff2-49cf-8b33-d5141b517b90","_uuid":"33b3a538627ea928146bc37d4ed5bb238f712733"},"cell_type":"markdown","source":"重要的事情说三遍!!  \n\n**把你的爪子从TEST DATA上拿开!!**   \n**把你的爪子从TEST DATA上拿开!!**  \n**把你的爪子从TEST DATA上拿开!!**  \n"},{"metadata":{"_cell_guid":"3dadfbc4-4e3c-40de-98bc-35405d0ffb1d","_uuid":"61ca9a1f0607b225327ce317c94f71b842d9e6fe","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data, test_data = train_test_split(data_set, test_size=0.2, random_state=33)#TODO\ntrainX, trainY = train_data[train_data.columns[1:]], pd.DataFrame(train_data[target])#TODO\ntestX, testY = test_data[test_data.columns[1:]], pd.DataFrame(test_data[target])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2931b13e-fc4e-4882-99e2-6dab89c96b04","_uuid":"595b1dad2bb40c97efc0f75b9b9bed2b1351cb15","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"10d43465-2322-4a71-b694-535299572521","_uuid":"5c0622a8bd45ac75f395a3d2599c3a995e0e96af"},"cell_type":"markdown","source":"## 建自己的决策树!  \n\n任务：  \n1 实现根据error来选择最佳划分特征的函数best_split()  \n2 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n3 实现树节点的类TreeNode  \n4 实现模型的类MyDecisionTree  "},{"metadata":{"_cell_guid":"d5c3ac35-9e48-42d6-b4ec-bd0e502ab839","_uuid":"e22e5624a06d1b338ae7bb171a4b15122bdf4e9c"},"cell_type":"markdown","source":"#### 任务1(Optional)， 实现根据error来选择最佳划分特征的函数best_split()  \n约定树的左边对应target == 0， 树的右边对应target == 1"},{"metadata":{"_cell_guid":"82618b83-265e-4ba9-83d5-38b907a78d0c","_uuid":"4b41169e2a4e05727eac84e8a59d049994f566fa"},"cell_type":"markdown","source":"def count_errors(labels_in_node):\n    if len(labels_in_node) == 0:\n        return 0\n    \n    positive_ones = labels_in_node.apply(lambda x: x==1).sum()\n    negative_ones = labels_in_node.apply(lambda x: x==-1).sum()\n    \n#错误数量的给出是基于major class prediction的，一个集合里有1有-1,如果做major class prediction以后，minor class的数量就是错误的数量    \n    return min(positive_ones, negative_ones) \n\n\n\ndef best_split(data, features, target):\n    # return the best feature\n    best_feature = None\n    best_error = 2.0 \n    num_data_points = float(len(data))  \n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split = data[data[feature] == 0]\n        \n        # 右分支对应当前特征为1的数据点\n        right_split = data[data[feature] == 1]\n        \n        # 计算左边分支里犯了多少错\n        left_misses = count_errors(left_split[target])           \n\n        # 计算右边分支里犯了多少错\n        right_misses = count_errors(right_split[target])\n            \n        # 计算当前划分之后的分类犯错率\n        error = (left_misses + right_misses) * 1.0 / num_data_points\n\n        # 更新应选特征和错误率，注意错误越低说明该特征越好\n        if error < best_error:\n            best_error = error\n            best_feature = feature\n    return best_feature"},{"metadata":{"_cell_guid":"37a44e8f-6ad4-4138-b47c-5b3ef7e097dc","_uuid":"07a2bf1914fb0e136ba0d59a64fc4711c12f82eb"},"cell_type":"markdown","source":"#### 任务2， 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n"},{"metadata":{"_cell_guid":"ff8f497e-3bf3-4845-ba07-1dc1eb8f20d8","_uuid":"182b6429b6f0e5bcde722af7806014d0392bb79b","trusted":false,"collapsed":true},"cell_type":"code","source":"def entropy(labels_in_node):\n    # 二分类问题: 0 or 1\n    n = len(labels_in_node)\n    s1 = (labels_in_node==1).sum()\n    if s1 == 0 or s1 == n: # indicates the labels are the same~\n        return 0\n    \n    p1 = float(s1) / n\n    p0 = 1 - p1\n    return -p0 * np.log2(p0) - p1 * np.log2(p1) #比较问题与底数是几无关\n\n\ndef best_split_entropy(data, features, target):\n    \n    best_feature = None\n    best_info_gain = float('-inf') \n    num_data_points = float(len(data))\n    # 计算划分之前数据集的整体熵值\n    entropy_original = entropy(data[target])#划分前的熵值\n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split = data[data[feature]==0]\n        \n        # 右分支对应当前特征为1的数据点\n        right_split = data[data[feature]==1] \n        \n        # 计算左边分支的熵值\n        left_entropy = entropy(left_split[target])           \n\n        # 计算右边分支的熵值\n        right_entropy = entropy(right_split[target])\n            \n        # 计算左边分支与右分支熵值的加权和（数据集划分后的熵值）\n        entropy_split = len(left_split)/num_data_points * left_entropy + len(right_split)/num_data_points * right_entropy#TODO\n        \n        # 计算划分前与划分后的熵值差得到信息增益\n        info_gain = entropy_original - entropy_split\n\n        # 更新最佳特征和对应的信息增益的值\n        if info_gain > best_info_gain:\n            best_info_gain = info_gain\n            best_feature = feature\n    return best_feature\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c604f37-9cc7-4f51-bebb-7ceb783b26ee","_uuid":"9476b35e0f1134d6a8dac22d238b902034ceab15"},"cell_type":"markdown","source":"#### 任务3，实现树节点的类TreeNode，每个树节点应该包含如下信息:  \n\n   3.1 is_leaf: True/False  表示当前节点是否为叶子节点  \n   \n   3.2 prediction: 当前节点做全民公投的预测结果\n   \n   3.3 left: 左子树  \n   \n   3.4 right: 右子树 \n   \n   3.5 split_feature: 当前节点用来划分数据时所采用的特征"},{"metadata":{"_cell_guid":"aebd8954-0670-4566-a8fa-584c5d8dc690","_uuid":"462ec55acb6a7bf3cd62868c251d81c5576c7fa2","trusted":false,"collapsed":true},"cell_type":"code","source":"class TreeNode:\n    def __init__(self, is_leaf, prediction, split_feature):\n        self.is_leaf = is_leaf\n        self.prediction = prediction\n        self.split_feature = split_feature\n        self.left = None\n        self.right = None\n    \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"649b5586-c78e-46f7-bd42-bcb6f4b557e1","_uuid":"76eec750f726027822ada0db6cc927571ef3c432"},"cell_type":"markdown","source":"#### 任务4，实现模型的类MyDecisionTree， 实现如下主要函数  \n  \n  \n   4.1 fit(): 模型在训练集上的学习  \n   \n   4.2 predict(): 模型在数据集上的预测\n   \n   4.3 score(): 模型在测试集上的得分   \n   \n   \n   \n   \n   为了实现4.1 - 4.3的方法， 需要实现如下辅助函数  \n   4.4 create_tree(): 创建一棵树  \n   \n   4.5 create_leaf(): 创建叶子节点  \n   \n   4.6 predict_single_data(): 模型预测单个数据  \n   \n   4.7 count_leaves(): 统计模型的叶子数"},{"metadata":{"_cell_guid":"503a40e2-5571-4cbf-83ef-f8e40aa73f95","_uuid":"9244c68ca6a400731bf2b201c791fd06cc273d7a","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator #模型中的父类\nfrom sklearn.metrics import accuracy_score\nclass MyDecisionTree(BaseEstimator):\n    \n    def __init__(self, max_depth, min_error):\n        self.max_depth = max_depth\n        self.min_error = min_error\n    \n    def fit(self, X, Y, data_weights = None):\n        \n        data_set = pd.concat([X, Y], axis=1) #concatenate pandas objects along a particular axis\n        features = X.columns\n        target = Y.columns[0]#Y其实只有一列\n        self.root_node = self.create_tree(data_set, features, target, current_depth=0, max_depth=self.max_depth, min_error=self.min_error)\n    \n    def create_tree(self, data, features, target, current_depth = 0, max_depth = 10, min_error=0):\n        \"\"\"\n        探索三种不同的终止划分数据集的条件  \n  \n        termination 1, 当错误率降到min_error以下, 终止划分并返回叶子节点  \n        termination 2, 当特征都用完了, 终止划分并返回叶子节点  \n        termination 3, 当树的深度等于最大max_depth时, 终止划分并返回叶子节点\n        \"\"\"\n        \n    \n        # 拷贝一下可用特征\n        remaining_features = features[:]\n\n        target_values = data[target]\n\n        \n        \n        #################\n        #  第一部分： 递归出口！\n        #################\n        # termination 1   bonus task\n        #if count_errors(target_values) <= min_error:\n            #print(\"Termination 1 reached.\")     \n            #return self.create_leaf(target_values) \n\n        # termination 2\n        if len(remaining_features) == 0:\n            print(\"Termination 2 reached.\")    \n            return self.create_leaf(target_values) #返回叶子节点\n\n        # termination 3\n        if current_depth >= max_depth: \n            print(\"Termination 3 reached.\")\n            return self.create_leaf(target_values)\n\n\n        \n        \n        #################\n        #  第二部分： 如果继续划分，划分数据集！\n        #################\n        \n        # 选出最佳当前划分特征\n        #split_feature = best_split(data, features, target)   #根据正确率划分   bonus task\n        split_feature = best_split_entropy(data, features, target)  # 根据信息增益来划分\n\n        # 选出最佳特征后，该特征为0的数据分到左边，该特征为1的数据分到右边\n        left_split = data[data[split_feature]==0]\n        right_split = data[data[split_feature]==1]\n\n        # 剔除已经用过的特征\n        remaining_features = remaining_features.drop(split_feature)\n        print(\"Split on feature %s. (%s, %s)\" % (split_feature, str(len(left_split)), str(len(right_split))))\n\n        # 如果当前数据全部划分到了一边，就不需要再划分了，直接创建叶子节点返回即可\n        if len(left_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(left_split[target])\n        if len(right_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(right_split[target])\n\n        # 递归上面的步骤\n        left_tree = self.create_tree(left_split, remaining_features, target, current_depth + 1, max_depth, min_error)     \n        right_tree = self.create_tree(right_split, remaining_features, target, current_depth + 1, max_depth, min_error)\n        \n        \n        #################\n        #  第三部分： 左右递归完之后，把当前结果返回！\n        #################\n        \n        #生成当前的树节点\n        result_node = TreeNode(False, None, split_feature)#False代表它不是叶子节点\n        result_node.left = left_tree\n        result_node.right = right_tree\n        return result_node    \n    \n    \n    \n    def create_leaf(self, target_values):\n        # 用于创建叶子的函数\n\n        # 初始化一个树节点\n        leaf = TreeNode(True, None, None)\n\n        # 统计当前数据集里标签为+1和-1的个数，较大的那个即为当前节点的预测结果\n        num_positive_ones = len(target_values[target_values == +1])\n        num_negative_ones = len(target_values[target_values == -1])\n\n        if num_positive_ones > num_negative_ones:\n            leaf.prediction = 1\n        else:\n            leaf.prediction = -1\n\n        # 返回叶子        \n        return leaf     \n    \n    \n        \n    def predict(self, X):\n        #某一行拿出来，给定结果，把结果应用到所有行\n        prediction = X.apply(lambda row: self.predict_single_data(self.root_node, row), axis=1)\n        return prediction\n    \n    \n    \n    def predict_single_data(self, tree, X, annotate = False):   \n        # 如果已经是叶子节点直接返回叶子节点的预测结果\n        if tree.is_leaf:\n            if annotate: \n                print(\"leaf node, predicting %s\" % tree.prediction)\n            return tree.prediction\n        else:\n            # 查询x对应当前节点特征的值\n            split_feature_value = X[tree.split_feature]\n\n            if annotate: \n                print(\"Split on %s = %s\" % (tree.split_feature, split_feature_value))\n            if split_feature_value == 0:\n                #如果x在该特征上的值为0，交给左子树来预测\n                return self.predict_single_data(tree.left, X, annotate) #annotate---是否打印中间信息\n            else:\n                #如果x在该特征上的值为1，交给右子树来预测\n                return self.predict_single_data(tree.right, X, annotate)\n    \n    \n    \n        \n    def score(self, testX, testY):\n        target = testY.columns[0]\n        result = self.predict(testX)\n        return accuracy_score(testY[target], result)\n\n\n    def count_leaves(self):\n        return self.count_leaves_helper(self.root_node)\n    \n    def count_leaves_helper(self, tree):\n        if tree.is_leaf:\n            return 1\n        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e9b37542-8cef-4542-b457-426fe37b83b4","_uuid":"987eac9819fd345cbb1eb8b7818ec2bab40f80fa","trusted":false,"collapsed":true},"cell_type":"code","source":"m = MyDecisionTree(max_depth = 10, min_error = 1e-15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"257ac28e-4974-411a-a608-eae508735f12","_uuid":"3f3e4a07f9d43d966570f0a987ab72d8e9197685","trusted":false,"collapsed":true},"cell_type":"code","source":"m.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be4e4f70-2d95-4e5b-8671-3695bba235c4","_uuid":"c3a7adaeb6b0a357c8d9846923e72d0efb241b73","trusted":false,"collapsed":true},"cell_type":"code","source":"m.score(testX, testY)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bd9404d7-0e3b-4b4a-941f-8cfede6e5f4d","_uuid":"ead657801e1a8c09898618b7a99a5e276c3a5879","collapsed":true},"cell_type":"markdown","source":"### 决策树复杂度的探讨"},{"metadata":{"_cell_guid":"b9bdf781-7734-4be5-a4fa-ca46243cd14f","_uuid":"923be331520d5fc94b371acaad821d55c8bfe4b9","trusted":false,"collapsed":true},"cell_type":"code","source":"m.count_leaves()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"029a2cec-0f6f-4858-acb8-f4b5b1d633a6","_uuid":"8c5b00734086e58304bc96ed74aa41604618e853"},"cell_type":"markdown","source":"#### 探索不同树深度对决策树的影响  \n\n1 max_depth = 3  \n2 max_depth = 7  \n3 max_depth = 15\n"},{"metadata":{"_cell_guid":"2c802f5a-27fe-42ef-9cf1-6b20dbfce59b","_uuid":"a220b1a1f884599dada6745ea5ed2d5d11e465fc","trusted":false,"collapsed":true},"cell_type":"code","source":"model_1 = MyDecisionTree(max_depth=3, min_error = 1e-15)\nmodel_2 = MyDecisionTree(max_depth=7, min_error = 1e-15)\nmodel_3 = MyDecisionTree(max_depth=15, min_error = 1e-15)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"75b7b5cc-b814-4c50-8a5c-62565218a82b","_uuid":"cac8b6dcb2c923c18714a50153c0bc75ec385536","trusted":false,"collapsed":true},"cell_type":"code","source":"model_1.fit(trainX, trainY)\nmodel_2.fit(trainX, trainY)\nmodel_3.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"957fc910-6229-49e3-b023-f937ab95fd0c","_uuid":"26d6fbddeb3e11ee43ae6f73bee8d20d400b672c","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"model_1 training accuracy :\", model_1.score(trainX, trainY))\nprint(\"model_2 training accuracy :\", model_2.score(trainX, trainY))\nprint(\"model_3 training accuracy :\", model_3.score(trainX, trainY))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"38232712-0b19-4206-a8af-f2204e872c84","_uuid":"4dad532d7c32503064e860c3206577e597c429c1","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"model_1 testing accuracy :\", model_1.score(testX, testY))\nprint(\"model_2 testing accuracy :\", model_2.score(testX, testY))\nprint(\"model_3 testing accuracy :\", model_3.score(testX, testY))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5773339a-43e5-454f-815e-a085b5177e62","_uuid":"6118e4e687d296a37f51ea77d3b1ab3ca789950f","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"model_1 complexity is: \", model_1.count_leaves())\nprint(\"model_2 complexity is: \", model_2.count_leaves())\nprint(\"model_3 complexity is: \", model_3.count_leaves())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81e08379-0964-4576-b61b-dec8bc99f46c","_uuid":"f6d8a45cd76bab415e748c4acce4ef309fcaeb11"},"cell_type":"markdown","source":"结论：model_1---underfitting    model_3---overfitting 叶子节点的个数可以代表模型的复杂程度"},{"metadata":{"_uuid":"e9a059dfc53d4edfcbf5a4465d002d2dcf4e0efa"},"cell_type":"markdown","source":"# 任务1(Optional)， 实现根据error来选择最佳划分特征的函数best_split()"},{"metadata":{"_uuid":"52b5891f76d8d6f0877b4298f483b85174453cbc"},"cell_type":"markdown","source":"约定树的左边对应target == 0， 树的右边对应target == 1"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f4c114eaa3849ce2e008df1f7ddaab7d4389f698"},"cell_type":"code","source":"def count_errors(labels_in_node):\n    if len(labels_in_node) == 0:\n        return 0\n    \n    positive_ones = labels_in_node.apply(lambda x: x==1).sum()\n    negative_ones = labels_in_node.apply(lambda x: x==-1).sum()\n    \n#错误数量的给出是基于major class prediction的，一个集合里有1有-1,如果做major class prediction以后，minor class的数量就是错误的数量    \n    return min(positive_ones, negative_ones) \n\n\n\ndef best_split(data, features, target):\n    # return the best feature\n    best_feature = None\n    best_error = 2.0 \n    num_data_points = float(len(data))  \n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split = data[data[feature] == 0]\n        \n        # 右分支对应当前特征为1的数据点\n        right_split = data[data[feature] == 1]\n        \n        # 计算左边分支里犯了多少错\n        left_misses = count_errors(left_split[target])           \n\n        # 计算右边分支里犯了多少错\n        right_misses = count_errors(right_split[target])\n            \n        # 计算当前划分之后的分类犯错率\n        error = (left_misses + right_misses) * 1.0 / num_data_points\n\n        # 更新应选特征和错误率，注意错误越低说明该特征越好\n        if error < best_error:\n            best_error = error\n            best_feature = feature\n    return best_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2cec48dd10e565f370369e11eb084766178e2177"},"cell_type":"code","source":"class TreeNode:\n    def __init__(self, is_leaf, prediction, split_feature):\n        self.is_leaf = is_leaf\n        self.prediction = prediction\n        self.split_feature = split_feature\n        self.left = None\n        self.right = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2daaf19eb31bcdc62fc036d38c812b81ca536b6a"},"cell_type":"code","source":"from sklearn.base import BaseEstimator #模型中的父类\nfrom sklearn.metrics import accuracy_score\nclass MyDecisionTree(BaseEstimator):\n    \n    def __init__(self, max_depth, min_error):\n        self.max_depth = max_depth\n        self.min_error = min_error\n    \n    def fit(self, X, Y, data_weights = None):\n        \n        data_set = pd.concat([X, Y], axis=1) #concatenate pandas objects along a particular axis\n        features = X.columns\n        target = Y.columns[0]#Y其实只有一列\n        self.root_node = self.create_tree(data_set, features, target, current_depth=0, max_depth=self.max_depth, min_error=self.min_error)\n    \n    def create_tree(self, data, features, target, current_depth = 0, max_depth = 10, min_error=0):\n        \"\"\"\n        探索三种不同的终止划分数据集的条件  \n  \n        termination 1, 当错误率降到min_error以下, 终止划分并返回叶子节点  \n        termination 2, 当特征都用完了, 终止划分并返回叶子节点  \n        termination 3, 当树的深度等于最大max_depth时, 终止划分并返回叶子节点\n        \"\"\"\n        \n    \n        # 拷贝一下可用特征\n        remaining_features = features[:]\n\n        target_values = data[target]\n\n        \n        \n        #################\n        #  第一部分： 递归出口！\n        #################\n        # termination 1   bonus task\n        if count_errors(target_values) <= min_error:\n            print(\"Termination 1 reached.\")     \n            return self.create_leaf(target_values) \n\n        # termination 2\n        if len(remaining_features) == 0:\n            print(\"Termination 2 reached.\")    \n            return self.create_leaf(target_values) #返回叶子节点\n\n        # termination 3\n        if current_depth >= max_depth: \n            print(\"Termination 3 reached.\")\n            return self.create_leaf(target_values)\n\n\n        \n        \n        #################\n        #  第二部分： 如果继续划分，划分数据集！\n        #################\n        \n        # 选出最佳当前划分特征\n        split_feature = best_split(data, features, target)   #根据正确率划分   bonus task\n        #split_feature = best_split_entropy(data, features, target)  # 根据信息增益来划分\n\n        # 选出最佳特征后，该特征为0的数据分到左边，该特征为1的数据分到右边\n        left_split = data[data[split_feature]==0]\n        right_split = data[data[split_feature]==1]\n\n        # 剔除已经用过的特征\n        remaining_features = remaining_features.drop(split_feature)\n        print(\"Split on feature %s. (%s, %s)\" % (split_feature, str(len(left_split)), str(len(right_split))))\n\n        # 如果当前数据全部划分到了一边，就不需要再划分了，直接创建叶子节点返回即可\n        if len(left_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(left_split[target])\n        if len(right_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(right_split[target])\n\n        # 递归上面的步骤\n        left_tree = self.create_tree(left_split, remaining_features, target, current_depth + 1, max_depth, min_error)     \n        right_tree = self.create_tree(right_split, remaining_features, target, current_depth + 1, max_depth, min_error)\n        \n        \n        #################\n        #  第三部分： 左右递归完之后，把当前结果返回！\n        #################\n        \n        #生成当前的树节点\n        result_node = TreeNode(False, None, split_feature)#False代表它不是叶子节点\n        result_node.left = left_tree\n        result_node.right = right_tree\n        return result_node    \n    \n    \n    \n    def create_leaf(self, target_values):\n        # 用于创建叶子的函数\n\n        # 初始化一个树节点\n        leaf = TreeNode(True, None, None)\n\n        # 统计当前数据集里标签为+1和-1的个数，较大的那个即为当前节点的预测结果\n        num_positive_ones = len(target_values[target_values == +1])\n        num_negative_ones = len(target_values[target_values == -1])\n\n        if num_positive_ones > num_negative_ones:\n            leaf.prediction = 1\n        else:\n            leaf.prediction = -1\n\n        # 返回叶子        \n        return leaf     \n    \n    \n        \n    def predict(self, X):\n        #某一行拿出来，给定结果，把结果应用到所有行\n        prediction = X.apply(lambda row: self.predict_single_data(self.root_node, row), axis=1)\n        return prediction\n    \n    \n    \n    def predict_single_data(self, tree, X, annotate = False):   \n        # 如果已经是叶子节点直接返回叶子节点的预测结果\n        if tree.is_leaf:\n            if annotate: \n                print(\"leaf node, predicting %s\" % tree.prediction)\n            return tree.prediction\n        else:\n            # 查询x对应当前节点特征的值\n            split_feature_value = X[tree.split_feature]\n\n            if annotate: \n                print(\"Split on %s = %s\" % (tree.split_feature, split_feature_value))\n            if split_feature_value == 0:\n                #如果x在该特征上的值为0，交给左子树来预测\n                return self.predict_single_data(tree.left, X, annotate) #annotate---是否打印中间信息\n            else:\n                #如果x在该特征上的值为1，交给右子树来预测\n                return self.predict_single_data(tree.right, X, annotate)\n    \n    \n    \n        \n    def score(self, testX, testY):\n        target = testY.columns[0]\n        result = self.predict(testX)\n        return accuracy_score(testY[target], result)\n\n\n    def count_leaves(self):\n        return self.count_leaves_helper(self.root_node)\n    \n    def count_leaves_helper(self, tree):\n        if tree.is_leaf:\n            return 1\n        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"01fba0bf66e010f77e6c18ab3de70bd28f93fbca"},"cell_type":"code","source":"m = MyDecisionTree(max_depth = 10, min_error = 1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1d00c6dc0734afc1328bbd342158b325160cb658"},"cell_type":"code","source":"m.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1d23711c09950bd46757a5c6cd756b8d1e82ba96"},"cell_type":"code","source":"m.score(testX, testY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90ded8ab5547b11fbb69d925f102101dbd713a59"},"cell_type":"markdown","source":"决策树复杂度的探讨"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9a1b8ad7567d021d10ad3c604ac1e9ea59fab2e8"},"cell_type":"code","source":"m.count_leaves()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b01dc21326b252c7472a7cce353883a237b04401"},"cell_type":"markdown","source":"探索不同树深度对决策树的影响\n4 max_depth = 3\n5 max_depth = 7\n6 max_depth = 15"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8fbc9badc70d54b6dbacb42877ed06962a28bdab"},"cell_type":"code","source":"model_4 = MyDecisionTree(max_depth=3, min_error = 1e-15)\nmodel_5 = MyDecisionTree(max_depth=7, min_error = 1e-15)\nmodel_6 = MyDecisionTree(max_depth=15, min_error = 1e-15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bce1ba2cb1093a54d4d5e4b25fad65ebd3088bd9"},"cell_type":"code","source":"model_4.fit(trainX, trainY)\nmodel_5.fit(trainX, trainY)\nmodel_6.fit(trainX, trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e73e2e636af2e7f2ca96d40a9c7ae736dfc977f3"},"cell_type":"code","source":"print(\"model_4 training accuracy :\", model_4.score(trainX, trainY))\nprint(\"model_5 training accuracy :\", model_5.score(trainX, trainY))\nprint(\"model_6 training accuracy :\", model_6.score(trainX, trainY))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5ad1889300dfeb52cadece7ec7a3ccdc51a61dbe"},"cell_type":"code","source":"print(\"model_4 testing accuracy :\", model_4.score(testX, testY))\nprint(\"model_5 testing accuracy :\", model_5.score(testX, testY))\nprint(\"model_6 testing accuracy :\", model_6.score(testX, testY))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b6e3634f9a3396d4bc5655715e0ad868562bda3a"},"cell_type":"code","source":"print(\"model_4 complexity is: \", model_4.count_leaves())\nprint(\"model_5 complexity is: \", model_5.count_leaves())\nprint(\"model_6 complexity is: \", model_6.count_leaves())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3632f20f7684e7ddbcb0fbea4bd5c41f63594be"},"cell_type":"markdown","source":"结论：model_6---一直表现不错"}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}