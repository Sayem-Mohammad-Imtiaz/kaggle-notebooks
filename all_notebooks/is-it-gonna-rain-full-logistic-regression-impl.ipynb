{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernal I will solve the \"*Will it rain tomorrow*\" using Logistic Regression.\n\nI will use my own LR implementation vs Scikit learn one\n\n**FOR ANY QUESTIONS OR NEEDED EXPLANATIONS JUST COMMENT AND I WILL REPLY ASAP**\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Import all needed libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the csv features file"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"view data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"view data information"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove the RISK_MM as mentioned on data scource"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['RISK_MM'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Date represent sequence data and as LR is not that good in capturing sequence model I will remove the date info"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"show all categorical data that need to be handeled"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint(\"The categorical features are : \",categorical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"use get dummies function to fo the hot encoding to convert categorical data to numirical one in a way that is not give extra weight for any above others "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat(      [df,\n                     pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True),\n                     pd.get_dummies(df.Location), \n                     pd.get_dummies(df.WindGustDir),\n                     pd.get_dummies(df.WindDir9am),\n                     pd.get_dummies(df.WindDir3pm)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"show data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove the old categorical data columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Location'], axis=1, inplace=True)\ndf.drop(['WindGustDir'], axis=1, inplace=True)\ndf.drop(['WindDir9am'], axis=1, inplace=True)\ndf.drop(['WindDir3pm'], axis=1, inplace=True)\ndf.drop(['RainToday'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"show data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"use RainTomorrow as target (labels) data and remove it from data frame\nconvert the Yes/No results to 0/1"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"RainTomorrow\"]\ndf.drop(['RainTomorrow'], axis=1, inplace=True)\n\ny=pd.Series(map(lambda x: dict(Yes=1, No=0)[x],\n              y.values.tolist()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"show the target data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fill NaN with the mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"split data into training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set_x,  test_set_x, train_set_y,test_set_y = train_test_split(df, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scalling the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(train_set_x)  \ntrain_set_x = scaler.transform(train_set_x)  \ntest_set_x = scaler.transform(test_set_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"view data shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_set_x\",train_set_x.shape)\nprint(\"train_set_y\",train_set_y.shape)\nprint(\"test_set_x\",test_set_x.shape)\nprint(\"test_set_y\",test_set_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"reshape the labels to elemenate rank one arraies"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_y=train_set_y.values.reshape(113754,1)\ntest_set_y=test_set_y.values.reshape(28439,1)\nprint(\"train_set_x\",train_set_x.shape)\nprint(\"train_set_y\",train_set_y.shape)\nprint(\"test_set_x\",test_set_x.shape)\nprint(\"test_set_y\",test_set_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**My own LR implementation start here:**\n\n"},{"metadata":{},"cell_type":"markdown","source":"define the sigmoid function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    s = 1/(1+np.exp(-z))\n    return s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialize weights and bias with zeros"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize(dim):\n    w = np.zeros((dim,1))\n    b = 0  \n    return w, b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"start the propagate, forward and backward to compute the activations/Cost and gradients respictivly"},{"metadata":{"trusted":true},"cell_type":"code","source":"def propagate(w, b, X, Y):\n    m = X.shape[1]\n    # FORWARD \n    A = sigmoid(np.dot(w.T,X)+b)   \n    cost = (-1/m)*np.sum(np.multiply(Y,np.log(A))+np.multiply((1-Y),np.log(1-A)))                               \n    # BACKWARD \n    dz = A-Y\n    dw = (1/m)*np.dot(X,dz.T)\n    db = (1/m)*np.sum(dz)\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n\n    return grads, cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"optimizing the weights and bias using the gradieent at each iteration"},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize(w, b, X, Y,print_cost, num_iterations, learning_rate):\n    costs = []\n    for i in range(num_iterations):\n        grads, cost = propagate(w, b, X, Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        w = w-learning_rate*dw\n        b = b-learning_rate*db\n\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"use the final weights and bias to predict the results for new unseen testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid(np.dot(w.T,X)+b)\n\n    for i in range(A.shape[1]):\n        if A[0,i]>0.5:\n            Y_prediction[0,i]=1\n        else:\n            Y_prediction[0,i]=0    \n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the main model Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X_train, Y_train, X_test, Y_test,print_cost, num_iterations = 2000, learning_rate = 0.5 ):\n    w, b = initialize(X_train.shape[0])\n\n    parameters, grads, costs = optimize(w, b, X_train, Y_train,print_cost, num_iterations, learning_rate)\n\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calling the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = model(train_set_x.T, train_set_y.T, test_set_x.T, test_set_y.T,print_cost = True, num_iterations = 3000, learning_rate = 0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the results with multible Learning Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rates = [0.1, 0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (\"learning rate is: \" + str(i))\n    models[str(i)] = model(train_set_x.T, train_set_y.T, test_set_x.T, test_set_y.T,print_cost = False, num_iterations = 1500, learning_rate = i)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scikit learn LR implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model #import the model library\nlogreg = linear_model.LogisticRegression(random_state = 0,max_iter= 3000,solver='liblinear') # sitting model parameters\nfitting = logreg.fit(train_set_x, train_set_y)\nprint(\"test accuracy: {} \".format(fitting.score(test_set_x, test_set_y))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(fitting.score(train_set_x, train_set_y))) # printing the results of fitting the model over the training set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our results and scikit learn results are almost the same\n"},{"metadata":{},"cell_type":"markdown","source":"big thanks for coursera and deeplearning.ai for the knowledge "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}