{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://keras.io/img/logo.png)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation and processing","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from random import randint\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = []\ntrain_samples = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As motivation for this data, let’s suppose that an experimental drug was tested on individuals ranging from age 13 to 100 in a clinical trial. The trial had 2100 participants. Half of the participants were under 65 years old, and the other half was 65 years of age or older.\n\nThe trial showed that around 95% of patients 65 or older experienced side effects from the drug, and around 95% of patients under 65 experienced no side effects, generally showing that elderly individuals were more likely to experience side effects.\n\nUltimately, we want to build a model to tell us whether or not a patient will experience side effects solely based on the patient's age. The judgement of the model will be based on the training data.\n\nNote that with the simplicity of the data along with the conclusions drawn from it, a neural network may be overkill, but understand this is just to first get introduced to working with data for deep learning, and later, we'll be making use of more advanced data sets.\n\nThe block of code below shows how to generate this dummy data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    # The ~5% of younger individuals who did experience side effects\n    random_younger = randint(13,64)\n    train_samples.append(random_younger)\n    train_labels.append(1)\n#     print(train_samples)\n\n    # The ~5% of older individuals who did not experience side effects\n    random_older = randint(65,100)\n    train_samples.append(random_older)\n    train_labels.append(0)\n\nfor i in range(1000):\n    # The ~95% of younger individuals who did not experience side effects\n    random_younger = randint(13,64)\n    train_samples.append(random_younger)\n    train_labels.append(0)\n\n    # The ~95% of older individuals who did experience side effects\n    random_older = randint(65,100)\n    train_samples.append(random_older)\n    train_labels.append(1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#creating dataframe and saving the data\ndf = pd.DataFrame(list(zip(train_samples, train_labels)), \n               columns =['Sample', 'label']) \ndf ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df.to_csv (r'dataframe.csv', index = False, header=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing\n\nWe now convert both lists into numpy arrays due to what we discussed the fit() function expects, and we then shuffle the arrays to remove any order that was imposed on the data during the creation process.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = np.array(train_labels)\ntrain_samples = np.array(train_samples)\ntrain_labels, train_samples = shuffle(train_labels, train_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this form, we now have the ability to pass the data to the model because it is now in the required format, however, before doing that, we'll first scale the data down to a range from 0 to 1.\n\nWe'll use scikit-learn’s MinMaxScaler class to scale all of the data down from a scale ranging from 13 to 100 to be on a scale from 0 to 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))\nprint(df.head())\nprint(scaled_train_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We reshape the data as a technical requirement just since the fit_transform() function doesn’t accept 1D data by default.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"type(scaled_train_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_train_samples.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create An Artificial Neural Network With TensorFlow's Keras API","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers import Adadelta\nfrom tensorflow.keras.metrics import categorical_crossentropy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  <font color='blue'>Build a Sequential Model</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# or you can do like this -->  model.add(l4)\n# so i prefer to do it like this --> model = sequential([l1,l2,l3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Dense(units=16, input_shape=(1,), activation='relu'),\n    Dense(units=32, activation='relu'),\n    #Dense(units=2, activation='sigmoid')\n    Dense(units=2, activation='softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"model is an instance of a Sequential object. A tf.keras.Sequential model is a linear stack of layers. It accepts a list, and each element in the list should be a layer.\n\nAs you can see, we have passed a list of layers to the Sequential constructor. Let's go through each of the layers in this list now.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">  Note, if you don’t explicitly set an activation function, then Keras will use the linear activation function.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*First Hidden Layer*\nOur first layer is a Dense layer. This type of layer is our standard fully-connected or densely-connected neural network layer. The first required parameter that the Dense layer expects is the number of neurons or units the layer has, and we’re arbitrarily setting this to 16.\n\nAdditionally, the model needs to know the shape of the input data. For this reason, we specify the shape of the input data in the first hidden layer in the model (and only this layer). The parameter called input_shape is how we specify this.\n\nAs discussed, we’ll be training our network on the data that we generated and processed in the previous episode, and recall, this data is one-dimensional. The input_shape parameter expects a tuple of integers that matches the shape of the input data, so we correspondingly specify (1,) as the input_shape of our one-dimensional data.\n\nYou can think of the way we specify the input_shape here as acting as an implicit input layer. The input layer of a neural network is the underlying raw data itself, therefore we don't create an explicit input layer. This first Dense layer that we're working with now is actually the first hidden layer.\n\nLastly, an optional parameter that we’ll set for the Dense layer is the activation function to use after this layer. We’ll use the popular choice of relu.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"2nd  hidden layer also the same as the above menioned\n*Output Layer*\nLastly, we specify the output layer. This layer is also a Dense layer, and it will have 2 neurons. This is because we have two possible outputs: either a patient experienced side effects, or the patient did not experience side effects.\n\nThis time, the activation function we’ll use is softmax, which will give us a probability distribution among the possible outputs.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Note that we can call summary() on our model to get a quick visualization of it.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Train An Artificial Neural Network With TensorFlow's Keras API","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font color='blue'>Compiling The Model</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adadelta(learning_rate=0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function configures the model for training and expects a number of parameters. First, we specify the optimizer Adam. Adam accepts an optional parameter learning_rate, which we’ll set to 0.0001.\n\nThe next parameter we specify is loss. We’ll be using sparse_categorical_crossentropy, given that our labels are in integer format.\n\nNote that when we have only two classes, we could instead configure our output layer to have only one output, rather than two, and use binary_crossentropy as our loss, rather than categorical_crossentropy. Both options work equally well and achieve the exact same result.\n\nWith binary_crossentropy, however, the last layer would need to use sigmoid, rather than softmax, as its activation function.\n\nMoving on, the last parameter we specify in compile() is metrics. This parameter expects a list of metrics that we’d like to be evaluated by the model during training and testing. We’ll set this to a list that contains the string ‘accuracy’.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we specify verbose=2. This just specifies how much output to the console we want to see during each epoch of training. The verbosity levels range from 0 to 2, so we’re getting the most verbose output.\n\nWhen we call fit() on the model, the model trains, and we get this output.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font color='orange'>What Is A Validation Set?</font>\n\nRecall that we previously built a training set on which we trained our model. With each epoch that our model is trained, the model will continue to learn the features and characteristics of the data in this training set.\n\nThe hope is that later we can take this model, apply it to new data, and have the model accurately predict on data that it hasn’t seen before based solely on what it learned from the training set.\n\nNow, let’s discuss where the addition of a validation set comes into play.\n\nBefore training begins, we can choose to remove a portion of the training set and place it in a validation set. Then, during training, the model will train only on the training set, and it will validate by evaluating the data in the validation set.\n\nEssentially, the model is learning the features of the data in the training set, taking what it's learned from this data, and then predicting on the validation set. During each epoch, we will see not only the loss and accuracy results for the training set, but also for the validation set.\n\nThis allows us to see how well the model is generalizing on data it wasn’t trained on because, recall, the validation data should not be part of the training data.\n\nThis also helps us see whether or not the model is overfitting. Overfitting occurs when the model only learns the specifics of the training data and is unable to generalize well on data that it wasn’t trained on.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font color='blue'>Creating A Validation Set</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are two ways to create a validation set to use with a tf.keras.Sequential model. The first way is to create a data structure to hold a validation set, and place data directly in that structure in the same nature we did for the training set.\n\nThis data structure should be a tuple valid_set = (x_val, y_val) of Numpy arrays or tensors, where x_val is a numpy array or tensor containing validation samples, and y_val is a numpy array or tensor containing validation labels.\n\nWhen we call model.fit(), we would pass in the validation set in addition to the training set. We pass the validation set by specifying the validation_data parameter.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"model.fit(\n      x=scaled_train_samples\n    , y=train_labels\n    , validation_data=valid_set\n    , batch_size=10\n    , epochs=30\n    , verbose=2\n)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"When the model trains, it would continue to train only on the training set, but additionally, it would also be evaluating the validation set.\n\nThere is another way to create a validation set, and it saves a step!\n\nIf we don’t already have a specified validation set created, then when we call model.fit(), we can set a value for the validation_split parameter. It expects a fractional number between 0 and 1. Suppose that we set this parameter to 0.1.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With this parameter specified, Keras will split apart a fraction (10% in this example) of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n\nNote that the fit() function shuffles the data before each epoch by default. When specifying the validation_split parameter, however, the validation data is selected from the last samples in the x and y data before shuffling.\n\nTherefore, in the case we're using validation_split in this way to create our validation data, we need to be sure that our data has been shuffled ahead of time\n\nNow, regardless of which method we use to create validation data, when we call model.fit(), then in addition to loss and accuracy being displayed for each epoch as we saw last time, we will now also see val_loss and val_acc to track the loss and accuracy on the validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=scaled_train_samples, y=train_labels,validation_split = 0.1, batch_size=10, epochs=30, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network Predictions With TensorFlow's Keras API","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font color='blue'>Creating A Test Set</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We’ll create a test set in the same fashion for which we created the training set. In general, the test set should always be processed in the same way as the training set.\n\nWe won’t go step-by-step over the code that generates and processes the test data below, as it has already been covered in detail where we generated the training data,  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels =  []\ntest_samples = []\n\nfor i in range(10):\n    # The 5% of younger individuals who did experience side effects\n    random_younger = randint(13,64)\n    test_samples.append(random_younger)\n    test_labels.append(1)\n    \n    # The 5% of older individuals who did not experience side effects\n    random_older = randint(65,100)\n    test_samples.append(random_older)\n    test_labels.append(0)\n\nfor i in range(200):\n    # The 95% of younger individuals who did not experience side effects\n    random_younger = randint(13,64)\n    test_samples.append(random_younger)\n    test_labels.append(0)\n    \n    # The 95% of older individuals who did experience side effects\n    random_older = randint(65,100)\n    test_samples.append(random_older)\n    test_labels.append(1)\n\ntest_labels = np.array(test_labels)\ntest_samples = np.array(test_samples)\n\ntest_labels, test_samples = shuffle(test_labels, test_samples)\n\nscaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))\nprint(scaled_test_samples[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='blue'>Evaluating The Test Set</font>\n\nTo get predictions from the model for the test set, we call model.predict().\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(\n      x=scaled_test_samples\n    , batch_size=10\n    , verbose=0\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To this function, we pass in the test samples x, specify a batch_size, and specify which level of verbosity we want from log messages during prediction generation. The output from the predictions won't be relevant for us, so we're setting verbose=0 for no output.\n\nNote that, unlike with training and validation sets, we do not pass the labels of the test set to the model during the inference stage.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"type(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see what the model's predictions look like, we can iterate over them and print them out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in predictions[:10]:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each element in the predictions list is itself a list of length 2. The sum of the two values in each list is 1. The reason for this is because the two columns contain probabilities for each possible output: experienced side effects and did not experience side effects. Each element in the predictions list is a probability distribution over all possible outputs.\n\nThe first column contains the probability for each patient not experiencing side effects, which is represented by a 0. The second column contains the probability for each patient experiencing side effects, which is represented by a 1.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can also look only at the most probable prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rounded_predictions = np.argmax(predictions, axis=-1)\nrounded_predictions[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the printed prediction results, we can observe the underlying predictions from the model, however, we cannot judge how accurate these predictions are just by looking at the predicted output.\n\nIf we have corresponding labels for the test set, (for which, in this case, we do), then we can compare these true labels to the predicted labels to judge the accuracy of the model's evaluations. We'll see how to visualize this using a tool called a confusion matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Create A Confusion Matrix For Neural Network Predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We’ll now demonstrate how to create a confusion matrix, which will aid us in being able to visually observe how well a neural network is predicting during inference.\n\nWe’ll continue working with the predictions we obtained from the tf.keras.Sequential model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As we showed how to use a trained model for inference on new data in a test set it hasn’t seen before. Also mentioned before, we had the labels for the test set, but we didn’t provide these labels to the network.\n\nAdditionally, we were able to see the values that the model was predicting for each of the samples in the test set by just observing the predictions themselves.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With a confusion matrix, we’ll be able to visually observe how well the model predicts on test data.\n\nLet’s jump into the code for how this is done.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Plotting A Confusion Matrix\n\nFirst, we import all the required libraries we’ll be working with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix we’ll be plotting comes from scikit-learn.\n\nWe then create the confusion matrix and assign it to the variable cm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To the confusion matrix, we pass in the true labels test_labels as well as the network’s predicted labels rounded_predictions for the test set.\n\nBelow, we have a function called plot_confusion_matrix() that came directly from scikit-learn’s website. This is code that they provide in order to plot the confusion matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we define the labels for the confusion matrix. In our case, the labels are titled “no side effects” and “had side effects.”","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_plot_labels = ['no_side_effects','had_side_effects']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, we plot the confusion matrix by using the plot_confusion_matrix() function we just discussed. To this function, we pass in the confusion matrix cm and the labels cm_plot_labels, as well as a title for the confusion matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the plot of the confusion matrix, we have the predicted labels on the x-axis and the true labels on the y-axis. The blue cells running from the top left to bottom right contain the number of samples that the model accurately predicted. The white cells contain the number of samples that were incorrectly predicted.\n\nThere are 420 total samples in the test set. Looking at the confusion matrix, we can see that the model accurately predicted 393 out of 420 total samples. The model incorrectly predicted 29 out of the 420.\n\nFor the samples the model got correct, we can see that it accurately predicted that the patients would experience no side effects 193 times. It incorrectly predicted that the patient would have no side effects 10 times when the patient did actually experience side effects.\n\nOn the other side, the model accurately predicted that the patient would experience side effects 200 times that the patient did indeed experience side effects. It incorrectly predicted that the patient would have side effects 17 times when the patient actually did not experience side effects.\n\nAs you can see, this is a good way we can visually interpret how well the model is doing at its predictions and understand where it may need some work.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Save And Load A Keras Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we’ll demonstrate how to save and load a tf.keras.Sequential neural network.\n\nThere are a few different ways to save a Keras model. The multiple mechanisms each save the model differently, so we'll check them all out.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font color='blue'>**Saving And Loading The Model In Its Entirety**</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If we want to save a model at its current state after it was trained so that we could make use of it later, we can call the save() function on the model. To save(), we pass in the file path and name of the file we want to save the model to with an h5 extension.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('medical_trial_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note, this function also allows for saving the model as a Tensorflow SavedModel as well if you'd prefer.\n\nThis method of saving will save everything about the model – the architecture, the weights, the optimizer, the state of the optimizer, the learning rate, the loss, etc.\n\nNow that we have this model saved, we can load the model at a later time.\n\nTo do so, we first import the load_model() function. Then, we can call the function to load the model by pointing to the saved model on disk.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\nnew_model = load_model('medical_trial_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can verify that the loaded model has the same architecture and weights as the saved model by calling summary() and get_weights() on the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also inspect attributes about the model, like the optimizer and loss by calling model.optimizer and model.loss on the loaded model and compare the results to the previously saved model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.optimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the most encompassing way to save and load a model.\n\n##### <font color='blue'>Saving And Loading Only The Architecture Of The Model</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There is another way we save only the architecture of the model. This will not save the model weights, configurations, optimizer, loss or anything else. This only saves the architecture of the model.\n\nWe can do this by calling model.to_json(). This will save the architecture of the model as a JSON string. If we print out the string, we can see exactly what this looks like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"json_string = model.to_json()\njson_string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have this saved, we can create a new model from it. First we’ll import the needed model_from_json function, and then we can load the model architecture.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import model_from_json\nmodel_architecture = model_from_json(json_string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By printing the summary of the model, we can verify that the new model has the same architecture of the model that was previously saved.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_architecture.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note, we can also use this same approach to saving and loading the model architecture to and from a YAML string. To do so, we use the functions to_yaml() and model_from_yaml() in the same fashion as we called the json functions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### <font color='blue'>Saving And Loading The Weights Of The Model</font>\nThe last saving mechanism we’ll discuss only saves the weights of the model.\n\nWe can do this by calling model.save_weights() and passing in the path and file name to save the weights to with an h5 extension.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('my_model_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At a later point, we could then load the saved weights in to a new model, but the new model will need to have the same architecture as the old model before the weights can be saved.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential([\n    Dense(units=16, input_shape=(1,), activation='relu'),\n    Dense(units=32, activation='relu'),\n    Dense(units=2, activation='softmax')\n])\n\nmodel2.load_weights('my_model_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’ve now seen how to save only the weights of a model and deploy those weights to a new model, how to save only the architecture and then deploy that architecture to a model, and how to save everything about a model and deploy it in its entirety at a later time. Each of these saving and loading mechanisms may come in useful in differing scenarios.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"these all tutorials are inspired from this [channel](https://www.youtube.com/watch?v=LhEMXbjGV_4&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=11)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Link to the 2nd [notebook](https://www.kaggle.com/bavalpreet26/cats-vs-dogs-basic-cnn-tutorial-keras-nb2)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}