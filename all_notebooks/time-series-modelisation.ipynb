{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"### Import libraries and data\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n%matplotlib inline\nplt.style.use('Solarize_Light2')\n\nraw_data = pd.read_csv('../input/air-passengers/AirPassengers.csv', header=0, index_col=0, names=['data'], parse_dates=True)\nraw_data.rename_axis(\"time\", inplace=True)\n\n# Add frequency on datetime index\nfreq = pd.infer_freq(raw_data.index)\nraw_data = raw_data.asfreq(freq)\n\ntrain = raw_data.iloc[:-12, :].copy(deep=True)\ndf = raw_data.copy(deep=True)\n\ntrain.plot(figsize=(12,3));\nplt.title(\"Airline passengers over time\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"### Helpers\nfrom calendar import monthrange\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom scipy.stats import shapiro\n\ndef average_per_day(df):\n    \"\"\" Average the monthly quantity per day. \"\"\"\n    for i, row in df.itertuples():\n        n_day_month = monthrange(i.year, i.month)[1]\n        df.loc[i, 'data'] = row / n_day_month\n\n        \ndef plot(df, col_name='data', title=None):\n    \"\"\" Plot the line and its moving average smoothing and std. \"\"\"\n    fig, ax = plt.subplots(1,1, figsize=(12,6))\n    data = ax.plot(df.index, df[col_name], label='raw data')\n    mean = ax.plot(df[col_name].rolling(window=12).mean(), label=\"rolling mean\")\n    ax.set_xlabel(\"Time by years\")\n    ax.set_ylabel(\"Data and trend\")\n    \n    ax_var = ax.twinx()\n    var = ax_var.plot(df[col_name].rolling(window=12).std(), label=\"rolling std\", color=\"#f088b3\")\n    ax_var.grid(False)\n    ax_var.set_ylabel(\"Std\")\n    \n    lines = data + mean + var\n    legend = [l.get_label() for l in lines]\n    ax.legend(lines, legend)\n    \n    plt.title(title)\n    plt.tight_layout()\n    fig.autofmt_xdate()\n\n    \ndef plot_ACF(df, col_name='data', n_lags=20):\n    \"\"\" Plot ACF/PACF graphs. \"\"\"\n    fig, ax = plt.subplots(2, figsize=(12,6))\n    ax[0] = plot_acf(df[col_name].dropna(), ax=ax[0], lags=n_lags)\n    ax[1] = plot_pacf(df[col_name].dropna(), ax=ax[1], lags=n_lags)\n    \n\ndef differentiate(df, k=1, col_data='data'):\n    \"Differentiate the time series by substracting values at instant t-1...t-k\"\n    values = [np.NaN for i in range(k)]\n    for t in range(k, len(df)):\n        val = df[col_data][t] - df[col_data][t-k]\n        values.append(val)\n\n    return pd.DataFrame(values, index=df.index, columns=[col_data])\n\n    \ndef test_ADF(df, col_name='data'):\n    \"\"\" Test unilatéral droit : Plus la stat de test est négative, plus H0 est rejeté. \"\"\"\n    print(\"> ADF test : H0 = there is a unit root. H1 = The process has a root outside the unit circle (=stationarity)\")\n    message = ['[H0 accepted] There is a unit root', '[H0 rejected] The process is stationary']\n    dftest = adfuller(df[col_name].dropna(), autolag='AIC')\n    print(\"Test statistic = {:.3f}\".format(dftest[0]))\n    print(\"P-value = {:.3f}\".format(dftest[1]))\n    print(\"Critical values :\")\n    for k, v in dftest[4].items():\n        result_test = 1 - int(dftest[0] > v)  # 0 if H0 accepted\n        print(\"\\t{}: {} - {} with {}% confidence\".format(k, v, message[result_test], 100-int(k[:-1])))\n\n\ndef test_KPSS(df, col_name='data', has_trend=False):\n    print(\"> KPSS test : H0 = the data is stationary. H1 = There is a unit root.\")\n    message = ['[H0 accepted ] The data is stationary', '[H0 rejected] There is a unit root']\n    regression = 'ct' if has_trend else 'c'\n    dftest = kpss(df[col_name].dropna(), regression=regression)\n    print(\"Test statistic = {:.3f}\".format(dftest[0]))\n    print(\"P-value = {:.3f}\".format(dftest[1]))\n    print(\"Critical values :\")\n    for k, v in dftest[3].items():\n        result_test = 1 - int(dftest[0] < v)  # 0 if H0 accepted\n        print(\"\\t{}: {} - {} with {}% confidence\".format(k, v, message[result_test], 100-float(k[:-1])))\n        \n\ndef test_LjungBox(res, lags=None, has_seasonality=False, seasonal_period=0, alpha=0.05):\n    print(\"\\n> Ljung-Box test : H0 : no autocorrelation between lags 1 to r(=independent distrib). H1 : autocorrelation between lags 1 to r.\")\n    # Compute the lags\n    if has_seasonality is False and lags is None:\n        lags = min(10, int(len(res)/5))\n    elif lags is None:\n        lags = min((len(res) // 2 - 2), 40)  # Default value according to the doc.\n    \n    message = [\"[H0 accepted] No autocorrelation between lags 1 to %s\" % lags, \"[H0 rejected] Autocorrelation between lags 1 to %s\" % lags]\n    \n    ljbtest = acorr_ljungbox(res, lags=lags)\n    p_val = ljbtest[1][len(ljbtest[1]) - 1]\n    \n    print(\"P-value = {:.3f}\".format(p_val))\n    print(message[alpha > p_val])\n    \n    \ndef test_shapiro(sample, alpha=0.05):\n    print(\"\\n> Shapiro-Wilk test : H0 = the data is drawn from normal distribution.\")\n    message = ['[H0 accepted] The data is drawn from normal distribution', '[H0 rejected] The data isn\\'t from normal distribution.']\n    p_val = shapiro(sample)[1]\n    print(\"P-value : {:.3f}\".format(p_val))\n    print(message[alpha > p_val])\n        \ndef rmse(x1, x2):\n    \" Compute the RMSE given two numpy array or pandas Series. \"\n    assert len(x1) == len(x2), 'The two samples must be of same size'\n    # If the sample given is a pd.Series, get the array of values instead\n    if isinstance(x1, pd.Series):\n        x1 = x1.values\n    if isinstance(x2, pd.Series):\n        x2 = x2.values\n        \n    T = len(x1)\n    rmse = math.sqrt(np.sum((x1 - x2)**2)/ T)\n    return rmse\n\ndef mape(forecast, data):\n    \" Compute the MAPE = 1/T * SUM(|data[i] - forecast[i]| / data[i]). Metric returned in percentage. \"\n    assert len(forecast) == len(data), 'The two samples must be of same size'\n    if isinstance(forecast, pd.Series):\n        forecast = forecast.values\n    if isinstance(data, pd.Series):\n        data = data.values\n        \n    T = len(forecast)\n    mape = 100 * np.sum(abs(forecast - data) / data) / T\n    return round(mape, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the rolling mean and standard deviation\nplot(train, title=\"Airline passengers over time\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The TS is made of a trend and a seasonality pattern.\n- We notice a linear trend growing with the years.\n- A seasonality pattern which repeats itself every year. The pattern is larger with the time as the level of the TS rises.\n\nFirst, we'll fit the data with an exponential smoothing method.\nAs we have a trend and a seasonality pattern, we choose the Holt-Winter model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Holt Winter exponential smoothing\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# We'll chose an additive trend and multiplicative seasonality because the size of the latter pattern depends on the level of the TS.\nhw_model = ExponentialSmoothing(train, trend='add', seasonal='mul', seasonal_periods=12).fit()\nfitted_val = hw_model.fittedvalues\nprediction = hw_model.predict(df.index[len(train)], df.index[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.gofplots import qqplot\nfrom scipy.stats import norm\nfrom statsmodels.nonparametric.kde import KDEUnivariate\n\n\n# Plot the data along with the fitted values and the predictions of the model\nfig, ax = plt.subplots(1,1, figsize=(12,6))\nax.plot(df.index, df['data'], label='Data')\nax.plot(df.index[:len(train)-1], fitted_val[:len(train)-1], label=\"Model on train data (AICC={:.3f})\".format(hw_model.aicc), color=\"#f088b3\")\nax.plot(df.index[len(train):], prediction, label=\"Forecast\")\nax.set_xlabel(\"Time by years\")\nax.set_ylabel(\"Monthly number of passengers\")\nax.legend()\nplt.title(\"Holt Winter exponential smoothing model\")\nplt.tight_layout()\nfig.autofmt_xdate()\n\n# Evaluate the model with RMSE\ntrain_RMSE_hw = rmse(fitted_val[:len(train)-1], df['data'][:len(train)-1])\ntrain_MAPE_hw = mape(fitted_val[:len(train)-1], df['data'][:len(train)-1])\ntest_RMSE_hw = rmse(prediction, df['data'][len(train):])\ntest_MAPE_hw = mape(prediction, df['data'][len(train):])\nprint(\"\\nTrain RMSE : {:.3f}\\nTest RMSE : {:.3f}\\nTrain MAPE : {:.3f} %\\nTest MAPE : {:.3f} %\".format(train_RMSE_hw, test_RMSE_hw, train_MAPE_hw, test_MAPE_hw))\n\n# Analyse residuals : plot the standardized residuals (divided by its std), a correlogram, a QQ-plot and an histogram\nresiduals = (df['data'].values - pd.concat([fitted_val, prediction]))\nstandardized_residuals = residuals / residuals.std()\n\nfig, ax = plt.subplots(2,2, figsize=(12,12))\n# Plot of the standardized residuals\nax[0, 0].plot(df.index, standardized_residuals)\nax[0, 0].axhline(standardized_residuals.mean(),color='r',ls='--', label='Mean')\nax[0, 0].set_title(\"Standardized Residuals\")\nax[0, 0].legend()\n\n# ACF of the residuals\nax[0, 1] = plot_acf(standardized_residuals, ax=ax[0, 1], lags=20)\n\n#QQplot\nax[1, 0] = qqplot(standardized_residuals, ax=ax[1, 0], line='45')\n\n# Histogram and kernel density estimation\nax[1, 1].hist(standardized_residuals, density=True, bins=20)\nnormal_distrib = norm.pdf(np.linspace(-3,4), 0, 1)\nax[1,1].plot(np.linspace(-3,4), normal_distrib, label=\"N(0,1)\", color='r')\n\nkde = KDEUnivariate(standardized_residuals)\nkde.fit(clip=(-3, 4))\nax[1,1].plot(kde.support, kde.density, label=\"KDE\", color='g')\nax[1,1].legend(loc=\"upper right\")\nax[1, 1].set_title(\"Histogram of standardized residuals\")\n\n# Test if there is correlation between the residuals\ntest_LjungBox(residuals, lags=20, has_seasonality=True, seasonal_period=12)\n\n# Test if the residuals is drawn from a normal distribution\ntest_shapiro(residuals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model has good result (MAPE of 2.3 % on test set) : it fits well the data and imitate the seasonality pattern of the TS. However, the residuals are correlated and aren't from a normal distribution. It can be improved.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ARIMA\nFirst, we'll make the data stationary (not dependant of the time). Then we will fit a SARIMA model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log transformation to have same magnitude of variance over time\nlog_train = train.data.apply(lambda x: math.log(x)).to_frame()\n\nplot(log_train, title=\"Log of airline passengers over time\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the trend and seasonality pattern by differing\nstationary_data = pd.DataFrame(differentiate(log_train.copy(deep=True), 12), columns=['data'])\nstationary_data = differentiate(stationary_data, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the stationarity\nplot(stationary_data, title='Stationary data')\nplot_ACF(stationary_data, n_lags=36)\ntest_ADF(stationary_data)\ntest_KPSS(stationary_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The differenced data is stationary. We can now fit the TS with a SARIMA model of parameters (p,d,q)x(P,D,Q). We already know that d = D = 1 because of the differencing we've done.\n\nBy the look of the ACF and PACF plots, we can suggest values of other parameters :\n- the spike at lag 1 is negative and corresponds to an overdifferencing. We can balance by adding MA(1). Thus, q=1.\n- it's the same thinking for the spike at lag 12 which is the first seasonal spike. Thereby, Q=1.\n\nWe'll train a ARIMA model with the parameters (0,1,1)x(0,1,1).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Search grid to choose the best SARIMA parameters using the software X-13ARIMA-SEATS made by the Census Bureau\n# It is needed to install the software (winx13 or winx12) at https://www.census.gov/srd/www/winx13/winx13_down.html\n\nfrom statsmodels.tsa.x13 import x13_arima_select_order\n\npath_to_folder = 'your_path_to_the_downloaded_folder'\npath_winx13 = os.path.join(path_to_folder, 'x13as', 'x13as')\n\n# I don't know how to import winx13 on Kaggle kernel so it's not working.\n\n#x13_order = x13_arima_select_order(log_train, x12path=path_winx13)\n\n#print(\"ARIMA parameter order : %s\\nSeasonal ARIMA parameter order : %s\" % (x13_order.order, x13_order.sorder))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Explanation below for the substraction of the first value to the data\nfirst_val = log_train['data'][0]\n\nsarima_model = SARIMAX(log_train['data'] - first_val, order=(0, 1, 1), seasonal_order=(0, 1, 1, 12))\nsarima_fit = sarima_model.fit()\n\nprint(sarima_fit.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explanation of the substraction\nThe TS isn't centered : the first value of the logged data is around 5. However, when I fit the model, the first fitted value is 0 and the second around 5. So, there is a big increase which is repeated one season after causing a big spike.\n\nTo resolve this issue, I decided before fitting to substract the first value to the data.\nIf you have other and better way, don't hesitate to hit me up !\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = df[-12:]\n\nsarima_pred = sarima_fit.get_prediction('1960-01', '1960-12')\npredicted_means = sarima_pred.predicted_mean.apply(lambda x: math.exp(x + first_val))\npredicted_intervals = sarima_pred.conf_int(alpha=0.05)\nlower_bounds = predicted_intervals['lower data'].apply(lambda x: math.exp(x + first_val))\nupper_bounds = predicted_intervals['upper data'].apply(lambda x: math.exp(x + first_val))\n\ntrain_RMSE_sarima = rmse(sarima_fit.fittedvalues.apply(lambda x: math.exp(x + first_val))[1:], raw_data['data'][1:-12])\ntest_RMSE_sarima = rmse(test['data'], predicted_means)\ntrain_MAPE_sarima = mape(sarima_fit.fittedvalues.apply(lambda x: math.exp(x + first_val))[1:], raw_data['data'][1:-12])\ntest_MAPE_sarima = mape(test['data'], predicted_means)\n\nfig, ax = plt.subplots(figsize=(12, 4))\nax.plot(df.data.index[:-12], df.data.values[:-12]);\nax.plot(test.index, test.values, label='truth');\nax.plot(test.index, predicted_means, color='#ff7823', linestyle='--', label=\"prediction (RMSE={:0.2f})\".format(test_RMSE_sarima));\nax.plot(df.data.index[:-12], sarima_fit.fittedvalues.apply(lambda x: math.exp(x + first_val)), label=\"SARIMA model\")\nax.fill_between(test.index, lower_bounds, upper_bounds, color='#ff7823', alpha=0.3, label=\"confidence interval (95%)\");\nax.legend();\nax.set_title(\"SARIMA\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Residual diagnostic\nprint(sarima_fit.plot_diagnostics(figsize=(12,12)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals = pd.DataFrame(sarima_fit._results.forecasts_error[0], columns=['data'])\ntest_LjungBox(residuals, has_seasonality=True, seasonal_period=12)\ntest_shapiro(standardized_residuals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The residuals aren't correlated but doesn't come from a normal distribution : we don't have white noise.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Comparing the models\n\nprint(\"# Modèle Holt-Winter\\nTrain RMSE : {:.3f}\\nTest RMSE : {:.3f}\\nTrain MAPE : {:.3f} %\\nTest MAPE : {:.3f} %\\n\\n\".format(train_RMSE_hw, test_RMSE_hw, train_MAPE_hw, test_MAPE_hw))\n\nprint(\"# Modèle SARIMA\\nTrain RMSE : {:.3f}\\nTest RMSE : {:.3f}\\nTrain MAPE : {:.3f} %\\nTest MAPE : {:.3f} %\\n\\n\".format(train_RMSE_sarima, test_RMSE_sarima, train_MAPE_sarima, test_MAPE_sarima))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Holt-Winter model is a bit more efficient that the ARIMA model.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}