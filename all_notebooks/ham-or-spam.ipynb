{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df =pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding= \"ISO-8859â€“1\" )\nprint(df.columns)\nprint(df[\"v1\"].notnull().value_counts())\nprint(df[\"v2\"].notnull().value_counts())\nprint(df[\"Unnamed: 2\"].notnull().value_counts())\nprint(df[\"Unnamed: 3\"].notnull().value_counts())\nprint(df[\"Unnamed: 4\"].notnull().value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean= df.copy()\ndf_clean.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.columns = [\"type\",\"text\"]\ndf_clean[\"type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a severely imbalanced dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# within ham texts\nham = df_clean[df_clean[\"type\"] == \"ham\"]\nspam = df_clean[df_clean[\"type\"] == \"spam\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef show(index):\n    ham_list = ham[\"text\"].tolist()\n    text = ham_list[index]\n    print(text)\n    pattern = re.compile(r\"[^\\w]\")\n    print(re.sub(pattern,\" \",text))\n\nshow(22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport seaborn as sns \nstopwords_list = stopwords.words('english')\ndef clean_text(df):\n    cleaned_reviews = []\n    text = df[\"text\"].tolist()\n    vocab = Counter()\n    for sentence in text:\n        pattern = re.compile(r\"[^\\w]\")\n        sentence = re.sub(pattern,\" \",sentence)\n        # tokenize each sentence into a list of words \n        token_list = word_tokenize(sentence)\n        # change all tokens to lower caps \n        token_list= [tokens.lower() for tokens in token_list]\n        # remove words that are in the stopwords list\n        cleaned = [tokens for tokens in token_list if tokens not in stopwords_list]\n        cleaned = [tokens for tokens in cleaned if tokens.isalpha()]\n        w = [tokens for tokens in cleaned if len(tokens) >1]\n        vocab.update(w)\n        cleaned = \" \".join(w)\n        cleaned_reviews.append(cleaned)\n    return vocab, cleaned_reviews\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set()\ndef plot_word_count(df):\n    # call the function to get our vocab count and cleaned_reviews \n    vocab,cleaned_reviews = clean_text(df)\n    # change counter to dictionary object\n    dict_vocab = dict(vocab)\n    # sort dictionary base on values \n    sorted_vocab_dict = sorted(dict_vocab.items(),key = lambda x: x[1],reverse = True)\n    # create x and y list to append to for plotting \n    y = []\n    x = []\n    for i in sorted_vocab_dict[:50]:\n        y.append(i[1])\n        x.append(i[0])\n    \n    fig,ax = plt.subplots(figsize = (20,8))\n    plot = ax.bar(x,y)\n    plt.xticks(rotation = 50)\n    plt.title(\"{}\".format(df[\"type\"].to_numpy()[0]))\n    return plot \n\nplot_word_count(ham)\nplot_word_count(spam)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most word count in an sms?"},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_vocab,ham_cleaned_reviews = clean_text(ham)\nspam_vocab,spam_cleaned_reviews = clean_text(spam)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find length of maximum sms within sms ham\nmax_ham_sms = len(max(ham_cleaned_reviews,key = len))\nmax_spam_sms = len(max(spam_cleaned_reviews,key = len))\nprint(\"Longest ham sms :\", max_ham_sms)\nprint(\"Longest spam sms : \",max_spam_sms )\nprint(\"No of unique tokens for ham\",len(ham_vocab))\nprint(\"No of unique tokens for spam\",len(spam_vocab))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use boxplot to get the spread of sms length for both spam and ham"},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_sms_size = [len(i) for i in ham_cleaned_reviews]\nspam_sms_size = [len(i) for i in spam_cleaned_reviews]\nfig,(ax1,ax2) = plt.subplots(2,1,figsize = (20,8))\nax1.boxplot(ham_sms_size,vert = False)\nax1.title.set_text(\"Distribution of sms length for ham\")\nax2.boxplot(spam_sms_size,vert = False)\nax2.title.set_text(\"Distribution of sms length for spam\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def percent(pad_limit):\n    num_of_sms_less_than_pad_limit = [i for i in ham_sms_size if i < pad_limit]\n    percent = len(num_of_sms_less_than_pad_limit)/len(ham_sms_size)* 100\n    return percent\npercent(300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to understand the lengths of the sms so we can choose the max_sequence_length when we are gonna pad the sms for inputs into our LSTM model. \nWe can set the pad length at 300, since most of the sms are underneath that value. "},{"metadata":{},"cell_type":"markdown","source":"LSTM Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_clean, cleaned_reviews = clean_text(df_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(vocab_clean))\nprint(len(cleaned_reviews))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_reviews[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nmax_words_to_keep = 10000\n# according to the distribution of \nmax_sequence_length = 300\ntokenizer = Tokenizer(num_words = max_words_to_keep)\n# here our inputs are our list of cleaned_reviews \ntokenizer.fit_on_texts(cleaned_reviews)\n# the wordIndex is the same as our vocab counter \nwordIndex = tokenizer.word_index\n\n#to transform all the texts from cleaned_reviews to sequences of integers. \n\nX = tokenizer.texts_to_sequences(cleaned_reviews)\nX = pad_sequences(X,maxlen = max_sequence_length)\nprint(\"Shape of X tensor: \",X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_clean[\"type\"].replace({\"ham\":0,\"spam\":1}).to_numpy()\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42, test_size = 0.2)\nprint(\"X_train size : \",X_train.shape)\nprint(\"X_test size : \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding \nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport keras\nMETRICS = [keras.metrics.TruePositives(name = 'tp'),\n           keras.metrics.FalsePositives(name = 'fp'),\n           keras.metrics.TrueNegatives(name = \"tn\"),\n           keras.metrics.FalseNegatives(name = \"fn\"),\n           keras.metrics.BinaryAccuracy(name = \"accuracy\"),\n           keras.metrics.Precision(name = \"precision\"),\n           keras.metrics.Recall(name = \"recall\"),\n           keras.metrics.AUC(name = \"auc\")]\n\nembedding_dim = 64\ndef make_model(output_bias = None):\n    model = Sequential([\n    Embedding(max_words_to_keep,embedding_dim, input_length = max_sequence_length),\n    SpatialDropout1D(0.8),    \n    LSTM(10,dropout = 0.8),\n    Dense(1, activation = \"sigmoid\",bias_initializer = output_bias)\n    ])\n    model.compile(loss = \"binary_crossentropy\",optimizer = \"adam\",metrics = 'accuracy')\n    return model \nmodel= make_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epochs = 30\nbatch_size = 64\nfile_path = 'model1.h5'\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 3)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = file_path, save_freq = 'epoch')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs = Epochs, batch_size = batch_size, validation_split = 0.1,callbacks=[early_stopping, model_checkpoint])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize = (20,8))\nax[0].plot(history.epoch,history.history[\"loss\"])\nax[0].plot(history.epoch,history.history[\"val_loss\"])\nax[0].legend([\"training\",\"validation\"])\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Loss\")\n\nax[1].plot(history.epoch,history.history[\"accuracy\"])\nax[1].plot(history.epoch,history.history[\"val_accuracy\"])\nax[1].legend([\"training\",\"validation\"])\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict_classes(X_test, batch_size = 64)\ny_train_pred = model.predict_classes(X_train, batch_size = 64)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \ndef plot_cm(labels, predictions):\n    cm = confusion_matrix(labels, predictions)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title(f'Confusion matrix')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n  \n    print('Ham Detected (True Negatives): ', cm[0][0])\n    print('Ham Incorrectly Detected (False Positives): ', cm[0][1])\n    print('Spam Missed (False Negatives): ', cm[1][0])\n    print('Spam Detected (True Positives): ', cm[1][1])\n    print('Total Spam in dataset: ', np.sum(cm[1]))\n    print('Total Ham in dataset:', np.sum(cm[0]))\n\nplot_cm(y_train,y_train_pred)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cm(y_test,y_test_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model.evaluate(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of model on test data: \",accr[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I read online that SpatialDropout1D is the same as applying dropout layer with noise shape (batch_size, 1, features). Let us change the dropout layer and investigate if they produce comparable results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 64\ndef make_model(output_bias = None):\n    model = Sequential([\n    Embedding(max_words_to_keep,embedding_dim, input_length = max_sequence_length),\n    Dropout(0.8, noise_shape = (1,embedding_dim)),    \n    LSTM(10,dropout = 0.8),\n    Dense(1, activation = \"sigmoid\",bias_initializer = output_bias)\n    ])\n    model.compile(loss = \"binary_crossentropy\",optimizer = \"adam\",metrics = 'accuracy')\n    return model \nmodel2= make_model()\nmodel2.summary()\nplot_model(model2,show_shapes = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path2 = 'model2.h5'\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = file_path2, save_freq = 'epoch')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model2.fit(X_train, y_train, epochs = Epochs, batch_size = batch_size, validation_split = 0.1,callbacks=[early_stopping, model_checkpoint])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize = (20,8))\nax[0].plot(history2.epoch,history2.history[\"loss\"])\nax[0].plot(history2.epoch,history2.history[\"val_loss\"])\nax[0].legend([\"training\",\"validation\"])\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Loss\")\n\nax[1].plot(history2.epoch,history2.history[\"accuracy\"])\nax[1].plot(history2.epoch,history2.history[\"val_accuracy\"])\nax[1].legend([\"training\",\"validation\"])\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Accuracy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that SpatialDropout1d and using Dropout layer with noise shape (batch_size, 1, features) are probably doing the same thing, ensuring that the same dropout mask is applied for all out timesteps."},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model2.evaluate(X_test,y_test)\nprint(\"Accuracy of model on test data: \",accr[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}