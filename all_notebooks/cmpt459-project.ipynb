{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Covid-19's symptoms analization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Motivation and background","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. In the scenario where the coronavirus is rapidly spreading among our society, medical testing is the most accurate method to detemine if a person is infected or not, but the current facility of many countries cannot execute the test for everyone. Therefore, I believe it is necessary to analize the symptoms of Covid-19 and find out a method that predict if a person is inffected or not without using medical testing method.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Problem statement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this project, my goal is to analyze the symptoms of covid-19 and try to find the patterns of the disease","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I used the dataset COVID-19 Open Research Dataset Challenge (CORD-19) in this project to analyze the symptoms of Covid-19. If a symptom is mentioned more frequent in the dataset, it means that infected people is more likely to develope that symptom.\nIn addition, I also want to go further than just plotting the data.So I trained a model to predict if people have covid-19 or not. I used the covid19 symptoms checker dataset to train this model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Methodology","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the text preprocessing, I used spacy library to tokenize the data in the jason data files and remove the punctuations.\nI also used the spacy.matcher to find and match all the symptoms exist in the dataset.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport seaborn as sns\n\nimport re\nimport json\nimport math\n\nimport spacy\nfrom spacy.matcher import Matcher\n\nfrom tqdm import tqdm\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nnlp = spacy.load(\"en_core_web_sm\")\n\narticles = {}\nstat = { }\n# Import json files\nfor dirpath, subdirs, files in os.walk('/kaggle/input'):\n    for i in files:\n        if i.endswith(\".json\"):\n            articles[i] = os.path.join(dirpath, i)\ndf = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"symptoms = ['weight loss','chills','shivering','convulsions','deformity','discharge','dizziness','vertigo','fatigue','malaise','asthenia','hypothermia','jaundice','muscle weakness','pyrexia','sweats','swelling','swollen','painful lymph node','weight gain','arrhythmia','bradycardia','chest pain','claudication','palpitations','tachycardia','dry mouth','epistaxis','halitosis','hearing loss','nasal discharge','otalgia','otorrhea','sore throat','toothache','tinnitus','trismus','abdominal pain','fever','bloating','belching','bleeding','blood in stool','melena','hematochezia', 'constipation','diarrhea','dysphagia','dyspepsia','fecal incontinence','flatulence','heartburn','nausea','odynophagia','proctalgia fugax','pyrosis','steatorrhea','vomiting','alopecia','hirsutism','hypertrichosis','abrasion','anasarca','bleeding into the skin','petechia','purpura','ecchymosis and bruising','blister','edema','itching','laceration','rash','urticaria','abnormal posturing','acalculia','agnosia','alexia','amnesia','anomia','anosognosia','aphasia and apraxia','apraxia','ataxia','cataplexy','confusion','dysarthria','dysdiadochokinesia','dysgraphia','hallucination','headache','akinesia','bradykinesia','akathisia','athetosis','ballismus','blepharospasm','chorea','dystonia','fasciculation','muscle cramps','myoclonus','opsoclonus','tic','tremor','flapping tremor','insomnia','loss of consciousness','syncope','neck stiffness','opisthotonus','paralysis and paresis','paresthesia','prosopagnosia','somnolence','abnormal vaginal bleeding','vaginal bleeding in early pregnancy', 'miscarriage','vaginal bleeding in late pregnancy','amenorrhea','infertility','painful intercourse','pelvic pain','vaginal discharge','amaurosis fugax','amaurosis','blurred vision','double vision','exophthalmos','mydriasis','miosis','nystagmus','amusia','anhedonia','anxiety','apathy','confabulation','depression','delusion','euphoria','homicidal ideation','irritability','mania','paranoid ideation','suicidal ideation','apnea','hypopnea','cough','dyspnea','bradypnea','tachypnea','orthopnea','platypnea','trepopnea','hemoptysis','pleuritic chest pain','sputum production','arthralgia','back pain','sciatica','Urologic','dysuria','hematospermia','hematuria','impotence','polyuria','retrograde ejaculation','strangury','urethral discharge','urinary frequency','urinary incontinence','urinary retention']\nhigher_terms = ['over', 'above', 'higher', 'older', '>', 'over', 'less']\nlower_terms = ['under', 'below', 'fewer', 'younger', '<', 'under', 'more']\nvirus_ref = ['covid-19', 'coronavirus', 'cov-2', 'sars-cov-2', 'sars-cov', 'hcov', '2019-ncov']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matchers = {    \n    \"Term Matcher\": lambda term: [{'LOWER': t} for t in term.split(' ')],\n    \"Terms Matcher\": lambda terms: [{\"LOWER\": {\"IN\": terms } }],\n    \"Number Suffix Matcher\": lambda periods: [\n        {'LIKE_NUM': True},\n        {\"TEXT\": {\"REGEX\": f'({\"|\".join(periods)})'}}\n    ],\n    \"Number Interval Matcher\": lambda periods: [\n        {'POS': 'NUM',},\n        {'TEXT': {'REGEX': f'({\"|\".join(periods)})'}, 'OP': '?'},\n        {'DEP': 'quantmod', 'OP': '?'},\n        {'DEP': 'punct', 'OP': '?'},\n        {'DEP': 'prep', 'OP': '?'},\n        {'POS': 'NUM'},\n        {'TEXT': {'REGEX': f'({\"|\".join(periods)})'}},\n    ],\n    \"Group Matcher\": [\n        {\"TEXT\": {\"IN\": higher_terms+lower_terms }}\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dict(stat, t = 10, sort_values = False, barh = False, width = 20, height = 4, title = ''):\n    filtered = dict(stat)\n    to_delete = []\n    for key in filtered:\n        if filtered[key] < t:\n            to_delete.append(key)\n    for key in to_delete:\n        del filtered[key]\n\n    \n    if sort_values == False:\n        lists = sorted(filtered.items())\n    else:\n        if sort_values == True:\n            lists = sorted(filtered.items(), key = lambda item : item[1])\n        else:\n            lists = sorted(filtered.items(), key = sort_values)\n               \n    fig = figure(num=None, figsize=(width, height))\n    \n    if title != '':\n        fig.suptitle(title, fontsize=20)\n        \n    x, y = zip(*lists) \n    \n    if barh == True:\n        plt.barh(x, y)\n    else:\n        plt.bar(x, y)\n    plt.show()\n    \n\ndef merge_keys(mergers, obj):\n    result = dict(obj)\n    for key, arr in mergers:\n        if key not in result:\n            result[key] = 0\n        for merger in arr:\n            if merger in result:\n                result[key] = result[key] + result[merger]\n                del result[merger]\n    return result\n\ndef dict_counter(res, arg):\n    try:\n        key = str(arg)\n        res.setdefault(key, 0)\n        res[key] = res[key] + 1\n    except:\n        pass\n\ndef numval(val):\n    try:\n        return int(float(str(val))) \n    except:\n        return None\n    \ndef day_value(val, rep = None):\n    \n    if rep != None:\n        val = numval(val.text)\n        if val != None and 'week' in rep.text:\n            val = val * 7\n        return val\n    else:\n        return None\n\ndef report_interval(res, min_val, max_val):       \n    if min_val != None and max_val != None:\n        for key in range(min_val, max_val):\n            res.setdefault(key, 0)\n            res[key] = res[key] + 1    \n\ndef virus_match(text):\n    return len(re.findall(rf'({\"|\".join(virus_ref)})', text, flags=re.IGNORECASE)) > 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"literature = []\nfor index, row in tqdm(df.iterrows(), total=df.shape[0]):\n    sha = str(row['sha'])\n    if sha != 'nan':\n        sha = sha + '.json';\n        try:\n            found = False\n            with open(articles[sha]) as f:\n                data = json.load(f)\n                for key in ['abstract', 'body_text']:\n                    if found == False and key in data:\n                        for content in data[key]:\n                            text = content['text']\n                            if virus_match(text) == True:                                \n                                literature.append({'file': articles[sha], 'body': text})                                \n        except KeyError:\n            pass\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def execute_matches(match_arr, root, sentence, file, index = 0, execution = []):\n    key, result = match_arr[0]\n    rest = match_arr[1:]\n    next_exec = execution + [(key, result, index)]\n    if key in root:\n        rule = root[key]\n        if callable(rule):\n            rule( (result, next_exec, sentence, file) )            \n        else:\n            if 'execute' in rule:\n                rule['execute']( (result, next_exec, sentence, file) )\n            if len(rest) > 0:\n                execute_matches(rest, rule, sentence, file, index+1, next_exec)\n    \n    if len(rest) > 0:               \n        execute_matches(rest, root, sentence, file, index + 1, execution)\n        \ndef merge_dict_values(original, rules, drop = []):\n    result = {}\n    arr_map = {}\n    for key, values in rules:\n        for val in values:\n            arr_map[val] = key\n    \n    for key in original.keys():\n        new_key = key if key not in arr_map else arr_map[key]        \n        if key not in drop and new_key not in drop:\n            val = original[key]            \n            result[new_key] = val if new_key not in result else result[new_key] + val\n            \n    return result\n    \ndef merge_matches(matches, doc):\n    match_list = []\n    current = (None, None, None)\n    for match_id, start, end in matches:   \n        if match_id != current[0] or current[2] < start:\n            if current[0] != None:\n                match_list.append(current)\n            current = (match_id, start, end)\n        elif current[2] < end:\n            current = (match_id, current[1], end)\n        \n    match_list.append(current)\n    return match_list;\n\ndef match_parser(matcher, doc, rule, file):\n    matches = matcher(doc)\n    if len(matches)>0:\n        to_process = []\n        for match_id, start, end in merge_matches(matches, doc):\n            string_id = nlp.vocab.strings[match_id]  # Get string representation\n            span = doc[start:end]  # The matched span\n            to_process.append((string_id, span))\n        execute_matches(to_process, rule['root'], doc, file)\n\ndef parse_body(matcher, text, rule, file = None, sentence_level = False):\n    text = text.lower()\n    doc = nlp(text)\n    \n    if sentence_level == True:    \n        for sent in doc.sents:\n            sent_doc = nlp(sent.text)\n            match_parser(matcher, sent_doc, rule, file)\n    else:\n        match_parser(matcher, doc, rule, file)\n\ndef execute_ruleset(term, rule, sentence_level = False, literature = literature):\n    matcher = Matcher(nlp.vocab)\n    for name, m in rule[\"Matchers\"]:\n        matcher.add(name, None, m)\n    \n    for article in tqdm(literature):\n#     for article in literature:\n        text_list = re.compile(\"\\. \").split(article['body'])\n        file = article['file']\n        for text in text_list:\n            if callable(term):\n                allow = term(text)\n            else:\n                allow = term == None or term in text\n            if allow == True:\n                parse_body(matcher, text, rule, file, sentence_level)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat['symptoms'] = {}\n\ndef match(text):\n    if virus_match(text) == True:\n        return len(re.findall(rf'\\ ({\"|\".join(symptoms)})\\ ', text)) > 0\n    else:\n        return False\n\ndef symptom(res):\n    ref, agregate, sentence, file = res\n    dict_counter(stat['symptoms'], ref.text)\n    \nrule = {    \n    \"Matchers\": [      \n       (\"Symptoms Reference\", matchers['Terms Matcher'](symptoms)),\n    ],\n    \"root\": {\n        \"Symptoms Reference\": symptom\n    }\n}\n\n\ndef symptom_match(text):\n    return len(re.findall(r'symptom', text)) > 0\n\nexecute_ruleset(symptom_match, rule)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to visualize the result, I used the matplotlib to draw a bar chart of the frequency of the symptoms. I also used the wordcloud to see how the symptoms are used in this dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dict(stat['symptoms'], 50, True, title = \"Symptoms\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stat['symptoms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\n    \n\nwc = WordCloud(background_color=\"black\",width=1000,height=1000, max_words=20,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(stat['symptoms'])\nplt.figure(figsize=(10,7))\nplt.imshow(wc, interpolation = \"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)\nsymptoms_checker = pd.read_csv('/kaggle/input/covid19-symptoms-checker/Cleaned-Data.csv')\nfrom wordcloud import WordCloud, STOPWORDS\n# Create and generate a word cloud image:\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#wordcloud = WordCloud(width=480, height=480,margin=0,stopwords=STOPWORDS, collocations=False).generate(' '.join(symptoms_checker))\n#plt.figure(figsize=(20,10), facecolor='k')\n#plt.imshow(wordcloud, interpolation='bilinear')\n#plt.axis(\"off\")\n#plt.margins(x=0, y=0)\n#plt.show()\n#symptoms_checker.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data0 = symptoms_checker.drop(\"Country\",1)\ndata0.apply(pd.value_counts).plot(y=[\"Fever\",\"Tiredness\", \"Dry-Cough\", \"Difficulty-in-Breathing\", \"Sore-Throat\", \"None_Sympton\", \"Pains\", \"Nasal-Congestion\", \"Runny-Nose\", \"Diarrhea\"],kind='bar',title='all types')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = symptoms_checker.copy()\ndata = data.drop(['Severity_None','None_Sympton','None_Experiencing','Contact_Dont-Know','Country','Contact_No'],axis = 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data.copy()\ndata1 = data.drop(['Severity_Moderate','Severity_Mild'],axis = 1)\ny_data = data1['Severity_Severe']\nx_data = data1.drop(['Severity_Severe'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nfrom sklearn.model_selection import train_test_split\nX_train,X_val,Y_train,Y_val = train_test_split(x_data,y_data,test_size = 0.3,random_state = SEED)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud1 = X_train.drop(['Age_0-9','Age_10-19','Age_20-24','Age_25-59','Age_60+','Gender_Female','Gender_Male','Gender_Transgender','Contact_Yes'],axis = 1)\nwordcloud = WordCloud(width=480, height=480,margin=0,stopwords=STOPWORDS, collocations=False).generate(' '.join(wordcloud1))\nplt.figure(figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\nsymptoms_checker.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf.fit(X_train, Y_train)\n\n\ny_pred_train = clf.predict(X_train)\ny_pred_val = clf.predict(X_val)\n# rf.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(Y_val,y_pred_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscore = cross_val_score(clf,X_val,Y_val,cv = k_fold,n_jobs=1,scoring=scoring)\na = 1\nfor i in score:\n    print(\"(\",a,\") \",i)\n    a+=1\ntype(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference\nhttps://www.kaggle.com/cstefanache/nlp-text-mining-disease-behaviour\nhttps://www.kaggle.com/ashishlabs/covid-19-india-data-analysis-and-prediction","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}