{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary\nPerform Attrition Analysis based on IBM's data set available [here](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/home), which is based on IBM's [gitrepo](https://github.com/IBM/employee-attrition-aif360)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install aif360 rfpimp\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation: Load, Clean and Format\n## Data Extraction"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Load data in pandas dataframe format\nhr_data = pd.read_csv(\"../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\nhr_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get description of data\nGenerate descriptive statistics that summarize the central tendency, dispersion and shape of dataset's distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How is data modeled (datatypes) in the dataset?\n# Can also be viewed instead using Data->Summary tab\n#hr_data.dtypes\nhr_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This gives description of only numeric fields (excludes NAN values)\n# hr_data.describe()\n# This gives description of all columns\nhr_data.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean Data\nIn this phase, we filter and extract only the information that is needed for problem solving. Model quality is highly dependant on the input data (GIGO).\n\n- Understand meaning of every feature & identify errors\n- Look for missing values and find a way to fill the missing values\n- Remove duplicates / corrupted records\n- Scale and normalize data\n- Character encoding (string -> numerical representation)\n- Handle inconsistencies"},{"metadata":{},"cell_type":"markdown","source":"### Check Data\nCheck missing values, etc."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def print_missing_values(data):\n    data_null = pd.DataFrame(data.isnull().sum(), columns = ['Count'])\n    data_null = data_null[data_null['Count'] > 0].sort_values(by='Count', ascending=False)\n    data_null = data_null/len(data) * 100\n\n    if data_null.empty:\n        print('No missing values in dataset')\n    else:\n        ax = sns.barplot(x=data_null.index, y=data_null['Count'])\n        ax.set_title('Columns with at least one missing value')\n        ax.set_ylabel('%age of dataset')\n        print('Total missing values: ', data.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Check missing values\nprint_missing_values(hr_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If there are missing values:**\n- Why is data missing? Human error / we missed it during extraction\n- Drop missing values\n- Some ways to fill missing values:\n  - Zero\n  - Mean (works well for normal distribution)\n  - Random values from same distribution (works well for equal distribution)\n  - Value after missing value (makes sense if data set has a logical order)"},{"metadata":{},"cell_type":"markdown","source":"### Drop any redundant features\nAre there features which are simply scaled values of other features in the dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = hr_data[['HourlyRate', 'DailyRate', 'MonthlyRate', 'MonthlyIncome']]\ndf_dph = (hr_data['DailyRate']/hr_data['HourlyRate']).to_frame()\ndf_dph.columns=['DailyPerHourRate']\ndf_mpd = (hr_data['MonthlyRate']/hr_data['DailyRate']).to_frame()\ndf_mpd.columns = ['MonthlyPerDayRate']\n\ndf = pd.concat([hr_data[['HourlyRate', 'DailyRate', 'MonthlyRate', 'MonthlyIncome']], df_dph, df_mpd], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The DailyRate, HourlyRate and MonthlyRate are not scaled values of each other.\n\nPerhaps, these are representing variations where employees work different number of hours / days, which may impact work-life balance. However, we already have a WorkLifeBalance field. Therefore, this might just be erroneous data, let us remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove any erroneous data\nhr_data.drop(columns=['HourlyRate', 'DailyRate', 'MonthlyRate'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Any features which simply represent incrementing data (series, etc.)\nDo we have employee numbers, serial numbers, etc.?\n\nUse:\n- plt.plot()\n- plt.hist() for histogram\n- sns.distplot() for distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check EmployeeNumber\n%matplotlib inline\n\nplt.plot(hr_data['EmployeeNumber'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop these features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop features\nhr_data.drop(columns=['EmployeeNumber'], inplace=True)\nhr_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode categorical features (in string) as most tools work with numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy categorical data\nhr_data_cat = hr_data.select_dtypes(exclude=np.number)\nhr_data_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace Yes and No in Attrition with 1 and 0\n#num_val = {'Yes': 1, 'No': 0}\n#hr_data_cat['Attrition'] = hr_data_cat['Attrition'].apply(lambda x: num_val[x])\n# Convert categorical variable into dummies (this is one-hot encoding)\n#hr_data_cat = pd.get_dummies(hr_data_cat)\n#hr_data_cat.head()\n# OR, use scikit-learn label encoding for mapping\nfrom sklearn import preprocessing\nlab_enc = preprocessing.LabelEncoder()\n# Deep-copy original data\nhr_data_enc = hr_data.copy(deep=True)\nfor col in hr_data_cat.columns:\n    hr_data_enc[col] = lab_enc.fit_transform(hr_data[col])\n    le_name_mapping = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))\n    print('Feature', col)\n    print('mapping', le_name_mapping)\nhr_data_enc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration\n- Find patterns in data through data visualization. Reveal hidden secrets of data through graphs, analysis and charts.\n  - Univariate analysis\n    - Continuous Variables: Histograms, boxplots. Gives us an understanding about central tendency and spread\n    - Categorical Variables: Bar chart showing frequency in each category\n  - Bivariate analysis\n    - Continuous & Continuous: Scatter plots to know how continuous variables interact with each other\n    - Categorical & Categorical: Stacked column chart to show how frequencies are spread between two categorical variables\n    - Categorical & Continuous: Boxplots, Swamplots or bar charts\n- Detect outliers\n- Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization methods\n\n# Plots distribution as bar and pie chart\n# e.g. plot_bar_and_pie('YearsSinceLastPromotion', hr_data)\ndef plot_bar_and_pie(y_var, data):\n    val = data[y_var]\n    \n    plt.style.use('seaborn-whitegrid')\n    plt.rcParams.update({'font.size': 12})\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    \n    cnt = val.value_counts().sort_values(ascending=True)\n    labels = cnt.index.values\n    \n    sizes = cnt.values\n    colors = sns.color_palette('PuBu', len(labels))\n    \n    # Count plot rendered as barh\n    ax1.barh(cnt.index.values, cnt.values, color=colors)\n    ax1.set_title('Count plot of ' + y_var)\n    \n    # Percentage rendered as pie\n    ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', startangle=145)\n    ax2.axis('equal')\n    ax2.set_title('Distribution of ' + y_var)\n    plt.show()\n    \n# Plots a histogram\n# e.g. plot_hist(hr_data, 'YearsSinceLastPromotion', ['YearsWithCurrManager', 'YearsAtCompany'])\ndef plot_hist(data, col, y_columns):\n    df = data.copy()\n    fig, ax = plt.subplots(1, len(y_columns), figsize=(20, 6))\n    \n    for i in range(0, len(y_columns)):\n        cnt = []\n        y_col = y_columns[i]\n        y_values = df[y_col].dropna().drop_duplicates().values\n        for val in y_values:\n            cnt += [df[df[y_col] == val][col].values]\n        bins = df[col].nunique()\n        \n        if (len(y_columns) > 1):\n            ax[i].hist(cnt, bins=bins, stacked=True)\n            ax[i].legend(y_values, loc='upper right')\n            ax[i].set_title('Histogram of ' + col + ' column by ' + y_col)\n        else:\n            ax.hist(cnt, bins=bins, stacked=True)\n            ax.legend(y_values, loc='upper right')\n            ax.set_title('Histogram of ' + col + ' column by ' + y_col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data distribution between output classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"hr_data_enc['Attrition'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data is unbalanced**"},{"metadata":{},"cell_type":"markdown","source":"## Drop columns where data doesn't change"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find all columns where data doesn't change, use .nunique(dropna=False) if we want to count NAs as separate value\nuc_columns = hr_data_enc.columns[hr_data_enc.nunique() <= 1]\nprint('Columns which do not change: {}'.format(uc_columns))\n# Remove columns which don't change at all\nhr_data_enc = hr_data_enc.drop(columns=uc_columns)\nhr_data_enc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find correlation between variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a seaborn heatmap\n%matplotlib inline\n\nplt.figure(figsize=(10,10), dpi=100)\nsns.heatmap(hr_data_enc.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation Analysis**\n- High correlations:\n  - MonthlyIncome - JobLevel\n  - TotalWorkingYears - JobLevel\n  - TotalWorkingYears - MonthlyIncome\n  - TotalWorkingYears - Age\n- Negative correlations:\n  - MaritalStatus - StockOptionLevel"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract correlated variables for further analysis\ncorr_cols = ['Age', 'JobLevel', 'MonthlyIncome', 'TotalWorkingYears', 'MaritalStatus', 'StockOptionLevel']\nfiltered_data = hr_data_enc[corr_cols]\n\n%matplotlib inline\n\nplt.figure(figsize=(20,5), dpi=100)\nsns.heatmap(filtered_data.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understand relationships and find patterns in data through visualization\nSome data visualization libraries:\n- Matplotlib\n- Seaborn\n- ggplot\n- Bokeh\n- pygal\n- Plotly\n- geoplotlib\n- Gleam\n- missingno\n- Leather"},{"metadata":{},"cell_type":"markdown","source":"### How does YearsSinceLastPromotion predictor behave?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(hr_data, 'YearsSinceLastPromotion', ['Attrition'])\n\ncnt_yes = hr_data.loc[hr_data['Attrition'] == 'Yes', ['YearsSinceLastPromotion']]['YearsSinceLastPromotion'].value_counts(sort=False)\ncnt_no  = hr_data.loc[hr_data['Attrition'] == 'No' , ['YearsSinceLastPromotion']]['YearsSinceLastPromotion'].value_counts(sort=False)\n\nyears_since_promo_attr_ratio = []\ncnt_yes_cutoff_sum = 0\ncnt_no_cutoff_sum = 0\n\nfor i in range(8):\n    years_since_promo_attr_ratio.append(cnt_yes[i] / (cnt_yes[i] + cnt_no[i]))\nfor i in range(8, len(cnt_no)):\n    if (i != 8) and (i != 12):\n        cnt_yes_cutoff_sum += cnt_yes[i]\n    cnt_no_cutoff_sum  += cnt_no[i]\n\nyears_since_promo_attr_ratio.append(cnt_yes_cutoff_sum / (cnt_yes_cutoff_sum + cnt_no_cutoff_sum))\n\nx_data = np.array(range(9))\ny_data = np.array(years_since_promo_attr_ratio)\n\n# Use two subplots, left would be linear, right would be log\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\nbbox_props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n# Linear plot\ncurve_fit = np.polyfit(x_data, y_data, 1, full=True)\ny = curve_fit[0][0] * x_data + curve_fit[0][1]\nax[0].plot(x_data, y_data, 'o')\nax[0].plot(x_data, y)\nax[0].set_xlabel('YearsSinceLastPromotion')\nax[0].set_ylabel('P(Attrition)')\nax[0].set_ylim(0, 1)\nax[0].set_title('y = {0:.4f}x + {0:.4f}'.format(curve_fit[0][0], curve_fit[0][1]))\nax[0].text(0.05, 0.95, 'Error {0:.4f}'.format(curve_fit[1][0]), transform=ax[0].transAxes, fontsize=12, verticalalignment='top', bbox=bbox_props)\n# Log plot\nlog_x_data = np.log2(x_data + 1)\ncurve_fit = np.polyfit(log_x_data, y_data, 1, full=True)\ny = curve_fit[0][0] * log_x_data + curve_fit[0][1]\nax[1].plot(log_x_data, y_data, 'o')\nax[1].plot(log_x_data, y)\nax[1].set_xlabel('log(YearsSinceLastPromotion)')\nax[1].set_ylabel('P(Attrition)')\nax[1].set_ylim(0, 1)\nax[1].set_title('y = {0:.4f} log2(x + 1) + {0:.4f}'.format(curve_fit[0][0], curve_fit[0][1]))\nax[1].text(0.05, 0.95, 'Error {0:.4f}'.format(curve_fit[1][0]), transform=ax[1].transAxes, fontsize=12, verticalalignment='top', bbox=bbox_props)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set input_data for modeling after data is ready\ninput_data = hr_data_enc\ninput_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Development\n## Extract label from input data"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = input_data['Attrition']\nall_features = input_data.drop('Attrition', axis = 1)\n\nprint('No of columns: {}'.format(len(all_features.columns)))\ntarget.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select Features\nChoose the best features that can be used for predictive modeling.\n- Reduce dimensionality\n- Reduce training time\n- Don't overfit\n- Increase generalizability\n\nFeature selection methods:\n1. Filter methods\n  - F Test\n  - Mutual information\n  - Variance threshold\n  - Chi squared\n  - Correlation coeffecient\n  - ANNOVA\n  - LDA\n2. Wrapper methods\n  - Forward search\n  - Backward search\n  - Recursive feature elimination\n3. Embedded methods\n  - LASSO Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_feature_cols = []\ncol_values = list(all_features.columns.values)\nprint(col_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mutual information\nMeasures dependence of one variable to another:\n  - Mutual information is 0 => X and Y are independent, X carries no info about Y\n  - Mutual information is 1 => X and Y are dependent, X can be derived from Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\n\n# Find top 10 features with maximum Mutual Information (dependent variables)\nfeature_scores = mutual_info_classif(all_features, target)\nfor score, fname in sorted(zip(feature_scores, col_values), reverse=True)[:10]:\n    print(fname, score)\n    sel_feature_cols.append(fname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### chi-squared\nchi-square tests independence of two events, and used to evaluate likelihood of correlation or association between features using their frequency distribution. Best for categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find top 10 features with maximum chi-square value\nfrom sklearn.feature_selection import chi2\nfeature_scores = chi2(all_features, target)[0]\nfor score, fname in sorted(zip(feature_scores, col_values), reverse=True)[:10]:\n    print(fname, score)\n    sel_feature_cols.append(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select features\nprint(np.unique(sel_feature_cols))\n#features = all_features[np.unique(sel_feature_cols)]\n\nfeatures = all_features # Select all the features (just to test)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split into Train and Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Create the train / test split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.15, random_state=10)\n\nprint('Shape of features: ', features.shape)\nprint('Shape of Training input data: ', X_train.shape, '\\tShape of Training target: ', y_train.shape)\nprint('Shape of Test     input data: ', X_test.shape,  '\\tShape of Test     target: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First-pass model"},{"metadata":{},"cell_type":"markdown","source":"Model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, roc_curve, auc\n\ndef get_model_performance(X_test, y_true, y_pred, probs):\n    # Test the accuracy\n    accuracy = accuracy_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    matrix = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    return accuracy, recall, matrix, f1, fpr, tpr, roc_auc\n\ndef plot_model_performance(model, X_test, y_true):\n    # Predict the results for test\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, recall, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n    print('Accuracy score: ', accuracy)\n    print('Recall score:   ', recall)\n    print('F1 score:       ', f1)\n    \n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(1, 2, 1)\n    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')\n    plt.title('Confusion Matrix')\n    \n    ax = fig.add_subplot(1, 2, 2)\n    lw = 2\n    plt.plot(fpr, tpr, color='orange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.grid(True)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic Curve')\n    plt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create the model and train\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model_performance(model, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bias Mitigation\nWe need to detect and remove bias in ML models.\n\nUse [IBM's AIF360](https://github.com/IBM/AIF360) package for bias metrics and algorithms.\n\n### Convert dataset into a format usable by bias mitigation algorithms\nSuspect that bias may be present for gender, e.g. female employees may be given favorable outcome (no attrition) compared to male emlpoyees.\n\nIdentify:\n1. Favorable label\n2. Unfavorable label\n3. Privileged group\n4. Unprivileged group"},{"metadata":{"trusted":true},"cell_type":"code","source":"from aif360.datasets import StandardDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print metadata for bias mitigation dataset\n# Note: dataset needs to be of type StandardDataset\ndef bm_meta_data(dataset):\n    print('Dataset shape: ', dataset.features.shape)\n    print('Favorable label: ', dataset.favorable_label)\n    print('Unfavorable label: ', dataset.unfavorable_label)\n    print('Protected attributes: ', dataset.protected_attribute_names)\n    print('Privileged protected attributes: ', dataset.privileged_protected_attributes)\n    print('Unprivileged protected attributes: ', dataset.unprivileged_protected_attributes)\n    print('Features: ', dataset.feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender suspected to have bias\nprivileged_groups   = [{'Gender': 0}] # Female\nunprivileged_groups = [{'Gender': 1}] # Male\nfavorable_label = 0\nunfavorable_label = 1\nbm_data_test = StandardDataset(input_data,\n                               label_name='Attrition',\n                               favorable_classes=[favorable_label], \n                               protected_attribute_names=['Gender'], \n                               privileged_classes=[[favorable_label]])\nbm_meta_data(bm_data_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bias Metrics\nWe will use the following metrics to detect bias:\n\n- Statistical Parity Difference\n\n$$ Bias = \\text{Probability that a random individual from unprivileged is labeled favorable} - \\text{Probability that a random individual from privileged is labeled favorable} $$\n\nShould be close to 0 to be fair\n\n[Reference](https://jeremykun.com/2015/10/19/one-definition-of-algorithmic-fairness-statistical-parity/)\n\n- Equal Opportunity Difference\n\n$$ Bias = \\text{TruePositiveRate unprivileged group} - \\text{TruePositiveRate privileged group} $$\n\nShould be close to 0 to be fair\n\n[Reference](https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.equal_opportunity_difference)\n\n- Average Absolute Odds Difference\n\nUses both FalsePositiveRate and TruePositiveRate to calculate bias.\n\nShould be close to 0 to be fair\n\n[Reference](https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.average_abs_odds_difference)\n\n- Disparate Impact\n\nUses Ratio instead of difference in Statistical Parity Difference above:\n$$ Bias = \\frac{\\text{Probability that a random individual from unprivileged is labeled favorable}}{\\text{Probability that a random individual from privileged is labeled favorable}} $$\n\nShould be close to 1 to be fair\n\n[Reference](https://aif360.readthedocs.io/en/latest/modules/metrics.html#aif360.metrics.ClassificationMetric.disparate_impact)\n\n- Theil Index\n\nGeneralized entropy index with $\\alpha$ equal to 1 [Reference](https://en.wikipedia.org/wiki/Generalized_entropy_index).\n\nShould be close to 0 to be fair"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fair_metrics(dataset, pred, pred_is_dataset=False):\n    if pred_is_dataset:\n        dataset_pred = pred\n    else:\n        dataset_pred = dataset.copy()\n        dataset_pred.labels = pred\n        \n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference', 'disparate_impact', 'theil_index']\n    obj_fairness = [[0,0,0,1,0]]\n    \n    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n    \n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups   = [{attr:dataset_pred.privileged_protected_attributes[idx][0]}]\n        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}]\n        \n        classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n        acc = classified_metric.accuracy()\n        row = pd.DataFrame([[metric_pred.mean_difference(),\n                             classified_metric.equal_opportunity_difference(),\n                             classified_metric.average_abs_odds_difference(),\n                             metric_pred.disparate_impact(),\n                             classified_metric.theil_index()\n                            ]], columns=cols, index=[attr])\n        fair_metrics = fair_metrics.append(row)\n    \n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n    return fair_metrics\n\ndef plot_fair_metrics(fair_metrics):\n    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n    \n    plt.subplots_adjust(\n        left = 0.125,\n        bottom = 0.1,\n        right = 0.9,\n        top = 0.9,\n        wspace = .5,\n        hspace = 1.1\n    )\n    y_title_margin = 1.2\n    \n    plt.suptitle('Fairness metrics', y=1.09, fontsize=20)\n    sns.set(style='dark')\n    \n    cols = fair_metrics.columns.values\n    obj = fair_metrics.loc['objective']\n    size_rect = [0.2,0.2,0.2,0.4,0.25]\n    rect = [-0.1,-0.1,-0.1,0.8,0]\n    bottom = [-1,-1,-1,0,0]\n    top = [1,1,1,2,1]\n    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n    \n    print('Check bias metrics (model may be biased if even one of these metrics show a bias)')\n    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n        print('Attribute: ' + attr + ', with default threshold, bias against unprivileged group detected in {} out of 5 metrics'.format(5 - sum(check)))\n    \n    for i in range(0, 5):\n        plt.subplot(1, 5, i+1)\n        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n        for j in range(0, len(fair_metrics)-1):\n            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n            marg = -0.2 if val < 0 else 0.1\n            ax.text(a.get_x() + a.get_width()/5, a.get_y() + a.get_height() + marg, round(val, 3), fontsize=15, color='black')\n        plt.ylim(bottom[i], top[i])\n        plt.setp(ax.patches, linewidth=0)\n        ax.add_patch(patches.Rectangle((-5, rect[i]), 10, size_rect[i], alpha=0.3, facecolor='green', linewidth=1, linestyle='solid'))\n        plt.axhline(obj[i], color='black', alpha=0.3)\n        plt.title(cols[i])\n        ax.set_ylabel('')\n        ax.set_xlabel('')\n\ndef get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n    fair = fair_metrics(data, pred)\n    \n    if plot:\n        plot_fair_metrics(fair)\n        display(fair)\n    \n    return fair","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for bias\nfair = get_fair_metrics_and_plot(bm_data_test, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Algorithm\nOur data is highly unbalanced. Some ways to compensate:\n1. Data Level Approach\n  - Random under-sampling\n  - Cluster based over-sampling\n  - Synthetic Minority over-sampling technique\n  - Modified Synthetic Minority over-sampling technique\n2. Algorithm Ensemble\n  - Bagging\n  - Boosting\n    - Adaptive Boosting (Ada-boost)\n    - Gradient Tree Boosting\n    - XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if data is unbalanced\ntarget.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# Max number of estimators at which boosting is terminated\nestimator = [50, 100, 200, 300, 400, 500, 700, 1000, 1500, 2000]\nfor i in estimator:\n    print('Results for {} estimators:'.format(i))\n    cls = AdaBoostClassifier(n_estimators=i)\n    cls.fit(X_train, y_train)\n    plot_model_performance(cls, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_estimators=1000 appears to be the best\ncls = AdaBoostClassifier(n_estimators=1000)\ncls.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model_performance(cls, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjust hyper-parameters\n\n# Try learning_rate of 0.1\n#cls = AdaBoostClassifier(n_estimators=1000, learning_rate = 0.1)\n#cls.fit(X_train, y_train)\n#plot_model_performance(cls, X_test, y_test)\n# Worse than default\n\n# Try Support vector classifier\n#from sklearn.svm import SVC\n#svc = SVC(probability=True, kernel='linear')\n#cls = AdaBoostClassifier(n_estimators=1000, base_estimator=svc, learning_rate = 1.0)\n#cls.fit(X_train, y_train)\n#plot_model_performance(cls, X_test, y_test)\n# Worse than default","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference\nWhat features contribute the most?\n\n## Default importance via gini"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import ceil, log10\n\ndef plot_importances(df_importance):\n    plt.style.use('seaborn-whitegrid')\n    plt.rcParams.update({'font.size': 12})\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n\n    cnt = df_importance[df_importance['Importance'] > 0]['Importance'].sort_values(ascending=True)\n    labels = cnt.index.values\n    colors = sns.color_palette('PuBu', len(labels))\n\n    # Bar\n    ax1.barh(labels, cnt.values, color=colors)\n\n    # Donut\n    # Pie will not complete if numbers are too small, and their sum < 1\n    pie_sum = cnt.values.sum()\n    if pie_sum < 1:\n        factor = 10^(ceil(log10(pie_sum)))\n        pie_values = (cnt * factor).values\n    else:\n        pie_values = cnt.values\n    ax2.pie(pie_values, labels=labels, colors=colors, textprops={'color': 'black'}, autopct='%1.0f%%', startangle=145)\n    center_circle = plt.Circle((0,0), 0.7, fc='white')\n    fig.gca().add_artist(center_circle)\n\n    ax2.axis('equal')\n\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from rfpimp import importances, dropcol_importances, oob_dropcol_importances, plot_corr_heatmap\n\nimp = pd.DataFrame()\nimp['Feature'] = X_train.columns\nimp['Importance'] = cls.feature_importances_\nimp = imp.sort_values('Importance', ascending=False)\nimp = imp.set_index('Feature')\nplot_importances(imp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Permutation importance\nimp = importances(cls, X_test, y_test)\nplot_importances(imp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop column importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop column importance\nimp = dropcol_importances(cls, X_train, y_train, X_test, y_test)\nplot_importances(imp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract Phase 1 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine RelationshipSatisfaction with JobSatisfaction\nJobAndRelationshipSatisfaction = np.sum(imp.loc[['RelationshipSatisfaction', 'JobSatisfaction']])\nJobAndRelationshipSatisfaction.rename('JobAndRelationshipSatisfaction', inplace = True)\np1_feat = imp.loc[['JobInvolvement', 'YearsSinceLastPromotion']].append(JobAndRelationshipSatisfaction)\nprint('Scaled Weight Values:\\n{}'.format(p1_feat / p1_feat.sum()))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}