{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read dataset\ndata = pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature correlation.\nplt.figure(figsize=(14, 14))\nplt.title(\"Credit Card Transactions features correlation plot (Pearson)\")\ncorr = data.corr()\nsns.heatmap(\n    corr,\n    xticklabels=corr.columns,\n    yticklabels=corr.columns,\n    linewidths=0.1,\n    cmap=\"Blues\",\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Fraud = data[data[\"Class\"] == 1]\nLegitimate = data[data[\"Class\"] == 0]\nprint(\"Number of Legitimate entries = {}\".format(len(Legitimate)))\nprint(\"Number of Fraud entries = {}\".format(len(Fraud)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gerçek ve sahte işlemlerin dağılımı\nprint(\"Normal(0) and Sahte(1) İşlemlerin Dağılımı: \")\nprint(data[\"Class\"].value_counts())\n\nplt.figure(figsize=(7,5))\nsns.countplot(data['Class'])\nplt.title(\"Sınıf\", fontsize=18)\nplt.xlabel(\"Sahte mi?\", fontsize=15)\nplt.ylabel(\"Miktar\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n        ax.set_title(feature+\" Distribution\",color='DarkRed')\n        ax.set_yscale('log')\n    fig.tight_layout()  \n    plt.show()\ndraw_histograms(data,data.columns,8,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns={'Class'})\ny = data['Class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\ny_test = y_test.ravel()\ny_train = y_train.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud = data[data['Class'] == 1]\nvalid = data[data['Class'] == 0]\n\nprint(\"Fraud transaction statistics\")\nprint(fraud[\"Amount\"].describe())\nprint(\"\\nNormal transaction statistics\")\nprint(valid[\"Amount\"].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describes info about train and test set\nprint(\"X_train dataset: \", X_train.shape)\nprint(\"y_train dataset: \", y_train.shape)\nprint(\"X_test dataset: \", X_test.shape)\nprint(\"y_test dataset: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"before applying smote:\",format(sum(y_train == 1)))\nprint(\"before applying smote:\",format(sum(y_train == 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import SMOTE module from imblearn library\n# pip install imblearn if you don't have\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=2)\nX_train, y_train = sm.fit_sample(X_train, y_train)\n\nprint('After applying smote X_train: {}\\n'.format(X_train.shape))\nprint('After applying smote y_train: {}\\n'.format(y_train.shape))\n\nprint(\"After applying smote label '1': {}\\n\".format(sum(y_train == 1)))\nprint(\"After applying smote label '0': {}\\n\".format(sum(y_train == 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train = X_train.T\ny_train = y_train.reshape(1, y_train.shape[0])\nX_test = X_test.T\ny_test = y_test.reshape(1, y_test.shape[0])\nprint (\"Train X Shape: \", X_train.shape)\nprint (\"Train Y Shape: \", y_train.shape)\nprint (\"I have m = %d training examples!\" % (X_train.shape[1]))\n\nprint (\"\\nTest X Shape: \", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_structure(X, Y):\n    input_unit = X.shape[0] # size of input layer\n    hidden_unit = 30 #hidden layer of size 4\n    output_unit = Y.shape[0] # size of output layer\n    return (input_unit, hidden_unit, output_unit)\n(input_unit, hidden_unit, output_unit) = define_structure(X_train, y_train)\nprint(\"The size of the input layer is:  = \" + str(input_unit))\nprint(\"The size of the hidden layer is:  = \" + str(hidden_unit))\nprint(\"The size of the output layer is:  = \" + str(output_unit))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parameters_initialization(input_unit, hidden_unit, output_unit):\n    np.random.seed(2) \n    W1 = np.random.randn(hidden_unit, input_unit)*0.01\n    b1 = np.zeros((hidden_unit, 1))\n    W2 = np.random.randn(output_unit, hidden_unit)*0.01\n    b2 = np.zeros((output_unit, 1))\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    return 1/(1+np.exp(-z))\n\ndef forward_propagation(X, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n    \n    return A2, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_entropy_cost(A2, Y, parameters):\n    # number of training example\n    m = Y.shape[1]\n    # Compute the cross-entropy cost\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n    cost = - np.sum(logprobs) / m\n    cost = float(np.squeeze(cost))\n    \n                                    \n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward_propagation(parameters, cache, X, Y):\n    #number of training example\n    m = X.shape[1]\n    \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    A1 = cache['A1']\n    A2 = cache['A2']\n   \n    dZ2 = A2-Y\n    dW2 = (1/m) * np.dot(dZ2, A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n    dW1 = (1/m) * np.dot(dZ1, X.T) \n    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n    \n    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n    \n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(parameters, grads, learning_rate = 0.01):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n   \n    dW1 = grads['dW1']\n    db1 = grads['db1']\n    dW2 = grads['dW2']\n    db2 = grads['db2']\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    \n    parameters = {\"W1\": W1, \"b1\": b1,\"W2\": W2,\"b2\": b2}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_model(X, Y, hidden_unit, num_iterations = 1000):\n    np.random.seed(3)\n    input_unit = define_structure(X, Y)[0]\n    output_unit = define_structure(X, Y)[2]\n    \n    parameters = parameters_initialization(input_unit, hidden_unit, output_unit)\n   \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    #print(W1)\n    #print(b1)\n    #print(W2)\n    #print(b1)\n    \n    for i in range(0, num_iterations):\n        A2, cache = forward_propagation(X, parameters)\n        cost = cross_entropy_cost(A2, Y, parameters)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = gradient_descent(parameters, grads)\n        if i % 5 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    return parameters\nparameters = neural_network_model(X_train, y_train, 30, num_iterations=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(parameters, X):\n    A2, cache = forward_propagation(X, parameters)\n    predictions = np.round(A2)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = prediction(parameters, X_train)\nprint ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')\npredictions = prediction(parameters, X_test)\nprint ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Accuracy: ', accuracy_score(y_test.reshape(-1,1), predictions.reshape(-1,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score, classification_report, confusion_matrix\ncm = confusion_matrix(y_test.reshape(-1,1),predictions.reshape(-1,1))\nprint(cm)\nprint('classification_report:')\nprint(classification_report(y_test.reshape(-1,1),predictions.reshape(-1,1)))\nprint('f1_score:',f1_score(y_test.reshape(-1,1),predictions.reshape(-1,1)))\nprint('precision_score:',precision_score(y_test.reshape(-1,1),predictions.reshape(-1,1)))\nprint('recall_score:',recall_score(y_test.reshape(-1,1),predictions.reshape(-1,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sensitivity ve specificity hesaplıyoruz\nTP = cm[1][1]\nTN = cm[0][0]\nFP = cm[0][1]\nFN = cm[1][0]\n\nsensitivity  = TP / (TP+FN)\nspecificity  = TN / (TN+FP)\n\nprint(\" Sensitivity : \",sensitivity)\nprint(\" Specificity : \",specificity)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}