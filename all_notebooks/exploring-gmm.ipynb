{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"\nLast amended: 27th June, 2020\nObjectives:\n\n1. Understanding basics of GMM\ni)    Read dataset and rename columns appropriately\nii)   Drop customerid column and also transform Gender column to [0,1]\niii)  Use seaborn to understand each feature and relationships among features.\niv)  Use sklearn's StandardScaler() to scale dataset\nv)   Perform clustering using Gaussian Mixture Modeling.\nvi)  Use aic and bic measures to draw a scree plot and discover ideal number of clusters\nviii) Lookup anomalous customers and try to understand their behavior.\n\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.0 Call libraries\n#%reset -f                       # Reset memory\n# 1.1 Data manipulation library\nimport pandas as pd\nimport numpy as np\n# 1.2 OS related package\nimport os\n# 1.3 Modeling librray\n# 1.3.1 Scale data\nfrom sklearn.preprocessing import StandardScaler\n# 1.3.2 Split dataset\nfrom sklearn.model_selection import train_test_split\n# 1.3.3 Class to develop kmeans model\nfrom sklearn.cluster import KMeans\n# 1.4 Plotting library\nimport seaborn as sns\n# 1.5 How good is clustering?\nfrom sklearn.metrics import silhouette_score\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nimport re\n\n# 1.3 For plotting\nimport matplotlib.pyplot as plt\nimport matplotlib\n# Install as: conda install -c plotly plotly \nimport plotly.express as px\n\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Import GaussianMixture class\nfrom sklearn.mixture import GaussianMixture\n\nimport time\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.1 Display multiple outputs from a jupyter cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions(precision = 3,          # Display upto 3 decimal places\n                    threshold=np.inf        # Display full array\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1) Read file 'Mall_Customers.csv' \ndf = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# i) Read dataset and rename columns appropriately\n\ncv = df.columns.values # Copy column names to another DF\ncv # Display new DF \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.1 Clean Column Names by replacing/removing special characters\nj = 0\nfor i in cv:\n    cv[j] = re.sub(' ', '_', cv[j]) # Replace space with _\n    cv[j] = re.sub('\\'','', cv[j])  # Replace apostrophe with blank\n    cv[j] = re.sub('[*|\\(\\)\\{\\}]','', cv[j]) # Replace special characters\n    cv[j] = re.sub('/','_', cv[j])    # Replace / with _\n    cv[j] = re.sub('&','_', cv[j])    # Replace & with _\n    cv[j] = re.sub('-','_', cv[j])    # Replace - with _    \n    cv[j] = re.sub('\\.','', cv[j])    # Replace . with _\n    cv[j] = re.sub('[,]','', cv[j])   # Replace , with blank         \n    cv[j] = re.sub('__.','_', cv[j])  # Replace multiple _ with single _          \n    j = j + 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show cleaned column names\ncv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a disctionary of Old & new column names\ny = dict(zip(df.columns.values, cv))\ny\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# i) Read dataset and rename columns appropriately\ndf.rename(\n         y,\n         inplace = True,\n         axis = 1             # Note the axis keyword. By default it is axis = 0\n         )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the new column names\ndf.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ii) Drop customerid column and also transform Gender column to [0,1]\n# ii) Drop customerid column \ndf.head() # Before dropping Customer_ID\ncid = df['CustomerID'].values\ndf.drop(columns = ['CustomerID'], inplace = True)\ndf.head() # After dropping Customer_ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ii) Drop customerid column and also transform Gender column to [0,1]\n# ii) transform Gender column to [0,1]\n\ndef trf_gender(x):\n    if x == 'Male':\n        return 0            # Male = 0\n    if x == 'Female':\n        return 1            # Female = 1\n\n\ndf[\"Gender_Transformed\"] = df[\"Gender\"].map(lambda x : trf_gender(x))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ii) Drop customerid column and also transform Gender column to [0,1]\n# Show the transformed DataFrame\ndf.head()\ndf.Gender_Transformed.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iii)  Use seaborn to understand each feature and relationships among features.\n# Select numeric column heads\ncolumns = list(df.select_dtypes(include = ['float64', 'int64']).columns.values)\ncolumns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iii)  Use seaborn to understand each feature and relationships among features.\n# 1. Using for loop to plot distribution plots all at once\n\nfig = plt.figure(figsize = (10,10))\nfor i in range(len(columns)):\n    plt.subplot(2,2,i+1)\n    sns.distplot(df[columns[i]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iii)  Use seaborn to understand each feature and relationships among features.\n# 2.0 Relationship of numeric variable with a categorical variable\n# 2.1 such relationships through for-loop\ncolumns = ['Age', 'Annual_Income_k$', 'Spending_Score_1_100']\ncatVar = ['Gender']\n\n\n# 2.2 Now for loop. First create pairs of cont and cat variables\nmylist = [(cont, cat)  for cont in columns  for cat in catVar]\nmylist\n\n# 2.3 Now run-through for-loop\nfig = plt.figure(figsize = (20,20))\nfor i, k in enumerate(mylist):\n    plt.subplot(4,2,i+1)\n    sns.boxplot(x = k[1], y = k[0], data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iii)  Use seaborn to understand each feature and relationships among features.\n# Relationship of numeric to numeric variables\nnumcolumns = list(df.select_dtypes(include = ['float64', 'int64']).columns.values)\n\nsns.jointplot(df[numcolumns[0]], df[numcolumns[1]], kind = \"hex\")\n\nsns.jointplot(df[numcolumns[0]], df[numcolumns[2]], kind = \"kde\")\n\nsns.jointplot(df[numcolumns[0]], df[numcolumns[3]])\n\nsns.jointplot(df[numcolumns[1]], df[numcolumns[2]], kind = \"reg\")\n\nsns.jointplot(df[numcolumns[1]], df[numcolumns[3]])\n\nsns.jointplot(df[numcolumns[2]], df[numcolumns[3]])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iii)  Use seaborn to understand each feature and relationships among features.\nsns.barplot(x = 'Gender',\n            y = 'Spending_Score_1_100',\n            estimator = np.mean,\n            ci = 95,\n            data =df\n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\n# create a dataframe of numeric columns\nnc = df.select_dtypes(include = ['float64', 'int64']).copy()\nnc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\n# Drop Categorical & discrete columns\ndf.head() # Before dropping Gender & Gender_Transformed\ndf.drop(columns = ['Gender'], inplace = True)\ngnd = df['Gender_Transformed'].values\ndf.drop(columns = ['Gender_Transformed'], inplace = True)\ndf.head() # After dropping Customer_ID\ngnd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\n\nss = StandardScaler()     # Create an instance of class\nss.fit(df)                # Train object on the data\ndf.shape\nX = ss.transform(df)      # Transform data\nX[:5, :]                  # See first 5 rows\nX.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\nX_train, X_test, _, gnd_test = train_test_split( X,               # np array without target\n                                               gnd,               # Target\n                                               test_size = 0.25 # test_size proportion\n                                               )\n# 4.1 Examine the results\nX_train.shape              # (150, 3)\nX_test.shape               # (50, 3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\nclf = KMeans(n_clusters = 3)\n# 5.2 Train the object over data\nclf.fit(X_train)\n\n# 5.3 So what are our clusters?\nclf.cluster_centers_\nclf.cluster_centers_.shape         # (3, 3)\nclf.labels_                        # Cluster labels for every observation\nclf.labels_.size                   # 150\nclf.inertia_                       # Sum of squared distance to respective centriods, SSE 194.72070292819043\n\nsilhouette_score(X_train, clf.labels_)    # 0.3742884754041953\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\n# Make prediction over our test data and check accuracy\ngnd_pred = clf.predict(X_test)\ngnd_pred\n# How good is prediction\nnp.sum(gnd_pred == gnd_test)/gnd_test.size # 0.42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\ndx = pd.Series(X_test[:, 0])\ndy = pd.Series(X_test[:,1])\nsns.scatterplot(dx,dy, hue = gnd_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\nsse = []\nfor i,j in enumerate(range(10)):\n    # 7.1.1 How many clusters?\n    n_clusters = i+1\n    # 7.1.2 Create an instance of class\n    clf1 = KMeans(n_clusters = n_clusters)\n    # 7.1.3 Train the kmeans object over data\n    clf1.fit(X_train)\n    # 7.1.4 Store the value of inertia in sse\n    sse.append(clf1.inertia_ )\n\n# 7.2 Plot the line now\nsns.lineplot(range(1, 11), sse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\nvisualizer = SilhouetteVisualizer(clf, colors='yellowbrick')\nvisualizer.fit(X_train)        # Fit the data to the visualizer\nvisualizer.show()              # Finalize and render the figure\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iv)  Use sklearn's StandardScaler() to scale dataset\n# Intercluster distance: Does not work\nfrom yellowbrick.cluster import InterclusterDistance\nvisualizer = InterclusterDistance(clf)\nvisualizer.fit(X_train)        # Fit the data to the visualizer\nvisualizer.show()              # Finalize and render the figure\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\ngm_mall = GaussianMixture(\n                           n_components = 3,   # More the clusters, more the time\n                           n_init = 10,\n                           max_iter = 100\n                         )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\nstart = time.time()\ngm_mall.fit(df)\nend = time.time()\n(end - start)/60     # 0.0015 minutes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\n# Did algorithm(s) converge?\ngm_mall.converged_     # True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\n# Clusters labels\ngm_mall.predict(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\n# How many iterations did they perform?\ngm_mall.n_iter_      #  5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\n#  What is the frequency of data-points\n#       for the three clusters. (np.unique()\n#       ouputs a tuple with counts at index 1)\n\nnp.unique(gm_mall.predict(X), return_counts = True)[1]/len(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\n# GMM is a generative model.\n#     Generate a sample from each cluster\n#     ToDo: Generate digits using MNIST\n\ngm_mall.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#v)   Perform clustering using Gaussian Mixture Modeling.\n# Plot cluster and cluster centers\n#     both from kmeans and from gmm\n\nfig = plt.figure()\n\nplt.scatter(X[:, 0], X[:, 1],\n            c=gm_mall.predict(X),\n            s=2)\n\nplt.scatter(gm_mall.means_[:, 0], gm_mall.means_[:, 1],\n            marker='v',\n            s=5,               # marker size\n            linewidths=5,      # linewidth of marker edges\n            color='red'\n            )\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n#     Anomaly detection\n#     Anomalous points are those that\n#     are in low-density region\n#     Or where density is in low-percentile\n#     of 4%\n#     score_samples() method gives score or\n#     density of a point at any location.\n#     Higher the value, higher its density\n\ndensities = gm_mall.score_samples(X)\ndensities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\ndensity_threshold = np.percentile(densities,4)\ndensity_threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\nanomalies = X[densities < density_threshold]\nanomalies\nanomalies.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n# Show anomalous points\nfig = plt.figure()\nplt.scatter(X[:, 0], X[:, 1], c = gm_mall.predict(X))\nplt.scatter(anomalies[:, 0], anomalies[:, 1],\n            marker='x',\n            s=50,               # marker size\n            linewidths=5,      # linewidth of marker edges\n            color='red'\n            )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n# Get first unanomalous data\nunanomalies = X[densities >= density_threshold]\nunanomalies.shape    # (192, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n# Transform both anomalous and unanomalous data\n#     to pandas DataFrame\ndf_anomalies = pd.DataFrame(anomalies, columns = ['x', 'y', 'p'])\ndf_anomalies['z'] = 'anomalous'   # Create a IIIrd constant column\ndf_normal = pd.DataFrame(unanomalies, columns = ['x','y', 'p'])\ndf_normal['z'] = 'unanomalous'    # Create a IIIrd constant column\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n# Let us see density plots\nsns.distplot(df_anomalies['x'])\nsns.distplot(df_normal['x'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n# Let us see density plots\nsns.distplot(df_anomalies['y'])\nsns.distplot(df_normal['y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n# Let us see density plots\nsns.distplot(df_anomalies['p'])\nsns.distplot(df_normal['p'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viii) Lookup anomalous customers and try to understand their behavior.\n# Draw side-by-side boxplots\n# Ist stack two dataframes\ndf = pd.concat([df_anomalies,df_normal])\n# Draw featurewise boxplots\nsns.boxplot(x = df['z'], y = df['y'])\nsns.boxplot(x = df['z'], y = df['x'])\nsns.boxplot(x = df['z'], y = df['p'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vi)  Use aic and bic measures to draw a scree plot and discover ideal number of clusters\nbic = []\naic = []\nfor i in range(3):\n    gm2 = GaussianMixture(\n                     n_components = i+1,\n                     n_init = 10,\n                     max_iter = 100)\n    gm2.fit(X)\n    bic.append(gm2.bic(X))\n    aic.append(gm2.aic(X))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vi)  Use aic and bic measures to draw a scree plot and discover ideal number of clusters\nfig = plt.figure()\nplt.plot([1,2,3], aic)\nplt.plot([1,2,3], bic)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t-stochaistic neighbourhood embedding\n#     Even though data is already in 2-dimension,\n#     for the sake of completion, \n#     darwing a 2-D t-sne plot and colour\n#     points by gmm-cluster labels\n\ntsne = TSNE(n_components = 3, perplexity = 30)\ntsne_out = tsne.fit_transform(X)\nplt.scatter(tsne_out[:, 0], tsne_out[:, 1],\n            marker='o',\n            s=50,              # marker size\n            linewidths=5,      # linewidth of marker edges\n            c=gm2.predict(X)   # Colour as per gmm\n            )\nplt.title('t-SNE visualization');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}