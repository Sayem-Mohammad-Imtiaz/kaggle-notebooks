{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"# Read data\n\nrow_data = pd.read_csv('../input/orbitclassification/classast - pha.csv')\nrow_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring data","metadata":{}},{"cell_type":"code","source":"# Checking for duplicate lines\n\nrow_data.duplicated().unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for empty cells in data\n\nrow_data.isnull().sum() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Сlass balance check\n\nplt.figure(figsize=(20,5))\nsns.countplot(x = row_data['class'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The classes are highly unbalanced. This must be taken into account.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Numerical features exploration\n\nnumerical_features = row_data.iloc[:, :11].columns.to_list()\n\nfor column_name in numerical_features:\n    plt.figure(figsize=(15,10))\n    sns.distplot(x = row_data[column_name])\n    plt.xlabel(column_name)\n    plt.show()\n    \n    plt.figure(figsize=(15,3))\n    sns.boxplot(x = row_data[column_name])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The boxplot shows strong outliers from a (AU), Q (AU) and P (yr).\n# Let's Explore them in more detail","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers_a_AU = row_data.loc[row_data['a (AU)'] > 15]\nprint('Orbit class with a (AU) > 15:', ', '.join([str(i) for i in outliers_a_AU['class'].unique()]))\nprint(outliers_a_AU)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All outliers belong to the same object\n# Deleted outliers object\n\nrow_data = row_data.loc[row_data['a (AU)'] < 15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Renaming the target variable\n\nrow_data['class'] = row_data['class'].replace('APO*', 1)\nrow_data['class'] = row_data['class'].replace('ATE*', 2)\nrow_data['class'] = row_data['class'].replace('AMO*', 3)\nrow_data['class'] = row_data['class'].replace('APO', 4)\nrow_data['class'] = row_data['class'].replace('IEO*', 5)\nrow_data['class'] = row_data['class'].replace('ATE', 6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features correlation exploration\n\n# Pearson correlation\nplt.figure(figsize=(10,8))\ncorr = row_data.corr(method='pearson')\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, fmt= '.2f', cmap='RdBu', mask=mask)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spearman correlation\nplt.figure(figsize=(10,8))\ncorr = row_data.corr(method='spearman')\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, fmt= '.2f', cmap='RdBu', mask=mask)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The data show a strong correlation between a(AU) and Q(AU); a(AU) and P(yr);\n# Q(AU) and P(yr); e and Q(AU).\n\n# The correlation between a(AU) (Semi-major axis) and Q(AU) (Aphelion distance) is explained by\n# the fact that the semi-major axis (a) is the average of the aphelion (Q) and \n# perihelion (q) distances. Also Q can be calculated from Q = a(1+e).\n\n# The correlation between a(AU) (Semi-major axis) and P(yr) (Orbital period) is explained by \n# the fact that they are related by the relationship P = 2*Pi*sqrt(a^3/μ)\n\n# The correlation between Q(AU) (Aphelion distance) and P(yr) (Orbital period) is explained \n# by the fact that Q(AU) (as I wrote above) is related to a(AU), which is related to P(yr)\n\n# The correlation between e (Eccentricity) and Q(AU) (Aphelion distance) is explained by \n# the fact that they are related by the relationship Q = a(1+e).\n\n# From all of the above, it follows that in the work you can ignore such parameters as \n# P(yr) (Orbital period) and Q(AU) (Aphelion distance). Both of these parameters can \n# be calculated from a(AU) (Semi-major axis) and e (Eccentricity). The same can be \n# said about q(AU) (Perihelion), but it does not show a strong correlation with other \n# parameters, so it can be written.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing columns P(yr) and Q(AU)\n\nrow_data.drop(['Q (AU)', 'P (yr)'], axis=1, inplace=True)\nrow_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing data","metadata":{}},{"cell_type":"code","source":"data_X = row_data.iloc[:, :9]  # data without target variable\ndata_y = row_data['class']  # target variable\n\n# Balanced data\noversample = SMOTE(k_neighbors = 4)\ndata_X_balanced, data_y_balanced = oversample.fit_resample(data_X, data_y.ravel())\n\n# Split data in to train and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data_X_balanced, data_y_balanced, test_size=0.33, random_state=42, stratify=data_y_balanced)\n\n# Scaling data\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying out different models using kFold cross-validation","metadata":{}},{"cell_type":"code","source":"models = []\n\nmodels.append(('KNN',KNeighborsClassifier(n_jobs=-1)))\nmodels.append(('LR',LogisticRegression(random_state=42,n_jobs=-1)))\nmodels.append(('DT',DecisionTreeClassifier(random_state=42)))\nmodels.append(('Bag_DT',BaggingClassifier(DecisionTreeClassifier(random_state=42), random_state=42, n_jobs=-1)))\nmodels.append(('RF',RandomForestClassifier(random_state=42, n_jobs=-1)))\nmodels.append(('GBC',GradientBoostingClassifier(random_state=42)))\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor name, model in models:\n    scores = cross_val_score(model, X_train, y_train, scoring='f1_weighted', cv=kf, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean F1 {round(accuracy, 3)} STD:({round(std, 3)})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The best results were shown by DecisionTreeClassifier, BaggingClassifier,\n# RandomForestClassifier and GradientBoostingClassifier.\n# Let's check them on the test set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try DecisionTreeClassifier on test set\n\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train, y_train)\ny_predicted = dt.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try BaggingClassifier on test set\n\nbg = BaggingClassifier(DecisionTreeClassifier(random_state=42))\nbg.fit(X_train, y_train)\ny_predicted_bg = bg.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted_bg)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try RandomForestClassifier on test set\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\ny_predicted_rf = rf.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted_rf)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try GradientBoostingClassifier on test set\n\ngb = GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=50, random_state=42)\ngb.fit(X_train, y_train)\ny_predicted_gb = gb.predict(X_test)\n\n# Creating a confusion matrix\n\nconf_matix = pd.crosstab(y_test, y_predicted_gb)\n\nsns.heatmap(conf_matix, cmap='Greys', annot=True, \n            linecolor='black', square='True',\n            linewidths=0.2, xticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'),\n            yticklabels=('APO*', 'ATE*', 'AMO*', 'APO', 'IEO*', 'ATE'))\nplt.ylabel(\"Real class of orbit\")\nplt.xlabel(\"Predicted class of orbit\") \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best result on test set shown by RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}