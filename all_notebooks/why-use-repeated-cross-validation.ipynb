{"cells":[{"metadata":{"_uuid":"d01280549895cc03f0fd269b4571e5699c9643d7","_cell_guid":"02d8b1d3-fe77-43ee-bb46-ae221530463d"},"cell_type":"markdown","source":"# Why use repeated cross-validation ? #\n\nFirst of all repeated cross-validation is just repeating cross-validation multiple times where in each repetition, the folds are split in a different way. After each repetition of the cross-validation, the model assessment metric is computed (e.g. accuracy or RMSE). The scores from all repetitions are finally averaged (you can also take the median), to get a final model assessment score. This gives a more “robust” model assessment score than performing cross-validation only once, which is what I aim to demonstrate. Then I will perform hyper-parameter tuning using cross-validation. \n\nFirst I will import the data and form the feature matrix (**$X$**) and the output variable vector (**$y$**). My output variable is whether the given wine is “good wine” or “bad wine”. I define a “good wine” as one that has a quality score above 5/10. Therefore this is a classification problem.\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom numpy import random\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\n\nwine_dataset = pd.read_csv('../input/winequality-red.csv')\n\n#form feature matrix and target variable vector\nX = wine_dataset[['fixed acidity', 'volatile acidity', 'citric acid',\n       'chlorides',  'total sulfur dioxide', 'density',\n        'sulphates', 'alcohol']].values #feature matrix\n\ny = (wine_dataset['quality']>5).values.astype(int) #target variable, considers wines with quality score>5 good wines\n\n","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"57461f6ec3d1fd351b81dd812c8c15d76cde46e9","_cell_guid":"10a55232-faba-423d-ac91-91f0a180f1c4"},"cell_type":"markdown","source":"Here is my function that will perform repeated cross-validation. Note that scikit-learn has it’s own repeated cross-validation function, but I wanted to have more control over the process so I wrote my own. The function takes the data set (feature matrix $X$ and output variable vector $y$) and returns the accuracy, precision and recall scores from all 50 repetitions of cross-validation performed. The model to be used (e.g. logistic regression, SVM etc.) is also an input of the function. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"def perform_repeated_cv(X, y , model):\n    #set random seed for repeartability\n    random.seed(1)\n\n    #set the number of repetitions\n    n_reps = 50\n\n    # perform repeated cross validation\n    accuracy_scores = np.zeros(n_reps)\n    precision_scores=  np.zeros(n_reps)\n    recall_scores =  np.zeros(n_reps)\n\n    for u in range(n_reps):\n\n        #randomly shuffle the dataset\n        indices = np.arange(X.shape[0])\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = y[indices] #dataset has been randomly shuffled\n\n        #initialize vector to keep predictions from all folds of the cross-validation\n        y_predicted = np.zeros(y.shape)\n\n        #perform 10-fold cross validation\n        kf = KFold(n_splits=5 , random_state=142)\n        for train, test in kf.split(X):\n\n            #split the dataset into training and testing\n            X_train = X[train]\n            X_test = X[test]\n            y_train = y[train]\n            y_test = y[test]\n\n            #standardization\n            scaler = preprocessing.StandardScaler().fit(X_train)\n            X_train = scaler.transform(X_train)\n            X_test = scaler.transform(X_test)\n\n            #train model\n            clf = model\n            clf.fit(X_train, y_train)\n\n            #make predictions on the testing set\n            y_predicted[test] = clf.predict(X_test)\n\n        #record scores\n        accuracy_scores[u] = accuracy_score(y, y_predicted)\n        precision_scores[u] = precision_score(y, y_predicted)\n        recall_scores[u]  = recall_score(y, y_predicted)\n\n    #return all scores\n    return accuracy_scores, precision_scores, recall_scores","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"14c1cd6acd195118d8d1a0627cd46d7f3c2dcc7e","_cell_guid":"fc5f5411-58ef-4503-9fd2-7931828d2e9e"},"cell_type":"markdown","source":"Now, I will perform repeated cross-validation on this dataset using random forest classification in order to look at the variability of the model assesment scores at each repetition."},{"metadata":{"_uuid":"f0fc8854bbb7051742605942ded4cb43b2f550f4","_cell_guid":"7134ae9b-0f4b-4bcb-8b3d-fab175ae0849","trusted":true},"cell_type":"code","source":"\n\n#perform repeted CV with logistic regression\naccuracy_scores, precision_scores, recall_scores = perform_repeated_cv(X, y ,  RandomForestClassifier(n_estimators=100) )\n\n#plot results from the 50 repetitions\nfig, axes = plt.subplots(3, 1)\n\naxes[0].plot(100*accuracy_scores , color = 'xkcd:cherry' , marker = 'o')\naxes[0].set_xlabel('Repetition')\naxes[0].set_ylabel('Accuracy (%)')\naxes[0].set_facecolor((1,1,1))\naxes[0].spines['left'].set_color('black')\naxes[0].spines['right'].set_color('black')\naxes[0].spines['top'].set_color('black')\naxes[0].spines['bottom'].set_color('black')\naxes[0].spines['left'].set_linewidth(0.5)\naxes[0].spines['right'].set_linewidth(0.5)\naxes[0].spines['top'].set_linewidth(0.5)\naxes[0].spines['bottom'].set_linewidth(0.5)\naxes[0].grid(linestyle='--', linewidth='0.5', color='grey', alpha=0.5)\n\naxes[1].plot(100*precision_scores , color = 'xkcd:royal blue' , marker = 'o')\naxes[1].set_xlabel('Repetition')\naxes[1].set_ylabel('Precision(%)')\naxes[1].set_facecolor((1,1,1))\naxes[1].spines['left'].set_color('black')\naxes[1].spines['right'].set_color('black')\naxes[1].spines['top'].set_color('black')\naxes[1].spines['bottom'].set_color('black')\naxes[1].spines['left'].set_linewidth(0.5)\naxes[1].spines['right'].set_linewidth(0.5)\naxes[1].spines['top'].set_linewidth(0.5)\naxes[1].spines['bottom'].set_linewidth(0.5)\naxes[1].grid(linestyle='--', linewidth='0.5', color='grey', alpha=0.5)\n\naxes[2].plot(100*precision_scores , color = 'xkcd:emerald' , marker = 'o')\naxes[2].set_xlabel('Repetition')\naxes[2].set_ylabel('Recall (%)')\naxes[2].set_facecolor((1,1,1))\naxes[2].spines['left'].set_color('black')\naxes[2].spines['right'].set_color('black')\naxes[2].spines['top'].set_color('black')\naxes[2].spines['bottom'].set_color('black')\naxes[2].spines['left'].set_linewidth(0.5)\naxes[2].spines['right'].set_linewidth(0.5)\naxes[2].spines['top'].set_linewidth(0.5)\naxes[2].spines['bottom'].set_linewidth(0.5)\naxes[2].grid(linestyle='--', linewidth='0.5', color='grey', alpha=0.5)\n\nplt.grid(True)\nplt.tight_layout()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"63495fa3dba73da343e32393b70d3bbb8ff2c931","_cell_guid":"c384bc5e-3b32-4051-80e5-36713861e37b"},"cell_type":"markdown","source":"The scores from each repetition of the cross-validation are seen in the figure above. Accuracy, precision and recall scores vary by ~2.5% across repetitions. Taking the mean of the scores across all repetitions therefore gives a more robust measure. The increased robustness comes at the expense of increased computation complexity and running time. \n___"},{"metadata":{"_uuid":"24b16085ff80f41fd6b43c30d8d6371d98d52676","_cell_guid":"f119c09c-61b6-4bee-a199-513bcd113240"},"cell_type":"markdown","source":"# Tuning Hyper-parameters using repeated cross-validation\nNow I will demonstrate how a hyper-parameter can be tuned using repeated cross-validation. I will tune the $C$ parameter of logistic regression. To do this I will perform repeated cross-validation on a range of $C$ values and compute the accuracy at each value of the grid. I will plot accuracy vs. $C$ and decide on what $C$ to choose. I chose logistic regression because it runs quickly (compared to SVM for example). "},{"metadata":{"_uuid":"02e11636556d1b6e2a48b00cd9bcefc642610399","_cell_guid":"c2c63c5b-950f-46f7-a443-2d7b27fc5c60","trusted":false,"collapsed":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\n\n#set up the parameter sweep\nc_sweep =  np.power(10, np.linspace(-4,4,50))\n\n#perform repeated cross-validation by sweeping the parameter\naccuracy_parameter_sweep = [] # keep scores here\nstd_parameter_sweep = [] #keep parameters in here\nfor c in c_sweep:\n\n    #perform repeated cross-validation\n    accuracy_scores, precision_scores, recall_scores = perform_repeated_cv(X, y ,  LogisticRegression(C=c) )\n\n    ##append scores\n    accuracy_parameter_sweep.append(np.mean(100*accuracy_scores))\n    std_parameter_sweep.append(np.std(100*accuracy_scores))\n\n\n#plot C vs. accuracy\nplt.fill_between(c_sweep , np.array(accuracy_parameter_sweep) - np.array(std_parameter_sweep) ,\n                 np.array(accuracy_parameter_sweep) + np.array(std_parameter_sweep) , facecolor = 'xkcd:light pink', alpha=0.7)\nplt.semilogx(c_sweep,accuracy_parameter_sweep , color= 'xkcd:red' , linewidth=4)\nplt.xlabel('C')\nplt.ylabel('Accuracy (%)')\nplt.title('Logistic Regression Accuracy vs. Hyper-parameter C')\nplt.grid(True, which='both')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"685d92d5e69dbadc4119ed3787a385868200dc4a","_cell_guid":"75c961dd-2c6a-443d-9a77-38b9b35d7703"},"cell_type":"markdown","source":"At $C=11.5$, accuracy is maximized at $74.2%$. As $C$ decreases and therefore the model is more heavilty regularized, accuracy decreases.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}