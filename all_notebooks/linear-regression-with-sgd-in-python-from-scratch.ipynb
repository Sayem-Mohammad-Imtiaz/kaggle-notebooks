{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Linear Regression with Stochastic Gradient Descent\n\nThis is an experimental notebook walking through the development of a linear regression model using Stochastic Gradient Descent (SGD). The data preprocessing has been kept to the minimum and can be expanded based on one's interest. Please feel free to fork the notebook and play with the values to create a model.\n\nThanks to [Mathurin Ach√©](https://www.kaggle.com/mathurinache) for providing this dataset on automobile pricing. The dataset is fairly clean and ready to use.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/autoprice/dataset_2193_autoPrice.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the description of the dataset, the author stated that all missing values from the dataset were eliminated. ","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All features seem have similar mean and median (50% mark), and small difference between minimum and 25% mark, and small difference between minimum and 75% mark except for 'compression-ratio' and 'class' features. This indicates large number of outlies. We can plot the features as individual histograms and visually analyse the data spread.","metadata":{}},{"cell_type":"code","source":"data.hist(figsize=(15, 10))\nNone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 'compression-ratio' feature has a few extereme values. We remove the feature before further processing the data in order to avoid any bias. The feature 'symboling' will be treated as a continous variable for this exercise.","metadata":{}},{"cell_type":"code","source":"new_data = data.drop(['compression-ratio'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The feature to predict will be 'class', which represents the price of the vehicle. The rest of the independent features will be **X** and the dependent feature, a.k.a. 'class' will be **y**.","metadata":{}},{"cell_type":"code","source":"X, y = new_data.iloc[:,:-1], new_data.iloc[:,-1]\n\n# Create train and test splits\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=1551)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The values of the features belong to different ranges. In order to simplify the model, scale the values using the MinMax scaler.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Scale the training set\nX_scale = MinMaxScaler().fit(X_train)\nX_train_trans = X_scale.transform(X_train) # fit on training set and transform the data\nX_train = pd.DataFrame(X_train_trans, columns = list(X_train.columns)) # convert matrix to data frame with columns\n\ny_scale = MinMaxScaler().fit(np.array(y_train).reshape(-1, 1))\ny_train = y_scale.transform(np.array(y_train).reshape(-1, 1))\n\n# Scale the test set using the X and y scalers\nX_test_trans = X_scale.transform(X_test)\nX_test = pd.DataFrame(X_test_trans, columns = list(X_test.columns))\ny_test = y_scale.transform(np.array(y_test).reshape(-1, 1))\ny_test = y_test.flatten()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression\n\nThe formula of a  linear regression model is :<br>\n>              y_pred = bias + theta * X\n\nwhere bias is the intercept and theta is the slope/coefficient. A multi-regression model will be written as:<br>\n>              y_pred = bias + theta_0 * X_0 + theta_1 * X_1 + ... + theta_n * X_n\n\nThe value of y are dependent on the theta values. In order to achieve optimum theta values, we use Stochastic Gradient Descent (SGD). The aim of the SGD is to minimize the error function which can also be written as:<br>\n>              J = 1/(2 * m) * sum((y_pred - y)^2)\n\nIn order to minimize the function above, the theta coefficients can be calculated as the derivative of the the cost function. Using a small alpha value, calculate the coefficient (theta) values as:<br>\n>              theta = theta - alpha/m * sum((y_pred - y) * X)\n\nThis can be repeated for a considerably large number of iterations until the local minima of the cost function is met. Various values of alpha can be tested before selecting the one that is large enough to converge at a local minima but small enough to reduce the cost function with every iteration [1].","metadata":{}},{"cell_type":"markdown","source":"First we initialise the values needed to produce the model. To accomodate the bias term, we add a column of ones to the dependent data set. This column is added to both the train and test set. ","metadata":{}},{"cell_type":"code","source":"X_train = np.column_stack(([1]*X_train.shape[0], X_train)) # add a column with ones for the bias value while converting it into a matrix\nm,n = X_train.shape # rows and columns \ntheta = np.array([1] * n) # initial theta\nX = np.array(X_train) # convert X_train into a numpy matrix\ny = y_train.flatten() # convert y into an array\n\nalpha = 0.001 # alpha value \niteration = 1000 # iterations\ncost = [] # list to store cost values\ntheta_new = [] # list to store updates coeffient values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Regression function\n\nfor i in range(0, iteration):\n    pred = np.matmul(X,theta) # Calculate predicted value\n    J = 1/2 * ((np.square(pred - y)).mean()) # Calculate cost function\n   \n    t_cols = 0 # iteration for theta values\n    \n    # Update the theta values for all the features with the gradient of the cost function\n    for t_cols in range(0,n): \n        t = round(theta[t_cols] - alpha/m * sum((pred-y)*X[:,t_cols]),4) # calculate new theta value\n        theta_new.append(t) # save new theta values in a temporary array\n        \n# update theta array\n    theta = [] # empty the theta array\n    theta = theta_new # assign new values of theta to array\n    theta_new = [] # empty temporary array\n    cost.append(J) # append cost function to the cost array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.plot(cost)\nplt.title('Cost Function')\nplt.xlabel('Iterations')\nplt.ylabel('Cost Function Value')\nNone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cost[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After experimenting with different values of alpha, I chose 0.001 since it converged well enough to provide optimal theta values and a low cost function value. A suffiently smaller value of alpha will help converge at the local minima by reducing the cost function at every iteration. The Cost Function plot for alpha=0.001 above shows the error values decreasing over the iterations but converging around the local minimum at around 800 itertions, beyond which, the theta values are stabalised. We can either run the function again with 800 iterations and choose the final theta values it produces or choose those at the 1000th iteration as an outcome of this function. ","metadata":{}},{"cell_type":"code","source":"print(\"The theta values for the model are :\", theta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the first theta value (theta0) is the bias value. Using these theta values, we can predict the price of the vehicles on the test data. We used the transformed test set to predict the values.","metadata":{}},{"cell_type":"markdown","source":"With the calculated theta values, predict the price of the vehicles using the test set.","metadata":{}},{"cell_type":"code","source":"X_test = np.column_stack(([1]*X_test.shape[0], X_test)) # add a column with ones for the bias value while converting it into a matrix\ny_pred = np.matmul(X_test,theta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate the RMSE and R-square statistic (Coefficient of determination). ","metadata":{}},{"cell_type":"code","source":"import math\nfrom sklearn.metrics import r2_score\nrmse = round(math.sqrt(((y_test-y_pred)**2).mean()),3)\nr2 = round(r2_score(y_test,y_pred),3)\nprint(\"The Root Mean Square error is: \",rmse)\nprint(\"The coefficient of determination is: \", r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is good to note that the RMSE value is quite low, but a small R-square value also means the linear model isn't a good fit for the data. We can compare these metrics with sklearn's SGD Regressor model. <br><br>\nNote: Here you can choose to retain the bias variable added to the independent data sets in train and test, or you can remove them by re-running the code above to create train-test splits and scale the data before creating the model. The outcome will be the same.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\nsgd_model = SGDRegressor(loss='squared_loss', penalty='l2') # Use Ridge regularization as alpha\nmodel_fit = sgd_model.fit(X_train,y_train.flatten())\ny_pred = sgd_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate metrics for the sklearn model.","metadata":{}},{"cell_type":"code","source":"rmse = round(math.sqrt(((y_test-y_pred)**2).mean()),3)\nr2 = round(r2_score(y_test, y_pred),3)\nprint(\"The average error (RMSE) is: \",rmse)\nprint(\"The coefficient of determination is: \", r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the RMSE score of the scikit-learn model same as the SGD model, the R-square value is slighty different. The R-squared values shows that the model created from scatch captures the same amount of variance of the dependent variable ('class' a.k.a. price of the vehicle) as does the in-built model, although the value is small. This means a linear model may not be fit for this data. The model is underfitting the data, thus giving a small value for R-square. Using a polynomial regression model could perhaps improve the model fit, or a more complex model like Random Forest or Neural Network.<br>\n\nAlthough the R-square metric isn't great, it proves that our model is as good as the in-built one. Using the same technique and process, a linear regression model with stochastic gradient descent can be developed from scratch for any a linear dataset. ","metadata":{}},{"cell_type":"markdown","source":"## References\n[1] https://www.coursera.org/learn/machine-learning","metadata":{}}]}