{"cells":[{"metadata":{},"cell_type":"markdown","source":"# İçindekiler\n1. [Veri Setinin Yüklenmesi](#1)\n2. [Veri Ön İşleme](#2)   \n3. [Veri Setinin Görselleştirilmesi](#3)\n4. [Modelin Test ve Eğitim Kümelerine Ayrılması](#4)\n5. [Sınıflandırma Algoritmalarının Denenmesi](#5)\n6. [Sonuç](#6)\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id= \"1\"> </a><br>\n## Veri Setini Yüklüyoruz"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/siber-zorbalk/tweetset.csv\",encoding=\"windows-1254\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Veri setinin içerisindeki özniteliklere bakacak olursak \"Tip\" ve \"Paylaşım\" olmak üzere 2 öznitelik vardır.\n\nBu özniteliklerin haricinde \"NaN\" yani boş değerlerle dolu olan öznitelikleri silmemiz gerekiyor .\n\n**Paylaşım :** Sosyal medya platformu olan Tweetter'da kullanıcıların yaptıkları paylaşımların içeriklerini göstermektedir.\n\n**Tip :** Atılan tweet lerin siber zorbalık ifade edip etmediğinin etiketlendiği öz niteliktir. \"Pozitif\",\"Negatif\""},{"metadata":{},"cell_type":"markdown","source":"<a id= \"2\"> </a><br>\n# Veri Ön İşleme"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Veri setinde kayıp verilerin olup olmadığına bakıyoruz ve düzeltilemeyecek kadar olan feature'leri siliyoruz\nprint(\"Kayıp Veriler :{}\".format(df.isnull().sum()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\",\"Unnamed: 5\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label encoder işlemi yaparak veri seti içerisinde bulunan \"Negatif\" değerli 0 \"Pozitif\" değerleri ise 1 yapıyoruz.\ndf[\"sınıf\"] = [0 if (i==\"Negatif\") else 1 for i in df[\"Tip\"]]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.stem.porter import PorterStemmer\nimport re\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from snowballstemmer import TurkishStemmer\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punctation = string.punctuation\n#punctuation ='''!()-[]{};':'\"\\,<>./?@#$%^&*_~'''\n#Özel karakterleri temizleme\ndef ozelkarakter_temizleme (metin):\n    return metin.translate(str.maketrans(\"\",\"\",punctation))\n\n#--------------------\n#Stopword temizleme\n\n#stopword = set(stopwords.words(\"turkish\"))\nstopword = \"acaba, ama, ancak, artık, asla, aslında, az,bana, bazen, bazı, bazıları, bazısı, belki, ben, beni, benim, beş, bile, bir, birçoğu, birçok, birçokları, biri, birisi, birkaç, birkaçı, birşey, birşeyi, biz, bize, bizi, bizim, böyle, böylece, bu, buna, bunda, bundan, bunu, bunun, burada, bütün,çoğu, çoğuna, çoğunu, çok, çünkü,da, daha, de, değil, demek, diğer, diğeri, diğerleri, diye, dolayı,elbette, en,fakat, falan, felan, filan, gene, gibi,hangi, hangisi, hani, hatta, hem, henüz, hep, hepsi, hepsine, hepsini, her, her biri, herkes, herkese, herkesi, hiç, hiç kimse, hiçbiri, hiçbirine, hiçbirini,için, içinde, ile, ise, işte,kaç, kadar, kendi, kendine, kendini, ki, kim, kime, kimi, kimin, kimisi,madem, mı, mi, mu, mü,nasıl, ne, ne kadar, ne zaman, neden, nedir, nerde, nerede, nereden, nereye, nesi, neyse, niçin, niye,ona, ondan, onlar, onlara, onlardan, onların, onu, onun, orada, oysa, oysaki,öbürü, ön, önce, ötürü, öyle, sana, sen, senden, seni, senin, siz, sizden, size, sizi, sizin, son, sonra, seobilog,şayet, şey, şimdi, şöyle, şu, şuna, şunda, şundan, şunlar, şunu, şunun,tabi, tamam, tüm, tümü, üzere,var, ve, veya, veyahut,ya, ya da, yani, yerine, yine, yoksa,zaten, zira\"\n\ndef stopwords_temizleme (metin):\n    return \" \".join([kelime for kelime in str(metin).split() if kelime not in stopword])\n\n#-------------------\ncount = Counter()\n\n#sık kullanılan kelimeleri temizleme\nfor metin in df[\"Paylaşım\"].values:\n    for kelime in metin.split():\n        count[kelime] += 1\ncount.most_common(10) # en sık tekrar eden 10 kelimeyi gösterir\nfrekans = set([i for (i,j) in count.most_common(15)])\nnadir = 15\nnadir_kelime = set([i for (i,j) in count.most_common()[:-nadir-1:-1]])\ndef frekans_sil(metin):\n    return \" \".join([kelime for kelime in str(metin).split() if kelime not in frekans])\n\n\n#----------------Kelime Kökünü Alma\n#lemma = WordNetLemmatizer(\"turkish\")\n#Lemmatizer\n\n#def kelime_kök_alma (metin):\n#    return \" \".join([lemma.lemmatize(kelime) for kelime in metin.split()])\n \n    \nsnowBallStememr = TurkishStemmer()\ndef kelime_kök_alma(metin):\n    wordlist = nltk.word_tokenize(metin)\n    stemWords = [snowBallStememr.stemWord(kelime) for kelime in wordlist]\n    return \" \".join(stemWords)\n    \n    \n    \n#---------\n#Emojileri Silme\n\ndef emoji_silme (metin):\n    emoji = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  \n                               u\"\\U0001F300-\\U0001F5FF\"                                 \n                               u\"\\U0001F680-\\U0001F6FF\"  \n                               u\"\\U0001F1E0-\\U0001F1FF\"  \n                               u\"\\U00002500-\\U00002BEF\"                                 \n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  \n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji.sub(r\"\",metin)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Paylaşım\"] = df[\"Paylaşım\"].str.lower()\ndf[\"ozel_karaktersiz\"] = df[\"Paylaşım\"].apply(lambda metin : ozelkarakter_temizleme(metin))\ndf[\"stop_word\"] = df[\"ozel_karaktersiz\"].apply(lambda metin : stopwords_temizleme(metin) )\ndf[\"sık_kullanılan\"] = df[\"stop_word\"].apply(lambda metin : frekans_sil(metin) )\ndf[\"kelime_kok\"] = df[\"sık_kullanılan\"].apply(lambda kelime : kelime_kök_alma(kelime))\ndf[\"emojisiz\"] = df[\"kelime_kok\"].apply(lambda metin : emoji_silme(metin))\ndf.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Veri seti üzerinde gerçekleşen değişimleri gözlemleyebilmek için her bir işlemi farklı feature'ler oluşturarak yapmıştık.\n# Şimdi ise işimize yaramayacak olan feature'leri siliyoruz \ndf.drop([\"Paylaşım\",\"ozel_karaktersiz\",\"stop_word\",\"sık_kullanılan\",\"kelime_kok\"],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id= \"3\"> </a><br>\n## Veri Setinin Görselleştirilmesi"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9,9))\nsorted_counts = df['sınıf'].value_counts()\nplt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n       autopct='%1.1f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n        colors = sns.color_palette(\"Paired\")[7:])\nplt.text(x = -0.35, y = 0, s = 'Toplam Paylaşım: {}'.format(df.shape[0]))\nplt.title('Veri Setindeki Paylaşımların Dağılımları', fontsize = 16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsns.barplot(x=[1,0],y = df[\"sınıf\"].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"karakter_len\"]= df[\"emojisiz\"].apply(len)\nplt.figure(\"0-1 histogram grafiği\")\n\nsns.distplot(df[df[\"sınıf\"]==0][\"karakter_len\"].values,bins=20 , label = \" Negatif değerlerinin histogram\")\n\nsns.distplot(df[df[\"sınıf\"]==1][\"karakter_len\"].values,bins = 20 ,label=\"Pozitif değerlerin Histogram\")\n\nplt.xlabel(\"Karakter Uzunluğu\")\nplt.ylabel(\"Frekans(Yoğunluk)\")\nplt.legend(loc=\"best\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"kelime\"] = df[\"emojisiz\"].apply(lambda x : len(x.split()))\n\nplt.figure(\"kelimelerin 0 ve 1 değerlerine göre kda sı\")\n\nsns.distplot(df[df[\"sınıf\"]==0][\"kelime\"].values,bins=20,label=\" 0 değeri için hist\")\nsns.distplot(df[df[\"sınıf\"]==1][\"kelime\"].values,bins=20,label=\"1 değeri için hist\")\n\nplt.xlabel(\"Kelimee Uzunlukları\")\nplt.ylabel(\"Frekans (yoğunluk)\")\nplt.legend(loc=\"best\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metin = df.emojisiz.tolist()\nmetin_kombin = \" \".join(metin)\nplt.figure(figsize=(14,14))\nplt.imshow(WordCloud().generate(metin_kombin))\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pozitif sınıflandırıcının görselleştirilmesi\n\npozitif = df.emojisiz[df.sınıf == 1]\n\npozitif_metin = pozitif.tolist()\npozitif_metin_ekleme = \" \".join(pozitif_metin)\nplt.figure(figsize=(14,14))\nplt.imshow(WordCloud().generate(pozitif_metin_ekleme))\nplt.axis(\"off\")\nplt.title(\"Pozitif Metin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negatif Metin içindeki en çok kullanılan kelimeler\n\nnegatif = df.emojisiz[df.sınıf==0]\nnegatif_metin= negatif.tolist()\nnegatif_metin_ekle = \" \".join(negatif_metin)\nplt.figure(figsize=(14,14))\nplt.imshow(WordCloud().generate(negatif_metin_ekle))\nplt.axis(\"off\")\nplt.title(\"Negatif Metnin Dağılımı\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id= \"4\"> </a><br>\n## Modelin Test ve Eğitim Kümelerine Ayrılması"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = int(len(df)*0.8)\ntest_size = int(len(df)-train_size)\n\nprint(\"Eğitim Boyuyu=\" ,train_size)\nprint(\"Test Boyuyu=\" ,test_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_split (df,train_size):\n    train = df[:train_size]\n    test= df[train_size:]\n    return train,test\ntrain_y,test_y = df_split(df[\"sınıf\"],train_size)\ntrain_x,test_x = df_split(df[\"emojisiz\"],train_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id= \"5\"> </a><br>\n# Sınıflandırma Algoritmalarının Denenmesi"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport joblib\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score\nfrom sklearn.model_selection import KFold\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef k_fold (vectorizer, model, data, name):\n   \n    pipeline = Pipeline([('vect', vectorizer),\n                         ('chi', SelectKBest(chi2, k=\"all\")),\n                         ('clf', model)])\n    kf = KFold(n_splits=11, shuffle=True)\n    scores = []\n    \n    for train_index, test_index in kf.split(df):\n        train_text = df.iloc[train_index]['emojisiz'].values.astype('U')\n        train_y = df.iloc[train_index]['sınıf'].values.astype('U')\n\n        test_text = df.iloc[test_index]['emojisiz'].values.astype('U')\n        test_y = df.iloc[test_index]['sınıf'].values.astype('U')\n\n        model = pipeline.fit(train_text, train_y)\n        predictions = model.predict(test_text)\n        with warnings.catch_warnings():\n            warnings.filterwarnings(action='once')\n            score = accuracy_score(test_y, predictions)\n            print(score)\n            scores.append(score)\n            skor=str(sum(scores)/len(scores))\n            \n    \n        return skor\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_result=[]\nstop_words = set(stopwords.words('turkish'))\nvectorizer = TfidfVectorizer(min_df=10, max_df=0.95, sublinear_tf=True, norm='l2',ngram_range=(1, 3), encoding='windows-1254', stop_words=stop_words, analyzer='word')\nmodels = [('LogisticRegression', LogisticRegression(solver='newton-cg', multi_class='multinomial')),\n          ('SVC', SVC(kernel = \"rbf\")),\n          ('SGDClassifier', SGDClassifier(tol=1e-3, penalty='l2')),\n          (\"MultinomialNB\",MultinomialNB()),\n          (\"KNeighborsClassifier\",KNeighborsClassifier()),\n          (\"RandomForestClassifier\",RandomForestClassifier (random_state=16)),\n          (\"DecisionTreeClassifier\",DecisionTreeClassifier(random_state=16)),\n          (\"AdaBoostClassifier\",AdaBoostClassifier()),\n          (\"Bagging Classifier\",BaggingClassifier())\n         ]\nsonuc = {}\nfor name, model in models:\n        \n        sonuc.update({name : k_fold(vectorizer, model, df, name)}) \n        print(\"***************************** \\n\",sonuc)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Denediğimi algoritmaları bir sonuc adlı Dictionary değişkenine kaydetmiştik. \n# Burada ise bu sözlüğü bir data frame dönüşütürüyoruz\nsozluk = list(sonuc.items())\narray_dic = np.array(sozluk)\nprint(\"Dizi Şekli*** \\n\",array_dic,\"\\n\")\nvisualDF = pd.DataFrame ( data = array_dic ,columns= [\"Model Adı\",\"Başarım Oranı\"])\nvisualDF.head(9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id= \"6\"> </a><br>\n# Sonuç"},{"metadata":{},"cell_type":"markdown","source":"9 farklı makine öğrenmesi algortiması kullanılarak sınıflandırma işlemi gerçekleştirilmiş ve başarım oranı en yüksek algoritma şekilde verilmiştir."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ng = sns.barplot(\"Başarım Oranı\",\"Model Adı\",data=visualDF)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def basarım (sonuc):\n    max_basarım = max(sonuc,key = sonuc.get)\n    return (max_basarım)\nprint(\"Yapmış olduğumuz 9 farklı makine öğrenmesi algoritması arasında en başarılı sonucu veren algoritma = {} ' dir\".format(basarım(sonuc)))    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}