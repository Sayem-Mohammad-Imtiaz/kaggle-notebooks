{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Reference from**\n> https://machinelearningmastery.com/imbalanced-classification-is-hard/"},{"metadata":{},"cell_type":"markdown","source":"> ******In this project, I will use a small breast cancer survival dataset, referred to generally as the\nHaberman Dataset. The dataset describes breast cancer patient data and the outcome is patient\nsurvival. Specifically whether the patient survived for five years or longer, or whether the\npatient did not survive. This is a standard dataset used in the study of imbalanced classification.\nAccording to the dataset description, the breast cancer surgery operations were conducted\nbetween 1958 and 1970 at the University of Chicago’s Billings Hospital. There are 306 examples\nin the dataset, and there are 3 input variables.**"},{"metadata":{},"cell_type":"markdown","source":"> **My Goal is- given patient breast cancer surgery details, what is the probability of\nsurvival of the patient to five years or more?**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# load the Haberman Breast Cancer Survival dataset\nhaberman = pd.read_csv(\"../input/haberman.csv/haberman.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# peek of the dataset\nhaberman.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ****Here-\n age: The age of the patient at the time of the operation.\n year: The two-digit year of the operation.\n nodes: The number of positive axillary nodes detected, a measure of a cancer has spread."},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of the dataset\nhaberman.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize each column\nhaberman.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Looking at the age, we can see that the youngest patient was 30 and the oldest was 78; that is quite a range. The mean patient age was about 52 years. If the occurrence of cancer is somewhat random, we might expect the age distribution to be Gaussian. We can see that all operations were performed between 1958 and 1969. If the number of breast cancer patients is somewhat fixed over time, we might expect this variable to have a uniform distribution. We can see nodes have values between 0 and 52. This might be a cancer diagnostic related to lymphatic nodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data types\nhaberman.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> All variables are integers. Therefore, it might be helpful to look at each variable as a\nhistogram to get an idea of the variable distribution. This might be helpful in case we choose\nmodels later that are sensitive to the data distribution or scale of the data, in which case, we\nmight need to transform or rescale the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create histograms of each variable\nfrom matplotlib import pyplot\nhaberman.hist()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that age appears\nto have a Gaussian distribution, as we might have expected. We can also see that year has\na uniform distribution, mostly, with an outlier in the first year showing nearly double the\nnumber of operations. We can see nodes has an exponential type distribution with perhaps most\nexamples showing 0 nodes, with a long tail of values after that. A transform to un-bunch this\ndistribution might help some models later on. Finally, we can see the two-class values with an\nunequal class distribution, showing perhaps 2 or 3 times more survival than non-survival cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how imbalanced the dataset actually is\nfrom collections import Counter\n# summarize the class distribution\ntarget = haberman['status'].values\ncounter = Counter(target)\nfor k,v in counter.items():\n    per = v / len(target) * 100\n    print('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that\nclass 1 for survival has the most examples at 225, or about 74 percent of the dataset.\nWe can also see class 2 for non-survival has fewer examples as 80, or about 26 percent of the dataset. The\nclass distribution is skewed, but it is not severely imbalanced"},{"metadata":{},"cell_type":"markdown","source":"**It is customary for an imbalanced dataset to model the minority class as a positive class. In\nthis dataset, the positive class represents non-survival. This means that we will be predicting\nthe probability of non-survival and will need to calculate the complement of the predicted\nprobability in order to get the probability of survival. As such, we can map the 1 class values\n(survival) to the negative case with a 0 class label, and the 2 class values (non-survival) to the\npositive case with a class label of 1.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve numpy array\nhaberman = haberman.values\n# split into input and output elements\nX, y = haberman[:, :-1], haberman[:, -1]\n\n# label encode the target variable to have the classes 0 and 1\nfrom sklearn.preprocessing import LabelEncoder\ny = LabelEncoder().fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> I will evaluate candidate models using repeated stratified k-fold cross-validation. The k-fold\ncross-validation procedure provides a good general estimate of model performance that is not\ntoo optimistically biased, at least compared to a single train-test split. We will use k = 10,\nmeaning each fold will contain 306 10 or about 30 examples.\nStratified means that each fold will contain the same mixture of examples by class, that is\nabout 74 percent to 26 percent survival and non-survival. Repeated means that the evaluation\nprocess will be performed multiple times to help avoid fluke results and better capture the\nvariance of the chosen model. I will use three repeats. This means a single model will be\nfit and evaluated 10 × 3 (30) times and the mean and standard deviation of these runs will be\nreported."},{"metadata":{},"cell_type":"markdown","source":"> Given that we are interested in predicting a probability of survival, we need a performance\nmetric that evaluates the skill of a model based on the predicted probabilities. In this case,\nwe will use the Brier score that calculates the mean squared error between the predicted\nprobabilities and the expected probabilities."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import brier_score_loss\nfrom numpy import mean\nfrom numpy import std\n# calculate brier skill score (BSS)\ndef brier_skill_score(y_true, y_prob):\n    # calculate reference brier score\n    ref_probs = [0.26471 for _ in range(len(y_true))]\n    bs_ref = brier_score_loss(y_true, ref_probs)\n    # calculate model brier score\n    bs_model = brier_score_loss(y_true, y_prob)\n    # calculate skill score\n    return 1.0 - (bs_model / bs_ref)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Next, we can make use of the brier_skill_score() function to evaluate a model using\nrepeated stratified k-fold cross-validation. To use our custom performance metric, we can\nuse the make scorer() scikit-learn function that takes the name of our custom function and\ncreates a metric that we can use to evaluate models with the scikit-learn API.\nWe will set the\nneeds proba argument to True to ensure that models that are evaluated make predictions using\nthe predict proba() function to ensure they give probabilities instead of class labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n# evaluate a model\ndef evaluate_model(X, y, model):\n    # define evaluation procedure\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # define the model evaluation metric\n    metric = make_scorer(brier_skill_score, needs_proba=True)\n    # evaluate model\n    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the loaded dataset\nprint(X.shape, y.shape, Counter(y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> we will evaluate the baseline strategy of predicting the distribution of positive\nexamples in the training set as the probability of each case in the test set. This can be\nimplemented automatically using the DummyClassifier class and setting the strategy to\n‘prior’ that will predict the prior probability of each class in the training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n# define the reference model\nmodel = DummyClassifier(strategy='prior')\n# evaluate the model\nscores = evaluate_model(X, y, model)\nprint('Mean BSS: %.3f (%.3f)' % (mean(scores), std(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> I will evaluate a suite of models that are known to be effective at predicting probabilities.\nSpecifically, these are models that are fit under a probabilistic framework and explicitly predict a\ncalibrated probability for each example.\nI will\ncompare each algorithm based on the mean score, as well as based on their distribution of scores.\nI can define a function to create models that I want to evaluate, each with their default\nconfiguration or configured as to not produce a warning."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define models to test\ndef get_models():\n    models, names = list(), list()\n    # LR\n    models.append(LogisticRegression(solver='lbfgs'))\n    names.append('LR')\n    # LDA\n    models.append(LinearDiscriminantAnalysis())\n    names.append('LDA')\n    # QDA\n    models.append(QuadraticDiscriminantAnalysis())\n    names.append('QDA')\n    # GNB\n    models.append(GaussianNB())\n    names.append('GNB')\n    # MNB\n    models.append(MultinomialNB())\n    names.append('MNB')\n    # GPC\n    models.append(GaussianProcessClassifier())\n    names.append('GPC')\n    return models, names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Running the below cell will first summarizes the mean and standard deviation of the BSS for each\nalgorithm (larger scores is better)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define models\nmodels, names = get_models()\nresults = list()\n# evaluate each model\nfor i in range(len(models)):\n    # evaluate the model and store results\n    scores = evaluate_model(X, y, models[i])\n    results.append(scores)\n    # summarize and store\n    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n    # plot the results\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> In my case, the results suggest that only two of the algorithms are not skillful, showing\nnegative scores, and that perhaps the LR and LDA algorithms are the best performing."},{"metadata":{},"cell_type":"markdown","source":"**It can be a good practice to scale data for some algorithms if the variables have different units\nof measure, as they do in this case. Algorithms like the LR and LDA are sensitive to the\ndistribution of the data and assume a Gaussian distribution for the input variables, which we\ndon’t have in all cases.\nNevertheless, we can test the algorithms with standardization, where each variable is shifted\nto a zero mean and unit standard deviation. We will drop the MNB algorithm as it does not\nsupport negative input values. We can achieve this by wrapping each model in a Pipeline\nwhere the first step is a StandardScaler, which will correctly be fit on the training dataset and\napplied to the test dataset within each k-fold cross-validation evaluation, preventing any data\nleakage.\n**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define models to test\ndef get_models():\n    models, names = list(), list()\n    # LR\n    models.append(LogisticRegression(solver='lbfgs'))\n    names.append('LR')\n    # LDA\n    models.append(LinearDiscriminantAnalysis())\n    names.append('LDA')\n    # QDA\n    models.append(QuadraticDiscriminantAnalysis())\n    names.append('QDA')\n    # GNB\n    models.append(GaussianNB())\n    names.append('GNB')\n    # GPC\n    models.append(GaussianProcessClassifier())\n    names.append('GPC')\n    return models, names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define models\nmodels, names = get_models()\nresults = list()\n# evaluate each model\nfor i in range(len(models)):\n    # create a pipeline\n    pipeline = Pipeline(steps=[('t', StandardScaler()),('m',models[i])])\n    # evaluate the model and store results\n    scores = evaluate_model(X, y, pipeline)\n    results.append(scores)\n    # summarize and store\n    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n# plot the results\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> we can see that the standardization has not had much of an impact on the\nalgorithms, except the GPC. The performance of the GPC with standardization has shot up\nand is now the best-performing technique. "},{"metadata":{},"cell_type":"markdown","source":"** Model Evaluation With Power Transform**\n> Power transforms, such as the Box-Cox and Yeo-Johnson transforms, are designed to change\nthe distribution to be more Gaussian. We can use the\nPowerTransformer scikit-learn class to perform the Yeo-Johnson transform and automatically\ndetermine the best parameters to apply based on the dataset. Importantly, this transformer will also standardize the dataset as part\nof the transform"},{"metadata":{},"cell_type":"markdown","source":"> We have zero values in our dataset, therefore we will scale the dataset prior to the power\ntransform using a MinMaxScaler. Again, we can use this transform in a Pipeline to ensure it\nis fit on the training dataset and applied to the train and test datasets correctly, without data\nleakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define models to test\ndef get_models():\n    models, names = list(), list()\n    # LR\n    models.append(LogisticRegression(solver='lbfgs'))\n    names.append('LR')\n    # LDA\n    models.append(LinearDiscriminantAnalysis())\n    names.append('LDA')\n    # GPC\n    models.append(GaussianProcessClassifier())\n    names.append('GPC')\n    return models, names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define models\nmodels, names = get_models()\nresults = list()\n# evaluate each model\nfor i in range(len(models)):\n    # create a pipeline\n    steps = [('t1', MinMaxScaler()), ('t2', PowerTransformer()),('m',models[i])]\n    pipeline = Pipeline(steps=steps)\n    # evaluate the model and store results\n    scores = evaluate_model(X, y, pipeline)\n    results.append(scores)\n    # summarize and store\n    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n# plot the results\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see a further lift in model skill for the three models that were evaluated.\nWe can see that the LR appears to have out-performed the other two methods**"},{"metadata":{},"cell_type":"markdown","source":"**Make Prediction on New Data**\n> We will select the Logistic Regression model with a power transform on the input data as our\nfinal model. We can define and fit this model on the entire training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nsteps = [('t1', MinMaxScaler()),('t2', PowerTransformer()),('m',LogisticRegression(solver='lbfgs'))]\nmodel = Pipeline(steps=steps)\nmodel.fit(X, y)\n# some survival cases\nprint('Survival Cases:')\ndata = [[31,59,2], [31,65,4], [34,60,1]]\nfor row in data:\n    # make prediction\n    yhat = model.predict_proba([row])\n    # get percentage of survival\n    p_survive = yhat[0, 0] * 100\n    # summarize\n    print('>data=%s, Survival=%.3f%%' % (row, p_survive))\n# some non-survival cases\nprint('Non-Survival Cases:')\ndata = [[44,64,6], [34,66,9], [38,69,21]]\nfor row in data:\n    # make prediction\n    yhat = model.predict_proba([row])\n    # get percentage of survival\n    p_survive = yhat[0, 0] * 100\n    # summarize\n    print('>data=%s, Survival=%.3f%%' % (row, p_survive))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**> We can see that for the chosen survival cases, the probability of survival was\nhigh, between 76 percent and 86 percent. Then some cases of non-survival are used as input to\nthe model and the probability of survival is predicted. As we might have hoped, the probability\nof non-survival is modest, hovering around 52 percent to 63 percent.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}