{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data****","metadata":{}},{"cell_type":"markdown","source":"\nTRAIN DATA\nVariable Definition ID Unique Identifier for a row City_Code Code for the City of the customers Region_Code Code for the Region of the customers Accomodation_Type Customer Owns or Rents the house Reco_Insurance_Type Joint or Individual type for the recommended insurance\nUpper_Age Maximum age of the customer Lower _Age Minimum age of the customer Is_Spouse If the customers are married to each other (in case of joint insurance) Health_Indicator Encoded values for health of the customer Holding_Policy_Duration Duration (in years) of holding policy (a policy that customer has already subscribed to with the company) Holding_Policy_Type Type of holding policy Reco_Policy_Cat Encoded value for recommended health insurance Reco_Policy_Premium Annual Premium (INR) for the recommended health insurance Response (Target) 0 : Customer did not show interest in the recommended policy, 1 : Customer showed interest in the recommended policy","metadata":{}},{"cell_type":"markdown","source":"TEST DATA\nVariable Definition ID Unique Identifier for a row City_Code Code for the City of the customers Region_Code Code for the Region of the customers Accomodation_Type Customer Owns or Rents the house Reco_Insurance_Type Joint or Individual type for the recommended insurance Upper_Age Maximum age of the customer Lower _Age Minimum age of the customer Is_Spouse If the customers are married to each other (in case of joint insurance) Health_Indicator Encoded values for health of the customer Holding_Policy_Duration Duration (in years) of holding policy (a policy that customer has already subscribed to with the company) Holding_Policy_Type Type of holding policy Reco_Policy_Cat Encoded value for recommended health insurance Reco_Policy_Premium Annual Premium (INR) for the recommended health insurance","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport pylab as py\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, precision_score, recall_score, precision_recall_curve\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.filterwarnings('ignore')\nsns.set(style = 'white')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/health-insurance/train_Df64byy.csv')\ndf_test = pd.read_csv('../input/health-insurance/test_YCcRUnU.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns, df_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the variables like 'city_code', 'Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse', 'Health Indicator' are object type datatype. This means python in not able to recognise the data type of these variables. So, we will have to find their right data type and assign them a new data type. #Also many of the variables are asssigned wrong datatype. This should also be solved","metadata":{}},{"cell_type":"code","source":"df_train['City_Code'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Accomodation_Type'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Reco_Insurance_Type'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Is_Spouse'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Health Indicator'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Holding_Policy_Duration'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Holding_Policy_Type'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Reco_Policy_Cat'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Response'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analysis: \nCity_Code is a categorical variable which has 36 different categories. So first we will use 'combine sparse technique and combine all sparse variables and also will convert its data type to category data type. ##Accomodation type has also two categoris. Therefore we will convert its data type to category. ##Reco_Insurance_Type and Is spouse is also a categorical datatype. ##Health Indicator too has many categories. Therefore we will first combine parse classes and convert it to category. ##Holding_Policy_Type will also be converted to categorical datatype.\n\nAlso we will generate new columns from Reco_Policy_Cat and Holding Policy Duration","metadata":{}},{"cell_type":"code","source":"fil =   (df_train['City_Code'] == 'C5') |(df_train['City_Code'] == 'C6') | (df_train['City_Code'] == 'C7') | (df_train['City_Code'] == 'C8')| (df_train['City_Code'] == 'C10') | (df_train['City_Code'] == 'C11') | (df_train['City_Code'] == 'C12') | (df_train['City_Code'] == 'C13') | (df_train['City_Code'] == 'C14') | (df_train['City_Code'] == 'C15') | (df_train['City_Code'] == 'C16' )| (df_train['City_Code']== 'C17') | (df_train['City_Code'] == 'C18') | (df_train['City_Code']== 'C19') | (df_train['City_Code'] == 'C20') | (df_train['City_Code'] == 'C21') | (df_train['City_Code'] == 'C22') | (df_train['City_Code'] == 'C23') | (df_train['City_Code'] == 'C24') | (df_train['City_Code'] == 'C25') | (df_train['City_Code'] == 'C26') | (df_train['City_Code'] == 'C27') | (df_train['City_Code'] == 'C28') | (df_train['City_Code']== 'C29') | (df_train['City_Code'] == 'C30') | (df_train['City_Code'] == 'C31') | (df_train['City_Code'] == 'C32') | (df_train['City_Code'] == 'C33') | (df_train['City_Code'] == 'C34') | (df_train['City_Code'] == 'C35') | (df_train['City_Code'] == 'C36')\ndf_train.loc[fil, 'City_Code'] = 'C4'\ndf_train['City_Code'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fil = (df_train['Health Indicator'] == 'X6') | (df_train['Health Indicator'] == 'X7') | (df_train['Health Indicator'] == 'X8') | (df_train['Health Indicator'] == 'X9') \ndf_train.loc[fil, 'Health Indicator'] = 'X5'\ndf_train['Health Indicator'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Holding_Policy_Duration'] = np.where(df_train['Holding_Policy_Duration'] == '14+', 15,df_train['Holding_Policy_Duration'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['Holding_Policy_Duration'] = np.where(df_test['Holding_Policy_Duration'] == '14+', 15,df_test['Holding_Policy_Duration'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_train['City_Code'] = df_train['City_Code'].astype('category')\ndf_train['Accomodation_Type'] = df_train['Accomodation_Type'].astype('category')\ndf_train['Reco_Insurance_Type'] = df_train['Reco_Insurance_Type'].astype('category')\ndf_train['Is_Spouse'] = df_train['Is_Spouse'].astype('category')\ndf_train['Health Indicator'] = df_train['Health Indicator'].astype('category')\ndf_train['Holding_Policy_Duration'] = df_train['Holding_Policy_Duration'].astype('float')\ndf_train['Holding_Policy_Type'] = df_train['Holding_Policy_Type'].astype('category')\ndf_train['Response'] = df_train['Response'].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Same thing will we done for test dataset.\nfil =   (df_test['City_Code'] == 'C5') |(df_test['City_Code'] == 'C6') | (df_test['City_Code'] == 'C7') | (df_test['City_Code'] == 'C8')| (df_test['City_Code'] == 'C10') | (df_test['City_Code'] == 'C11') | (df_test['City_Code'] == 'C12') | (df_test['City_Code'] == 'C13') | (df_test['City_Code'] == 'C14') | (df_test['City_Code'] == 'C15') | (df_test['City_Code'] == 'C16' )| (df_test['City_Code']== 'C17') | (df_test['City_Code'] == 'C18') | (df_test['City_Code']== 'C19') | (df_test['City_Code'] == 'C20') | (df_test['City_Code'] == 'C21') | (df_test['City_Code'] == 'C22') | (df_test['City_Code'] == 'C23') | (df_test['City_Code'] == 'C24') | (df_test['City_Code'] == 'C25') | (df_test['City_Code'] == 'C26') | (df_test['City_Code'] == 'C27') | (df_test['City_Code'] == 'C28') | (df_test['City_Code']== 'C29') | (df_test['City_Code'] == 'C30') | (df_test['City_Code'] == 'C31') | (df_test['City_Code'] == 'C32') | (df_test['City_Code'] == 'C33') | (df_test['City_Code'] == 'C34') | (df_test['City_Code'] == 'C35') | (df_test['City_Code'] == 'C36')\ndf_test.loc[fil, 'City_Code'] = 'C4'\ndf_test['City_Code'].value_counts(normalize = True)\nfil = (df_test['Health Indicator'] == 'X6') | (df_test['Health Indicator'] == 'X7') | (df_test['Health Indicator'] == 'X8') | (df_test['Health Indicator'] == 'X9') \ndf_test.loc[fil, 'Health Indicator'] = 'X5'\ndf_test['Health Indicator'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['City_Code'] = df_test['City_Code'].astype('category')\ndf_test['Accomodation_Type'] = df_test['Accomodation_Type'].astype('category')\ndf_test['Reco_Insurance_Type'] = df_test['Reco_Insurance_Type'].astype('category')\ndf_test['Is_Spouse'] = df_test['Is_Spouse'].astype('category')\ndf_test['Health Indicator'] = df_test['Health Indicator'].astype('category')\ndf_test['Holding_Policy_Duration'] = df_test['Holding_Policy_Duration'].astype('float')\ndf_test['Holding_Policy_Type'] = df_test['Holding_Policy_Type'].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dtypes , df_test.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum(), df_test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We have missing values in \"Health Indicator\" , \"Holding_Policy_Duration\" and \"Holding_Policy_Type\".\n#We know \"Health Indicator\" and \"Holding_Policy_Type\" are categorical varibales. So we will use mode for missing value treatments.\n#And for \"Health_Policy_Duration\" we will use median.\ndf_train['Health Indicator'].mode()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Health Indicator'] = df_train['Health Indicator'].fillna('X1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Holding_Policy_Duration'].mode()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Holding_Policy_Duration'] = df_train['Holding_Policy_Duration'].fillna(1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Holding_Policy_Type'].mode()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Holding_Policy_Type'] = df_train['Holding_Policy_Type'].fillna(3.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['Health Indicator'] = df_test['Health Indicator'].fillna('X1')\ndf_test['Holding_Policy_Duration'] = df_test['Holding_Policy_Duration'].fillna(1.0)\ndf_test['Holding_Policy_Type'] = df_test['Holding_Policy_Type'].fillna(3.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" I have created two new variables 'holiday_policy_duration' and 'reco' from the variables \"Holiday_Policy_Duration\" and \"Reco_Policy_Cat\"\nholiday_duration_period had divided the policy in 4 different categories on the time frame whereas reco has divide the policy in 4 categories on the basis of recommendation.","metadata":{}},{"cell_type":"code","source":"\ndf_train['holding_policy_duration'] = \" \"\ndf_train['holding_policy_duration'] = 'categorical'\ndf_train['holding_policy_duration'][df_train['Holding_Policy_Duration']>=11] = 'very long'\ndf_train['holding_policy_duration'][(df_train['Holding_Policy_Duration']<11) & (df_train['Holding_Policy_Duration']>=6)] = 'long'\ndf_train['holding_policy_duration'][(df_train['Holding_Policy_Duration']<6) & (df_train['Holding_Policy_Duration']>=2)] = 'short'\ndf_train['holding_policy_duration'][df_train['Holding_Policy_Duration']<2] = 'very short'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['holding_policy_duration'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['reco'] = \" \"\ndf_train['reco'] = 'categorical'\ndf_train['reco'][df_train['Reco_Policy_Cat']>=20] = 'frequent'\ndf_train['reco'][(df_train['Reco_Policy_Cat']<20) & (df_train['Reco_Policy_Cat']>=15)] = 'moderate'\ndf_train['reco'][(df_train['Reco_Policy_Cat']<15) & (df_train['Reco_Policy_Cat']>=5)] = 'least'\ndf_train['reco'][df_train['Reco_Policy_Cat']<5] = 'very_least'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['reco'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Same will be done for test data set.\ndf_test['holding_policy_duration'] = \" \"\ndf_test['holding_policy_duration'] = 'categorical'\ndf_test['holding_policy_duration'][df_test['Holding_Policy_Duration']>=11] = 'very long'\ndf_test['holding_policy_duration'][(df_test['Holding_Policy_Duration']<11) & (df_test['Holding_Policy_Duration']>=6)] = 'long'\ndf_test['holding_policy_duration'][(df_test['Holding_Policy_Duration']<6) & (df_test['Holding_Policy_Duration']>=2)] = 'short'\ndf_test['holding_policy_duration'][df_test['Holding_Policy_Duration']<2] = 'very short'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['reco'] = \" \"\ndf_test['reco'] = 'categorical'\ndf_test['reco'][df_test['Reco_Policy_Cat']>=20] = 'frequent'\ndf_test['reco'][(df_test['Reco_Policy_Cat']<20) & (df_test['Reco_Policy_Cat']>=15)] = 'moderate'\ndf_test['reco'][(df_test['Reco_Policy_Cat']<15) & (df_test['Reco_Policy_Cat']>=5)] = 'least'\ndf_test['reco'][df_test['Reco_Policy_Cat']<5] = 'very_least'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def UVA_outlier(data, var_group, include_outlier = True):\n  '''\n  Univariate_Analysis_outlier:\n  takes a group of variables (INTEGER and FLOAT) and plot/print boplot and descriptives\\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot/print it \\n\\n\n\n  data : dataframe from which to plot from\\n\n  var_group : {list} type Group of Continuous variables\\n\n  include_outlier : {bool} whether to include outliers or not, default = True\\n\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,4), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    quant25 = data[i].quantile(0.25)\n    quant75 = data[i].quantile(0.75)\n    IQR = quant75 - quant25\n    med = data[i].median()\n    whis_low = med-(1.5*IQR)\n    whis_high = med+(1.5*IQR)\n\n    # Calculating Number of Outliers\n    outlier_high = len(data[i][data[i]>whis_high])\n    outlier_low = len(data[i][data[i]<whis_low])\n\n    if include_outlier == True:\n      print(include_outlier)\n      #Plotting the variable with every information\n      plt.subplot(1,size,j+1)\n      sns.boxplot(data[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('With Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2),\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))\n      \n    else:\n      # replacing outliers with max/min whisker\n      df_train = data[var_group][:]\n      df_train[i][df_train[i]>whis_high] = whis_high+1\n      df_train[i][df_train[i]<whis_low] = whis_low-1\n      \n      # plotting without outliers\n      plt.subplot(1,size,j+1)\n      sns.boxplot(df_train[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('Without Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2)\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))  \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = ['Upper_Age', 'Lower_Age','Reco_Policy_Premium']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UVA_outlier(df_train, num_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UVA_outlier(df_test, num_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see in both test and train data set we have outliers on the upper side","metadata":{}},{"cell_type":"code","source":"#Bivariate Analysis\n#Continuius Continuous\nnumericals = df_train[['Upper_Age', 'Lower_Age','Reco_Policy_Premium']]\ncorrelation = numericals.corr()\ncorrelation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = numericals.dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see we have high correlation between both the age variables","metadata":{}},{"cell_type":"code","source":"#BIVARIATE ANALYSIS: CONTINUOUS CATEGORICAL VARIABLES\n\n#List of Hypothesis and investigation to perform under this combination.  \n#1 Do Upper_Age plays a role in determining the probability of customer interest in recommended polcy?\n#2 Do Lower_Age plays a role in determining the probability of customer interest in recommended polcy?\n#3 Do premium amount plays a role in determining the probability of customer interest in recommended polcy?\n  \ndef TwoSampZ(X1, X2, sigma1, sigma2, N1, N2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sampled Z-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import norm\n  ovr_sigma = sqrt(sigma1**2/N1 + sigma2**2/N2)\n  z = (X1 - X2)/ovr_sigma\n  pval = 2*(1 - norm.cdf(abs(z)))\n  return pval\ndef TwoSampT(X1, X2, sd1, sd2, n1, n2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sample T-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import t as t_dist\n  ovr_sd = sqrt(sd1**2/n1 + sd2**2/n2)\n  t = (X1 - X2)/ovr_sd\n  df = n1+n2-2\n  pval = 2*(1 - t_dist.cdf(abs(t),df))\n  return pval\n\ndef Bivariate_cont_cat(df_train, cont, cat, category):\n  #creating 2 samples\n  x1 = df_train[cont][df_train[cat]==category][:]\n  x2 = df_train[cont][~(df_train[cat]==category)][:]\n  \n  #calculating descriptives\n  n1, n2 = x1.shape[0], x2.shape[0]\n  m1, m2 = x1.mean(), x2.mean()\n  std1, std2 = x1.std(), x2.mean()\n  \n  #calculating p-values\n  t_p_val = TwoSampT(m1, m2, std1, std2, n1, n2)\n  z_p_val = TwoSampZ(m1, m2, std1, std2, n1, n2)\n\n  #table\n  table = pd.pivot_table(data=df_train, values=cont, columns=cat, aggfunc = np.mean)\n\n  #plotting\n  plt.figure(figsize = (15,6), dpi=140)\n  \n  #barplot\n  plt.subplot(1,2,1)\n  sns.barplot([str(category),'not {}'.format(category)], [m1, m2])\n  plt.ylabel('mean {}'.format(cont))\n  plt.xlabel(cat)\n  plt.title('t-test p-value = {} \\n z-test p-value = {}\\n {}'.format(t_p_val,\n                                                                z_p_val,\n                                                                table))\n\n  # boxplot\n  plt.subplot(1,2,2)\n  sns.boxplot(x=cat, y=cont, data=df_train)\n  plt.title('categorical boxplot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bivariate_cont_cat(df_train, 'Upper_Age','Response', 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bivariate_cont_cat(df_train, 'Lower_Age','Response', 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bivariate_cont_cat(df_train, 'Reco_Policy_Premium','Response', 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above we can see that there seems to be no significant difference between the ages and premium about of both the class.","metadata":{}},{"cell_type":"code","source":"##List of all the hypothesis:-\n#1 Do residence of city plays a role in determining the probability of customer interest in recommended polcy?\n#2 Do accomodation type of individual plays a role in determining the probability of customer interest in recommended polcy?\n#3 Do marraige in case of joint account of individual plays a role in determining the probability of customer interest in recommended polcy?\n#4 Do health of individual plays a role in determining the probability of customer interest in recommended polcy?\n#5 Do type of policy choosen by individual plays a role in determining the probability of customer interest in recommended polcy?\n#6 Do holding duration period of individual plays a role in determining the probability of customer interest in recommended polcy?\n#7 Do recommended policy type plays a role in determining the probability of customer interest in recommended polcy?\n\n#Bivariate : Categorical-Categorical\ndef BVA_categorical_plot(data, tar, cat):\n  '''\n  take data and two categorical variables,\n  calculates the chi2 significance between the two variables \n  and prints the result with countplot & CrossTab\n  '''\n  #isolating the variables\n  data = df_train[[cat,tar]][:]\n\n  #forming a crosstab\n  table = pd.crosstab(df_train[tar],df_train[cat],)\n  f_obs = np.array([table.iloc[0][:].values,\n                    table.iloc[1][:].values])\n\n  #performing chi2 test\n  from scipy.stats import chi2_contingency\n  chi, p, dof, expected = chi2_contingency(f_obs)\n  \n  #checking whether results are significant\n  if p<0.05:\n    sig = True\n  else:\n    sig = False\n\n  #plotting grouped plot\n  sns.countplot(x=cat, hue=tar, data=df_train)\n  plt.title(\"p-value = {}\\n difference significant? = {}\\n\".format(round(p,8),sig))\n\n  #plotting percent stacked bar plot\n  #sns.catplot(ax, kind='stacked')\n  ax1 = df_train.groupby(cat)[tar].value_counts(normalize=True).unstack()\n  ax1.plot(kind='bar', stacked='True',title=str(ax1))\n  int_level = df_train[cat].value_counts()\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'City_Code', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'Accomodation_Type', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'Reco_Insurance_Type', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'Is_Spouse', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'Health Indicator', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'Holding_Policy_Type', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'holding_policy_duration', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BVA_categorical_plot(df_train, 'reco', 'Response')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above analysis we can see that only categories variables like reco, holding_policy_duration and Reco_Insurance_Type are statistically different for response categories.\nALl rest categories show no significant difference.","metadata":{}},{"cell_type":"code","source":"#Outliers Treatment\nQ1 = df_train['Reco_Policy_Premium'].quantile(0.25)\nQ3 = df_train['Reco_Policy_Premium'].quantile(0.75)\n\nIQR = df_train['Reco_Policy_Premium'].quantile(0.75) - df_train['Reco_Policy_Premium'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will replace outliers by upper whisker value because we have outliers at the upper side\ndf_train['Reco_Policy_Premium'] = np.where(df_train['Reco_Policy_Premium'] >31576.49, whisker_2,df_train['Reco_Policy_Premium'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Q1 = df_test['Reco_Policy_Premium'].quantile(0.25)\nQ3 = df_test['Reco_Policy_Premium'].quantile(0.75)\n\nIQR = df_test['Reco_Policy_Premium'].quantile(0.75) - df_test['Reco_Policy_Premium'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['Reco_Policy_Premium'] = np.where(df_test['Reco_Policy_Premium'] >31576.49, whisker_2,df_test['Reco_Policy_Premium'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we will make our baseline model based on the hypothesis satisfied above.\ndf_train1 = df_train[[ 'reco','holding_policy_duration','Reco_Insurance_Type', 'Response' ]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test1 = df_test[[ 'reco','holding_policy_duration','Reco_Insurance_Type' ]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create dummies\ndf_train1 = pd.concat([df_train1,pd.get_dummies(df_train1['holding_policy_duration'],prefix = str('holding_policy_duration'),prefix_sep='_')],axis = 1)\ndf_train1 = pd.concat([df_train1,pd.get_dummies(df_train1['reco'],prefix = str('reco'),prefix_sep='_')],axis = 1)\ndf_train1= pd.concat([df_train1,pd.get_dummies(df_train1['Reco_Insurance_Type'],prefix = str('Reco_Insurance_Type'),prefix_sep='_')],axis = 1)\ndf_test1= pd.concat([df_test1,pd.get_dummies(df_test1['reco'],prefix = str('reco'),prefix_sep='_')],axis = 1)\ndf_test1= pd.concat([df_test1,pd.get_dummies(df_test1['holding_policy_duration'],prefix = str('holding_policy_duration'),prefix_sep='_')],axis = 1)\ndf_test1= pd.concat([df_test1,pd.get_dummies(df_test1['Reco_Insurance_Type'],prefix = str('Reco_Insurance_Type'),prefix_sep='_')],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train1.drop(['Reco_Insurance_Type'], axis =1, inplace = True)\ndf_train1.drop(['reco'], axis =1, inplace = True)\ndf_test1.drop(['reco'], axis =1, inplace = True)\ndf_test1.drop(['Reco_Insurance_Type'], axis =1, inplace = True)\ndf_train1.drop(['holding_policy_duration'], axis =1, inplace = True)\ndf_test1.drop(['holding_policy_duration'], axis =1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test1.dtypes.reset_index()\ndf_train1.dtypes.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train1.Response\nx = df_train1.drop(['Response'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain1, xvald1, ytrain1, yvald1 = train_test_split(x,y,test_size=1/3, random_state=11, stratify = y)\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(xtrain1,ytrain1)\npred = model.predict_proba(xvald1)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, _ = roc_curve(yvald1,pred) \nauc = roc_auc_score(yvald1, pred) \nplt.figure(figsize=(12,8)) \nplt.plot(fpr,tpr,label=\"Validation AUC-ROC=\"+str(auc)) \nx = np.linspace(0, 1, 1000)\nplt.plot(x, x, linestyle='-')\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_val = model.predict(xvald1)\n\nlabel_preds = pred_val\n\ncm = confusion_matrix(yvald1,label_preds)\n\n\ndef plot_confusion_matrix(cm, normalized=True, cmap='bone'):\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap=cmap)\n\nplot_confusion_matrix(cm, ['No', 'Yes'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My baseline model results are not so good therefore we will choose other two models:\n1) In first model I will take all the variables except the ID, Region Code, Lower Age, Holding_Policy_Duration and Reco_Policy_Cat\n2) Second model will be based on backward feature selection","metadata":{}},{"cell_type":"code","source":"df_train2  = df_train[['City_Code', 'Accomodation_Type', 'Reco_Insurance_Type', 'Upper_Age','Is_Spouse', 'Health Indicator', 'Holding_Policy_Type','Reco_Policy_Premium', 'Response','holding_policy_duration','reco']] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2  = df_test[['City_Code', 'Accomodation_Type', 'Reco_Insurance_Type', 'Upper_Age','Is_Spouse', 'Health Indicator', 'Holding_Policy_Type','Reco_Policy_Premium','holding_policy_duration','reco']] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['Accomodation_Type'],prefix = str('Accomodation_Type'),prefix_sep='_')],axis = 1)\ndf_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['Reco_Insurance_Type'],prefix = str('Reco_Insurance_Type'),prefix_sep='_')],axis = 1)\ndf_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['Is_Spouse'],prefix = str('Is_Spouse'),prefix_sep='_')],axis = 1)\ndf_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['Health Indicator'],prefix = str('Health Indicator'),prefix_sep='_')],axis = 1)\ndf_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['Holding_Policy_Type'],prefix = str('Holding_Policy_Type'),prefix_sep='_')],axis = 1)\ndf_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['holding_policy_duration'],prefix = str('holding_policy_duration'),prefix_sep='_')],axis = 1)\ndf_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['City_Code'],prefix = str('City_Code'),prefix_sep='_')],axis = 1)\ndf_train2 = pd.concat([df_train2,pd.get_dummies(df_train2['reco'],prefix = str('reco'),prefix_sep='_')],axis = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['Accomodation_Type'],prefix = str('Accomodation_Type'),prefix_sep='_')],axis = 1)\ndf_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['Reco_Insurance_Type'],prefix = str('Reco_Insurance_Type'),prefix_sep='_')],axis = 1)\ndf_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['Is_Spouse'],prefix = str('Is_Spouse'),prefix_sep='_')],axis = 1)\ndf_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['Health Indicator'],prefix = str('Health Indicator'),prefix_sep='_')],axis = 1)\ndf_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['Holding_Policy_Type'],prefix = str('Holding_Policy_Type'),prefix_sep='_')],axis = 1)\ndf_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['holding_policy_duration'],prefix = str('holding_policy_duration'),prefix_sep='_')],axis = 1)\ndf_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['City_Code'],prefix = str('City_Code'),prefix_sep='_')],axis = 1)\ndf_test2 = pd.concat([df_test2,pd.get_dummies(df_test2['reco'],prefix = str('reco'),prefix_sep='_')],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train2.drop(['reco'], axis = 1, inplace = True)\ndf_train2.drop(['Accomodation_Type'], axis = 1, inplace = True)\ndf_train2.drop(['Reco_Insurance_Type'], axis = 1, inplace = True)\ndf_train2.drop(['Is_Spouse'], axis =1 , inplace = True)\ndf_train2.drop(['Health Indicator'], axis =1 , inplace = True)\ndf_train2.drop(['Holding_Policy_Type'], axis = 1, inplace = True)\ndf_train2.drop(['holding_policy_duration'], axis =1, inplace = True)\ndf_train2.drop(['City_Code'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2.drop(['reco'], axis = 1, inplace = True)\ndf_test2.drop(['Accomodation_Type'], axis = 1, inplace = True)\ndf_test2.drop(['Reco_Insurance_Type'], axis = 1, inplace = True)\ndf_test2.drop(['Is_Spouse'], axis =1 , inplace = True)\ndf_test2.drop(['Health Indicator'], axis =1 , inplace = True)\ndf_test2.drop(['Holding_Policy_Type'], axis = 1, inplace = True)\ndf_test2.drop(['holding_policy_duration'], axis =1, inplace = True)\ndf_test2.drop(['City_Code'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2.shape , df_train2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2.dtypes.reset_index()\ndf_train2.dtypes.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train3 = df_train2.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train2 = df_train3.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##model 1\ny = df_train2.Response\nx = df_train2.drop(['Response'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smk = SMOTETomek(random_state = 42)\ndf_train2,y_all2 = smk.fit_resample(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain3, xvald3, ytrain3, yvald3 = train_test_split(df_train2,y_all2,test_size=1/3, random_state=11, stratify = y_all2)\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(xtrain3,ytrain3)\npred = model.predict_proba(xvald3)[:,1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, _ = roc_curve(yvald3,pred) \nauc = roc_auc_score(yvald3, pred) \nplt.figure(figsize=(12,8)) \nplt.plot(fpr,tpr,label=\"Validation AUC-ROC=\"+str(auc)) \nx = np.linspace(0, 1, 1000)\nplt.plot(x, x, linestyle='-')\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_val = model.predict(xvald3)\n\nlabel_preds = pred_val\n\ncm = confusion_matrix(yvald3,label_preds)\n\n\ndef plot_confusion_matrix(cm, normalized=True, cmap='bone'):\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap=cmap)\n\nplot_confusion_matrix(cm, ['No', 'Yes'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL 2\nmodel = tree.DecisionTreeClassifier()\nrfe = RFE(estimator = model, n_features_to_select =1, step = 1)\nrfe.fit(df_train2, y_all2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranking_df_train2 = pd.DataFrame()\nranking_df_train2['Feature_name'] = df_train2.columns\nranking_df_train2['Rank'] = rfe.ranking_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranked = ranking_df_train2.sort_values(by = [\"Rank\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranked","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cv_score(ml_model, rstate = 12, thres = 0.5, cols = df_train2.columns):\n    i = 1\n    cv_scores = []\n    df2 = df_train2.copy()\n    df2 = df_train2[cols]\n    \n    # 5 Fold cross validation stratified on the basis of target\n    kf = StratifiedKFold(n_splits=5,random_state=rstate,shuffle=True)\n    for df_train1_index,vald_index in kf.split(df_train2,y_all2):\n        print('\\n{} of kfold {}'.format(i,kf.n_splits))\n        xtr,xvl = df2.loc[df_train1_index],df2.loc[vald_index]\n        ytr,yvl = y_all2.loc[df_train1_index],y_all2.loc[vald_index]\n            \n        # Define model for fitting on the training set for each fold\n        model = ml_model\n        model.fit(xtr, ytr)\n        pred_probs = model.predict_proba(xvl)\n        pp = []\n         \n        # Use threshold to define the classes based on probability values\n        for j in pred_probs[:,1]:\n            if j>thres:\n                pp.append(1)\n            else:\n                pp.append(0)\n         \n        # Calculate scores for each fold and print\n        pred_val = pp\n        roc_score = roc_auc_score(yvl,pred_probs[:,1])\n        recall = recall_score(yvl,pred_val)\n        precision = precision_score(yvl,pred_val)\n        sufix = \"\"\n        msg = \"\"\n        msg += \"ROC AUC Score: {}, Recall Score: {:.4f}, Precision Score: {:.4f} \".format(roc_score, recall,precision)\n        print(\"{}\".format(msg))\n         \n         # Save scores\n        cv_scores.append(roc_score)\n        i+=1\n    return cv_scores\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ref_top10_scores = cv_score( tree.DecisionTreeClassifier(), cols = ranked['Feature_name'][:10].values, thres = 0.14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that model 1 and model 2 has almost the same accuracy but both have higher accuracy than the base model. So, well use model 2 to predict our results.","metadata":{}},{"cell_type":"code","source":"df_train2.shape, df_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train4 = df_train3[['reco_very_least', 'reco_least', 'reco_frequent', 'reco_moderate', 'Health Indicator_X3', 'Health Indicator_X4','Health Indicator_X5','Health Indicator_X2','Health Indicator_X1','holding_policy_duration_long','Response']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test4 = df_test2[['reco_very_least', 'reco_least', 'reco_frequent', 'reco_moderate', 'Health Indicator_X3', 'Health Indicator_X4','Health Indicator_X5','Health Indicator_X2','Health Indicator_X1','holding_policy_duration_long']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train4.Response\nx = df_train4.drop(['Response'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smk = SMOTETomek(random_state = 42)\ndf_t,y_t = smk.fit_resample(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tree.DecisionTreeClassifier(random_state=0).fit(df_t,y_t)\nmodel.fit(df_t,y_t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(df_test4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame ({'prediction': pred})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['prediction'].value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}