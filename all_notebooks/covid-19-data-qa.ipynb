{"cells":[{"metadata":{},"cell_type":"markdown","source":"As QA I want to care about data quality. Data quality is important for climbing a excellent result.\nUsually data testing should start from defining expected results with product owners and end-users. In this case I have no access to owners and i assume some expected results by myself. Anyway, I start QA from some exploratory testing session."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CSV file data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)**CSV-Test-1**: Sha key should be not null if has_full_test is True"},{"metadata":{"trusted":true},"cell_type":"code","source":"isempty = df[(df['has_full_text'] == True) & (df['sha'] == 'NaN')].empty\nprint('Is the DataFrame empty :', isempty)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" I'm only interested in sources with sha is not NaN"},{"metadata":{},"cell_type":"markdown","source":"* [](http://)**CSV-Test-2**: Sha key should be uniq"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_verified = df.dropna(subset=['sha'])\nuniq_count = df_verified['sha'].drop_duplicates().count()\ntotal_count = df_verified['sha'].count()\nprint('Uniq sha: ', uniq_count)\nprint('Total sha: ', total_count)\nprint(total_count - uniq_count)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"12 sha values are not uniq"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_verified[df_verified.duplicated(subset=['sha'],keep=False)].sort_values('sha')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CSV-Test-3:** article should be existed"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os.path\ndef full_path(sha,full_text_file):\n    data_path_pattern = \"/kaggle/input/CORD-19-research-challenge/{full_text_file}/{full_text_file}/{sha}.json\"\n    path = data_path_pattern.format(\n            full_text_file=full_text_file,\n            sha=sha\n            )\n    return path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_file_exists(shas, full_text_file):\n        result = all(os.path.exists(full_path(sha,full_text_file)) for sha in shas.split(\"; \"))    \n        return result\n\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"])\ndf_uniq['is_file_exists'] = df_uniq.apply(lambda x: is_file_exists(x['sha'], x['full_text_file']), axis=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_uniq[df_uniq['is_file_exists'] == False])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of wp's data\n\ndata_path_pattern = \"/kaggle/input/CORD-19-research-challenge/{full_text_file}/{full_text_file}/{sha}.json\"\n\npath = data_path_pattern.format(\n    full_text_file=\"custom_license\",\n    sha=\"be7e8df88e63d2579e8d61e2c3d716d57d347676\"\n)\n\nwith open(path, \"r\") as f:\n    data = json.load(f)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*sha key*\n1. Should be not null if full body is true (+)\n2. Should be uniq (-) 12 sha keys are not uniq\n3. Article with sha and has_full_text==true should be existed (+)"},{"metadata":{},"cell_type":"markdown","source":"***Completeness***\n0. Abstract is presented (-) many articles have no abstract\n1. full body (?) There are articles with non clear body. len(body_text) = 1\n2. number of words in full body (30k symbols)\n3. link t external resources: (10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def object_size(name, sha, full_text_file):\n    path = full_path(sha,full_text_file)\n    with open(path, \"r\") as f:\n        data = json.load(f)\n        return len(data[name])\n    \ndef abstract_size(sha, full_text_file):\n    return object_size('abstract', sha, full_text_file)\n\n    \ndef body_size(sha, full_text_file):\n    return object_size('body_text', sha, full_text_file)\n\ndef authors_size(sha, full_text_file):\n    path = full_path(sha,full_text_file)\n    with open(path, \"r\") as f:\n        data = json.load(f)\n        return len(data['metadata']['authors'])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import trange, tqdm\n\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"])[['sha','full_text_file']]\nrows = []\n\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        new_row = {'sha': sha, 'full_text_file': full_text_file}\n        rows.append(new_row)\n        \ndf_test = pd.DataFrame(rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['is_file_exists'] = df_test.apply(lambda x: is_file_exists(x['sha'], x['full_text_file']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['abstarct_size'] = df_test.apply(lambda x: abstract_size(x['sha'], x['full_text_file']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['authors_size'] = df_test.apply(lambda x: authors_size(x['sha'], x['full_text_file']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['body_size'] = df_test.apply(lambda x: body_size(x['sha'], x['full_text_file']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['bib_entries_size'] = df_test.apply(lambda x: object_size('bib_entries', x['sha'], x['full_text_file']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['authors_size'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Found all articles with body test len = 1***\nprint body\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import trange, tqdm\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"]).head(100)[['sha','full_text_file']]\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        path = data_path_pattern.format(\n        full_text_file=full_text_file,\n        sha=sha\n        )\n        with open(path, \"r\") as f:\n            data = json.load(f)\n            if (len(data['body_text']) == 1):\n                print(data['body_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I think I could assume that let(data['body_text']) == 1 is not good data too.\n"},{"metadata":{},"cell_type":"markdown","source":"***Uniqueness***\n1. Data should contain uniq articals \n2. Sha key should be uniq : Look at test CSV-Test-2 "},{"metadata":{},"cell_type":"markdown","source":"> **Uniq-Test-1: Article should be uniq**\n\nIdea: Data set should contain only uniq articles. \nI can check articles in Metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_title_df = df_verified[df_verified.duplicated(subset=['title'],keep=False)].sort_values('title')\nduplicate_title_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_title_df['title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. I found that  titles have title \"Index' and 'Author index'"},{"metadata":{},"cell_type":"markdown","source":"As bonus let's define journal distribution\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_verified['journal'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> PLOS One is a peer-reviewed open access scientific journal published by the Public Library of Science since 2006. The journal covers primary research from any discipline within science and medicine. Wikipedia"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"***Credibility***\nIt is the most difficult for testing. How can o define Credibility for article? It means that should i find all \"fake\" articles? I'd like to found all articles by next creteriases:\n1. Article should has a bib_entries\n2. Article should has Authors"},{"metadata":{},"cell_type":"markdown","source":"**Cred-Test-1****: article should have at least 3 bib_entries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Article should has a bib_entries > 3 items\n\nfrom tqdm.notebook import trange, tqdm\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"]).head(10)[['sha','full_text_file']]\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        path = data_path_pattern.format(\n        full_text_file=full_text_file,\n        sha=sha\n        )\n        with open(path, \"r\") as f:\n            data = json.load(f)\n            if (len(data['bib_entries']) < 3):\n                print(data['bib_entries'])\n                print(sha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cred-Test-2****: article should have at least 3 authors"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Article should has a **Cred-Test-2****: article should have at least 3 authors > 3 items\n\nfrom tqdm.notebook import trange, tqdm\ni = 0\ndf_uniq = df_verified.drop_duplicates(subset = [\"sha\"])[['sha','full_text_file']]\nfor row in tqdm(df_uniq.iterrows()):\n    _,(shas,full_text_file) = row\n    for sha in shas.split(\"; \"):\n        path = data_path_pattern.format(\n        full_text_file=full_text_file,\n        sha=sha\n        )\n\n        with open(path, \"r\") as f:\n            data = json.load(f)\n            if (len(data['metadata']['authors']) < 2):\n                i += 1\n\nprint(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***accuracy***\n1. no typo\n2. relevants links\n3. Structure and style"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}