{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text preprocessing steps and universal pipeline\n\nBefore feeding any ML model some kind data, it has to be properly preprocessed. You must have heard the byword: `Garbage in, garbage out` (GIGO). Text is a specific kind of data and can't be directly fed to most ML models, so before feeding it to a model you have to somehow extract numerical features from it, in other word `vectorize`. Vectorization is not the topic of this tutorial, but the main thing you have to understand is that GIGO is also aplicable on vectorization too, you can extract qualitative features only from qualitatively preprocessed text.\n\nThings we are going to discuss:\n\n1. Tokenization\n1. Cleaning\n1. Normalization\n1. Lemmatization\n1. Steaming\n\nFinally, we'll create reusable pipeline, which you'll be able to use in your applications."},{"metadata":{"trusted":true},"cell_type":"code","source":"example_text = \"\"\"\nAn explosion targeting a tourist bus has injured at least 16 people near the Grand Egyptian Museum, \nnext to the pyramids in Giza, security sources say E.U.\n\nSouth African tourists are among the injured. Most of those hurt suffered minor injuries, \nwhile three were treated in hospital, N.A.T.O. say.\n\nhttp://localhost:8888/notebooks/Text%20preprocessing.ipynb\n\n@nickname of twitter user and his email is email@gmail.com . \n\nA device went off close to the museum fence as the bus was passing on 16/02/2012.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization\n\n`Tokenization` - text preprocessing step, which assumes splitting text into `tokens`(words, senteces, etc.)\n\nSeems like you can use somkeind of simple seperator to achieve it, but you don't have to forget that there are a lot of different situations, where separators just don't work. For example, `.` separator for tokenization into sentences will fail if you have abbreviations with dots. So you have to have more complex model to achieve good enough result. Commonly this problem is solved using `nltk` or `spacy` nlp libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\n\nnltk_words = word_tokenize(example_text)\ndisplay(f\"Tokenized words: {nltk_words}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\n\ndoc = nlp(example_text)\nspacy_words = [token.text for token in doc]\ndisplay(f\"Tokenized words: {spacy_words}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(f\"In spacy but not in nltk: {set(spacy_words).difference(set(nltk_words))}\")\ndisplay(f\"In nltk but not in spacy: {set(nltk_words).difference(set(spacy_words))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that `spacy` tokenized some weird staff like `\\n`, `\\n\\n`, but was able to handle urls, emails and twitter-like mentions. Also we see that `nltk` tokenized abbreviations without the last `.`"},{"metadata":{},"cell_type":"markdown","source":"# Cleaning\n\n`Cleaning` step assumes removing all undesirable content."},{"metadata":{},"cell_type":"markdown","source":"### Punctuation removal\n`Punctuation removal` might be a good step, when punctuation does not brings additional value for text vectorization. Punctuation removal is better to be done after tokenization step, doing it before might cause undesirable effects. Good choice for `TF-IDF`, `Count`, `Binary` vectorization."},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\ndisplay(f\"Punctuation symbols: {string.punctuation}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_with_punct = \"@nickname of twitter user, and his email is email@gmail.com .\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_without_punct = text_with_punct.translate(str.maketrans('', '', string.punctuation))\ndisplay(f\"Text without punctuation: {text_without_punct}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see that important symbols for correct tokenizations were removed. Now email can't be properly detected. As you could mention from the `Tokenization` step, punctuation symbors were parsed as single tokens, so better way would be to tokenize first and then remove punctuation symbols. "},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(text_with_punct)\ntokens = [t.text for t in doc]\n# python \ntokens_without_punct_python = [t for t in tokens if t not in string.punctuation]\ndisplay(f\"Python based removal: {tokens_without_punct_python}\")\n\ntokens_without_punct_spacy = [t.text for t in doc if t.pos_ != 'PUNCT']\ndisplay(f\"Spacy based removal: {tokens_without_punct_spacy}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you see that `python-based` removal worked even better than spacy, because spacy tagged `@nicname` as `PUNCT` part-of-speech."},{"metadata":{},"cell_type":"markdown","source":"### Stop words removal\n\n`Stop words` usually refers to the most common words in a language, which usualy does not bring additional meaning. There is no single universal list of stop words used by all nlp tools, because this term has very fuzzy definition. Although practice has shown, that this step is much have, when preparing text for indexing, but might be tricky for text classification purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"This movie is just not good enough\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n\ndisplay(f\"Spacy stop words count: {len(spacy_stop_words)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\ndisplay(f\"Spacy text without stop words: {text_without_stop_words}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\nnltk_stop_words = nltk.corpus.stopwords.words('english')\ndisplay(f\"nltk stop words count: {len(nltk_stop_words)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_without_stop_words = [t for t in word_tokenize(text) if t not in nltk_stop_words]\ndisplay(f\"nltk text without stop words: {text_without_stop_words}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you see that nltk and spacy has different vocabulary size, so the results of filtering are different. But the main thing I want to underline is that the word `not` was filtered, which in the most cases will be allright, but in the case when you want determine the polarity of this sentence `not` will bring the additional meaning.\n\nFor such cases you are able to set stop words you can ignore in spacy library. In the case of nltk you cat just remove or add custom words to `nltk_stop_words`, it is just a list."},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_web_sm\n\nnlp = en_core_web_sm.load()\n\ncustomize_stop_words = [\n    'not'\n]\n\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = False\n\ntext_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\ndisplay(f\"Spacy text without updated stop words: {text_without_stop_words}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization\n\nLike any data text requires normalization. In case of text it is:\n\n1. Converting dates to text\n2. Numbers to text\n3. Currency/Percent signs to text\n4. Expanding of abbreviations (content dependent) NLP - Natural Language Processing, Neuro-linguistic programming, Non-Linear programming\n5. Spelling mistakes correction\n\nTo summarize, normalization is a convertion of any non-text information into textual equivalent.\n\nFor this purposes exists a great library - [normalize](https://github.com/EFord36/normalise). I'll show you usage of this library from its README. This library is based on `nltk` package, so it expects `nltk` word tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"from normalise import normalise\n\ntext = \"\"\"\nOn the 13 Feb. 2007, Theresa May announced on MTV news that the rate of childhod obesity had \nrisen from 7.3-9.6% in just 3 years , costing the N.A.T.O Â£20m\n\"\"\"\n\nuser_abbr = {\n    \"N.A.T.O\": \"North Atlantic Treaty Organization\"\n}\n\nnormalized_tokens = normalise(word_tokenize(text), user_abbrevs=user_abbr, verbose=False)\ndisplay(f\"Normalized text: {' '.join(normalized_tokens)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The worst thing in this library is that for now you can't disable some modules, like abbreviation expanding, and int causes things like `MTV` -> `M T V`. But I have already added an appropriate issue on this repository, maybe it would be fixed in a while."},{"metadata":{},"cell_type":"markdown","source":"# Lematization and Steaming\n\n`Stemming` is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. \n\n`Lemmatization`, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nimport numpy as np\n\ntext = ' '.join(normalized_tokens)\ntokens = word_tokenize(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"porter=PorterStemmer()\nstem_words = np.vectorize(porter.stem)\nstemed_text = ' '.join(stem_words(tokens))\ndisplay(f\"Stemed text: {stemed_text}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nlemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)\nlemmatized_text = ' '.join(lemmatize_words(tokens))\ndisplay(f\"nltk lemmatized text: {lemmatized_text}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmas = [t.lemma_ for t in nlp(text)]\ndisplay(f\"Spacy lemmatized text: {' '.join(lemmas)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that `spacy` lemmatized much better than nltk, one of examples `risen` -> `rise`, only `spacy` handeled that."},{"metadata":{},"cell_type":"markdown","source":"# Reusable pipeline\n\nAnd now my favourite part! We are going to cretate reusable pipeline, which you could use on any of you projects."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport multiprocessing as mp\n\nimport string\nimport spacy \nimport en_core_web_sm\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom normalise import normalise\n\nnlp = en_core_web_sm.load()\n\n\nclass TextPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self,\n                 variety=\"BrE\",\n                 user_abbrevs={},\n                 n_jobs=1):\n        \"\"\"\n        Text preprocessing transformer includes steps:\n            1. Text normalization\n            2. Punctuation removal\n            3. Stop words removal\n            4. Lemmatization\n        \n        variety - format of date (AmE - american type, BrE - british format) \n        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n        n_jobs - parallel jobs to run\n        \"\"\"\n        self.variety = variety\n        self.user_abbrevs = user_abbrevs\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, *_):\n        X_copy = X.copy()\n\n        partitions = 1\n        cores = mp.cpu_count()\n        if self.n_jobs <= -1:\n            partitions = cores\n        elif self.n_jobs <= 0:\n            return X_copy.apply(self._preprocess_text)\n        else:\n            partitions = min(self.n_jobs, cores)\n\n        data_split = np.array_split(X_copy, partitions)\n        pool = mp.Pool(cores)\n        data = pd.concat(pool.map(self._preprocess_part, data_split))\n        pool.close()\n        pool.join()\n\n        return data\n\n    def _preprocess_part(self, part):\n        return part.apply(self._preprocess_text)\n\n    def _preprocess_text(self, text):\n        normalized_text = self._normalize(text)\n        doc = nlp(normalized_text)\n        removed_punct = self._remove_punct(doc)\n        removed_stop_words = self._remove_stop_words(removed_punct)\n        return self._lemmatize(removed_stop_words)\n\n    def _normalize(self, text):\n        # some issues in normalise package\n        try:\n            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n        except:\n            return text\n\n    def _remove_punct(self, doc):\n        return [t for t in doc if t.text not in string.punctuation]\n\n    def _remove_stop_words(self, doc):\n        return [t for t in doc if not t.is_stop]\n\n    def _lemmatize(self, doc):\n        return ' '.join([t.lemma_ for t in doc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf_bbc = pd.read_csv('../input/bbc-text.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\ntext = TextPreprocessor(n_jobs=-1).transform(df_bbc['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Performance of transformer on {len(df_bbc)} texts and {mp.cpu_count()} processes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}