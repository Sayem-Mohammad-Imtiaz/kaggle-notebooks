{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\n# model use for prediction\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\n\n# for split the dataset and test set\nfrom sklearn.model_selection import train_test_split\n\n# for turning plot from text to frequency matrix\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\n# for choosing the features (words)\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# for stemming and lemmatizing the texts\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:48:59.54121Z","iopub.execute_input":"2021-06-09T15:48:59.541712Z","iopub.status.idle":"2021-06-09T15:48:59.554634Z","shell.execute_reply.started":"2021-06-09T15:48:59.541634Z","shell.execute_reply":"2021-06-09T15:48:59.553493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have split the original dataset to new set plot and label\n# we have convert the label from string to indicator variable using panda.get_dummies sand then make it a new csv file name labels.csv\n# we have drop all the other atrributes and make it a new csv file name data.csv \n\n# read the input file\nX = pd.read_csv('../input/movie-plot-title-data/data.csv', index_col = 'Unnamed: 0')['0']\ny = pd.read_csv('../input/movie-plot-title-data/labels.csv', index_col = 'Unnamed: 0')\n\n# list of the 19 genres\ngenre_list = ['action', 'adventure', 'animated', 'biopic', 'comedy', 'crime', 'drama', 'family', 'fantasy', 'film-noir', 'horror', 'musical', 'mystery', 'romance', 'sci-fi', 'thriller', 'war', 'western']","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:48:59.556961Z","iopub.execute_input":"2021-06-09T15:48:59.557586Z","iopub.status.idle":"2021-06-09T15:49:00.631272Z","shell.execute_reply.started":"2021-06-09T15:48:59.557542Z","shell.execute_reply":"2021-06-09T15:49:00.630389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the plot example\nprint(X[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:49:00.632524Z","iopub.execute_input":"2021-06-09T15:49:00.633055Z","iopub.status.idle":"2021-06-09T15:49:00.639802Z","shell.execute_reply.started":"2021-06-09T15:49:00.633007Z","shell.execute_reply":"2021-06-09T15:49:00.639017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the distribution of genres \nfig = plt.figure(figsize = (20, 12))\nplt.bar(y.columns, y.sum(axis = 0))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:49:00.64188Z","iopub.execute_input":"2021-06-09T15:49:00.642461Z","iopub.status.idle":"2021-06-09T15:49:00.930152Z","shell.execute_reply.started":"2021-06-09T15:49:00.642416Z","shell.execute_reply":"2021-06-09T15:49:00.9291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the dataset has 26259 rows\nprint(len(X))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:49:00.932135Z","iopub.execute_input":"2021-06-09T15:49:00.932423Z","iopub.status.idle":"2021-06-09T15:49:00.937214Z","shell.execute_reply.started":"2021-06-09T15:49:00.932396Z","shell.execute_reply":"2021-06-09T15:49:00.936356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# we use nltk stemmer and lemmatizer for out plot\nstemmer = SnowballStemmer('english')\nlemmatizer = WordNetLemmatizer()\n\n# function for stemming and lemmatizing text\n# the function take very long times to run \ndef text_processing():\n    # preprocessing text by stemming and lemmatizing\n    for i in range(len(X)):\n        X[i] = ' '.join(stemmer.stem(word) for word in X[i].split())\n        X[i] = ' '.join(lemmatizer.lemmatize(word) for word in X[i].split())    \ntext_processing()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:49:00.938632Z","iopub.execute_input":"2021-06-09T15:49:00.93895Z","iopub.status.idle":"2021-06-09T15:52:22.943521Z","shell.execute_reply.started":"2021-06-09T15:49:00.93892Z","shell.execute_reply":"2021-06-09T15:52:22.942214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:52:22.945266Z","iopub.execute_input":"2021-06-09T15:52:22.945753Z","iopub.status.idle":"2021-06-09T15:52:22.9519Z","shell.execute_reply.started":"2021-06-09T15:52:22.945705Z","shell.execute_reply":"2021-06-09T15:52:22.950784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for tranform the text to tf-Ã¬df matrix or occurance matrix\n# Choose between tfidfvectorizer and countvectorizer, change the param max_features\n# tfidf_vec_ = CountVectorizer(stop_words = 'english', max_features = 10000)\ntfidf_vec_ = TfidfVectorizer(stop_words = 'english')\n\n#split the dataset into train set and test set with train set size = 0.8, test set size = 0.2\n#train_test_split() automatic shuffle the rows\nxtrain, xtest, ytrain, ytest = train_test_split(X,y, random_state = 0, train_size = 0.8)\n\n# tranform x train to matrix and fit x test to matrix\nxtrain = tfidf_vec_.fit_transform(xtrain)\nxtest = tfidf_vec_.transform(xtest)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:52:22.953315Z","iopub.execute_input":"2021-06-09T15:52:22.953648Z","iopub.status.idle":"2021-06-09T15:52:33.902449Z","shell.execute_reply.started":"2021-06-09T15:52:22.953609Z","shell.execute_reply":"2021-06-09T15:52:33.901301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef reduce_feature():\n    # the function choose the features with highest chi2 score\n    univariate_selection = SelectKBest(chi2 ,k = 50000)\n    univariate_selection.fit(xtrain, ytrain)\n    univariate_score = univariate_selection.scores_\n\n#     plot the distribution of the score\n    fig = plt.figure(figsize = (20, 20))\n    plt.hist(univariate_score,bins = [0,1,2,3,4,5,6,7,8,9, 10])\n    fig = plt.figure(figsize = (20, 20))\n\n    plt.hist(univariate_score,bins = [10,20,30,40,50,60,70,80,90,100])\n    \n\n\n    chosen_features = np.where(univariate_score > 5)[0]\n    print(chosen_features)\n    print(len(chosen_features))\n    reduced_train = xtrain[:,chosen_features]\n    reduced_test = xtest[:,chosen_features]\n    return reduced_train, reduced_test\nreduced_train, reduced_test = reduce_feature()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:52:33.903979Z","iopub.execute_input":"2021-06-09T15:52:33.904323Z","iopub.status.idle":"2021-06-09T15:52:34.577148Z","shell.execute_reply.started":"2021-06-09T15:52:33.904289Z","shell.execute_reply":"2021-06-09T15:52:34.576359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model with reduced features, return list of f1 score of classes\ndef eval_model_reduced(estimator):\n    model = OneVsRestClassifier(estimator)\n    model.fit(reduced_train, ytrain)\n    predict = model.predict(reduced_test)\n    test_score = f1_score(ytest, predict, average = None)\n    print(test_score)\n    return test_score\n# evaluate the model, return list of f1 score of classes\ndef eval_model_normal(estimator):\n    model = OneVsRestClassifier(estimator)\n    model.fit(xtrain, ytrain)\n    predict = model.predict(xtest)\n    test_score = f1_score(ytest, predict, average = None)\n    print(test_score)\n    return test_score\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:52:34.578799Z","iopub.execute_input":"2021-06-09T15:52:34.57938Z","iopub.status.idle":"2021-06-09T15:52:34.587548Z","shell.execute_reply.started":"2021-06-09T15:52:34.579341Z","shell.execute_reply":"2021-06-09T15:52:34.586473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the test\nresult = eval_model_reduced(BernoulliNB(alpha=0.05))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:52:34.58894Z","iopub.execute_input":"2021-06-09T15:52:34.589226Z","iopub.status.idle":"2021-06-09T15:52:35.211457Z","shell.execute_reply.started":"2021-06-09T15:52:34.589198Z","shell.execute_reply":"2021-06-09T15:52:35.210069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the results\nfig = plt.figure(figsize = (20, 12))\naxes = plt.axes()\naxes.set_ylim([0, 0.7])\nplt.bar(y.columns, result)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:52:35.213008Z","iopub.execute_input":"2021-06-09T15:52:35.213473Z","iopub.status.idle":"2021-06-09T15:52:35.477836Z","shell.execute_reply.started":"2021-06-09T15:52:35.213426Z","shell.execute_reply":"2021-06-09T15:52:35.476627Z"},"trusted":true},"execution_count":null,"outputs":[]}]}