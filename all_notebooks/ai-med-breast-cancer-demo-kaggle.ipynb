{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Breast Cancer Detection with Decision Trees"},{"metadata":{},"cell_type":"markdown","source":"A decision tree is a logic structure composed of connected nodes. Data enters at the root of the tree, and logical decisions are made about that data at each node. They're a simple, general concept easy to grasp which is a benefit for operational use.\n\nTherefore, by making our way down the decision tree, we can apply classifiers to data. In the diagram below, the input data are \"job offers,\" and we are classifying each offer as \"decline\" or \"accept.\""},{"metadata":{},"cell_type":"markdown","source":"![](https://i0.wp.com/dataaspirant.com/wp-content/uploads/2017/01/B03905_05_01-compressor.png?resize=768%2C424&ssl=1)\n"},{"metadata":{},"cell_type":"markdown","source":"Everything starts at the top of the decision tree from the root node.\n\nEach of the internal decision nodes asks a question about the data, and determines which branch to continue down. Notice that all decisions must result in a logical \"yes\" or \"no.\"\nEx. Instead of asking how long a commute time is, ask if it was over 1 hour\nEach branch line denotes the outcome of a test (yes/no)\nFinally, the data reaches an end, or a leaf node, which contains a classification label.\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/2400/1*UCd6KrmBxpzUpWt3bnoKEA.png)"},{"metadata":{},"cell_type":"markdown","source":"**Overfitting** is when a machine learning algorithm has become too specific to the training set. For example, a decision tree could have so many nodes that each piece of data in the training set will end at a different leaf. Unless each new datum is identical to one in the training set, such a tree would have innacurate results. While not always to this extreme, decision trees have a tendancy to overfit because of the granular nature of decision nodes.**"},{"metadata":{},"cell_type":"markdown","source":"**Feature selection** will be focused on in this notebook!\nOther important parameters of decision trees include maximum depth and minimum sample leafs.[](http://)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get rid of unnecessary columns\nLooks like we can get rid of id, diagnosis, and Unnamed: 32"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.diagnosis                          # M or B \nlist = ['Unnamed: 32','id','diagnosis'] #makes a list of unnecessary columns\nx = data.drop(list,axis = 1 ) #deletes those columns\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization\nWe're gonna skip other data visualization, besides heatmap, which visualizes correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time for Decision Trees!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate feature selection and decision tree classification\n\nInstead of hand picking features, we're gonna use a sklearn method called 'SelectKBest', to do it for us, statistically. This will pick out k number of informative features. https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n\nThere are other ways like feature selection with correlation, recursive feature elimination, tree based feature selection, principle component analysis (PCA), etc. \n@DATAI's notebook on \"Feature Selection and Data Visualization\" for this dataset is helpful for reference: https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization#Feature-Selection-and-Random-Forest-Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\nprint(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = select_feature.transform(x_train)\nx_test = select_feature.transform(x_test)\nprint(x_train.shape)\n\n\n#random forest classifier with n_estimators=10 (default)\nmodel = DecisionTreeClassifier()      \nmodel = model.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation\n![](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n\n\nConfusion matrix helps visualize your model's predictions against the actual diagnosis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#print accuracy\nac_2 = accuracy_score(y_test,model.predict(x_test))\nprint('Accuracy is: ',ac_2)\n\n#print confusion matrix\ncm_2 = confusion_matrix(y_test,model.predict(x_test))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You want to maximize the diagonal from top left to bottom right, which is the true positive data (predicted malignant when there is breast cancer and benign when there isn't breast cancer)."},{"metadata":{},"cell_type":"markdown","source":"# Visualize your decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\nfrom sklearn import tree\n\ndot_data = tree.export_graphviz(model, out_file=None, filled=True)\n\ngraph = graphviz.Source(dot_data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pros and Cons of Decision Trees\n\nPros:\nEasy to understand and interpret, perfect for visual representation.\nCan work with numerical and categorical features.\n\nCons:\nTends to overfit"},{"metadata":{},"cell_type":"markdown","source":"You can explore other parameters of DecisionTreeClassifier here: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n\nIn addition, try exploring RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}