{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hyper-parameters","metadata":{}},{"cell_type":"code","source":"TRAIN_IMG_SIZE = 128\nVAL_IMG_SIZE = 128\nTRAINING = True\nEPOCHS = 20\nBATCH_SIZE = 32\n\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.18\n\n#DeepLabV3+\nENCODER = 'resnet101'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\nENCODER_OUTPUT_STRIDE=16 #default: 16\nDECODER_ATROUS_RATES=(2, 4, 6) #default: (12, 24, 36)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:06:25.308068Z","iopub.execute_input":"2021-07-11T22:06:25.308551Z","iopub.status.idle":"2021-07-11T22:06:25.322229Z","shell.execute_reply.started":"2021-07-11T22:06:25.308514Z","shell.execute_reply":"2021-07-11T22:06:25.32134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\n## Base Line\n\n| model | val Acc(%) | val IoU(%) |\n| ---- | ---- | ---- |\n| mobilenet_v2 + U-Net | 88.80 | - |\n| mobilenet_v2 + DeepLabV3+ | 93.82 | 83.05 |\n\n## Model\n\n| model | val IoU(%) |\n| ---- | ---- |\n| mobilenet_v2 | 83.61 |\n| resnet34 | 85.27 |\n| resnet50 | 85.91 |\n| resnet101 | 86.13 |\n| resnet152 | **86.14** |\n\n## Image Size\n\n| train | val | val IoU(%) |\n| ---- | ---- |  |\n| 128 | 128 | 86.13 |\n| 128 | 256 | 83.67 |\n| 256 | 256 | **88.97** |\n\n## Batch Size\n\n| size | val IoU(%) |\n| ---- | ---- |\n| 64 | 85.75 |\n| 32 | **86.01** |\n| 16 | 85.3 |\n\n## Learning Rate\n\n### epochs=20\n\n| lr | val IoU(%) |\n| ---- | ---- |\n| 0.0001 | 85.92 |\n| 0.00009 | 86.13 |\n| 0.000085 | 86.05 |\n| 0.00008 | 86.13 |\n| 0.00007 | 85.69 |\n| CA0.0001-0.00005,T_mult1 | 85.74 |\n| CA0.00009-0.00008,T_mult1 | **86.33** |\n| CA0.00008-0.00005,T_mult1 | 85.64 |\n\n### epochs=30\n\n| lr | val IoU(%) |\n| ---- | ---- |\n| CA0.00009-0.000075,T_mult1 | 86.71 |\n| CA0.00009-0.000075,T_mult2 | 86.67 |\n\n## Loss Function\n\n| Loss | val IoU(%) |\n| ---- | ---- |\n| CrossEntropy | 86.01 |\n| Dice | 85.86 |\n| 0.75*CE + 0.25*Dice | **86.09** |\n\n## Atrous Rates\n\n### Image Size (128, 128), Output Strides 16\n\n| rates | val IoU(%) |\n| ---- | ---- |\n| (2, 4, 6) | **86.13** |\n| (2, 4, 8) | 86.09 |\n| (12, 24, 36) | 85.63 |\n\n## Augmentation\n\n| Aug | val IoU(%) |\n| ---- | ---- |\n| No Aug | 59.79 |\n| Aug | 59.99 |\n\n## Best Score\n\n| val Accuracy(%) | val IoU(%) |\n| ---- | ---- |\n| 96.27 | 89.41 |\n\n- encoder: ResNet152\n- epoch: 30\n- image size: 256\n- batch size: 32\n- learning rate: 0.00009-8 cosin annealing\n- loss: 0.75\\*cross entropy + 0.25\\*dice\n- output stride: 16\n- atrous rates: (4, 8, 12)\n","metadata":{}},{"cell_type":"markdown","source":"# Reference\n\n## Data Augmentation\n\nhttps://blog.shikoan.com/manual-augmentation/#6_Auto_Contrast\nhttps://qiita.com/kurilab/items/b69e1be8d0224ae139ad#randombrightnesscontrast\n\n## Batch size tuning\n\nhttps://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\n\n### learning rate scheduler\n\nhttps://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b  \nhttps://arxiv.org/abs/1608.03983\n\n## segmentation_models_pytorch\n\nhttps://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/deeplabv3/model.py\n","metadata":{}},{"cell_type":"markdown","source":"# Note\n\n## batch size\n\n### 64 -> 32\n\n- Â≠¶ÁøíÈÄüÂ∫¶„ÉªÁ≤æÂ∫¶Âêë‰∏ä\n\n### 32 -> 16\n\n- Â≠¶ÁøíÈÄüÂ∫¶Âêë‰∏ä„ÄÅÁ≤æÂ∫¶‰Ωé‰∏ã\n- 12epoch„Åª„Å©„Åßtrain,val„Å®„ÇÇ„Å´È†≠Êâì„Å°","metadata":{}},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"markdown","source":"### Libraries üìö‚¨á","metadata":{}},{"cell_type":"code","source":"import os, cv2\nimport numpy as np\nimport pandas as pd\nimport random, tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.utils import shuffle\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport albumentations as album","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-11T22:03:40.278093Z","iopub.execute_input":"2021-07-11T22:03:40.278395Z","iopub.status.idle":"2021-07-11T22:03:43.058596Z","shell.execute_reply.started":"2021-07-11T22:03:40.278367Z","shell.execute_reply":"2021-07-11T22:03:43.057851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:03:43.061363Z","iopub.execute_input":"2021-07-11T22:03:43.061627Z","iopub.status.idle":"2021-07-11T22:03:57.462394Z","shell.execute_reply.started":"2021-07-11T22:03:43.061602Z","shell.execute_reply":"2021-07-11T22:03:57.461381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Data & Create train / valid splits üìÅ","metadata":{}},{"cell_type":"code","source":"def split_paths(paths, train_rate, val_rate):\n    train_n = int(len(paths) * train_rate)\n    val_n = int(len(paths) * val_rate)\n    \n    train = paths[:train_n]\n    val = paths[train_n:train_n + val_n]\n    test = paths[train_n + val_n:]\n    \n    return train, val, test","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:03:57.464037Z","iopub.execute_input":"2021-07-11T22:03:57.464385Z","iopub.status.idle":"2021-07-11T22:03:57.470358Z","shell.execute_reply.started":"2021-07-11T22:03:57.464346Z","shell.execute_reply":"2021-07-11T22:03:57.469578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_PATH = '/kaggle/input/the-oxfordiiit-pet-dataset/images/images'\nANNOTATION_PATH = '/kaggle/input/the-oxfordiiit-pet-dataset/annotations/annotations/trimaps'\n\ninput_img_paths = []\nannotation_img_paths = []\nfor fname in os.listdir(IMG_PATH):\n    img_path = os.path.join(IMG_PATH, fname)\n    ann_path = os.path.join(ANNOTATION_PATH, fname.replace(\".jpg\", \".png\"))\n    if cv2.imread(img_path) is None or cv2.imread(ann_path) is None:\n        print(f\"cannot not load {fname}\")\n        continue\n\n    input_img_paths.append(img_path)\n    annotation_img_paths.append(ann_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:03:57.472204Z","iopub.execute_input":"2021-07-11T22:03:57.472745Z","iopub.status.idle":"2021-07-11T22:06:25.291525Z","shell.execute_reply.started":"2021-07-11T22:03:57.472706Z","shell.execute_reply":"2021-07-11T22:06:25.290711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_img_paths, annotation_img_paths = shuffle(input_img_paths, annotation_img_paths)\n\ntrain_img_paths, val_img_paths, test_img_paths = split_paths(input_img_paths, TRAIN_SPLIT, VAL_SPLIT)\nprint(len(train_img_paths), len(val_img_paths), len(test_img_paths))\n\ntrain_label_paths, val_label_paths, test_label_paths = split_paths(annotation_img_paths, TRAIN_SPLIT, VAL_SPLIT)\nprint(len(train_label_paths), len(val_label_paths), len(test_label_paths))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:06:25.292993Z","iopub.execute_input":"2021-07-11T22:06:25.293326Z","iopub.status.idle":"2021-07-11T22:06:25.306707Z","shell.execute_reply.started":"2021-07-11T22:06:25.29329Z","shell.execute_reply":"2021-07-11T22:06:25.305906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## visualize mask","metadata":{}},{"cell_type":"code","source":"mask = cv2.cvtColor(cv2.imread(annotation_img_paths[0]), cv2.COLOR_BGR2RGB)\nmask.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:39.681466Z","iopub.execute_input":"2021-07-11T22:08:39.681953Z","iopub.status.idle":"2021-07-11T22:08:39.711837Z","shell.execute_reply.started":"2021-07-11T22:08:39.68191Z","shell.execute_reply":"2021-07-11T22:08:39.711198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(mask[:,:,:])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:39.714917Z","iopub.execute_input":"2021-07-11T22:08:39.715168Z","iopub.status.idle":"2021-07-11T22:08:39.87883Z","shell.execute_reply.started":"2021-07-11T22:08:39.715144Z","shell.execute_reply":"2021-07-11T22:08:39.878006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.imshow(mask[:,:,0])\nplt.figure()\nplt.imshow(mask[:,:,1])\nplt.figure()\nplt.imshow(mask[:,:,2])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:39.880481Z","iopub.execute_input":"2021-07-11T22:08:39.880832Z","iopub.status.idle":"2021-07-11T22:08:40.282687Z","shell.execute_reply.started":"2021-07-11T22:08:39.880796Z","shell.execute_reply":"2021-07-11T22:08:40.281851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RGB„Åô„Åπ„Å¶„ÅÆ„É¨„Ç§„É§„Éº„ÅßÊï∞ÂÄ§„ÅåÂêå„Åò„Ç∞„É¨„Éº„Çπ„Ç±„Éº„É´„ÅÆÁîªÂÉè","metadata":{}},{"cell_type":"code","source":"print((mask[:,:,0] == mask[:,:,1]).all())\nprint((mask[:,:,0] == mask[:,:,2]).all())","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.2845Z","iopub.execute_input":"2021-07-11T22:08:40.284858Z","iopub.status.idle":"2021-07-11T22:08:40.293088Z","shell.execute_reply.started":"2021-07-11T22:08:40.284819Z","shell.execute_reply":"2021-07-11T22:08:40.29199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### „É©„Éô„É´„ÅÆÁ¢∫Ë™ç\n\n- 1: object\n- 2: background\n- 3: edge","metadata":{}},{"cell_type":"code","source":"mask_ = cv2.resize(mask, dsize=(28, 28))\nprint(mask_[:,:,0])\nplt.imshow(mask_[:,:,0])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.297259Z","iopub.execute_input":"2021-07-11T22:08:40.297571Z","iopub.status.idle":"2021-07-11T22:08:40.437182Z","shell.execute_reply.started":"2021-07-11T22:08:40.297517Z","shell.execute_reply":"2021-07-11T22:08:40.436293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA_DIR = '../input/deepglobe-road-extraction-dataset'\n\n# metadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\n# metadata_df = metadata_df[metadata_df['split']=='train']\n# metadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\n# metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n# metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n# # Shuffle DataFrame\n# metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n\n# # Perform 90/10 split for train / val\n# valid_df = metadata_df.sample(frac=0.1, random_state=42)\n# train_df = metadata_df.drop(valid_df.index)\n# len(train_df), len(valid_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.440378Z","iopub.execute_input":"2021-07-11T22:08:40.440701Z","iopub.status.idle":"2021-07-11T22:08:40.447451Z","shell.execute_reply.started":"2021-07-11T22:08:40.440669Z","shell.execute_reply":"2021-07-11T22:08:40.446492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.cvtColor(cv2.imread(input_img_paths[0]), cv2.COLOR_BGR2RGB)\nimg = cv2.resize(img, dsize=(10, 10))\nprint(img[:,:,0])\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.448559Z","iopub.execute_input":"2021-07-11T22:08:40.449047Z","iopub.status.idle":"2021-07-11T22:08:40.577418Z","shell.execute_reply.started":"2021-07-11T22:08:40.449001Z","shell.execute_reply":"2021-07-11T22:08:40.576624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## metadata","metadata":{}},{"cell_type":"code","source":"# Get class names\nclass_names = [\"object\", \"background\", \"edge\"]\n# Get class RGB values\nclass_rgb_values = [[1,1,1], [2,2,2], [3,3,3]]\n\nprint('All dataset classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.578634Z","iopub.execute_input":"2021-07-11T22:08:40.578969Z","iopub.status.idle":"2021-07-11T22:08:40.585121Z","shell.execute_reply.started":"2021-07-11T22:08:40.578935Z","shell.execute_reply":"2021-07-11T22:08:40.584303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shortlist specific classes to segment","metadata":{}},{"cell_type":"code","source":"# Useful to shortlist specific classes in datasets with large number of classes\nselect_classes = [\"object\", \"background\", \"edge\"]\n\n# Get RGB values of required classes\nselect_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\nselect_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n\nprint('Selected classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.586486Z","iopub.execute_input":"2021-07-11T22:08:40.586715Z","iopub.status.idle":"2021-07-11T22:08:40.596508Z","shell.execute_reply.started":"2021-07-11T22:08:40.586692Z","shell.execute_reply":"2021-07-11T22:08:40.595598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions for viz. & one-hot encoding/decoding","metadata":{}},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"\n    Plot images in one row\n    \"\"\"\n    n_images = len(images)\n    plt.figure(figsize=(20,8))\n    for idx, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n_images, idx + 1)\n        plt.xticks([]); \n        plt.yticks([])\n        # get title from the parameter names\n        plt.title(name.replace('_',' ').title(), fontsize=20)\n        plt.imshow(image)\n    plt.show()\n\n# Perform one hot encoding on label\ndef one_hot_encode(label, label_values):\n    \"\"\"\n    Convert a segmentation image label array to one-hot format\n    by replacing each pixel value with a vector of length num_classes\n    # Arguments\n        label: The 2D array segmentation image label\n        label_values\n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of num_classes\n    \"\"\"\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis = -1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1)\n\n    return semantic_map\n    \n# Perform reverse one-hot-encoding on labels / preds\ndef reverse_one_hot(image):\n    \"\"\"\n    Transform a 2D array in one-hot format (depth is num_classes),\n    to a 2D array with only 1 channel, where each pixel value is\n    the classified class key.\n    # Arguments\n        image: The one-hot format image \n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of 1, where each pixel value is the classified \n        class key.\n    \"\"\"\n    x = np.argmax(image, axis = -1)\n    return x\n\n# Perform colour coding on the reverse-one-hot outputs\ndef colour_code_segmentation(image, label_values):\n    \"\"\"\n    Given a 1-channel array of class keys, colour code the segmentation results.\n    # Arguments\n        image: single channel array where each value represents the class key.\n        label_values\n\n    # Returns\n        Colour coded image for segmentation visualization\n    \"\"\"\n    colour_codes = np.array(label_values)\n    x = colour_codes[image.astype(int)]\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.598657Z","iopub.execute_input":"2021-07-11T22:08:40.599179Z","iopub.status.idle":"2021-07-11T22:08:40.614178Z","shell.execute_reply.started":"2021-07-11T22:08:40.599144Z","shell.execute_reply":"2021-07-11T22:08:40.613241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoadsDataset(torch.utils.data.Dataset):\n\n    \"\"\"DeepGlobe Road Extraction Challenge Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        df (str): DataFrame containing images / labels paths\n        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    def __init__(\n            self, \n            image_paths,\n            mask_paths,\n            class_rgb_values=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        \n        self.class_rgb_values = class_rgb_values\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read images and masks\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n        \n        # one-hot-encode the mask\n        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # return length of \n        return len(self.image_paths)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.615935Z","iopub.execute_input":"2021-07-11T22:08:40.616375Z","iopub.status.idle":"2021-07-11T22:08:40.629701Z","shell.execute_reply.started":"2021-07-11T22:08:40.616338Z","shell.execute_reply":"2021-07-11T22:08:40.628676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize Sample Image and Mask üìà","metadata":{}},{"cell_type":"code","source":"dataset = RoadsDataset(input_img_paths, annotation_img_paths, class_rgb_values=select_class_rgb_values)\nrandom_idx = random.randint(0, len(dataset)-1)\nimage, mask = dataset[random_idx]\n\nprint(image[:5,:5,0])\n\nground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values)\nprint(ground_truth_mask.shape)\nprint(ground_truth_mask[:10,:10,0])\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = reverse_one_hot(mask),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.631057Z","iopub.execute_input":"2021-07-11T22:08:40.631444Z","iopub.status.idle":"2021-07-11T22:08:40.837496Z","shell.execute_reply.started":"2021-07-11T22:08:40.631408Z","shell.execute_reply":"2021-07-11T22:08:40.836569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Augmentations üôÉ","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        album.Resize(TRAIN_IMG_SIZE, TRAIN_IMG_SIZE, p=1),\n        album.HorizontalFlip(p=0.5),\n        album.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=15, p=0.5),\n#         album.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        album.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n        album.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),\n    ]\n    return album.Compose(train_transform)\n\ndef get_validation_augmentation():\n    train_transform = [\n        album.Resize(VAL_IMG_SIZE, VAL_IMG_SIZE, p=1),\n        album.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),\n    ]\n    return album.Compose(train_transform)\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn=None):\n    \"\"\"Construct preprocessing transform    \n    Args:\n        preprocessing_fn (callable): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \"\"\"\n    _transform = []\n    if preprocessing_fn:\n        _transform.append(album.Lambda(image=preprocessing_fn))\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n        \n    return album.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:56:25.261054Z","iopub.execute_input":"2021-07-11T22:56:25.261386Z","iopub.status.idle":"2021-07-11T22:56:25.272723Z","shell.execute_reply.started":"2021-07-11T22:56:25.261356Z","shell.execute_reply":"2021-07-11T22:56:25.27189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize Augmented Images & Masks","metadata":{}},{"cell_type":"code","source":"augmented_dataset = RoadsDataset(\n    train_img_paths,\n    train_label_paths, \n    augmentation=get_training_augmentation(),\n    class_rgb_values=select_class_rgb_values,\n)\n\nrandom_idx = random.randint(0, len(augmented_dataset)-1)\n\n# Different augmentations on image/mask pairs\nfor _ in range(10):\n    image, mask = augmented_dataset[random_idx]\n    visualize(\n        original_image = image,\n        ground_truth_mask = reverse_one_hot(mask)\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:08:40.854629Z","iopub.execute_input":"2021-07-11T22:08:40.854955Z","iopub.status.idle":"2021-07-11T22:08:42.416421Z","shell.execute_reply.started":"2021-07-11T22:08:40.854917Z","shell.execute_reply":"2021-07-11T22:08:42.41556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training DeepLabV3+","metadata":{}},{"cell_type":"markdown","source":"<h3><center>DeepLabV3+ Model Architecture</center></h3>\n<img src=\"https://miro.medium.com/max/1000/1*2mYfKnsX1IqCCSItxpXSGA.png\" width=\"750\" height=\"750\"/>\n<h4></h4>\n<h4><center><a href=\"https://arxiv.org/abs/1802.02611\">Image Source: DeepLabV3+ [Liang-Chieh Chen et al.]</a></center></h4>","metadata":{}},{"cell_type":"markdown","source":"### Model Definition","metadata":{}},{"cell_type":"code","source":"CLASSES = select_classes\n\n# create segmentation model with pretrained encoder\nmodel = smp.DeepLabV3Plus(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n    encoder_output_stride=ENCODER_OUTPUT_STRIDE, #default: 16\n    decoder_atrous_rates=DECODER_ATROUS_RATES, #default: (12, 24, 36)\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:40.484677Z","iopub.execute_input":"2021-07-11T22:57:40.48503Z","iopub.status.idle":"2021-07-11T22:57:41.679432Z","shell.execute_reply.started":"2021-07-11T22:57:40.484998Z","shell.execute_reply":"2021-07-11T22:57:41.67866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get Train / Val DataLoaders","metadata":{}},{"cell_type":"code","source":"# Get train and val dataset instances\ntrain_dataset = RoadsDataset(\n    train_img_paths,\n    train_label_paths,\n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\nvalid_dataset = RoadsDataset(\n    val_img_paths,\n    val_label_paths,\n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\n# Get train and val data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:41.681419Z","iopub.execute_input":"2021-07-11T22:57:41.681782Z","iopub.status.idle":"2021-07-11T22:57:41.689158Z","shell.execute_reply.started":"2021-07-11T22:57:41.681744Z","shell.execute_reply":"2021-07-11T22:57:41.68802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Set Hyperparams","metadata":{}},{"cell_type":"code","source":"# Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"DEVICE: {DEVICE}\")\n\n# define loss function\nclass CeDiceLoss(smp.utils.losses.base.Loss):\n    def __init__(self, alpha=0.75, **kwargs):\n        super().__init__(**kwargs)\n        self.alpha = alpha\n\n    def forward(self, y_pr, y_gt):\n        return self.alpha * smp.utils.losses.BCELoss().forward(y_pr, y_gt) \\\n                + (1-self.alpha) * smp.utils.losses.DiceLoss().forward(y_pr, y_gt)\n\nloss = CeDiceLoss(alpha=0.75)\n# loss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [\n    smp.utils.metrics.Accuracy(),\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\n# define optimizer\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])\n\n# load best saved model checkpoint from previous commit (if present)\nif os.path.exists('../input/the-oxfordiiit-pet-dataset-deeplabv3/best_model.pth'):\n    model = torch.load('../input/the-oxfordiiit-pet-dataset-deeplabv3/best_model.pth', map_location=DEVICE)\n    print('Loaded pre-trained DeepLabV3+ model!')","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:41.690562Z","iopub.execute_input":"2021-07-11T22:57:41.690936Z","iopub.status.idle":"2021-07-11T22:57:41.708662Z","shell.execute_reply.started":"2021-07-11T22:57:41.690899Z","shell.execute_reply":"2021-07-11T22:57:41.707617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:41.710416Z","iopub.execute_input":"2021-07-11T22:57:41.710855Z","iopub.status.idle":"2021-07-11T22:57:41.79595Z","shell.execute_reply.started":"2021-07-11T22:57:41.710818Z","shell.execute_reply":"2021-07-11T22:57:41.795093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define lr scheduler of Cosin Annealing","metadata":{}},{"cell_type":"code","source":"class CosineAnnealingWarmRestarts:\n    def __init__(self, epoch=10, min_lr=1e-4, max_lr=1e-3, T_0=1, T_mult=1):\n        self.epoch = epoch\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.T_0 = T_0\n        self.T_mult = T_mult\n\n    def calc_lr(self, ep):\n        if ep < self.T_0:\n            return self.max_lr\n        ep = ep - (self.T_0-1)\n\n        if self.T_mult == 1:\n            T_cur = ep-1\n            T_i = self.epoch - self.T_0\n            return self.min_lr + 1/2 * (self.max_lr - self.min_lr) * (1 + np.cos(T_cur/T_i*np.pi))\n\n        def calc_restart_count(e, cnt):\n            if e * self.T_mult > ep:\n                return cnt\n            return calc_restart_count(e * self.T_mult, cnt+1)\n\n        restart_count = calc_restart_count(1, 0)\n        T_cur = ep - self.T_mult ** restart_count\n        T_i = max(self.T_mult ** restart_count - 1, 1)\n    #     print(epoch, restart_count, T_cur, T_i)\n\n        return self.min_lr + 1/2 * (self.max_lr - self.min_lr) * (1 + np.cos(T_cur/T_i*np.pi))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:41.797894Z","iopub.execute_input":"2021-07-11T22:57:41.798242Z","iopub.status.idle":"2021-07-11T22:57:41.81023Z","shell.execute_reply.started":"2021-07-11T22:57:41.798207Z","shell.execute_reply":"2021-07-11T22:57:41.809212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ca = CosineAnnealingWarmRestarts(epoch=35, min_lr=0, max_lr=1, T_0=5, T_mult=1)\na = [ca.calc_lr(i) for i in range(1, 36)]\nplt.plot(range(1, len(a)+1), a)\nplt.title(\"T_mult: 1\")\n\nca = CosineAnnealingWarmRestarts(epoch=35, min_lr=0, max_lr=1, T_0=6, T_mult=2)\na = [ca.calc_lr(i) for i in range(1, 36)]\nplt.figure()\nplt.plot(range(1, len(a)+1), a)\nplt.title(\"T_mult: 2\")","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:41.811569Z","iopub.execute_input":"2021-07-11T22:57:41.812177Z","iopub.status.idle":"2021-07-11T22:57:42.220938Z","shell.execute_reply.started":"2021-07-11T22:57:41.812134Z","shell.execute_reply":"2021-07-11T22:57:42.219922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ca_lr_scheduler = CosineAnnealingWarmRestarts(epoch=EPOCHS, min_lr=0.00008, max_lr=0.00009, T_0=1, T_mult=1)\n\na = [ca_lr_scheduler.calc_lr(i) for i in range(1, EPOCHS+1)]\nplt.figure()\nplt.plot(range(1, len(a)+1), a)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:42.22273Z","iopub.execute_input":"2021-07-11T22:57:42.223104Z","iopub.status.idle":"2021-07-11T22:57:42.358891Z","shell.execute_reply.started":"2021-07-11T22:57:42.223065Z","shell.execute_reply":"2021-07-11T22:57:42.357771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training DeepLabV3+","metadata":{}},{"cell_type":"code","source":"%%time\n\nif TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(1, EPOCHS+1):\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n\n        lr = ca_lr_scheduler.calc_lr(i)\n        print('Learning rate: {}'.format(lr))\n\n        optimizer = torch.optim.Adam([ \n            dict(params=model.parameters(), lr=lr),\n        ])\n\n        train_epoch = smp.utils.train.TrainEpoch(\n            model, \n            loss=loss, \n            metrics=metrics, \n            optimizer=optimizer,\n            device=DEVICE,\n            verbose=True,\n        )\n\n        train_logs = train_epoch.run(train_loader)\n        valid_logs = valid_epoch.run(valid_loader)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, './best_model.pth')\n            print('Model saved!')","metadata":{"execution":{"iopub.status.busy":"2021-07-11T22:57:42.360703Z","iopub.execute_input":"2021-07-11T22:57:42.361097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction on Test Data","metadata":{}},{"cell_type":"code","source":"# load best saved model checkpoint from the current run\nif os.path.exists('./best_model.pth'):\n    best_model = torch.load('./best_model.pth', map_location=DEVICE)\n    print('Loaded DeepLabV3+ model from this run.')\n\n# load best saved model checkpoint from previous commit (if present)\nelif os.path.exists('../input/the-oxfordiiit-pet-dataset-deeplabv3/best_model.pth'):\n    best_model = torch.load('../input/the-oxfordiiit-pet-dataset-deeplabv3/best_model.pth', map_location=DEVICE)\n    print('Loaded DeepLabV3+ model from a previous commit.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create test dataloader to be used with DeepLabV3+ model (with preprocessing operation: to_tensor(...))\ntest_dataset = RoadsDataset(\n    test_img_paths,\n    test_label_paths,\n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\ntest_dataloader = DataLoader(test_dataset)\n\n# test dataset for visualization (without preprocessing augmentations & transformations)\ntest_dataset_vis = RoadsDataset(\n    test_img_paths,\n    test_label_paths,\n    class_rgb_values=select_class_rgb_values,\n)\n\n# get a random test image/mask index\nrandom_idx = random.randint(0, len(test_dataset_vis)-1)\nimage, mask = test_dataset_vis[random_idx]\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = reverse_one_hot(mask),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_preds_folder = 'sample_predictions/'\nif not os.path.exists(sample_preds_folder):\n    os.makedirs(sample_preds_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_mask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 30\nidxes = np.random.randint(0,len(test_img_paths),n)\nfor idx in idxes:\n    image, gt_mask = test_dataset[idx]\n    image_vis, _ = test_dataset_vis[idx]\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    # Predict test image\n    pred_mask = best_model(x_tensor)\n    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n\n#     print(image_vis.shape, gt_mask.shape, pred_mask.shape)\n\n    # Convert pred_mask from `CHW` format to `HWC` format\n    pred_mask = np.transpose(pred_mask,(1,2,0))\n    gt_mask = np.transpose(gt_mask,(1,2,0))\n    \n    visualize(\n        original_image = cv2.resize(image_vis, dsize=(128, 128)),\n        ground_truth_mask = reverse_one_hot(gt_mask),\n        predicted_mask = reverse_one_hot(pred_mask),\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation on Test Dataset","metadata":{}},{"cell_type":"code","source":"test_epoch = smp.utils.train.ValidEpoch(\n    model,\n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_logs = test_epoch.run(test_dataloader)\nprint(\"Evaluation on Test Data: \")\nprint(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\nprint(f\"Mean Dice Loss: {valid_logs['ce_dice_loss']:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Dice Loss & IoU Metric for Train vs. Val","metadata":{}},{"cell_type":"code","source":"train_logs_df = pd.DataFrame(train_logs_list)\ntrain_logs_df.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_logs_df = pd.DataFrame(valid_logs_list)\nvalid_logs_df.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('IoU Score', fontsize=20)\nplt.title('IoU Score Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('iou_score_plot.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.ce_dice_loss.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.ce_dice_loss.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('Dice Loss', fontsize=20)\nplt.title('Dice Loss Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('dice_loss_plot.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}