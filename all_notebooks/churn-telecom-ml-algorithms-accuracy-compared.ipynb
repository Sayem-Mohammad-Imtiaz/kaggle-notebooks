{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom math import sqrt\n\npd.set_option('display.max_columns', 0)\npd.set_option('display.max_rows', 500)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score\n\n\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df=pd.read_csv(\"../input/churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df=churn_df.rename(columns={'Churn?':'churn'})\nchurn_df=churn_df.rename(columns={\"Int'l Plan\":\"Intl Plan\"})\nchurn_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df['churn'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df['churn']=churn_df['churn'].apply(lambda x:1 if x==\"True.\" else 0 )\nchurn_df['Intl Plan']=churn_df['Intl Plan'].apply(lambda x:1 if x==\"yes\" else 0 )\nchurn_df['VMail Plan']=churn_df['VMail Plan'].apply(lambda x:1 if x==\"yes\" else 0 )\nchurn_df['VMail Plan'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df_dropped=churn_df.drop(['State','Area Code','Phone'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_df_dropped.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=churn_df_dropped.drop(columns=['churn'])\ny=churn_df_dropped[['churn']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size = 0.2, random_state = 100)\n\n#y_train = y_train.ravel()\n#y_test = y_test.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using logistic regression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ntrain_Pred = logreg.predict(X_train)\ntest_pred = logreg.predict(X_test)\nprint(\"Accuracy of Logistic regression for train\",metrics.accuracy_score(y_train,train_Pred))\nprint(\"Accuracy of Logistic regression for test\",metrics.accuracy_score(y_test,test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\naccuracy_train_dict={}\naccuracy_test_dict={}\ndf_len=round(sqrt(len(churn_df_dropped)))\nfor k in range(3,df_len):\n    K_value = k+1\n    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n    neigh.fit(X_train, y_train) \n    y_pred_train = neigh.predict(X_train)\n    y_pred_test = neigh.predict(X_test)    \n    train_accuracy=accuracy_score(y_train,y_pred_train)*100\n    test_accuracy=accuracy_score(y_test,y_pred_test)*100\n    accuracy_train_dict.update(({k:train_accuracy}))\n    accuracy_test_dict.update(({k:test_accuracy}))\n    print (\"Accuracy for train :\",train_accuracy ,\" and test :\",test_accuracy,\"% for K-Value:\",K_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using Naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nNB=GaussianNB()\nNB.fit(X_train, y_train)\ntrain_pred=NB.predict(X_train)\ntest_pred=NB.predict(X_test)\nprint(\"Accuracy of Naive bayes train set\",accuracy_score(train_pred,y_train))\nprint(\"Accuracy of Naive bayes test set\",accuracy_score(test_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Boosting\nimport matplotlib.pyplot as plt\n# Adaboost Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\ndtree = DecisionTreeClassifier(criterion='gini',max_depth=1)\n\nadabst_fit = AdaBoostClassifier(base_estimator= dtree,\n        n_estimators=5000,learning_rate=0.05,random_state=42)\n\nadabst_fit.fit(X_train, y_train)\n\n#print (\"\\nAdaBoost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,adabst_fit.predict(X_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nAdaBoost  - Train accuracy\",round(accuracy_score(y_train,adabst_fit.predict(X_train)),3))\n#print (\"\\nAdaBoost  - Train Classification Report\\n\",classification_report(y_train,adabst_fit.predict(X_train)))\n\n#print (\"\\n\\nAdaBoost  - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,adabst_fit.predict(X_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nAdaBoost  - Test accuracy\",round(accuracy_score(y_test,adabst_fit.predict(X_test)),3))\n#print (\"\\nAdaBoost - Test Classification Report\\n\",classification_report(y_test,adabst_fit.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradientboost Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbc_fit = GradientBoostingClassifier(loss='deviance',learning_rate=0.05,n_estimators=5000,\n                                     min_samples_split=2,min_samples_leaf=1,max_depth=1,random_state=42 )\ngbc_fit.fit(X_train,y_train)\n\n#print (\"\\nGradient Boost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,gbc_fit.predict(X_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nGradient Boost - Train accuracy\",round(accuracy_score(y_train,gbc_fit.predict(X_train)),3))\n#print (\"\\nGradient Boost  - Train Classification Report\\n\",classification_report(y_train,gbc_fit.predict(X_train)))\n\n#print (\"\\n\\nGradient Boost - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,gbc_fit.predict(X_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nGradient Boost - Test accuracy\",round(accuracy_score(y_test,gbc_fit.predict(X_test)),3))\n#print (\"\\nGradient Boost - Test Classification Report\\n\",classification_report(y_test,gbc_fit.predict(X_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Xgboost Classifier\nimport xgboost as xgb\n\nxgb_fit = xgb.XGBClassifier(max_depth=2, n_estimators=5000, learning_rate=0.05)\nxgb_fit.fit(X_train, y_train)\n\n#print (\"\\nXGBoost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,xgb_fit.predict(X_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nXGBoost - Train accuracy\",round(accuracy_score(y_train,xgb_fit.predict(X_train)),3))\n#print (\"\\nXGBoost  - Train Classification Report\\n\",classification_report(y_train,xgb_fit.predict(X_train)))\n\n#print (\"\\n\\nXGBoost - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,xgb_fit.predict(X_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nXGBoost - Test accuracy\",round(accuracy_score(y_test,xgb_fit.predict(X_test)),3))\n#print (\"\\nXGBoost - Test Classification Report\\n\",classification_report(y_test,xgb_fit.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = DecisionTreeClassifier()\n\n\n# Choose some parameter combinations to try\nparameters = {'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [i for i in range(1,20)], \n              'min_samples_split': [i for i in range(2,10)],\n              'min_samples_leaf': [i for i in range(2,10)]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nprint(clf.fit(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict target value and find accuracy score\ny_pred_train = clf.predict(X_train)\nprint(\"Accuracy score of train is \",accuracy_score(y_train, y_pred_train))\ny_pred = clf.predict(X_test)\nprint(\"Accuracy score of test is \",accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}