{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nWhen faced with a Machine Learning task, finding the right algorithm for the problem could be the difference between success and failure of the assignment.\nThis project is about finding the most suitable Machine learning algorithms to solve a given regression problem \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the important libraries\n# =============================================================================\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Reading data and Data Preprocessing\n# =============================================================================\ndf1 = pd.read_csv('/kaggle/input/bmi-data/bmi_data.csv', skiprows = 1, names = ['Sex', 'Age','Height', 'Weight', 'BMI'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data exploration\n# =============================================================================\ndf1.info()\ndf1.columns\ndf1.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for missing values\nsns.heatmap(df1.isnull(), cbar =False)  #Too small to show on the map?\ndf1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for outliers\nax = sns.boxplot(data=df1, orient=\"h\", palette=\"Set2\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Null values are only 0.2% of the dataset. Since the affected variables are all numeric, we can fill them with their respective means.\n#df2 = df1.dropna(axis=0)             \ndf2 = df1.fillna(df1.mean())  #checked using Paired T Test and Chi Square, but found no statistical difference between the mean and standard deviations before and after the fill.\ndf2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding the categorical variable \"Sex\"\ndf = pd.get_dummies(df2,columns=['Sex'],drop_first=True)\n#describe() method to show the summary statistics.\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To get a feel for the type of data we are dealing with, we shall plot a histogram.\ndf.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The variable we are going to predict is the “BMI”. So let’s look at how much each independent variable correlates with this dependent variable\ncorr = df.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the corr matrix for better understanding of the correlation \ncorr_matrix = df.corr()\ncorr_matrix['BMI'].sort_values(ascending=False)           \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can clearly see from the corr map and/or matrix that there's negligible linear relationship between\n#age and BMI, and similarly Sex and BMI. Later, we will see with OLS what their p_values might be.\n\n#Let’s create scatter plots for each independent variable to visualize the data:\nplt.scatter(df.Height, df.BMI, c='red', label='Height vs BMI')\nplt.xlabel('Height(inches)')\nplt.ylabel('BMI(kg/sqm')\nplt.legend()\nplt.show()\n\nplt.scatter(df.Weight, df.BMI, c='green', label='Weight vs BMI')\nplt.xlabel('Weight(pounds)')\nplt.ylabel('BMI(kg/sqm')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's visualize the scatter plots between variables by using Pandas’ scatter_matrix function\nattributes = ['BMI', 'Height', 'Weight']\nscatter_matrix(df[attributes], figsize=(12, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There appears to be some linear relationship between the dependent and independent variables\n# So let's do Regression plot\nsns.regplot(x=df['Height'], y=df['BMI'], color = 'orange')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=df['Weight'], y=df['BMI'], color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The scales of measurement and range of the independent variables differ, so we will do scaling\n#We will try different scales to see which suits best\nx = pd.DataFrame({\n    'Height': df.Height,\n    'Weight': df.Weight,\n})\n\nscaler = preprocessing.MinMaxScaler()\nminmax_scaled_df = scaler.fit_transform(x)\nminmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['Height', 'Weight'])\n\nscaler = preprocessing.StandardScaler()\nscaled_df = scaler.fit_transform(x)\nscaled_df = pd.DataFrame(scaled_df, columns=['Height', 'Weight'])\n    \nscaler = preprocessing.RobustScaler()\nrobust_scaled_df = scaler.fit_transform(x)\nrobust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['Height', 'Weight'])\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(9, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(x['Height'], ax=ax1)\nsns.kdeplot(x['Weight'], ax=ax1)\n\nax2.set_title('After Min-Max Scaling')\nsns.kdeplot(minmax_scaled_df['Height'], ax=ax2)\nsns.kdeplot(minmax_scaled_df['Weight'], ax=ax2)\n\nax3.set_title('After Standard Scaler')\nsns.kdeplot(scaled_df['Height'], ax=ax3)\nsns.kdeplot(scaled_df['Weight'], ax=ax3)\n\nax4.set_title('After Robust Scaling')\nsns.kdeplot(robust_scaled_df['Height'], ax=ax4)\nsns.kdeplot(robust_scaled_df['Weight'], ax=ax4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Based on the result, Standard Scaler is preferred. We shall be using it to scale for knn later.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Splitting data set for modelling\n# =============================================================================\n# Scaling is not required for Linear regression\nX = df.drop(['BMI'], axis = 1)\ny = df['BMI']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data set \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0, test_size=0.3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Backward Elimination\n# =============================================================================\n\nX1 = sm.add_constant(X_train)\nols = sm.OLS(y_train,X1)\nlr = ols.fit()\n\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features_BE = cols\nprint(selected_features_BE)         #Weight and Height\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.summary())         \nmodel.pvalues\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Modelling\n# =============================================================================\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# =============================================================================\n# 1.Fitting linear regressor on the test set\n# =============================================================================\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred1 = regressor.predict(X_test)\n\nMSE_lr = mean_squared_error(y_test,y_pred1)\nprint(MSE_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,y_pred1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_train,regressor.predict(X_train)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('intercept:', regressor.intercept_)\nprint('slope:', regressor.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We use K-Fold Validation to check the performance of our model\nfrom sklearn.model_selection import cross_val_score\nclf = LinearRegression()\ncross_val_score(clf,X,y, cv=4).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets plot the actual vs predicted BMI and see\nplt.scatter(y_test, y_pred1)\nplt.xlabel(\"Actual BMI\")\nplt.ylabel(\"Predicted BMI\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regularisation:\nfrom sklearn.linear_model import LassoCV, RidgeCV, ElasticNet\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Implementation of LassoCV\nlasso = LassoCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100])\nprint(\"Root Mean Squared Error (Lasso): \", np.sqrt(-cross_val_score(lasso, X, y, cv=4, scoring='neg_mean_squared_error')).mean())\nprint ('MSE_Lasso: ', (np.sqrt(-cross_val_score(lasso, X, y, cv=4, scoring='neg_mean_squared_error')).mean())**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Implementation of ElasticNet\nelastic = ElasticNet(alpha=0.001)\nprint(\"Root Mean Squared Error (ElasticNet): \", np.sqrt(-cross_val_score(elastic, X, y, cv=4, scoring='neg_mean_squared_error')).mean())\nprint('MSE_ElasticNet: ', (np.sqrt(-cross_val_score(elastic, X, y, cv=4, scoring='neg_mean_squared_error')).mean())**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Implementation of RidgeCV\nridge = RidgeCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100])\nprint(\"Root Mean Squared Error (Ridge): \", np.sqrt(-cross_val_score(ridge, X, y, cv=4, scoring='neg_mean_squared_error')).mean())\nprint('MSE_Ridge: ', (np.sqrt(-cross_val_score(ridge, X, y, cv=4, scoring='neg_mean_squared_error')).mean())**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# 2. Implementation of xgboost\n# =============================================================================\n#Tuning parameters for xgboost\ntuned_parameters = [{'max_depth': [5,10, 15, 20, 25, 30],'learning_rate':[0.001, 0.01, 0.1, 0.5], 'n_estimators': [10,20, 50, 100,150,200]}]\nMSE_xgb = ['mean_squared_error(y_test,y_pred2)']\n\nfor value in MSE_xgb:\n    regr = GridSearchCV(xgb.XGBRegressor(), tuned_parameters, cv=4)\n    regr.fit(X_train, y_train)\n    y_true, y_pred2 = y_test, regr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regr.best_params_   # we best accuracy at learning_rate=0.1, max_depth =10  and n_estimators = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regr.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The tuned parameters can be passed directly into the model\nregr = xgb.XGBRegressor(learning_rate=0.1, max_depth=10, n_estimators=200, random_state = 0)\nregr.fit(X_train, y_train)\n\n#Predicting with Xgboost\ny_pred2 = regr.predict(X_test)\n\nMSE_xgb = mean_squared_error(y_test,y_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MSE_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_train,regr.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_pred2)\nplt.xlabel(\"Actual BMI\")\nplt.ylabel(\"Predicted BMI\")\nplt.title(\"xgboost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# 3. Adaboost\n# =============================================================================\n# Finding the best hyper-parameters for AdaBoost\ntuned_parameters = [{'learning_rate': [0.1, 0.5,1,2,3,4,5], 'n_estimators': [25, 50, 100,200,300]}]\nMSE_ada = ['mean_squared_error(y_test,y_pred3)']\n\nfor value in MSE_ada:\n    adaregr = GridSearchCV(AdaBoostRegressor(), tuned_parameters, cv=4)\n    adaregr.fit(X_train, y_train)\n    y_true, y_pred3 = y_test, adaregr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adaregr.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adaregr.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now you can plug in the best hyper-parameter value and run the model straightaway\nadaregr = AdaBoostRegressor(random_state=0, learning_rate = 2, n_estimators=50)\nadaregr.fit(X, y)\n#Predicting with Adaboost\ny_pred3 = adaregr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MSE_ada = mean_squared_error(y_test,y_pred3)\nprint(r2_score(y_test,y_pred3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MSE_ada)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_pred3)\nplt.xlabel(\"Actual BMI\")\nplt.ylabel(\"Predicted BMI\")\nplt.title(\"Adaboost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# 4. For Decision Tree\n# =============================================================================\n# finding the best depth\ntuned_parameters = [{'max_depth': [1,2,3,4,5,10, 15, 20, 25, 50, 100,200]}]\nMSE_dt = ['mean_squared_error(y_test,y_pred4)']\nfor value in MSE_dt:\n    regressor_dt = GridSearchCV(DecisionTreeRegressor(), tuned_parameters, cv=4)\n    regressor_dt.fit(X_train, y_train)\n    y_true, y_pred4 = y_test, regressor_dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_dt.best_params_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_dt.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We found our best result on max_depth = 20. Now let's plug that in and run our model\nregressor_dt = DecisionTreeRegressor(random_state=0, max_depth = 20)\nregressor_dt.fit(X,y)\n\n#Predicting with Decision Tree\ny_pred4 = regressor_dt.predict(X_test)\n\nMSE_dt = mean_squared_error(y_test,y_pred4)\nprint(MSE_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,y_pred4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_train,regressor_dt.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_pred4)\nplt.xlabel(\"Actual BMI\")\nplt.ylabel(\"Predicted BMI\")\nplt.title(\"Decision Tree\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# 5. Random Forest\n# =============================================================================\n#First, let's find the best tuned_parameters for our model\n\ntuned_parameters = [{'max_depth': [5,10, 15, 20, 50, 70], 'n_estimators': [10, 25, 50, 100,150, 200, 250]}]\nMSE_rf = ['mean_squared_error(y_test, y_pred5)']\n\nfor value in MSE_rf:\n    regr_rf = GridSearchCV(RandomForestRegressor(), tuned_parameters, cv=4)\n    regr_rf.fit(X_train, y_train)\n\n    y_true, y_pred5 = y_test, regr_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regr_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now, let's set the best parameters and run our model\nregr_rf = RandomForestRegressor(max_depth=70, random_state=0,\n                             n_estimators=250)\nregr_rf.fit(X, y)  \n#Predicting with Random Forest\ny_pred5 = regr_rf.predict(X_test)\n\nMSE_rf = mean_squared_error(y_test,y_pred5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MSE_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,y_pred5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_train,regr_rf.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_pred5)\nplt.xlabel(\"Actual BMI\")\nplt.ylabel(\"Predicted BMI\")\nplt.title(\"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# 6. KNN\n# =============================================================================\n# Feature Scaling is required for Distance-based algorithms like KNN\nsc = StandardScaler()\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us use GridSearchCV to find the best hyper-parameters for our algorithm\ntuned_parameters = [{'n_neighbors': [1,2,3,4,5,10,15,20], 'p': [1,2]}]\nMSE_knn = ['mean_squared_error(y_test,y_pred)']\n\nfor i in MSE_knn:\n    model = GridSearchCV(KNeighborsRegressor(), tuned_parameters, cv=4)\n    model.fit(X_train_scaled, y_train)\n \n    y_true, y_pred6 = y_test, model.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_params_  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Implementing knn with the best hyper-parameters\n#Fitting knn on the training set\nneigh = KNeighborsRegressor(n_neighbors = 5, metric = 'minkowski', p = 2)\nneigh.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred6 = neigh.predict(X_test)\n\nMSE_knn = mean_squared_error(y_test,y_pred6)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MSE_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,y_pred6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_train,neigh.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_pred6)\nplt.xlabel(\"Actual BMI\")\nplt.ylabel(\"Predicted BMI\")\nplt.title(\"KNN\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# 7. SVM\n# =============================================================================\n#Due to high computational cost, we will reduce the size of the datasets for our convenience\nX_train7 = X_train_scaled[:1000]\ny_train7 = y_train[:1000]\nX_test7 = X_test[:1000]\ny_test7 = y_test[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_parameters = [{'kernel': ['linear', 'rbf', 'poly'], 'C':[0.1, 1], 'gamma': [0.1, 1]}]\nMSE_svm = ['mean_squared_error(y_test,y_pred7)']\n\nfor value in MSE_svm:\n    svr_regr = GridSearchCV(SVR(), tuned_parameters, cv=4)\n    svr_regr.fit(X_train7, y_train7)\n    y_true, y_pred7 = y_test7, svr_regr.predict(X_test7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_regr.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_regr.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using the hyper-parameters to run the model on the entire dataset for best results\nsvr_regr = SVR(gamma=0.1, kernel = 'linear', C =1)\nsvr_regr.fit(X_train_scaled, y_train) \n\n#Predicting with SVM\ny_pred7 = svr_regr.predict(X_test_scaled)\n\nMSE_svm = mean_squared_error(y_test,y_pred7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MSE_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,y_pred7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, y_pred7)\nplt.xlabel(\"Actual BMI\")\nplt.ylabel(\"Predicted BMI\")\nplt.title(\"SVM\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Since we set out to compare the models, let,s bring it all together\n# =============================================================================\nmse_lr = print(mean_squared_error(y_test,y_pred1))\nmse_xgb = print(mean_squared_error(y_test,y_pred2))\nmse_ada = print(mean_squared_error(y_test,y_pred3))\nmse_dt = print(mean_squared_error(y_test,y_pred4))\nmse_rf = print(mean_squared_error(y_test,y_pred5))\nmse_knn = print(mean_squared_error(y_test,y_pred6))\nmse_svm = print(mean_squared_error(y_test,y_pred7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"As can be seen from the results above, all the regressors performed well on this data set.\nHowever, Decision Tree showed the lowest MSE and almost perfect scatter plot of predicted vs actual values thereby showing the best fit.\nSimilarly, it returned R2 value of approximately 100%. On the other hand, \nAdaboost had the highest MSE and the least R2 score to emerge as the relatively worst fit model for this problem.\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}