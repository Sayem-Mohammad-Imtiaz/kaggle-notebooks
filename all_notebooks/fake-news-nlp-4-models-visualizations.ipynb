{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h3 align=\"center\"><font size=\"15\"><b>Fake News detection</b></font></h3> \n\n<img src=\"https://www.txstate.edu/cache78a0c25d34508c9d84822109499dee61/imagehandler/scaler/gato-docs.its.txstate.edu/jcr:21b3e33f-31c9-4273-aeb0-5b5886f8bcc4/fake-fact.jpg?mode=fit&width=1600\" height=200 width=400>\n\n<br></br>\n\n**Task type:** Classification\n\n**Models used:** LinearSVC, MultinomialNB, XGBoost, PyCaret, CatBoost\n\n**Tools used:** NLP preprocessing tools, semi-supervised learning technique, new feature engineering, Word Cloud"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nfake['flag'] = 0\nfake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\ntrue['flag'] = 1\ntrue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndf = true.append(fake)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. EDA + Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"**Let's check the datatypes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing the duplicates and preventing problems with indexing.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop_duplicates()\ndf = df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see that the date format is not the one we need. I will apply the appropriate date format for future purposes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correcting some data\ndf['date'] = df['date'].replace(['19-Feb-18'],'February 19, 2018')\ndf['date'] = df['date'].replace(['18-Feb-18'],'February 18, 2018')\ndf['date'] = df['date'].replace(['17-Feb-18'],'February 17, 2018')\ndf['date'] = df['date'].replace(['16-Feb-18'],'February 16, 2018')\ndf['date'] = df['date'].replace(['15-Feb-18'],'February 15, 2018')\ndf['date'] = df['date'].replace(['14-Feb-18'],'February 14, 2018')\ndf['date'] = df['date'].replace(['13-Feb-18'],'February 13, 2018')\n\n\ndf['date'] = df['date'].str.replace('Dec ', 'December ')\ndf['date'] = df['date'].str.replace('Nov ', 'November ')\ndf['date'] = df['date'].str.replace('Oct ', 'October ')\ndf['date'] = df['date'].str.replace('Sep ', 'September ')\ndf['date'] = df['date'].str.replace('Aug ', 'August ')\ndf['date'] = df['date'].str.replace('Jul ', 'July ')\ndf['date'] = df['date'].str.replace('Jun ', 'June ')\ndf['date'] = df['date'].str.replace('Apr ', 'April ')\ndf['date'] = df['date'].str.replace('Mar ', 'March ')\ndf['date'] = df['date'].str.replace('Feb ', 'February ')\ndf['date'] = df['date'].str.replace('Jan ', 'January ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] = df['date'].str.replace(' ', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, val in enumerate(df['date']):\n    df['date'].iloc[i] = pd.to_datetime(df['date'].iloc[i], format='%B%d,%Y', errors='coerce') # by setting the parameter to \"coerce\", we will set unappropriate values to NaT (null)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] = df['date'].astype('datetime64[ns]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\ndf['year'] = pd.to_datetime(df['date']).dt.to_period('Y')\ndf['month'] = pd.to_datetime(df['date']).dt.to_period('M')\n\ndf['month'] = df['month'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next we will try to elicit insights from non-text features to get to know if they will help us boost the Text Classifier.**"},{"metadata":{},"cell_type":"markdown","source":"## Fake news dynamics"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = df[['month', 'flag']]\nsub = sub.dropna()\nsub = sub.groupby(['month'])['flag'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sub.drop('NaT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.suptitle('Dynamics of fake news')\nplt.xticks(rotation=90)\nplt.ylabel('Number of fake news')\nplt.xlabel('Month-Year')\nplt.plot(sub.index, sub.values, linewidth=2, color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What a spike in the dynamics of fake news in late 2017!**"},{"metadata":{},"cell_type":"markdown","source":"## Subject distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = df[['subject', 'flag']]\nsub2 = sub2.dropna()\nsub2 = sub2.groupby(['subject'])['flag'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.suptitle('Fake news among different categories')\nplt.xticks(rotation=90)\nplt.ylabel('Number of fake news')\nplt.xlabel('Category')\n\nplt.bar(sub2.index, height=sub2.values, color='green')\n#ax1.plot(x, y)\n#ax2.plot(x, -y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we have discovered, such features as**\n* subject\n* date\n\n**might be also crucial for the algorithm to decide whether the piece of news is fake or real. We will try to include them in the model.**"},{"metadata":{},"cell_type":"markdown","source":"# 3. Text preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will add the 'subject' feature to the title field as it might have an influence on the outcome of classification.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#nlp['title'] = nlp['title'] + ' ' + nlp['subject']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Word Cloud visualization"},{"metadata":{},"cell_type":"markdown","source":"**Here I am going to take one example and try visualize tfidf as a wordcloud.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = nlp[nlp['flag'] == 1]['title'].iloc[0:500] # We will take a slice of fake news, to see what vocabulary there looks like\ntfidf1 = TfidfVectorizer()\nvecs = tfidf1.fit_transform(corpus)\n\nfeature_names = tfidf1.get_feature_names()\ndense = vecs.todense()\nlist_words = dense.tolist()\ndf_words = pd.DataFrame(list_words, columns=feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\ndf_words.T.sum(axis=1)\nCloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(df_words.T.sum(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,5))\nplt.imshow(Cloud, interpolation='bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Indeed, looks definitely like fake news :)**\n\n**And we can also see out 'subject' feature in the foreground as it has been added manually in every title. Therefore, out vectorizer considers it as an important & frequent word.**"},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Tfidf-vectorizing"},{"metadata":{},"cell_type":"markdown","source":"**First, I will tokenize words to pass it on to the SnowballStemmer method, which will take out lemmas from words.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk import word_tokenize\n\nnlp['title'] = nlp['title'].apply(lambda x: word_tokenize(str(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**An important step in every NLP-task is to get the roots of words in order not to distract the model by 'different' words.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import SnowballStemmer\n\nsnowball = SnowballStemmer(language='english')\nnlp['title'] = nlp['title'].apply(lambda x: [snowball.stem(y) for y in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp['title'] = nlp['title'].apply(lambda x: ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take the standard english bag of stopwords from nltk.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \n\nnltk.download('words')\nnltk.download('stopwords')\nstopwords = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And finally TfidfVectorizing. You can also take CountVectorizer, but I prefer Tfidf as it has masses of advantages.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\nX_text = tfidf.fit_transform(nlp['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_text, nlp['flag'], test_size=0.33, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Model building"},{"metadata":{},"cell_type":"markdown","source":"**I will use several approaches to solve the classification task, such as:**\n\n1) Traditional (which are known as efficient for text classification):\n\n    1.1) SVM\n    1.2) Naive Bayes\n    1.3) XGBoost\n    \n2) Not-very-traditional (Experimental): PyCaret NLP toolkit (I will apply unsupervised model to generate features which I will in turn pass on to the supervised model)"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Linear SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nclf = LinearSVC(max_iter=100, C=1.0)\nclf.fit(X_train, y_train)\n\ny_pred_SVM = clf.predict(X_test)\nprint(cross_val_score(clf, X_text, nlp['flag'], cv=3))\nprint(accuracy_score(y_pred_SVM, y_test))\n\nscores['LinearSVC'] = accuracy_score(y_pred_SVM, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This looks suspiciously good, but lets try another algorithm.**"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf2 = MultinomialNB()\nclf2.fit(X_train, y_train)\n\ny_pred_MNB = clf2.predict(X_test)\nprint(cross_val_score(clf2, X_text, nlp['flag'], cv=3))\nprint(accuracy_score(y_pred_MNB, y_test))\n\nscores['MultinomialNB'] = accuracy_score(y_pred_MNB, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Okay, this model performs a little worse, but still very good.**"},{"metadata":{},"cell_type":"markdown","source":"## 4.3 XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nclf3 = XGBClassifier(eval_metric='rmse', use_label_encoder=False)\nclf3.fit(X_train, y_train)\n\ny_pred_XGB = clf3.predict(X_test)\nprint(cross_val_score(clf3, X_text, nlp['flag'], cv=3))\nprint(accuracy_score(y_pred_XGB, y_test))\n\nscores['XGB'] = accuracy_score(y_pred_XGB, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 PyCaret + CatBoost"},{"metadata":{},"cell_type":"markdown","source":"**PyCaret’s Natural Language Processing module is an unsupervised machine learning module that can be used for analyzing text data by creating topic models that can find hidden semantic structures  within documents. PyCaret’s NLP module comes with a wide range of text pre-processing techniques. It has over 5 ready-to-use algorithms and several plots to analyze the performance of trained models and text corpus.**\n\n*Read more:* https://pycaret.org/nlp/"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycaret","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setting up the model which will implement all traditional NLP-preprocessing operation (tokenizing, lemmatizing etc.**\n\n**The PyCaret is almost fully automatic!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pycaret.nlp import *\n\ncaret_nlp = setup(data=nlp, target='title', session_id=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LDA stands for Latent Dirichlet Allocation and is widely used in unsupervised learning tasks.**\n\n*Read more:* https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = create_model('lda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_data = assign_model(lda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here's the outcome dataset:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We'll utilize the 'Topic' features generated by PyCaret.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_cat = lda_data.drop(['text','date','Perc_Dominant_Topic','flag','year'], axis=1)\ninput_cat['month'] = input_cat['month'].astype(str)\ntarget_cat = lda_data['flag']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(input_cat, target_cat, test_size=0.33, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf4 = CatBoostClassifier(iterations=1000, \n                          cat_features=['title','subject','Dominant_Topic','month']\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf4.fit(X_train_cat, y_train_cat, early_stopping_rounds=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores['CatBoost'] = clf4.score(X_test_cat, y_test_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(scores.keys(), scores.values())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusion\n\n**We have trained & tested 4 models for NLP task (implementing the traditional NLP preprocessing strategies). They all perform very good, however this is most likely due to the high correlation of the target other categorical features (such as 'subject'). If we did not add it to analysis, the result could have been totally different.**\n\n**We also used a combination of supervised & unsupervised learning, which can be an interesting method to use.**\n\n**Also, for text classification tasks I recommend using BERT models and DNN.**\n\n*For more information on this and code snippets, read here:* https://medium.com/engineering-zemoso/text-classification-bert-vs-dnn-b226497c9de7\n\n<font color='blue'><b>Thank you for your attention!</b><br></br><br></br>\nYour comments and discussion contributions are always welcome.</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}