{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport os\nfrom catboost import CatBoostClassifier, Pool\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport shap\nshap.initjs()\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/air-marshal-misconduct/FederalAirMarshalMisconduct.csv\",parse_dates=[\"Date Case Opened\"])\ndisplay(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### EDA\n* Analyze input data & output/`target` frequencies\n* Drop \"Final Disposition\" column (leak).\n* Keep subset of target column (due to cardinality) \n* We could also strip the whitespace from column names if handling further"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% duplicate rows\", (100*(df.shape[0] - df.drop_duplicates().shape[0])/df.shape[0]))\n## note: dropping Final Disposition results in more duplicates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Date Case Opened\"].describe(datetime_is_numeric=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in df.columns:\n    print(c)\n    print(\"# unique values/cardinality\",df[c].nunique())\n    print(df[c].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `Field Office` : a (very few) misspellings (lAX vs LAX). We'll clean by uppercasing all. \n    * This column could be attached to external data about the airports - how busy they are for example, how many international flights, specific carriers, hubs etc\n    * We will clean the short input texts and target columns by uppercasing\n* `target` = `Final Disposition` cafter cleaning (e.g. merging of Suspension 1/3/X days together, and text lowercasing). \n\n"},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n* Clean text\n* Add basic calendar features\n    * Much more can be done - adding external data, checking holidays, time-series/history of office (nvm individual in question) etc'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Field Office \"] = df[\"Field Office \"].str.upper()\ndf[\"Field Office \"] = df[\"Field Office \"].fillna(\"\") # fill missing values\nprint(\"Field Office unique values after casing\",df[\"Field Office \"].nunique())\ndf['Allegation '] = df['Allegation '].str.lower()\nprint(\"Allegation unique values after casing\",df[\"Allegation \"].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"year\"] = df[\"Date Case Opened\"].dt.year\ndf[\"month\"] = df[\"Date Case Opened\"].dt.month\ndf[\"week\"] = df[\"Date Case Opened\"].dt.isocalendar().week.astype(int) # replaces dt.week","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Some simple feature/target analysis\n\n* TODO: Join and divide by background target frequency, to see change in target freq, given feature/time. \n* apply this to other features for EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_freq_background = df[\"target\"].value_counts(normalize=True).round(4)\ntarget_freq_background","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"year\")[\"target\"].value_counts(normalize=True)\n##TODO: Join and divide by background target frequency, to see change in target freq, given feature/time. \n## apply this to other features for EDA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model & Target\n* Drop rarest target outcomes - keep those with at least 20 outcomes cases\n* Simple catboost model as a baseline + Shap for explanations\n    * Use pool and define categorical columns for catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"## code from: https://stackoverflow.com/questions/29836836/how-do-i-filter-a-pandas-dataframe-based-on-value-counts    \ndf = df.loc[df['target'].map(df['target'].value_counts()) > 20]\n\n## drop original datecol and the \"raw\" outcomes col\ndf.drop([\"Date Case Opened\",\"Final Disposition\"],axis=1,inplace=True)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% duplicate rows\", (100*(df.shape[0] - df.drop_duplicates().shape[0])/df.shape[0]))\nprint(\"# rows left if we drop duplicates:\",df.drop_duplicates().shape[0])\nprint(\"missing vals \\n\",df.isna().sum())\n\n\n# df = df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Dropping \"genuine\" duplicates isn't ideal as we lose EDA info and recurring complaints that may be easy to classify."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"target\",axis=1)\ny= df[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42) # the \"stop word\" of ML code ;)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Identufy categorical columns in features:  (We coyuld also use textCols, but then we lose interpretability with current catbosot)\n## good practice for cases with lots of cols: \nCAT_COLS = list(X.select_dtypes(include=\"O\").columns)\nCAT_COLS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostClassifier(\n#         iterations=600,\n        loss_function='MultiClass',\n        eval_metric='AUC',         \n#         task_type=\"GPU\",\n        verbose=False)\n\n# we could improve model with a validation split or CV to avoid overfitting\nmodel.fit(\n        X_train, y_train, plot=True,\n        cat_features=CAT_COLS,       \n#     cat_features=['Field Office '],text_features= ['Allegation '] # catboost doesn't yet support feature importance for text it's own text features. We can do without here\n);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\nprint(classification_report(y_true=y_test, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features importance\n* Use SHAP for now - score is importance of each feature, given all other features in model. \n* SHAP doesn't seem to include anything \"easy\" for getting the class names"},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(Pool(X, y,\n#                                          cat_features=['Field Office '],text_features= ['Allegation ']\n                                         cat_features=CAT_COLS\n                                        ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance plot overall\nshap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also get a shap plot of features importance per class (i.e a \"binary\" view):\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"list(y.unique())\n## hopefully shap picked classes in order the yappeared in data? ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## class \"7\" - retirement?\nshap.summary_plot(shap_values[6], X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## class \"3\" - resignation?\nshap.summary_plot(shap_values[3], X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We see that resignations/class 3 are much less likely to occur in later years!\n* Lets plot the total amount of resignations by year "},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df[[\"target\",\"year\"]].copy()\ndf2[\"is_target\"] = (df2[\"target\"]==\"suspension\").astype(int)\nprint(\"total suspension by year:\")\ndf2.groupby(\"year\").sum().plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"fraction of complaints handled as suspension by year:\");\ndf2.groupby(\"year\").mean().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## class \"5\" -removal ?\nshap.summary_plot(shap_values[5], X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## class \"2\" -  letter of counsel\nshap.summary_plot(shap_values[2], X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## class \"0\" - verbal counsel\nshap.summary_plot(shap_values[0], X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## class \"1\" - suspension ?\nshap.summary_plot(shap_values[1], X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## class \"6\" - no further action?\nshap.summary_plot(shap_values[6], X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}