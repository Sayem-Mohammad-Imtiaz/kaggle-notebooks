{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action='ignore')\nplt.style.use(['seaborn-bright','dark_background'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing required dataset of heart failure record downloaded from the Kaggle dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here in our dataset DEATH EVENT is our dependent feature which consists of binary values so it will be classification problem and we will consider it as our target variable. Also, other  features are our independent variable.\n\n### Cheacking null values if any present in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cheacking data type of all features present in our data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding  number of unique values present in each feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In our dataset features like anaemia,diabetes,high_blood_pressure,sex,smoking consists of bianary values just because they consists only two unique values."},{"metadata":{},"cell_type":"markdown","source":"#### Visualization of correlation between independent features and target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = data.corr()\ncorrelation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For better visualization ploting a heatmap with the seaborn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc = {'figure.figsize':(8,8)})\nsns.heatmap(correlation,cmap=\"PiYG\")\nplt.title(\"Heatmap\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It seems like the feature age and serum creatinine has little bit positive correlation and ejection fraction also has little negetive correlation on death event, on other hand time has better negetive correlation with target.But others features dosen't impact that much."},{"metadata":{},"cell_type":"markdown","source":"#### Dividing out data into two parts as independent variables and target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns=['DEATH_EVENT'])\nY = data['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaling the X dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ncolumn = X.columns\nscaled_X = scale.fit_transform(X)\nscaled_X = pd.DataFrame(scaled_X,columns=column)\nscaled_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spliting data as train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(scaled_X,Y,test_size=0.2,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Importing the different classification models for compairing score with each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble  import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append((\"LogisticRegression\",LogisticRegression()))\nmodels.append((\"DescisionTree\",DecisionTreeClassifier()))\nmodels.append((\"RandomForest\",RandomForestClassifier()))\nmodels.append((\"SupportVector\",SVC()))\nmodels.append((\"KNeighbors\",KNeighborsClassifier()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name,model in models:\n    model.fit(X_train,Y_train)\n    train_score = model.score(X_train,Y_train)\n    test_score = model.score(X_test,Y_test)\n    print(name,\"train score =\",train_score)\n    print(name,\"test score =\",test_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It looks like Random Forest leads score comparision on our dataset against logistic regression,descisiontree,svm and KNN."},{"metadata":{},"cell_type":"markdown","source":"#### Cheacking score of our Random Forest model on our test data set for different max_depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,20):\n    model = RandomForestClassifier(max_depth=i)\n    model.fit(X_train,Y_train)\n    score = model.score(X_test,Y_test)\n    print(\"for max depth \",i,\"score =\",score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For max depth 3 it gives maximum score"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(max_depth=3)\nmodel.fit(X_train,Y_train)\nprediction = model.predict(X_test)\nprobablities = model.predict_proba(X_test)\nmodel.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We have created a model using RandomForestClassifier with accuracy score 0.934 i.e 93.4%"},{"metadata":{},"cell_type":"markdown","source":"#### Creating confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(Y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(Y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting the Precision-Recall Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nprecision_points, recall_points, threshold_points = precision_recall_curve(Y_test,probablities[:,1])\nprecision_points.shape, recall_points.shape, threshold_points.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(['seaborn-dark','dark_background'])\nplt.figure(dpi =100, figsize=(6,6))\nplt.plot(threshold_points, precision_points[:-1], color = 'r', label = 'Precision')\nplt.plot(threshold_points, recall_points[:-1], color = 'b', label = 'Recall')\nplt.xlabel('Threshold')\nplt.ylabel('Frequency')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting AUC-ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve,roc_auc_score\nfpr, tpr, threshold = roc_curve(Y_test ,probablities[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(['seaborn-dark','dark_background'])\nplt.figure(dpi = 100, figsize=(8,6))\nplt.plot(fpr,tpr, color = 'r', label='FPR-TPR')\nplt.plot([0,1],[0,1], color = 'g', label = 'Baseline')\nplt.title('AUC-ROC Curve')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}