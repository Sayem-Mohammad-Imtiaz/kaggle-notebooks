{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Diamond Price Predictions\nThis notebook will explore the diamonds dataset and build a model for price predictions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import files and initialise input_path\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plot\nimport seaborn as sns #plot\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        input_path = os.path.join(dirname, filename)\n        print(input_path)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read file\ndiamonds = pd.read_csv(input_path)\ndiamonds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Unnamed\ndiamonds.drop(\n    diamonds.filter(regex='Unnamed'),\n    axis = 1,\n    inplace = True)\ndiamonds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Examine distribution of target variable\nsns.displot(diamonds['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A 5-number summary of the target variable\nprint('Min: ', diamonds['price'].min())\nprint('Q1: ', np.percentile(diamonds['price'],25))\nprint('Median: ', np.percentile(diamonds['price'],50))\nprint('Q3: ', np.percentile(diamonds['price'],75))\nprint('Max: ', diamonds['price'].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot is an easy way to see the correlation between numerical variables\nsns.pairplot(diamonds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the scatter plots, we can see that there is an exponential relationship between the variables and the price. However, for depth and table it seems that there is an optimum range, and the diamonds with non-optimum depth and table get lower price.\n\n## Encoding Categorical Variable\nWe need to encode the categorical variable into numerical before feeding it into the predictive algorithms, but we must take note that these are actually ordinal values. Therefore, we cannot directly use the label encoder. Label encoder will assign a label based on the alphabetical order, which might not be the same with the rank of the values.\n\nOnly for color, in which D is the best color and J is the worst, we can use label encoder. The ranking is in line with the alphabetical order"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use label encoder for color\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(diamonds['color'])\ndiamonds['color'] = le.transform(diamonds['color'])\nprint('color: ', set(diamonds['color']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define our own encoding using a dictionary\nprint(set(diamonds['cut']))\nprint(set(diamonds['clarity']))\n\n\nreplace = {\n    'cut':{\n        'Fair': 0,\n        'Good': 1,\n        'Very Good': 2,\n        'Ideal': 3,\n        'Premium': 4},\n    'clarity':{\n        'I1':0,\n        'SI2':1,\n        'SI1':2,\n        'VS2':3,\n        'VS1':4,\n        'VVS2':5,\n        'VVS1':6,\n        'IF':7}\n}\n\ndiamonds.replace(replace,inplace=True)\nprint('cut: ', set(diamonds['cut']))\nprint('clarity: ',set(diamonds['clarity']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We an also use heatmap to see the correlation between variables\nplt.figure(figsize=(20,15))\nax=plt.subplot(111)\nsns.heatmap(diamonds.corr(),cmap=\"coolwarm\",center=0,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression Model\n## Random Forest\nTo prevent overfitting, first we need to split the data into training and test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Test Split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(diamonds.drop(['price'],axis=1),\n                                                    diamonds['price'],\n                                                    test_size=0.3,\n                                                    random_state=42)\n\nprint(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Model\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict Test Dataset\npredictions = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Examine results of prediction vs the actual price. A perfect linear line \nplt.scatter(predictions,y_test)\nplt.title('Prediction vs Actuals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print model score (R^2)\nprint(f'Training score: {model.score(x_train,y_train):.5f}')\nprint(f'Test score: {model.score(x_test,y_test):.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print error metrics \nfrom sklearn.metrics import * \n\nprint('MAE\\t\\tMSE\\t\\tRMSE')\nprint(f'{mean_absolute_error(y_test,predictions):.2f}\\t\\t',\n      f'{mean_squared_error(y_test,predictions):.2f}\\t',\n      f'{np.sqrt(mean_squared_error(y_test,predictions)):.2f}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get feature importance to see which variable is an important predictor to the diamond prices\nfeature_importance = pd.DataFrame({\n    'Column':x_train.columns,\n    'Importance':model.feature_importances_})\nfeature_importance.sort_values('Importance',inplace=True)\nplt.barh(feature_importance['Column'],feature_importance['Importance'])\n\nplt.title('Diamond Price Prediction Feature Importances')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"This model is able to predict with a 98.13% R^2 on the test dataset. Features that are found to be important in predicting the price in unsprisingly the carat, followed by y, clarity, and color. This is an interesting observation because traditionally we would value color dan clarity more, but apparently y plays an important value."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}