{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.model_selection import RandomizedSearchCV\nimport category_encoders as ce\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,PowerTransformer,StandardScaler\nfrom scipy.stats import chi2_contingency\n%matplotlib inline\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading training and testing files\ntrain_data = pd.read_csv('../input/churn-risk-rate-hackerearth-ml/train.csv')\ntest_data  = pd.read_csv('../input/churn-risk-rate-hackerearth-ml/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying total recorfds in training and testing data\nprint(\"There are {} number of rows and {} number of columns in training data\".format(train_data.shape[0],train_data.shape[1]))\nprint(\"There are {} number of rows and {} number of columns in testing data\".format(test_data.shape[0],test_data.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describing training data\ntrain_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"code","source":"def check_null(train,test):\n  ''' Checking null values in dataset using heatmap''' \n  plt.figure(figsize=(16,9))\n  plt.subplot(1,2,1)\n  train_visual = sns.heatmap(train.isnull(),yticklabels=False,cmap='viridis')\n  plt.subplot(1,2,2)\n  test_visual  = sns.heatmap(test.isnull(),yticklabels=False,cmap='viridis')\n  plt.show()\n\ncheck_null(train_data,test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From the above heatmap, it appears that there are three columns with null values i.e. 'region_category', 'preferred_offer_types', 'points_in_wallet'. Also, it appears that 'region_category' column have higher null values in both dataset while 'preferred_offer_types' column has the least.**","metadata":{}},{"cell_type":"code","source":"def visualize_null_relationship(train):\n    '''visualize the relationship of null values with target variables'''\n    features_with_nan = [features for features in train.columns if train[features].isnull().sum()>=1]\n    for feature in features_with_nan:\n      data = train.copy()\n      data[feature] = np.where(data[feature].isnull(), 1,0)\n      data.groupby(feature)['churn_risk_score'].median().plot.bar()\n      plt.title(feature)\n      plt.show()\n\nvisualize_null_relationship(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysing numerical variables\nnumerical_features = [feature for feature in train_data.columns if train_data[feature].dtypes!='O']\nprint(\"The number of numerical features in training data is {}.\".format(len(numerical_features)))\ntrain_data[numerical_features].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discrete Features\ndiscrete_features = [features for features in numerical_features if len(train_data[features].unique())<=25 and features not in ['churn_risk_score']]\nprint(\"The number of discrete features are {} \".format(len(discrete_features)))\ntrain_data[discrete_features].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between cont. and target features\nfor feature in numerical_features:\n  data = train_data.copy()\n  data[feature].hist(bins=45)\n  plt.xlabel(feature)\n  plt.ylabel('Churn_Risk')\n  plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From visualization, it appears that some of the numerical features are skewed. Also, the target variable contains outliers.**","metadata":{}},{"cell_type":"code","source":"#Detecting Outliers with boxplot\nfor feature in numerical_features:\n  data_copy = train_data.copy()\n  data_copy.boxplot(column=feature)\n  plt.title(feature)\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Further, visualizing outliers with scatter plot\nfor feature in numerical_features:\n    data_copy = train_data.copy()\n    plt.scatter(data_copy[feature],data_copy['churn_risk_score'])\n    plt.xlabel(feature)\n    plt.ylabel('Churn_Risk')\n    plt.title(feature)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From the scatter plot and boxplot it appears that most of the columns contains outliers even the target variable with churn rate= -1.**","metadata":{}},{"cell_type":"code","source":"# Computing the correlation b/w the features\nplt.figure(figsize=(10,9))\nsns.heatmap(train_data[numerical_features].corr(),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It appears that no columns have a relationship.**","metadata":{}},{"cell_type":"code","source":"# Analysing Categorical Features \ncategorical_features = [feature for feature in train_data.columns if feature not in numerical_features]\nprint(\"Total number of categorical features are {}\".format(len(categorical_features)))\ntrain_data[categorical_features].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determining the cadinality of features\nfor features in categorical_features:\n  print(\"The name of the features is {} and its cardinality is {} \".format(features,len(train_data[features].unique())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are most of the columns with high cardinality.**","metadata":{}},{"cell_type":"code","source":"# Visualizing relationship of categorical variables with target\nfor feature in categorical_features:\n    data_copy = train_data.copy()\n    print(feature,chi2_contingency(pd.crosstab(data_copy[feature],train_data['churn_risk_score'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From statistics, features: region_category,membership_category,joined_through_reference, preferred_off_types,medium_of_operation, avg_frequency_login_days,offer_application_preference and feedback are related to target.**","metadata":{}},{"cell_type":"code","source":"# Plotting a countplot for columns having cardinality less than 10\ncard_less_than_10 = [feature for feature in categorical_features if len(train_data[feature].unique())<=10]\nplt.figure(figsize=(30,9))\nfor i in range(0,len(card_less_than_10)):\n    plt.subplot(4,3,i+1)\n    sns.countplot(y=train_data[card_less_than_10[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, following observations can be deduced:\nGender: Both number of Males and Females are equal.\n\n    There is an 'unknown' category in the column\nJoined through referral : Equally people joined/not joined.\n\n    ? represents about status of 4500 unknown people.\nInternet Option : Contains equal no. of subscribers.\n\nPast Complaints: Contains equal no. of people.\n\nRegion_category :\n\n              Shows about 14000 people living in Town.\n              Shows about 4500 people living in Villages.\nPreferred offer types: Mopstly Contains equal no. of people distribution.\n\nUsed Special Discout :\n\n         Shows 20000+ people applied for discount\n         Shows about 16500 people didn't applied for \nComplaint_status:\n\n             Shows most complaints are not applicable\nMembership category :\n\nShows max. count of people with no and basic membership.\nShows least count for people with premium/platinum membership\nMode of Operation:\n\nShows most people use Desktop and smartfone for operation\nShows ? about 5800 unknown no of people.\nPreferes offers: Shows max. count for people preferes offers.\n\nFeedback:\n\n      Most of the people had given negative comments ","metadata":{}},{"cell_type":"code","source":"train_data['churn_risk_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Converting records having churn rate == -1 to 1 .\ntraining_data = train_data.copy()\ntraining_data['churn_risk_score'] = training_data['churn_risk_score'].apply(lambda x:1 if x==-1 else 0 if x==5 else x)\ntraining_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data['churn_risk_score'].value_counts().plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling Misssing values in Numerical Variables\ndef missing_numerical(train,test,feature):\n  median_value_train = train[feature].median()\n  median_value_test  = test[feature].median()\n  train[feature+'_nan'] = np.where(train[feature].isnull(),1,0)\n  train[feature] = np.where(train[feature].isnull(),median_value_train,train[feature])\n  test[feature+'_nan'] = np.where(test[feature].isnull(),1,0)\n  test[feature]  = np.where(test[feature].isnull(),median_value_test,test[feature])\n  return train,test\n\ntraining_data,test_data = missing_numerical(training_data,test_data,'points_in_wallet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping Id Column as it doesn't contributes in prediction\ntraining_data = training_data.drop(['customer_id'],axis=1)\ntesting_data = test_data.copy()\ntesting_data = testing_data.drop(['customer_id'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling Categorical Variables in dataset\ncategorical_var = ['region_category', 'preferred_offer_types','joined_through_referral','medium_of_operation']\n\ndef calc_mode(data,feature):\n  return data[feature].mode()\n\ndef categorical_null(train,test,features):\n  for feature in features[0:2]:\n    train[feature+\"_nan\"] = np.where(train[feature].isnull(),1,0) \n    train[feature] = np.where(train[feature].isnull(),calc_mode(train,feature),train[feature])\n    test[feature+\"_nan\"] = np.where(test[feature].isnull(),1,0)  \n    test[feature] = np.where(test[feature].isnull(),calc_mode(test,feature),test[feature]) \n\n\n  for feature in features[3:]:\n    train[feature+\"_missing\"] = np.where(train[feature]==\"?\",1,0)  \n    train[feature] = np.where(train[feature]=='?',calc_mode(train,feature),train[feature])\n    test[feature+\"_missing\"] = np.where(test[feature]==\"?\",1,0) \n    test[feature] = np.where(test[feature]=='?',calc_mode(test,feature),test[feature]) \n\n  return train,test\n\ntraining_data,testing_data = categorical_null(training_data,testing_data,categorical_var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.isnull().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_data.isnull().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performing Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Performing Feature Engineering on Numerical Variables [age\tdays_since_last_login\tavg_time_spent\tavg_transaction_value\tpoints_in_wallet]\nnumerical_features = [feature for feature in training_data.columns if training_data[feature].dtypes!='O' and feature not in ['churn_risk_score']]\nfeatures = [\"age\",\"days_since_last_login\",\"avg_time_spent\"]\ndef encode_neg_val(data,feature):\n  '''Handle negative values'''\n  data[feature] = np.where(data[feature]<0,0,data[feature])\n  return data[feature]\n\ndef dsl_eng(train,test,feature):\n    ''' Feature Engineering Days Since Last Login '''\n    train[feature] = encode_neg_val(train,feature)\n    test[feature] = encode_neg_val(test,feature)\n    return train,test\n\ndef ats_eng(train,test,feature):\n    ''' Feature Engineering Avg. time spent '''\n    train[feature] = encode_neg_val(train,feature)\n    test[feature] =  encode_neg_val(test,feature)\n    return train,test\n\ndef feature_eng_numerical(train,test,features):\n\n  ''' Feature Engineering Numerical Columns'''\n  train,test = dsl_eng(train,test,features[1])  #Days Since Last Login\n  train,test = ats_eng(train,test,features[2])  # avg_time_spent\n  return train,test\n\ntraining_data,testing_data = feature_eng_numerical(training_data,testing_data,features)\n  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performing Feature Engineering on Categorical Variables \ncategorical_feat = ['joining_date','avg_frequency_login_days',\"referral_id\"]\n\n\ndef jd_eng(train,test,feature):\n  ''' Feature Eng. Joining date'''\n  present_yr = 2021\n  train[feature] = train[feature].str.split(\"-\",expand=True)[0].astype(int)\n  train[feature] = present_yr-train[feature]\n  test[feature]  = test[feature].str.split(\"-\",expand=True)[0].astype(int)\n  test[feature]  = present_yr-test[feature]\n  return train,test\n\ndef calc_login_act(data,feature):\n \n  data[feature] = np.where(data[feature].str.contains('Error'),\n                                             0.0,data[feature])\n  return data[feature]\n\ndef alg_eng(train,test,feature):\n  ''' Feature Eng. Avg Login Days'''\n  train[feature] = calc_login_act(train,feature)\n  test[feature] =  calc_login_act(test,feature)\n  return train,test\n\ndef rid_eng(train,test,feature):\n  ''' Feature Eng. Referral Id'''\n  encoder = ce.CountEncoder()\n  train[feature] = encoder.fit_transform(train[feature])\n  test[feature] = encoder.transform(test[feature])\n  train[feature] = train[feature].apply(lambda x:\"Not_Referred\" if x==1 else \"Referred\" if x<20 else \"Unknown\")\n  test[feature] = test[feature].apply(lambda x:\"Not_Referred\" if x==1 else \"Referred\" if x<20 else \"Unknown\")\n  return train,test\n\ndef categorical_eng(train,test,features):\n  ''' Feature Engineering Categorical Variables'''\n\n  train,test = jd_eng(train,test,features[0]) # joining_date\n  train,test = alg_eng(train,test,features[1]) #avg_login_days\n  train,test = rid_eng(train,test,features[2]) #referral_id\n  return train,test\n\ntraining_data,testing_data = categorical_eng(training_data,testing_data,categorical_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing columns with high cardinality and less contribution\ncols_to_remove = ['Name','security_no','last_visit_time']\ntraining_data = training_data.drop(columns=cols_to_remove,axis=1)\ntesting_data = testing_data.drop(columns=cols_to_remove,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seperating independent and dependent features\ny = training_data['churn_risk_score']\nX_train = training_data.drop(columns=['churn_risk_score'],axis=1)\nX_test = testing_data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding Categorical Variables\n\ncols_to_encode = ['membership_category',\n              'gender','region_category','joined_through_referral',\n               'preferred_offer_types','medium_of_operation','internet_option',\n               'used_special_discount','offer_application_preference','past_complaint',\n               'complaint_status','feedback',\"referral_id\"]\n               \n\nX_train_le = X_train.copy()\nX_test_le  = X_test.copy()\n\ndef label_encode(data,features):\n  dummies = data.copy()\n  dummies = pd.get_dummies(dummies[features])\n  data = pd.concat([data,dummies],axis=1)\n  data = data.drop(columns=features,axis=1)\n\n  return data\n\nX_train_le = label_encode(X_train_le,cols_to_encode)\nX_test_le  = label_encode(X_test_le,cols_to_encode)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_le.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_le.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"code","source":"# First transforming numerical variables to gaussian curve using power transformer\npt = PowerTransformer(method='yeo-johnson',standardize=False)\ncols_to_pt = ['age','points_in_wallet','avg_time_spent','avg_transaction_value']\nX_train_pt = X_train_le.copy()\nX_test_pt  = X_test_le.copy()\nX_train_pt = pd.DataFrame(pt.fit_transform(X_train_pt[cols_to_pt]),columns=cols_to_pt)\nX_train_transformed = X_train_le.drop(columns=cols_to_pt)\nX_train_ptransformed = pd.concat([X_train_pt,X_train_transformed],axis=1)\nX_test_pt  = pd.DataFrame(pt.transform(X_test_pt[cols_to_pt]),columns=cols_to_pt)\nX_test_transformed = X_test_le.drop(columns=cols_to_pt)\nX_test_ptransformed = pd.concat([X_test_transformed,X_test_pt],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing using Standard Scaler\nsc = StandardScaler()\nX_train_scaled = pd.DataFrame(sc.fit_transform(X_train_ptransformed),columns= X_train_ptransformed.columns)\nX_test_scaled = pd.DataFrame(sc.fit_transform(X_test_ptransformed),columns = X_test_ptransformed.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_scaled.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_scaled.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(X_train_scaled,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances=pd.Series(model.feature_importances_,index=X_train_scaled.columns)\nfeat_importances.nlargest(66).plot(kind='barh')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will keep all the features in our training and testing data since all the features are contributing in prediction.**","metadata":{}},{"cell_type":"code","source":"# Model Splitting\nx_train,x_test,y_train,y_test = train_test_split(X_train_scaled,y,test_size=0.1,random_state=1,stratify=y)\nx_train,x_valid,y_train,y_valid = train_test_split(x_train,y_train,test_size=0.1,random_state=1,stratify = y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"markdown","source":"**1. Logistic Regression.**","metadata":{}},{"cell_type":"code","source":"model_1 = LogisticRegression(max_iter=400)\nmodel_1.fit(x_train,y_train.values.ravel())\npredictions_1 = model_1.predict(x_valid)\nscore_1 = f1_score(y_valid,predictions_1,average='macro')\nscore_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_1 = model_1.predict(x_test)\nscore_1 = f1_score(y_test,predictions_1,average='macro')\nscore_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. KNN**","metadata":{}},{"cell_type":"code","source":"model_2 = KNeighborsClassifier()\nmodel_2.fit(x_train,y_train)\npredictions_2 = model_2.predict(x_valid)\nscore_2 = f1_score(y_valid,predictions_2,average='macro')\nscore_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_2 = model_2.predict(x_test)\nscore_2 = f1_score(y_test,predictions_2,average='macro')\nscore_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. SVM**","metadata":{}},{"cell_type":"code","source":"model_3 = SVC()\nmodel_3.fit(x_train,y_train.values.ravel())\npredictions_3 = model_3.predict(x_valid)\nscore_3 = f1_score(y_valid,predictions_3,average='macro')\nscore_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_3 = model_3.predict(x_test)\nscore_3 = f1_score(y_test,predictions_3,average='macro')\nscore_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Decision Tree**","metadata":{}},{"cell_type":"code","source":"model_4 = DecisionTreeClassifier()\nmodel_4.fit(x_train,y_train.values.ravel())\npredictions_4 = model_4.predict(x_valid)\nscore_4 = f1_score(y_valid,predictions_4,average='macro')\nscore_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_4 = model_4.predict(x_test)\nscore_4 = f1_score(y_test,predictions_4,average='macro')\nscore_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. Random Forest**","metadata":{}},{"cell_type":"code","source":"model_5 = RandomForestClassifier()\nmodel_5.fit(x_train,y_train.values.ravel())\npredictions_5 = model_5.predict(x_valid)\nscore_5 = f1_score(y_valid,predictions_5,average='macro')\nscore_5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_5 = model_5.predict(x_test)\nscore_5 = f1_score(y_test,predictions_5,average='macro')\nscore_5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. XGBOOST**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel_6 = XGBClassifier()\nmodel_6.fit(x_train,y_train.values.ravel())\npredictions_6 = model_6.predict(x_valid)\nscore_6 = f1_score(y_valid,predictions_6,average='macro')\nscore_6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_6 = model_6.predict(x_test)\nscore_6 = f1_score(y_test,predictions_6,average='macro')\nscore_6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7. CatBoost Classifier","metadata":{}},{"cell_type":"code","source":"import catboost as cb\ncat_model = cb.CatBoostClassifier(verbose=2,iterations=90,depth=3,learning_rate=0.2,bagging_temperature=0.8,border_count=236,l2_leaf_reg=2)\ncat_model.fit(x_train,y_train,eval_set=(x_valid,y_valid))\nprint(cat_model.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_8 = cat_model.predict(x_test)\nscore_8 = f1_score(y_test,predictions_8,average='macro')\nscore_8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nfrom sklearn.metrics import classification_report\ncr = classification_report(y_test,predictions_8)\nprint(cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since,Catbbost Algorithm gives the higest f1-score, therefore we choose it as the final model for the prediction.**","metadata":{}},{"cell_type":"markdown","source":"# Prediction on Test Data","metadata":{}},{"cell_type":"code","source":"final_predictions = pd.DataFrame(cat_model.predict(X_test_scaled))\nfinal_predictions.columns = ['churn_risk_score']\nfinal_predictions = pd.concat([test_data[\"customer_id\"],final_predictions],axis=1)\nfinal_predictions.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions['churn_risk_score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions['churn_risk_score'] = final_predictions['churn_risk_score'].apply(lambda x:5 if x==0 else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}