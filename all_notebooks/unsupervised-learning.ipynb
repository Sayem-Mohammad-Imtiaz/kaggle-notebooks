{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import Imputer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve,roc_auc_score\n\nfrom sklearn import svm\n\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/vehicle2/vehicle-2.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that many columns has null values in it. So we need to deal with them.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DATA PREPROCESSING"},{"metadata":{},"cell_type":"markdown","source":"#### Univariant Analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing null values in all columns with mean/median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['circularity'].isnull().sum()\n\ndata['circularity'].fillna(data['circularity'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['distance_circularity'].isnull().sum()\n\ndata['distance_circularity'].fillna(data['distance_circularity'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['radius_ratio'].isnull().sum()\n\ndata['radius_ratio'].fillna(data['radius_ratio'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['pr.axis_aspect_ratio'].isnull().sum()\n\ndata['pr.axis_aspect_ratio'].fillna(data['pr.axis_aspect_ratio'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['scatter_ratio'].isnull().sum()\n\ndata['scatter_ratio'].fillna(data['scatter_ratio'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['elongatedness'].isnull().sum()\n\ndata['elongatedness'].fillna(data['elongatedness'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['pr.axis_rectangularity'].isnull().sum()\n\ndata['pr.axis_rectangularity'].fillna(data['pr.axis_rectangularity'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['scaled_variance'].isnull().sum()\n\ndata['scaled_variance'].fillna(data['scaled_variance'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['scaled_variance.1'].isnull().sum()\n\ndata['scaled_variance.1'].fillna(data['scaled_variance.1'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['scaled_radius_of_gyration'].isnull().sum()\n\ndata['scaled_radius_of_gyration'].fillna(data['scaled_radius_of_gyration'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['scaled_radius_of_gyration.1'].isnull().sum()\n\ndata['scaled_radius_of_gyration.1'].fillna(data['scaled_radius_of_gyration.1'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['skewness_about'].isnull().sum()\n\ndata['skewness_about'].fillna(data['skewness_about'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['skewness_about.1'].isnull().sum()\n\ndata['skewness_about.1'].fillna(data['skewness_about.1'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['skewness_about.2'].isnull().sum()\n\ndata['skewness_about.2'].fillna(data['skewness_about.2'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,40))\nplt.subplot(6,3,1)\nsns.boxplot(data['compactness'])\nplt.subplot(6,3,2)\nsns.boxplot(data['circularity'])\nplt.subplot(6,3,3)\nsns.boxplot(data['distance_circularity'])\nplt.subplot(6,3,4)\nsns.boxplot(data['pr.axis_aspect_ratio'])\nplt.subplot(6,3,5)\nsns.boxplot(data['max.length_aspect_ratio'])\nplt.subplot(6,3,6)\nsns.boxplot(data['scatter_ratio'])\nplt.subplot(6,3,7)\nsns.boxplot(data['elongatedness'])\nplt.subplot(6,3,8)\nsns.boxplot(data['pr.axis_rectangularity'])\nplt.subplot(6,3,9)\nsns.boxplot(data['max.length_rectangularity'])\nplt.subplot(6,3,10)\nsns.boxplot(data['scaled_variance'])\nplt.subplot(6,3,11)\nsns.boxplot(data['scaled_radius_of_gyration'])\nplt.subplot(6,3,12)\nsns.boxplot(data['scaled_variance.1'])\nplt.subplot(6,3,13)\nsns.boxplot(data['scaled_radius_of_gyration.1'])\nplt.subplot(6,3,14)\nsns.boxplot(data['skewness_about'])\nplt.subplot(6,3,15)\nsns.boxplot(data['skewness_about.1'])\nplt.subplot(6,3,16)\nsns.boxplot(data['skewness_about.2'])\nplt.subplot(6,3,17)\nsns.boxplot(data['hollows_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.boxplot(figsize=(35,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that the columns pr.axis_aspect_ratio,max.length_aspect_ratio,scaled_radius_of_gyration.1,skewness_about,radius_ratio are largely affected by the outliers.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data['pr.axis_aspect_ratio'],data['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that from 76 it follows same pattern,\n\ndata['pr.axis_aspect_ratio']=np.where(data['pr.axis_aspect_ratio']>76,76,data['pr.axis_aspect_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data['max.length_aspect_ratio'],data['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that from 19 it follows same pattern,\n\ndata['max.length_aspect_ratio']=np.where(data['max.length_aspect_ratio']>19,19,data['max.length_aspect_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data['scaled_radius_of_gyration.1'],data['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that from 89 it follows same pattern,\n\ndata['scaled_radius_of_gyration.1']=np.where(data['scaled_radius_of_gyration.1']>89,89,data['scaled_radius_of_gyration.1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data['skewness_about'],data['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that from 17 it follows same pattern,\n\ndata['skewness_about']=np.where(data['skewness_about']>17,17,data['skewness_about'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data['radius_ratio'],data['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that from 235 it follows same pattern,\n\ndata['radius_ratio']=np.where(data['radius_ratio']>235,235,data['radius_ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we have done some tings to overcome the outliers, Lets see the box plot now,\n\ndata.boxplot(figsize=(40,15))\n\n\n#The outliers has been handled to some extend.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data,diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=data.corr()\nplt.figure(figsize=(20,10))\nplt.subplot(1,1,1)\nsns.heatmap(corr,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Inference from above Bivariant analysis:\n    compactness has negative correlation with elongatedness\n    circularity has negative correlation with elongatedness\n    distance_circularity has negative correlation with elongatedness\n    radius_ratio has negative correlation with elongatedness\n    pr.axis_regularity has negative correlation with elongatedness\n    max.length_regularity has negative correlation with elongatedness\n    scaled_variance has negative correlation with elongatedness\n    scaled_variance.1 has negative correlation with elongatedness\n    scales_radius_of_gyration has negative correlation with elongatedness\n    \n    scales_radius_of_gyration.1 has has negative correlation with skewness_about.2\n    scales_radius_of_gyration.1 has has negative correlation with hollow_ratio\n    \n    compactness has positive correlation with scatter_ratio\n    compactness has positive correlation with scaled_variance.1\n    \n    circularity has positive correlation with max.length_regularity\n    circularity has positive correlation with scales_radius_of_gyration\n    circularity has positive correlation with pr.axis_regularity"},{"metadata":{},"cell_type":"raw","source":"We see that the columns, \n    scatter_ratio\n    elongatedness\n    pr.axis_regularity has strong correlation with other columns.\n    \nSo we can remove them from the Dataframe, But it also dont have correlation with some other columns. So lets not remove it annd try using SVM with all columns."},{"metadata":{},"cell_type":"markdown","source":"## SVM Modelling - Without PCM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting of Independent and Dependent variables\n\ny=data['class']\nX=data.drop(columns='class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardization of Data\n\ndef standardization(X_train,X_test):\n    scaler=preprocessing.StandardScaler()\n    X_train=scaler.fit_transform(X_train)\n    X_test=scaler.transform(X_test)\n    return X_train,X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM\n\ndef svm_fun(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)\n    X_train,X_test=standardization(X_train,X_test)\n    \n    clf = svm.SVC(gamma=0.025,C=3)\n    #when C increases Marigin shrinks\n    # gamma is a measure of influence of a data point. It is inverse of distance of influence. C is complexity of the model\n    # lower C value creates simple hyper surface while higher C creates complex surface\n\n    clf.fit(X_train,y_train)\n    svm_pred=clf.predict(X_test)\n    svm_score=clf.score(X_test,y_test)\n    print(\"The KNN model prediction is \" + str(svm_score*100) + \"%\")\n    \n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(y_test,svm_pred))\n    print(\"the Classification report is\")\n    print(metrics.classification_report(y_test,svm_pred))\n    #roc=roc_auc_score(y_test, svm_pred)\n    #print(\"ROC value for svm model is \"+ str(roc*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVM\n\nsvm_fun(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM Modelling - With PCM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting of Independent and Dependent variables\n\ny_pcm=data['class']\nX_pcm=data.drop(columns='class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We transform (centralize) the entire X (independent variable data) to zscores through transformation. We will create the PCA dimensions\n# on this distribution. \n\n#Covariance is done only on Independent variables\nsc = preprocessing.StandardScaler()\nX_std =  sc.fit_transform(X_pcm)          \ncov_matrix = np.cov(X_std.T)\nprint('Covariance Matrix \\n%s', cov_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The dimensions are rotated\n\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eigenvectors)\nprint('\\n Eigen Values \\n%s', eigenvalues)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 3 (continued): Sort eigenvalues in descending order\n\n# Make a set of (eigenvalue, eigenvector) pairs\neig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n\n# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\neig_pairs.sort()\n\n#Desc sort\neig_pairs.reverse()\nprint(eig_pairs)\n\n# Extract the descending ordered eigenvalues and eigenvectors\neigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\neigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n\n# Let's confirm our sorting worked, print out eigenvalues\nprint('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tot = sum(eigenvalues)\nvar_explained = [(i / tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n# eigen vector... there will be 8 entries as there are 8 eigen vectors)\ncum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 8 entries with 8 th entry \n# cumulative reaching almost 100%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1,19 depends on covariance matrix - count\n\nplt.figure(figsize=(20,10))\nplt.bar(range(1,19), var_explained, alpha=0.5, align='center', label='individual explained variance')\nplt.step(range(1,19),cum_var_exp, where= 'mid', label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# P_reduce represents reduced mathematical space....\n\n#From the above graph we see that 10 to 16 would be good.So lets try with that\n#After trying with the above range we see that 13 would be perfect,\n\nP_reduce = np.array(eigvectors_sorted[0:13])   # Reducing from 18 to 13 dimension space\n\nX_std_13D = np.dot(X_std,P_reduce.T)   # projecting original data into principal component dimensions\n\nProj_data_df = pd.DataFrame(X_std_13D)  # converting array to dataframe for pairplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us check it visually\n\nsns.pairplot(Proj_data_df, diag_kind='kde') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling SVM function using PCM data\n\nsvm_fun(Proj_data_df, y_pcm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inferences:"},{"metadata":{},"cell_type":"raw","source":"On comparing the modelling with and without PCM, \n    we see that the model performance and the class level performance also increased using PCM.\n    So lets try with Hyper parameters.\n"},{"metadata":{},"cell_type":"markdown","source":"### Hyper Parameter tuning and Randomized Search "},{"metadata":{"trusted":true},"cell_type":"code","source":"  #RandomizedSearchCV - SVM\n#Implement Hyperparameter\n\ndef hyper_params_svm(X,y):\n#gamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set\n    gammas = [0.1, 1, 10, 100]\n#kernel parameters selects the type of hyperplane used to separate the data. Using ‘linear’ will use a linear hyperplane \n        #(a line in the case of 2D data). ‘rbf’ and ‘poly’ uses a non linear hyper-plane\n    kernels  = ['linear', 'rbf', 'poly']\n#C is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n    cs = [0.1, 1, 10, 100, 1000]\n#degree is a parameter used when kernel is set to ‘poly’. It’s basically the degree of the polynomial used to find the hyperplane to split the data.\n    degrees = [0, 1, 2, 3, 4, 5, 6]\n\n# Create the random grid\n    random_grid = {'gamma': gammas,\n                   'kernel': kernels,\n                   'C': cs,\n                   'degree': degrees}\n\n    pprint(random_grid)\n    return random_grid\n\ndef randomizedsearch_svm(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)\n    X_train,X_test=standardization(X_train,X_test)\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n    svm_obj = svm.SVC(random_state=1)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\n    rf_random = RandomizedSearchCV(estimator = svm_obj, param_distributions = hyper_params_svm(X,y), n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    rf_random.fit(X_train, y_train)\n    print(\"Best Hyper Parameters:\",rf_random.best_params_)\n    \n    pred=rf_random.predict(X_test)\n    score=rf_random.score(X_test,y_test)\n    print(\"The model prediction is \" + str(score*100) + \"%\")\n    print(\"The confusion matrix is \")\n    print(metrics.confusion_matrix(y_test, pred))\n    print(\"the Classification report is\")\n    print(metrics.classification_report(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling Randmized search for SVM\n\nrandomizedsearch_svm(Proj_data_df, y_pcm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Inference:\n    We see that even the Hyper parameter doesnt help us in improving the Performance of the model compared to simple PCS method."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}