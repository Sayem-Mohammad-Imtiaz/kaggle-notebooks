{"cells":[{"metadata":{"_uuid":"6f8bb16aa2da63880714ef8616dcd6839f294025"},"cell_type":"markdown","source":"# Spam message prediction\n![](https://docs.google.com/drawings/d/e/2PACX-1vQLYLBjVP79VTna2EpUUh22dcuvpvqvR5cQQ_iR40bAVBgwwHgZp1H3OOTSdCDxTQGw0s13O4syogLJ/pub?w=1195&h=1029)\n\n\nWe will following the approach outlined below to identify whether the message is spam or not ?                                                                                   \n1. Import data & Understand data\n    \n2. Write a clean function\n   \n3. Create a vectorizer  & Transform into column features\n \n4. Feature engineering\n   \n5. How do these features look like ? \n  \n6. Create x features & Split data in test and train \n    \n7. Predict & check your score    "},{"metadata":{"_uuid":"946846b1f12bdf5a3b2c347b62de8c89265fb0ad"},"cell_type":"markdown","source":"\n## 1. Import data & Understand data\n    - Just eyeball data and see what messages are like\n    - Messages labelled as Ham - are from a normal human being\n    - Messages labelled as Spam - are marketing messages\n    - Our objective is to design a model to predict whether a message is from a human or is a spam"},{"metadata":{"trusted":true,"_uuid":"4153fc28cfcb11e5a971aa9876dedb4bfb15716f"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')\ndata = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n\ndata.columns =['label','body']\ndata.head()\npd.set_option('display.max_colwidth', 0) \nprint(\"sample messages from human\")\nprint(data[data['label']=='ham']['body'].head(15))\nprint(\"sample messages which are spam\")\nprint(data[data['label']=='spam']['body'].head(15))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66c5b30cc12d44b5c7f6784b35ffaafa55028b43"},"cell_type":"markdown","source":"\n## 2. Write a clean function\n    - Python needs to understand what are these words\n    - We will let go of punctuations\n    - Get rid of stop words\n    - Break every sentence"},{"metadata":{"trusted":true,"_uuid":"dba9dc718fa0e74f7e41dd984335cc1bd300daea","collapsed":true},"cell_type":"code","source":"import string\nimport nltk\n#ps = nltk.PorterStemmer()\nstopwords= nltk.corpus.stopwords.words('english')\n\ndef clean(sentence):\n    s = \"\".join(x for x in sentence if x not in string.punctuation)\n    temp = s.lower().split(' ')\n    temp2 = [x for x in temp if x not in stopwords]\n    return temp2\nclean(\"hell peOople  are hOOow ! AAare ! you. enough.. are\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6c753379bd0db114e677937b316c47648268191"},"cell_type":"markdown","source":"\n## 3. Create a vectorizer  & Transform into column features\n    - Vectorizer is our engine which will take all sentences and convert them into columns\n    - pass the clean function to apply our logic on it\n    - The real magic begins here \n    - it creates columns of all known words \n    - values are assigned based on logic of tf idf method"},{"metadata":{"trusted":true,"_uuid":"4b005dcf28262a396edf6532379b9e587dc824b8","collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer(analyzer=clean)\nvector_output = vect.fit_transform(data['body'])\n\nprint(vect.get_feature_names()[0:100])\n# these numbers are the columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"197b799d240799dd2e90c8511dc12d64ecc5d21d","scrolled":true,"collapsed":true},"cell_type":"code","source":"print (vector_output [0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ef78196459fb4784886f1e1092ce0b0c59c8616","collapsed":true},"cell_type":"code","source":"pd.DataFrame(vector_output.toarray())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8d9c2efb1df99efae1dbc90a62545e1f9687d86"},"cell_type":"markdown","source":"\n## 4. Feature engineering\n    - Time to be creative now\n    - what features you think could be indicative of spam message\n    - length\n    - punctuation\n    - contact numbers ? sms numbers"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d054c07c6e1000085f543e9f6f63a298f2d78ddd"},"cell_type":"code","source":"import re\ndata['len'] = data['body'].apply(lambda x : len(x) - x.count(\" \"))\n# METHOD 2  : data['len'] = data['body'].apply(lambda x : len(re.split('\\s+',x)))\n#print(data['body'][1]+\"  - \"+str(data['len'][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2fd933fb2124468c47cfd7987049df2007481cf","collapsed":true},"cell_type":"code","source":"test = \"Hello people this is my contact 999999999 222 888888888 20000002222\"\nlen(re.findall('\\d{7,}',test))\n# for finding numbers with digits 4,5,6,7 we will write \\d{4,7}\n# for finding numbers with digits 7,8,9,10 .... and many more . We will write \\d{7,}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ebb142daa374189235a467ad1fd48929deb55dd1"},"cell_type":"code","source":"data['long_number'] = data['body'].apply(lambda x : len(re.findall('\\d{7,}',x)))\ndata['short_number'] = data['body'].apply(lambda x : len(re.findall('\\d{4,6}',x)))\n\n#data[data['label']=='spam']\n#a=data.iloc[8,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aad866ceeaaa14355bcd566c70a1cd27834c45d","collapsed":true},"cell_type":"code","source":"import string\ndef count_punct (text):\n    count = sum([1 for x in text if x in string.punctuation])\n    pp = round(100*count/(len(text)-text.count(\" \")),3)\n    return pp\n\ndata['punct'] = data['body'].apply(lambda x : count_punct(x))\n\ntestlink = \"hello buddwwy http how com are you.co ww ww.\"\n\ndef  website (text):\n    if (len(re.findall('www|http|com|\\.co',text))>0):\n        return 1\n    else:\n        return 0\n\n#pd.set_option('display.max_colwidth', 0) \n#pd.DataFrame(data[data['label']=='spam']['body'])\nprint(website(testlink))\ndata['website'] = data['body'].apply(lambda x : website(x))\n#pd.DataFrame(data[data['label']=='spam'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"702b5c3be929d7337e4c08ee9874c5e7bfcffddb"},"cell_type":"markdown","source":"\n## 5. How do these features look like ? \n    - do they make sense ? \n    - how do they differ with spam and ham characteristic\n    - are they really worth calling features"},{"metadata":{"trusted":true,"_uuid":"3b6ebbb03d3c903a88f43f7e9950b6bcfce1a5af","collapsed":true},"cell_type":"code","source":"# how do they look like ? \n#1 len\nfrom matplotlib import pyplot\n%matplotlib inline\npyplot.figure(figsize=(15,6))\n\nbins = np.linspace(0,200,num=40)\npyplot.hist(data[data['label']=='spam']['len'],bins,alpha=0.5,label='spam',normed=True)\npyplot.hist(data[data['label']=='ham']['len'],bins,alpha =0.5,label ='ham', normed=True)\npyplot.legend(loc ='upper left')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fda29d9ef5a1752aa0843ba787b6eca7d991d2fa","collapsed":true},"cell_type":"code","source":"# punctuation \npyplot.figure(figsize=(15,6))\ni=4\nbins = np.linspace(0,40**(1/i),num=40)\npyplot.hist(data[data['label']=='spam']['punct']**(1/i),bins,normed=True,label ='spam',alpha=0.5)\npyplot.hist(data[data['label']=='ham']['punct']**(1/i),bins, normed = True, label='ham',alpha=0.5)\npyplot.show\n\n#using box cox transformation to see if the data reveal distinction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2674d0b7493b44b7946f9b7c4c0e79841a4d9b17","collapsed":true},"cell_type":"code","source":"# Numbers\n\npyplot.figure(figsize=(6,6))\npyplot.pie(data[data['label']=='spam']['long_number'].value_counts(),labels=['0','1','2','3'], \n           colors=['#5f675c','#197632','#6cdfdc','blue'],)\npyplot.title(\"Spam - long numbers\")\npyplot.show()\n\n\npyplot.figure(figsize=(6,6))\npyplot.pie(data[data['label']=='ham']['long_number'].value_counts(),labels=['0','1'], \n           colors=['#5f675c','#197632'],)\npyplot.title(\"Ham - long numbers\")\npyplot.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"acf1e8494e07a5751b711084d8ee3b0ba6e8dc5b","collapsed":true},"cell_type":"code","source":"# short Numbers\ngreen_pallete = ['#5f675c','#3db161','#66cdaa','#bee687','#6cdfdc','#d7d7ff','#ffdb00','white']\n\nspam_x = data[data['label']=='spam']['short_number'].value_counts()\nspam_x.sort_index(inplace=True)\npyplot.figure(figsize=(8,8))\npyplot.pie(spam_x,labels=spam_x.index,startangle=0,colors=green_pallete)\npyplot.title(\"Spam - short numbers\")\npyplot.show()\n\nham_x = data[data['label']=='ham']['short_number'].value_counts()\nham_x.sort_index(inplace=True)\npyplot.figure(figsize=(8,8))\npyplot.pie(ham_x,labels=ham_x.index, colors=green_pallete)\npyplot.title(\"Ham - short numbers\")\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47705138183ce96217ea9133f7dd14aa88553e0b"},"cell_type":"markdown","source":"> \n## 6. Create x features & Split data in test and train \n    - need to extract test and train data\n    - we will split it by 1: 5"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a1a2e66144f7de20e26d8f046063a3ee72fbfcc"},"cell_type":"code","source":"x_features = pd.concat([data['len'],data['long_number'],data['short_number'],data['punct'],data['website'],pd.DataFrame(vector_output.toarray())],axis=1)\n#,pd.DataFrame(vector_output.toarray())\n#,data['long_number'],data['short_number']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a7955a1afac0113a8376dceb827043665e11cba","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\nx_train, x_test, y_train, y_test = train_test_split(x_features,data['label'])\nrf = RandomForestClassifier(n_estimators=100,max_depth=None,n_jobs=-1)\nrf_model = rf.fit(x_train,y_train)\nsorted(zip(rf_model.feature_importances_,x_train.columns),reverse=True)[0:20]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b9a2984296933f23c55a1e4e206e91cb7ca87ad"},"cell_type":"markdown","source":"**\n## 7. Predict & Check your score\n    - need to understand what would happen if we use only len & punctuation\n    - OR if we used  all features but not tf idf ?\n    - you can see below - how the precision jumps when we add tf idf "},{"metadata":{"trusted":true,"_uuid":"c5f182f32c272d3b6db7b57996f5f8f90b28ee90","collapsed":true},"cell_type":"code","source":"y_pred=rf_model.predict(x_test)\nprecision,recall,fscore,support =score(y_test,y_pred,pos_label='spam', average ='binary')\nprint('Precision : {} / Recall : {} / fscore : {} / Accuracy: {}'.format(round(precision,3),round(recall,3),round(fscore,3),round((y_pred==y_test).sum()/len(y_test),3)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"327b4fa295373e7161e7af0ec1edf750d70ce17b"},"cell_type":"markdown","source":" Len + punct                       ***  Precision : 0.557 / Recall : 0.566 / fscore : 0.562 / Acc: 0.89\n\nLen + punct + nums       ***  Precision : 0.914 / Recall : 0.905 / fscore : 0.909 / Acc: 0.974\n(major jump in performance)\n\nrest + website               ***  Precision : 0.901 / Recall : 0.901 / fscore : 0.901 / Acc: 0.974\n\nAll features + tfidf     ***  Precision : 0.984 / Recall : 0.909 / fscore : 0.945 / Acc: 0.985\n(Marginal but very critical jump in precision)\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}