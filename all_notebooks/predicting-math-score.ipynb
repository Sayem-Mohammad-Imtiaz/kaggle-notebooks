{"cells":[{"metadata":{"_uuid":"0ed096c8020cf2b55713cc5be1848ee72903ebeb"},"cell_type":"markdown","source":"# Predicting students' math performances through the data set\n\nIn this kernel, I am just focusing on predicting the math scores. I reach quite a low RMSE with XGBoost (~5 ish) which is not too shabby. "},{"metadata":{"_uuid":"e30c33e140ae9fe4c3bc21b7b87d48c6c0b999c1"},"cell_type":"markdown","source":"First, we load in the data and inspect it. "},{"metadata":{"trusted":true,"_uuid":"35bef7f5db4f7503ceb3206b313acae5de9b8f70"},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\n\ndata = pd.read_csv('../input/StudentsPerformance.csv')\n\n# Convert to categorical!\nfor i in range(data.shape[1]):\n    if i not in [5,6,7]:\n        data.iloc[:, i] = data.iloc[:,i].astype(\"category\")\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\nfor i in range(data.shape[1]):\n    if i not in [1,2,5,6,7]:\n        data.iloc[:, i] = label_encoder.fit_transform(data.iloc[:,i])\n\n\n\ndata = pd.get_dummies(data)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54b16d853e7dd92509eccd722fde505d91009962"},"cell_type":"markdown","source":"So this is the data. What do we do with it now? We could try a logistic regression analysis, since we have many classes. The classes are mostly binary or tertiary, apart from parental level of education. Random Forest and XGBoost are also great options, so I will implement these with cross-validation. For each score, I will create one of each model mentioned above and compute their MSE scores. But first, we divide into training and test data. "},{"metadata":{"trusted":true,"_uuid":"b21eedf5c5cd6c379249dc4cdabae5c9329c39c3"},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split \n\ntrain, test = train_test_split(data, random_state = 123)\n\nprint(train.shape)\nprint(test.shape)\n\ncol_names = list(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdc35730df8cfd12f766228621429587af635456"},"cell_type":"markdown","source":"## Predicting Math score\n\nFirst, we predict the math score. "},{"metadata":{"trusted":true,"_uuid":"cf4fbb4520d7cb4102334c6be6f4c3091c323701"},"cell_type":"code","source":"math_pred_feats = [x for i, x in enumerate(col_names) if i in list(range(0,data.shape[1])) and i not in [3]]\nmath_pred_label = col_names[3]\ntrain_math = train[math_pred_feats]\ny_train_math = train[math_pred_label]\ntest_math = test[math_pred_feats]\ny_test_math = test[math_pred_label]\n\n\n\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\n\n\nxgb_model = XGBRegressor(max_depth = 5,\n                n_estimators=500,\n                n_jobs=4,\n                subsample=1.0,\n                colsample_bytree=0.7,\n                random_state=1302)\nxgb_params = xgb_model.get_xgb_params()\n\nxgb_model.fit(train_math, y_train_math, verbose = True)\n\ntrain_math.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6190058700bee08a7ba5ad5c9122287d541140e"},"cell_type":"code","source":"test_math.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83d73d189d602ab8e95590621c63b9c7b72be40c"},"cell_type":"markdown","source":"Now, we have a model. Now, we use that to predict. "},{"metadata":{"trusted":true,"_uuid":"88b5cae2056b4cfb818bf77d1bd80b97f83975e5"},"cell_type":"code","source":"preds = xgb_model.predict(test_math)\nfrom sklearn.metrics import mean_squared_error\n\nprint(\"RMSE: \",np.sqrt(mean_squared_error(y_test_math,preds)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bd4a533326ad825a35af20ae5921ab233558a69"},"cell_type":"markdown","source":"Quite a nice RMSE. Let's see if we can reduce this by performing a randomized search. Randomized searches are quicker and thus more convenient, and usually performs more or less as good as exhaustive grid searches. "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b722fcdb9552c53b0e28603c35702c0013f8ec5b"},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform, randint\n\n\nparam_dist = {\n    'colsample_bytree':uniform(0.1,0.9),\n    'gamma':reciprocal(1e-5,1),\n    'min_child_weight':[1,3],\n    'learning_rate':reciprocal(1e-4,1),\n    'max_depth':randint(2,6),\n    'n_estimators':randint(100,1000),\n    'reg_alpha':[1e-5, 0.1],\n    'reg_lambda':[1e-5, 0.1],\n    'subsample':[0.8]\n}\n\nrand_search = RandomizedSearchCV(estimator = xgb_model, param_distributions = param_dist, n_iter = 3, n_jobs=3, iid=False,verbose=True, scoring = 'neg_mean_squared_error', random_state = 123)\nprint(\"Fitting model...\")\nrand_search.fit(train_math, y_train_math)\nprint(\"Model fitted\")\nprint(\"Best score: \")\nprint(rand_search.best_score_)\nprint(\"Best model: \")\nprint(rand_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d88f3ab1b98e0491aa235d21f1cd96b6a8da946"},"cell_type":"markdown","source":"Now use the best model to fit, and check how much the MSE has been reduced. "},{"metadata":{"trusted":true,"_uuid":"6b4a9c0372f9d17ea62980037abb2fe832dd4307"},"cell_type":"code","source":"best_model = XGBRegressor(**rand_search.best_params_)\nbest_model.fit(train_math, y_train_math)\n\ny_preds = best_model.predict(test_math)\nprint(\"RMSE: \",np.sqrt(mean_squared_error(y_test_math,y_preds)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff40a942384e57943955836f9a3410e7eb160309"},"cell_type":"markdown","source":"Nice. Now, let's try randomforest. For speed, let's just do a quick Randomized search. "},{"metadata":{"trusted":true,"_uuid":"0bf7eb5afd0d6ee289e6701a6fdedb2e7c9364ab"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor()\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"max_depth\":randint(3,5),\n    \"max_features\":randint(3,5),\n    \"bootstrap\":[True,False],\n    \"min_samples_split\":randint(2,7),\n    'n_estimators':randint(10,500)\n}\n\nrand_search = RandomizedSearchCV(rf_model, param_distributions = param_dist, n_jobs=5, cv = 5, verbose = True, n_iter=90, random_state = 123)\nrand_search.fit(train_math, y_train_math)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7db92a475b85200c21cd4106572c1fab741a0a5c"},"cell_type":"code","source":"print(rand_search.best_score_)\nprint(rand_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a704289512b92a8a6a1110f0b7bd8b97f97b969"},"cell_type":"markdown","source":"Now use the best model!"},{"metadata":{"trusted":true,"_uuid":"b95337efda8af2e372f05ef3616ba0ab3696b1d1"},"cell_type":"code","source":"best_rf_model = RandomForestRegressor(**rand_search.best_params_)\nbest_rf_model.fit(train_math,y_train_math)\n\nrf_preds = best_rf_model.predict(test_math)\nprint(\"MSE:\", mean_squared_error(y_test_math,y_preds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b294a4d4fcedcd9677b3c84dc69ce07435611fa1"},"cell_type":"markdown","source":"Not very good either. Better can be achieved. \n\n## Adaboost with decision stumps\n\nLet's try decision stumps, and see if they work well. "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5b985dcf5834476c3bc91dfeb8a4fea07f111815"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nparam_dist = {'criterion':['mse'], \n              'max_depth':[1], \n              'max_features':[None],\n           'max_leaf_nodes':[None], \n              'min_impurity_decrease':uniform(0,1),\n           'min_impurity_split':[None], \n              'min_samples_leaf':uniform(1e-9,(0.5-1e-9)),\n           'min_samples_split':randint(2,5), \n              'min_weight_fraction_leaf':reciprocal(1e-7,0.5),\n           'presort':[False], \n              'random_state':[123], \n              'splitter':['best']\n             }\n\nrand_search = RandomizedSearchCV(DecisionTreeRegressor(), param_distributions = param_dist, n_jobs=5, cv = 5, verbose = True, n_iter=90)\nrand_search.fit(train_math, y_train_math)\n\nadaboost_model = AdaBoostRegressor(DecisionTreeRegressor(**rand_search.best_params_), n_estimators=50).fit(train_math, y_train_math)\n\nprint(adaboost_model)\ny_preds = adaboost_model.predict(test_math)\nprint(\"MSE:\", mean_squared_error(y_test_math, y_preds))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_test_math, y_preds)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1bbe3d4312baf5b3731c559884f51ac812389ad"},"cell_type":"markdown","source":"Okay, these decision stumps did not work that well. Let's try something else.  "},{"metadata":{"_uuid":"e57f2b99a30060a0c5d044786abac6e753f95e60"},"cell_type":"markdown","source":"## Gaussian process regression\n\nHow about Gaussian Process regression? We have very few dimensions, which definitely makes it suitable. We just want to scale the data of course, to ensure a mean of 0 on all variables which is needed for Gaussian Processes. Of course, it probably won't be a mean of 0 as it is unbalanced between -1 and 1, but let's try. "},{"metadata":{"trusted":true,"_uuid":"5081b67b94ccfb402603afe568a1feacdc4c16e3"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40f3cc6642aeda96413a30b3f5b0fbf4f313f5f9"},"cell_type":"code","source":"from sklearn.preprocessing import scale\nmath_score_mean = np.mean(data['math score'])\nmath_score_sd = np.std(data['math score'])\ndata.iloc[:,0:3] = data.iloc[:,0:3].replace({1:1, 0:-1})\ndata.iloc[:,3:6] = data.iloc[:,3:6].apply(lambda x: (x - np.mean(x)) / np.std(x), axis=0)\ndata.iloc[:,6:17] = data.iloc[:,6:17].replace({1:1, 0:-1})\n\n\ntrain, test = train_test_split(data, random_state = 123)\ndata.head()\n\ntrain_math_gp = train.drop(['math score','reading score','writing score'], axis = 1)\ny_train_math_gp = train['math score']\n\ntest_math_gp = test.drop(['math score','reading score', 'writing score'], axis = 1)\ny_test_math_gp = test['math score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"884ce492eda83f6365b67304ad1e7f2223e86e01"},"cell_type":"code","source":"print(test_math_gp.shape)\ntest_math_gp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b157b23ecf26cbebd035e3fa17fc8440a700a7f3"},"cell_type":"code","source":"print(train_math_gp.shape)\ntrain_math_gp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8bd549d220b1bac1c2adeadbc2a331ab3cbbee0c"},"cell_type":"code","source":"from sklearn.gaussian_process import GaussianProcessRegressor\n\ngp_model = GaussianProcessRegressor(n_restarts_optimizer = 50)\ngp_model.fit(train_math_gp, y_train_math_gp)\ny_preds = gp_model.predict(test_math_gp)\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test_math_gp, y_preds)))\nprint(\"MSE: \", mean_squared_error(y_test_math_gp, y_preds))\n\nfrom sklearn.model_selection import cross_val_score\nprint(cross_val_score(gp_model,cv=5,X = data.drop(['math score','reading score','writing score'], axis = 1), y = data['math score'], scoring = 'neg_mean_squared_error'))\n\n# Rescale it. \n\nMSE = mean_squared_error(y_test_math_gp, y_preds)*math_score_sd + math_score_mean\n\nprint(\"MSE\", MSE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aebf5d80cfa6afaa9de05064232d570d0c940405"},"cell_type":"markdown","source":"Okay, this was not that nice to be honest - not a particularly good score. It seems like XGBoost was the best in this case, achieving quite a low MSE."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}