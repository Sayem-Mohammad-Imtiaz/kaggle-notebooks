{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Employee Attrition Prediction || Comparing different classification algorithms"},{"metadata":{},"cell_type":"markdown","source":"According to:\n* **Bureau of Labour Statistics** 4.9 million monthly separations (employee turnover) occurred between August and December in 2016.\n* **a research conducted by The Center for American Progress**, the median cost of turnover was 21% of an employee’s annual salary.\n* **Society for Human Resource Management studies**, every time a business replaces a salaried\nemployee, it costs 6 to 9 months’ salary on average. For a manager making \\$40,000 a year, that's\n\\$20,000 to \\$30,000 in recruiting and training\n\nThis notebook is structured as follows:\n   1. **Exploratory Data Analysis (EDA)**: It is an approach to analyze data sets and summarize their characteristics. This section outlines the differents statistical analyses performed.\n   2. **Data balacing**: In this section seviral tachniques performed to balance data. \n   3. **Data pre-processing**: is a common process in machine learning projects, it had used to handle datasets that contain missing entries, varying degrees of noise and substantial differences in scale per feature\n   4. **Feature selection**\n   4. **Applay suprvised ML algorithims**: differents classification algorithms applayed and compared.\n   5. **Models interpretability**: the permutation feature importance technique was used to measure how much a feature is important to predict employee attrition."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, FunctionTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis (EDA) \nFirstly, lets take a quik look into dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have 34 features(categorical and numerical) and a categorical target column 'Attrition' with tow value; 'Yes' if employyee exit or 'No' if stayed.\n\nHowever that was just an overview of dataset. Now, lets deeve more into EDA to determine the quality of features and their predictive power in contrast with target value or label. The exploration of the data is conducted from two different angles: descriptive and correlative. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df.copy()\ntarget_map = {'Yes':1, 'No':0}\ny = df_copy['Attrition'].apply(lambda value: target_map[value]) # Encode target column\nX = df_copy.drop(['Attrition'], axis = 1) # Separate predictor variables from predicted value\n\n# Devide our dataframe into numerical dataframe and categorical dataframe\nnum_df = X.select_dtypes(exclude=object)  \ncat_df = X.select_dtypes(include=object)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Descriptive Analysis\nDescriptive analysis (or univariate analysis) provides an understanding of the characteristics of each attribute of the dataset. It also offers important evidence for feature selection in a later state. in this section we will cheeck:\n* If there are any missing values\n* Data type of each attribute\n* Data distribution for each attribute"},{"metadata":{},"cell_type":"markdown","source":"The code bellow cheecks if there is any NaN value in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fortunally, the are no missing values for this dataset !"},{"metadata":{},"cell_type":"markdown","source":"The code bellow cheecks data type for each columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, there are two data types: object and int64. object columns contains non numerical data and should be encoded into numric format, we will talk about this later."},{"metadata":{},"cell_type":"markdown","source":"The code bellow generate show underlying frequency distribution of each numerical variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i, col in enumerate(num_df.columns, 1):\n    plt.subplot(5, 6, i)\n    sns.violinplot(x = df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that 'EmployeeCount' and 'StandardHours' have the same value within all dataset, so they should be taken into consideration when performing feature selection.\n\nFurthemore, We can observe that there is continue variables (like Age, HourlyRate and monthlyRate) and discreet variables(like Education and JobSatisfaction). Also, all variables have different data distributions, which the majors are Skewed Right Distributions with long-tail shape (like Years at Company and Years since last promotion) . This is a problem because statistical models like SVM, KNN and LR will considered the tail region as outliers. and we know that outliers adversely affect the model’s performance. Actually, There are statistical models that are robust to outlier like a Tree-based models but it will limit the possibility to try other models. Therefore, a transformation to normal distribution should be performed when preprocessing data."},{"metadata":{},"cell_type":"markdown","source":"Now, lets see how distributions of categorical varibles looks like. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i, col in enumerate(cat_df.columns, 1):\n    plt.subplot(3, 3, i)\n    sns.countplot(x = df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that 'Over18' columns have one unique value and should be excluded too from dataset."},{"metadata":{},"cell_type":"markdown","source":"### Correlation analysis\nCorrelation analysis (or bivariate analysis) examines the relationship between two attributes, and determines whether the two are correlated. it devided into tow sections:\n* Numerical columns versus target\n* Categorical verus target\n"},{"metadata":{},"cell_type":"markdown","source":"The code below generate distribution of numerical variables in terms of attrition. Rapprochement observed between negative and positive classes with all variables, especially with 'PercentSalaryHike' and 'NumCompaniesWorked' variables."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,30))\nfor i, col in enumerate(num_df.columns, 1):\n    plt.subplot(5, 6, i)\n    sns.violinplot(x = df['Attrition'], y= df[col])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For correlation between categorical attributes and target attribute, the figure in the following can be generated. We can observe that all categorical attributes have values with different attrition rate, except gender columns, it seems less correlated to attrition. In addition, we can observe that employees who work overtime are more likely to exit."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,30))\nfor i, col in enumerate(cat_df.columns, 1):\n    plt.subplot(3, 3, i)\n    sns.barplot(x = df[col], y= y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data pre-processing\nData preprocessing techniques performed within this project described as follow:\n1. Data Type Conversion\n2. Feature Scaling and Log transformation\n\nIn order to simplify those pre-processing tasks, piplines used according to its sevreral benifits:\n\n* **Convenience and encapsulation**: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n* **Joint parameter selection**: You can grid search over parameters of all estimators in the pipeline at once.\n* **Safety**: Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n"},{"metadata":{},"cell_type":"markdown","source":"### Data Type Conversion\nOne of the most important preprocessing procedures is convert categorical features to numerical format. Some algorithms such as logistic regression, K-nearest neighbor, SVM and neural networks are not able to work with non-numeric data.\n\nThe code bellow declare a pipline to apply ordinal encoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Pipeline(steps=[\n    ('encoder', OrdinalEncoder())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling and Log transformation\nFeature scaling is essential prepressing step when features has disparate scales. It may help some machine learning classifiers perform better, because significant scale gaps among features are generally not favored within the optimization stage of these algorithms. Standardization and normalization are two techniques to handle such a problem. Dataset's features has a disparate scale. For example, daily rate range between 100 to 1500, whereas job level range between 1 to 5. normalization performed to adjust range of features and reduce disparate feature scales in IBM HR dataset.\n\nThe log transformation can be used to make highly skewed distributions less skewed like in our case, in which most numerical features are right skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.\n\nThe code bellow declare a pipline to apply normalization and log transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_transformer = Pipeline(\n    steps=[\n        ('scaler', MinMaxScaler()),\n        ('transfomer', FunctionTransformer(np.log1p, validate=True))\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To join `encoder` and `scaler_transfomer` piplines `ColumnTransformer` object used"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', encoder, cat_df),\n        ('cat', scaler_transformer, num_df)\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature selection\nThe feature selection techniques are often used to further improve the classifier’s predictive capability by selecting the relevant features. Feature selection is primarily focused on removing non-informative or redundant predictors from the model. We will use the following techniques to perform feature selection:\n* Statistical-based method\n* Feature importance based method\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop unused columns\nX.drop(['EmployeeCount','Over18','StandardHours','EmployeeNumber'], axis = 1, inplace = True) \nnum_df = X.select_dtypes(exclude = object)\ncat_df = X.select_dtypes(include = object)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statistical-based method"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\nmask = np.zeros_like(num_df.join(y).corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nfig, ax = plt.subplots(figsize=(15,10))\ncmap = sns.diverging_palette(255, 10, as_cmap=True)\nsns.heatmap(num_df.join(y).corr().round(2), mask=mask, annot=True,\n            cmap=cmap , vmin=-1, vmax=1, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)"},{"metadata":{},"cell_type":"markdown","source":"I'm still updating this notebook ..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}