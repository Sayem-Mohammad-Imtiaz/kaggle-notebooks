{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Version v04-08\n# Import all libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt # ploting the data\nimport seaborn as sns # ploting the data\nimport math # calculation\nimport sys\n!{sys.executable} -m pip install scipy\nfrom scipy import stats\nfrom scipy.stats import zscore\nimport datetime\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as sm\nfrom sklearn.metrics import r2_score\nfrom datetime import datetime\nimport time\nimport dateutil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data load \na=[\"date\",\"Appliances\",\"T1\",\"RH_1\",\"T2\",\"RH_2\",\"T3\",\"RH_3\",\"T4\",\"RH_4\",\"T5\",\"RH_5\",\"T6\",\"RH_6\",\"T7\",\"RH_7\",\"T8\",\"RH_8\",\"T9\",\"RH_9\",\"T_out\",\"RH_out\",\"Windspeed\",\"Visibility\",'Press_mm_hg', 'Tdewpoint', 'lights']\n#load the datasets\ndf = pd.read_csv(r\"../input/appliances-energy-prediction/KAG_energydata_complete.csv\",usecols=a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize data info\ndf.info()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Data Preprosessing\nAfter loading both data set I checked the info of both data set Checked and sum the null values Cheeked the unique values of different attributes Dropped the irrelevant attributes according to evaluation data set Recalculated the null values and then fill the null values Checked the description of data sets and checked the relevant attributes (mean, minimum, maximum, and std ) for regression make a new development data set according to the price is greater than 0 I applied all the unmentioned steps on the both data sets. I checked the correlation on development data set Rationalize the related attributes values for regression analysis For visualization I plotted the different graphs as well"},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Extracting components from date object"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"exact_date\"]=df['date'].str.split(' ').str[0]\n\ndf[\"hours\"]=(df['date'].str.split(':').str[0].str.split(\" \").str[1]).astype(str).astype(int)\ndf[\"seconds\"]=((df['date'].str.split(':').str[1])).astype(str).astype(int).mul(60)\n\ndf[\"days\"]=(df['date'].str.split(' ').str[0])\ndf[\"days\"]=(df['days'].apply(dateutil.parser.parse, dayfirst=True))\ndf[\"days_num\"]=(df['days'].dt.dayofweek).astype(str).astype(int)\ndf[\"days\"]=(df['days'].dt.day_name())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 Visualizing Per day consumption\nSeasonality\nthis is used to track seasonality. but we have only 4.25 months data so there were no traces of seasonality\n\nThe traces shows that there is varaiation of electric usage on granular terms (like days)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dates=df[\"exact_date\"].unique()\narranged_day = pd.Categorical(df[\"exact_date\"], categories=dates,ordered=True)\ndate_series = pd.Series(arranged_day)\ntable = pd.pivot_table(df,\n               values=\"Appliances\",index=date_series,\n               aggfunc=[np.sum],fill_value=0)\ntable.plot(kind=\"bar\",figsize=(20, 7))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.3 Visualizing data \nby Hour Wise Daily Consumption\nwe also saw a trend in hourly consumption"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\ndays=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\",\"Saturday\",\"Sunday\"]\narranged_day = pd.Categorical(df[\"days\"], categories=days,ordered=True)\nday_series = pd.Series(arranged_day)\ntable = pd.pivot_table(df,index=[\"hours\"],\n               values=\"Appliances\",columns=day_series,\n               aggfunc=[np.sum],fill_value=0)\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.set_title('Heatmap : Appliances(wh)')\n\nheatmap = ax.pcolor(table)\n\nax.set_xlabel(\"Week Days\")\nax.set_ylabel(\"Hours\")\n\nplt.colorbar(heatmap)\nax.set_yticks(range(len(table.index)+1))\nax.set_xticks(range(len(table.columns)+1))\n\nplt.xlabel(\"Week\")\nplt.ylabel(\"Hours of Day\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.4 Ploting\nThis plot shows that there is no such coorelation between energy conuption on weekdays vs off days"},{"metadata":{"trusted":true},"cell_type":"code","source":"table.plot.box(figsize=(20, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntable=df[[\"Appliances\",'lights', 'T1', 'RH_1',\"Press_mm_hg\",\"RH_out\",\"Windspeed\",\"Visibility\",\"Tdewpoint\",'hours','days_num']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Null Values Check and remove \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d=df\ndf_d.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.6 Outlier Removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d['Appliances'].plot(kind=\"hist\",figsize=(20, 7))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df_d[['Appliances']]\nQ1 = X.quantile(0.25)\nQ3 = X.quantile(0.75)\nIQR = Q3 - Q1\nprint(\"Outlier threshold of Appliances \",IQR)\n\ndfOutlier=X.describe()\noutlierSet=set()\nfor column in dfOutlier.columns:\n    Q1 = dfOutlier[column]['25%']\n    Q3 = dfOutlier[column]['75%']\n    IQR = Q3 - Q1\n    outlierDf= ( ((X[column] < (Q1 - 1.5 * IQR)) |(X[column] > (Q3 + 1.5 * IQR))) )\n    outlierSet.update(set(outlierDf[outlierDf==True].index))\n        \n\ndf_d.drop(outlierSet, inplace=True, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#examine the dataset\ndf_d.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d['Appliances'].plot(kind=\"hist\",figsize=(20, 7))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.8 Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df_d.corr(method='pearson')\nmatrix=corr_matrix[\"Appliances\"].sort_values(ascending=False)\nprint((matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\ntitle = 'Correlation matrix of numerical variables'\nsns.heatmap(df.corr(), square=True, cmap='RdYlGn')\nplt.title(title)\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Split dataset into Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the dataset\nx= df_d[['lights','T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4',\n       'RH_4', 'T5', 'RH_5', 'T6','RH_6', 'T7', 'RH_7','T8', 'RH_8', 'T9',\n       'RH_9', 'T_out', 'RH_out', 'Windspeed', 'Visibility', \n       'hours', 'days_num','Tdewpoint','Press_mm_hg']]\nX=x.values\ny = df_d['Appliances'].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling\nMulti liner regression\nLasso\nGBM\nRandom forest regression\nSVM\nAlgorithm for regression. MLR provided the best results. During the data preprocessing I did not get any polynomial or liner relation of any attributes with price. So I chosen the most relevant attributes values for MLR. I run almost MLR and RFR almost 12 times with the different attributes combination and changing the rationalize factors but at the end MLR gave me the best results."},{"metadata":{"trusted":true},"cell_type":"code","source":"def result (y_pred_train, y_pred_test,y_train,y_test):\n  print(\"train results\")\n  print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_train, y_pred_train), 2)) \n  print(\"Mean squared error =\", round(sm.mean_squared_error(y_train, y_pred_train), 2)) \n  print(\"Median absolute error =\", round(sm.median_absolute_error(y_train, y_pred_train), 2)) \n  print(\"Explain variance score =\", round(sm.explained_variance_score(y_train, y_pred_train), 2)) \n  print(\"R2 score =\", round(sm.r2_score(y_train, y_pred_train), 2))\n\n\n  print(\"test results\")\n  print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_pred_test), 2)) \n  print(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_pred_test), 2)) \n  print(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_pred_test), 2)) \n  print(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_pred_test), 2)) \n  print(\"R2 score =\", round(sm.r2_score(y_test, y_pred_test), 2))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.1 Plot Training Deviance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Training Deviance\n\n# compute test set deviance\ntest_score=np.zeros((params['n_estimators'],), dtype=np.float64)\n\nfor i,y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i]=clf.loss_(y_test,y_pred)\n\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.title('Deviance')\n\nplt.plot(np.arange(params['n_estimators'])+1, model.train_score_,'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators'])+1,test_score,'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the forest's predict method on the test data\npredictions = clf.predict(X_test)\n# Calculate the absolute errors\nerrors = abs(predictions - y_test)\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / y_test)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  4.3 Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=500, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n        max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=[True, False],\n        oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\nregressor.fit(X_train, y_train)\n\ny_pred_train = regressor.predict(X_train)\ny_pred_test = regressor.predict(X_test)\n\nrfr_result=result (y_pred_train, y_pred_test,y_train,y_test)\nrfr_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3.1 Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the forest's predict method on the test data\npredictions = regressor.predict(X_test)\n# Calculate the absolute errors\nerrors = abs(predictions - y_test)\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / y_test)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3.2 Variable Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving feature names for later use\nfactor_list = list(x.columns)\n# Convert to numpy array\nfactors = np.array(x)\n#Get numerical feature importances\nimportances = list(regressor.feature_importances_)\n# List of tuples with variable and importance\nfactor_importances = [(factor, round(importance, 2)) \n    for factor, importance in zip(factor_list, importances)]\n# Sort the feature importances by most important first\nfactor_importances = sorted(factor_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in factor_importances];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import matplotlib for plotting and use magic command for Jupyter Notebooks\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Set the style\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, factor_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); \nplt.title('Variable Importances');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3.3 ACTUAL VS PREDICTED"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred= pd.DataFrame({'Actual': np.round(y_test, 0), \n                   'Predicted': np.round(y_pred_test, 0)})\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}