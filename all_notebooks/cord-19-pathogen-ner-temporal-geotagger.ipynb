{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Open Research Dataset (CORD-19) Temporal Geotagger\n<hr>\nThis Jupyter Notebook reads a time-series that was generated by applying a spaCy model to perform Natural Language Processing (NLP) Named Entity Recognition (NER) on the CORD-19 set of full text articles. The NER of interest is the SARS-CoV virus. Each article identified as describing research on this virus is then searched for a representative country. If a country is identified, the article is added to the time-series. The time-series is then displayed on a globe with a time slider to represent the number of articles published on this topic and in what country the research describes. Country tooltips are displayed with total article count on the given date and the most recent article CORD_UID.\n<hr>\n>DISCLAIMER:  THIS SET OF CODE IS FOR USE IN A NON-CLINICAL RESEARCH SETTING.  IT IS NOT INTENDED FOR USE \nTO TREAT OR DIAGNOSE PATIENTS, DRIVE CLINICAL MANAGEMENT OR INFORM CLINICAL MANAGEMENT.   \nThis software is provided on an “as is” basis without any warranty or liability whatsoever. Northrop Grumman \nSystems Corporation (NGSC) expressly disclaims, to the maximum extent permissible by applicable law, all \nwarranties, express, implied and statutory, including without limitation any implied warranty of merchantability, \nfitness for a particular purpose, non-infringement, or arising from course of performance, dealing, usage, or trade. \nNGSC shall not be liable to the Government or any user of the software for any incidental, consequential, special or \nother damages, including loss of profits, revenue, or data, resulting, directly or indirectly, from use of the software. \"©2020 Northrop Grumman Systems Corporation.\""},{"metadata":{},"cell_type":"markdown","source":"### Horseshoe bat (Rhinolophus): Natural reservoir of SARS-CoV-2 virus, cause of COVID-19.\n![CORD-19](https://upload.wikimedia.org/wikipedia/commons/3/35/Rhinolophus_rouxii.jpg)\n*Image Credits : By Aditya Joshi - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=20131071\n<br><hr>"},{"metadata":{},"cell_type":"markdown","source":"# Prerequisites\n<ul><li>pandas</li>\n    <li>numpy</li>\n    <li>pycountry</li>\n    <li>plotly</li></ul>\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Installs\n#!pip list\n#!pip install pandas\n#!pip install numpy\n#!pip install pycountry\n#!pip install plotly\n#!pip install matplotlib","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pycountry as pc\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\n## COVID-19 Open Research Dataset (CORD-19)\nLatest release contains papers up until 2020-04-10 with over 40,000 full text articles.<br>\n<br>   Commercial use subset (includes PMC content) 350Mb: \n<ul><li>        PDF - 9524 full text (new: 174, removed: 15)</li>\n    <li>        PMC - 9148 full text (new: 174, removed: 21)</li></ul>\n    Non-commercial use subset (includes PMC content) 73MB:\n<ul><li>        PDF - 2490 full text (new: 130, removed: 17)</li>\n    <li>        PMC - 2217 full text (new: 128, removed: 4)</li></ul>\n    Custom license subset (includes PMC, Elsevier content) 630MB:\n<ul><li>        PDF - 26505 full text (new: 3647, removed: 294)</li>\n    <li>        PMC - 7802 full text (new: 3081, removed: 52)</li></ul>\n    bioRxiv/medRxiv subset (pre-prints that are not peer reviewed) 22Mb: \n<ul><li>        PDF - 1625 full text (new: 353, removed: 70)</li></ul>\n    Metadata file -- 74Mb (with Microsoft Academic ID mapping)\n<br>\nEach paper is represented as a single JSON object. The schema is available.\n<br><hr>\nDescription: The dataset contains all COVID-19 and coronavirus-related research (e.g. SARS, MERS, etc.) from the following sources:\n<ul><li>PubMed's PMC open access corpus using this query (COVID-19 and coronavirus research)</li>\n    <li>Additional COVID-19 research articles from a corpus maintained by the WHO</li>\n    <li>bioRxiv and medRxiv pre-prints using the same query as PMC (COVID-19 and coronavirus research)</li></ul>\nProvided also is a comprehensive metadata file of 51,078 coronavirus and COVID-19 research articles with links to PubMed, Microsoft.\n<hr>\n### Citations:\n#### COVID-19 Open Research Dataset (CORD-19). 2020. Version 2020-03-20. Retrieved from https://pages.semanticscholar.org/coronavirus-research. Accessed 2020-04-10. doi:10.5281/zenodo.3715505.\n#### Ashish Patil - CORD-19 Research Dataset: Analysis & Visualization - [Visit this link](https://www.kaggle.com/finalepoch/cord-19-research-dataset-analysis-visualization) for the main page. In this notebook Ashish created a Named Entity Recognition system for analyzing the [CORD 19 Research Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) dataset. Ashish used Spacy's [custom named entity recognition](https://spacy.io/usage/training/) to train a model that can detect mentions of three things in text: MedicalCondition, Medicine, and Pathogen. Ashish used [LightTag](https://www.lighttag.io/) for creating a manually tagged corpus from randomly selected text from Wikipedia. The dataset is available here : [https://www.kaggle.com/finalepoch/medical-ner](https://www.kaggle.com/finalepoch/medical-ner)."},{"metadata":{},"cell_type":"markdown","source":"# Load NLP Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n#with open(\"/tf/CORD-19_20200410/Corona2.json\") as f:\n#https://www.kaggle.com/medical-ner/Corona2.json\nwith open(\"/kaggle/input/medical-ner/Corona2.json\") as f:\n    annotation = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert the tagged data to a format understood by Spacy. Remove anything that has spaces and does **not** have   *\"human_annotations\"*."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA  = []\nfor e in annotation[\"examples\"]:\n    content = e[\"content\"]\n    entities = []\n    for an in e[\"annotations\"]:        \n        if len(an[\"value\"]) == len(an[\"value\"].strip()):          \n            if len(an['human_annotations']) == 0:\n                continue\n            info = (an[\"start\"],an[\"end\"],an[\"tag_name\"])\n            entities.append(info)\n            #print(an[\"start\"],an[\"end\"],an[\"tag_name\"])\n    if len(entities) > 0:\n        TRAIN_DATA.append(([content,{\"entities\":entities}]))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train spaCy Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function\nimport random\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\nimport spacy\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.util.use_gpu(0)\n#def train_model(model=None, output_dir=\"/tf/CORD-19_20200410/medical-ner\", n_iter=1000):\ndef train_model(model=None, output_dir=\"/kaggle/working/medical-ner\", n_iter=1000):\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        if model is None:\n            nlp.begin_training(device=0)\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 64.0, 1.2))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  \n                    annotations,  \n                    drop=0.20, \n                    losses=losses\n                   \n                )\n            print(\"Losses\", losses)\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perform Pathogen NER on CORD-19 Challenge Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#nlp2 = spacy.load(\"/tf/CORD-19_20200410/medical-ner\")\n#nlp2 = spacy.load(\"/kaggle/working/medical-ner\")\nnlp2 = spacy.load(\"/kaggle/input/cord19-temporal-geotagger-dataset/medical-ner\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport csv\nimport json\nimport random\n#metadata = open('/tf/CORD-19_20200410/metadata.csv', 'r')\nmetadata = open('/kaggle/input/CORD-19-research-challenge/metadata.csv', 'r')\nmetadata_fieldnames=('cord_uid','sha','source_x','title','doi','pmcid','pubmed_id','license','abstract','publish_time','authors','journal','Microsoft Academic Paper ID','WHO #Covidence','has_pdf_parse','has_pmc_xml_parse','full_text_file','url')\nmetadata_reader = csv.DictReader(metadata, metadata_fieldnames)\ncord_uid = []\nsha = []\ntitle = []\npmcid = []\npubmed_id = []\nabstract = []\npublish_time = []\nfor row in metadata_reader:\n    cord_uid.append(row['cord_uid'].strip())\n    sha.append(row['sha'].strip())\n    title.append(row['title'].strip())\n    pmcid.append(row['pmcid'].strip())\n    pubmed_id.append(row['pubmed_id'].strip())\n    abstract.append(row['abstract'].strip())\n    publish_time.append(row['publish_time'].strip())\nmetadata.close()\n    \nfiles = []\n#for dirname, _, filenames in os.walk('/tf/CORD-19_20200410/dat/'):\nfor dirname, _, filenames in os.walk('/kaggle/input/CORD-19-research-challenge/'):\n    for filename in filenames:\n        if \".json\" in filename:           \n            fpath = os.path.join(dirname, filename)\n            if len(files) < 300:\n                files.append(fpath)\nrandom.shuffle(files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Local transmission of SARS took place in Toronto, Ottawa, San Francisco, Ulaanbaatar, Manila, Singapore, Taiwan, Hanoi and Hong Kong\n# within China it spread to Guangdong, Jilin, Hebei, Hubei, Shaanxi, Jiangsu, Shanxi, Tianjin, and Inner Mongolia.\nregions_of_interest = [['China','China'],\n                       ['Hong Kong','China'],\n                       ['Mongolia','Mongolia'],\n                       ['Ulaanbaatar','Mongolia'],\n                       ['Philippines','Philippines'],\n                       ['Manila','Philippines'],\n                       ['Taiwan','Taiwan'],\n                       ['Vietnam','Vietnam'],\n                       ['Hanoi','Vietnam'],\n                       ['Canada','Canada'],\n                       ['Toronto','Canada'],\n                       ['Ottawa','Canada'],\n                       ['Singapore','Thailand'],\n                       ['Taiwan','Taiwan'],\n                       ['San Francisco','US']]\ncountry_date = [['','','']]\nfirst_country_date = True\noutput = []\nentities = []\nfor i in range(0,len(files)):\n    if i%100 == 0:\n        print('completed ', i)\n    with open(files[i]) as f:\n        file_data = json.load(f)\n    paper_id = file_data[\"paper_id\"]\n    for o in file_data[\"body_text\"]: \n            doc = nlp2(o[\"text\"],disable=['parser','tagger'])\n            for ent in doc.ents:\n                #if len(ent.text) > 2:\n                if ent.text == 'SARS-CoV':\n                    entities.append((ent.text, ent.label_))\n                    #print(paper_id)\n                    index_paper_id = -1\n                    if paper_id in pmcid:\n                        index_paper_id = pmcid.index(paper_id)\n                        #print('index_pmcid=' + str(index_paper_id))\n                    elif paper_id in pubmed_id:\n                        index_paper_id = pubmed_id.index(paper_id)\n                        #print('index_pubmed_id=' + str(index_paper_id))\n                    elif paper_id in sha:\n                        index_paper_id = sha.index(paper_id)\n                        #print('index_sha=' + str(index_paper_id))\n                    if index_paper_id >= 0:\n                        for roi in regions_of_interest:\n                            region = roi[0]\n                            country = roi[1]\n                            if (region in title[index_paper_id]) or (region in abstract[index_paper_id]) or (region in ent.text):\n                                #cases_time_ner.write(\"%s,%s,1,0,,,0,\\n\" % (country,publish_time[index_paper_id]))\n                                if first_country_date:\n                                    country_date[0][0] = country\n                                    country_date[0][1] = publish_time[index_paper_id]\n                                    country_date[0][2] = cord_uid[index_paper_id]\n                                    first_country_date = False\n                                else:\n                                    country_date.append([country,publish_time[index_paper_id],cord_uid[index_paper_id]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cases_time_ner_fieldnames='Country_Region,Last_Update,Publications,CORD_UID,Recovered,Active,Delta_Confirmed,Delta_Recovered'\n#cases_time_ner = open('/tf/CORD-19_20200410/cases_time_ner.csv', 'w')\ncases_time_ner = open('/kaggle/working/cases_time_ner_300.csv', 'w')\ncases_time_ner.write(\"%s\\n\" % cases_time_ner_fieldnames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from operator import itemgetter\ndates = []\ncountries = []\ncountry_count = []\ndate_country_count = [['','',0,'']]\ncd_sorted_by_country = sorted(country_date, key=itemgetter(0))\ncd_sorted_by_date = sorted(country_date, key=itemgetter(1))\nfirst_date_country_count = True\nfor sorted_by_country in cd_sorted_by_country:\n    country = sorted_by_country[0]\n    if country not in countries:\n        countries.append(country)\n        country_count.append(0)\n\ncounter = 0\nprevious_date = ''\nfor sorted_by_date in cd_sorted_by_date:\n    date = sorted_by_date[1]\n    date_country = sorted_by_date[0]\n    date_cord_uid = sorted_by_date[2]\n    if date not in dates:\n        dates.append(date)\n    for country in countries:\n        if previous_date != date:\n            if first_date_country_count:\n                date_country_count[0][0] = date\n                date_country_count[0][1] = country\n                date_country_count[0][2] = country_count[countries.index(country)]\n                date_country_count[0][3] = date_cord_uid\n                first_date_country_count = False\n            else:\n                date_country_count.append([date,country,country_count[countries.index(country)],date_cord_uid])\n            if country == date_country:\n                country_count[countries.index(country)] += 1\n                date_country_count[counter][2] = country_count[countries.index(country)]\n            counter += 1\n        elif country == date_country:\n            country_count[countries.index(country)] += 1\n            counter_index = counter - len(countries) + countries.index(country)\n            date_country_count[counter_index][2] = country_count[countries.index(country)]\n    previous_date = date\n            \ncounter = 0\nfor date in dates:\n    for country in countries:\n        publications = date_country_count[counter][2]\n        cord_uid = date_country_count[counter][3]\n        cases_time_ner.write(\"%s,%s,%d,%s,,,0,\\n\" % (country,date,publications,cord_uid))\n        counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cases_time_ner.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read from CORD-19 Temporal Geotagger Dataset: cases_time_ner.csv"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#df_table = pd.read_csv(\"/tf/CORD-19_20200410/cases_time_ner.csv\",parse_dates=['Last_Update'])\ndf_table = pd.read_csv(\"/kaggle/input/cord19-temporal-geotagger-dataset/cases_time_ner.csv\",parse_dates=['Last_Update'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SARS-CoV: Progression of Spread Publication Coverage"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_data = df_table.groupby(['Last_Update', 'Country_Region'])['Publications', 'CORD_UID'].max().reset_index()\ndf_data[\"Last_Update\"] = pd.to_datetime( df_data[\"Last_Update\"]).dt.strftime('%m/%d/%Y')\n\nfig = px.scatter_geo(df_data,\n                     locations              = \"Country_Region\",\n                     locationmode           = 'country names', \n                     color                  = np.power(df_data[\"Publications\"],0.3)-2,\n                     size                   = np.power(df_data[\"Publications\"]+1,0.3)-1,\n                     hover_name             = \"Country_Region\",\n                     hover_data             = [\"Publications\",\"CORD_UID\"],\n                     range_color            = [0, max(np.power(df_data[\"Publications\"],0.3))], \n                     projection             = \"natural earth\",\n                     animation_frame        = \"Last_Update\", \n                     color_continuous_scale = px.colors.sequential.Plasma,\n                     title                  = 'SARS-CoV: Progression of Spread Publication Coverage'\n                    )\nfig.update_coloraxes(colorscale=\"hot\")\nfig.update(layout_coloraxis_showscale=False)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CORD-19 Temporal Geotagger &  Analysis of Risk Factors"},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in range(15):\n    slide = '/kaggle/input/cord19-temporal-geotagger-dataset/CORD-19_Temporal_Geotagger_and_Analysis_of_Risk_Factors_20-0800/Slide' + str(n+1) + '.JPG'\n    plt.figure(figsize=(12,9))\n    plt.axis('off')\n    img=mpimg.imread(slide)\n    imgplot = plt.imshow(img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"nbdime-conflicts":{"local_diff":[{"key":"language_info","op":"add","value":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}}],"remote_diff":[{"key":"language_info","op":"add","value":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}}]}},"nbformat":4,"nbformat_minor":4}