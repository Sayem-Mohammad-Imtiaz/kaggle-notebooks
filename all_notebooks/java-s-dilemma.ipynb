{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let us begin with a dilemma...\n\n> Java has a startup, but lately he finds that his customers are leaving the services he provides. So he call's us to help him. As a data scientist, we need to look into data about his customers and find out which customers are likely to leave.\n\nLet us go about this task\n\n# What we will go through in this notebook:\n* [First Steps - Preliminary work](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#First-Steps)\n* [Data Preprocessing](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Data-Preprocessing)\n* [Checking for Missing Values](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Checking-for-Missing-Values(NaN))\n* [Visualization of Data](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Data-Visualization)\n* [Converting Non numeric features to numeric features](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Converting-non-numeric-features-to-numeric-features)\n* [Oversampling](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Generating-new-data-by-oversampling)\n* [Scaling](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Scaling)\n* [Various Models](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Models)\n* [Hyperparameter Tuning](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Hyperparameter-Tuning)\n* [Scope For Improvement](https://www.kaggle.com/duttasd28/java-s-dilemma?scriptVersionId=50296668#Scope-for-improvement)","metadata":{}},{"cell_type":"markdown","source":"# First Steps","metadata":{}},{"cell_type":"code","source":"# Import the data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tabulate import tabulate\nnp.random.seed(0)\n#==========================================================================\n#==========================================================================\ndata = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv',\n                   index_col = 'RowNumber')\ndata = data.sample(5000)\ndata.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, **Exited** is our dependent feature. Other columns are independent features\n\nLet us check how many values of __Exited__ columns are there so that we can figure out if there is class imbalance or not","metadata":{}},{"cell_type":"code","source":"plt.figure(1, dpi=100)\nvalues = data['Exited'].values\n# Analysis\nplt.text(\n    x=0.2,\n    y=7.0,\n    s = \"80%\",\n    fontsize=44,\n    c='#ff8c00'\n)\n# text\nplt.text(\n    x=0.2,\n    y=6.0,\n    s = \"of data points are non churn category\\npoints, suggesting imbalance\",\n    c=\"gray\"\n)\n# Hist\nplt.hist(\n    values,\n    density=True,\n    color='gray'\n)\n# 0 label\nplt.annotate(\n    s = \"0\",\n    xy = (0.05, 7),\n    fontsize=12,\n    c='white'\n)\n# 1 label\nplt.annotate(\n    s = \"1\",\n    xy = (0.95, 1.5),\n    fontsize = 12,\n    c='white'\n)\nplt.box(on=None)\nplt.xlabel('Customer Distribution')\n\nplt.yticks([])\nplt.xticks([])\nplt.title('Need to implement Data Imbalance Measures')\nplt.show();","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\nIn this step, we are going to preprocess our data so that we can use it on our models.\n\nPreprocessing involves the following:\n* Checking for NaN values that is missing values in the data\n* Visualise the data so that we can derive meaningful insights\n* Split to training and test datasets\n* Fill in NaN Values\n* Convert non numeric features to numeric features so that we can do predictions\n* Scale the data \n\nLet us go ahead with the first step, __checking for NaN/missing values__","metadata":{}},{"cell_type":"markdown","source":"# Checking for Missing Values(NaN)","metadata":{}},{"cell_type":"code","source":"# check for missing values\ndata.isnull().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Phew! We are lucky we did not get any null values. \nUsually there are null values in the dataset and we need to remove them.\n\nUsually, There are various techniques to handle missing values. \n\n[This awesome notebook by Kaggle Grandmaster Parul Pandey](https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python) helped me learn a lot. Do check out if you like it!","metadata":{}},{"cell_type":"markdown","source":"# Data Visualization\nHere we are going to plot graphs regarding the data to get a deeper insight.","metadata":{}},{"cell_type":"code","source":"# Import necessary plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make figures inline\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us get a list of columns in the data so that we can predict better. \nWe use the .info() method to get the datatypes too","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Geography, Gender, Surname** are object data-types, while others are either int / float.","metadata":{}},{"cell_type":"markdown","source":"# Plotting with Matplotlib and Seaborn","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nsns.set()\nsns.boxplot(y = 'CreditScore', x = 'Exited', data = data, palette = 'husl');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.violinplot(y = 'Exited' , x = 'Gender' , data = data, kind='boxen', palette = 'hot');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.countplot(x = 'Geography' , data = data);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us plot a heatmap of the correlations of the features with each other. That will help us discard non useful features.\nIt also gives us some idea as to what features predict dependent column best","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.set(style = 'white')\nsns.heatmap(data.select_dtypes(include='number').corr(), annot = True, cmap = 'magma', square = True);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pairplot - This plots graphs between every two variables. This is useful for visualisation","metadata":{}},{"cell_type":"code","source":"# Pairplot\ndata_random_sample = data.sample(frac = 0.4).reset_index()\n\nplt.figure(figsize=(12, 8))\nsns.pairplot(data_random_sample, corner = True, hue = 'Exited');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting non numeric features to numeric features\nWe convert non numeric features to numeric features.\nAlso we drop columns which do not seem to contribute anything useful like **CustomerId**, **Surname**.\n\nBut first we will split the dataset into train and test dataset.","metadata":{}},{"cell_type":"code","source":"# Drop a useless feature\ndata.drop(['CustomerId', 'Surname'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get dependent and independent features\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1].astype('float')\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting to train test dataset\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 1)\nlen(y_train), len(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reset the indexes of the splitted data frames\nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols = [col for col in X_train.columns if X_train[col].dtypes == object]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encoder object\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Create two empty data frames\nX_train_categorical, X_val_categorical = pd.DataFrame(), pd.DataFrame()\n\n# Label Encode the features\nfor col in categorical_cols:\n    X_train_categorical[col] = label_encoder.fit_transform(X_train[col])\n    X_val_categorical[col] = label_encoder.transform(X_val[col])\n\n# Drop the non required columns\nX_train.drop(categorical_cols, axis = 1, inplace = True)\nX_val.drop(categorical_cols, axis = 1, inplace=True)\n\n# put new colums in dataframe\nX_train = X_train.join(X_train_categorical)\nX_val = X_val.join(X_val_categorical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating new data by oversampling\nSince we have an imbalanced dataset, we will increase the number of samples by SMOTE technique","metadata":{}},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\nsmk = SMOTETomek()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_val, y_val = smk.fit_sample(X_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final check at the dataset before putting in model\nNow we take a final look at the dataset","metadata":{}},{"cell_type":"code","source":"X_train.shape, X_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling\nWe scale the data so that datapoints are on the same level\n\n### Note: we have labelled data, so we should not scale all the data.Otherwise meaning will be lost","metadata":{}},{"cell_type":"code","source":"columns = ['Balance', 'EstimatedSalary']  ## Columns to modify\n\n## Subtract the mean, divide by standard deviation.\nfor col in columns:\n    colMean = X_train[col].mean()\n    colStdDev = X_train[col].std()\n    X_train[col] = X_train[col].apply(lambda x : (x - colMean) / colStdDev)\n    X_val[col] = X_val[col].apply(lambda x : (x - colMean) / colStdDev)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\nWe will be using the following models \n* Logistic Regression\n* Decision Tree\n* Random Forest Classifier\n* Extra Trees Classifier\n* XGBClassifier\n* ANN","metadata":{}},{"cell_type":"code","source":"# metric\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = 'lbfgs', max_iter = 300)\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_preds, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_preds, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(class_weight='balanced')\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extra Trees Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\n\n# fit the data\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_preds = model.predict(X_val)\n\n# Get score\nf1_score(y_val, y_preds)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network(TensorFlow)","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras as K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = K.Sequential()\n\nmodel.add(K.layers.Dense(512, input_dim = 10, activation = 'relu'))\n\nmodel.add(K.layers.Dense(256, activation = 'relu'))\nmodel.add(K.layers.BatchNormalization())\n\nmodel.add(K.layers.Dense(64, activation = 'relu'))\nmodel.add(K.layers.Dropout(0.4))\n\nmodel.add(K.layers.Dense(8, activation = 'relu'))\nmodel.add(K.layers.BatchNormalization())\nmodel.add(K.layers.Dense(1, activation = 'sigmoid'))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = K.optimizers.Adam(learning_rate=0.00001)\n\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=(X_val, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = model.predict_classes(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_val, y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\nLet us tune hyperparameters of XGBoost to further improve our results.\n\nWe will be using RandomisedSearchCV for this. This searches randomly through a search space and gets the best parameters\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nmodel = XGBClassifier()  ## Model to tune\n\n# define a parameters dictionary, which contains the search space to see\nparamSearchSpace = {\n    'n_estimators' : [10, 25, 70],  ## Number of trees\n    'gamma' : [1, 0.05, 0.1],    ## Regularisation parameter\n    'max_depth' : [2, 3, 5, 7],    ## max depth of tree\n    'scale_pos_weight' : [60, 70, 80] # Num pos / num Neg\n}\n\n# make Grid Search CV object\nclf = RandomizedSearchCV(model, param_distributions=  paramSearchSpace)\n\n# Fit with data\nclf.fit(X_train, y_train)\n\n# See the best values we obtain\nclf.best_params_, clf.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Randomized Search CV takes time!! Please wait!","metadata":{}},{"cell_type":"code","source":"finalModel = XGBClassifier(**clf.best_params_)\nfinalModel.fit(X_train, y_train)\ny_preds = finalModel.predict(X_val)\n\n## Final f1 score\nf1_score(y_preds, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving Best Model","metadata":{}},{"cell_type":"code","source":"import pickle\n# Dump the model\npickle.dump(finalModel, open('ChurnModelFinal.pkl', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scope for improvement\n\n* Better Hyperparameter Tuning\n* Change optimizer for model\n\n## Thank you!","metadata":{}}]}