{"cells":[{"metadata":{},"cell_type":"markdown","source":"variational autoencoders are types for networks that can be used to generate datapoints that don't exist in the dataset  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![VAE](https://miro.medium.com/max/3374/1*22cSCfmktNIwH5m__u2ffA.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"it's a neural network that learns to encode an image into a mean and std (or the log of that)  and during the training we sample from those mean and std to regenerate the same image \nbut in the generation part for data not in the dataset we could just sample from a random multivariate normal ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"this playlist can give some intuition about them but the topic needs more  research\n\nhttps://www.youtube.com/playlist?list=PLdxQ7SoCLQANizknbIiHzL_hYjEaI-wUe","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here I used  pytorch to build a model(a weak one) that could generate faces with specific features like hair color , gender , beard , specific skin ","execution_count":null},{"metadata":{"trusted":true,"id":"uvokIGt2cZNx","outputId":"20f88dc7-03a1-4c26-cdcc-cbbf9572e17f"},"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nimport torch.optim as  optim \nif torch.cuda.is_available():  \n  dev = \"cuda:0\" \n  print(\"gpu up\")\nelse:  \n  dev = \"cpu\"  \ndevice = torch.device(dev)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"gFpm3es0cZN6"},"cell_type":"code","source":"df = pd.read_csv(\"../input/celeba-dataset/list_attr_celeba.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"PK2VcrE3cZOV"},"cell_type":"code","source":"def haircolor(x):\n    if x[\"Blond_Hair\"] == 1:\n        return 0\n    elif x[\"Brown_Hair\"] == 1:\n        return 1\n    elif x[\"Black_Hair\"] == 1:\n        return 2\n    else :\n        return 3\n    \ndf[\"Hair_Color\"] = df.apply(haircolor,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"JWNP5HVucZOb"},"cell_type":"code","source":"\"\"\"\nfor simplicity I decided to make the VAE capture just for attributes \n\nHair Color (blond,brown,black and neither of these(or unknown))\nPale Skin \nGender \nBeard (in case of male)\n\"\"\"\n\ndf = df[[\"image_id\",\"Hair_Color\",'Pale_Skin',\"Male\",\"No_Beard\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"tV-bCS_FcZOx"},"cell_type":"code","source":"\"\"\"\nreplacing -1 with 0 without affecting the the 1 values \n\n\"\"\"\ndf.Pale_Skin = df.Pale_Skin.apply(lambda x: max(x,0)) \ndf.Male = df.Male.apply(lambda x: max(x,0))\ndf.No_Beard = df.No_Beard.apply(lambda x: max(x,0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"w31LTAHwcZPY"},"cell_type":"code","source":"faces =[]\nfor i in df.iloc[:20000].image_id:\n    pic = Image.open(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/\"+i)\n    pic = pic.resize((64,64))\n    pix = np.array(pic.getdata()).reshape(pic.size[0], pic.size[1], 3)\n    pix = pix/255\n    \"\"\"\n    for the images I had to use np.moveaxis to change the shape from  (64,64,3) to (3,64,64)\n    without messing up the image \n    \n    \"\"\"\n    faces.append(np.moveaxis(pix,-1,0).tolist())\n    \nfaces = np.array(faces)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"n7G5F_qEcZPi","outputId":"6b5f0243-8c40-490b-b17c-faea06bc155e"},"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder,self).__init__()\n        # channels_in ,  channels_out, kernel_size, stride , padding,\n        self.conv1 = nn.Conv2d(3,64,3,1,1)\n        self.conv2 = nn.Conv2d(64,64,3,1,1)\n        self.conv3 = nn.Conv2d(64,64,4,2,1)\n        self.conv4 = nn.Conv2d(64,128,4,2,1)\n        self.maxp1 = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.maxp2 = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.maxp3 = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.maxp4 = nn.MaxPool2d(kernel_size=2,stride=2)\n\n\n        \n    def forward(self,x):\n        out = self.conv1(x)\n        out = self.maxp1(out)\n        out = F.relu(out) \n        out = self.conv2(out)\n        out = self.maxp2(out)\n        out = F.relu(out)\n        out = self.conv3(out)\n        out = self.maxp3(out)\n        out = F.relu(out)\n        out= self.conv4(out)\n        out = self.maxp4(out)\n        out = F.relu(out)\n        return out.view(out.shape[0],-1)\n    \nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder,self).__init__()\n        # channels_in ,  channels_out, kernel_size, stride , padding,\n        \"\"\"\n        convtranspose  is different from the regular conv layer (looking at the equations of two of them )\n        this link shows examples about it \n        https://towardsdatascience.com/is-the-transposed-convolution-layer-and-convolution-layer-the-same-thing-8655b751c3a1\n        \n        \"\"\"\n        self.transconv1 = nn.ConvTranspose2d(64+ 40,64,8,4,2)\n        self.transconv2 = nn.ConvTranspose2d(64,64,8,4,2)\n        self.transconv3 = nn.ConvTranspose2d(64,64,4,2,1)\n        self.transconv4 = nn.ConvTranspose2d(64,3,4,2,1)\n        \n        \"\"\"\n        I think Embeddings layers were pretty good  so every state was converted into a vector like \n        Beard or no beard -> vector of 10 weights\n        Male or Female -> vector  of 10 weights \n        \n        \"\"\"\n        self.hairEmbedding = nn.Embedding(4,10)\n        self.beardEmbedding = nn.Embedding(2,10)\n        self.genderEmbedding = nn.Embedding(2,10)\n        self.paleSkinEmbedding = nn.Embedding(2,10)\n\n        \n    def forward(self,x):\n        z = x[:,:64]\n        hair = self.hairEmbedding(x[:,64].long())\n        paleSkin = self.paleSkinEmbedding(x[:,65].long())\n        gender = self.genderEmbedding(x[:,66].long())\n        beard = self.beardEmbedding(x[:,67].long())\n        \"\"\"\n        Concating the embeddings and the encoded image\n        \"\"\"\n        z = torch.cat([z,hair,beard,gender,paleSkin],dim=1)\n        \n        out= self.transconv1(z.view(z.shape[0],z.shape[1],1,1))\n        out = F.relu(out)\n        out= self.transconv2(out)\n        out = F.relu(out)\n\n        out= self.transconv3(out)\n        out = F.relu(out)\n\n        out= self.transconv4(out)\n        out = F.relu(out)\n\n        return out\n        \nclass CVAE(nn.Module):\n    def __init__(self,encoder,decoder):\n        super(CVAE,self).__init__()\n        self.encoder = encoder()\n        self.decoder = decoder()\n    def forward(self,x,attrs):\n        h = self.encoder(x)\n        \n        mu = h[:,:64]\n        logvar = h[:,64:]\n        # this part is for the reparameterization trick\n        s= torch.exp(logvar)\n        eps = torch.randn_like(s)\n        z = s*eps + mu \n        \n        z= torch.cat([z,attrs],dim=1)\n        out = self.decoder(z)\n        return out,mu,logvar\n        \nvae = CVAE(Encoder,Decoder)\nvae.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"LJ9v0Kj7cZP_"},"cell_type":"code","source":"def ceil(a,b):\n    return -(-a//b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"LIfB7jPTcZQG","outputId":"fd3eb022-3a81-4814-9815-9fa183328dbe"},"cell_type":"code","source":"\n\"\"\"\n\nloss function contains two parts \nreconstruction  loss and kullback leibler divergence (it basically measures how two distributions are different)\n\n\"\"\"\ndef loss_function(recon_x,x,mu,logvar):\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    KLD /= x.shape[0] * 3 * 64 * 64\n    recon_loss = F.mse_loss(recon_x,x)\n    return recon_loss +  KLD\n\nepochs = 1201\n\nbatch_size= 256\noptimizer = optim.Adagrad(vae.parameters(),lr = 0.001)\n\nnpData = df.iloc[:20000].to_numpy()\n\n\nn_samples = len(npData)\nbetter_batch_size = ceil(n_samples, ceil(n_samples, batch_size))\nlosstrack = []\nfor e in range(epochs):\n    losses = []\n    for i in range(ceil(n_samples, better_batch_size)):\n        batch = npData[i * better_batch_size: (i+1) * better_batch_size]\n        attrs = torch.Tensor(batch[:,1:].astype('float16')).to(device)\n#         break\n        imgs = faces[i * better_batch_size: (i+1) * better_batch_size]\n        imgs = torch.Tensor(imgs.astype('float16')).to(device)\n        vae.zero_grad()\n        recon_imgs,mu,logvar = vae(imgs,attrs)\n        err = loss_function(recon_imgs,imgs,mu,logvar)\n        err.backward()\n        optimizer.step()\n        losses.append(err.item())\n    losstrack.append(np.mean(losses))\n    if e % 100 == 0: \n        torch.save(vae.state_dict(), \"./vae.pt\")\n        print(np.mean(losses), \"mean loss\", e)\n\n        \n    \n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"I just loaded the model I saved while training\"\"\"\n\ncheckpoint = torch.load(\"../input/my-cvae-model/vae.pt\")\nvae.load_state_dict(checkpoint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"O3eJt5-XcZQ7"},"cell_type":"code","source":"def showImage(x):\n    attrs = torch.Tensor(x)\n    h = torch.cat((torch.randn(1,64),attrs),dim=1).to(device)\n    img = vae.decoder(h)\n    img = img.cpu().detach().numpy().reshape(3,64,64)\n    img = np.moveaxis(img,0,-1)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"id":"GvmW09CzVDy0","outputId":"c3ee412e-9611-402b-b957-c423237cf3ca","_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"f, axarr = plt.subplots(1,2)\naxarr[0].imshow(showImage([[2,0,0,1]]))\naxarr[0].set_title(\"Woman\")\naxarr[1].imshow(showImage([[2,0,1,0]]))\naxarr[1].set_title(\"Man\")\n\nf.subplots_adjust(hspace=0.3,left=2,right=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here showing what could the model do after the training \n\nit could kinda generate different genders","execution_count":null},{"metadata":{"trusted":true,"id":"NqIGvz1ccZRE","outputId":"9f190830-cb22-40ae-9ccb-037445ee47dc"},"cell_type":"code","source":"f, axarr = plt.subplots(1,3)\naxarr[0].imshow(showImage([[2,0,0,1]]))\naxarr[0].set_title(\"Black hair\")\naxarr[1].imshow(showImage([[1,0,0,1]]))\naxarr[1].set_title(\"brown hair\")\naxarr[2].imshow(showImage([[0,0,0,1]]))\naxarr[2].set_title(\"blond hair\")\n\n\nf.subplots_adjust(hspace=0.3,left=2,right=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it also can make different hair colors but it mostly was trying to just fill the image outside with the color","execution_count":null},{"metadata":{"trusted":true,"id":"_mykTq2PcZRL","outputId":"cb3147a8-f573-4af1-8836-812f754e8ad3"},"cell_type":"code","source":"f, axarr = plt.subplots(1,2)\naxarr[0].imshow(showImage([[2,0,0,1]]))\naxarr[0].set_title(\"not Pale skin\")\naxarr[1].imshow(showImage([[2,1,0,1]]))\naxarr[1].set_title(\"Pale Skin\")\n\nf.subplots_adjust(hspace=0.3,left=2,right=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it could get doing the skin part correctly ","execution_count":null},{"metadata":{"trusted":true,"id":"v63aPK-rcZRZ","outputId":"17ad6645-68c2-4c91-85c4-8d435ac1b54f"},"cell_type":"code","source":"f, axarr = plt.subplots(1,2)\naxarr[0].imshow(showImage([[2,0,1,1]]))\naxarr[0].set_title(\"No Beard\")\naxarr[1].imshow(showImage([[2,0,1,0]]))\naxarr[1].set_title(\"Beard\")\n\nf.subplots_adjust(hspace=0.3,left=2,right=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"well this one is not good maybe the model needs to be bigger(and also more data)  because there is barely a difference between the two images ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"upvote if you find this kernel useful","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}