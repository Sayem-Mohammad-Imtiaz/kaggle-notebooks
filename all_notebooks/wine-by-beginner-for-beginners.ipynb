{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a classification problem\n### we will have to find the quality of wine given the various independent features","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xbg\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/beginner-datasets/beginner_datasets/wine.csv\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"df.info()\n\n# looks like we dont have any null values \n# (sometimes some other values can be used instead of null (eg:- -999 or -1), so we'll use df.describe to check for such values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()\n\n# looks like we're good but we notice their are some values above the 75th percentile\n# these might be outliers, so we'll check them","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets do a box plot to check for outliers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\ndf.iloc[:,:5].boxplot();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use boxplot to see the outliers\nfor col in df.iloc[:,:-1].columns:\n    sns.boxplot(x=df[col])\n    plt.show()\n    \n# some of the first columns have a lot of outliers...\n# for a classification problem we might not have to worry about them as we can use random forest which is unaffected by ouliers\n\n# but ofcourse if we try to use logistic regression then we might need to do something about that...","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check their distributions\n\nfor col in df.iloc[:,:-1].columns:\n    sns.distplot(df[col])\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These distributions are actually quiet good\n# we can apply a log transform to level them up a little bit\n\n# lets try\n\nfor col in df.iloc[:,:-1].columns:\n    sns.distplot(np.log1p(df[col]))     # log1p is good because log(0) is infinity so log1p adds 1 to every value\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are other transformations but, i currently need more knowledge about where all each performs best...\n# log and boxcox have a good name...","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature engineering","metadata":{}},{"cell_type":"code","source":"X_log = df.drop(['quality'], axis=1).copy()\ny = df['quality']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in X_log.iloc[:,:-1]:\n    X_log[col] = np.log1p(X_log[col])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now lets use label encoder on the last column 'type'\n\nencoder = LabelEncoder()\nX_log['type'] = encoder.fit_transform(X_log['type'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets standardize it as well...","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X_log), columns=X_log.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature selection","metadata":{}},{"cell_type":"code","source":"data = pd.concat([X_scaled,y],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor = data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(cor, annot=True, cmap='Greens');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Their aren't that many correlations for quality...\n\n### But for \"type\" there is a lot of correlation from independent features (thinking of taking it instead, maybe in the end)","metadata":{}},{"cell_type":"code","source":"### Lets use a sklearn library","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\nmutual_info_vals = mutual_info_classif(X_scaled,y)\nmutual_val_df = pd.DataFrame({\"vals\":mutual_info_vals},index=X_scaled.columns) # we're keeping the passenger id\nplt.figure(figsize=(10,5))\nmutual_val_df.vals.sort_values(ascending=False).plot(kind='bar');\n\n\n### this shows us the mutual information (infomation gain)\n\n## lets try one which calculates linear relationship","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import f_classif\n\nf_vals,_ = f_classif(X_scaled, y)\nf_vals_df = pd.DataFrame({\"vals\":f_vals},index=X_scaled.columns) # we're keeping the passenger id\nplt.figure(figsize=(10,5))\nf_vals_df.vals.sort_values(ascending=False).plot(kind='bar');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mut_feat = mutual_val_df.vals.sort_values(ascending=False)[:7].index\nlin_feat = f_vals_df.vals.sort_values(ascending=False)[:5].index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model building","metadata":{}},{"cell_type":"code","source":"models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier() ,xbg.XGBRFClassifier() ,SVC()]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_cv = []\nfor model in models:\n    result_cv.append((model,cross_val_score(model,X_scaled[mut_feat],y,cv=5)));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(result_cv)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I've seen in towardsdatascence post (by Terence shin ) that you can divide the wine category into 2 ( that is <7 is bad, >7 is good)\n### we'll try that\n### he also made subsets of data based on good and bad quality and saw how do they vary? ( use dot describe on subsets to see the means, max, mins of independent features... that was good)","metadata":{}},{"cell_type":"code","source":"X_scaled[mut_feat]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_easy = np.where(y>7,1,0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_cv = []\nfor model in models:\n    result_cv_easy.append(cross_val_score(model,X_scaled[mut_feat],y_easy,cv=5));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets just compute the average score\n\nfor vals in result_cv_easy:\n    print(np.mean(vals))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# logistic and svc have preformed the best...","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remember we had talked about the using the dataset to predict the wine type?\n### let's go... we'll use the x_log version","metadata":{}},{"cell_type":"code","source":"y_type = X_log['type']\nX_type = pd.concat([ X_log.drop(['type'], axis=1), y], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_cv_type = []\nfor model in models:\n    result_cv_type.append(cross_val_score(model,X_scaled,y_easy,cv=5));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_cv_type","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for vals in result_cv_type:\n    print(np.mean(vals))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Further we would use some metrics to check the performance and so hyperparameter tuning...","metadata":{}},{"cell_type":"code","source":"# im just gonna be using it on good/bad wine prediction\n# we'll go with logistic regression\n# we will need y_predicted and y_test so lets perform train_test_split on it...\n\nX_train,X_test,y_train,y_test = train_test_split(X_scaled[mut_feat] ,y_easy, test_size = 0.3, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_clf = LogisticRegression()\nlog_clf.fit(X_train,y_train)\nlog_clf.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = log_clf.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,y_pred)\n\n# now, the top right is false positive which means all good wines were acurately predicted...\n# the bottom left one is the false negative which are the negative values that were falsely predicted...\n\n### yeah... it classified everything as good and we had less bad wine in test set (50) so our accuracy was high...\n\n### lets check other's","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models:\n    model.fit(X_train,y_train)\n    print(confusion_matrix(y_test,y_pred),end='\\n\\n')\n    \n# all are bad models...","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We can use sampling techniques on this to make it predict both good and bad wines but this is my first notebook so like...           \"Thank you\"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}