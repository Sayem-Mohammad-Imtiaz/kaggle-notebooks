{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing necessary libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\nimport glob\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading list_attr_celeba.csv and setting image id as index for easier access when loading images","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('/kaggle/input/celeba-dataset/list_attr_celeba.csv')\ndata.set_index('image_id',inplace = True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking only Male column as only it is useful for our project","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gender = data.Male","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training and test arrays. In the training and test arrays we store images and in the label arrays we store labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train = []\nraw_train_labels = []\nraw_test = []\nraw_test_labels = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function for loading images\n\n* Here data is the array of images which are laoded\n* no of images is the no of images we load from the dataset\n* labels are labels for each image respectively\n* inp and outp are the partition in which take take data from. Here for training we take images in between 0 50000 images in files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_images(data,no_of_images,labels,inp,outp):\n    path = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba'\n    files = os.listdir(path)\n    mcount, fcount = 0, 0\n    for i in files[inp:outp]:\n        if gender[i] == 1:\n            mcount = mcount + 1\n            if mcount == no_of_images:\n                continue\n        elif gender[i] == -1:\n            gender[i] = 0\n            fcount = fcount + 1\n            if fcount == no_of_images:\n                continue\n        img = cv2.imread(os.path.join(path,i))\n        data.append(img)\n        labels.append(gender[i])\n        if len(data) == 2 * no_of_images:\n            return data, labels\n        if len(data) % 100 == 0:\n            print(len(data),'images')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the images and labels in raw_train, raw_train_labels. We need to process them before use so the name.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train,raw_train_labels = get_images(raw_train,2500,raw_train_labels,0,50000)\nraw_test,raw_test_labels = get_images(raw_test,500,raw_test_labels,50000,100000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking their length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(raw_train),len(raw_train_labels),len(raw_test),len(raw_test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the arrays to tensorflow datasets, as in this format we have various good methods we can use to handle data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = tf.data.Dataset.from_tensor_slices((raw_train,raw_train_labels))\ntest_data = tf.data.Dataset.from_tensor_slices((raw_test,raw_test_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Formatting the images\n\n* here we are casting image to float\n* standarding the values between 0 and 1\n* resizing the image to (160,160) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 160\ndef format_example(image,label):\n    image = tf.cast(image,dtype = tf.float32)\n    image = (image / 255) - 1\n    image = tf.image.resize(image,(IMG_SIZE,IMG_SIZE))\n    return image, label\ntrain_data = train_data.map(format_example)\ntest_data = test_data.map(format_example)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here with the help of methods in tf.data.datasets\n\n* we are shuffing the data\n* we making batches out of them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 2000\ntrain_data = train_data.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\ntest_data = test_data.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are using transfer learning, MobileNetV2 as our base model. \n\nwe pass include top = False as we do not use its last layers which is used to classfy on 1000 classes. We set trainable = False as we donot train the base model because we are using less data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SHAPE = (IMG_SIZE,IMG_SIZE,3)\nbase_model = tf.keras.applications.MobileNetV2(input_shape = IMG_SHAPE,include_top = False,weights = 'imagenet')\nbase_model.trainable = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the shape of output shape of the image batches in training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_batch, label_batch in train_data.take(1):\n    pass\nprint(image_batch.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we check the output shape of the base model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(base_model(image_batch).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add global average pooling layer after the base model, to average its output and make the output feedable to dense layers, and check whether it is working properly by checking its output shape.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nprint(global_average_layer(base_model(image_batch)).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add a 256 neuron as our first hidden layer and dropout layer with 0.5 dropout. Check their output.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_layer1 = tf.keras.layers.Dense(256,activation = 'relu')\ndropout1 = tf.keras.layers.Dropout(0.5)\nprint(dropout1(hidden_layer1(global_average_layer(base_model(image_batch)))).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add a 128 neuron hidden layer as our second layer and Check its output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_layer2 = tf.keras.layers.Dense(128,activation = 'relu')\nprint(hidden_layer2(dropout1(hidden_layer1(global_average_layer(base_model(image_batch))))).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add a final prediction layer with 1 neuron with sigmoid activation as output layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_layer = tf.keras.layers.Dense(1)\nprint(prediction_layer(hidden_layer2(dropout1(hidden_layer1(global_average_layer(base_model(image_batch)))))).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now as all these layers are working properly we make a sequential model out of them and name it model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n     base_model,\n     global_average_layer,\n     hidden_layer1,\n     dropout1,\n     hidden_layer2,\n     prediction_layer\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use learning rate 0.0001 and rmsprop as optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_learning_rate = 0.0001\nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(lr = base_learning_rate),\n              loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the summary of our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's calculate steps per epoch and fix validation steps.Let's also how our model performs before training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train = 10000\nnum_test = 2500\ninitial_epochs = 20\nsteps_per_epochs = round(num_train) //  BATCH_SIZE\nvalidation_steps = 4\n\nloss0, accuracy0 = model.evaluate(test_data, steps = validation_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add callbacks to save best model weights and load them to evaluate on test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = tf.keras.callbacks.ModelCheckpoint('/kaggle/working/best_model.h5',save_best_only = True,monitor = 'val_accuracy',mode = max,verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_data,epochs = initial_epochs, validation_data = test_data,callbacks = [callbacks])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('best_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we got 86.5% percent accuracy on the test set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"so, that is it. We simply got 87% accuracy approximately. This is power of transfer learning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you have liked the kernel, Please give upvote.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}