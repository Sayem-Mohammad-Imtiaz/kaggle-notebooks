{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Text Preprocessing\nimport re\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# Text Visualisation\nfrom wordcloud import WordCloud\nfrom nltk import FreqDist\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# LinReg\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Misc\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Utils Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(df):\n    \"\"\"\n    This function updates the all the dataframe. This function is rather specific. It helps when you want to do a global \n    update on all the dataframes.\n    \n    Parameters\n    ----------\n    df: pandas DataFrame\n    This specifies the dataframe to be updated.\n    \n    Returns\n    -------\n    df: pandas DataFrame\n    Updated main dataframe.\n    \n    object_df: pandas DataFrame\n    Updated object dataframe.\n    \n    numeric_df: pandas DataFrame\n    Updated numeric dataframe.\n    \n    Notes\n    -----\n    Please check the datatypes of the df before using this function to update the dataframe.\n    \"\"\"\n    object_df = df.select_dtypes(\"object\")\n    numeric_df = df.select_dtypes(['int64', 'float64'])\n    \n    return object_df, numeric_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def uniqueness(col, df):\n    \"\"\"\n    This function prints the unique labels, occurances of unique labels and number of labels in the given categorical feature.\n    \n    Parameters\n    ----------\n    col: string-like\n    This specifies the categorical feature.\n    \n    df: pandas DataFrame\n    This specifies the dataframe.\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    unique = df[col].unique()\n    count = df[col].value_counts()\n    print(\"Unique Labels:\", unique)\n    print(\"Number of Labels:\", len(unique))\n    print(\"Occurances of Labels:\\n\", count, sep='')\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Function to Measure Skew and Kurtosis\ndef skewKurtosis(cols, df):\n    \"\"\"\n    This function analysis the skew and kurtosis of the data.\n    \n    Parameters\n    ----------\n    cols: string-like or array-like\n    This specifies the column(s) to be analysed.\n    \n    df: pandas DataFrame\n    This specifies the DataFrame\n    \n    Returns\n    -------\n    None\n    \n    Skew\n    ----\n    Left-Skew / Right-Modal: Skew < 0 || Median > Mean\n    Normal: Skew = 0 \n    Right-Skew / Left-Modal: Skew > 0 || Median < Mean\n    \n    Kurtosis\n    --------\n    Platykurtic Distribution: Kurtosis < 0\n    Mesokurtic Distribution: Kurtosis = 0\n    Leupokurtic Distribution: Kurtosis > 0\n    \n    \"\"\"\n    if type(cols) == str:\n        skew = df[cols].skew()\n        kurtosis = df[cols].kurtosis()\n        if skew == 0:\n            print(\"Skew: %f. This represents a Normal Skew where Mean = Median.\" % skew)\n        elif skew < 0:\n            print(\"Skew: %f. This represents a Negative Skew / Right Modal / Left-Skew where the Median is greater than Mean.\" % skew)\n        else:\n            print(\"Skew: %f. This represents a Positive Skew / Left Modal / Right-Skew where the Median is less than Mean.\" % skew)\n        \n        if kurtosis == 0:\n            print(\"Kurtosis: %s. This represents a Mesokurtic (Normal) Distribution.\" % kurtosis)\n        elif kurtosis < 0:\n            print(\"Kurtosis: %s. This represents a Platykurtic (Fat) Distribution.\" % kurtosis)\n        else:\n            print(\"Kurtosis: %s. This represents a Leupokurtic (Skinny) Distribution.\" % kurtosis)\n    \n    else:\n        for col in cols:\n            print(col)\n            skew = df[col].skew()\n            kurtosis = df[col].kurtosis()\n            if skew == 0:\n                print(\"Skew: %f. This represents a Normal Skew where Mean = Median.\" % skew)\n            elif skew < 0:\n                print(\"Skew: %f. This represents a Negative Skew / Right Modal / Left-Skew where the Median is greater than Mean.\" % skew)\n            else:\n                print(\"Skew: %f. This represents a Positive Skew / Left Modal / Right-Skew where the Median is less than Mean.\" % skew)\n\n            if kurtosis == 0:\n                print(\"Kurtosis: %s. This represents a Mesokurtic (Normal) Distribution.\" % kurtosis)\n            elif kurtosis < 0:\n                print(\"Kurtosis: %s. This represents a Platykurtic (Fat) Distribution.\" % kurtosis)\n            else:\n                print(\"Kurtosis: %s. This represents a Leupokurtic (Skinny) Distribution.\" % kurtosis)\n            print()\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the Features using LabelEncoder()\ndef encode(col, df):\n    \"\"\"\n    This function encodes the categorical columns.\n    \n    Parameters\n    ----------\n    col: string-like\n    This specifies the column to be encoded.\n    \n    df: pandas DataFrame\n    This specifies the DataFrame t\n    \"\"\"\n    encoder = LabelEncoder()\n    \n    df[col] = encoder.fit_transform(df[col])\n    \n    print(f\"Classes for {col}:\",encoder.classes_)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Function to Compare PMCC\ndef correlation(cols, target, df):\n    \"\"\"\n    This function prints the correlation between column(s) and the target.\n    \n    Parameters\n    ----------\n    cols: string-like or array-like\n    This specifies the columns to test against the target.\n    \n    target: string-like\n    This specifies the column to be tested with columns.\n    \n    df: pandas DataFrame\n    This specifies the DataFrame.\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    if type(cols) == str:\n        if cols != target: \n            print(f\"Correlation between {target} and {cols}:\", df[target].corr(df[cols]))\n    else:\n        for col in cols:\n            if col == target: continue\n            print(f\"Correlation between {target} and {col}:\", df[target].corr(df[col]))\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression Function \n\ndef linreg(X, y, df, testsize=0.25):\n    \"\"\"\n    This function encapsulates the pipeline for using Linear Regression and predictors to predict the SalePrice.\n    \n    Parameters\n    ----------\n    X: array-like \n    This parameter contains the predictors to be used for predicting y.\n    \n    y: string\n    This parameter specifies the output column.\n    \n    df: pandas DataFrame or panda Series.\n    This parameter specifies the data.\n    \n    testsize: float or integer greater than 1.\n    This parameter specifies the train_test_split size.\n    \n    Returns\n    -------\n    coeff: float-like\n    This represents the coefficient of the best fit line (y=aX+c).\n    \n    intercept: float-like\n    This represents the intercept of the best fit line (y=aX+c).\n    \n    pred_train: Array-like\n    This represents the model predictions on X_train. \n    \n    pred_test: Array-like\n    This represents the model predictions on X_test.\n    \n    train_r2: float-like; between 0 and 1\n    This value is the Explain Variance for the training data. It measures the goodness of fit on training data. \n    Explained Variance suggests how much of the data can be explained by the model.\n    \n    test_r2: float-like; between 0 and 1\n    This value is the Explain Variance for the testing data. It measures the goodness of fit on testing data. \n    Explained Variance suggests how much of the data can be explained by the model.\n    \n    train_mse: float-like\n    This value represents the mean square error on training data.\n    \n    \n    test_mse: float-like\n    This value represents the mean square error on testing data.\n    \n    X_train: array-like\n    This represents the input data used in training the model.\n    \n    X_test: array-like\n    This represents the input data used in testing the model.\n    \n    y_train: array-like\n    This represents the output data used in training the model.\n    \n    y_test: array-like\n    This represents the output data used in testing the model.\n    \"\"\"\n    \n    # Define Input and Output Variables\n    if type(X) != list:\n        X = df[[X]]\n    else:\n        X = df[X]\n    \n    y = df[[y]]\n    \n    # Train Test Split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize)\n    \n    # Initialise Linear Regression Model\n    lr = LinearRegression()\n    \n    # Fit Model\n    lr.fit(X_train, y_train)\n    coeff, intercept = lr.coef_, lr.intercept_\n    \n    # Predict Model\n    pred_train = lr.predict(X_train)\n    pred_test = lr.predict(X_test)\n    \n    # Metrics \n    train_r2 = lr.score(X_train, y_train)\n    test_r2 = lr.score(X_test, y_test)\n    \n    train_mse = mean_squared_error(y_train, pred_train)\n    test_mse = mean_squared_error(y_test, pred_test)\n    \n    return coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualise the predictions\n# 1. print out metrics scores\n# 2. display best-fit lines on the train and test data\n\ndef visualiseModel(col, y, df, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test):\n    \"\"\"\n    This function visualises the model's training and predictions. It prints out the metrics (R2 and MSE) scores as well as\n    displays the best-fit lines on the train and test data.\n    \n    Parameters\n    ----------\n    col: string or array-like\n    This represents the column(s) that was used in training and evaluting the model. \n    \n    y: string-like\n    This specifies the response variable.\n    \n    df: pandas DataFrame\n    This represents the DataFrame.\n    \n    coeff: float-like\n    This represents the coefficient of the best fit line (y=aX+c).\n    \n    intercept: float-like\n    This represents the intercept of the best fit line (y=aX+c).\n    \n    pred_train: Array-like\n    This represents the model predictions on X_train. \n    \n    pred_test: Array-like\n    This represents the model predictions on X_test.\n    \n    train_r2: float-like; between 0 and 1\n    This value is the Explain Variance for the training data. It measures the goodness of fit on training data. \n    Explained Variance suggests how much of the data can be explained by the model.\n    \n    test_r2: float-like; between 0 and 1\n    This value is the Explain Variance for the testing data. It measures the goodness of fit on testing data. \n    Explained Variance suggests how much of the data can be explained by the model.\n    \n    train_mse: float-like\n    This value represents the mean square error on training data.\n    \n    test_mse: float-like\n    This value represents the mean square error on testing data.\n    \n    X_train: array-like\n    This represents the input data used in training the model.\n    \n    X_test: array-like\n    This represents the input data used in testing the model.\n    \n    y_train: array-like\n    This represents the output data used in training the model.\n    \n    y_test: array-like\n    This represents the output data used in testing the model.\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    print(f\"Linear Regression with {col}\")\n    \n    # Print out metrics score\n    print(\"TRAINING\")\n    print(\"train_r2:\", train_r2)\n    print(\"train_mse:\", train_mse)\n    print('\\nTESTING')\n    print(\"test_r2:\", test_r2)\n    print(\"test_mse\", test_mse)\n    print()\n    \n    # Print equations of Best Fit Lines\n    if type(col) == list:\n        data = {\n            'coef_': coeff[0],\n            'intercept_': intercept,\n        }\n        print(pd.DataFrame(data, index=col))\n    else:\n        print(f\"Best Fit Line Equation for {col}: y = %f * {col} + %f\" % (coeff[0], intercept))\n    print()\n    \n    if not type(col) == list:\n        # Display Fits\n        f, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\n\n        # Display Training\n        axes[0].scatter(X_train[[col]], y_train[y], s=20, alpha=0.5)\n        axes[0].plot(X_train[[col]], coeff[0] * X_train + intercept, 'r-', label='Best Fit Line')\n        axes[0].set_title(f\"Training Fit for {col}\")\n        \n        # Display Testing\n        axes[1].scatter(X_test[[col]], y_test[y], s=20, alpha=0.5)\n        axes[1].plot(X_test[[col]], coeff[0] * X_test + intercept, 'r-', label='Best Fit Line')\n        axes[1].set_title(f\"Testing Fit for {col}\")\n        \n        plt.legend()\n    else:\n        print(\"Too many cols to plot!\")\n    \n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How do I go about transforming the data?\ndef logTransform(col):\n    \"\"\"\n    This function transforms the right-skew (non-zero) data to something more normally distributed. \n    - Strong algorithm for shifting left skew data.\n    - Does not work well with zero values.\n    \n    Parameters\n    ----------\n    col: pandas Series \n    This specifies the data to be manipulated.\n    \n    Returns\n    -------\n    np.log(col): pandas Series\n    Log Transformed data.\n    \"\"\"\n    return np.log(col)\n\ndef squareRootTransform(col):\n    \"\"\"\n    This function transforms the right-skew data to something more normally distributed. \n    - Weaker algorithm as compared to log or power transformation functions. \n    - Works for zero values.\n    \n    Parameters\n    ----------\n    col: pandas Series \n    This specifies the data to be manipulated.\n    \n    Returns\n    -------\n    np.sqrt(col): pandas Series\n    Square Root Transformed data.\n    \n    \"\"\"\n    return np.sqrt(col)\n\ndef powerTransform(col):\n    \"\"\"\n    This function transforms a left-skew data to something more normally distributed. \n    - Works for zero values.\n    \n    Parameters\n    ----------\n    col: pandas Series \n    This specifies the data to be manipulated.\n    \n    Returns\n    -------\n    pow(col, 2): pandas Series\n    Power Two Transformed data.\n    \"\"\"\n    return pow(col, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessingOutlier(col, df, threshold=0.05):\n    \"\"\"\n    This function removes outliers from the data. It only removes the outliers (below min_bound and above max_bound) if the \n    number of outliers constitutes less than a threshold (default = 0.05 (5%)) of the entire DataFrame (df).\n    \n    Parameters\n    ----------\n    col: string-type\n    This specifies the col in the DataFrame to check and remove (if applicable) outliers.\n    \n    df: pandas DataFrame or pandas Series\n    \n    threshold: float-like\n    This specifies the percentage threshold where outliers should be removed. Removing outliers might not always be a good\n    choice as it might remove important information about the dataset.\n    \n    Returns\n    -------\n    df_copy: pandas DataFrame or pandas Series\n    Preprocessed pandas DataFrame or pandas Series.\n    \"\"\"\n    df_copy = df.copy()\n    \n    # Check percentage of outliers \n    column = df_copy[col]\n    q1 = np.percentile(column, 25)\n    q3 = np.percentile(column, 75)\n    iqr = q3 - q1\n    min_bound = q1 - 1.5 * iqr\n    max_bound = q3 + 1.5 * iqr\n    \n    outliers_df = df_copy[(df_copy[col] <= min_bound) | (df_copy[col] >= max_bound)]\n    outlier_counts = outliers_df.shape[0]\n    outlier_percentage = outlier_counts / df.shape[0]\n    \n    if outlier_percentage <= threshold:\n        # Remove Outliers\n        print(\"%s Outlier Percentage is %.3f, that is less than or equals to the threshold value of %f\" % (col, outlier_percentage, threshold))\n        df_copy.drop(labels=outliers_df.index, inplace=True)\n    else:\n        # Don't Remove Outliers\n        print(\"%s Outlier Percentage is %.3f, that is more than the threshold value of %f\" % (col, outlier_percentage, threshold))\n    return df_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessingSkew(col, threshold = 1):\n    \"\"\"\n    This function decides if transformation functions should be applied to the given column based on the skew of the column and\n    the threshold value.\n    \n    Parameters\n    ----------\n    col: pandas Series\n    This specifies the data to be tested and skewed if necessary.\n    \n    threshold: float-like or int-like (positive value only)\n    This specifies the threshold for which a given column will be transformed. If the skew of the given column is greater than\n    the threshold, transformation function will be applied to the data. \n    \n    Returns\n    -------\n    col: pandas Series\n    Transformed (or not) col.\n    \n    \n    \"\"\"\n    skew = col.skew()\n    if skew >= threshold or skew <= -threshold:\n        if skew < 0:\n            # Left Skew \n            col = powerTransform(col)\n        else:\n            # Right Skew\n            if np.min(col) <= 0:\n                col = squareRootTransform(col)\n            else:\n                col = logTransform(col)\n    return col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a Pipeline that includes preprocessing and linear regression\ndef pipeline(X, y, df, testsize=0.25, thresholdOutlier=0.05, thresholdSkew=1):\n    \"\"\"\n    This function is a pipeline that includes training and testing with and without preprocessing to test evaluate the \n    goodness of fit of the model.\n    \n    Original DataFrame is not updated by the preprocessing function.\n    \n    Parameters\n    ----------\n    X: array-like or string-like\n    This specifies the predictor variable(s) (input variable(s)).\n    \n    y: string-like\n    This specifies the response variable (output variable).\n    \n    df: pandas DataFrame\n    This specifies the DataFrame.\n    \n    testsize: float-like\n    This specifies the test size for train_test_split.\n    \n    thresholdOutlier: float-like\n    This specifies the threshold for removing outliers.\n    \n    thresholdSkew: float-like or int-like\n    This specifies the threshold for skewing of data.\n    \n    Returns\n    -------\n    None\n    \"\"\"    \n    # Show type of Linear Regression\n    if type(X) == str:\n        print(\"Performing Univariate Linear Regression\\n\")\n    else:\n        print(\"Performing Multivariate Linear Regression\\n\")\n        \n    # Defining Variables for Visualisation\n    train_r2_list = []\n    test_r2_list = []\n    X_LABELS = X\n    \n    # Check for Categorical Data\n    categorical_columns = list(df.select_dtypes('category'))\n    \n    # Get Results for Not Processed Data\n    coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test = linreg(X, y, df, testsize)\n    visualiseModelV2(X, df, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test)\n    train_r2_list.append(train_r2)\n    test_r2_list.append(test_r2)\n    \n    # Preprocessed Data\n    df_1 = df.copy()  # DataFrame for Removing Outliers \n    df_2 = df.copy()  # DataFrame for Normalising Data\n    df_3 = df.copy() # DataFrame for Removing Outliers + Normalising Data\n    \n    # Removing Outliers \n    print(\"*****REMOVE OUTLIERS*****\")\n    if type(X) == str and not X in categorical_columns:\n        df_1 = preprocessingOutlier(X, df_1, thresholdOutlier)\n    else:\n        for col in X:\n            if col in categorical_columns: continue\n            df_1 = preprocessingOutlier(col, df_1, thresholdOutlier)\n    \n    coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test = linreg(X, y, df_1, testsize)\n    visualiseModelV2(X, df_1, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test)\n    train_r2_list.append(train_r2)\n    test_r2_list.append(test_r2)\n    \n    # Normalising Data\n    print(\"*****NORMALISING DATA*****\")\n    if type(X) == str and not X in categorical_columns:\n        df_2[X] = preprocessingSkew(df_2[X], thresholdSkew)\n    else:\n        for col in X:\n            if col in categorical_columns: continue\n            df_2[col] = preprocessingSkew(df_2[col], thresholdSkew)\n    coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test = linreg(X, y, df_2, testsize)\n    visualiseModelV2(X, df_2, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test)\n    train_r2_list.append(train_r2)\n    test_r2_list.append(test_r2)\n    \n    # Removing Outliers + Normalising Data\n    print(\"*****REMOVING OUTLIER + NORMALISING DATA*****\")\n    if type(X) ==str and not X in categorical_columns:\n        df_3 = preprocessingOutlier(X, df_3, thresholdOutlier)\n        df_3[X] = preprocessingSkew(df_3[X], thresholdSkew)\n    else:\n        for col in X:\n            if col in categorical_columns: continue\n            df_3 = preprocessingOutlier(col, df_3, thresholdOutlier)\n        for col in X:\n            if col in categorical_columns: continue\n            df_3[col] = preprocessingSkew(df_3[col], thresholdSkew)\n    coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test = linreg(X, y, df_3, testsize)\n    visualiseModelV2(X, df_3, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test)\n    train_r2_list.append(train_r2)\n    test_r2_list.append(test_r2)\n    \n    # Visualise Change in R2    \n    f, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n    \n    axes[0].plot(train_r2_list, 'o-')\n    axes[0].set_title(\"Change in Train R2\")\n    \n    axes[1].plot(test_r2_list, 'o-')\n    axes[1].set_title(\"Change in Test R2\")\n\n    return train_r2_list, test_r2_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualiseModelV2(col, df, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test):\n    \"\"\"\n    This function visualises the model's training and predictions. It prints out the metrics (R2 and MSE).\n    \n    Parameters\n    ----------\n    col: string or array-like\n    This represents the column(s) that was used in training and evaluting the model. \n    \n    df: pandas DataFrame\n    This represents the DataFrame.\n    \n    coeff: float-like\n    This represents the coefficient of the best fit line (y=aX+c).\n    \n    intercept: float-like\n    This represents the intercept of the best fit line (y=aX+c).\n    \n    pred_train: Array-like\n    This represents the model predictions on X_train. \n    \n    pred_test: Array-like\n    This represents the model predictions on X_test.\n    \n    train_r2: float-like; between 0 and 1\n    This value is the Explain Variance for the training data. It measures the goodness of fit on training data. \n    Explained Variance suggests how much of the data can be explained by the model.\n    \n    test_r2: float-like; between 0 and 1\n    This value is the Explain Variance for the testing data. It measures the goodness of fit on testing data. \n    Explained Variance suggests how much of the data can be explained by the model.\n    \n    train_mse: float-like\n    This value represents the mean square error on training data.\n    \n    test_mse: float-like\n    This value represents the mean square error on testing data.\n    \n    X_train: array-like\n    This represents the input data used in training the model.\n    \n    X_test: array-like\n    This represents the input data used in testing the model.\n    \n    y_train: array-like\n    This represents the output data used in training the model.\n    \n    y_test: array-like\n    This represents the output data used in testing the model.\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    print(f\"Linear Regression with {col}\")\n    \n    # Print out metrics score\n    print(\"TRAINING\")\n    print(\"train_r2:\", train_r2)\n    print(\"train_mse:\", train_mse)\n    print('\\nTESTING')\n    print(\"test_r2:\", test_r2)\n    print(\"test_mse\", test_mse)\n    print()\n    \n    # Print equations of Best Fit Lines\n    if type(col) == list:\n        data = {\n            'coef_': coeff[0],\n            'intercept_': intercept,\n        }\n        print(pd.DataFrame(data, index=col))\n    else:\n        print(f\"Best Fit Line Equation for {col}: y = %f * SalePrice + %f\" % (coeff[0], intercept))\n    print()\n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculateVIF(cols):\n    \"\"\"\n    This function calculates VIF to check for multi-collinearity and displays them in a DataFrame.\n    \n    Parameters\n    ----------\n    cols: string-like or array-like\n    This specifies the column(s) to check.\n    \n    Returns\n    -------\n    df: pandas DataFrame\n    DataFrame that contains the VIF of the columns.\n    \"\"\"\n    df = pd.DataFrame()\n    df['Variables'] = cols.columns\n    df[\"VIF\"] = [variance_inflation_factor(cols.values, i) for i in range(cols.shape[1])]\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def greedyFeatureSelection(X, y, df):\n    \"\"\"\n    This function does greedy search to select the best feature combination that will yield the best result. \"Best Result\" is \n    user defined. In this example, we want to maximise R2 score, hence, we determine \"Best Result\" by choosing combinations of \n    features that yields the highest (closest to 1) R2 score.\n    \n    Parameters\n    ----------\n    X: array-like\n    This represents the data.\n    \n    y: string-like\n    This represents the response variable.\n    \n    df: pandas DataFrame\n    This represents the dataframe.\n    \n    Returns\n    -------\n    best_score: array-like\n    This contains best scores (R2).\n    \n    best_features: array-like\n    This contains the best features that yields the maximum score (R2).\n    \"\"\"\n    best_scores, best_features = [], []\n    \n    while True:\n        this_feature = []\n        best_score = 0\n        \n        for feature in X:\n            if feature in best_features:\n                continue\n            \n            selected_features = best_features + [feature]\n            \n            coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test = linreg(selected_features, y, df)\n            score = max(train_r2, test_r2)\n            \n            if score > best_score:\n                best_score = score\n                this_feature = feature\n                \n        if this_feature != None:\n            best_features.append(this_feature)\n            best_scores.append(best_score)\n            \n        if len(best_scores) > 2:\n            if best_scores[-1] < best_scores[-2]:\n                break\n    return best_scores[:-1], best_features[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/vehicle-dataset-from-cardekho/car data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic EDA and Data Cleaning"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['name', 'year', 'selling_price', 'present_price', 'km_driven', 'fuel', 'seller_type',\n       'transmission', 'owner']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, cols = df.shape\nprint(\"Number of Rows:\", rows)\nprint(\"Number of Columns:\", cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_columns = df.columns\nprint(\"Columns:\", df_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing data.\n<br>\nThere are 4 ```object``` columns and 6 ```int64```/```float64``` columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"object_df = df.select_dtypes('object')\nnumeric_df = df.select_dtypes(['int64', 'float64'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"object_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"numeric_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hypothesis\n1. ```selling_price``` of cars with **higher** ```km_driven``` will sell for a lower price. This is because the cars experience wear and tear.\n2. ```selling_price``` of cars with manual ```transmission``` is **higher** than that of auto.\n3. ```selling_price``` of cars that run on diesel ```fuel``` is **lower** thant that of fuel.\n4. ```selling_price``` of cars with more ```owner```(s) is **lower**.\n5. Earlier ```year``` of cars will have **higher** ```km_driven```."},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis\nIn this section, I aim to explore, extract insights and provide answers for the hypothesises above. I will perform Univariate Analysis, Bivariate Analysis and Multivariate Analysis. \n<br>\nIn Univariate Analysis, we are interested in knowing the **Central Tendency** and the **Spread of Data**. In Bivariate Analysis, we are interested in finding out if the variables are **Mutually Dependent** and **Correlated**. In Multivariate Analysis, we are interestesd in observing how different (combinations of) features interact with one another."},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis and Visualisation"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"object_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in object_df.columns[1:]:\n    uniqueness(col, object_df)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation \nf, axes = plt.subplots(nrows=len(object_df.columns[1:]), ncols=1, figsize=(7,12))\n\nfor i in range(len(object_df.columns[1:])):\n    g = sns.countplot(x=object_df.columns[1:][i], data=object_df, ax=axes[i])\n    g.set_title(f\"\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analyse and Visualise ```name``` \n- Preprocess text data\n    - Lowercase \n    - Remove words with numbers\n    - Remove white spaces\n    - tokenise and create sparse matrix\n    - Remove stop words (if any)\n- Create wordcloud \n- Create freqdist and plot"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Unique Values Set A\nuniqueness('name', object_df[:4340])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Unique Values Set B\nuniqueness('name', object_df[4340:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessLowerCase(string):\n    return string.lower()\n\ndef preprocessRemoveStopWords(string, stopwords):\n    if isinstance(string, str):\n        # Tokenise\n        string = string.split(' ')\n    \n    for word in string:\n        if word in stopwords:\n            word = ''\n    return ' '.join(string)\n\ndef preprocessRemoveWhiteSpace(string):\n    if isinstance(string, str):\n        # Tokenise\n        string = string.split(' ')\n    \n    return ' '.join([i for i in string if i != ''])\n\ndef preprocessRemoveNumbersWord(string):\n    return re.sub(r'\\w*\\d\\w*', '', string).strip()\n\ndef preprocessPunctuation(string):\n    return ''.join([i for i in string if not i in punctuation])\n\ndef processText(string):\n    nltk_stopwords = stopwords.words('english')\n    string = preprocessLowerCase(string)\n    string = preprocessRemoveWhiteSpace(string)\n    string = preprocessRemoveNumbersWord(string)\n    string = preprocessPunctuation(string)\n    string = preprocessRemoveWhiteSpace(string)\n    string = preprocessRemoveStopWords(string, nltk_stopwords)\n    return string\nname = df['name']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE STOPWORDS LIBRARY \nnltk_stopwords = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name = name.apply(processText)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"corpus = ' '.join(list(name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(min_font_size=5, background_color= 'white').generate(corpus)\nplt.figure(figsize=(20,20))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"freqDist = FreqDist(corpus.split(' '))\nfreqDist.most_common()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further processing\n- Remove words with less then 3 letters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeShortWords(string):\n    if isinstance(string, str):\n        string = string.split(\" \")\n    \n    return ' '.join([word for word in string if len(word) >= 4])\n\nname = name.apply(removeShortWords)\ncorpus = ' '.join(list(name))\nwc = WordCloud(min_font_size=5, background_color= 'white').generate(corpus)\nplt.figure(figsize=(20,20))\nplt.imshow(wc, interpolation= 'bilinear')\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"freqDist = FreqDist(corpus.split(' '))\nfreqDist.most_common(50)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"numeric_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df[['selling_price', 'km_driven']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['selling_price'] == 0.1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like the dataset is not limited to 4 wheeled vehicles! \n<img src='bajaj-pulsar-150-black-red.png'>\n<center>Picture of a Binjaj Pulsar 150</centre>"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,7))\ncols = numeric_df[['selling_price', 'km_driven']].columns\n\nfor i in range(2):\n    g1 = sns.boxplot(x=cols[i], data= numeric_df, ax=axes[i, 0])\n    g1.set_title(f\"Boxplot of {cols[i]}\")\n    \n    g2 = sns.histplot(x=cols[i], data= numeric_df, ax=axes[i, 1], kde=True)\n    g2.set_title(f\"Histplot and KDEplot of {cols[i]}\")\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewKurtosis(cols, numeric_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analyse and Visualise ```year```\n- Earliest Year \n- Latest Year\n- Spread of Years"},{"metadata":{"trusted":true},"cell_type":"code","source":"year = numeric_df['year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_year = np.min(year)\nmax_year = np.max(year)\nprint(\"Min Year:\", min_year)\nprint(\"Max Year:\", max_year)\nprint(\"Range:\", max_year - min_year, 'years')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows=3, ncols=1, figsize=(15,10))\n\ng1 = sns.histplot(year, ax=axes[0])\ng1.set_title('Distributions of year')\n\ng2 = sns.kdeplot(year, ax=axes[1], color='r')\ng2.set_title('KDEplot of year')\n\ng3 = sns.boxenplot(year, ax=axes[2], color='g')\ng3.set_title(\"Boxenplot of year\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Central Tendencies and Spread of Year\nyear.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can expect the skew to be negative and kurtosis to be platykurtic! "},{"metadata":{"trusted":true},"cell_type":"code","source":"skewKurtosis('year', numeric_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh wait... never mind it is Leupokurtic!"},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis and Visualisation\nIn this section, I aim to provide answers for the hypothesis that I drafted above!\n>```selling_price``` of vehicles with **higher** ```km_driven``` will sell for a lower price. This is because the vehicles experience wear and tear.\n<br>\n>```selling_price``` of vehicles with manual ```transmission``` is **higher** than that of auto.\n<br>\n>```selling_price``` of vehicles that run on diesel ```fuel``` is **lower** thant that of fuel.\n<br>\n>```selling_price``` of vehicles with more ```owner```(s) is **lower**.\n<br>\n>Earlier ```year``` of vehicles will have **higher** ```km_driven```.\n<br>\n\nNote: I changed \"cars\" to \"vehicles\" because I found out that the dataset is not limited to 4-wheeled vehicles (cars)."},{"metadata":{},"cell_type":"markdown","source":"#### Hypothesis 1\n```selling_price``` of vehicles with **higher** ```km_driven``` will sell for a lower price. This is because the vehicles experience wear and tear."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='selling_price', y='km_driven', data=df, \n              height=7, space=0, alpha=0.7, kind='hex', cmap='BuGn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df['selling_price'].corr(df['km_driven'])\nprint(\"Correlation between selling_price and km_driven:\", corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PMCC is relatively close to 0 and it's magnitude is negative. This indicates a weak (almost negligible) negative correlation between ```selling_price``` and ```km_driven```. Therefore, vehicles with **higher** ```km_driven``` does not translate to **lower** ```selling_price```."},{"metadata":{},"cell_type":"markdown","source":"#### Hypothesis 2\n```selling_price``` of vehicles with manual ```transmission``` is **higher** than that of auto."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ng = sns.countplot(x='transmission', data=df)\ng.set_title(\"Countplot of Transmission\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more Manual cars as compared to Automatic cars."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows=3, ncols=1, figsize=(10,15))\n\n# KDEplot\ng1 = sns.kdeplot(x='selling_price', data=df, hue='transmission', ax=axes[0])\ng1.set_title('KDEplot of selling_price and transmission')\n\n# Boxplot\ng2 = sns.boxplot(x='selling_price', y='transmission', data=df, ax=axes[1])\ng2.set_title('Boxplot of selling_price and transmission')\n\n# Histplot\ng3 = sns.histplot(x='selling_price', data=df, hue='transmission', ax=axes[2])\ng3.set_title('Histplot of selling_price and transmission')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, we can observe that the ```selling_price``` for Automatic ```transmission``` is generally higher than that of Manual ```transmission```."},{"metadata":{"trusted":true},"cell_type":"code","source":"auto = df[df['transmission'] == 'Automatic']\nmanual = df[df['transmission'] == 'Manual']\n\ndef comparison(auto, manual, comparison):\n    auto_selling_price = auto[comparison]\n    manual_selling_price = manual[comparison]\n    \n    # Auto\n    min_auto = np.min(auto_selling_price)\n    max_auto = np.max(auto_selling_price)\n    range_auto = max_auto - min_auto\n    mean_auto = np.mean(auto_selling_price) \n    median_auto = np.median(auto_selling_price) \n    \n    # Manual\n    min_manual = np.min(manual_selling_price)\n    max_manual = np.max(manual_selling_price)\n    range_manual = max_manual - min_manual\n    mean_manual = np.mean(manual_selling_price)\n    median_manual = np.median(manual_selling_price)\n    \n    manual_list = [min_manual, max_manual, range_manual, mean_manual, median_manual]\n    auto_list = [min_auto, max_auto, range_auto, mean_auto, median_auto]\n    attributes = ['Min', 'Max', 'Range', 'Mean', 'Median']\n    \n    for i in range(len(attributes)):\n        if manual_list[i] > auto_list[i]:\n            print(f'Manual Tranissmion has a higher {attributes[i]} {comparison}:', manual_list[i])\n        else:\n            print(f'Auto Transmission has a higher {attributes[i]} {comparison}:', auto_list[i])\n    \n    return None\ncomparison(auto, manual, 'selling_price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can conclude that the ```selling_price``` of Automatic ```transmission``` is generally **higher** than that of Manual ```transmission```."},{"metadata":{},"cell_type":"markdown","source":"#### Hypothesis 3\n```selling_price``` of vehicles that run on diesel ```fuel``` is **lower** thant that of fuel."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ng = sns.countplot(x='fuel', data=df)\ng.set_title(\"Countplot of Transmission\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more Petrol ```fuel``` vehicles as compared to that of Diesel ```fuel```."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows=3, ncols=1, figsize=(10,15))\n\n# KDEplot\ng1 = sns.kdeplot(x='selling_price', data=df, hue='fuel', ax=axes[0])\ng1.set_title('KDEplot of selling_price and fuel')\n\n# Boxplot\ng2 = sns.boxplot(x='selling_price', y='fuel', data=df, ax=axes[1])\ng2.set_title('Boxplot of selling_price and fuel')\n\n# Histplot\ng3 = sns.histplot(x='selling_price', data=df, hue='fuel', ax=axes[2])\ng3.set_title('Histplot of selling_price and fuel')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, we can conclude that the ```selling_price``` of Diesel ```fuel``` vehicles is generally **higher** than that of Petrol ```fuel```! \n<br>\nWhile there are other ```fuel``` type vehicles, we are not interested in them as they are not as significant as Petrol and Diesel ```fuel``` type vehicles."},{"metadata":{},"cell_type":"markdown","source":"#### Hypothesis 4\n```selling_price``` of vehicles with more ```owner```(s) is **lower**."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows=3, ncols=1, figsize=(10,15))\n\n# KDEplot\ng1 = sns.kdeplot(x='selling_price', data=df, hue='owner', ax=axes[0])\ng1.set_title('KDEplot of selling_price and owner')\n\n# Boxplot\ng2 = sns.boxplot(y='selling_price', x='owner', data=df, ax=axes[1])\ng2.set_title('Boxplot of selling_price and owner')\n\n# Histplot\ng3 = sns.histplot(x='selling_price', data=df, hue='owner', ax=axes[2])\ng3.set_title('Histplot of selling_price and owner')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe a general downward ```selling_price``` trend as the number of owners increase!"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# year and selling_price\nplt.figure(figsize=(15, 10))\ng = sns.swarmplot(x='year', y='selling_price', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that there is an increase in ```selling_price``` over the years! "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df['selling_price'].corr(df['year'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hypothesis 5\nEarlier ```year``` of vehicles will have **higher**  ```km_driven```."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.boxplot(y='km_driven', x='year', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df['year'].corr(df['km_driven'])\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PMCC indicates a moderate negative correlation between ```year``` and ```km_driven```. As ```year``` increases, ```km_driven``` decreases! Hence, we can support our hypothesis!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Heatmap\ncorr = df.corr()\n\nplt.figure(figsize=(10,10))\ng = sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt='0.2f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode Categorical Features\ndf_copy = df.copy()\nfor col in ['fuel', 'seller_type', 'transmission', 'owner']:\n    df_copy = encode(col, df_copy)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Correlation Heatmap\ncorr = df_copy.corr()\n\nplt.figure(figsize=(10,10))\ng = sns.heatmap(corr, annot=True, vmin=-1, vmax=1, fmt='0.2f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```selling_price``` seem to have pretty low correlation with all the other features...."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in ['year', 'km_driven', 'fuel', 'seller_type', 'transmission', 'owner']:\n    correlation(col, 'selling_price', df_copy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```selling_price``` seem to have pretty low correlation with all the other features...."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.boxplot(y='selling_price', x='seller_type', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Welp. ```selling_price``` of Trustmark Dealer is **higher** than Dealer which is in turn **higher** than that of Individual! Well.... commission-based salary scheme could the result of this! Dealers will generally mark up the price to earn more commission for themselves!\n<br><br>\nNote: Buy from individual sellers!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.boxplot(y='km_driven', x='fuel', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that vehicles that run on Diesel ```fuel``` generally have further ```km_driven```! Diesel ```fuel``` is cheaper as compared to Petrol.. This might explain the trend!"},{"metadata":{},"cell_type":"markdown","source":"### TODO (DONE BELOW)\nCan we improve the correlation between ```selling_price``` and other features in the DataFrame by doing **Feature Engineering** and/or **Feature Preprocessing**?\n<br><br>\n**Feature Engineering**\n<br>\nUse other existing features to create new features that may provide us with more valuable information.\n<br><br>\n**Feature Preprocessing** (Done in pipeline())\n<br>\n1. Remove Outliers\n2. Normal (Gaussian) Transforms\n3. Scaling to StandardScaler"},{"metadata":{},"cell_type":"markdown","source":"### Multivariate Analysis and Visualisation\nSome questions that I want to answer in mutlivariate analysis!\n\n1. Is there a relationship between ```selling_price```, ```km_driven``` and ```fuel```?\n2. Is there a relationship between ```selling_price```, ```owner``` and ```km_driven```?\n3. Is there a relationship between ```km_driven```, ```fuel``` and ```transmission```?"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Explore Relationship between km_driven, fuel and transmission\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n\ng1 = sns.boxplot(x='km_driven', y='fuel', hue='transmission', data=df, ax=axes[0])\ng1.set_title(\"Boxplot between km_driven, fuel and transmission\")\ng2 = sns.boxplot(x='km_driven', y='transmission', hue='fuel', data=df, ax=axes[1])\ng2.set_title(\"Boxplot between km_driven, fuel and transmission\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, we can make some observations:\n1. There are no manual electric cars.\n2. ```km_driven``` on Automatic ```transmission``` is generally less as compared to that of Manual ```transmission```."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['fuel'] == 'Electric']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Relationship between selling_price, km_driven and fuel\nplt.figure(figsize=(15, 7))\n\ng = sns.scatterplot(x='selling_price', y='km_driven', hue='fuel', data=df, size='fuel', alpha=0.7)\ng.set_title(\"Group Scatterplot between selling_price, km_driven and fuel\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There isn't an apparent relationship between the 3 variables..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Relationship between selling_price, km_driven and owner\nplt.figure(figsize=(15, 7))\n\ng = sns.scatterplot(x='selling_price', y='km_driven', hue='owner', data=df, size='owner', alpha=0.7)\ng.set_title(\"Group Scatterplot between selling_price, km_driven and owner\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There isn't an apparent relationship between the 3 variables..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```python\nsns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\",\n    col=\"time\", hue=\"day\", style=\"day\",\n    kind=\"scatter\"\n)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Relationship between selling_price, km_driven, transmission and fuel\nsns.relplot(x='selling_price', y='km_driven', col='transmission', hue='fuel',\n            kind='scatter', data=df, alpha=0.5, height=10, size='fuel')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, we can observe:\n1. Diesel ```fuel``` vehicles generally have higher ```selling_price``` and ```km_driven```. This trend is observed in both Automatic and Manual ```transmission```.\n2. There is significantly less data points in Automatic ```transmission``` as compared to that of Manual ```transmission```."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore Relationship between selling_price, km_driven, transmission and owner\nsns.relplot(x='selling_price', y='km_driven', col='transmission', hue='owner',\n            kind='scatter', data=df, alpha=0.5, height=10, size='owner')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots, we can observe:\n1. First Owner ```owner``` have generally lower ```selling_price``` and ```km_driven```. This is observed in both ```transmission``` types.\n2. As expected, ```km_driven``` for Manual ```transmission``` is generally higher as compared to that of Automatic ```transmission``."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age of Vehicle (as of 2021)\nage = 2021 - df['year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n\ng1 = sns.histplot(age, ax=axes[0])\ng1.set_title(\"Histplot of Age\")\n\ng2 = sns.kdeplot(age, ax=axes[1], color='r')\ng2.set_title(\"KDEplot of Age\")\n\ng3 = sns.boxplot(age, ax=axes[2], color='g')\ng3.set_title(\"Boxplot of Age\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'] = age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode Categorical Columns\nfor col in ['fuel', 'seller_type', 'transmission', 'owner']:\n    df_copy = encode(col, df_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_copy.drop(columns=['selling_price', 'name'])\ny = df_copy[['selling_price']]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Greedy Feature Selection and Some Fun :D\nThis section is merely for fun, please see read!"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X = list(X.columns)\ny = 'selling_price'","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"bestest_of_the_best_features = []\n\nfor i in range(25):\n    best_scores, best_features=greedyFeatureSelection(X, y, df_copy)\n    bestest_of_the_best_features.append(best_features)\n    print(best_scores)\n    print(best_features)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"bestest_of_the_best_features = [i for j in bestest_of_the_best_features for i in j]\nCounter(bestest_of_the_best_features).most_common()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest counts would be the best predictor among the other features!"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pipeline(X, 'selling_price', df_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Regression \nfor col in X:\n    coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test = linreg(col, 'selling_price', df_copy)\n    visualiseModel(col, 'selling_price', df_copy, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Mutli-Variate Regression \ncoeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test = linreg(X, 'selling_price', df_copy)\nvisualiseModel(X, 'selling_price', df_copy, coeff, intercept, pred_train, pred_test, train_r2, test_r2, train_mse, test_mse, X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I exemplied the use of Linear Regression (though it might not be the best model) to predict ```selling_price``` of the vehicle! I aim to maximise R2 Score (Explained Variance). \n\n- If you have any questions, please post it in the comments!\n- Please suggest some improvements for me!\n- Please upvote if you found it useful!\n\nHave a nice day :D"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}