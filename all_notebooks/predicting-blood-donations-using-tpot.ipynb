{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# 1. Introduction\n\nBlood transfusion saves lives - from replacing lost blood during major surgery or a serious injury to treating various illnesses and blood disorders. Ensuring that there's enough blood in supply whenever needed is a serious challenge for the health professionals. According to [WebMD](https://www.webmd.com/a-to-z-guides/blood-transfusion-what-to-know#1), \"about 5 million Americans need a blood transfusion every year\".\n\nThis dataset is from a mobile blood donation vehicle in Taiwan. The Blood Transfusion Service Center drives to different universities and collects blood as part of a blood drive. I want to predict whether or not a donor will give blood the next time the vehicle comes to campus.\n\nThe data is structured according to RFMTC marketing model (a variation of RFM). Let's get started exploring the data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tpot import TPOTClassifier\nfrom sklearn.metrics import roc_auc_score\n\ntransfusion = pd.read_csv(\"../input/donations.csv\")\ntransfusion.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd9bcdc7a7e478e0e9c76635c3649a84595f5814"},"cell_type":"markdown","source":"The RFM model stands for Recency, Frequency and Monetary Value and it is commonly used in marketing for identifying the best customers. In this case, the customers are blood donors.\n\nRFMTC is a variation of the RFM model. Below is a description of what each column means in the dataset:\n*     R (Recency - months since the last donation)\n*     F (Frequency - total number of donation)\n*     M (Monetary - total blood donated in c.c.)\n*     T (Time - months since the first donation)\n*     a binary variable representing whether he/she donated blood in March 2007 (2 stands for donating blood; 1 stands for not donating blood)\n\nIt will be helpful to rename these columns as such; except for the last column, which will be the <code>Target</code> column, as the aim is to predict whether someone donated blood in March 2007.\n"},{"metadata":{"trusted":true,"_uuid":"1e1b669234bd882d3e2bbd3b0a0c5273acb1420e"},"cell_type":"code","source":"transfusion.rename(\n    columns={'V1':'Recency (months)',\n             'V2':'Frequency(times)',\n             'V3':'Monetary (c.c. blood)',\n             'V4':'Time (months)',\n             'Class':'Target'},\n    inplace=True\n)\n\ntransfusion.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc1dcba56b7dce77513f2bd0dd548211ea6e9bd8"},"cell_type":"markdown","source":"It looks like every column in this DataFrame has the numeric type, which is exactly what is required when building a machine learning model. Let's verify the hypothesis."},{"metadata":{"trusted":true,"_uuid":"0ad2c51794121bca7e06bfb20128df35a546257a"},"cell_type":"code","source":"transfusion.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34e94aaf1c2ef582f117360fb4cecd1f6281e940"},"cell_type":"markdown","source":"I want to predict whether or not the same donor will give blood the next time the vehicle comes to campus. The model for this is a binary classifier, meaning that there are only 2 possible outcomes:\n\n*     <code>1</code> - the donor will not give blood\n*     <code>2</code> - the donor will give blood\n\nTarget incidence is defined as the number of cases of each individual target value in a dataset. That is, how many 1s in the target column compared to how many 2s? Target incidence gives us an idea of how balanced (or imbalanced) is our dataset.\n\nFurther, it'll be later useful to convert the (1, 2) values of Target to (0, 1)."},{"metadata":{"trusted":true,"_uuid":"ef85f5308051a1fa001279f2ddd933ba73f92b2a"},"cell_type":"code","source":"transfusion['Target'] = transfusion['Target'].replace([1, 2], [0, 1])\n\ndisplay(transfusion['Target'].value_counts(normalize = True))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d38a88b90e159a5e2ee1acf8533fe7a9f84259a"},"cell_type":"markdown","source":"Target incidence indicates that about 76% of the time an individual does not give blood.\n\nI will now split this dataframe into train and test datasets, with testing data 25% of the total data. In doing so, I will also take care to keep the target incidence the same in both these datasets, i.e. they should both have roughly 76% 1s in their <code>Target</code> columns."},{"metadata":{"trusted":true,"_uuid":"4f569cb7f16cf8adb707f27389d5de7ca33fbe11"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    transfusion.drop(columns = 'Target'),\n    transfusion.Target,\n    test_size = 0.25,\n    random_state = 42,\n    stratify = transfusion.Target\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66c468f2b209a2784e9a5809448483adc792037b"},"cell_type":"markdown","source":"# 2. Using TPOT to select model\n\n[TPOT](https://github.com/EpistasisLab/tpot) is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. It automates the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for given data.\n\n![](https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-ml-pipeline.png)\n\nTPOT is built on top of scikit-learn, so all of the code it generates will be a scikit-learn pipeline, meaning it will include any pre-processing steps as well as the model.\n\nI am using TPOT to help zero in on one model that can then be explored and optimized further."},{"metadata":{"trusted":true,"_uuid":"2843a4524c988a4375ad193363ae4b651152061b"},"cell_type":"code","source":"tpot = TPOTClassifier(\n    generations = 5,\n    population_size = 20,\n    verbosity = 2,\n    scoring = 'roc_auc',\n    random_state = 42,\n    disable_update_check = True,\n    config_dict = 'TPOT light'\n)\ntpot.fit(X_train, y_train)\n\ntpot_auc_score = roc_auc_score(y_test, tpot.predict_proba(X_test)[:, 1])\nprint(f'\\nAUC score: {tpot_auc_score:.4f}')\n\nprint('\\nBest pipeline steps:', end='\\n')\nfor idx, (name, transform) in enumerate(tpot.fitted_pipeline_.steps, start=1):\n    print(f'{idx}. {transform}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50960a82efbf92a15f8fd45b3476a91419cb2fef"},"cell_type":"markdown","source":"TPOT has picked <code>LogisticRegression</code> as the best pipeline step for this data with an AUC score of 0.7850. However, one of the assumptions for linear regression models is that the data and the features we are giving it are related in a linear fashion, or can be measured with a linear distance metric. If a feature in the dataset has a high variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset.\n\nSo before applying regression, we need to check our data for its variance, and normalize if needed."},{"metadata":{"trusted":true,"_uuid":"4c2cf6cb673f5a27d0fc2eb2ce9a640308e1339b"},"cell_type":"code","source":"display(X_train.var())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b70f7a1912c245f1abd693a6d2c922fe75a7916"},"cell_type":"markdown","source":"<code>Monetary (c.c. blood)</code>'s variance is very high in comparison to any other column in the dataset. This means that, unless accounted for, this feature may get more weight by the model (i.e., be seen as more important) than any other feature.\n\nOne way to correct for high variance is to use log normalization."},{"metadata":{"trusted":true,"_uuid":"d63916fafa701ae0dfe50c70315a5664365d03ed"},"cell_type":"code","source":"import numpy as np\n\nX_train_normed, X_test_normed = X_train.copy(), X_test.copy()\ncol_to_normalize = \"Monetary (c.c. blood)\"\n\n# Log normalization\nfor df_ in [X_train_normed, X_test_normed]:\n    df_['Monetary_log'] = np.log(df_[col_to_normalize])\n    df_.drop(columns = col_to_normalize, inplace=True)\n\ndisplay(X_train_normed.var())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33350df04ef45adc6b3be3a153ed06ee1943289e"},"cell_type":"markdown","source":"The variance now looks much better. While the variance of <code>Time (months)</code> is still high, it is not different by several orders of magnitude, which means that the data is now ready for regression.\n\n# 3. Linear Regression"},{"metadata":{"trusted":true,"_uuid":"ac0b3a4ddf492509fa26ab48d17060e3c9b0af04"},"cell_type":"code","source":"from sklearn import linear_model\n\nlogreg = linear_model.LogisticRegression(\n    solver='liblinear',\n    random_state=42\n)\n\nlogreg.fit(X_train_normed, y_train)\n\nlogreg_auc_score = roc_auc_score(y_test, logreg.predict_proba(X_test_normed)[:, 1])\nprint(f'\\nAUC score: {logreg_auc_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff3119294a458d61c1cc2f1b5053e2c1fbafeb9a"},"cell_type":"markdown","source":"In this notebook, I explored automatic model selection using TPOT and AUC score we got was 0.7850. This is better than simply choosing 0 all the time (the target incidence suggests that such a model would have 76% success rate). We then log normalized our training data and improved the AUC score by 0.5%. In the field of machine learning, even small improvements in accuracy can be important, depending on the purpose."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}