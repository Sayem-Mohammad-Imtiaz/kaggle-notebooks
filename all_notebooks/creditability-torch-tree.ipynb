{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://cdn.cms-twdigitalassets.com/content/blog-twitter/engineering/en_us/topics/open-source/2017/Introducing-Torch-Decision-Trees/_jcr_content/par/rail-blog-container/column/image.img.png/1507323277313.png)blog.twitter.com"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Code By Alin Cijov  https://www.kaggle.com/alincijov/credit-card-pytorch-decision-tree/comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import regularizers, optimizers\nfrom keras import losses\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Input, Dropout, Embedding, LSTM\nfrom keras.optimizers import RMSprop, Adam, Nadam\nfrom keras.preprocessing import sequence\n\nimport torch\nfrom torch import nn\nfrom functools import reduce\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport tensorflow\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('../input/cusersmarildownloadsgermancsv/german.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf.dataframeName = 'german.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize\ndf['Account_Balance'] = StandardScaler().fit_transform(df['Account_Balance'].values.reshape(-1, 1))\ndf['Duration_of_Credit_monthly'] = StandardScaler().fit_transform(df['Duration_of_Credit_monthly'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalies = df[df[\"Creditability\"] == 1]\nnormal = df[df[\"Creditability\"] == 0]\n\nanomalies.shape, normal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in range(0, 20):\n    normal = normal.iloc[np.random.permutation(len(normal))]\n    \n\ndata_set = pd.concat([normal[:2000], anomalies])\n\nx_train, x_test = train_test_split(data_set, test_size = 0.4, random_state = 42)\n\nx_train = x_train.sort_values(by=['Duration_of_Credit_monthly'])\nx_test = x_test.sort_values(by=['Duration_of_Credit_monthly'])\n\ny_train = x_train[\"Creditability\"]\ny_test = x_test[\"Creditability\"]\n\nx_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.array(x_train).reshape(x_train.shape[0], x_train.shape[1])\nx_test = np.array(x_test).reshape(x_test.shape[0], x_test.shape[1])\ninput_shape = (x_train.shape[1], 1)\n\ny_train = keras.utils.to_categorical(y_train, 2)\ny_test = keras.utils.to_categorical(y_test, 2)\n\nprint(\"Shapes:\\nx_train:%s\\ny_train:%s\\n\" % (x_train.shape, y_train.shape))\nprint(\"x_test:%s\\ny_test:%s\\n\" % (x_test.shape, y_test.shape))\nprint(\"input_shape:{}\\n\".format(input_shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tree():\n    def __init__(self):\n        self.num_cut = [1, 1]\n        self.num_leaf = np.prod(np.array(self.num_cut) + 1)\n        self.num_class = 2\n        \n    def torch_kron_prod(self, a, b):\n        res = torch.einsum('ij,ik->ijk', [a, b])\n        res = torch.reshape(res, [-1, np.prod(res.shape[1:])])\n        return res\n    \n    def torch_bin(self, x, cut_points, temperature=0.1):\n        D = cut_points.shape[0]\n        W = torch.reshape(torch.linspace(1.0, D + 1.0, D + 1), [1, -1])\n        cut_points, _ = torch.sort(cut_points)\n        b = torch.cumsum(torch.cat([torch.zeros([1]), -cut_points], 0),0)\n        h = torch.matmul(x, W) + b\n        res = torch.exp(h-torch.max(h))\n        res = res/torch.sum(res, dim=-1, keepdim=True)\n        return h\n    \n    def nn_decision_tree(self, x, cut_points_list, leaf_score, temperature=0.1):\n        leaf = reduce(self.torch_kron_prod,\n                      map(lambda z: self.torch_bin(x[:, z[0]:z[0] + 1], z[1], temperature), enumerate(cut_points_list)))\n        return torch.matmul(leaf, leaf_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = Tree()\n\ncut_points_list = [torch.rand([i], requires_grad=True) for i in tree.num_cut]\nleaf_score = torch.rand([tree.num_leaf, tree.num_class], requires_grad=True)\n\nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cut_points_list + [leaf_score], lr=0.001, weight_decay=0.001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Numbers below were taken from shapes. (Input 8)  Shapes:\nx_train:(600, 21)\ny_train:(600, 2)\n\nSo I wrote 21 (columns number) and 2 (last number). The 16 and 8, I just copied from Alin Cijov. Have no clue about them.\n\nhttps://www.kaggle.com/alincijov/credit-card-pytorch-decision-tree/comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = nn.Sequential(\n          nn.Linear(21,16),\n          nn.ReLU(),\n          nn.Linear(16,8),\n          nn.ReLU(),\n          nn.Linear(8,2),\n          nn.Sigmoid()\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(300):\n    optimizer.zero_grad()\n    x_batches = torch.split(torch.tensor(x_train).type(torch.float32), 32)\n    y_batches = torch.split(torch.tensor(y_train).type(torch.long), 32)\n    \n    losses = torch.zeros(len(x_batches))\n    accs = torch.zeros(len(x_batches))\n    for j,x in enumerate(x_batches):\n        out = model(x)\n        y_pred = tree.nn_decision_tree(out, cut_points_list, \n                                       leaf_score, temperature=0.1)\n        y_max = torch.max(y_batches[j], axis=1)[1]\n        y_pred_max = torch.max(y_pred, axis=1)[1]\n        \n        acc = len(torch.where(y_max == y_pred_max)[0]) / len(y_max)\n        accs[j] = acc\n        \n        loss = loss_function(y_pred, y_max)\n        losses[j] = loss\n        \n        loss.backward()\n        optimizer.step()\n    if((i+1) % 20 == 0):\n        print(\"i:{:4d}, loss:{:1.3f}, acc:{:1.3f}\".format(i+1, losses.mean(), accs.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(tree, X, y):\n    X = torch.tensor(X).type(torch.float32)\n    y = torch.tensor(y).type(torch.long)\n\n    out = model(X)\n    y_pred = tree.nn_decision_tree(out, cut_points_list, leaf_score, temperature=0.1)\n    y_max = torch.max(y, axis=1)[1]\n    y_pred_max = torch.max(y_pred, axis=1)[1]\n\n    acc = len(torch.where(y_max == y_pred_max)[0]) / len(y_max)\n    loss = loss_function(y_pred, y_max)\n    return y_pred, acc, loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred, acc, loss = predict(tree, x_test, y_test)\nprint(\"Accuracy:{:1.3f}, Loss:{:1.3f}\"\n         .format(acc, loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Visualization:\n    labels = [\"Normal\", \"Anomaly\"]\n\n    def draw_confusion_matrix(self, y, ypred):\n        matrix = confusion_matrix(y, ypred)\n\n        plt.figure(figsize=(10, 8))\n        colors=[ \"pink\",\"purple\"]\n        sns.heatmap(matrix, xticklabels=self.labels, yticklabels=self.labels, cmap=colors, annot=True, fmt=\"d\")\n        plt.title(\"Confusion Matrix\")\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n        plt.show()\n\n\n    def draw_anomaly(self, y, error, threshold):\n        groupsDF = pd.DataFrame({'error': error,\n                                 'true': y}).groupby('true')\n\n        figure, axes = plt.subplots(figsize=(12, 8))\n\n        for name, group in groupsDF:\n            axes.plot(group.index, group.error, marker='x' if name == 1 else 'o', linestyle='',\n                    color='r' if name == 1 else 'g', label=\"Anomaly\" if name == 1 else \"Normal\")\n\n        axes.hlines(threshold, axes.get_xlim()[0], axes.get_xlim()[1], colors=\"b\", zorder=100, label='Threshold')\n        axes.legend()\n        \n        plt.title(\"Anomalies\")\n        plt.ylabel(\"Error\")\n        plt.xlabel(\"Data\")\n        plt.show()\n\n    def draw_error(self, error, threshold):\n            plt.plot(error, marker='o', ms=3.5, linestyle='',\n                     label='Point')\n\n            plt.hlines(threshold, xmin=0, xmax=len(error)-1, colors=\"b\", zorder=100, label='Threshold')\n            plt.legend()\n            plt.title(\"Reconstruction error\")\n            plt.ylabel(\"Error\")\n            plt.xlabel(\"Data\")\n            plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize = Visualization()\ny_pred2 = torch.max(y_pred, axis=1)[1].detach().numpy()\ny_test2 = np.argmax(y_test, axis=1)\nvisualize.draw_confusion_matrix(y_test2, y_pred2)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Alin Cijov for all the code' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}