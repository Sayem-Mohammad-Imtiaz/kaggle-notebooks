{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom keras.preprocessing.text import Tokenizer,text_to_word_sequence\nimport nltk\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, GRU\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\nfrom tensorflow.compat.v1.nn.rnn_cell import GRUCell\nfrom keras.engine.topology import Layer\nfrom keras.models import Model\n# nltk.download('punkt')\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 80\nmax_sentences = 15\nmax_words = 20000\nembedding_dim = 100\nbatch_size = 256\nvalidation_split = 0.2\nhidden_size=150 \nattention_size = 50\nkeep_prob1 = 0.8\nkeep_prob2 = 0.8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n  '''\n  Remove non-ascii characters, multiple spaces, and newlines\n  '''\n  text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n  text = re.sub(r'\\n',' ', text)\n  text = re.sub(r\" +\",\" \",text)\n  return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/wine-reviews/winemag-data_first150k.csv')\ndata = data[data.duplicated('description', keep=False)]\ndata = data[:20000]\n\noutlier_cutoff = np.percentile(data['points'], 95)\nc_vec = np.clip(data['points'], 0, outlier_cutoff)\nmedian_score = np.median(c_vec)\nstd_score = data['points'].std()\nprint(outlier_cutoff, median_score, std_score)\ndata['points'] = c_vec\ndata['points_zscore'] = ((c_vec-median_score)/std_score)\n\ntext = data['description']\npara = text.apply(lambda x: x.split('.'))\n\nmerged_sent = []\nfor sentences in para:\n    m = len(sentences)\n    for sentence in sentences:\n        merged_sent.append(sentence)\n        \nmerged_sent = np.array(merged_sent)\n\n\nX = []\ny = []\nreviews = []\nfor index, row in data.iterrows():\n    X.append(row['description'])\n    y.append(row['points_zscore'])\n\nX = np.array(X)\ny = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(merged_sent)\n\ndata = np.zeros((para.shape[0], max_sentences, maxlen), dtype='int32')\n# print(reviews)\nfor i, review in enumerate(para):\n    #print(\"in review\")\n    for j, sentence in enumerate(review):\n        if j < max_sentences:\n            tokens = text_to_word_sequence(sentence)\n            k = 0\n            for _, word in enumerate(tokens):\n                if k < maxlen and tokenizer.word_index[word] < max_words:\n                    data[i, j, k] = tokenizer.word_index[word]\n                    k = k + 1\n\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = y[indices]\nnb_validation_samples = int(validation_split * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = y[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = y[-nb_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X\ndel labels\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open('../input/glove6b100dtxt/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nword_index = tokenizer.word_index\nembedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\ndel embeddings_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_attention(inputs, att_size):\n    inputs = tf.concat(inputs, 2) # (n, embedding_size, hiddensize) -> (n, 80, 300)\n    hiddensize = inputs.shape[2].value  # D value - hidden size of the RNN layer -> 300\n    \n    # Trainable parameters\n    W_word = tf.Variable(tf.random_normal([hiddensize, att_size], stddev=0.1)) # (hiddensize, attsize) -> (300, 50)\n    b_word = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n    u_word = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n    v_word = tf.tanh(tf.tensordot(inputs, W_word, axes=1) + b_word)  # v = tanh(W.x + b) . (1,attsize) -> (n, 80, 50)\n    vu_word = tf.tensordot(v_word, u_word, axes=1, name='vu')  # v.u (n,80)\n    scores = tf.nn.softmax(vu_word)   # attention scores (n,80)\n\n    output = tf.reduce_sum(inputs * tf.expand_dims(scores, -1), 1)\n    return output\n\ndef sentence_attention(inputs, att_size):\n    inputs = tf.concat(inputs, 2) # (n, embedding_size, hiddensize) -> (n, 80, 300)\n    hiddensize = inputs.shape[2].value  # D value - hidden size of the RNN layer -> 300\n    \n    # Trainable parameters\n    W_sent = tf.Variable(tf.random_normal([hiddensize, att_size], stddev=0.1)) # (hiddensize, attsize) -> (300, 50)\n    b_sent = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n    u_sent = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n    v_sent = tf.tanh(tf.tensordot(inputs, W_sent, axes=1) + b_sent)  # v = tanh(W.x + b) . (1,attsize) -> (n, 80, 50)\n    vu_sent = tf.tensordot(v_sent, u_sent, axes=1, name='vu')  # v.u (n,80)\n    scores = tf.nn.softmax(vu_sent)   # attention scores (n,80)\n\n    output = tf.reduce_sum(inputs * tf.expand_dims(scores, -1), 1)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\n\nsentence_input = tf.placeholder(tf.int32, [None,max_sentences, maxlen])\ninput_len = tf.placeholder(tf.int32)\noutput_x = tf.placeholder(tf.int32, [None])\noutput_x = tf.cast(output_x,tf.float32) \n\n# Word level\n\nwith tf.variable_scope(\"word\") as scope:\n  count = 0\n  mat = []\n  for word in tf.unstack(sentence_input,axis=1):\n    embeddings_var = tf.Variable(embedding_matrix, trainable=True)\n    embeddings = tf.nn.embedding_lookup(embeddings_var, word, partition_strategy='div')\n    embeddings = tf.cast(embeddings,tf.float32) \n    rnn_outputs, _ = bidirectional_dynamic_rnn(GRUCell(hidden_size, dtype=tf.float32), GRUCell(hidden_size, dtype=tf.float32), inputs=embeddings, dtype=tf.float32)\n    weighted_inputs = word_attention(rnn_outputs,attention_size)\n    weighted_inputs = tf.reshape(weighted_inputs, [input_len, 1, weighted_inputs.shape[1]])\n    scope.reuse_variables()    \n    mat.append(weighted_inputs)\n    count+=1\n\nafter_word_attention = tf.stack(mat, axis=1)\nafter_word_attention = tf.reshape(after_word_attention, [input_len, 15, 300])\ndropout_1 = tf.nn.dropout(after_word_attention, keep_prob1)\nrnn_outputs_sent, _ = bidirectional_dynamic_rnn(GRUCell(hidden_size*1.5, dtype=tf.float32), GRUCell(hidden_size*1.5, dtype=tf.float32), inputs=dropout_1, dtype=tf.float32)\nweighted_inputs_sent = sentence_attention(rnn_outputs_sent,attention_size)\ndropout_2 = tf.nn.dropout(weighted_inputs_sent, keep_prob2)\nfc = tf.keras.layers.Dense(units=1, activation='relu')(dropout_2)\n#loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=fc, labels=output_x)\nloss = tf.reduce_mean(tf.keras.losses.MSE(fc,output_x))\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)\n\npred = tf.round(fc)\n# n_correct = tf.equal(pred, output_x)\n# accuracy = tf.reduce_mean(tf.cast(n_correct, tf.float32))\naccuracy = tf.reduce_mean(tf.keras.losses.MAE(pred,output_x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_generator(X, y, batchsize):\n    size = X.shape[0]\n    x_copy = X.copy()\n    y_copy = y.copy()\n    ind = np.arange(size)\n    np.random.shuffle(ind)\n    x_copy = x_copy[ind]\n    y_copy = y_copy[ind]\n    i = 0\n    while True:\n        if i + batchsize <= size:\n            yield x_copy[i:i + batchsize], y_copy[i:i + batchsize]\n            i += batchsize\n        else:\n            i = 0\n            ind = np.arange(size)\n            np.random.shuffle(ind)\n            x_copy = x_copy[ind]\n            y_copy = y_copy[ind]\n            continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_batch_generator = batch_generator(x_train, y_train, batch_size)\ntest_batch_generator = batch_generator(x_val, y_val, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as sess:\n    tf.initialize_all_variables().run()\n\n    # Training \n    epochs = 10\n    for epoch in range(epochs):\n        acc_sum = 0\n        #     n_batches = 0 \n        print(\"\\nEpoch: \",epoch+1)\n        #     for i in range(0,20000, 200):\n        #       try:\n        #         xpoo = x_train[i:i+200]\n        #         ypoo = y_train[i:i+200]\n        #       except:\n        #         xpoo = x_train[i::]\n        #         ypoo = y_train[i::]\n        num_batches = x_train.shape[0] // batch_size\n        for b in tqdm_notebook(range(num_batches)):\n            x_batch_train, y_batch_train = next(train_batch_generator)\n            length = len(y_batch_train)\n            loss_acc,opt, acc = sess.run([loss,optimizer,accuracy], feed_dict={sentence_input:x_batch_train, output_x:y_batch_train, input_len: length})\n            acc_sum += acc\n        #       n_batches += 1\n        #           if(b % batch_size == 0):\n        #         print(loss_acc, end =\" \")\n        #         print(acc)\n        # print(acc_sum, n_batches)\n        print('==> MSE Error: ', acc_sum/num_batches)\n  # Validation\n        val_accs=[]\n        num_batches = x_val.shape[0] // batch_size\n        for b in tqdm_notebook(range(num_batches)):\n            x_batch_test, y_batch_test = next(test_batch_generator)\n            length = len(y_batch_test)\n            val_accs.append(sess.run(accuracy, feed_dict={sentence_input:x_batch_test, output_x:y_batch_test, input_len: length}))\n\n    print(\"==> Test Accuracy: \", val_accs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}